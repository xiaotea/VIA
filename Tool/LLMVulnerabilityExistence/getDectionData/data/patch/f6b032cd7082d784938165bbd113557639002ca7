{
    "ckan/lib/search/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from ckan.lib.search.common import ("
            },
            "2": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": "     make_connection, SearchIndexError, SearchQueryError,  # type: ignore"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+    SolrConnectionError, # type: ignore"
            },
            "4": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 26,
                "PatchRowcode": "     SearchError, is_available, SolrSettings, config"
            },
            "5": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " )"
            },
            "6": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from ckan.lib.search.index import ("
            }
        },
        "frontPatchFile": [
            "# encoding: utf-8",
            "",
            "from __future__ import annotations",
            "",
            "import logging",
            "import sys",
            "import cgitb",
            "import warnings",
            "import traceback",
            "",
            "import xml.dom.minidom",
            "from typing import Collection, Any, Optional, Type, cast, overload",
            "",
            "import requests",
            "from requests.auth import HTTPBasicAuth",
            "",
            "import ckan.model as model",
            "import ckan.model.domain_object as domain_object",
            "import ckan.plugins as p",
            "import ckan.logic as logic",
            "from ckan.types import Context",
            "",
            "from ckan.lib.search.common import (",
            "    make_connection, SearchIndexError, SearchQueryError,  # type: ignore",
            "    SearchError, is_available, SolrSettings, config",
            ")",
            "from ckan.lib.search.index import (",
            "    SearchIndex, PackageSearchIndex, NoopSearchIndex",
            ")",
            "from ckan.lib.search.query import (",
            "    SearchQuery,",
            "    TagSearchQuery, ResourceSearchQuery, PackageSearchQuery,",
            "    QueryOptions, convert_legacy_parameters_to_solr  # type: ignore",
            ")",
            "from ckan.lib.search.index import SearchIndex",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def text_traceback() -> str:",
            "    info = sys.exc_info()",
            "    with warnings.catch_warnings():",
            "        warnings.simplefilter(\"ignore\")",
            "        try:",
            "            text = cgitb.text(info)",
            "        except RuntimeError:",
            "            # there is werkzeug.local.LocalProxy object inside traceback, that",
            "            # cannot be printed out by the cgitb",
            "            res = \"\".join(traceback.format_tb(info[-1]))",
            "        else:",
            "            res = 'the original traceback:'.join(",
            "                text.split('the original traceback:')[1:]",
            "            ).strip()",
            "",
            "    return res",
            "",
            "",
            "SUPPORTED_SCHEMA_VERSIONS = ['2.8', '2.9', '2.10']",
            "",
            "DEFAULT_OPTIONS = {",
            "    'limit': 20,",
            "    'offset': 0,",
            "    # about presenting the results",
            "    'order_by': 'rank',",
            "    'return_objects': False,",
            "    'ref_entity_with_attr': 'name',",
            "    'all_fields': False,",
            "    'search_tags': True,",
            "    'callback': None,  # simply passed through",
            "}",
            "",
            "_INDICES: dict[str, Type[SearchIndex]] = {",
            "    'package': PackageSearchIndex",
            "}",
            "",
            "_QUERIES: dict[str, Type[SearchQuery]] = {",
            "    'tag': TagSearchQuery,",
            "    'resource': ResourceSearchQuery,",
            "    'package': PackageSearchQuery",
            "}",
            "",
            "SOLR_SCHEMA_FILE_OFFSET_MANAGED = '/schema?wt=schema.xml'",
            "SOLR_SCHEMA_FILE_OFFSET_CLASSIC = '/admin/file/?file=schema.xml'",
            "",
            "",
            "def _normalize_type(_type: Any) -> str:",
            "    if isinstance(_type, domain_object.DomainObject):",
            "        _type = _type.__class__",
            "    if isinstance(_type, type):",
            "        _type = _type.__name__",
            "    return _type.strip().lower()",
            "",
            "",
            "@overload",
            "def index_for(_type: Type[model.Package]) -> PackageSearchIndex: ...",
            "",
            "@overload",
            "def index_for(_type: Any) -> SearchIndex: ...",
            "",
            "def index_for(_type: Any) -> SearchIndex:",
            "    \"\"\" Get a SearchIndex instance sub-class suitable for",
            "        the specified type. \"\"\"",
            "    try:",
            "        _type_n = _normalize_type(_type)",
            "        return _INDICES[_type_n]()",
            "    except KeyError:",
            "        log.warn(\"Unknown search type: %s\" % _type)",
            "        return NoopSearchIndex()",
            "",
            "",
            "@overload",
            "def query_for(_type: Type[model.Package]) -> PackageSearchQuery:",
            "    ...",
            "",
            "",
            "@overload",
            "def query_for(_type: Type[model.Resource]) -> ResourceSearchQuery:",
            "    ...",
            "",
            "",
            "@overload",
            "def query_for(_type: Type[model.Tag]) -> TagSearchQuery:",
            "    ...",
            "",
            "",
            "def query_for(_type: Any) -> SearchQuery:",
            "    \"\"\" Get a SearchQuery instance sub-class suitable for the specified",
            "        type. \"\"\"",
            "    try:",
            "        _type_n = _normalize_type(_type)",
            "        return _QUERIES[_type_n]()",
            "    except KeyError:",
            "        raise SearchError(\"Unknown search type: %s\" % _type)",
            "",
            "",
            "def dispatch_by_operation(entity_type: str, entity: dict[str, Any],",
            "                          operation: str) -> None:",
            "    \"\"\"Call the appropriate index method for a given notification.\"\"\"",
            "    try:",
            "        index = index_for(entity_type)",
            "        if operation == domain_object.DomainObjectOperation.new:",
            "            index.insert_dict(entity)",
            "        elif operation == domain_object.DomainObjectOperation.changed:",
            "            index.update_dict(entity)",
            "        elif operation == domain_object.DomainObjectOperation.deleted:",
            "            index.remove_dict(entity)",
            "        else:",
            "            log.warn(\"Unknown operation: %s\" % operation)",
            "    except Exception as ex:",
            "        log.exception(ex)",
            "        # we really need to know about any exceptions, so reraise",
            "        # (see #1172)",
            "        raise",
            "",
            "",
            "class SynchronousSearchPlugin(p.SingletonPlugin):",
            "    \"\"\"Update the search index automatically.\"\"\"",
            "    p.implements(p.IDomainObjectModification, inherit=True)",
            "",
            "    def notify(self, entity: Any, operation: str) -> None:",
            "        if not isinstance(entity, model.Package):",
            "            return",
            "        if operation != domain_object.DomainObjectOperation.deleted:",
            "            dispatch_by_operation(",
            "                entity.__class__.__name__,",
            "                logic.get_action('package_show')(cast(",
            "                    Context, {",
            "                        'model': model,",
            "                        'ignore_auth': True,",
            "                        'validate': False,",
            "                        'use_cache': False",
            "                    }), {'id': entity.id}), operation)",
            "        elif operation == domain_object.DomainObjectOperation.deleted:",
            "            dispatch_by_operation(entity.__class__.__name__,",
            "                                  {'id': entity.id}, operation)",
            "        else:",
            "            log.warn(\"Discarded Sync. indexing for: %s\" % entity)",
            "",
            "",
            "def rebuild(package_id: Optional[str] = None,",
            "            only_missing: bool = False,",
            "            force: bool = False,",
            "            defer_commit: bool = False,",
            "            package_ids: Optional[Collection[str]] = None,",
            "            quiet: bool = False,",
            "            clear: bool = False):",
            "    '''",
            "        Rebuilds the search index.",
            "",
            "        If a dataset id is provided, only this dataset will be reindexed.",
            "        When reindexing all datasets, if only_missing is True, only the",
            "        datasets not already indexed will be processed. If force equals",
            "        True, if an exception is found, the exception will be logged, but",
            "        the process will carry on.",
            "    '''",
            "    log.info(\"Rebuilding search index...\")",
            "",
            "    package_index = index_for(model.Package)",
            "    context = cast(Context, {",
            "        'model': model,",
            "        'ignore_auth': True,",
            "        'validate': False,",
            "        'use_cache': False",
            "    })",
            "",
            "    if package_id:",
            "        pkg_dict = logic.get_action('package_show')(context, {",
            "            'id': package_id",
            "        })",
            "        log.info('Indexing just package %r...', pkg_dict['name'])",
            "        package_index.remove_dict(pkg_dict)",
            "        package_index.insert_dict(pkg_dict)",
            "    elif package_ids is not None:",
            "        for package_id in package_ids:",
            "            pkg_dict = logic.get_action('package_show')(context,",
            "                {'id': package_id})",
            "            log.info('Indexing just package %r...', pkg_dict['name'])",
            "            package_index.update_dict(pkg_dict, True)",
            "    else:",
            "        packages = model.Session.query(model.Package.id)",
            "        if config.get('ckan.search.remove_deleted_packages'):",
            "            packages = packages.filter(model.Package.state != 'deleted')",
            "",
            "        package_ids = [r[0] for r in packages.all()]",
            "",
            "        if only_missing:",
            "            log.info('Indexing only missing packages...')",
            "            package_query = query_for(model.Package)",
            "            indexed_pkg_ids = set(package_query.get_all_entity_ids(",
            "                max_results=len(package_ids)))",
            "            # Packages not indexed",
            "            package_ids = set(package_ids) - indexed_pkg_ids",
            "",
            "            if len(package_ids) == 0:",
            "                log.info('All datasets are already indexed')",
            "                return",
            "        else:",
            "            log.info('Rebuilding the whole index...')",
            "            # When refreshing, the index is not previously cleared",
            "            if clear:",
            "                package_index.clear()",
            "",
            "        total_packages = len(package_ids)",
            "        for counter, pkg_id in enumerate(package_ids):",
            "            if not quiet:",
            "                sys.stdout.write(",
            "                    \"\\rIndexing dataset {0}/{1}\".format(",
            "                        counter +1, total_packages)",
            "                )",
            "                sys.stdout.flush()",
            "            try:",
            "                package_index.update_dict(",
            "                    logic.get_action('package_show')(context,",
            "                        {'id': pkg_id}",
            "                    ),",
            "                    defer_commit",
            "                )",
            "            except Exception as e:",
            "                log.error(u'Error while indexing dataset %s: %s' %",
            "                          (pkg_id, repr(e)))",
            "                if force:",
            "                    log.error(text_traceback())",
            "                    continue",
            "                else:",
            "                    raise",
            "",
            "    model.Session.commit()",
            "    log.info('Finished rebuilding search index.')",
            "",
            "",
            "def commit() -> None:",
            "    package_index = index_for(model.Package)",
            "    package_index.commit()",
            "    log.info('Committed pending changes on the search index')",
            "",
            "",
            "def check() -> None:",
            "    package_query = query_for(model.Package)",
            "",
            "    log.debug(\"Checking packages search index...\")",
            "    pkgs_q = model.Session.query(model.Package).filter_by(",
            "        state=model.State.ACTIVE)",
            "    pkgs = {pkg.id for pkg in pkgs_q}",
            "    indexed_pkgs = set(package_query.get_all_entity_ids(max_results=len(pkgs)))",
            "    pkgs_not_indexed = pkgs - indexed_pkgs",
            "    print('Packages not indexed = %i out of %i' % (len(pkgs_not_indexed),",
            "                                                   len(pkgs)))",
            "    for pkg_id in pkgs_not_indexed:",
            "        pkg = model.Session.query(model.Package).get(pkg_id)",
            "        assert pkg",
            "        print((pkg.metadata_modified.strftime('%Y-%m-%d'), pkg.name))",
            "",
            "",
            "def show(package_reference: str) -> dict[str, Any]:",
            "    package_query = query_for(model.Package)",
            "    return package_query.get_index(package_reference)",
            "",
            "",
            "def clear(package_reference: str) -> None:",
            "    package_index = index_for(model.Package)",
            "    log.debug(\"Clearing search index for dataset %s...\" %",
            "              package_reference)",
            "    package_index.delete_package({'id': package_reference})",
            "",
            "",
            "def clear_all() -> None:",
            "    package_index = index_for(model.Package)",
            "    log.debug(\"Clearing search index...\")",
            "    package_index.clear()",
            "",
            "def _get_schema_from_solr(file_offset: str):",
            "",
            "    timeout = config.get('ckan.requests.timeout')",
            "",
            "    solr_url, solr_user, solr_password = SolrSettings.get()",
            "",
            "    url = solr_url.strip('/') + file_offset",
            "",
            "    timeout = config.get('ckan.requests.timeout')",
            "    if solr_user is not None and solr_password is not None:",
            "        response = requests.get(",
            "            url,",
            "            timeout=timeout,",
            "            auth=HTTPBasicAuth(solr_user, solr_password))",
            "    else:",
            "        response = requests.get(url, timeout=timeout)",
            "",
            "    return response",
            "",
            "def check_solr_schema_version(schema_file: Optional[str]=None) -> bool:",
            "    '''",
            "        Checks if the schema version of the SOLR server is compatible",
            "        with this CKAN version.",
            "",
            "        The schema will be retrieved from the SOLR server, using the",
            "        offset defined in SOLR_SCHEMA_FILE_OFFSET_MANAGED",
            "        ('/schema?wt=schema.xml'). If SOLR is set to use the manually",
            "        edited `schema.xml`, the schema will be retrieved from the SOLR",
            "        server using the offset defined in",
            "        SOLR_SCHEMA_FILE_OFFSET_CLASSIC ('/admin/file/?file=schema.xml').",
            "",
            "        The schema_file parameter allows to override this pointing to",
            "        different schema file, but it should only be used for testing",
            "        purposes.",
            "",
            "        If the CKAN instance is configured to not use SOLR or the SOLR",
            "        server is not available, the function will return False, as the",
            "        version check does not apply. If the SOLR server is available,",
            "        a SearchError exception will be thrown if the version could not",
            "        be extracted or it is not included in the supported versions list.",
            "",
            "        :schema_file: Absolute path to an alternative schema file. Should",
            "                      be only used for testing purposes (Default is None)",
            "    '''",
            "",
            "    if not is_available():",
            "        # Something is wrong with the SOLR server",
            "        log.warn('Problems were found while connecting to the SOLR server')",
            "        return False",
            "",
            "    # Try to get the schema XML file to extract the version",
            "    if not schema_file:",
            "        try:",
            "            # Try Managed Schema",
            "            res = _get_schema_from_solr(SOLR_SCHEMA_FILE_OFFSET_MANAGED)",
            "            res.raise_for_status()",
            "        except requests.HTTPError:",
            "            # Fallback to Manually Edited schema.xml",
            "            res = _get_schema_from_solr(SOLR_SCHEMA_FILE_OFFSET_CLASSIC)",
            "        schema_content = res.text",
            "    else:",
            "        with open(schema_file, 'rb') as f:",
            "            schema_content = f.read()",
            "",
            "    tree = xml.dom.minidom.parseString(schema_content)",
            "",
            "    # Up to CKAN 2.9 the schema version was stored in the `version` attribute.",
            "    # Going forward, we are storing it in the `name` one in the form `ckan-X.Y`",
            "    version = ''",
            "    name_attr = tree.documentElement.getAttribute('name')",
            "    if name_attr.startswith('ckan-'):",
            "        version = name_attr.split('-')[1]",
            "    else:",
            "        version = tree.documentElement.getAttribute('version')",
            "",
            "    if not len(version):",
            "        msg = 'Could not extract version info from the SOLR schema'",
            "        if schema_file:",
            "            msg += ', using file {}'.format(schema_file)",
            "        raise SearchError(msg)",
            "",
            "    if not version in SUPPORTED_SCHEMA_VERSIONS:",
            "        raise SearchError('SOLR schema version not supported: %s. Supported'",
            "                          ' versions are [%s]'",
            "                          % (version, ', '.join(SUPPORTED_SCHEMA_VERSIONS)))",
            "    return True"
        ],
        "afterPatchFile": [
            "# encoding: utf-8",
            "",
            "from __future__ import annotations",
            "",
            "import logging",
            "import sys",
            "import cgitb",
            "import warnings",
            "import traceback",
            "",
            "import xml.dom.minidom",
            "from typing import Collection, Any, Optional, Type, cast, overload",
            "",
            "import requests",
            "from requests.auth import HTTPBasicAuth",
            "",
            "import ckan.model as model",
            "import ckan.model.domain_object as domain_object",
            "import ckan.plugins as p",
            "import ckan.logic as logic",
            "from ckan.types import Context",
            "",
            "from ckan.lib.search.common import (",
            "    make_connection, SearchIndexError, SearchQueryError,  # type: ignore",
            "    SolrConnectionError, # type: ignore",
            "    SearchError, is_available, SolrSettings, config",
            ")",
            "from ckan.lib.search.index import (",
            "    SearchIndex, PackageSearchIndex, NoopSearchIndex",
            ")",
            "from ckan.lib.search.query import (",
            "    SearchQuery,",
            "    TagSearchQuery, ResourceSearchQuery, PackageSearchQuery,",
            "    QueryOptions, convert_legacy_parameters_to_solr  # type: ignore",
            ")",
            "from ckan.lib.search.index import SearchIndex",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "def text_traceback() -> str:",
            "    info = sys.exc_info()",
            "    with warnings.catch_warnings():",
            "        warnings.simplefilter(\"ignore\")",
            "        try:",
            "            text = cgitb.text(info)",
            "        except RuntimeError:",
            "            # there is werkzeug.local.LocalProxy object inside traceback, that",
            "            # cannot be printed out by the cgitb",
            "            res = \"\".join(traceback.format_tb(info[-1]))",
            "        else:",
            "            res = 'the original traceback:'.join(",
            "                text.split('the original traceback:')[1:]",
            "            ).strip()",
            "",
            "    return res",
            "",
            "",
            "SUPPORTED_SCHEMA_VERSIONS = ['2.8', '2.9', '2.10']",
            "",
            "DEFAULT_OPTIONS = {",
            "    'limit': 20,",
            "    'offset': 0,",
            "    # about presenting the results",
            "    'order_by': 'rank',",
            "    'return_objects': False,",
            "    'ref_entity_with_attr': 'name',",
            "    'all_fields': False,",
            "    'search_tags': True,",
            "    'callback': None,  # simply passed through",
            "}",
            "",
            "_INDICES: dict[str, Type[SearchIndex]] = {",
            "    'package': PackageSearchIndex",
            "}",
            "",
            "_QUERIES: dict[str, Type[SearchQuery]] = {",
            "    'tag': TagSearchQuery,",
            "    'resource': ResourceSearchQuery,",
            "    'package': PackageSearchQuery",
            "}",
            "",
            "SOLR_SCHEMA_FILE_OFFSET_MANAGED = '/schema?wt=schema.xml'",
            "SOLR_SCHEMA_FILE_OFFSET_CLASSIC = '/admin/file/?file=schema.xml'",
            "",
            "",
            "def _normalize_type(_type: Any) -> str:",
            "    if isinstance(_type, domain_object.DomainObject):",
            "        _type = _type.__class__",
            "    if isinstance(_type, type):",
            "        _type = _type.__name__",
            "    return _type.strip().lower()",
            "",
            "",
            "@overload",
            "def index_for(_type: Type[model.Package]) -> PackageSearchIndex: ...",
            "",
            "@overload",
            "def index_for(_type: Any) -> SearchIndex: ...",
            "",
            "def index_for(_type: Any) -> SearchIndex:",
            "    \"\"\" Get a SearchIndex instance sub-class suitable for",
            "        the specified type. \"\"\"",
            "    try:",
            "        _type_n = _normalize_type(_type)",
            "        return _INDICES[_type_n]()",
            "    except KeyError:",
            "        log.warn(\"Unknown search type: %s\" % _type)",
            "        return NoopSearchIndex()",
            "",
            "",
            "@overload",
            "def query_for(_type: Type[model.Package]) -> PackageSearchQuery:",
            "    ...",
            "",
            "",
            "@overload",
            "def query_for(_type: Type[model.Resource]) -> ResourceSearchQuery:",
            "    ...",
            "",
            "",
            "@overload",
            "def query_for(_type: Type[model.Tag]) -> TagSearchQuery:",
            "    ...",
            "",
            "",
            "def query_for(_type: Any) -> SearchQuery:",
            "    \"\"\" Get a SearchQuery instance sub-class suitable for the specified",
            "        type. \"\"\"",
            "    try:",
            "        _type_n = _normalize_type(_type)",
            "        return _QUERIES[_type_n]()",
            "    except KeyError:",
            "        raise SearchError(\"Unknown search type: %s\" % _type)",
            "",
            "",
            "def dispatch_by_operation(entity_type: str, entity: dict[str, Any],",
            "                          operation: str) -> None:",
            "    \"\"\"Call the appropriate index method for a given notification.\"\"\"",
            "    try:",
            "        index = index_for(entity_type)",
            "        if operation == domain_object.DomainObjectOperation.new:",
            "            index.insert_dict(entity)",
            "        elif operation == domain_object.DomainObjectOperation.changed:",
            "            index.update_dict(entity)",
            "        elif operation == domain_object.DomainObjectOperation.deleted:",
            "            index.remove_dict(entity)",
            "        else:",
            "            log.warn(\"Unknown operation: %s\" % operation)",
            "    except Exception as ex:",
            "        log.exception(ex)",
            "        # we really need to know about any exceptions, so reraise",
            "        # (see #1172)",
            "        raise",
            "",
            "",
            "class SynchronousSearchPlugin(p.SingletonPlugin):",
            "    \"\"\"Update the search index automatically.\"\"\"",
            "    p.implements(p.IDomainObjectModification, inherit=True)",
            "",
            "    def notify(self, entity: Any, operation: str) -> None:",
            "        if not isinstance(entity, model.Package):",
            "            return",
            "        if operation != domain_object.DomainObjectOperation.deleted:",
            "            dispatch_by_operation(",
            "                entity.__class__.__name__,",
            "                logic.get_action('package_show')(cast(",
            "                    Context, {",
            "                        'model': model,",
            "                        'ignore_auth': True,",
            "                        'validate': False,",
            "                        'use_cache': False",
            "                    }), {'id': entity.id}), operation)",
            "        elif operation == domain_object.DomainObjectOperation.deleted:",
            "            dispatch_by_operation(entity.__class__.__name__,",
            "                                  {'id': entity.id}, operation)",
            "        else:",
            "            log.warn(\"Discarded Sync. indexing for: %s\" % entity)",
            "",
            "",
            "def rebuild(package_id: Optional[str] = None,",
            "            only_missing: bool = False,",
            "            force: bool = False,",
            "            defer_commit: bool = False,",
            "            package_ids: Optional[Collection[str]] = None,",
            "            quiet: bool = False,",
            "            clear: bool = False):",
            "    '''",
            "        Rebuilds the search index.",
            "",
            "        If a dataset id is provided, only this dataset will be reindexed.",
            "        When reindexing all datasets, if only_missing is True, only the",
            "        datasets not already indexed will be processed. If force equals",
            "        True, if an exception is found, the exception will be logged, but",
            "        the process will carry on.",
            "    '''",
            "    log.info(\"Rebuilding search index...\")",
            "",
            "    package_index = index_for(model.Package)",
            "    context = cast(Context, {",
            "        'model': model,",
            "        'ignore_auth': True,",
            "        'validate': False,",
            "        'use_cache': False",
            "    })",
            "",
            "    if package_id:",
            "        pkg_dict = logic.get_action('package_show')(context, {",
            "            'id': package_id",
            "        })",
            "        log.info('Indexing just package %r...', pkg_dict['name'])",
            "        package_index.remove_dict(pkg_dict)",
            "        package_index.insert_dict(pkg_dict)",
            "    elif package_ids is not None:",
            "        for package_id in package_ids:",
            "            pkg_dict = logic.get_action('package_show')(context,",
            "                {'id': package_id})",
            "            log.info('Indexing just package %r...', pkg_dict['name'])",
            "            package_index.update_dict(pkg_dict, True)",
            "    else:",
            "        packages = model.Session.query(model.Package.id)",
            "        if config.get('ckan.search.remove_deleted_packages'):",
            "            packages = packages.filter(model.Package.state != 'deleted')",
            "",
            "        package_ids = [r[0] for r in packages.all()]",
            "",
            "        if only_missing:",
            "            log.info('Indexing only missing packages...')",
            "            package_query = query_for(model.Package)",
            "            indexed_pkg_ids = set(package_query.get_all_entity_ids(",
            "                max_results=len(package_ids)))",
            "            # Packages not indexed",
            "            package_ids = set(package_ids) - indexed_pkg_ids",
            "",
            "            if len(package_ids) == 0:",
            "                log.info('All datasets are already indexed')",
            "                return",
            "        else:",
            "            log.info('Rebuilding the whole index...')",
            "            # When refreshing, the index is not previously cleared",
            "            if clear:",
            "                package_index.clear()",
            "",
            "        total_packages = len(package_ids)",
            "        for counter, pkg_id in enumerate(package_ids):",
            "            if not quiet:",
            "                sys.stdout.write(",
            "                    \"\\rIndexing dataset {0}/{1}\".format(",
            "                        counter +1, total_packages)",
            "                )",
            "                sys.stdout.flush()",
            "            try:",
            "                package_index.update_dict(",
            "                    logic.get_action('package_show')(context,",
            "                        {'id': pkg_id}",
            "                    ),",
            "                    defer_commit",
            "                )",
            "            except Exception as e:",
            "                log.error(u'Error while indexing dataset %s: %s' %",
            "                          (pkg_id, repr(e)))",
            "                if force:",
            "                    log.error(text_traceback())",
            "                    continue",
            "                else:",
            "                    raise",
            "",
            "    model.Session.commit()",
            "    log.info('Finished rebuilding search index.')",
            "",
            "",
            "def commit() -> None:",
            "    package_index = index_for(model.Package)",
            "    package_index.commit()",
            "    log.info('Committed pending changes on the search index')",
            "",
            "",
            "def check() -> None:",
            "    package_query = query_for(model.Package)",
            "",
            "    log.debug(\"Checking packages search index...\")",
            "    pkgs_q = model.Session.query(model.Package).filter_by(",
            "        state=model.State.ACTIVE)",
            "    pkgs = {pkg.id for pkg in pkgs_q}",
            "    indexed_pkgs = set(package_query.get_all_entity_ids(max_results=len(pkgs)))",
            "    pkgs_not_indexed = pkgs - indexed_pkgs",
            "    print('Packages not indexed = %i out of %i' % (len(pkgs_not_indexed),",
            "                                                   len(pkgs)))",
            "    for pkg_id in pkgs_not_indexed:",
            "        pkg = model.Session.query(model.Package).get(pkg_id)",
            "        assert pkg",
            "        print((pkg.metadata_modified.strftime('%Y-%m-%d'), pkg.name))",
            "",
            "",
            "def show(package_reference: str) -> dict[str, Any]:",
            "    package_query = query_for(model.Package)",
            "    return package_query.get_index(package_reference)",
            "",
            "",
            "def clear(package_reference: str) -> None:",
            "    package_index = index_for(model.Package)",
            "    log.debug(\"Clearing search index for dataset %s...\" %",
            "              package_reference)",
            "    package_index.delete_package({'id': package_reference})",
            "",
            "",
            "def clear_all() -> None:",
            "    package_index = index_for(model.Package)",
            "    log.debug(\"Clearing search index...\")",
            "    package_index.clear()",
            "",
            "def _get_schema_from_solr(file_offset: str):",
            "",
            "    timeout = config.get('ckan.requests.timeout')",
            "",
            "    solr_url, solr_user, solr_password = SolrSettings.get()",
            "",
            "    url = solr_url.strip('/') + file_offset",
            "",
            "    timeout = config.get('ckan.requests.timeout')",
            "    if solr_user is not None and solr_password is not None:",
            "        response = requests.get(",
            "            url,",
            "            timeout=timeout,",
            "            auth=HTTPBasicAuth(solr_user, solr_password))",
            "    else:",
            "        response = requests.get(url, timeout=timeout)",
            "",
            "    return response",
            "",
            "def check_solr_schema_version(schema_file: Optional[str]=None) -> bool:",
            "    '''",
            "        Checks if the schema version of the SOLR server is compatible",
            "        with this CKAN version.",
            "",
            "        The schema will be retrieved from the SOLR server, using the",
            "        offset defined in SOLR_SCHEMA_FILE_OFFSET_MANAGED",
            "        ('/schema?wt=schema.xml'). If SOLR is set to use the manually",
            "        edited `schema.xml`, the schema will be retrieved from the SOLR",
            "        server using the offset defined in",
            "        SOLR_SCHEMA_FILE_OFFSET_CLASSIC ('/admin/file/?file=schema.xml').",
            "",
            "        The schema_file parameter allows to override this pointing to",
            "        different schema file, but it should only be used for testing",
            "        purposes.",
            "",
            "        If the CKAN instance is configured to not use SOLR or the SOLR",
            "        server is not available, the function will return False, as the",
            "        version check does not apply. If the SOLR server is available,",
            "        a SearchError exception will be thrown if the version could not",
            "        be extracted or it is not included in the supported versions list.",
            "",
            "        :schema_file: Absolute path to an alternative schema file. Should",
            "                      be only used for testing purposes (Default is None)",
            "    '''",
            "",
            "    if not is_available():",
            "        # Something is wrong with the SOLR server",
            "        log.warn('Problems were found while connecting to the SOLR server')",
            "        return False",
            "",
            "    # Try to get the schema XML file to extract the version",
            "    if not schema_file:",
            "        try:",
            "            # Try Managed Schema",
            "            res = _get_schema_from_solr(SOLR_SCHEMA_FILE_OFFSET_MANAGED)",
            "            res.raise_for_status()",
            "        except requests.HTTPError:",
            "            # Fallback to Manually Edited schema.xml",
            "            res = _get_schema_from_solr(SOLR_SCHEMA_FILE_OFFSET_CLASSIC)",
            "        schema_content = res.text",
            "    else:",
            "        with open(schema_file, 'rb') as f:",
            "            schema_content = f.read()",
            "",
            "    tree = xml.dom.minidom.parseString(schema_content)",
            "",
            "    # Up to CKAN 2.9 the schema version was stored in the `version` attribute.",
            "    # Going forward, we are storing it in the `name` one in the form `ckan-X.Y`",
            "    version = ''",
            "    name_attr = tree.documentElement.getAttribute('name')",
            "    if name_attr.startswith('ckan-'):",
            "        version = name_attr.split('-')[1]",
            "    else:",
            "        version = tree.documentElement.getAttribute('version')",
            "",
            "    if not len(version):",
            "        msg = 'Could not extract version info from the SOLR schema'",
            "        if schema_file:",
            "            msg += ', using file {}'.format(schema_file)",
            "        raise SearchError(msg)",
            "",
            "    if not version in SUPPORTED_SCHEMA_VERSIONS:",
            "        raise SearchError('SOLR schema version not supported: %s. Supported'",
            "                          ' versions are [%s]'",
            "                          % (version, ', '.join(SUPPORTED_SCHEMA_VERSIONS)))",
            "    return True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "ckan/lib/search/common.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": "     pass"
            },
            "1": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+class SolrConnectionError(Exception):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+    pass"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+"
            },
            "7": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " DEFAULT_SOLR_URL = 'http://127.0.0.1:8983/solr/ckan'"
            },
            "8": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import datetime",
            "import logging",
            "import re",
            "from typing import Any, Optional",
            "",
            "import pysolr",
            "import simplejson",
            "",
            "from six.moves.urllib.parse import quote_plus  # type: ignore",
            "from pysolr import Solr",
            "",
            "from ckan.common import config",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "class SearchIndexError(Exception):",
            "    pass",
            "",
            "",
            "class SearchError(Exception):",
            "    pass",
            "",
            "",
            "class SearchQueryError(SearchError):",
            "    pass",
            "",
            "",
            "DEFAULT_SOLR_URL = 'http://127.0.0.1:8983/solr/ckan'",
            "",
            "",
            "class SolrSettings(object):",
            "    _is_initialised: bool = False",
            "    _url: Optional[str] = None",
            "    _user: Optional[str] = None",
            "    _password: Optional[str] = None",
            "",
            "    @classmethod",
            "    def init(cls,",
            "             url: Optional[str],",
            "             user: Optional[str] = None,",
            "             password: Optional[str] = None) -> None:",
            "        if url is not None:",
            "            cls._url = url",
            "            cls._user = user",
            "            cls._password = password",
            "        else:",
            "            cls._url = DEFAULT_SOLR_URL",
            "        cls._is_initialised = True",
            "",
            "    @classmethod",
            "    def get(cls) -> tuple[str, Optional[str], Optional[str]]:",
            "        if not cls._is_initialised:",
            "            raise SearchIndexError('SOLR URL not initialised')",
            "        if not cls._url:",
            "            raise SearchIndexError('SOLR URL is blank')",
            "        return (cls._url, cls._user, cls._password)",
            "",
            "",
            "def is_available() -> bool:",
            "    \"\"\"",
            "    Return true if we can successfully connect to Solr.",
            "    \"\"\"",
            "    try:",
            "        conn = make_connection()",
            "        conn.search(q=\"*:*\", rows=1)",
            "    except Exception as e:",
            "        log.exception(e)",
            "        return False",
            "    return True",
            "",
            "",
            "def make_connection(decode_dates: bool = True) -> Solr:",
            "    solr_url, solr_user, solr_password = SolrSettings.get()",
            "",
            "    if solr_url and solr_user and solr_password:",
            "        # Rebuild the URL with the username/password",
            "        match = re.search('http(?:s)?://', solr_url)",
            "        assert match",
            "        protocol = match.group()",
            "        solr_url = re.sub(protocol, '', solr_url)",
            "        solr_url = \"{}{}:{}@{}\".format(protocol,",
            "                                       quote_plus(solr_user),",
            "                                       quote_plus(solr_password),",
            "                                       solr_url)",
            "",
            "    timeout = config.get('solr_timeout')",
            "",
            "    if decode_dates:",
            "        decoder = simplejson.JSONDecoder(object_hook=solr_datetime_decoder)",
            "        return pysolr.Solr(solr_url, decoder=decoder, timeout=timeout)",
            "    else:",
            "        return pysolr.Solr(solr_url, timeout=timeout)",
            "",
            "",
            "def solr_datetime_decoder(d: dict[str, Any]) -> dict[str, Any]:",
            "    for k, v in d.items():",
            "        if isinstance(v, str):",
            "            possible_datetime = re.search(pysolr.DATETIME_REGEX, v)",
            "            if possible_datetime:",
            "                date_values: dict[str, Any] = possible_datetime.groupdict()",
            "                for dk, dv in date_values.items():",
            "                    date_values[dk] = int(dv)",
            "",
            "                d[k] = datetime.datetime(date_values['year'],",
            "                                         date_values['month'],",
            "                                         date_values['day'],",
            "                                         date_values['hour'],",
            "                                         date_values['minute'],",
            "                                         date_values['second'])",
            "    return d"
        ],
        "afterPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import datetime",
            "import logging",
            "import re",
            "from typing import Any, Optional",
            "",
            "import pysolr",
            "import simplejson",
            "",
            "from six.moves.urllib.parse import quote_plus  # type: ignore",
            "from pysolr import Solr",
            "",
            "from ckan.common import config",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "",
            "class SearchIndexError(Exception):",
            "    pass",
            "",
            "",
            "class SearchError(Exception):",
            "    pass",
            "",
            "",
            "class SearchQueryError(SearchError):",
            "    pass",
            "",
            "",
            "class SolrConnectionError(Exception):",
            "    pass",
            "",
            "",
            "DEFAULT_SOLR_URL = 'http://127.0.0.1:8983/solr/ckan'",
            "",
            "",
            "class SolrSettings(object):",
            "    _is_initialised: bool = False",
            "    _url: Optional[str] = None",
            "    _user: Optional[str] = None",
            "    _password: Optional[str] = None",
            "",
            "    @classmethod",
            "    def init(cls,",
            "             url: Optional[str],",
            "             user: Optional[str] = None,",
            "             password: Optional[str] = None) -> None:",
            "        if url is not None:",
            "            cls._url = url",
            "            cls._user = user",
            "            cls._password = password",
            "        else:",
            "            cls._url = DEFAULT_SOLR_URL",
            "        cls._is_initialised = True",
            "",
            "    @classmethod",
            "    def get(cls) -> tuple[str, Optional[str], Optional[str]]:",
            "        if not cls._is_initialised:",
            "            raise SearchIndexError('SOLR URL not initialised')",
            "        if not cls._url:",
            "            raise SearchIndexError('SOLR URL is blank')",
            "        return (cls._url, cls._user, cls._password)",
            "",
            "",
            "def is_available() -> bool:",
            "    \"\"\"",
            "    Return true if we can successfully connect to Solr.",
            "    \"\"\"",
            "    try:",
            "        conn = make_connection()",
            "        conn.search(q=\"*:*\", rows=1)",
            "    except Exception as e:",
            "        log.exception(e)",
            "        return False",
            "    return True",
            "",
            "",
            "def make_connection(decode_dates: bool = True) -> Solr:",
            "    solr_url, solr_user, solr_password = SolrSettings.get()",
            "",
            "    if solr_url and solr_user and solr_password:",
            "        # Rebuild the URL with the username/password",
            "        match = re.search('http(?:s)?://', solr_url)",
            "        assert match",
            "        protocol = match.group()",
            "        solr_url = re.sub(protocol, '', solr_url)",
            "        solr_url = \"{}{}:{}@{}\".format(protocol,",
            "                                       quote_plus(solr_user),",
            "                                       quote_plus(solr_password),",
            "                                       solr_url)",
            "",
            "    timeout = config.get('solr_timeout')",
            "",
            "    if decode_dates:",
            "        decoder = simplejson.JSONDecoder(object_hook=solr_datetime_decoder)",
            "        return pysolr.Solr(solr_url, decoder=decoder, timeout=timeout)",
            "    else:",
            "        return pysolr.Solr(solr_url, timeout=timeout)",
            "",
            "",
            "def solr_datetime_decoder(d: dict[str, Any]) -> dict[str, Any]:",
            "    for k, v in d.items():",
            "        if isinstance(v, str):",
            "            possible_datetime = re.search(pysolr.DATETIME_REGEX, v)",
            "            if possible_datetime:",
            "                date_values: dict[str, Any] = possible_datetime.groupdict()",
            "                for dk, dv in date_values.items():",
            "                    date_values[dk] = int(dv)",
            "",
            "                d[k] = datetime.datetime(date_values['year'],",
            "                                         date_values['month'],",
            "                                         date_values['day'],",
            "                                         date_values['hour'],",
            "                                         date_values['minute'],",
            "                                         date_values['second'])",
            "    return d"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "ckan/lib/search/query.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from ckan.common import config"
            },
            "2": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from ckan.lib.search.common import ("
            },
            "3": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    make_connection, SearchError, SearchQueryError"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+    make_connection, SearchError, SearchQueryError, SolrConnectionError"
            },
            "5": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " )"
            },
            "6": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from ckan.types import Context"
            },
            "7": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 473,
                "afterPatchRowNumber": 473,
                "PatchRowcode": "                         \"Can't determine Sort Order\" in e.args[0] or \\"
            },
            "9": {
                "beforePatchRowNumber": 474,
                "afterPatchRowNumber": 474,
                "PatchRowcode": "                         'Unknown sort order' in e.args[0]:"
            },
            "10": {
                "beforePatchRowNumber": 475,
                "afterPatchRowNumber": 475,
                "PatchRowcode": "                     raise SearchQueryError('Invalid \"sort\" parameter')"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 476,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 477,
                "PatchRowcode": "+                if (\"Failed to connect to server\" in e.args[0] or "
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 478,
                "PatchRowcode": "+                        \"Connection to server\" in e.args[0]):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 479,
                "PatchRowcode": "+                    log.warning(\"Connection Error: Failed to connect to Solr server.\")"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 480,
                "PatchRowcode": "+                    raise SolrConnectionError(\"Solr returned an error while searching.\")"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 481,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 482,
                "PatchRowcode": "             raise SearchError('SOLR returned an error running query: %r Error: %r' %"
            },
            "18": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": 483,
                "PatchRowcode": "                               (query, e))"
            },
            "19": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": 484,
                "PatchRowcode": "         self.count = solr_response.hits"
            }
        },
        "frontPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import re",
            "import logging",
            "from typing import Any, NoReturn, Optional, Union, cast, Dict",
            "from pyparsing import (",
            "    Word, QuotedString, Suppress, OneOrMore, Group, alphanums",
            ")",
            "from pyparsing.exceptions import ParseException",
            "import pysolr",
            "",
            "from ckan.common import asbool",
            "from werkzeug.datastructures import MultiDict",
            "",
            "import ckan.logic as logic",
            "import ckan.model as model",
            "",
            "from ckan.common import config",
            "from ckan.lib.search.common import (",
            "    make_connection, SearchError, SearchQueryError",
            ")",
            "from ckan.types import Context",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "_open_licenses: Optional[list[str]] = None",
            "",
            "VALID_SOLR_PARAMETERS = set([",
            "    'q', 'fl', 'fq', 'rows', 'sort', 'start', 'wt', 'qf', 'bf', 'boost',",
            "    'facet', 'facet.mincount', 'facet.limit', 'facet.field',",
            "    'extras', 'fq_list', 'tie', 'defType', 'mm', 'df'",
            "])",
            "",
            "# for (solr) package searches, this specifies the fields that are searched",
            "# and their relative weighting",
            "QUERY_FIELDS = \"name^4 title^4 tags^2 groups^2 text\"",
            "",
            "solr_regex = re.compile(r'([\\\\+\\-&|!(){}\\[\\]^\"~*?:])')",
            "",
            "def escape_legacy_argument(val: str) -> str:",
            "    # escape special chars \\+-&|!(){}[]^\"~*?:",
            "    return solr_regex.sub(r'\\\\\\1', val)",
            "",
            "",
            "def convert_legacy_parameters_to_solr(",
            "        legacy_params: dict[str, Any]) -> dict[str, Any]:",
            "    '''API v1 and v2 allowed search params that the SOLR syntax does not",
            "    support, so use this function to convert those to SOLR syntax.",
            "    See tests for examples.",
            "",
            "    raises SearchQueryError on invalid params.",
            "    '''",
            "    options = QueryOptions(**legacy_params)",
            "    options.validate()",
            "    solr_params = legacy_params.copy()",
            "    solr_q_list: list[str] = []",
            "    if solr_params.get('q'):",
            "        solr_q_list.append(solr_params['q'].replace('+', ' '))",
            "    non_solr_params = set(legacy_params.keys()) - VALID_SOLR_PARAMETERS",
            "    for search_key in non_solr_params:",
            "        value_obj = legacy_params[search_key]",
            "        value = value_obj.replace('+', ' ') if isinstance(value_obj, str) else value_obj",
            "        if search_key == 'all_fields':",
            "            if value:",
            "                solr_params['fl'] = '*'",
            "        elif search_key == 'offset':",
            "            solr_params['start'] = value",
            "        elif search_key == 'limit':",
            "            solr_params['rows'] = value",
            "        elif search_key == 'order_by':",
            "            solr_params['sort'] = '%s asc' % value",
            "        elif search_key == 'tags':",
            "            if isinstance(value_obj, list):",
            "                tag_list = value_obj",
            "            elif isinstance(value_obj, str):",
            "                tag_list = [value_obj]",
            "            else:",
            "                raise SearchQueryError('Was expecting either a string or JSON list for the tags parameter: %r' % value)",
            "            solr_q_list.extend(['tags:\"%s\"' % escape_legacy_argument(tag) for tag in tag_list])",
            "        else:",
            "            if len(value.strip()):",
            "                value = escape_legacy_argument(value)",
            "                if ' ' in value:",
            "                    value = '\"%s\"' % value",
            "                solr_q_list.append('%s:%s' % (search_key, value))",
            "        del solr_params[search_key]",
            "    solr_params['q'] = ' '.join(solr_q_list)",
            "    if non_solr_params:",
            "        log.debug('Converted legacy search params from %r to %r',",
            "                 legacy_params, solr_params)",
            "    return solr_params",
            "",
            "",
            "def _parse_local_params(local_params: str) -> list[Union[str, list[str]]]:",
            "    \"\"\"",
            "    Parse a local parameters section as return it as a list, eg:",
            "",
            "    {!dismax qf=myfield v='some value'} -> ['dismax', ['qf', 'myfield'], ['v', 'some value']]",
            "",
            "",
            "    {!type=dismax qf=myfield v='some value'} -> [['type', 'dismax'], ['qf', 'myfield'], ['v', 'some value']]",
            "",
            "    \"\"\"",
            "    key = Word(alphanums + \"_.\")",
            "    value = QuotedString('\"') | QuotedString(\"'\") | Word(alphanums + \"_$\")",
            "    pair = Group(key + Suppress(\"=\") + value)",
            "    expression = Suppress(\"{!\") + OneOrMore(pair | key) + Suppress(\"}\")",
            "",
            "    return expression.parse_string(local_params).as_list()",
            "",
            "",
            "def _get_local_query_parser(q: str) -> str:",
            "    \"\"\"",
            "    Given a Solr parameter, extract any custom query parsers used in the",
            "    local parameters, .e.g. q={!child ...}...",
            "    \"\"\"",
            "    qp_type = \"\"",
            "    q = q.strip()",
            "    if not q.startswith(\"{!\"):",
            "        return qp_type",
            "",
            "    try:",
            "        local_params = q[:q.rindex(\"}\") + 1]",
            "        parts = _parse_local_params(local_params)",
            "    except (ParseException, ValueError) as e:",
            "        raise SearchQueryError(f\"Could not parse incoming local parameters: {e}\")",
            "",
            "    if isinstance(parts[0], str):",
            "        # Most common form of defining the query parser type e.g. {!knn ...}",
            "        qp_type = parts[0]",
            "    else:",
            "        # Alternative syntax e.g. {!type=knn ...}",
            "        type_part = [p for p in parts if p[0] == \"type\"]",
            "        if type_part:",
            "            qp_type = type_part[0][1]",
            "    return qp_type",
            "",
            "",
            "class QueryOptions(Dict[str, Any]):",
            "    \"\"\"",
            "    Options specify aspects of the search query which are only tangentially related",
            "    to the query terms (such as limits, etc.).",
            "    NB This is used only by legacy package search and current resource & tag search.",
            "       Modern SOLR package search leaves this to SOLR syntax.",
            "    \"\"\"",
            "",
            "    BOOLEAN_OPTIONS = ['all_fields']",
            "    INTEGER_OPTIONS = ['offset', 'limit']",
            "    UNSUPPORTED_OPTIONS = ['filter_by_downloadable', 'filter_by_openness']",
            "",
            "    limit: int",
            "    offset: int",
            "    order_by: str",
            "    return_objects: bool",
            "    ref_entity_with_attr: str",
            "    all_fields: bool",
            "    search_tags: bool",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        from ckan.lib.search import DEFAULT_OPTIONS",
            "",
            "        # set values according to the defaults",
            "        for option_name, default_value in DEFAULT_OPTIONS.items():",
            "            if not option_name in self:",
            "                self[option_name] = default_value",
            "",
            "        super(QueryOptions, self).__init__(**kwargs)",
            "",
            "    def validate(self) -> None:",
            "        for key, value in self.items():",
            "            if key in self.BOOLEAN_OPTIONS:",
            "                try:",
            "                    value = asbool(value)",
            "                except ValueError:",
            "                    raise SearchQueryError('Value for search option %r must be True or False (1 or 0) but received %r' % (key, value))",
            "            elif key in self.INTEGER_OPTIONS:",
            "                try:",
            "                    value = int(value)",
            "                except ValueError:",
            "                    raise SearchQueryError('Value for search option %r must be an integer but received %r' % (key, value))",
            "            elif key in self.UNSUPPORTED_OPTIONS:",
            "                raise SearchQueryError('Search option %r is not supported' % key)",
            "            self[key] = value",
            "",
            "    def __getattr__(self, name: str) -> Any:",
            "        return self.get(name)",
            "",
            "    def __setattr__(self, name: str, value: Any):",
            "        self[name] = value",
            "",
            "",
            "class SearchQuery(object):",
            "    \"\"\"",
            "    A query is ... when you ask the search engine things. SearchQuery is intended",
            "    to be used for only one query, i.e. it sets state. Definitely not thread-safe.",
            "    \"\"\"",
            "    count: int",
            "    results: list[Any]",
            "    facets: dict[str, Any]",
            "",
            "    def __init__(self) -> None:",
            "        self.results = []",
            "        self.count = 0",
            "",
            "    @property",
            "    def open_licenses(self) -> list[str]:",
            "        # this isn't exactly the very best place to put these, but they stay",
            "        # there persistently.",
            "        # TODO: figure out if they change during run-time.",
            "        global _open_licenses",
            "        if not isinstance(_open_licenses, list):",
            "            _open_licenses = []",
            "            for license in model.Package.get_license_register().values():",
            "                if license and license.isopen():",
            "                    _open_licenses.append(license.id)",
            "        return _open_licenses",
            "",
            "    def get_all_entity_ids(self, max_results: int=1000) -> list[str]:",
            "        \"\"\"",
            "        Return a list of the IDs of all indexed packages.",
            "        \"\"\"",
            "        return []",
            "",
            "    def run(self,",
            "            query: Optional[Union[str, dict[str, Any]]] = None,",
            "            terms: Optional[list[str]] = None,",
            "            fields: Optional[dict[str, Any]] = None,",
            "            facet_by: Optional[list[str]] = None,",
            "            options: Optional[QueryOptions] = None,",
            "            **kwargs: Any) -> NoReturn:",
            "        raise SearchError(\"SearchQuery.run() not implemented!\")",
            "",
            "    # convenience, allows to query(..)",
            "    __call__ = run",
            "",
            "",
            "class TagSearchQuery(SearchQuery):",
            "    \"\"\"Search for tags.\"\"\"",
            "    def run(self,",
            "            query: Optional[Union[str, list[str]]] = None,",
            "            fields: Optional[dict[str, Any]] = None,",
            "            options: Optional[QueryOptions] = None,",
            "            **kwargs: Any) -> dict[str, Any]:",
            "        query = [] if query is None else query",
            "        fields = {} if fields is None else fields",
            "",
            "        if options is None:",
            "            options = QueryOptions(**kwargs)",
            "        else:",
            "            options.update(kwargs)",
            "",
            "        if isinstance(query, str):",
            "            query = [query]",
            "",
            "        query = query[:]  # don't alter caller's query list.",
            "        for field, value in fields.items():",
            "            if field in ('tag', 'tags'):",
            "                query.append(value)",
            "",
            "        context = cast(Context, {'model': model, 'session': model.Session})",
            "        data_dict = {",
            "            'query': query,",
            "            'offset': options.get('offset'),",
            "            'limit': options.get('limit')",
            "        }",
            "        results = logic.get_action('tag_search')(context, data_dict)",
            "",
            "        if not options.return_objects:",
            "            # if options.all_fields is set, return a dict",
            "            # if not, return a list of resource IDs",
            "            if options.all_fields:",
            "                results['results'] = [r.as_dict() for r in results['results']]",
            "            else:",
            "                results['results'] = [r['name'] for r in results['results']]",
            "",
            "        self.count = results['count']",
            "        self.results = results['results']",
            "        return results",
            "",
            "",
            "class ResourceSearchQuery(SearchQuery):",
            "    \"\"\"Search for resources.\"\"\"",
            "    def run(self,",
            "            fields: Optional[dict[str, Any]] = None,",
            "            options: Optional[QueryOptions] = None,",
            "            **kwargs: Any) -> dict[str, Any]:",
            "        if options is None:",
            "            options = QueryOptions(**kwargs)",
            "        else:",
            "            options.update(kwargs)",
            "",
            "        context = cast(Context,{",
            "            'model': model,",
            "            'session': model.Session,",
            "            'search_query': True,",
            "        })",
            "",
            "        # Transform fields into structure required by the resource_search",
            "        # action.",
            "        query: list[str] = []",
            "",
            "        if fields:",
            "            for field, terms in fields.items():",
            "                if isinstance(terms, str):",
            "                    terms = terms.split()",
            "                for term in terms:",
            "                    query.append(':'.join([field, term]))",
            "",
            "        data_dict = {",
            "            'query': query,",
            "            'offset': options.get('offset'),",
            "            'limit': options.get('limit'),",
            "            'order_by': options.get('order_by')",
            "        }",
            "        results = logic.get_action('resource_search')(context, data_dict)",
            "",
            "        if not options.return_objects:",
            "            # if options.all_fields is set, return a dict",
            "            # if not, return a list of resource IDs",
            "            if options.all_fields:",
            "                results['results'] = [r.as_dict() for r in results['results']]",
            "            else:",
            "                results['results'] = [r.id for r in results['results']]",
            "",
            "        self.count = results['count']",
            "        self.results = results['results']",
            "        return results",
            "",
            "",
            "class PackageSearchQuery(SearchQuery):",
            "    def get_all_entity_ids(self, max_results: int = 1000) -> list[str]:",
            "        \"\"\"",
            "        Return a list of the IDs of all indexed packages.",
            "        \"\"\"",
            "        query = \"*:*\"",
            "        fq = \"+site_id:\\\"%s\\\" \" % config.get('ckan.site_id')",
            "        fq += \"+state:active \"",
            "",
            "        conn = make_connection()",
            "        data = conn.search(query, fq=fq, rows=max_results, fl='id')",
            "        return [r.get('id') for r in data.docs]",
            "",
            "    def get_index(self, reference: str) -> dict[str, Any]:",
            "        query = {",
            "            'rows': 1,",
            "            'q': 'name:\"%s\" OR id:\"%s\"' % (reference, reference),",
            "            'wt': 'json',",
            "            'fq': 'site_id:\"%s\" ' % config.get('ckan.site_id') + '+entity_type:package'}",
            "",
            "        conn = make_connection(decode_dates=False)",
            "        log.debug('Package query: %r' % query)",
            "        try:",
            "            solr_response = conn.search(**query)",
            "        except pysolr.SolrError as e:",
            "            raise SearchError(",
            "                'SOLR returned an error running query: %r Error: %r' %",
            "                (query, e))",
            "",
            "        if solr_response.hits == 0:",
            "            raise SearchError('Dataset not found in the search index: %s' %",
            "                              reference)",
            "        else:",
            "            return cast(\"list[dict[str, Any]]\", solr_response.docs)[0]",
            "",
            "    def run(self,",
            "            query: dict[str, Any],",
            "            permission_labels: Optional[list[str]] = None,",
            "            **kwargs: Any) -> dict[str, Any]:",
            "        '''",
            "        Performs a dataset search using the given query.",
            "",
            "        :param query: dictionary with keys like: q, fq, sort, rows, facet",
            "        :type query: dict",
            "        :param permission_labels: filter results to those that include at",
            "            least one of these labels. None to not filter (return everything)",
            "        :type permission_labels: list of unicode strings; or None",
            "",
            "        :returns: dictionary with keys results and count",
            "",
            "        May raise SearchQueryError or SearchError.",
            "        '''",
            "        assert isinstance(query, (dict, MultiDict))",
            "        # check that query keys are valid",
            "        if not set(query.keys()) <= VALID_SOLR_PARAMETERS:",
            "            invalid_params = [s for s in set(query.keys()) - VALID_SOLR_PARAMETERS]",
            "            raise SearchQueryError(\"Invalid search parameters: %s\" % invalid_params)",
            "",
            "        # default query is to return all documents",
            "        q = query.get('q')",
            "        if not q or q == '\"\"' or q == \"''\":",
            "            query['q'] = \"*:*\"",
            "",
            "        # number of results",
            "        rows_to_return = int(query.get('rows', 10))",
            "        # query['rows'] should be a defaulted int, due to schema, but make",
            "        # certain, for legacy tests",
            "        if rows_to_return > 0:",
            "            # #1683 Work around problem of last result being out of order",
            "            #       in SOLR 1.4",
            "            rows_to_query = rows_to_return + 1",
            "        else:",
            "            rows_to_query = rows_to_return",
            "        query['rows'] = rows_to_query",
            "",
            "        fq = []",
            "        if 'fq' in query:",
            "            fq.append(query['fq'])",
            "        fq.extend(query.get('fq_list', []))",
            "",
            "        # show only results from this CKAN instance",
            "        fq.append('+site_id:%s' % solr_literal(config.get('ckan.site_id')))",
            "",
            "        # filter for package status",
            "        if not any('+state:' in _item for _item in fq):",
            "            fq.append('+state:active')",
            "",
            "        # only return things we should be able to see",
            "        if permission_labels is not None:",
            "            fq.append('+permission_labels:(%s)' % ' OR '.join(",
            "                solr_literal(p) for p in permission_labels))",
            "        query['fq'] = fq",
            "",
            "        # faceting",
            "        query['facet'] = query.get('facet', 'true')",
            "        query['facet.limit'] = query.get('facet.limit', config.get('search.facets.limit'))",
            "        query['facet.mincount'] = query.get('facet.mincount', 1)",
            "",
            "        # return the package ID and search scores",
            "        query['fl'] = query.get('fl', 'name')",
            "",
            "        # return results as json encoded string",
            "        query['wt'] = query.get('wt', 'json')",
            "",
            "        # If the query has a colon in it then consider it a fielded search and do use dismax.",
            "        defType = query.get('defType', 'dismax')",
            "        if ':' not in query['q'] or defType == 'edismax':",
            "            query['defType'] = defType",
            "            query['tie'] = query.get('tie', '0.1')",
            "            # this minimum match is explained",
            "            # http://wiki.apache.org/solr/DisMaxQParserPlugin#mm_.28Minimum_.27Should.27_Match.29",
            "            query['mm'] = query.get('mm', '2<-1 5<80%')",
            "            query['qf'] = query.get('qf', QUERY_FIELDS)",
            "",
            "        query.setdefault(\"df\", \"text\")",
            "        query.setdefault(\"q.op\", \"AND\")",
            "",
            "        def _check_query_parser(param: str, value: Any):",
            "            if isinstance(value, str) and value.strip().startswith(\"{!\"):",
            "                if not _get_local_query_parser(value) in config[\"ckan.search.solr_allowed_query_parsers\"]:",
            "                   raise SearchError(f\"Local parameters are not supported in param '{param}'.\")",
            "",
            "        for param in query.keys():",
            "            if isinstance(query[param], str):",
            "                _check_query_parser(param, query[param])",
            "            elif isinstance(query[param], list):",
            "                for item in query[param]:",
            "                    _check_query_parser(param, item)",
            "",
            "",
            "        conn = make_connection(decode_dates=False)",
            "        log.debug('Package query: %r' % query)",
            "        try:",
            "            solr_response = conn.search(**query)",
            "        except pysolr.SolrError as e:",
            "            # Error with the sort parameter.  You see slightly different",
            "            # error messages depending on whether the SOLR JSON comes back",
            "            # or Jetty gets in the way converting it to HTML - not sure why",
            "            #",
            "            if e.args and isinstance(e.args[0], str):",
            "                if \"Can't determine a Sort Order\" in e.args[0] or \\",
            "                        \"Can't determine Sort Order\" in e.args[0] or \\",
            "                        'Unknown sort order' in e.args[0]:",
            "                    raise SearchQueryError('Invalid \"sort\" parameter')",
            "            raise SearchError('SOLR returned an error running query: %r Error: %r' %",
            "                              (query, e))",
            "        self.count = solr_response.hits",
            "        self.results = cast(\"list[Any]\", solr_response.docs)",
            "",
            "",
            "        # #1683 Filter out the last row that is sometimes out of order",
            "        self.results = self.results[:rows_to_return]",
            "",
            "        # get any extras and add to 'extras' dict",
            "        for result in self.results:",
            "            extra_keys = filter(lambda x: x.startswith('extras_'), result.keys())",
            "            extras = {}",
            "            for extra_key in list(extra_keys):",
            "                value = result.pop(extra_key)",
            "                extras[extra_key[len('extras_'):]] = value",
            "            if extra_keys:",
            "                result['extras'] = extras",
            "",
            "        # if just fetching the id or name, return a list instead of a dict",
            "        if query.get('fl') in ['id', 'name']:",
            "            self.results = [r.get(query['fl']) for r in self.results]",
            "",
            "        # get facets and convert facets list to a dict",
            "        self.facets = solr_response.facets.get('facet_fields', {})",
            "        for field, values in self.facets.items():",
            "            self.facets[field] = dict(zip(values[0::2], values[1::2]))",
            "",
            "        return {'results': self.results, 'count': self.count}",
            "",
            "",
            "def solr_literal(t: str) -> str:",
            "    '''",
            "    return a safe literal string for a solr query. Instead of escaping",
            "    each of + - && || ! ( ) { } [ ] ^ \" ~ * ? : \\\\ / we're just dropping",
            "    double quotes -- this method currently only used by tokens like site_id",
            "    and permission labels.",
            "    '''",
            "    return u'\"' + t.replace(u'\"', u'') + u'\"'"
        ],
        "afterPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import re",
            "import logging",
            "from typing import Any, NoReturn, Optional, Union, cast, Dict",
            "from pyparsing import (",
            "    Word, QuotedString, Suppress, OneOrMore, Group, alphanums",
            ")",
            "from pyparsing.exceptions import ParseException",
            "import pysolr",
            "",
            "from ckan.common import asbool",
            "from werkzeug.datastructures import MultiDict",
            "",
            "import ckan.logic as logic",
            "import ckan.model as model",
            "",
            "from ckan.common import config",
            "from ckan.lib.search.common import (",
            "    make_connection, SearchError, SearchQueryError, SolrConnectionError",
            ")",
            "from ckan.types import Context",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "_open_licenses: Optional[list[str]] = None",
            "",
            "VALID_SOLR_PARAMETERS = set([",
            "    'q', 'fl', 'fq', 'rows', 'sort', 'start', 'wt', 'qf', 'bf', 'boost',",
            "    'facet', 'facet.mincount', 'facet.limit', 'facet.field',",
            "    'extras', 'fq_list', 'tie', 'defType', 'mm', 'df'",
            "])",
            "",
            "# for (solr) package searches, this specifies the fields that are searched",
            "# and their relative weighting",
            "QUERY_FIELDS = \"name^4 title^4 tags^2 groups^2 text\"",
            "",
            "solr_regex = re.compile(r'([\\\\+\\-&|!(){}\\[\\]^\"~*?:])')",
            "",
            "def escape_legacy_argument(val: str) -> str:",
            "    # escape special chars \\+-&|!(){}[]^\"~*?:",
            "    return solr_regex.sub(r'\\\\\\1', val)",
            "",
            "",
            "def convert_legacy_parameters_to_solr(",
            "        legacy_params: dict[str, Any]) -> dict[str, Any]:",
            "    '''API v1 and v2 allowed search params that the SOLR syntax does not",
            "    support, so use this function to convert those to SOLR syntax.",
            "    See tests for examples.",
            "",
            "    raises SearchQueryError on invalid params.",
            "    '''",
            "    options = QueryOptions(**legacy_params)",
            "    options.validate()",
            "    solr_params = legacy_params.copy()",
            "    solr_q_list: list[str] = []",
            "    if solr_params.get('q'):",
            "        solr_q_list.append(solr_params['q'].replace('+', ' '))",
            "    non_solr_params = set(legacy_params.keys()) - VALID_SOLR_PARAMETERS",
            "    for search_key in non_solr_params:",
            "        value_obj = legacy_params[search_key]",
            "        value = value_obj.replace('+', ' ') if isinstance(value_obj, str) else value_obj",
            "        if search_key == 'all_fields':",
            "            if value:",
            "                solr_params['fl'] = '*'",
            "        elif search_key == 'offset':",
            "            solr_params['start'] = value",
            "        elif search_key == 'limit':",
            "            solr_params['rows'] = value",
            "        elif search_key == 'order_by':",
            "            solr_params['sort'] = '%s asc' % value",
            "        elif search_key == 'tags':",
            "            if isinstance(value_obj, list):",
            "                tag_list = value_obj",
            "            elif isinstance(value_obj, str):",
            "                tag_list = [value_obj]",
            "            else:",
            "                raise SearchQueryError('Was expecting either a string or JSON list for the tags parameter: %r' % value)",
            "            solr_q_list.extend(['tags:\"%s\"' % escape_legacy_argument(tag) for tag in tag_list])",
            "        else:",
            "            if len(value.strip()):",
            "                value = escape_legacy_argument(value)",
            "                if ' ' in value:",
            "                    value = '\"%s\"' % value",
            "                solr_q_list.append('%s:%s' % (search_key, value))",
            "        del solr_params[search_key]",
            "    solr_params['q'] = ' '.join(solr_q_list)",
            "    if non_solr_params:",
            "        log.debug('Converted legacy search params from %r to %r',",
            "                 legacy_params, solr_params)",
            "    return solr_params",
            "",
            "",
            "def _parse_local_params(local_params: str) -> list[Union[str, list[str]]]:",
            "    \"\"\"",
            "    Parse a local parameters section as return it as a list, eg:",
            "",
            "    {!dismax qf=myfield v='some value'} -> ['dismax', ['qf', 'myfield'], ['v', 'some value']]",
            "",
            "",
            "    {!type=dismax qf=myfield v='some value'} -> [['type', 'dismax'], ['qf', 'myfield'], ['v', 'some value']]",
            "",
            "    \"\"\"",
            "    key = Word(alphanums + \"_.\")",
            "    value = QuotedString('\"') | QuotedString(\"'\") | Word(alphanums + \"_$\")",
            "    pair = Group(key + Suppress(\"=\") + value)",
            "    expression = Suppress(\"{!\") + OneOrMore(pair | key) + Suppress(\"}\")",
            "",
            "    return expression.parse_string(local_params).as_list()",
            "",
            "",
            "def _get_local_query_parser(q: str) -> str:",
            "    \"\"\"",
            "    Given a Solr parameter, extract any custom query parsers used in the",
            "    local parameters, .e.g. q={!child ...}...",
            "    \"\"\"",
            "    qp_type = \"\"",
            "    q = q.strip()",
            "    if not q.startswith(\"{!\"):",
            "        return qp_type",
            "",
            "    try:",
            "        local_params = q[:q.rindex(\"}\") + 1]",
            "        parts = _parse_local_params(local_params)",
            "    except (ParseException, ValueError) as e:",
            "        raise SearchQueryError(f\"Could not parse incoming local parameters: {e}\")",
            "",
            "    if isinstance(parts[0], str):",
            "        # Most common form of defining the query parser type e.g. {!knn ...}",
            "        qp_type = parts[0]",
            "    else:",
            "        # Alternative syntax e.g. {!type=knn ...}",
            "        type_part = [p for p in parts if p[0] == \"type\"]",
            "        if type_part:",
            "            qp_type = type_part[0][1]",
            "    return qp_type",
            "",
            "",
            "class QueryOptions(Dict[str, Any]):",
            "    \"\"\"",
            "    Options specify aspects of the search query which are only tangentially related",
            "    to the query terms (such as limits, etc.).",
            "    NB This is used only by legacy package search and current resource & tag search.",
            "       Modern SOLR package search leaves this to SOLR syntax.",
            "    \"\"\"",
            "",
            "    BOOLEAN_OPTIONS = ['all_fields']",
            "    INTEGER_OPTIONS = ['offset', 'limit']",
            "    UNSUPPORTED_OPTIONS = ['filter_by_downloadable', 'filter_by_openness']",
            "",
            "    limit: int",
            "    offset: int",
            "    order_by: str",
            "    return_objects: bool",
            "    ref_entity_with_attr: str",
            "    all_fields: bool",
            "    search_tags: bool",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        from ckan.lib.search import DEFAULT_OPTIONS",
            "",
            "        # set values according to the defaults",
            "        for option_name, default_value in DEFAULT_OPTIONS.items():",
            "            if not option_name in self:",
            "                self[option_name] = default_value",
            "",
            "        super(QueryOptions, self).__init__(**kwargs)",
            "",
            "    def validate(self) -> None:",
            "        for key, value in self.items():",
            "            if key in self.BOOLEAN_OPTIONS:",
            "                try:",
            "                    value = asbool(value)",
            "                except ValueError:",
            "                    raise SearchQueryError('Value for search option %r must be True or False (1 or 0) but received %r' % (key, value))",
            "            elif key in self.INTEGER_OPTIONS:",
            "                try:",
            "                    value = int(value)",
            "                except ValueError:",
            "                    raise SearchQueryError('Value for search option %r must be an integer but received %r' % (key, value))",
            "            elif key in self.UNSUPPORTED_OPTIONS:",
            "                raise SearchQueryError('Search option %r is not supported' % key)",
            "            self[key] = value",
            "",
            "    def __getattr__(self, name: str) -> Any:",
            "        return self.get(name)",
            "",
            "    def __setattr__(self, name: str, value: Any):",
            "        self[name] = value",
            "",
            "",
            "class SearchQuery(object):",
            "    \"\"\"",
            "    A query is ... when you ask the search engine things. SearchQuery is intended",
            "    to be used for only one query, i.e. it sets state. Definitely not thread-safe.",
            "    \"\"\"",
            "    count: int",
            "    results: list[Any]",
            "    facets: dict[str, Any]",
            "",
            "    def __init__(self) -> None:",
            "        self.results = []",
            "        self.count = 0",
            "",
            "    @property",
            "    def open_licenses(self) -> list[str]:",
            "        # this isn't exactly the very best place to put these, but they stay",
            "        # there persistently.",
            "        # TODO: figure out if they change during run-time.",
            "        global _open_licenses",
            "        if not isinstance(_open_licenses, list):",
            "            _open_licenses = []",
            "            for license in model.Package.get_license_register().values():",
            "                if license and license.isopen():",
            "                    _open_licenses.append(license.id)",
            "        return _open_licenses",
            "",
            "    def get_all_entity_ids(self, max_results: int=1000) -> list[str]:",
            "        \"\"\"",
            "        Return a list of the IDs of all indexed packages.",
            "        \"\"\"",
            "        return []",
            "",
            "    def run(self,",
            "            query: Optional[Union[str, dict[str, Any]]] = None,",
            "            terms: Optional[list[str]] = None,",
            "            fields: Optional[dict[str, Any]] = None,",
            "            facet_by: Optional[list[str]] = None,",
            "            options: Optional[QueryOptions] = None,",
            "            **kwargs: Any) -> NoReturn:",
            "        raise SearchError(\"SearchQuery.run() not implemented!\")",
            "",
            "    # convenience, allows to query(..)",
            "    __call__ = run",
            "",
            "",
            "class TagSearchQuery(SearchQuery):",
            "    \"\"\"Search for tags.\"\"\"",
            "    def run(self,",
            "            query: Optional[Union[str, list[str]]] = None,",
            "            fields: Optional[dict[str, Any]] = None,",
            "            options: Optional[QueryOptions] = None,",
            "            **kwargs: Any) -> dict[str, Any]:",
            "        query = [] if query is None else query",
            "        fields = {} if fields is None else fields",
            "",
            "        if options is None:",
            "            options = QueryOptions(**kwargs)",
            "        else:",
            "            options.update(kwargs)",
            "",
            "        if isinstance(query, str):",
            "            query = [query]",
            "",
            "        query = query[:]  # don't alter caller's query list.",
            "        for field, value in fields.items():",
            "            if field in ('tag', 'tags'):",
            "                query.append(value)",
            "",
            "        context = cast(Context, {'model': model, 'session': model.Session})",
            "        data_dict = {",
            "            'query': query,",
            "            'offset': options.get('offset'),",
            "            'limit': options.get('limit')",
            "        }",
            "        results = logic.get_action('tag_search')(context, data_dict)",
            "",
            "        if not options.return_objects:",
            "            # if options.all_fields is set, return a dict",
            "            # if not, return a list of resource IDs",
            "            if options.all_fields:",
            "                results['results'] = [r.as_dict() for r in results['results']]",
            "            else:",
            "                results['results'] = [r['name'] for r in results['results']]",
            "",
            "        self.count = results['count']",
            "        self.results = results['results']",
            "        return results",
            "",
            "",
            "class ResourceSearchQuery(SearchQuery):",
            "    \"\"\"Search for resources.\"\"\"",
            "    def run(self,",
            "            fields: Optional[dict[str, Any]] = None,",
            "            options: Optional[QueryOptions] = None,",
            "            **kwargs: Any) -> dict[str, Any]:",
            "        if options is None:",
            "            options = QueryOptions(**kwargs)",
            "        else:",
            "            options.update(kwargs)",
            "",
            "        context = cast(Context,{",
            "            'model': model,",
            "            'session': model.Session,",
            "            'search_query': True,",
            "        })",
            "",
            "        # Transform fields into structure required by the resource_search",
            "        # action.",
            "        query: list[str] = []",
            "",
            "        if fields:",
            "            for field, terms in fields.items():",
            "                if isinstance(terms, str):",
            "                    terms = terms.split()",
            "                for term in terms:",
            "                    query.append(':'.join([field, term]))",
            "",
            "        data_dict = {",
            "            'query': query,",
            "            'offset': options.get('offset'),",
            "            'limit': options.get('limit'),",
            "            'order_by': options.get('order_by')",
            "        }",
            "        results = logic.get_action('resource_search')(context, data_dict)",
            "",
            "        if not options.return_objects:",
            "            # if options.all_fields is set, return a dict",
            "            # if not, return a list of resource IDs",
            "            if options.all_fields:",
            "                results['results'] = [r.as_dict() for r in results['results']]",
            "            else:",
            "                results['results'] = [r.id for r in results['results']]",
            "",
            "        self.count = results['count']",
            "        self.results = results['results']",
            "        return results",
            "",
            "",
            "class PackageSearchQuery(SearchQuery):",
            "    def get_all_entity_ids(self, max_results: int = 1000) -> list[str]:",
            "        \"\"\"",
            "        Return a list of the IDs of all indexed packages.",
            "        \"\"\"",
            "        query = \"*:*\"",
            "        fq = \"+site_id:\\\"%s\\\" \" % config.get('ckan.site_id')",
            "        fq += \"+state:active \"",
            "",
            "        conn = make_connection()",
            "        data = conn.search(query, fq=fq, rows=max_results, fl='id')",
            "        return [r.get('id') for r in data.docs]",
            "",
            "    def get_index(self, reference: str) -> dict[str, Any]:",
            "        query = {",
            "            'rows': 1,",
            "            'q': 'name:\"%s\" OR id:\"%s\"' % (reference, reference),",
            "            'wt': 'json',",
            "            'fq': 'site_id:\"%s\" ' % config.get('ckan.site_id') + '+entity_type:package'}",
            "",
            "        conn = make_connection(decode_dates=False)",
            "        log.debug('Package query: %r' % query)",
            "        try:",
            "            solr_response = conn.search(**query)",
            "        except pysolr.SolrError as e:",
            "            raise SearchError(",
            "                'SOLR returned an error running query: %r Error: %r' %",
            "                (query, e))",
            "",
            "        if solr_response.hits == 0:",
            "            raise SearchError('Dataset not found in the search index: %s' %",
            "                              reference)",
            "        else:",
            "            return cast(\"list[dict[str, Any]]\", solr_response.docs)[0]",
            "",
            "    def run(self,",
            "            query: dict[str, Any],",
            "            permission_labels: Optional[list[str]] = None,",
            "            **kwargs: Any) -> dict[str, Any]:",
            "        '''",
            "        Performs a dataset search using the given query.",
            "",
            "        :param query: dictionary with keys like: q, fq, sort, rows, facet",
            "        :type query: dict",
            "        :param permission_labels: filter results to those that include at",
            "            least one of these labels. None to not filter (return everything)",
            "        :type permission_labels: list of unicode strings; or None",
            "",
            "        :returns: dictionary with keys results and count",
            "",
            "        May raise SearchQueryError or SearchError.",
            "        '''",
            "        assert isinstance(query, (dict, MultiDict))",
            "        # check that query keys are valid",
            "        if not set(query.keys()) <= VALID_SOLR_PARAMETERS:",
            "            invalid_params = [s for s in set(query.keys()) - VALID_SOLR_PARAMETERS]",
            "            raise SearchQueryError(\"Invalid search parameters: %s\" % invalid_params)",
            "",
            "        # default query is to return all documents",
            "        q = query.get('q')",
            "        if not q or q == '\"\"' or q == \"''\":",
            "            query['q'] = \"*:*\"",
            "",
            "        # number of results",
            "        rows_to_return = int(query.get('rows', 10))",
            "        # query['rows'] should be a defaulted int, due to schema, but make",
            "        # certain, for legacy tests",
            "        if rows_to_return > 0:",
            "            # #1683 Work around problem of last result being out of order",
            "            #       in SOLR 1.4",
            "            rows_to_query = rows_to_return + 1",
            "        else:",
            "            rows_to_query = rows_to_return",
            "        query['rows'] = rows_to_query",
            "",
            "        fq = []",
            "        if 'fq' in query:",
            "            fq.append(query['fq'])",
            "        fq.extend(query.get('fq_list', []))",
            "",
            "        # show only results from this CKAN instance",
            "        fq.append('+site_id:%s' % solr_literal(config.get('ckan.site_id')))",
            "",
            "        # filter for package status",
            "        if not any('+state:' in _item for _item in fq):",
            "            fq.append('+state:active')",
            "",
            "        # only return things we should be able to see",
            "        if permission_labels is not None:",
            "            fq.append('+permission_labels:(%s)' % ' OR '.join(",
            "                solr_literal(p) for p in permission_labels))",
            "        query['fq'] = fq",
            "",
            "        # faceting",
            "        query['facet'] = query.get('facet', 'true')",
            "        query['facet.limit'] = query.get('facet.limit', config.get('search.facets.limit'))",
            "        query['facet.mincount'] = query.get('facet.mincount', 1)",
            "",
            "        # return the package ID and search scores",
            "        query['fl'] = query.get('fl', 'name')",
            "",
            "        # return results as json encoded string",
            "        query['wt'] = query.get('wt', 'json')",
            "",
            "        # If the query has a colon in it then consider it a fielded search and do use dismax.",
            "        defType = query.get('defType', 'dismax')",
            "        if ':' not in query['q'] or defType == 'edismax':",
            "            query['defType'] = defType",
            "            query['tie'] = query.get('tie', '0.1')",
            "            # this minimum match is explained",
            "            # http://wiki.apache.org/solr/DisMaxQParserPlugin#mm_.28Minimum_.27Should.27_Match.29",
            "            query['mm'] = query.get('mm', '2<-1 5<80%')",
            "            query['qf'] = query.get('qf', QUERY_FIELDS)",
            "",
            "        query.setdefault(\"df\", \"text\")",
            "        query.setdefault(\"q.op\", \"AND\")",
            "",
            "        def _check_query_parser(param: str, value: Any):",
            "            if isinstance(value, str) and value.strip().startswith(\"{!\"):",
            "                if not _get_local_query_parser(value) in config[\"ckan.search.solr_allowed_query_parsers\"]:",
            "                   raise SearchError(f\"Local parameters are not supported in param '{param}'.\")",
            "",
            "        for param in query.keys():",
            "            if isinstance(query[param], str):",
            "                _check_query_parser(param, query[param])",
            "            elif isinstance(query[param], list):",
            "                for item in query[param]:",
            "                    _check_query_parser(param, item)",
            "",
            "",
            "        conn = make_connection(decode_dates=False)",
            "        log.debug('Package query: %r' % query)",
            "        try:",
            "            solr_response = conn.search(**query)",
            "        except pysolr.SolrError as e:",
            "            # Error with the sort parameter.  You see slightly different",
            "            # error messages depending on whether the SOLR JSON comes back",
            "            # or Jetty gets in the way converting it to HTML - not sure why",
            "            #",
            "            if e.args and isinstance(e.args[0], str):",
            "                if \"Can't determine a Sort Order\" in e.args[0] or \\",
            "                        \"Can't determine Sort Order\" in e.args[0] or \\",
            "                        'Unknown sort order' in e.args[0]:",
            "                    raise SearchQueryError('Invalid \"sort\" parameter')",
            "",
            "                if (\"Failed to connect to server\" in e.args[0] or ",
            "                        \"Connection to server\" in e.args[0]):",
            "                    log.warning(\"Connection Error: Failed to connect to Solr server.\")",
            "                    raise SolrConnectionError(\"Solr returned an error while searching.\")",
            "",
            "            raise SearchError('SOLR returned an error running query: %r Error: %r' %",
            "                              (query, e))",
            "        self.count = solr_response.hits",
            "        self.results = cast(\"list[Any]\", solr_response.docs)",
            "",
            "",
            "        # #1683 Filter out the last row that is sometimes out of order",
            "        self.results = self.results[:rows_to_return]",
            "",
            "        # get any extras and add to 'extras' dict",
            "        for result in self.results:",
            "            extra_keys = filter(lambda x: x.startswith('extras_'), result.keys())",
            "            extras = {}",
            "            for extra_key in list(extra_keys):",
            "                value = result.pop(extra_key)",
            "                extras[extra_key[len('extras_'):]] = value",
            "            if extra_keys:",
            "                result['extras'] = extras",
            "",
            "        # if just fetching the id or name, return a list instead of a dict",
            "        if query.get('fl') in ['id', 'name']:",
            "            self.results = [r.get(query['fl']) for r in self.results]",
            "",
            "        # get facets and convert facets list to a dict",
            "        self.facets = solr_response.facets.get('facet_fields', {})",
            "        for field, values in self.facets.items():",
            "            self.facets[field] = dict(zip(values[0::2], values[1::2]))",
            "",
            "        return {'results': self.results, 'count': self.count}",
            "",
            "",
            "def solr_literal(t: str) -> str:",
            "    '''",
            "    return a safe literal string for a solr query. Instead of escaping",
            "    each of + - && || ! ( ) { } [ ] ^ \" ~ * ? : \\\\ / we're just dropping",
            "    double quotes -- this method currently only used by tokens like site_id",
            "    and permission labels.",
            "    '''",
            "    return u'\"' + t.replace(u'\"', u'') + u'\"'"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "21": []
        },
        "addLocation": []
    },
    "ckan/tests/controllers/test_api.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 416,
                "afterPatchRowNumber": 416,
                "PatchRowcode": "     res = app.post(url, environ_overrides=env, data=data)"
            },
            "1": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 417,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 418,
                "PatchRowcode": "     assert res.status_code == 200"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 419,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 420,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 421,
                "PatchRowcode": "+@pytest.mark.ckan_config(\"solr_url\", \"https://xxxx/notofund\")"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 422,
                "PatchRowcode": "+def test_package_search_connection_errors(app):"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 423,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 424,
                "PatchRowcode": "+    res = app.get("
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 425,
                "PatchRowcode": "+        url_for(\"api.action\", logic_function=\"package_search\", ver=3),"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 426,
                "PatchRowcode": "+    )"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 427,
                "PatchRowcode": "+    assert res.json[\"error\"][\"__type\"] == \"Search Connection Error\""
            }
        },
        "frontPatchFile": [
            "# encoding: utf-8",
            "\"\"\"",
            "NB Don't test logic functions here. This is just for the mechanics of the API",
            "controller itself.",
            "\"\"\"",
            "import json",
            "import re",
            "",
            "import pytest",
            "import six",
            "from io import BytesIO",
            "from ckan.lib.helpers import url_for",
            "import ckan.tests.helpers as helpers",
            "from ckan.tests import factories",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"ver, expected, status\",",
            "    [(0, 1, 404), (1, 1, 200), (2, 2, 200), (3, 3, 200), (4, 1, 404)],",
            ")",
            "def test_get_api_version(ver, expected, status, app):",
            "    resp = app.get(url_for(\"api.get_api\", ver=str(ver)), status=status)",
            "    if status == 200:",
            "        assert resp.json[\"version\"] == expected",
            "",
            "",
            "def test_readonly_is_get_able_with_normal_url_params(app):",
            "    \"\"\"Test that a read-only action is GET-able",
            "",
            "    Picks an action within `get.py` and checks that it works if it's",
            "    invoked with a http GET request.  The action's data_dict is",
            "    populated from the url parameters.",
            "    \"\"\"",
            "    params = {\"q\": \"russian\"}",
            "    app.get(",
            "        url_for(\"api.action\", logic_function=\"package_search\", ver=3),",
            "        params=params,",
            "        status=200,",
            "    )",
            "",
            "",
            "def test_sideeffect_action_is_not_get_able(app):",
            "    \"\"\"Test that a non-readonly action is not GET-able.",
            "",
            "    Picks an action outside of `get.py`, and checks that it 400s if an",
            "    attempt to invoke with a http GET request is made.",
            "    \"\"\"",
            "    data_dict = {\"type\": \"dataset\", \"name\": \"a-name\"}",
            "    resp = app.get(",
            "        url_for(\"api.action\", logic_function=\"package_create\", ver=3),",
            "        json=data_dict,",
            "        status=400,",
            "    )",
            "    msg = (",
            "        \"Bad request - JSON Error: Invalid request.\"",
            "        \" Please use POST method for your request\"",
            "    )",
            "    assert msg in resp",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\", \"with_request_context\")",
            "class TestApiController(object):",
            "    def test_resource_create_upload_file(",
            "        self, app, monkeypatch, tmpdir, ckan_config",
            "    ):",
            "        monkeypatch.setitem(ckan_config, u\"ckan.storage_path\", str(tmpdir))",
            "",
            "        user = factories.User()",
            "        user_token = factories.APIToken(user=user[\"name\"])",
            "        pkg = factories.Dataset(creator_user_id=user[\"id\"])",
            "",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"resource_create\",",
            "            ver=3,",
            "        )",
            "        env = {\"Authorization\": user_token[\"token\"]}",
            "",
            "        content = six.ensure_binary('upload-content')",
            "        upload_content = BytesIO(content)",
            "        postparams = {",
            "            \"name\": \"test-flask-upload\",",
            "            \"package_id\": pkg[\"id\"],",
            "            \"upload\": (upload_content, \"test-upload.txt\"),",
            "        }",
            "",
            "        resp = app.post(",
            "            url,",
            "            extra_environ=env,",
            "            data=postparams,",
            "            content_type=\"multipart/form-data\",",
            "        )",
            "        result = resp.json[\"result\"]",
            "        assert \"upload\" == result[\"url_type\"]",
            "        assert len(content) == result[\"size\"]",
            "",
            "    def test_unicode_in_error_message_works_ok(self, app):",
            "        # Use tag_delete to echo back some unicode",
            "",
            "        org_url = \"/api/action/tag_delete\"",
            "        data_dict = {\"id\": u\"Delta symbol: \\u0394\"}  # unicode gets rec'd ok",
            "        response = app.post(url=org_url, data=data_dict, status=404)",
            "        # The unicode is backslash encoded (because that is the default when",
            "        # you do str(exception) )",
            "        assert helpers.body_contains(response, \"Delta symbol: \\\\u0394\")",
            "",
            "    @pytest.mark.usefixtures(\"clean_index\")",
            "    def test_dataset_autocomplete_name(self, app):",
            "        dataset = factories.Dataset(name=\"rivers\")",
            "        url = url_for(\"api.dataset_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/dataset/autocomplete\"",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"incomplete\": u\"rive\"}, status=200",
            "        )",
            "",
            "        results = json.loads(response.body)",
            "        assert results == {",
            "            u\"ResultSet\": {",
            "                u\"Result\": [",
            "                    {",
            "                        u\"match_field\": u\"name\",",
            "                        u\"name\": u\"rivers\",",
            "                        u\"match_displayed\": u\"rivers\",",
            "                        u\"title\": dataset[\"title\"],",
            "                    }",
            "                ]",
            "            }",
            "        }",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    @pytest.mark.usefixtures(\"clean_index\")",
            "    def test_dataset_autocomplete_title(self, app):",
            "        dataset = factories.Dataset(name=\"test_ri\", title=\"Rivers\")",
            "        url = url_for(\"api.dataset_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/dataset/autocomplete\"",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"incomplete\": u\"riv\"}, status=200",
            "        )",
            "",
            "        results = json.loads(response.body)",
            "        assert results == {",
            "            u\"ResultSet\": {",
            "                u\"Result\": [",
            "                    {",
            "                        u\"match_field\": u\"title\",",
            "                        u\"name\": dataset[\"name\"],",
            "                        u\"match_displayed\": u\"Rivers (test_ri)\",",
            "                        u\"title\": u\"Rivers\",",
            "                    }",
            "                ]",
            "            }",
            "        }",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_tag_autocomplete(self, app):",
            "        factories.Dataset(tags=[{\"name\": \"rivers \u30a2\"}])",
            "        url = url_for(\"api.tag_autocomplete\", ver=2)",
            "",
            "        assert url == \"/api/2/util/tag/autocomplete\"",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"incomplete\": u\"rs \u30a2\"}, status=200",
            "        )",
            "",
            "        assert response.json == {",
            "            \"ResultSet\": {\"Result\": [{\"Name\": \"rivers \u30a2\"}]}",
            "        }",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_group_autocomplete_by_name(self, app):",
            "        factories.Group(name=\"rivers\", title=\"Bridges\")",
            "        url = url_for(\"api.group_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/group/autocomplete\"",
            "",
            "        response = app.get(url=url, query_string={\"q\": u\"rive\"}, status=200)",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"name\"] == \"rivers\"",
            "        assert results[0][\"title\"] == \"Bridges\"",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_group_autocomplete_by_title(self, app):",
            "        factories.Group(name=\"frogs\", title=\"Bugs\")",
            "        url = url_for(\"api.group_autocomplete\", ver=2)",
            "",
            "        response = app.get(url=url, query_string={\"q\": u\"bug\"}, status=200)",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"name\"] == \"frogs\"",
            "",
            "    def test_organization_autocomplete_by_name(self, app):",
            "        org = factories.Organization(name=\"simple-dummy-org\")",
            "        url = url_for(\"api.organization_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/organization/autocomplete\"",
            "",
            "        response = app.get(url=url, query_string={\"q\": u\"simple\"}, status=200)",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"name\"] == \"simple-dummy-org\"",
            "        assert results[0][\"title\"] == org[\"title\"]",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_organization_autocomplete_by_title(self, app):",
            "        factories.Organization(title=\"Simple dummy org\")",
            "        url = url_for(\"api.organization_autocomplete\", ver=2)",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"q\": u\"simple dum\"}, status=200",
            "        )",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"title\"] == \"Simple dummy org\"",
            "",
            "    def test_config_option_list_access_sysadmin(self, app):",
            "        user = factories.Sysadmin()",
            "        user_token = factories.APIToken(user=user[\"name\"])",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"config_option_list\",",
            "            ver=3,",
            "        )",
            "        env = {\"Authorization\": user_token[\"token\"]}",
            "        app.get(url=url, extra_environ=env, query_string={}, status=200)",
            "",
            "    def test_config_option_list_access_sysadmin_jsonp(self, app):",
            "        user = factories.Sysadmin()",
            "        user_token = factories.APIToken(user=user[\"name\"])",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"config_option_list\",",
            "            ver=3,",
            "        )",
            "        env = {\"Authorization\": user_token[\"token\"]}",
            "        app.get(url=url, extra_environ=env, query_string={\"callback\": \"myfn\"}, status=403)",
            "",
            "    def test_jsonp_works_on_get_requests(self, app):",
            "",
            "        dataset1 = factories.Dataset()",
            "        dataset2 = factories.Dataset()",
            "",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"package_list\",",
            "            ver=3,",
            "        )",
            "",
            "        res = app.get(url=url, query_string={\"callback\": \"my_callback\"})",
            "        assert re.match(r\"my_callback\\(.*\\);\", six.ensure_str(res.body)), res",
            "        # Unwrap JSONP callback (we want to look at the data).",
            "        start = len(\"my_callback\") + 1",
            "        msg = res.body[start:-2]",
            "        res_dict = json.loads(msg)",
            "        assert res_dict[\"success\"]",
            "        assert sorted(res_dict[\"result\"]) == sorted(",
            "            [dataset1[\"name\"], dataset2[\"name\"]]",
            "        )",
            "",
            "    def test_jsonp_returns_javascript_content_type(self, app):",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"status_show\",",
            "            ver=3,",
            "        )",
            "",
            "        res = app.get(url=url, query_string={\"callback\": \"my_callback\"})",
            "        assert \"application/javascript\" in res.headers.get(\"Content-Type\")",
            "",
            "    def test_jsonp_does_not_work_on_post_requests(self, app):",
            "",
            "        dataset1 = factories.Dataset()",
            "        dataset2 = factories.Dataset()",
            "",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"package_list\",",
            "            ver=3,",
            "            callback=\"my_callback\",",
            "        )",
            "",
            "        res = app.post(url=url)",
            "        # The callback param is ignored and the normal response is returned",
            "        assert not six.ensure_str(res.body).startswith(\"my_callback\")",
            "        res_dict = json.loads(res.body)",
            "        assert res_dict[\"success\"]",
            "        assert sorted(res_dict[\"result\"]) == sorted(",
            "            [dataset1[\"name\"], dataset2[\"name\"]]",
            "        )",
            "",
            "    @pytest.mark.parametrize(",
            "        \"incomplete, expected\",",
            "        [",
            "            (None, set()),",
            "            (\"\", set()),",
            "            (\"cs\", {\"csv\"}),",
            "            (\"on\", {\"json\"}),",
            "            (\"s\", {\"csv\", \"json\"}),",
            "            (\"xls\", set()),",
            "        ],",
            "    )",
            "    def test_format_autocomplete(self, incomplete, expected, app):",
            "        factories.Resource(format=\"CSV\")",
            "        factories.Resource(format=\"JSON\")",
            "",
            "        resp = app.get(",
            "            url_for(\"api.format_autocomplete\", ver=2, incomplete=incomplete)",
            "        )",
            "        result = {res[\"Format\"] for res in resp.json[\"ResultSet\"][\"Result\"]}",
            "",
            "        assert result == expected",
            "",
            "",
            "def test_i18n_only_known_locales_are_accepted(app):",
            "",
            "    url = url_for(\"api.i18n_js_translations\", ver=2, lang=\"fr\")",
            "",
            "    assert app.get(url).status_code == 200",
            "",
            "    url = url_for(\"api.i18n_js_translations\", ver=2, lang=\"unknown_lang\")",
            "    r = app.get(url, status=400)",
            "    assert \"Bad request - Unknown locale\" in r.get_data(as_text=True)",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "def test_cookie_based_auth_default(app):",
            "",
            "    sysadmin = factories.Sysadmin()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_show\", id=dataset[\"id\"])",
            "",
            "    env = {\"REMOTE_USER\": sysadmin[\"name\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 200",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "@pytest.mark.ckan_config(\"ckan.auth.enable_cookie_auth_in_api\", False)",
            "def test_cookie_based_auth_disabled(app):",
            "",
            "    sysadmin = factories.Sysadmin()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_show\", id=dataset[\"id\"])",
            "",
            "    env = {\"REMOTE_USER\": sysadmin[\"name\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 403",
            "",
            "    # Check that token auth still works",
            "    user_token = factories.APIToken(user=sysadmin[\"name\"])",
            "    env = {\"Authorization\": user_token[\"token\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 200",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "def test_header_based_auth_default(app):",
            "",
            "    sysadmin = factories.SysadminWithToken()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_show\", id=dataset[\"id\"])",
            "",
            "    env = {\"Authorization\": sysadmin[\"token\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 200",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "def test_header_based_auth_default_post(app):",
            "",
            "    sysadmin = factories.SysadminWithToken()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_patch\")",
            "",
            "    env = {\"Authorization\": sysadmin[\"token\"]}",
            "",
            "    data = {",
            "        \"id\": dataset[\"id\"],",
            "        \"notes\": \"updated\",",
            "    }",
            "    res = app.post(url, environ_overrides=env, data=data)",
            "",
            "    assert res.status_code == 200"
        ],
        "afterPatchFile": [
            "# encoding: utf-8",
            "\"\"\"",
            "NB Don't test logic functions here. This is just for the mechanics of the API",
            "controller itself.",
            "\"\"\"",
            "import json",
            "import re",
            "",
            "import pytest",
            "import six",
            "from io import BytesIO",
            "from ckan.lib.helpers import url_for",
            "import ckan.tests.helpers as helpers",
            "from ckan.tests import factories",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"ver, expected, status\",",
            "    [(0, 1, 404), (1, 1, 200), (2, 2, 200), (3, 3, 200), (4, 1, 404)],",
            ")",
            "def test_get_api_version(ver, expected, status, app):",
            "    resp = app.get(url_for(\"api.get_api\", ver=str(ver)), status=status)",
            "    if status == 200:",
            "        assert resp.json[\"version\"] == expected",
            "",
            "",
            "def test_readonly_is_get_able_with_normal_url_params(app):",
            "    \"\"\"Test that a read-only action is GET-able",
            "",
            "    Picks an action within `get.py` and checks that it works if it's",
            "    invoked with a http GET request.  The action's data_dict is",
            "    populated from the url parameters.",
            "    \"\"\"",
            "    params = {\"q\": \"russian\"}",
            "    app.get(",
            "        url_for(\"api.action\", logic_function=\"package_search\", ver=3),",
            "        params=params,",
            "        status=200,",
            "    )",
            "",
            "",
            "def test_sideeffect_action_is_not_get_able(app):",
            "    \"\"\"Test that a non-readonly action is not GET-able.",
            "",
            "    Picks an action outside of `get.py`, and checks that it 400s if an",
            "    attempt to invoke with a http GET request is made.",
            "    \"\"\"",
            "    data_dict = {\"type\": \"dataset\", \"name\": \"a-name\"}",
            "    resp = app.get(",
            "        url_for(\"api.action\", logic_function=\"package_create\", ver=3),",
            "        json=data_dict,",
            "        status=400,",
            "    )",
            "    msg = (",
            "        \"Bad request - JSON Error: Invalid request.\"",
            "        \" Please use POST method for your request\"",
            "    )",
            "    assert msg in resp",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\", \"with_request_context\")",
            "class TestApiController(object):",
            "    def test_resource_create_upload_file(",
            "        self, app, monkeypatch, tmpdir, ckan_config",
            "    ):",
            "        monkeypatch.setitem(ckan_config, u\"ckan.storage_path\", str(tmpdir))",
            "",
            "        user = factories.User()",
            "        user_token = factories.APIToken(user=user[\"name\"])",
            "        pkg = factories.Dataset(creator_user_id=user[\"id\"])",
            "",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"resource_create\",",
            "            ver=3,",
            "        )",
            "        env = {\"Authorization\": user_token[\"token\"]}",
            "",
            "        content = six.ensure_binary('upload-content')",
            "        upload_content = BytesIO(content)",
            "        postparams = {",
            "            \"name\": \"test-flask-upload\",",
            "            \"package_id\": pkg[\"id\"],",
            "            \"upload\": (upload_content, \"test-upload.txt\"),",
            "        }",
            "",
            "        resp = app.post(",
            "            url,",
            "            extra_environ=env,",
            "            data=postparams,",
            "            content_type=\"multipart/form-data\",",
            "        )",
            "        result = resp.json[\"result\"]",
            "        assert \"upload\" == result[\"url_type\"]",
            "        assert len(content) == result[\"size\"]",
            "",
            "    def test_unicode_in_error_message_works_ok(self, app):",
            "        # Use tag_delete to echo back some unicode",
            "",
            "        org_url = \"/api/action/tag_delete\"",
            "        data_dict = {\"id\": u\"Delta symbol: \\u0394\"}  # unicode gets rec'd ok",
            "        response = app.post(url=org_url, data=data_dict, status=404)",
            "        # The unicode is backslash encoded (because that is the default when",
            "        # you do str(exception) )",
            "        assert helpers.body_contains(response, \"Delta symbol: \\\\u0394\")",
            "",
            "    @pytest.mark.usefixtures(\"clean_index\")",
            "    def test_dataset_autocomplete_name(self, app):",
            "        dataset = factories.Dataset(name=\"rivers\")",
            "        url = url_for(\"api.dataset_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/dataset/autocomplete\"",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"incomplete\": u\"rive\"}, status=200",
            "        )",
            "",
            "        results = json.loads(response.body)",
            "        assert results == {",
            "            u\"ResultSet\": {",
            "                u\"Result\": [",
            "                    {",
            "                        u\"match_field\": u\"name\",",
            "                        u\"name\": u\"rivers\",",
            "                        u\"match_displayed\": u\"rivers\",",
            "                        u\"title\": dataset[\"title\"],",
            "                    }",
            "                ]",
            "            }",
            "        }",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    @pytest.mark.usefixtures(\"clean_index\")",
            "    def test_dataset_autocomplete_title(self, app):",
            "        dataset = factories.Dataset(name=\"test_ri\", title=\"Rivers\")",
            "        url = url_for(\"api.dataset_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/dataset/autocomplete\"",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"incomplete\": u\"riv\"}, status=200",
            "        )",
            "",
            "        results = json.loads(response.body)",
            "        assert results == {",
            "            u\"ResultSet\": {",
            "                u\"Result\": [",
            "                    {",
            "                        u\"match_field\": u\"title\",",
            "                        u\"name\": dataset[\"name\"],",
            "                        u\"match_displayed\": u\"Rivers (test_ri)\",",
            "                        u\"title\": u\"Rivers\",",
            "                    }",
            "                ]",
            "            }",
            "        }",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_tag_autocomplete(self, app):",
            "        factories.Dataset(tags=[{\"name\": \"rivers \u30a2\"}])",
            "        url = url_for(\"api.tag_autocomplete\", ver=2)",
            "",
            "        assert url == \"/api/2/util/tag/autocomplete\"",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"incomplete\": u\"rs \u30a2\"}, status=200",
            "        )",
            "",
            "        assert response.json == {",
            "            \"ResultSet\": {\"Result\": [{\"Name\": \"rivers \u30a2\"}]}",
            "        }",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_group_autocomplete_by_name(self, app):",
            "        factories.Group(name=\"rivers\", title=\"Bridges\")",
            "        url = url_for(\"api.group_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/group/autocomplete\"",
            "",
            "        response = app.get(url=url, query_string={\"q\": u\"rive\"}, status=200)",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"name\"] == \"rivers\"",
            "        assert results[0][\"title\"] == \"Bridges\"",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_group_autocomplete_by_title(self, app):",
            "        factories.Group(name=\"frogs\", title=\"Bugs\")",
            "        url = url_for(\"api.group_autocomplete\", ver=2)",
            "",
            "        response = app.get(url=url, query_string={\"q\": u\"bug\"}, status=200)",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"name\"] == \"frogs\"",
            "",
            "    def test_organization_autocomplete_by_name(self, app):",
            "        org = factories.Organization(name=\"simple-dummy-org\")",
            "        url = url_for(\"api.organization_autocomplete\", ver=2)",
            "        assert url == \"/api/2/util/organization/autocomplete\"",
            "",
            "        response = app.get(url=url, query_string={\"q\": u\"simple\"}, status=200)",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"name\"] == \"simple-dummy-org\"",
            "        assert results[0][\"title\"] == org[\"title\"]",
            "        assert (",
            "            response.headers[\"Content-Type\"]",
            "            == \"application/json;charset=utf-8\"",
            "        )",
            "",
            "    def test_organization_autocomplete_by_title(self, app):",
            "        factories.Organization(title=\"Simple dummy org\")",
            "        url = url_for(\"api.organization_autocomplete\", ver=2)",
            "",
            "        response = app.get(",
            "            url=url, query_string={\"q\": u\"simple dum\"}, status=200",
            "        )",
            "",
            "        results = json.loads(response.body)",
            "        assert len(results) == 1",
            "        assert results[0][\"title\"] == \"Simple dummy org\"",
            "",
            "    def test_config_option_list_access_sysadmin(self, app):",
            "        user = factories.Sysadmin()",
            "        user_token = factories.APIToken(user=user[\"name\"])",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"config_option_list\",",
            "            ver=3,",
            "        )",
            "        env = {\"Authorization\": user_token[\"token\"]}",
            "        app.get(url=url, extra_environ=env, query_string={}, status=200)",
            "",
            "    def test_config_option_list_access_sysadmin_jsonp(self, app):",
            "        user = factories.Sysadmin()",
            "        user_token = factories.APIToken(user=user[\"name\"])",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"config_option_list\",",
            "            ver=3,",
            "        )",
            "        env = {\"Authorization\": user_token[\"token\"]}",
            "        app.get(url=url, extra_environ=env, query_string={\"callback\": \"myfn\"}, status=403)",
            "",
            "    def test_jsonp_works_on_get_requests(self, app):",
            "",
            "        dataset1 = factories.Dataset()",
            "        dataset2 = factories.Dataset()",
            "",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"package_list\",",
            "            ver=3,",
            "        )",
            "",
            "        res = app.get(url=url, query_string={\"callback\": \"my_callback\"})",
            "        assert re.match(r\"my_callback\\(.*\\);\", six.ensure_str(res.body)), res",
            "        # Unwrap JSONP callback (we want to look at the data).",
            "        start = len(\"my_callback\") + 1",
            "        msg = res.body[start:-2]",
            "        res_dict = json.loads(msg)",
            "        assert res_dict[\"success\"]",
            "        assert sorted(res_dict[\"result\"]) == sorted(",
            "            [dataset1[\"name\"], dataset2[\"name\"]]",
            "        )",
            "",
            "    def test_jsonp_returns_javascript_content_type(self, app):",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"status_show\",",
            "            ver=3,",
            "        )",
            "",
            "        res = app.get(url=url, query_string={\"callback\": \"my_callback\"})",
            "        assert \"application/javascript\" in res.headers.get(\"Content-Type\")",
            "",
            "    def test_jsonp_does_not_work_on_post_requests(self, app):",
            "",
            "        dataset1 = factories.Dataset()",
            "        dataset2 = factories.Dataset()",
            "",
            "        url = url_for(",
            "            \"api.action\",",
            "            logic_function=\"package_list\",",
            "            ver=3,",
            "            callback=\"my_callback\",",
            "        )",
            "",
            "        res = app.post(url=url)",
            "        # The callback param is ignored and the normal response is returned",
            "        assert not six.ensure_str(res.body).startswith(\"my_callback\")",
            "        res_dict = json.loads(res.body)",
            "        assert res_dict[\"success\"]",
            "        assert sorted(res_dict[\"result\"]) == sorted(",
            "            [dataset1[\"name\"], dataset2[\"name\"]]",
            "        )",
            "",
            "    @pytest.mark.parametrize(",
            "        \"incomplete, expected\",",
            "        [",
            "            (None, set()),",
            "            (\"\", set()),",
            "            (\"cs\", {\"csv\"}),",
            "            (\"on\", {\"json\"}),",
            "            (\"s\", {\"csv\", \"json\"}),",
            "            (\"xls\", set()),",
            "        ],",
            "    )",
            "    def test_format_autocomplete(self, incomplete, expected, app):",
            "        factories.Resource(format=\"CSV\")",
            "        factories.Resource(format=\"JSON\")",
            "",
            "        resp = app.get(",
            "            url_for(\"api.format_autocomplete\", ver=2, incomplete=incomplete)",
            "        )",
            "        result = {res[\"Format\"] for res in resp.json[\"ResultSet\"][\"Result\"]}",
            "",
            "        assert result == expected",
            "",
            "",
            "def test_i18n_only_known_locales_are_accepted(app):",
            "",
            "    url = url_for(\"api.i18n_js_translations\", ver=2, lang=\"fr\")",
            "",
            "    assert app.get(url).status_code == 200",
            "",
            "    url = url_for(\"api.i18n_js_translations\", ver=2, lang=\"unknown_lang\")",
            "    r = app.get(url, status=400)",
            "    assert \"Bad request - Unknown locale\" in r.get_data(as_text=True)",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "def test_cookie_based_auth_default(app):",
            "",
            "    sysadmin = factories.Sysadmin()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_show\", id=dataset[\"id\"])",
            "",
            "    env = {\"REMOTE_USER\": sysadmin[\"name\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 200",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "@pytest.mark.ckan_config(\"ckan.auth.enable_cookie_auth_in_api\", False)",
            "def test_cookie_based_auth_disabled(app):",
            "",
            "    sysadmin = factories.Sysadmin()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_show\", id=dataset[\"id\"])",
            "",
            "    env = {\"REMOTE_USER\": sysadmin[\"name\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 403",
            "",
            "    # Check that token auth still works",
            "    user_token = factories.APIToken(user=sysadmin[\"name\"])",
            "    env = {\"Authorization\": user_token[\"token\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 200",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "def test_header_based_auth_default(app):",
            "",
            "    sysadmin = factories.SysadminWithToken()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_show\", id=dataset[\"id\"])",
            "",
            "    env = {\"Authorization\": sysadmin[\"token\"]}",
            "",
            "    res = app.get(url, environ_overrides=env)",
            "",
            "    assert res.status_code == 200",
            "",
            "",
            "@pytest.mark.usefixtures(\"clean_db\")",
            "def test_header_based_auth_default_post(app):",
            "",
            "    sysadmin = factories.SysadminWithToken()",
            "    org = factories.Organization()",
            "    dataset = factories.Dataset(private=True, owner_org=org[\"id\"])",
            "",
            "    url = url_for(\"api.action\", ver=3, logic_function=\"package_patch\")",
            "",
            "    env = {\"Authorization\": sysadmin[\"token\"]}",
            "",
            "    data = {",
            "        \"id\": dataset[\"id\"],",
            "        \"notes\": \"updated\",",
            "    }",
            "    res = app.post(url, environ_overrides=env, data=data)",
            "",
            "    assert res.status_code == 200",
            "",
            "",
            "@pytest.mark.ckan_config(\"solr_url\", \"https://xxxx/notofund\")",
            "def test_package_search_connection_errors(app):",
            "",
            "    res = app.get(",
            "        url_for(\"api.action\", logic_function=\"package_search\", ver=3),",
            "    )",
            "    assert res.json[\"error\"][\"__type\"] == \"Search Connection Error\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "lib.ansible.modules.user.SunOS.modify_user_usermod"
        ]
    },
    "ckan/views/api.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " from ckan.lib.navl.dictization_functions import DataError"
            },
            "2": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " from ckan.logic import get_action, ValidationError, NotFound, NotAuthorized"
            },
            "3": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from ckan.lib.search import SearchError, SearchIndexError, SearchQueryError"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+from ckan.lib.search import ("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+    SearchError, SearchIndexError, SearchQueryError, SolrConnectionError"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+)"
            },
            "7": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from ckan.types import Context, Response, ActionResult"
            },
            "8": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 333,
                "afterPatchRowNumber": 335,
                "PatchRowcode": "                        str(e)}"
            },
            "11": {
                "beforePatchRowNumber": 334,
                "afterPatchRowNumber": 336,
                "PatchRowcode": "         return_dict[u'success'] = False"
            },
            "12": {
                "beforePatchRowNumber": 335,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "         return _finish(500, return_dict, content_type=u'json')"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 338,
                "PatchRowcode": "+    except SolrConnectionError:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 339,
                "PatchRowcode": "+        return_dict[u'error'] = {"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 340,
                "PatchRowcode": "+            u'__type': u'Search Connection Error',"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 341,
                "PatchRowcode": "+            u'message': u'Unable to connect to the search server'}"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 342,
                "PatchRowcode": "+        return_dict[u'success'] = False"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+        return _finish(500, return_dict, content_type=u'json')"
            },
            "19": {
                "beforePatchRowNumber": 336,
                "afterPatchRowNumber": 344,
                "PatchRowcode": "     except Exception as e:"
            },
            "20": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 345,
                "PatchRowcode": "         return_dict[u'error'] = {"
            },
            "21": {
                "beforePatchRowNumber": 338,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "             u'__type': u'Internal Server Error',"
            }
        },
        "frontPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import os",
            "import logging",
            "import html",
            "import io",
            "import datetime",
            "",
            "from typing import Any, Callable, Optional, cast, Union",
            "",
            "from flask import Blueprint, make_response",
            "",
            "from werkzeug.exceptions import BadRequest",
            "from werkzeug.datastructures import MultiDict",
            "",
            "import ckan.model as model",
            "from ckan.common import json, _, g, request, current_user",
            "from ckan.lib.helpers import url_for",
            "from ckan.lib.base import render",
            "from ckan.lib.i18n import get_locales_from_config",
            "",
            "from ckan.lib.navl.dictization_functions import DataError",
            "from ckan.logic import get_action, ValidationError, NotFound, NotAuthorized",
            "from ckan.lib.search import SearchError, SearchIndexError, SearchQueryError",
            "from ckan.types import Context, Response, ActionResult",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "CONTENT_TYPES = {",
            "    u'text': u'text/plain;charset=utf-8',",
            "    u'html': u'text/html;charset=utf-8',",
            "    u'json': u'application/json;charset=utf-8',",
            "    u'javascript': u'application/javascript;charset=utf-8',",
            "}",
            "",
            "API_REST_DEFAULT_VERSION = 1",
            "",
            "API_DEFAULT_VERSION = 3",
            "API_MAX_VERSION = 3",
            "",
            "api = Blueprint(u'api', __name__, url_prefix=u'/api')",
            "",
            "",
            "def _json_serial(obj: Any):",
            "    if isinstance(obj, (datetime.datetime, datetime.date)):",
            "        return obj.isoformat()",
            "    raise TypeError(\"Unhandled Object\")",
            "",
            "",
            "def _finish(status_int: int,",
            "            response_data: Any = None,",
            "            content_type: str = u'text',",
            "            headers: Optional[dict[str, Any]] = None) -> Response:",
            "    u'''When a controller method has completed, call this method",
            "    to prepare the response.",
            "",
            "    :param status_int: The HTTP status code to return",
            "    :type status_int: int",
            "    :param response_data: The body of the response",
            "    :type response_data: object if content_type is `text` or `json`,",
            "        a string otherwise",
            "    :param content_type: One of `text`, `html` or `json`. Defaults to `text`",
            "    :type content_type: string",
            "    :param headers: Extra headers to serve with the response",
            "    :type headers: dict",
            "",
            "    :rtype: response object. Return this value from the view function",
            "        e.g. return _finish(404, 'Dataset not found')",
            "    '''",
            "    assert isinstance(status_int, int)",
            "    response_msg = u''",
            "    if headers is None:",
            "        headers = {}",
            "    if response_data is not None:",
            "        headers[u'Content-Type'] = CONTENT_TYPES[content_type]",
            "        if content_type == u'json':",
            "            response_msg = json.dumps(",
            "                response_data,",
            "                default=_json_serial,   # handle datetime objects",
            "                for_json=True)  # handle objects with for_json methods",
            "        else:",
            "            response_msg = response_data",
            "        # Support JSONP callback.",
            "        if (status_int == 200 and u'callback' in request.args and",
            "                request.method == u'GET'):",
            "            # escape callback to remove '<', '&', '>' chars",
            "            callback = html.escape(request.args[u'callback'])",
            "            response_msg = _wrap_jsonp(callback, response_msg)",
            "            headers[u'Content-Type'] = CONTENT_TYPES[u'javascript']",
            "    return make_response((response_msg, status_int, headers))",
            "",
            "",
            "def _finish_ok(response_data: Any = None,",
            "               content_type: str = u'json',",
            "               resource_location: Optional[str] = None) -> Response:",
            "    u'''If a controller method has completed successfully then",
            "    calling this method will prepare the response.",
            "",
            "    :param response_data: The body of the response",
            "    :type response_data: object if content_type is `text` or `json`,",
            "        a string otherwise",
            "    :param content_type: One of `text`, `html` or `json`. Defaults to `json`",
            "    :type content_type: string",
            "    :param resource_location: Specify this if a new resource has just been",
            "        created and you need to add a `Location` header",
            "    :type headers: string",
            "",
            "    :rtype: response object. Return this value from the view function",
            "        e.g. return _finish_ok(pkg_dict)",
            "    '''",
            "    status_int = 200",
            "    headers = None",
            "    if resource_location:",
            "        status_int = 201",
            "        try:",
            "            resource_location = str(resource_location)",
            "        except Exception as inst:",
            "            msg = \\",
            "                u\"Couldn't convert '%s' header value '%s' to string: %s\" % \\",
            "                (u'Location', resource_location, inst)",
            "            raise Exception(msg)",
            "        headers = {u'Location': resource_location}",
            "",
            "    return _finish(status_int, response_data, content_type, headers)",
            "",
            "",
            "def _finish_bad_request(extra_msg: Optional[str] = None) -> Response:",
            "    response_data = _(u'Bad request')",
            "    if extra_msg:",
            "        response_data = u'%s - %s' % (response_data, extra_msg)",
            "    return _finish(400, response_data, u'json')",
            "",
            "",
            "def _wrap_jsonp(callback: str, response_msg: str) -> str:",
            "    return u'{0}({1});'.format(callback, response_msg)",
            "",
            "",
            "def _get_request_data(try_url_params: bool = False):",
            "    u'''Returns a dictionary, extracted from a request.",
            "",
            "    If there is no data, None or \"\" is returned.",
            "    ValueError will be raised if the data is not a JSON-formatted dict.",
            "",
            "    The data is retrieved as a JSON-encoded dictionary from the request",
            "    body.  Or, if the `try_url_params` argument is True and the request is",
            "    a GET request, then an attempt is made to read the data from the url",
            "    parameters of the request.",
            "",
            "    try_url_params",
            "        If try_url_params is False, then the data_dict is read from the",
            "        request body.",
            "",
            "        If try_url_params is True and the request is a GET request then the",
            "        data is read from the url parameters.  The resulting dict will only",
            "        be 1 level deep, with the url-param fields being the keys.  If a",
            "        single key has more than one value specified, then the value will",
            "        be a list of strings, otherwise just a string.",
            "",
            "    '''",
            "    def mixed(multi_dict: \"MultiDict[str, Any]\") -> dict[str, Any]:",
            "        u'''Return a dict with values being lists if they have more than one",
            "           item or a string otherwise",
            "        '''",
            "        out: dict[str, Any] = {}",
            "        for key, value in multi_dict.to_dict(flat=False).items():",
            "            out[key] = value[0] if len(value) == 1 else value",
            "        return out",
            "",
            "    if not try_url_params and request.method == u'GET':",
            "        raise ValueError(u'Invalid request. Please use POST method '",
            "                         'for your request')",
            "",
            "    request_data: Union[dict[str, Any], Any] = {}",
            "    if request.method in [u'POST', u'PUT'] and request.form:",
            "        values = list(request.form.values())",
            "        if (len(values) == 1 and",
            "                values[0] in [u'1', u'']):",
            "            try:",
            "                keys = list(request.form.keys())",
            "                request_data = json.loads(keys[0])",
            "            except ValueError as e:",
            "                raise ValueError(",
            "                    u'Error decoding JSON data. '",
            "                    'Error: %r '",
            "                    'JSON data extracted from the request: %r' %",
            "                    (e, request_data))",
            "        else:",
            "            request_data = mixed(request.form)",
            "    elif request.args and try_url_params:",
            "        request_data = mixed(request.args)",
            "    elif (request.data and request.data != b'' and",
            "          request.content_type != u'multipart/form-data'):",
            "        try:",
            "            request_data = request.get_json()",
            "        except BadRequest as e:",
            "            raise ValueError(u'Error decoding JSON data. '",
            "                             'Error: %r '",
            "                             'JSON data extracted from the request: %r' %",
            "                             (e, request_data))",
            "    if not isinstance(request_data, dict):",
            "        raise ValueError(u'Request data JSON decoded to %r but '",
            "                         'it needs to be a dictionary.' % request_data)",
            "",
            "    if request.method == u'PUT' and not request_data:",
            "        raise ValueError(u'Invalid request. Please use the POST method for '",
            "                         'your request')",
            "    for field_name, file_ in request.files.items():",
            "        request_data[field_name] = file_",
            "    log.debug(u'Request data extracted: %r', request_data)",
            "",
            "    return request_data",
            "",
            "",
            "# View functions",
            "",
            "def action(logic_function: str, ver: int = API_DEFAULT_VERSION) -> Response:",
            "    u'''Main endpoint for the action API (v3)",
            "",
            "    Creates a dict with the incoming request data and calls the appropiate",
            "    logic function. Returns a JSON response with the following keys:",
            "",
            "        * ``help``: A URL to the docstring for the specified action",
            "        * ``success``: A boolean indicating if the request was successful or",
            "                an exception was raised",
            "        * ``result``: The output of the action, generally an Object or an Array",
            "    '''",
            "",
            "    # Check if action exists",
            "    try:",
            "        function = get_action(logic_function)",
            "    except KeyError:",
            "        msg = u'Action name not known: {0}'.format(logic_function)",
            "        log.info(msg)",
            "        return _finish_bad_request(msg)",
            "",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'api_version': ver,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    model.Session()._context = context",
            "",
            "    return_dict: dict[str, Any] = {",
            "        u'help': url_for(u'api.action',",
            "                         logic_function=u'help_show',",
            "                         ver=ver,",
            "                         name=logic_function,",
            "                         _external=True,",
            "                         )",
            "    }",
            "",
            "    # Get the request data",
            "    try:",
            "        side_effect_free = getattr(function, u'side_effect_free', False)",
            "",
            "        request_data = _get_request_data(",
            "            try_url_params=side_effect_free)",
            "    except ValueError as inst:",
            "        log.info(u'Bad Action API request data: %s', inst)",
            "        return _finish_bad_request(",
            "            _(u'JSON Error: %s') % inst)",
            "    if not isinstance(request_data, dict):",
            "        # this occurs if request_data is blank",
            "        log.info(u'Bad Action API request data - not dict: %r',",
            "                 request_data)",
            "        return _finish_bad_request(",
            "            _(u'Bad request data: %s') %",
            "            u'Request data JSON decoded to %r but '",
            "            u'it needs to be a dictionary.' % request_data)",
            "    if u'callback' in request_data:",
            "        del request_data[u'callback']",
            "        g.user = None",
            "        g.userobj = None",
            "        context[u'user'] = ''",
            "        context[u'auth_user_obj'] = None",
            "",
            "    # Call the action function, catch any exception",
            "    try:",
            "        result = function(context, request_data)",
            "        return_dict[u'success'] = True",
            "        return_dict[u'result'] = result",
            "    except DataError as e:",
            "        log.info(u'Format incorrect (Action API): %s - %s',",
            "                 e.error, request_data)",
            "        return_dict[u'error'] = {u'__type': u'Integrity Error',",
            "                                 u'message': e.error,",
            "                                 u'data': request_data}",
            "        return_dict[u'success'] = False",
            "        return _finish(400, return_dict, content_type=u'json')",
            "    except NotAuthorized as e:",
            "        return_dict[u'error'] = {u'__type': u'Authorization Error',",
            "                                 u'message': _(u'Access denied')}",
            "        return_dict[u'success'] = False",
            "",
            "        if str(e):",
            "            return_dict[u'error'][u'message'] += u': %s' % e",
            "",
            "        return _finish(403, return_dict, content_type=u'json')",
            "    except NotFound as e:",
            "        return_dict[u'error'] = {u'__type': u'Not Found Error',",
            "                                 u'message': _(u'Not found')}",
            "        if str(e):",
            "            return_dict[u'error'][u'message'] += u': %s' % e",
            "        return_dict[u'success'] = False",
            "        return _finish(404, return_dict, content_type=u'json')",
            "    except ValidationError as e:",
            "        error_dict = e.error_dict",
            "        error_dict[u'__type'] = u'Validation Error'",
            "        return_dict[u'error'] = error_dict",
            "        return_dict[u'success'] = False",
            "        # CS nasty_string ignore",
            "        log.info(u'Validation error (Action API): %r', str(e.error_dict))",
            "        return _finish(409, return_dict, content_type=u'json')",
            "    except SearchQueryError as e:",
            "        return_dict[u'error'] = {u'__type': u'Search Query Error',",
            "                                 u'message': u'Search Query is invalid: %r' %",
            "                                 e.args}",
            "        return_dict[u'success'] = False",
            "        return _finish(400, return_dict, content_type=u'json')",
            "    except SearchError as e:",
            "        return_dict[u'error'] = {u'__type': u'Search Error',",
            "                                 u'message': u'Search error: %r' % e.args}",
            "        return_dict[u'success'] = False",
            "        return _finish(409, return_dict, content_type=u'json')",
            "    except SearchIndexError as e:",
            "        return_dict[u'error'] = {",
            "            u'__type': u'Search Index Error',",
            "            u'message': u'Unable to add package to search index: %s' %",
            "                       str(e)}",
            "        return_dict[u'success'] = False",
            "        return _finish(500, return_dict, content_type=u'json')",
            "    except Exception as e:",
            "        return_dict[u'error'] = {",
            "            u'__type': u'Internal Server Error',",
            "            u'message': u'Internal Server Error'}",
            "        return_dict[u'success'] = False",
            "        log.exception(e)",
            "        return _finish(500, return_dict, content_type=u'json')",
            "",
            "    return _finish_ok(return_dict)",
            "",
            "",
            "def get_api(ver: int = 1) -> Response:",
            "    u'''Root endpoint for the API, returns the version number'''",
            "",
            "    response_data = {",
            "        u'version': ver",
            "    }",
            "    return _finish_ok(response_data)",
            "",
            "",
            "def dataset_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'incomplete', u'')",
            "    limit = request.args.get(u'limit', 10)",
            "    package_dicts: ActionResult.PackageAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "",
            "        package_dicts = get_action(",
            "            u'package_autocomplete')(context, data_dict)",
            "",
            "    result_set = {u'ResultSet': {u'Result': package_dicts}}",
            "    return _finish_ok(result_set)",
            "",
            "",
            "def tag_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'incomplete', u'')",
            "    limit = request.args.get(u'limit', 10)",
            "    vocab = request.args.get(u'vocabulary_id', u'')",
            "    tag_names: ActionResult.TagAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        if vocab != u'':",
            "            data_dict[u'vocabulary_id'] = vocab",
            "",
            "        tag_names = get_action(u'tag_autocomplete')(context, data_dict)",
            "",
            "    result_set = {",
            "        u'ResultSet': {",
            "            u'Result': [{u'Name': tag} for tag in tag_names]",
            "        }",
            "    }",
            "    return _finish_ok(result_set)",
            "",
            "",
            "def format_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'incomplete', u'')",
            "    limit = request.args.get(u'limit', 5)",
            "    formats: ActionResult.FormatAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        formats = get_action(u'format_autocomplete')(context, data_dict)",
            "",
            "    result_set = {",
            "        u'ResultSet': {",
            "            u'Result': [{u'Format': format} for format in formats]",
            "        }",
            "    }",
            "    return _finish_ok(result_set)",
            "",
            "",
            "def user_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'q', u'')",
            "    limit = request.args.get(u'limit', 20)",
            "    ignore_self = request.args.get(u'ignore_self', False)",
            "    user_list: ActionResult.UserAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "",
            "        data_dict: dict[str, Any] = {",
            "            u'q': q, u'limit': limit, u'ignore_self': ignore_self}",
            "",
            "        user_list = get_action(u'user_autocomplete')(context, data_dict)",
            "    return _finish_ok(user_list)",
            "",
            "",
            "def group_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'q', u'')",
            "    limit = request.args.get(u'limit', 20)",
            "    group_list: ActionResult.GroupAutocomplete = []",
            "",
            "    if q:",
            "        context = cast(",
            "            Context, {",
            "                u'user': current_user.name,",
            "                u'model': model}",
            "        )",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        group_list = get_action(u'group_autocomplete')(context, data_dict)",
            "    return _finish_ok(group_list)",
            "",
            "",
            "def organization_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'q', u'')",
            "    limit = request.args.get(u'limit', 20)",
            "    organization_list = []",
            "",
            "    if q:",
            "        context = cast(Context, {",
            "            u'user': current_user.name,",
            "            u'model': model})",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        organization_list = get_action(",
            "            u'organization_autocomplete')(context, data_dict)",
            "    return _finish_ok(organization_list)",
            "",
            "",
            "def snippet(snippet_path: str, ver: int = API_REST_DEFAULT_VERSION) -> str:",
            "    u'''Renders and returns a snippet used by ajax calls",
            "",
            "        We only allow snippets in templates/ajax_snippets and its subdirs",
            "    '''",
            "    snippet_path = u'ajax_snippets/' + snippet_path",
            "    # werkzeug.datastructures.ImmutableMultiDict.to_dict",
            "    # by default returns flattened dict with first occurences of each key.",
            "    # For retrieving multiple values per key, use named argument `flat`",
            "    # set to `False`",
            "    extra_vars = request.args.to_dict()",
            "    return render(snippet_path, extra_vars=extra_vars)",
            "",
            "",
            "def i18n_js_translations(",
            "        lang: str,",
            "        ver: int = API_REST_DEFAULT_VERSION) -> Union[str, Response]:",
            "",
            "    if lang not in get_locales_from_config():",
            "        return _finish_bad_request('Unknown locale: {}'.format(lang))",
            "",
            "    ckan_path = os.path.join(os.path.dirname(__file__), u'..')",
            "    source = os.path.abspath(os.path.join(ckan_path, u'public',",
            "                             u'base', u'i18n', u'{0}.js'.format(lang)))",
            "    if not os.path.exists(source):",
            "        return u'{}'",
            "    translations = json.load(io.open(source, u'r', encoding='utf-8'))",
            "    return _finish_ok(translations)",
            "",
            "",
            "# Routing",
            "",
            "# Root",
            "api.add_url_rule(u'/', view_func=get_api, strict_slashes=False)",
            "api.add_url_rule(u'/<int(min=1, max={0}):ver>'.format(API_MAX_VERSION),",
            "                 view_func=get_api, strict_slashes=False)",
            "",
            "# Action API (v3)",
            "",
            "api.add_url_rule(u'/action/<logic_function>', methods=[u'GET', u'POST'],",
            "                 view_func=action)",
            "api.add_url_rule(u'/<int(min=3, max={0}):ver>/action/<logic_function>'.format(",
            "                 API_MAX_VERSION),",
            "                 methods=[u'GET', u'POST'],",
            "                 view_func=action)",
            "",
            "",
            "# Util API",
            "",
            "util_rules: list[tuple[str, Callable[..., Union[str, Response]]]] = [",
            "    (u'/util/dataset/autocomplete', dataset_autocomplete),",
            "    (u'/util/user/autocomplete', user_autocomplete),",
            "    (u'/util/tag/autocomplete', tag_autocomplete),",
            "    (u'/util/group/autocomplete', group_autocomplete),",
            "    (u'/util/organization/autocomplete', organization_autocomplete),",
            "    (u'/util/resource/format_autocomplete', format_autocomplete),",
            "    (u'/util/snippet/<snippet_path>', snippet),",
            "    (u'/i18n/<lang>', i18n_js_translations),",
            "]",
            "",
            "version_rule = u'/<int(min=1, max=2):ver>'",
            "for rule, view_func in util_rules:",
            "    api.add_url_rule(rule, view_func=view_func)",
            "    api.add_url_rule(version_rule + rule, view_func=view_func)"
        ],
        "afterPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import os",
            "import logging",
            "import html",
            "import io",
            "import datetime",
            "",
            "from typing import Any, Callable, Optional, cast, Union",
            "",
            "from flask import Blueprint, make_response",
            "",
            "from werkzeug.exceptions import BadRequest",
            "from werkzeug.datastructures import MultiDict",
            "",
            "import ckan.model as model",
            "from ckan.common import json, _, g, request, current_user",
            "from ckan.lib.helpers import url_for",
            "from ckan.lib.base import render",
            "from ckan.lib.i18n import get_locales_from_config",
            "",
            "from ckan.lib.navl.dictization_functions import DataError",
            "from ckan.logic import get_action, ValidationError, NotFound, NotAuthorized",
            "from ckan.lib.search import (",
            "    SearchError, SearchIndexError, SearchQueryError, SolrConnectionError",
            ")",
            "from ckan.types import Context, Response, ActionResult",
            "",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "CONTENT_TYPES = {",
            "    u'text': u'text/plain;charset=utf-8',",
            "    u'html': u'text/html;charset=utf-8',",
            "    u'json': u'application/json;charset=utf-8',",
            "    u'javascript': u'application/javascript;charset=utf-8',",
            "}",
            "",
            "API_REST_DEFAULT_VERSION = 1",
            "",
            "API_DEFAULT_VERSION = 3",
            "API_MAX_VERSION = 3",
            "",
            "api = Blueprint(u'api', __name__, url_prefix=u'/api')",
            "",
            "",
            "def _json_serial(obj: Any):",
            "    if isinstance(obj, (datetime.datetime, datetime.date)):",
            "        return obj.isoformat()",
            "    raise TypeError(\"Unhandled Object\")",
            "",
            "",
            "def _finish(status_int: int,",
            "            response_data: Any = None,",
            "            content_type: str = u'text',",
            "            headers: Optional[dict[str, Any]] = None) -> Response:",
            "    u'''When a controller method has completed, call this method",
            "    to prepare the response.",
            "",
            "    :param status_int: The HTTP status code to return",
            "    :type status_int: int",
            "    :param response_data: The body of the response",
            "    :type response_data: object if content_type is `text` or `json`,",
            "        a string otherwise",
            "    :param content_type: One of `text`, `html` or `json`. Defaults to `text`",
            "    :type content_type: string",
            "    :param headers: Extra headers to serve with the response",
            "    :type headers: dict",
            "",
            "    :rtype: response object. Return this value from the view function",
            "        e.g. return _finish(404, 'Dataset not found')",
            "    '''",
            "    assert isinstance(status_int, int)",
            "    response_msg = u''",
            "    if headers is None:",
            "        headers = {}",
            "    if response_data is not None:",
            "        headers[u'Content-Type'] = CONTENT_TYPES[content_type]",
            "        if content_type == u'json':",
            "            response_msg = json.dumps(",
            "                response_data,",
            "                default=_json_serial,   # handle datetime objects",
            "                for_json=True)  # handle objects with for_json methods",
            "        else:",
            "            response_msg = response_data",
            "        # Support JSONP callback.",
            "        if (status_int == 200 and u'callback' in request.args and",
            "                request.method == u'GET'):",
            "            # escape callback to remove '<', '&', '>' chars",
            "            callback = html.escape(request.args[u'callback'])",
            "            response_msg = _wrap_jsonp(callback, response_msg)",
            "            headers[u'Content-Type'] = CONTENT_TYPES[u'javascript']",
            "    return make_response((response_msg, status_int, headers))",
            "",
            "",
            "def _finish_ok(response_data: Any = None,",
            "               content_type: str = u'json',",
            "               resource_location: Optional[str] = None) -> Response:",
            "    u'''If a controller method has completed successfully then",
            "    calling this method will prepare the response.",
            "",
            "    :param response_data: The body of the response",
            "    :type response_data: object if content_type is `text` or `json`,",
            "        a string otherwise",
            "    :param content_type: One of `text`, `html` or `json`. Defaults to `json`",
            "    :type content_type: string",
            "    :param resource_location: Specify this if a new resource has just been",
            "        created and you need to add a `Location` header",
            "    :type headers: string",
            "",
            "    :rtype: response object. Return this value from the view function",
            "        e.g. return _finish_ok(pkg_dict)",
            "    '''",
            "    status_int = 200",
            "    headers = None",
            "    if resource_location:",
            "        status_int = 201",
            "        try:",
            "            resource_location = str(resource_location)",
            "        except Exception as inst:",
            "            msg = \\",
            "                u\"Couldn't convert '%s' header value '%s' to string: %s\" % \\",
            "                (u'Location', resource_location, inst)",
            "            raise Exception(msg)",
            "        headers = {u'Location': resource_location}",
            "",
            "    return _finish(status_int, response_data, content_type, headers)",
            "",
            "",
            "def _finish_bad_request(extra_msg: Optional[str] = None) -> Response:",
            "    response_data = _(u'Bad request')",
            "    if extra_msg:",
            "        response_data = u'%s - %s' % (response_data, extra_msg)",
            "    return _finish(400, response_data, u'json')",
            "",
            "",
            "def _wrap_jsonp(callback: str, response_msg: str) -> str:",
            "    return u'{0}({1});'.format(callback, response_msg)",
            "",
            "",
            "def _get_request_data(try_url_params: bool = False):",
            "    u'''Returns a dictionary, extracted from a request.",
            "",
            "    If there is no data, None or \"\" is returned.",
            "    ValueError will be raised if the data is not a JSON-formatted dict.",
            "",
            "    The data is retrieved as a JSON-encoded dictionary from the request",
            "    body.  Or, if the `try_url_params` argument is True and the request is",
            "    a GET request, then an attempt is made to read the data from the url",
            "    parameters of the request.",
            "",
            "    try_url_params",
            "        If try_url_params is False, then the data_dict is read from the",
            "        request body.",
            "",
            "        If try_url_params is True and the request is a GET request then the",
            "        data is read from the url parameters.  The resulting dict will only",
            "        be 1 level deep, with the url-param fields being the keys.  If a",
            "        single key has more than one value specified, then the value will",
            "        be a list of strings, otherwise just a string.",
            "",
            "    '''",
            "    def mixed(multi_dict: \"MultiDict[str, Any]\") -> dict[str, Any]:",
            "        u'''Return a dict with values being lists if they have more than one",
            "           item or a string otherwise",
            "        '''",
            "        out: dict[str, Any] = {}",
            "        for key, value in multi_dict.to_dict(flat=False).items():",
            "            out[key] = value[0] if len(value) == 1 else value",
            "        return out",
            "",
            "    if not try_url_params and request.method == u'GET':",
            "        raise ValueError(u'Invalid request. Please use POST method '",
            "                         'for your request')",
            "",
            "    request_data: Union[dict[str, Any], Any] = {}",
            "    if request.method in [u'POST', u'PUT'] and request.form:",
            "        values = list(request.form.values())",
            "        if (len(values) == 1 and",
            "                values[0] in [u'1', u'']):",
            "            try:",
            "                keys = list(request.form.keys())",
            "                request_data = json.loads(keys[0])",
            "            except ValueError as e:",
            "                raise ValueError(",
            "                    u'Error decoding JSON data. '",
            "                    'Error: %r '",
            "                    'JSON data extracted from the request: %r' %",
            "                    (e, request_data))",
            "        else:",
            "            request_data = mixed(request.form)",
            "    elif request.args and try_url_params:",
            "        request_data = mixed(request.args)",
            "    elif (request.data and request.data != b'' and",
            "          request.content_type != u'multipart/form-data'):",
            "        try:",
            "            request_data = request.get_json()",
            "        except BadRequest as e:",
            "            raise ValueError(u'Error decoding JSON data. '",
            "                             'Error: %r '",
            "                             'JSON data extracted from the request: %r' %",
            "                             (e, request_data))",
            "    if not isinstance(request_data, dict):",
            "        raise ValueError(u'Request data JSON decoded to %r but '",
            "                         'it needs to be a dictionary.' % request_data)",
            "",
            "    if request.method == u'PUT' and not request_data:",
            "        raise ValueError(u'Invalid request. Please use the POST method for '",
            "                         'your request')",
            "    for field_name, file_ in request.files.items():",
            "        request_data[field_name] = file_",
            "    log.debug(u'Request data extracted: %r', request_data)",
            "",
            "    return request_data",
            "",
            "",
            "# View functions",
            "",
            "def action(logic_function: str, ver: int = API_DEFAULT_VERSION) -> Response:",
            "    u'''Main endpoint for the action API (v3)",
            "",
            "    Creates a dict with the incoming request data and calls the appropiate",
            "    logic function. Returns a JSON response with the following keys:",
            "",
            "        * ``help``: A URL to the docstring for the specified action",
            "        * ``success``: A boolean indicating if the request was successful or",
            "                an exception was raised",
            "        * ``result``: The output of the action, generally an Object or an Array",
            "    '''",
            "",
            "    # Check if action exists",
            "    try:",
            "        function = get_action(logic_function)",
            "    except KeyError:",
            "        msg = u'Action name not known: {0}'.format(logic_function)",
            "        log.info(msg)",
            "        return _finish_bad_request(msg)",
            "",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'api_version': ver,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    model.Session()._context = context",
            "",
            "    return_dict: dict[str, Any] = {",
            "        u'help': url_for(u'api.action',",
            "                         logic_function=u'help_show',",
            "                         ver=ver,",
            "                         name=logic_function,",
            "                         _external=True,",
            "                         )",
            "    }",
            "",
            "    # Get the request data",
            "    try:",
            "        side_effect_free = getattr(function, u'side_effect_free', False)",
            "",
            "        request_data = _get_request_data(",
            "            try_url_params=side_effect_free)",
            "    except ValueError as inst:",
            "        log.info(u'Bad Action API request data: %s', inst)",
            "        return _finish_bad_request(",
            "            _(u'JSON Error: %s') % inst)",
            "    if not isinstance(request_data, dict):",
            "        # this occurs if request_data is blank",
            "        log.info(u'Bad Action API request data - not dict: %r',",
            "                 request_data)",
            "        return _finish_bad_request(",
            "            _(u'Bad request data: %s') %",
            "            u'Request data JSON decoded to %r but '",
            "            u'it needs to be a dictionary.' % request_data)",
            "    if u'callback' in request_data:",
            "        del request_data[u'callback']",
            "        g.user = None",
            "        g.userobj = None",
            "        context[u'user'] = ''",
            "        context[u'auth_user_obj'] = None",
            "",
            "    # Call the action function, catch any exception",
            "    try:",
            "        result = function(context, request_data)",
            "        return_dict[u'success'] = True",
            "        return_dict[u'result'] = result",
            "    except DataError as e:",
            "        log.info(u'Format incorrect (Action API): %s - %s',",
            "                 e.error, request_data)",
            "        return_dict[u'error'] = {u'__type': u'Integrity Error',",
            "                                 u'message': e.error,",
            "                                 u'data': request_data}",
            "        return_dict[u'success'] = False",
            "        return _finish(400, return_dict, content_type=u'json')",
            "    except NotAuthorized as e:",
            "        return_dict[u'error'] = {u'__type': u'Authorization Error',",
            "                                 u'message': _(u'Access denied')}",
            "        return_dict[u'success'] = False",
            "",
            "        if str(e):",
            "            return_dict[u'error'][u'message'] += u': %s' % e",
            "",
            "        return _finish(403, return_dict, content_type=u'json')",
            "    except NotFound as e:",
            "        return_dict[u'error'] = {u'__type': u'Not Found Error',",
            "                                 u'message': _(u'Not found')}",
            "        if str(e):",
            "            return_dict[u'error'][u'message'] += u': %s' % e",
            "        return_dict[u'success'] = False",
            "        return _finish(404, return_dict, content_type=u'json')",
            "    except ValidationError as e:",
            "        error_dict = e.error_dict",
            "        error_dict[u'__type'] = u'Validation Error'",
            "        return_dict[u'error'] = error_dict",
            "        return_dict[u'success'] = False",
            "        # CS nasty_string ignore",
            "        log.info(u'Validation error (Action API): %r', str(e.error_dict))",
            "        return _finish(409, return_dict, content_type=u'json')",
            "    except SearchQueryError as e:",
            "        return_dict[u'error'] = {u'__type': u'Search Query Error',",
            "                                 u'message': u'Search Query is invalid: %r' %",
            "                                 e.args}",
            "        return_dict[u'success'] = False",
            "        return _finish(400, return_dict, content_type=u'json')",
            "    except SearchError as e:",
            "        return_dict[u'error'] = {u'__type': u'Search Error',",
            "                                 u'message': u'Search error: %r' % e.args}",
            "        return_dict[u'success'] = False",
            "        return _finish(409, return_dict, content_type=u'json')",
            "    except SearchIndexError as e:",
            "        return_dict[u'error'] = {",
            "            u'__type': u'Search Index Error',",
            "            u'message': u'Unable to add package to search index: %s' %",
            "                       str(e)}",
            "        return_dict[u'success'] = False",
            "        return _finish(500, return_dict, content_type=u'json')",
            "    except SolrConnectionError:",
            "        return_dict[u'error'] = {",
            "            u'__type': u'Search Connection Error',",
            "            u'message': u'Unable to connect to the search server'}",
            "        return_dict[u'success'] = False",
            "        return _finish(500, return_dict, content_type=u'json')",
            "    except Exception as e:",
            "        return_dict[u'error'] = {",
            "            u'__type': u'Internal Server Error',",
            "            u'message': u'Internal Server Error'}",
            "        return_dict[u'success'] = False",
            "        log.exception(e)",
            "        return _finish(500, return_dict, content_type=u'json')",
            "",
            "    return _finish_ok(return_dict)",
            "",
            "",
            "def get_api(ver: int = 1) -> Response:",
            "    u'''Root endpoint for the API, returns the version number'''",
            "",
            "    response_data = {",
            "        u'version': ver",
            "    }",
            "    return _finish_ok(response_data)",
            "",
            "",
            "def dataset_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'incomplete', u'')",
            "    limit = request.args.get(u'limit', 10)",
            "    package_dicts: ActionResult.PackageAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "",
            "        package_dicts = get_action(",
            "            u'package_autocomplete')(context, data_dict)",
            "",
            "    result_set = {u'ResultSet': {u'Result': package_dicts}}",
            "    return _finish_ok(result_set)",
            "",
            "",
            "def tag_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'incomplete', u'')",
            "    limit = request.args.get(u'limit', 10)",
            "    vocab = request.args.get(u'vocabulary_id', u'')",
            "    tag_names: ActionResult.TagAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        if vocab != u'':",
            "            data_dict[u'vocabulary_id'] = vocab",
            "",
            "        tag_names = get_action(u'tag_autocomplete')(context, data_dict)",
            "",
            "    result_set = {",
            "        u'ResultSet': {",
            "            u'Result': [{u'Name': tag} for tag in tag_names]",
            "        }",
            "    }",
            "    return _finish_ok(result_set)",
            "",
            "",
            "def format_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'incomplete', u'')",
            "    limit = request.args.get(u'limit', 5)",
            "    formats: ActionResult.FormatAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        formats = get_action(u'format_autocomplete')(context, data_dict)",
            "",
            "    result_set = {",
            "        u'ResultSet': {",
            "            u'Result': [{u'Format': format} for format in formats]",
            "        }",
            "    }",
            "    return _finish_ok(result_set)",
            "",
            "",
            "def user_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'q', u'')",
            "    limit = request.args.get(u'limit', 20)",
            "    ignore_self = request.args.get(u'ignore_self', False)",
            "    user_list: ActionResult.UserAutocomplete = []",
            "    if q:",
            "        context = cast(",
            "            Context,",
            "            {u'model': model,",
            "             u'session': model.Session,",
            "             u'user': current_user.name,",
            "             u'auth_user_obj': current_user})",
            "",
            "        data_dict: dict[str, Any] = {",
            "            u'q': q, u'limit': limit, u'ignore_self': ignore_self}",
            "",
            "        user_list = get_action(u'user_autocomplete')(context, data_dict)",
            "    return _finish_ok(user_list)",
            "",
            "",
            "def group_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'q', u'')",
            "    limit = request.args.get(u'limit', 20)",
            "    group_list: ActionResult.GroupAutocomplete = []",
            "",
            "    if q:",
            "        context = cast(",
            "            Context, {",
            "                u'user': current_user.name,",
            "                u'model': model}",
            "        )",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        group_list = get_action(u'group_autocomplete')(context, data_dict)",
            "    return _finish_ok(group_list)",
            "",
            "",
            "def organization_autocomplete(ver: int = API_REST_DEFAULT_VERSION) -> Response:",
            "    q = request.args.get(u'q', u'')",
            "    limit = request.args.get(u'limit', 20)",
            "    organization_list = []",
            "",
            "    if q:",
            "        context = cast(Context, {",
            "            u'user': current_user.name,",
            "            u'model': model})",
            "        data_dict: dict[str, Any] = {u'q': q, u'limit': limit}",
            "        organization_list = get_action(",
            "            u'organization_autocomplete')(context, data_dict)",
            "    return _finish_ok(organization_list)",
            "",
            "",
            "def snippet(snippet_path: str, ver: int = API_REST_DEFAULT_VERSION) -> str:",
            "    u'''Renders and returns a snippet used by ajax calls",
            "",
            "        We only allow snippets in templates/ajax_snippets and its subdirs",
            "    '''",
            "    snippet_path = u'ajax_snippets/' + snippet_path",
            "    # werkzeug.datastructures.ImmutableMultiDict.to_dict",
            "    # by default returns flattened dict with first occurences of each key.",
            "    # For retrieving multiple values per key, use named argument `flat`",
            "    # set to `False`",
            "    extra_vars = request.args.to_dict()",
            "    return render(snippet_path, extra_vars=extra_vars)",
            "",
            "",
            "def i18n_js_translations(",
            "        lang: str,",
            "        ver: int = API_REST_DEFAULT_VERSION) -> Union[str, Response]:",
            "",
            "    if lang not in get_locales_from_config():",
            "        return _finish_bad_request('Unknown locale: {}'.format(lang))",
            "",
            "    ckan_path = os.path.join(os.path.dirname(__file__), u'..')",
            "    source = os.path.abspath(os.path.join(ckan_path, u'public',",
            "                             u'base', u'i18n', u'{0}.js'.format(lang)))",
            "    if not os.path.exists(source):",
            "        return u'{}'",
            "    translations = json.load(io.open(source, u'r', encoding='utf-8'))",
            "    return _finish_ok(translations)",
            "",
            "",
            "# Routing",
            "",
            "# Root",
            "api.add_url_rule(u'/', view_func=get_api, strict_slashes=False)",
            "api.add_url_rule(u'/<int(min=1, max={0}):ver>'.format(API_MAX_VERSION),",
            "                 view_func=get_api, strict_slashes=False)",
            "",
            "# Action API (v3)",
            "",
            "api.add_url_rule(u'/action/<logic_function>', methods=[u'GET', u'POST'],",
            "                 view_func=action)",
            "api.add_url_rule(u'/<int(min=3, max={0}):ver>/action/<logic_function>'.format(",
            "                 API_MAX_VERSION),",
            "                 methods=[u'GET', u'POST'],",
            "                 view_func=action)",
            "",
            "",
            "# Util API",
            "",
            "util_rules: list[tuple[str, Callable[..., Union[str, Response]]]] = [",
            "    (u'/util/dataset/autocomplete', dataset_autocomplete),",
            "    (u'/util/user/autocomplete', user_autocomplete),",
            "    (u'/util/tag/autocomplete', tag_autocomplete),",
            "    (u'/util/group/autocomplete', group_autocomplete),",
            "    (u'/util/organization/autocomplete', organization_autocomplete),",
            "    (u'/util/resource/format_autocomplete', format_autocomplete),",
            "    (u'/util/snippet/<snippet_path>', snippet),",
            "    (u'/i18n/<lang>', i18n_js_translations),",
            "]",
            "",
            "version_rule = u'/<int(min=1, max=2):ver>'",
            "for rule, view_func in util_rules:",
            "    api.add_url_rule(rule, view_func=view_func)",
            "    api.add_url_rule(version_rule + rule, view_func=view_func)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "25": []
        },
        "addLocation": []
    },
    "ckan/views/dataset.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " from ckan.common import _, config, g, request"
            },
            "1": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " from ckan.views.home import CACHE_PARAMETERS"
            },
            "2": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from ckan.lib.plugins import lookup_package_plugin"
            },
            "3": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from ckan.lib.search import SearchError, SearchQueryError, SearchIndexError"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+from ckan.lib.search import ("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+    SearchError, SearchQueryError, SearchIndexError, SolrConnectionError"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+)"
            },
            "7": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from ckan.types import Context, Response"
            },
            "8": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": 350,
                "PatchRowcode": "             _(u'Invalid search query: {error_message}')"
            },
            "11": {
                "beforePatchRowNumber": 349,
                "afterPatchRowNumber": 351,
                "PatchRowcode": "             .format(error_message=str(se))"
            },
            "12": {
                "beforePatchRowNumber": 350,
                "afterPatchRowNumber": 352,
                "PatchRowcode": "         )"
            },
            "13": {
                "beforePatchRowNumber": 351,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    except SearchError as se:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 353,
                "PatchRowcode": "+    except (SearchError, SolrConnectionError) as se:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 354,
                "PatchRowcode": "+        if isinstance(se, SolrConnectionError):"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 355,
                "PatchRowcode": "+            base.abort(500, se.args[0])"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 356,
                "PatchRowcode": "+"
            },
            "18": {
                "beforePatchRowNumber": 352,
                "afterPatchRowNumber": 357,
                "PatchRowcode": "         # May be bad input from the user, but may also be more serious like"
            },
            "19": {
                "beforePatchRowNumber": 353,
                "afterPatchRowNumber": 358,
                "PatchRowcode": "         # bad code causing a SOLR syntax error, or a problem connecting to"
            },
            "20": {
                "beforePatchRowNumber": 354,
                "afterPatchRowNumber": 359,
                "PatchRowcode": "         # SOLR"
            }
        },
        "frontPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import logging",
            "import inspect",
            "from collections import OrderedDict",
            "from functools import partial",
            "from typing_extensions import TypeAlias",
            "from urllib.parse import urlencode",
            "from typing import Any, Iterable, Optional, Union, cast",
            "",
            "from flask import Blueprint",
            "from flask.views import MethodView",
            "from jinja2.exceptions import TemplateNotFound",
            "from werkzeug.datastructures import MultiDict",
            "from ckan.common import asbool, current_user",
            "",
            "import ckan.lib.base as base",
            "from ckan.lib.helpers import helper_functions as h",
            "from ckan.lib.helpers import Page",
            "import ckan.lib.navl.dictization_functions as dict_fns",
            "import ckan.logic as logic",
            "import ckan.model as model",
            "import ckan.plugins as plugins",
            "import ckan.authz as authz",
            "from ckan.common import _, config, g, request",
            "from ckan.views.home import CACHE_PARAMETERS",
            "from ckan.lib.plugins import lookup_package_plugin",
            "from ckan.lib.search import SearchError, SearchQueryError, SearchIndexError",
            "from ckan.types import Context, Response",
            "",
            "",
            "NotFound = logic.NotFound",
            "NotAuthorized = logic.NotAuthorized",
            "ValidationError = logic.ValidationError",
            "check_access = logic.check_access",
            "get_action = logic.get_action",
            "tuplize_dict = logic.tuplize_dict",
            "clean_dict = logic.clean_dict",
            "parse_params = logic.parse_params",
            "flatten_to_string_key = logic.flatten_to_string_key",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "dataset = Blueprint(",
            "    u'dataset',",
            "    __name__,",
            "    url_prefix=u'/dataset',",
            "    url_defaults={u'package_type': u'dataset'}",
            ")",
            "",
            "",
            "def _setup_template_variables(context: Context,",
            "                              data_dict: dict[str, Any],",
            "                              package_type: Optional[str] = None) -> None:",
            "    return lookup_package_plugin(package_type).setup_template_variables(",
            "        context, data_dict",
            "    )",
            "",
            "",
            "def _get_pkg_template(template_type: str,",
            "                      package_type: Optional[str] = None) -> str:",
            "    pkg_plugin = lookup_package_plugin(package_type)",
            "    method = getattr(pkg_plugin, template_type)",
            "    signature = inspect.signature(method)",
            "    if len(signature.parameters):",
            "        return method(package_type)",
            "    else:",
            "        return method()",
            "",
            "",
            "def _encode_params(params: Iterable[tuple[str, Any]]):",
            "    return [(k, v.encode(u'utf-8') if isinstance(v, str) else str(v))",
            "            for k, v in params]",
            "",
            "",
            "Params: TypeAlias = \"list[tuple[str, Any]]\"",
            "",
            "",
            "def url_with_params(url: str, params: Params) -> str:",
            "    params = _encode_params(params)",
            "    return url + u'?' + urlencode(params)",
            "",
            "",
            "def search_url(params: Params, package_type: Optional[str] = None) -> str:",
            "    if not package_type:",
            "        package_type = u'dataset'",
            "    url = h.url_for(u'{0}.search'.format(package_type))",
            "    return url_with_params(url, params)",
            "",
            "",
            "def remove_field(package_type: Optional[str],",
            "                 key: str,",
            "                 value: Optional[str] = None,",
            "                 replace: Optional[str] = None):",
            "    if not package_type:",
            "        package_type = u'dataset'",
            "    url = h.url_for(u'{0}.search'.format(package_type))",
            "    return h.remove_url_param(",
            "        key,",
            "        value=value,",
            "        replace=replace,",
            "        alternative_url=url",
            "    )",
            "",
            "",
            "def _sort_by(params_nosort: Params, package_type: str,",
            "             fields: Iterable[tuple[str, str]]) -> str:",
            "    \"\"\"Sort by the given list of fields.",
            "",
            "    Each entry in the list is a 2-tuple: (fieldname, sort_order)",
            "    eg - [(u'metadata_modified', u'desc'), (u'name', u'asc')]",
            "    If fields is empty, then the default ordering is used.",
            "    \"\"\"",
            "    params = params_nosort[:]",
            "",
            "    if fields:",
            "        sort_string = u', '.join(u'%s %s' % f for f in fields)",
            "        params.append((u'sort', sort_string))",
            "    return search_url(params, package_type)",
            "",
            "",
            "def _pager_url(params_nopage: Params,",
            "               package_type: str,",
            "               q: Any = None,  # noqa",
            "               page: Optional[int] = None) -> str:",
            "    params = list(params_nopage)",
            "    params.append((u'page', page))",
            "    return search_url(params, package_type)",
            "",
            "",
            "def _tag_string_to_list(tag_string: str) -> list[dict[str, str]]:",
            "    \"\"\"This is used to change tags from a sting to a list of dicts.",
            "    \"\"\"",
            "    out: list[dict[str, str]] = []",
            "    for tag in tag_string.split(u','):",
            "        tag = tag.strip()",
            "        if tag:",
            "            out.append({u'name': tag, u'state': u'active'})",
            "    return out",
            "",
            "",
            "def _form_save_redirect(pkg_name: str,",
            "                        action: str,",
            "                        package_type: Optional[str] = None) -> Response:",
            "    \"\"\"This redirects the user to the CKAN package/read page,",
            "    unless there is request parameter giving an alternate location,",
            "    perhaps an external website.",
            "    @param pkg_name - Name of the package just edited",
            "    @param action - What the action of the edit was",
            "    \"\"\"",
            "    assert action in (u'new', u'edit')",
            "    url = request.args.get(u'return_to') or config.get(",
            "        u'package_%s_return_url' % action",
            "    )",
            "    if url:",
            "        url = url.replace(u'<NAME>', pkg_name)",
            "    else:",
            "        if not package_type:",
            "            package_type = u'dataset'",
            "        url = h.url_for(u'{0}.read'.format(package_type), id=pkg_name)",
            "    return h.redirect_to(url)",
            "",
            "",
            "def _get_package_type(id: str) -> str:",
            "    \"\"\"",
            "    Given the id of a package this method will return the type of the",
            "    package, or 'dataset' if no type is currently set",
            "    \"\"\"",
            "    pkg = model.Package.get(id)",
            "    if pkg:",
            "        return pkg.type or u'dataset'",
            "    return u'dataset'",
            "",
            "",
            "def _get_search_details() -> dict[str, Any]:",
            "    fq = u''",
            "",
            "    # fields_grouped will contain a dict of params containing",
            "    # a list of values eg {u'tags':[u'tag1', u'tag2']}",
            "",
            "    fields = []",
            "    fields_grouped = {}",
            "    search_extras: 'MultiDict[str, Any]' = MultiDict()",
            "",
            "    for (param, value) in request.args.items(multi=True):",
            "        if param not in [u'q', u'page', u'sort'] \\",
            "                and len(value) and not param.startswith(u'_'):",
            "            if not param.startswith(u'ext_'):",
            "                fields.append((param, value))",
            "                fq += u' %s:\"%s\"' % (param, value)",
            "                if param not in fields_grouped:",
            "                    fields_grouped[param] = [value]",
            "                else:",
            "                    fields_grouped[param].append(value)",
            "            else:",
            "                search_extras.update({param: value})",
            "",
            "    extras = dict([",
            "        (k, v[0]) if len(v) == 1 else (k, v)",
            "        for k, v in search_extras.lists()",
            "    ])",
            "    return {",
            "        u'fields': fields,",
            "        u'fields_grouped': fields_grouped,",
            "        u'fq': fq,",
            "        u'search_extras': extras,",
            "    }",
            "",
            "",
            "def search(package_type: str) -> str:",
            "    extra_vars: dict[str, Any] = {}",
            "",
            "    try:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user",
            "        })",
            "        check_access(u'site_read', context)",
            "    except NotAuthorized:",
            "        base.abort(403, _(u'Not authorized to see this page'))",
            "",
            "    # unicode format (decoded from utf8)",
            "    extra_vars[u'q'] = q = request.args.get(u'q', u'')",
            "",
            "    extra_vars['query_error'] = False",
            "    page = h.get_page_number(request.args)",
            "",
            "    limit = config.get(u'ckan.datasets_per_page')",
            "",
            "    # most search operations should reset the page counter:",
            "    params_nopage = [(k, v) for k, v in request.args.items(multi=True)",
            "                     if k != u'page']",
            "",
            "    extra_vars[u'remove_field'] = partial(remove_field, package_type)",
            "",
            "    sort_by = request.args.get(u'sort', None)",
            "    params_nosort = [(k, v) for k, v in params_nopage if k != u'sort']",
            "",
            "    extra_vars[u'sort_by'] = partial(_sort_by, params_nosort, package_type)",
            "",
            "    if not sort_by:",
            "        sort_by_fields = []",
            "    else:",
            "        sort_by_fields = [field.split()[0] for field in sort_by.split(u',')]",
            "    extra_vars[u'sort_by_fields'] = sort_by_fields",
            "",
            "    pager_url = partial(_pager_url, params_nopage, package_type)",
            "",
            "    details = _get_search_details()",
            "    extra_vars[u'fields'] = details[u'fields']",
            "    extra_vars[u'fields_grouped'] = details[u'fields_grouped']",
            "    fq = details[u'fq']",
            "    search_extras = details[u'search_extras']",
            "",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "",
            "    # Unless changed via config options, don't show other dataset",
            "    # types any search page. Potential alternatives are do show them",
            "    # on the default search page (dataset) or on one other search page",
            "    search_all_type = config.get(u'ckan.search.show_all_types')",
            "    search_all = False",
            "",
            "    try:",
            "        # If the \"type\" is set to True or False, convert to bool",
            "        # and we know that no type was specified, so use traditional",
            "        # behaviour of applying this only to dataset type",
            "        search_all = asbool(search_all_type)",
            "        search_all_type = u'dataset'",
            "    # Otherwise we treat as a string representing a type",
            "    except ValueError:",
            "        search_all = True",
            "",
            "    if not search_all or package_type != search_all_type:",
            "        # Only show datasets of this particular type",
            "        fq += u' +dataset_type:{type}'.format(type=package_type)",
            "",
            "    facets: dict[str, str] = OrderedDict()",
            "",
            "    org_label = h.humanize_entity_type(",
            "        u'organization',",
            "        h.default_group_type(u'organization'),",
            "        u'facet label') or _(u'Organizations')",
            "",
            "    group_label = h.humanize_entity_type(",
            "        u'group',",
            "        h.default_group_type(u'group'),",
            "        u'facet label') or _(u'Groups')",
            "",
            "    default_facet_titles = {",
            "        u'organization': org_label,",
            "        u'groups': group_label,",
            "        u'tags': _(u'Tags'),",
            "        u'res_format': _(u'Formats'),",
            "        u'license_id': _(u'Licenses'),",
            "    }",
            "",
            "    for facet in h.facets():",
            "        if facet in default_facet_titles:",
            "            facets[facet] = default_facet_titles[facet]",
            "        else:",
            "            facets[facet] = facet",
            "",
            "    # Facet titles",
            "    for plugin in plugins.PluginImplementations(plugins.IFacets):",
            "        facets = plugin.dataset_facets(facets, package_type)",
            "",
            "    extra_vars[u'facet_titles'] = facets",
            "    data_dict: dict[str, Any] = {",
            "        u'q': q,",
            "        u'fq': fq.strip(),",
            "        u'facet.field': list(facets.keys()),",
            "        u'rows': limit,",
            "        u'start': (page - 1) * limit,",
            "        u'sort': sort_by,",
            "        u'extras': search_extras,",
            "        u'include_private': config.get(",
            "            u'ckan.search.default_include_private'),",
            "    }",
            "    try:",
            "        query = get_action(u'package_search')(context, data_dict)",
            "",
            "        extra_vars[u'sort_by_selected'] = query[u'sort']",
            "",
            "        extra_vars[u'page'] = Page(",
            "            collection=query[u'results'],",
            "            page=page,",
            "            url=pager_url,",
            "            item_count=query[u'count'],",
            "            items_per_page=limit",
            "        )",
            "        extra_vars[u'search_facets'] = query[u'search_facets']",
            "        extra_vars[u'page'].items = query[u'results']",
            "    except SearchQueryError as se:",
            "        # User's search parameters are invalid, in such a way that is not",
            "        # achievable with the web interface, so return a proper error to",
            "        # discourage spiders which are the main cause of this.",
            "        log.info(u'Dataset search query rejected: %r', se.args)",
            "        base.abort(",
            "            400,",
            "            _(u'Invalid search query: {error_message}')",
            "            .format(error_message=str(se))",
            "        )",
            "    except SearchError as se:",
            "        # May be bad input from the user, but may also be more serious like",
            "        # bad code causing a SOLR syntax error, or a problem connecting to",
            "        # SOLR",
            "        log.error(u'Dataset search error: %r', se.args)",
            "        extra_vars[u'query_error'] = True",
            "        extra_vars[u'search_facets'] = {}",
            "        extra_vars[u'page'] = Page(collection=[])",
            "",
            "    # FIXME: try to avoid using global variables",
            "    g.search_facets_limits = {}",
            "    default_limit: int = config.get(u'search.facets.default')",
            "    for facet in cast(Iterable[str], extra_vars[u'search_facets'].keys()):",
            "        try:",
            "            limit = int(",
            "                request.args.get(",
            "                    u'_%s_limit' % facet,",
            "                    default_limit",
            "                )",
            "            )",
            "        except ValueError:",
            "            base.abort(",
            "                400,",
            "                _(u'Parameter u\"{parameter_name}\" is not '",
            "                  u'an integer').format(parameter_name=u'_%s_limit' % facet)",
            "            )",
            "",
            "        g.search_facets_limits[facet] = limit",
            "",
            "    _setup_template_variables(context, {}, package_type=package_type)",
            "",
            "    extra_vars[u'dataset_type'] = package_type",
            "",
            "    # TODO: remove",
            "    for key, value in extra_vars.items():",
            "        setattr(g, key, value)",
            "",
            "    return base.render(",
            "        _get_pkg_template(u'search_template', package_type), extra_vars",
            "    )",
            "",
            "",
            "def resources(package_type: str, id: str) -> Union[Response, str]:",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict: dict[str, Any] = {u'id': id, u'include_tracking': True}",
            "",
            "    try:",
            "        check_access(u'package_update', context, data_dict)",
            "    except NotFound:",
            "        return base.abort(404, _(u'Dataset not found'))",
            "    except NotAuthorized:",
            "        return base.abort(",
            "            403,",
            "            _(u'User %r not authorized to edit %s') % (current_user.name, id)",
            "        )",
            "    # check if package exists",
            "    try:",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        pkg = context[u'package']",
            "    except (NotFound, NotAuthorized):",
            "        return base.abort(404, _(u'Dataset not found'))",
            "",
            "    package_type = pkg_dict[u'type'] or u'dataset'",
            "    _setup_template_variables(context, {u'id': id}, package_type=package_type)",
            "",
            "    # TODO: remove",
            "    g.pkg_dict = pkg_dict",
            "    g.pkg = pkg",
            "",
            "    return base.render(",
            "        u'package/resources.html', {",
            "            u'dataset_type': package_type,",
            "            u'pkg_dict': pkg_dict,",
            "            u'pkg': pkg",
            "        }",
            "    )",
            "",
            "",
            "def read(package_type: str, id: str) -> Union[Response, str]:",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict = {u'id': id, u'include_tracking': True}",
            "",
            "    # check if package exists",
            "    try:",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        pkg = context[u'package']",
            "    except NotFound:",
            "        return base.abort(",
            "            404,",
            "            _(u'Dataset not found or you have no permission to view it')",
            "        )",
            "    except NotAuthorized:",
            "        if config.get(u'ckan.auth.reveal_private_datasets'):",
            "            if current_user.is_authenticated:",
            "                return base.abort(",
            "                    403, _(u'Unauthorized to read package %s') % id)",
            "            else:",
            "                return h.redirect_to(",
            "                    \"user.login\",",
            "                    came_from=h.url_for('{}.read'.format(package_type), id=id)",
            "                )",
            "        return base.abort(",
            "            404,",
            "            _(u'Dataset not found or you have no permission to view it')",
            "        )",
            "",
            "    g.pkg_dict = pkg_dict",
            "    g.pkg = pkg",
            "",
            "    if plugins.plugin_loaded(\"activity\"):",
            "        activity_id = request.args.get(\"activity_id\")",
            "        if activity_id:",
            "            return h.redirect_to(",
            "                \"activity.package_history\",",
            "                id=id, activity_id=activity_id",
            "            )",
            "",
            "    # if the user specified a package id, redirect to the package name",
            "    if data_dict['id'] == pkg_dict['id'] and \\",
            "            data_dict['id'] != pkg_dict['name']:",
            "        return h.redirect_to(u'{}.read'.format(package_type),",
            "                             id=pkg_dict['name'])",
            "",
            "    # can the resources be previewed?",
            "    for resource in pkg_dict[u'resources']:",
            "        resource_views = get_action(u'resource_view_list')(",
            "            context, {",
            "                u'id': resource[u'id']",
            "            }",
            "        )",
            "        resource[u'has_views'] = len(resource_views) > 0",
            "",
            "    package_type = pkg_dict[u'type'] or package_type",
            "    _setup_template_variables(context, {u'id': id}, package_type=package_type)",
            "",
            "    template = _get_pkg_template(u'read_template', package_type)",
            "    try:",
            "        return base.render(",
            "            template, {",
            "                u'dataset_type': package_type,",
            "                u'pkg_dict': pkg_dict,",
            "                u'pkg': pkg,",
            "            }",
            "        )",
            "    except TemplateNotFound as e:",
            "        msg = _(",
            "            u\"Viewing datasets of type \\\"{package_type}\\\" is \"",
            "            u\"not supported ({file_!r}).\".format(",
            "                package_type=package_type, file_=e.message",
            "            )",
            "        )",
            "        return base.abort(404, msg)",
            "",
            "",
            "class CreateView(MethodView):",
            "    def _is_save(self) -> bool:",
            "        return u'save' in request.form",
            "",
            "    def _prepare(self) -> Context:  # noqa",
            "",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user,",
            "            u'save': self._is_save()",
            "        })",
            "        try:",
            "            check_access(u'package_create', context)",
            "        except NotAuthorized:",
            "            return base.abort(403, _(u'Unauthorized to create a package'))",
            "        return context",
            "",
            "    def post(self, package_type: str) -> Union[Response, str]:",
            "        # The staged add dataset used the new functionality when the dataset is",
            "        # partially created so we need to know if we actually are updating or",
            "        # this is a real new.",
            "        context = self._prepare()",
            "        is_an_update = False",
            "        ckan_phase = request.form.get(u'_ckan_phase')",
            "        try:",
            "            data_dict = clean_dict(",
            "                dict_fns.unflatten(tuplize_dict(parse_params(request.form)))",
            "            )",
            "        except dict_fns.DataError:",
            "            return base.abort(400, _(u'Integrity Error'))",
            "        try:",
            "            if ckan_phase:",
            "                # prevent clearing of groups etc",
            "                context[u'allow_partial_update'] = True",
            "                # sort the tags",
            "                if u'tag_string' in data_dict:",
            "                    data_dict[u'tags'] = _tag_string_to_list(",
            "                        data_dict[u'tag_string']",
            "                    )",
            "                if data_dict.get(u'pkg_name'):",
            "                    is_an_update = True",
            "                    # This is actually an update not a save",
            "                    data_dict[u'id'] = data_dict[u'pkg_name']",
            "                    del data_dict[u'pkg_name']",
            "                    # don't change the dataset state",
            "                    data_dict[u'state'] = u'draft'",
            "                    # this is actually an edit not a save",
            "                    pkg_dict = get_action(u'package_update')(",
            "                        context, data_dict",
            "                    )",
            "",
            "                    # redirect to add dataset resources",
            "                    url = h.url_for(",
            "                        u'{}_resource.new'.format(package_type),",
            "                        id=pkg_dict[u'name']",
            "                    )",
            "                    return h.redirect_to(url)",
            "                # Make sure we don't index this dataset",
            "                if request.form[u'save'] not in [",
            "                    u'go-resource', u'go-metadata'",
            "                ]:",
            "                    data_dict[u'state'] = u'draft'",
            "                # allow the state to be changed",
            "                context[u'allow_state_change'] = True",
            "",
            "            data_dict[u'type'] = package_type",
            "            pkg_dict = get_action(u'package_create')(context, data_dict)",
            "",
            "            create_on_ui_requires_resources = config.get(",
            "                'ckan.dataset.create_on_ui_requires_resources'",
            "            )",
            "            if ckan_phase:",
            "                if create_on_ui_requires_resources:",
            "                    # redirect to add dataset resources if",
            "                    # create_on_ui_requires_resources is set to true",
            "                    url = h.url_for(",
            "                        u'{}_resource.new'.format(package_type),",
            "                        id=pkg_dict[u'name']",
            "                    )",
            "                    return h.redirect_to(url)",
            "",
            "                get_action(u'package_update')(",
            "                    cast(Context, dict(context, allow_state_change=True)),",
            "                    dict(pkg_dict, state=u'active')",
            "                )",
            "                return h.redirect_to(",
            "                    u'{}.read'.format(package_type),",
            "                    id=pkg_dict[\"id\"]",
            "                )",
            "",
            "            return _form_save_redirect(",
            "                pkg_dict[u'name'], u'new', package_type=package_type",
            "            )",
            "        except NotAuthorized:",
            "            return base.abort(403, _(u'Unauthorized to read package'))",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except SearchIndexError as e:",
            "            try:",
            "                exc_str = str(repr(e.args))",
            "            except Exception:  # We don't like bare excepts",
            "                exc_str = str(str(e))",
            "            return base.abort(",
            "                500,",
            "                _(u'Unable to add package to search index.') + exc_str",
            "            )",
            "        except ValidationError as e:",
            "            errors = e.error_dict",
            "            error_summary = e.error_summary",
            "            if is_an_update:",
            "                # we need to get the state of the dataset to show the stage we",
            "                # are on.",
            "                pkg_dict = get_action(u'package_show')(context, data_dict)",
            "                data_dict[u'state'] = pkg_dict[u'state']",
            "                return EditView().get(",
            "                    package_type,",
            "                    data_dict[u'id'],",
            "                    data_dict,",
            "                    errors,",
            "                    error_summary",
            "                )",
            "            data_dict[u'state'] = u'none'",
            "            return self.get(package_type, data_dict, errors, error_summary)",
            "",
            "    def get(self,",
            "            package_type: str,",
            "            data: Optional[dict[str, Any]] = None,",
            "            errors: Optional[dict[str, Any]] = None,",
            "            error_summary: Optional[dict[str, Any]] = None) -> str:",
            "        context = self._prepare()",
            "        if data and u'type' in data:",
            "            package_type = data[u'type']",
            "",
            "        data = data or clean_dict(",
            "            dict_fns.unflatten(",
            "                tuplize_dict(",
            "                    parse_params(request.args, ignore_keys=CACHE_PARAMETERS)",
            "                )",
            "            )",
            "        )",
            "        resources_json = h.dump_json(data.get(u'resources', []))",
            "        # convert tags if not supplied in data",
            "        if data and not data.get(u'tag_string'):",
            "            data[u'tag_string'] = u', '.join(",
            "                h.dict_list_reduce(data.get(u'tags', {}), u'name')",
            "            )",
            "",
            "        errors = errors or {}",
            "        error_summary = error_summary or {}",
            "        # in the phased add dataset we need to know that",
            "        # we have already completed stage 1",
            "        stage = [u'active']",
            "        if data.get(u'state', u'').startswith(u'draft'):",
            "            stage = [u'active', u'complete']",
            "",
            "        # if we are creating from a group then this allows the group to be",
            "        # set automatically",
            "        data[",
            "            u'group_id'",
            "        ] = request.args.get(u'group') or request.args.get(u'groups__0__id')",
            "",
            "        form_snippet = _get_pkg_template(",
            "            u'package_form', package_type=package_type",
            "        )",
            "        form_vars: dict[str, Any] = {",
            "            u'data': data,",
            "            u'errors': errors,",
            "            u'error_summary': error_summary,",
            "            u'action': u'new',",
            "            u'stage': stage,",
            "            u'dataset_type': package_type,",
            "            u'form_style': u'new'",
            "        }",
            "        errors_json = h.dump_json(errors)",
            "",
            "        # TODO: remove",
            "        g.resources_json = resources_json",
            "        g.errors_json = errors_json",
            "",
            "        _setup_template_variables(context, {}, package_type=package_type)",
            "",
            "        new_template = _get_pkg_template(u'new_template', package_type)",
            "        return base.render(",
            "            new_template,",
            "            extra_vars={",
            "                u'form_vars': form_vars,",
            "                u'form_snippet': form_snippet,",
            "                u'dataset_type': package_type,",
            "                u'resources_json': resources_json,",
            "                u'form_snippet': form_snippet,",
            "                u'errors_json': errors_json",
            "            }",
            "        )",
            "",
            "",
            "class EditView(MethodView):",
            "    def _prepare(self) -> Context:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user,",
            "            u'save': u'save' in request.form",
            "        })",
            "        return context",
            "",
            "    def post(self, package_type: str, id: str) -> Union[Response, str]:",
            "        context = self._prepare()",
            "        package_type = _get_package_type(id) or package_type",
            "        log.debug(u'Package save request name: %s POST: %r', id, request.form)",
            "        try:",
            "            data_dict = clean_dict(",
            "                dict_fns.unflatten(tuplize_dict(parse_params(request.form)))",
            "            )",
            "        except dict_fns.DataError:",
            "            return base.abort(400, _(u'Integrity Error'))",
            "        try:",
            "            if u'_ckan_phase' in data_dict:",
            "                # we allow partial updates to not destroy existing resources",
            "                context[u'allow_partial_update'] = True",
            "                if u'tag_string' in data_dict:",
            "                    data_dict[u'tags'] = _tag_string_to_list(",
            "                        data_dict[u'tag_string']",
            "                    )",
            "                del data_dict[u'_ckan_phase']",
            "                del data_dict[u'save']",
            "            data_dict['id'] = id",
            "            pkg_dict = get_action(u'package_update')(context, data_dict)",
            "",
            "            return _form_save_redirect(",
            "                pkg_dict[u'name'], u'edit', package_type=package_type",
            "            )",
            "        except NotAuthorized:",
            "            return base.abort(403, _(u'Unauthorized to read package %s') % id)",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except SearchIndexError as e:",
            "            try:",
            "                exc_str = str(repr(e.args))",
            "            except Exception:  # We don't like bare excepts",
            "                exc_str = str(str(e))",
            "            return base.abort(",
            "                500,",
            "                _(u'Unable to update search index.') + exc_str",
            "            )",
            "        except ValidationError as e:",
            "            errors = e.error_dict",
            "            error_summary = e.error_summary",
            "            return self.get(package_type, id, data_dict, errors, error_summary)",
            "",
            "    def get(self,",
            "            package_type: str,",
            "            id: str,",
            "            data: Optional[dict[str, Any]] = None,",
            "            errors: Optional[dict[str, Any]] = None,",
            "            error_summary: Optional[dict[str, Any]] = None",
            "            ) -> Union[Response, str]:",
            "        context = self._prepare()",
            "        package_type = _get_package_type(id) or package_type",
            "        try:",
            "            view_context = context.copy()",
            "            view_context['for_view'] = True",
            "            pkg_dict = get_action(u'package_show')(",
            "                view_context, {u'id': id})",
            "            context[u'for_edit'] = True",
            "            old_data = get_action(u'package_show')(context, {u'id': id})",
            "            # old data is from the database and data is passed from the",
            "            # user if there is a validation error. Use users data if there.",
            "            if data:",
            "                old_data.update(data)",
            "            data = old_data",
            "        except (NotFound, NotAuthorized):",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        assert data is not None",
            "        # are we doing a multiphase add?",
            "        if data.get(u'state', u'').startswith(u'draft'):",
            "            g.form_action = h.url_for(u'{}.new'.format(package_type))",
            "            g.form_style = u'new'",
            "",
            "            return CreateView().get(",
            "                package_type,",
            "                data=data,",
            "                errors=errors,",
            "                error_summary=error_summary",
            "            )",
            "",
            "        pkg = context.get(u\"package\")",
            "        resources_json = h.dump_json(data.get(u'resources', []))",
            "        user = current_user.name",
            "        try:",
            "            check_access(u'package_update', context)",
            "        except NotAuthorized:",
            "            return base.abort(",
            "                403,",
            "                _(u'User %r not authorized to edit %s') % (user, id)",
            "            )",
            "        # convert tags if not supplied in data",
            "        if data and not data.get(u'tag_string'):",
            "            data[u'tag_string'] = u', '.join(",
            "                h.dict_list_reduce(pkg_dict.get(u'tags', {}), u'name')",
            "            )",
            "        errors = errors or {}",
            "        form_snippet = _get_pkg_template(",
            "            u'package_form', package_type=package_type",
            "        )",
            "        form_vars: dict[str, Any] = {",
            "            u'data': data,",
            "            u'errors': errors,",
            "            u'error_summary': error_summary,",
            "            u'action': u'edit',",
            "            u'dataset_type': package_type,",
            "            u'form_style': u'edit'",
            "        }",
            "        errors_json = h.dump_json(errors)",
            "",
            "        # TODO: remove",
            "        g.pkg = pkg",
            "        g.resources_json = resources_json",
            "        g.errors_json = errors_json",
            "",
            "        _setup_template_variables(",
            "            context, {u'id': id}, package_type=package_type",
            "        )",
            "",
            "        # we have already completed stage 1",
            "        form_vars[u'stage'] = [u'active']",
            "        if data.get(u'state', u'').startswith(u'draft'):",
            "            form_vars[u'stage'] = [u'active', u'complete']",
            "",
            "        edit_template = _get_pkg_template(u'edit_template', package_type)",
            "        return base.render(",
            "            edit_template,",
            "            extra_vars={",
            "                u'form_vars': form_vars,",
            "                u'form_snippet': form_snippet,",
            "                u'dataset_type': package_type,",
            "                u'pkg_dict': pkg_dict,",
            "                u'pkg': pkg,",
            "                u'resources_json': resources_json,",
            "                u'form_snippet': form_snippet,",
            "                u'errors_json': errors_json",
            "            }",
            "        )",
            "",
            "",
            "class DeleteView(MethodView):",
            "    def _prepare(self) -> Context:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user",
            "        })",
            "        return context",
            "",
            "    def post(self, package_type: str, id: str) -> Response:",
            "        if u'cancel' in request.form:",
            "            return h.redirect_to(u'{}.edit'.format(package_type), id=id)",
            "        context = self._prepare()",
            "        try:",
            "            get_action(u'package_delete')(context, {u'id': id})",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except NotAuthorized:",
            "            return base.abort(",
            "                403,",
            "                _(u'Unauthorized to delete package %s') % u''",
            "            )",
            "",
            "        h.flash_notice(_(u'Dataset has been deleted.'))",
            "        return h.redirect_to(package_type + u'.search')",
            "",
            "    def get(self, package_type: str, id: str) -> Union[Response, str]:",
            "        context = self._prepare()",
            "        try:",
            "            pkg_dict = get_action(u'package_show')(context, {u'id': id})",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except NotAuthorized:",
            "            return base.abort(",
            "                403,",
            "                _(u'Unauthorized to delete package %s') % u''",
            "            )",
            "",
            "        dataset_type = pkg_dict[u'type'] or package_type",
            "",
            "        # TODO: remove",
            "        g.pkg_dict = pkg_dict",
            "",
            "        return base.render(",
            "            u'package/confirm_delete.html', {",
            "                u'pkg_dict': pkg_dict,",
            "                u'dataset_type': dataset_type",
            "            }",
            "        )",
            "",
            "",
            "def follow(package_type: str, id: str) -> Response:",
            "    \"\"\"Start following this dataset.",
            "    \"\"\"",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict = {u'id': id}",
            "    try:",
            "        get_action(u'follow_dataset')(context, data_dict)",
            "        package_dict = get_action(u'package_show')(context, data_dict)",
            "        id = package_dict['name']",
            "    except ValidationError as e:",
            "        error_message = (e.message or e.error_summary or e.error_dict)",
            "        h.flash_error(error_message)",
            "    except NotAuthorized as e:",
            "        h.flash_error(e.message)",
            "    else:",
            "        h.flash_success(",
            "            _(u\"You are now following {0}\").format(package_dict[u'title'])",
            "        )",
            "",
            "    return h.redirect_to(u'{}.read'.format(package_type), id=id)",
            "",
            "",
            "def unfollow(package_type: str, id: str) -> Union[Response, str]:",
            "    \"\"\"Stop following this dataset.",
            "    \"\"\"",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict = {u'id': id}",
            "    try:",
            "        get_action(u'unfollow_dataset')(context, data_dict)",
            "        package_dict = get_action(u'package_show')(context, data_dict)",
            "        id = package_dict['name']",
            "    except ValidationError as e:",
            "        error_message = (e.message or e.error_summary or e.error_dict)",
            "        h.flash_error(error_message)",
            "    except NotFound as e:",
            "        error_message = e.message or ''",
            "        base.abort(404, _(error_message))",
            "    except NotAuthorized as e:",
            "        error_message = e.message or ''",
            "        base.abort(403, _(error_message))",
            "    else:",
            "        h.flash_success(",
            "            _(u\"You are no longer following {0}\").format(",
            "                package_dict[u'title']",
            "            )",
            "        )",
            "",
            "    return h.redirect_to(u'{}.read'.format(package_type), id=id)",
            "",
            "",
            "def followers(package_type: str,",
            "              id: Optional[str] = None) -> Union[Response, str]:",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "",
            "    data_dict = {u'id': id}",
            "    try:",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        pkg = context[u'package']",
            "        followers = get_action(u'dataset_follower_list')(",
            "            context, {",
            "                u'id': pkg_dict[u'id']",
            "            }",
            "        )",
            "",
            "        dataset_type = pkg.type or package_type",
            "    except NotFound:",
            "        return base.abort(404, _(u'Dataset not found'))",
            "    except NotAuthorized:",
            "        return base.abort(403, _(u'Unauthorized to read package %s') % id)",
            "",
            "    # TODO: remove",
            "    g.pkg_dict = pkg_dict",
            "    g.pkg = pkg",
            "    g.followers = followers",
            "",
            "    return base.render(",
            "        u'package/followers.html', {",
            "            u'dataset_type': dataset_type,",
            "            u'pkg_dict': pkg_dict,",
            "            u'pkg': pkg,",
            "            u'followers': followers",
            "        }",
            "    )",
            "",
            "",
            "class GroupView(MethodView):",
            "    def _prepare(self, id: str) -> tuple[Context, dict[str, Any]]:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'for_view': True,",
            "            u'auth_user_obj': current_user,",
            "            u'use_cache': False",
            "        })",
            "",
            "        try:",
            "            pkg_dict = get_action(u'package_show')(context, {u'id': id})",
            "        except (NotFound, NotAuthorized):",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        return context, pkg_dict",
            "",
            "    def post(self, package_type: str, id: str) -> Response:",
            "        context = self._prepare(id)[0]",
            "        new_group = request.form.get(u'group_added')",
            "        if new_group:",
            "            data_dict = {",
            "                u\"id\": new_group,",
            "                u\"object\": id,",
            "                u\"object_type\": u'package',",
            "                u\"capacity\": u'public'",
            "            }",
            "            try:",
            "                get_action(u'member_create')(context, data_dict)",
            "            except NotFound:",
            "                return base.abort(404, _(u'Group not found'))",
            "",
            "        removed_group = None",
            "        for param in request.form:",
            "            if param.startswith(u'group_remove'):",
            "                removed_group = param.split(u'.')[-1]",
            "                break",
            "        if removed_group:",
            "            data_dict = {",
            "                u\"id\": removed_group,",
            "                u\"object\": id,",
            "                u\"object_type\": u'package'",
            "            }",
            "",
            "            try:",
            "                get_action(u'member_delete')(context, data_dict)",
            "            except NotFound:",
            "                return base.abort(404, _(u'Group not found'))",
            "        return h.redirect_to(u'{}.groups'.format(package_type), id=id)",
            "",
            "    def get(self, package_type: str, id: str) -> str:",
            "        context, pkg_dict = self._prepare(id)",
            "        dataset_type = pkg_dict[u'type'] or package_type",
            "        context[u'is_member'] = True",
            "        users_groups = get_action(u'group_list_authz')(context, {u'id': id})",
            "",
            "        pkg_group_ids = set(",
            "            group[u'id'] for group in pkg_dict.get(u'groups', [])",
            "        )",
            "",
            "        user_group_ids = set(group[u'id'] for group in users_groups)",
            "",
            "        group_dropdown = [[group[u'id'], group[u'display_name']]",
            "                          for group in users_groups",
            "                          if group[u'id'] not in pkg_group_ids]",
            "",
            "        for group in pkg_dict.get(u'groups', []):",
            "            group[u'user_member'] = (group[u'id'] in user_group_ids)",
            "",
            "        # TODO: remove",
            "        g.pkg_dict = pkg_dict",
            "        g.group_dropdown = group_dropdown",
            "",
            "        return base.render(",
            "            u'package/group_list.html', {",
            "                u'dataset_type': dataset_type,",
            "                u'pkg_dict': pkg_dict,",
            "                u'group_dropdown': group_dropdown",
            "            }",
            "        )",
            "",
            "",
            "def collaborators_read(package_type: str, id: str) -> Union[Response, str]:  # noqa",
            "    context = cast(Context, {u'model': model, u'user': current_user.name})",
            "    data_dict = {u'id': id}",
            "",
            "    try:",
            "        check_access(u'package_collaborator_list', context, data_dict)",
            "        # needed to ckan_extend package/edit_base.html",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "    except NotAuthorized:",
            "        message = _(u'Unauthorized to read collaborators {}').format(id)",
            "        return base.abort(401, message)",
            "    except NotFound:",
            "        return base.abort(404, _(u'Dataset not found'))",
            "",
            "    return base.render(u'package/collaborators/collaborators.html', {",
            "        u'pkg_dict': pkg_dict})",
            "",
            "",
            "def collaborator_delete(package_type: str,",
            "                        id: str, user_id: str) -> Union[Response, str]:  # noqa",
            "    context: Context = {'user': current_user.name}",
            "",
            "    if u'cancel' in request.form:",
            "        return h.redirect_to(u'{}.collaborators_read'",
            "                             .format(package_type), id=id)",
            "",
            "    try:",
            "        if request.method == u'POST':",
            "            get_action(u'package_collaborator_delete')(context, {",
            "                u'id': id,",
            "                u'user_id': user_id",
            "            })",
            "        user_dict = logic.get_action(u'user_show')(context, {u'id': user_id})",
            "    except NotAuthorized:",
            "        message = _(u'Unauthorized to delete collaborators {}').format(id)",
            "        return base.abort(401, _(message))",
            "    except NotFound as e:",
            "        return base.abort(404, _(e.message))",
            "",
            "    if request.method == u'POST':",
            "        h.flash_success(_(u'User removed from collaborators'))",
            "",
            "        return h.redirect_to(u'dataset.collaborators_read', id=id)",
            "",
            "    # TODO: Remove",
            "    # ckan 2.9: Adding variables that were removed from c object for",
            "    # compatibility with templates in existing extensions",
            "    g.user_dict = user_dict",
            "    g.user_id = user_id",
            "    g.package_id = id",
            "",
            "    extra_vars = {",
            "        u\"user_id\": user_id,",
            "        u\"user_dict\": user_dict,",
            "        u\"package_id\": id,",
            "        u\"package_type\": package_type",
            "    }",
            "    return base.render(",
            "        u'package/collaborators/confirm_delete.html', extra_vars)",
            "",
            "",
            "class CollaboratorEditView(MethodView):",
            "",
            "    def post(self, package_type: str, id: str) -> Response:  # noqa",
            "        context = cast(Context, {u'model': model, u'user': current_user.name})",
            "",
            "        try:",
            "            form_dict = logic.clean_dict(",
            "                dict_fns.unflatten(",
            "                    logic.tuplize_dict(",
            "                        logic.parse_params(request.form))))",
            "",
            "            user = get_action(u'user_show')(",
            "                context, {u'id': form_dict[u'username']}",
            "            )",
            "",
            "            data_dict: dict[str, Any] = {",
            "                u'id': id,",
            "                u'user_id': user[u'id'],",
            "                u'capacity': form_dict[u'capacity']",
            "            }",
            "",
            "            get_action(u'package_collaborator_create')(",
            "                context, data_dict)",
            "",
            "        except dict_fns.DataError:",
            "            return base.abort(400, _(u'Integrity Error'))",
            "        except NotAuthorized:",
            "            message = _(u'Unauthorized to edit collaborators {}').format(id)",
            "            return base.abort(401, _(message))",
            "        except NotFound:",
            "            h.flash_error(_('User not found'))",
            "            return h.redirect_to(u'dataset.new_collaborator', id=id)",
            "        except ValidationError as e:",
            "            h.flash_error(e.error_summary)",
            "            return h.redirect_to(u'dataset.new_collaborator', id=id)",
            "        else:",
            "            h.flash_success(_(u'User added to collaborators'))",
            "",
            "        return h.redirect_to(u'dataset.collaborators_read', id=id)",
            "",
            "    def get(self, package_type: str, id: str) -> Union[Response, str]:  # noqa",
            "        context = cast(Context, {u'model': model, u'user': current_user.name})",
            "        data_dict = {u'id': id}",
            "",
            "        try:",
            "            check_access(u'package_collaborator_list', context, data_dict)",
            "            # needed to ckan_extend package/edit_base.html",
            "            pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        except NotAuthorized:",
            "            message = u'Unauthorized to read collaborators {}'.format(id)",
            "            return base.abort(401, _(message))",
            "        except NotFound:",
            "            return base.abort(404, _(u'Resource not found'))",
            "",
            "        user = request.args.get(u'user_id')",
            "        user_capacity = u'member'",
            "",
            "        if user:",
            "            collaborators = get_action(u'package_collaborator_list')(",
            "                context, data_dict)",
            "            for c in collaborators:",
            "                if c[u'user_id'] == user:",
            "                    user_capacity = c[u'capacity']",
            "            user = get_action(u'user_show')(context, {u'id': user})",
            "",
            "        capacities: list[dict[str, str]] = []",
            "        if authz.check_config_permission(u'allow_admin_collaborators'):",
            "            capacities.append({u'name': u'admin', u'value': u'admin'})",
            "        capacities.extend([",
            "            {u'name': u'editor', u'value': u'editor'},",
            "            {u'name': u'member', u'value': u'member'}",
            "        ])",
            "",
            "        extra_vars: dict[str, Any] = {",
            "            u'capacities': capacities,",
            "            u'user_capacity': user_capacity,",
            "            u'user': user,",
            "            u'pkg_dict': pkg_dict,",
            "        }",
            "",
            "        return base.render(",
            "            u'package/collaborators/collaborator_new.html', extra_vars)",
            "",
            "",
            "def register_dataset_plugin_rules(blueprint: Blueprint):",
            "    blueprint.add_url_rule(u'/', view_func=search, strict_slashes=False)",
            "    blueprint.add_url_rule(u'/new', view_func=CreateView.as_view(str(u'new')))",
            "    blueprint.add_url_rule(u'/<id>', view_func=read)",
            "    blueprint.add_url_rule(u'/resources/<id>', view_func=resources)",
            "    blueprint.add_url_rule(",
            "        u'/edit/<id>', view_func=EditView.as_view(str(u'edit'))",
            "    )",
            "    blueprint.add_url_rule(",
            "        u'/delete/<id>', view_func=DeleteView.as_view(str(u'delete'))",
            "    )",
            "    blueprint.add_url_rule(",
            "        u'/follow/<id>', view_func=follow, methods=(u'POST', )",
            "    )",
            "    blueprint.add_url_rule(",
            "        u'/unfollow/<id>', view_func=unfollow, methods=(u'POST', )",
            "    )",
            "    blueprint.add_url_rule(u'/followers/<id>', view_func=followers)",
            "    blueprint.add_url_rule(",
            "        u'/groups/<id>', view_func=GroupView.as_view(str(u'groups'))",
            "    )",
            "",
            "    if authz.check_config_permission(u'allow_dataset_collaborators'):",
            "        blueprint.add_url_rule(",
            "            rule=u'/collaborators/<id>',",
            "            view_func=collaborators_read,",
            "            methods=['GET', ]",
            "        )",
            "",
            "        blueprint.add_url_rule(",
            "            rule=u'/collaborators/<id>/new',",
            "            view_func=CollaboratorEditView.as_view(str(u'new_collaborator')),",
            "            methods=[u'GET', u'POST', ]",
            "        )",
            "",
            "        blueprint.add_url_rule(",
            "            rule=u'/collaborators/<id>/delete/<user_id>',",
            "            view_func=collaborator_delete, methods=['POST', 'GET']",
            "        )",
            "",
            "",
            "register_dataset_plugin_rules(dataset)",
            "# remove this when we improve blueprint registration to be explicit:",
            "dataset.auto_register = False  # type: ignore"
        ],
        "afterPatchFile": [
            "# encoding: utf-8",
            "from __future__ import annotations",
            "",
            "import logging",
            "import inspect",
            "from collections import OrderedDict",
            "from functools import partial",
            "from typing_extensions import TypeAlias",
            "from urllib.parse import urlencode",
            "from typing import Any, Iterable, Optional, Union, cast",
            "",
            "from flask import Blueprint",
            "from flask.views import MethodView",
            "from jinja2.exceptions import TemplateNotFound",
            "from werkzeug.datastructures import MultiDict",
            "from ckan.common import asbool, current_user",
            "",
            "import ckan.lib.base as base",
            "from ckan.lib.helpers import helper_functions as h",
            "from ckan.lib.helpers import Page",
            "import ckan.lib.navl.dictization_functions as dict_fns",
            "import ckan.logic as logic",
            "import ckan.model as model",
            "import ckan.plugins as plugins",
            "import ckan.authz as authz",
            "from ckan.common import _, config, g, request",
            "from ckan.views.home import CACHE_PARAMETERS",
            "from ckan.lib.plugins import lookup_package_plugin",
            "from ckan.lib.search import (",
            "    SearchError, SearchQueryError, SearchIndexError, SolrConnectionError",
            ")",
            "from ckan.types import Context, Response",
            "",
            "",
            "NotFound = logic.NotFound",
            "NotAuthorized = logic.NotAuthorized",
            "ValidationError = logic.ValidationError",
            "check_access = logic.check_access",
            "get_action = logic.get_action",
            "tuplize_dict = logic.tuplize_dict",
            "clean_dict = logic.clean_dict",
            "parse_params = logic.parse_params",
            "flatten_to_string_key = logic.flatten_to_string_key",
            "",
            "log = logging.getLogger(__name__)",
            "",
            "dataset = Blueprint(",
            "    u'dataset',",
            "    __name__,",
            "    url_prefix=u'/dataset',",
            "    url_defaults={u'package_type': u'dataset'}",
            ")",
            "",
            "",
            "def _setup_template_variables(context: Context,",
            "                              data_dict: dict[str, Any],",
            "                              package_type: Optional[str] = None) -> None:",
            "    return lookup_package_plugin(package_type).setup_template_variables(",
            "        context, data_dict",
            "    )",
            "",
            "",
            "def _get_pkg_template(template_type: str,",
            "                      package_type: Optional[str] = None) -> str:",
            "    pkg_plugin = lookup_package_plugin(package_type)",
            "    method = getattr(pkg_plugin, template_type)",
            "    signature = inspect.signature(method)",
            "    if len(signature.parameters):",
            "        return method(package_type)",
            "    else:",
            "        return method()",
            "",
            "",
            "def _encode_params(params: Iterable[tuple[str, Any]]):",
            "    return [(k, v.encode(u'utf-8') if isinstance(v, str) else str(v))",
            "            for k, v in params]",
            "",
            "",
            "Params: TypeAlias = \"list[tuple[str, Any]]\"",
            "",
            "",
            "def url_with_params(url: str, params: Params) -> str:",
            "    params = _encode_params(params)",
            "    return url + u'?' + urlencode(params)",
            "",
            "",
            "def search_url(params: Params, package_type: Optional[str] = None) -> str:",
            "    if not package_type:",
            "        package_type = u'dataset'",
            "    url = h.url_for(u'{0}.search'.format(package_type))",
            "    return url_with_params(url, params)",
            "",
            "",
            "def remove_field(package_type: Optional[str],",
            "                 key: str,",
            "                 value: Optional[str] = None,",
            "                 replace: Optional[str] = None):",
            "    if not package_type:",
            "        package_type = u'dataset'",
            "    url = h.url_for(u'{0}.search'.format(package_type))",
            "    return h.remove_url_param(",
            "        key,",
            "        value=value,",
            "        replace=replace,",
            "        alternative_url=url",
            "    )",
            "",
            "",
            "def _sort_by(params_nosort: Params, package_type: str,",
            "             fields: Iterable[tuple[str, str]]) -> str:",
            "    \"\"\"Sort by the given list of fields.",
            "",
            "    Each entry in the list is a 2-tuple: (fieldname, sort_order)",
            "    eg - [(u'metadata_modified', u'desc'), (u'name', u'asc')]",
            "    If fields is empty, then the default ordering is used.",
            "    \"\"\"",
            "    params = params_nosort[:]",
            "",
            "    if fields:",
            "        sort_string = u', '.join(u'%s %s' % f for f in fields)",
            "        params.append((u'sort', sort_string))",
            "    return search_url(params, package_type)",
            "",
            "",
            "def _pager_url(params_nopage: Params,",
            "               package_type: str,",
            "               q: Any = None,  # noqa",
            "               page: Optional[int] = None) -> str:",
            "    params = list(params_nopage)",
            "    params.append((u'page', page))",
            "    return search_url(params, package_type)",
            "",
            "",
            "def _tag_string_to_list(tag_string: str) -> list[dict[str, str]]:",
            "    \"\"\"This is used to change tags from a sting to a list of dicts.",
            "    \"\"\"",
            "    out: list[dict[str, str]] = []",
            "    for tag in tag_string.split(u','):",
            "        tag = tag.strip()",
            "        if tag:",
            "            out.append({u'name': tag, u'state': u'active'})",
            "    return out",
            "",
            "",
            "def _form_save_redirect(pkg_name: str,",
            "                        action: str,",
            "                        package_type: Optional[str] = None) -> Response:",
            "    \"\"\"This redirects the user to the CKAN package/read page,",
            "    unless there is request parameter giving an alternate location,",
            "    perhaps an external website.",
            "    @param pkg_name - Name of the package just edited",
            "    @param action - What the action of the edit was",
            "    \"\"\"",
            "    assert action in (u'new', u'edit')",
            "    url = request.args.get(u'return_to') or config.get(",
            "        u'package_%s_return_url' % action",
            "    )",
            "    if url:",
            "        url = url.replace(u'<NAME>', pkg_name)",
            "    else:",
            "        if not package_type:",
            "            package_type = u'dataset'",
            "        url = h.url_for(u'{0}.read'.format(package_type), id=pkg_name)",
            "    return h.redirect_to(url)",
            "",
            "",
            "def _get_package_type(id: str) -> str:",
            "    \"\"\"",
            "    Given the id of a package this method will return the type of the",
            "    package, or 'dataset' if no type is currently set",
            "    \"\"\"",
            "    pkg = model.Package.get(id)",
            "    if pkg:",
            "        return pkg.type or u'dataset'",
            "    return u'dataset'",
            "",
            "",
            "def _get_search_details() -> dict[str, Any]:",
            "    fq = u''",
            "",
            "    # fields_grouped will contain a dict of params containing",
            "    # a list of values eg {u'tags':[u'tag1', u'tag2']}",
            "",
            "    fields = []",
            "    fields_grouped = {}",
            "    search_extras: 'MultiDict[str, Any]' = MultiDict()",
            "",
            "    for (param, value) in request.args.items(multi=True):",
            "        if param not in [u'q', u'page', u'sort'] \\",
            "                and len(value) and not param.startswith(u'_'):",
            "            if not param.startswith(u'ext_'):",
            "                fields.append((param, value))",
            "                fq += u' %s:\"%s\"' % (param, value)",
            "                if param not in fields_grouped:",
            "                    fields_grouped[param] = [value]",
            "                else:",
            "                    fields_grouped[param].append(value)",
            "            else:",
            "                search_extras.update({param: value})",
            "",
            "    extras = dict([",
            "        (k, v[0]) if len(v) == 1 else (k, v)",
            "        for k, v in search_extras.lists()",
            "    ])",
            "    return {",
            "        u'fields': fields,",
            "        u'fields_grouped': fields_grouped,",
            "        u'fq': fq,",
            "        u'search_extras': extras,",
            "    }",
            "",
            "",
            "def search(package_type: str) -> str:",
            "    extra_vars: dict[str, Any] = {}",
            "",
            "    try:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user",
            "        })",
            "        check_access(u'site_read', context)",
            "    except NotAuthorized:",
            "        base.abort(403, _(u'Not authorized to see this page'))",
            "",
            "    # unicode format (decoded from utf8)",
            "    extra_vars[u'q'] = q = request.args.get(u'q', u'')",
            "",
            "    extra_vars['query_error'] = False",
            "    page = h.get_page_number(request.args)",
            "",
            "    limit = config.get(u'ckan.datasets_per_page')",
            "",
            "    # most search operations should reset the page counter:",
            "    params_nopage = [(k, v) for k, v in request.args.items(multi=True)",
            "                     if k != u'page']",
            "",
            "    extra_vars[u'remove_field'] = partial(remove_field, package_type)",
            "",
            "    sort_by = request.args.get(u'sort', None)",
            "    params_nosort = [(k, v) for k, v in params_nopage if k != u'sort']",
            "",
            "    extra_vars[u'sort_by'] = partial(_sort_by, params_nosort, package_type)",
            "",
            "    if not sort_by:",
            "        sort_by_fields = []",
            "    else:",
            "        sort_by_fields = [field.split()[0] for field in sort_by.split(u',')]",
            "    extra_vars[u'sort_by_fields'] = sort_by_fields",
            "",
            "    pager_url = partial(_pager_url, params_nopage, package_type)",
            "",
            "    details = _get_search_details()",
            "    extra_vars[u'fields'] = details[u'fields']",
            "    extra_vars[u'fields_grouped'] = details[u'fields_grouped']",
            "    fq = details[u'fq']",
            "    search_extras = details[u'search_extras']",
            "",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "",
            "    # Unless changed via config options, don't show other dataset",
            "    # types any search page. Potential alternatives are do show them",
            "    # on the default search page (dataset) or on one other search page",
            "    search_all_type = config.get(u'ckan.search.show_all_types')",
            "    search_all = False",
            "",
            "    try:",
            "        # If the \"type\" is set to True or False, convert to bool",
            "        # and we know that no type was specified, so use traditional",
            "        # behaviour of applying this only to dataset type",
            "        search_all = asbool(search_all_type)",
            "        search_all_type = u'dataset'",
            "    # Otherwise we treat as a string representing a type",
            "    except ValueError:",
            "        search_all = True",
            "",
            "    if not search_all or package_type != search_all_type:",
            "        # Only show datasets of this particular type",
            "        fq += u' +dataset_type:{type}'.format(type=package_type)",
            "",
            "    facets: dict[str, str] = OrderedDict()",
            "",
            "    org_label = h.humanize_entity_type(",
            "        u'organization',",
            "        h.default_group_type(u'organization'),",
            "        u'facet label') or _(u'Organizations')",
            "",
            "    group_label = h.humanize_entity_type(",
            "        u'group',",
            "        h.default_group_type(u'group'),",
            "        u'facet label') or _(u'Groups')",
            "",
            "    default_facet_titles = {",
            "        u'organization': org_label,",
            "        u'groups': group_label,",
            "        u'tags': _(u'Tags'),",
            "        u'res_format': _(u'Formats'),",
            "        u'license_id': _(u'Licenses'),",
            "    }",
            "",
            "    for facet in h.facets():",
            "        if facet in default_facet_titles:",
            "            facets[facet] = default_facet_titles[facet]",
            "        else:",
            "            facets[facet] = facet",
            "",
            "    # Facet titles",
            "    for plugin in plugins.PluginImplementations(plugins.IFacets):",
            "        facets = plugin.dataset_facets(facets, package_type)",
            "",
            "    extra_vars[u'facet_titles'] = facets",
            "    data_dict: dict[str, Any] = {",
            "        u'q': q,",
            "        u'fq': fq.strip(),",
            "        u'facet.field': list(facets.keys()),",
            "        u'rows': limit,",
            "        u'start': (page - 1) * limit,",
            "        u'sort': sort_by,",
            "        u'extras': search_extras,",
            "        u'include_private': config.get(",
            "            u'ckan.search.default_include_private'),",
            "    }",
            "    try:",
            "        query = get_action(u'package_search')(context, data_dict)",
            "",
            "        extra_vars[u'sort_by_selected'] = query[u'sort']",
            "",
            "        extra_vars[u'page'] = Page(",
            "            collection=query[u'results'],",
            "            page=page,",
            "            url=pager_url,",
            "            item_count=query[u'count'],",
            "            items_per_page=limit",
            "        )",
            "        extra_vars[u'search_facets'] = query[u'search_facets']",
            "        extra_vars[u'page'].items = query[u'results']",
            "    except SearchQueryError as se:",
            "        # User's search parameters are invalid, in such a way that is not",
            "        # achievable with the web interface, so return a proper error to",
            "        # discourage spiders which are the main cause of this.",
            "        log.info(u'Dataset search query rejected: %r', se.args)",
            "        base.abort(",
            "            400,",
            "            _(u'Invalid search query: {error_message}')",
            "            .format(error_message=str(se))",
            "        )",
            "    except (SearchError, SolrConnectionError) as se:",
            "        if isinstance(se, SolrConnectionError):",
            "            base.abort(500, se.args[0])",
            "",
            "        # May be bad input from the user, but may also be more serious like",
            "        # bad code causing a SOLR syntax error, or a problem connecting to",
            "        # SOLR",
            "        log.error(u'Dataset search error: %r', se.args)",
            "        extra_vars[u'query_error'] = True",
            "        extra_vars[u'search_facets'] = {}",
            "        extra_vars[u'page'] = Page(collection=[])",
            "",
            "    # FIXME: try to avoid using global variables",
            "    g.search_facets_limits = {}",
            "    default_limit: int = config.get(u'search.facets.default')",
            "    for facet in cast(Iterable[str], extra_vars[u'search_facets'].keys()):",
            "        try:",
            "            limit = int(",
            "                request.args.get(",
            "                    u'_%s_limit' % facet,",
            "                    default_limit",
            "                )",
            "            )",
            "        except ValueError:",
            "            base.abort(",
            "                400,",
            "                _(u'Parameter u\"{parameter_name}\" is not '",
            "                  u'an integer').format(parameter_name=u'_%s_limit' % facet)",
            "            )",
            "",
            "        g.search_facets_limits[facet] = limit",
            "",
            "    _setup_template_variables(context, {}, package_type=package_type)",
            "",
            "    extra_vars[u'dataset_type'] = package_type",
            "",
            "    # TODO: remove",
            "    for key, value in extra_vars.items():",
            "        setattr(g, key, value)",
            "",
            "    return base.render(",
            "        _get_pkg_template(u'search_template', package_type), extra_vars",
            "    )",
            "",
            "",
            "def resources(package_type: str, id: str) -> Union[Response, str]:",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict: dict[str, Any] = {u'id': id, u'include_tracking': True}",
            "",
            "    try:",
            "        check_access(u'package_update', context, data_dict)",
            "    except NotFound:",
            "        return base.abort(404, _(u'Dataset not found'))",
            "    except NotAuthorized:",
            "        return base.abort(",
            "            403,",
            "            _(u'User %r not authorized to edit %s') % (current_user.name, id)",
            "        )",
            "    # check if package exists",
            "    try:",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        pkg = context[u'package']",
            "    except (NotFound, NotAuthorized):",
            "        return base.abort(404, _(u'Dataset not found'))",
            "",
            "    package_type = pkg_dict[u'type'] or u'dataset'",
            "    _setup_template_variables(context, {u'id': id}, package_type=package_type)",
            "",
            "    # TODO: remove",
            "    g.pkg_dict = pkg_dict",
            "    g.pkg = pkg",
            "",
            "    return base.render(",
            "        u'package/resources.html', {",
            "            u'dataset_type': package_type,",
            "            u'pkg_dict': pkg_dict,",
            "            u'pkg': pkg",
            "        }",
            "    )",
            "",
            "",
            "def read(package_type: str, id: str) -> Union[Response, str]:",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict = {u'id': id, u'include_tracking': True}",
            "",
            "    # check if package exists",
            "    try:",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        pkg = context[u'package']",
            "    except NotFound:",
            "        return base.abort(",
            "            404,",
            "            _(u'Dataset not found or you have no permission to view it')",
            "        )",
            "    except NotAuthorized:",
            "        if config.get(u'ckan.auth.reveal_private_datasets'):",
            "            if current_user.is_authenticated:",
            "                return base.abort(",
            "                    403, _(u'Unauthorized to read package %s') % id)",
            "            else:",
            "                return h.redirect_to(",
            "                    \"user.login\",",
            "                    came_from=h.url_for('{}.read'.format(package_type), id=id)",
            "                )",
            "        return base.abort(",
            "            404,",
            "            _(u'Dataset not found or you have no permission to view it')",
            "        )",
            "",
            "    g.pkg_dict = pkg_dict",
            "    g.pkg = pkg",
            "",
            "    if plugins.plugin_loaded(\"activity\"):",
            "        activity_id = request.args.get(\"activity_id\")",
            "        if activity_id:",
            "            return h.redirect_to(",
            "                \"activity.package_history\",",
            "                id=id, activity_id=activity_id",
            "            )",
            "",
            "    # if the user specified a package id, redirect to the package name",
            "    if data_dict['id'] == pkg_dict['id'] and \\",
            "            data_dict['id'] != pkg_dict['name']:",
            "        return h.redirect_to(u'{}.read'.format(package_type),",
            "                             id=pkg_dict['name'])",
            "",
            "    # can the resources be previewed?",
            "    for resource in pkg_dict[u'resources']:",
            "        resource_views = get_action(u'resource_view_list')(",
            "            context, {",
            "                u'id': resource[u'id']",
            "            }",
            "        )",
            "        resource[u'has_views'] = len(resource_views) > 0",
            "",
            "    package_type = pkg_dict[u'type'] or package_type",
            "    _setup_template_variables(context, {u'id': id}, package_type=package_type)",
            "",
            "    template = _get_pkg_template(u'read_template', package_type)",
            "    try:",
            "        return base.render(",
            "            template, {",
            "                u'dataset_type': package_type,",
            "                u'pkg_dict': pkg_dict,",
            "                u'pkg': pkg,",
            "            }",
            "        )",
            "    except TemplateNotFound as e:",
            "        msg = _(",
            "            u\"Viewing datasets of type \\\"{package_type}\\\" is \"",
            "            u\"not supported ({file_!r}).\".format(",
            "                package_type=package_type, file_=e.message",
            "            )",
            "        )",
            "        return base.abort(404, msg)",
            "",
            "",
            "class CreateView(MethodView):",
            "    def _is_save(self) -> bool:",
            "        return u'save' in request.form",
            "",
            "    def _prepare(self) -> Context:  # noqa",
            "",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user,",
            "            u'save': self._is_save()",
            "        })",
            "        try:",
            "            check_access(u'package_create', context)",
            "        except NotAuthorized:",
            "            return base.abort(403, _(u'Unauthorized to create a package'))",
            "        return context",
            "",
            "    def post(self, package_type: str) -> Union[Response, str]:",
            "        # The staged add dataset used the new functionality when the dataset is",
            "        # partially created so we need to know if we actually are updating or",
            "        # this is a real new.",
            "        context = self._prepare()",
            "        is_an_update = False",
            "        ckan_phase = request.form.get(u'_ckan_phase')",
            "        try:",
            "            data_dict = clean_dict(",
            "                dict_fns.unflatten(tuplize_dict(parse_params(request.form)))",
            "            )",
            "        except dict_fns.DataError:",
            "            return base.abort(400, _(u'Integrity Error'))",
            "        try:",
            "            if ckan_phase:",
            "                # prevent clearing of groups etc",
            "                context[u'allow_partial_update'] = True",
            "                # sort the tags",
            "                if u'tag_string' in data_dict:",
            "                    data_dict[u'tags'] = _tag_string_to_list(",
            "                        data_dict[u'tag_string']",
            "                    )",
            "                if data_dict.get(u'pkg_name'):",
            "                    is_an_update = True",
            "                    # This is actually an update not a save",
            "                    data_dict[u'id'] = data_dict[u'pkg_name']",
            "                    del data_dict[u'pkg_name']",
            "                    # don't change the dataset state",
            "                    data_dict[u'state'] = u'draft'",
            "                    # this is actually an edit not a save",
            "                    pkg_dict = get_action(u'package_update')(",
            "                        context, data_dict",
            "                    )",
            "",
            "                    # redirect to add dataset resources",
            "                    url = h.url_for(",
            "                        u'{}_resource.new'.format(package_type),",
            "                        id=pkg_dict[u'name']",
            "                    )",
            "                    return h.redirect_to(url)",
            "                # Make sure we don't index this dataset",
            "                if request.form[u'save'] not in [",
            "                    u'go-resource', u'go-metadata'",
            "                ]:",
            "                    data_dict[u'state'] = u'draft'",
            "                # allow the state to be changed",
            "                context[u'allow_state_change'] = True",
            "",
            "            data_dict[u'type'] = package_type",
            "            pkg_dict = get_action(u'package_create')(context, data_dict)",
            "",
            "            create_on_ui_requires_resources = config.get(",
            "                'ckan.dataset.create_on_ui_requires_resources'",
            "            )",
            "            if ckan_phase:",
            "                if create_on_ui_requires_resources:",
            "                    # redirect to add dataset resources if",
            "                    # create_on_ui_requires_resources is set to true",
            "                    url = h.url_for(",
            "                        u'{}_resource.new'.format(package_type),",
            "                        id=pkg_dict[u'name']",
            "                    )",
            "                    return h.redirect_to(url)",
            "",
            "                get_action(u'package_update')(",
            "                    cast(Context, dict(context, allow_state_change=True)),",
            "                    dict(pkg_dict, state=u'active')",
            "                )",
            "                return h.redirect_to(",
            "                    u'{}.read'.format(package_type),",
            "                    id=pkg_dict[\"id\"]",
            "                )",
            "",
            "            return _form_save_redirect(",
            "                pkg_dict[u'name'], u'new', package_type=package_type",
            "            )",
            "        except NotAuthorized:",
            "            return base.abort(403, _(u'Unauthorized to read package'))",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except SearchIndexError as e:",
            "            try:",
            "                exc_str = str(repr(e.args))",
            "            except Exception:  # We don't like bare excepts",
            "                exc_str = str(str(e))",
            "            return base.abort(",
            "                500,",
            "                _(u'Unable to add package to search index.') + exc_str",
            "            )",
            "        except ValidationError as e:",
            "            errors = e.error_dict",
            "            error_summary = e.error_summary",
            "            if is_an_update:",
            "                # we need to get the state of the dataset to show the stage we",
            "                # are on.",
            "                pkg_dict = get_action(u'package_show')(context, data_dict)",
            "                data_dict[u'state'] = pkg_dict[u'state']",
            "                return EditView().get(",
            "                    package_type,",
            "                    data_dict[u'id'],",
            "                    data_dict,",
            "                    errors,",
            "                    error_summary",
            "                )",
            "            data_dict[u'state'] = u'none'",
            "            return self.get(package_type, data_dict, errors, error_summary)",
            "",
            "    def get(self,",
            "            package_type: str,",
            "            data: Optional[dict[str, Any]] = None,",
            "            errors: Optional[dict[str, Any]] = None,",
            "            error_summary: Optional[dict[str, Any]] = None) -> str:",
            "        context = self._prepare()",
            "        if data and u'type' in data:",
            "            package_type = data[u'type']",
            "",
            "        data = data or clean_dict(",
            "            dict_fns.unflatten(",
            "                tuplize_dict(",
            "                    parse_params(request.args, ignore_keys=CACHE_PARAMETERS)",
            "                )",
            "            )",
            "        )",
            "        resources_json = h.dump_json(data.get(u'resources', []))",
            "        # convert tags if not supplied in data",
            "        if data and not data.get(u'tag_string'):",
            "            data[u'tag_string'] = u', '.join(",
            "                h.dict_list_reduce(data.get(u'tags', {}), u'name')",
            "            )",
            "",
            "        errors = errors or {}",
            "        error_summary = error_summary or {}",
            "        # in the phased add dataset we need to know that",
            "        # we have already completed stage 1",
            "        stage = [u'active']",
            "        if data.get(u'state', u'').startswith(u'draft'):",
            "            stage = [u'active', u'complete']",
            "",
            "        # if we are creating from a group then this allows the group to be",
            "        # set automatically",
            "        data[",
            "            u'group_id'",
            "        ] = request.args.get(u'group') or request.args.get(u'groups__0__id')",
            "",
            "        form_snippet = _get_pkg_template(",
            "            u'package_form', package_type=package_type",
            "        )",
            "        form_vars: dict[str, Any] = {",
            "            u'data': data,",
            "            u'errors': errors,",
            "            u'error_summary': error_summary,",
            "            u'action': u'new',",
            "            u'stage': stage,",
            "            u'dataset_type': package_type,",
            "            u'form_style': u'new'",
            "        }",
            "        errors_json = h.dump_json(errors)",
            "",
            "        # TODO: remove",
            "        g.resources_json = resources_json",
            "        g.errors_json = errors_json",
            "",
            "        _setup_template_variables(context, {}, package_type=package_type)",
            "",
            "        new_template = _get_pkg_template(u'new_template', package_type)",
            "        return base.render(",
            "            new_template,",
            "            extra_vars={",
            "                u'form_vars': form_vars,",
            "                u'form_snippet': form_snippet,",
            "                u'dataset_type': package_type,",
            "                u'resources_json': resources_json,",
            "                u'form_snippet': form_snippet,",
            "                u'errors_json': errors_json",
            "            }",
            "        )",
            "",
            "",
            "class EditView(MethodView):",
            "    def _prepare(self) -> Context:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user,",
            "            u'save': u'save' in request.form",
            "        })",
            "        return context",
            "",
            "    def post(self, package_type: str, id: str) -> Union[Response, str]:",
            "        context = self._prepare()",
            "        package_type = _get_package_type(id) or package_type",
            "        log.debug(u'Package save request name: %s POST: %r', id, request.form)",
            "        try:",
            "            data_dict = clean_dict(",
            "                dict_fns.unflatten(tuplize_dict(parse_params(request.form)))",
            "            )",
            "        except dict_fns.DataError:",
            "            return base.abort(400, _(u'Integrity Error'))",
            "        try:",
            "            if u'_ckan_phase' in data_dict:",
            "                # we allow partial updates to not destroy existing resources",
            "                context[u'allow_partial_update'] = True",
            "                if u'tag_string' in data_dict:",
            "                    data_dict[u'tags'] = _tag_string_to_list(",
            "                        data_dict[u'tag_string']",
            "                    )",
            "                del data_dict[u'_ckan_phase']",
            "                del data_dict[u'save']",
            "            data_dict['id'] = id",
            "            pkg_dict = get_action(u'package_update')(context, data_dict)",
            "",
            "            return _form_save_redirect(",
            "                pkg_dict[u'name'], u'edit', package_type=package_type",
            "            )",
            "        except NotAuthorized:",
            "            return base.abort(403, _(u'Unauthorized to read package %s') % id)",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except SearchIndexError as e:",
            "            try:",
            "                exc_str = str(repr(e.args))",
            "            except Exception:  # We don't like bare excepts",
            "                exc_str = str(str(e))",
            "            return base.abort(",
            "                500,",
            "                _(u'Unable to update search index.') + exc_str",
            "            )",
            "        except ValidationError as e:",
            "            errors = e.error_dict",
            "            error_summary = e.error_summary",
            "            return self.get(package_type, id, data_dict, errors, error_summary)",
            "",
            "    def get(self,",
            "            package_type: str,",
            "            id: str,",
            "            data: Optional[dict[str, Any]] = None,",
            "            errors: Optional[dict[str, Any]] = None,",
            "            error_summary: Optional[dict[str, Any]] = None",
            "            ) -> Union[Response, str]:",
            "        context = self._prepare()",
            "        package_type = _get_package_type(id) or package_type",
            "        try:",
            "            view_context = context.copy()",
            "            view_context['for_view'] = True",
            "            pkg_dict = get_action(u'package_show')(",
            "                view_context, {u'id': id})",
            "            context[u'for_edit'] = True",
            "            old_data = get_action(u'package_show')(context, {u'id': id})",
            "            # old data is from the database and data is passed from the",
            "            # user if there is a validation error. Use users data if there.",
            "            if data:",
            "                old_data.update(data)",
            "            data = old_data",
            "        except (NotFound, NotAuthorized):",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        assert data is not None",
            "        # are we doing a multiphase add?",
            "        if data.get(u'state', u'').startswith(u'draft'):",
            "            g.form_action = h.url_for(u'{}.new'.format(package_type))",
            "            g.form_style = u'new'",
            "",
            "            return CreateView().get(",
            "                package_type,",
            "                data=data,",
            "                errors=errors,",
            "                error_summary=error_summary",
            "            )",
            "",
            "        pkg = context.get(u\"package\")",
            "        resources_json = h.dump_json(data.get(u'resources', []))",
            "        user = current_user.name",
            "        try:",
            "            check_access(u'package_update', context)",
            "        except NotAuthorized:",
            "            return base.abort(",
            "                403,",
            "                _(u'User %r not authorized to edit %s') % (user, id)",
            "            )",
            "        # convert tags if not supplied in data",
            "        if data and not data.get(u'tag_string'):",
            "            data[u'tag_string'] = u', '.join(",
            "                h.dict_list_reduce(pkg_dict.get(u'tags', {}), u'name')",
            "            )",
            "        errors = errors or {}",
            "        form_snippet = _get_pkg_template(",
            "            u'package_form', package_type=package_type",
            "        )",
            "        form_vars: dict[str, Any] = {",
            "            u'data': data,",
            "            u'errors': errors,",
            "            u'error_summary': error_summary,",
            "            u'action': u'edit',",
            "            u'dataset_type': package_type,",
            "            u'form_style': u'edit'",
            "        }",
            "        errors_json = h.dump_json(errors)",
            "",
            "        # TODO: remove",
            "        g.pkg = pkg",
            "        g.resources_json = resources_json",
            "        g.errors_json = errors_json",
            "",
            "        _setup_template_variables(",
            "            context, {u'id': id}, package_type=package_type",
            "        )",
            "",
            "        # we have already completed stage 1",
            "        form_vars[u'stage'] = [u'active']",
            "        if data.get(u'state', u'').startswith(u'draft'):",
            "            form_vars[u'stage'] = [u'active', u'complete']",
            "",
            "        edit_template = _get_pkg_template(u'edit_template', package_type)",
            "        return base.render(",
            "            edit_template,",
            "            extra_vars={",
            "                u'form_vars': form_vars,",
            "                u'form_snippet': form_snippet,",
            "                u'dataset_type': package_type,",
            "                u'pkg_dict': pkg_dict,",
            "                u'pkg': pkg,",
            "                u'resources_json': resources_json,",
            "                u'form_snippet': form_snippet,",
            "                u'errors_json': errors_json",
            "            }",
            "        )",
            "",
            "",
            "class DeleteView(MethodView):",
            "    def _prepare(self) -> Context:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'auth_user_obj': current_user",
            "        })",
            "        return context",
            "",
            "    def post(self, package_type: str, id: str) -> Response:",
            "        if u'cancel' in request.form:",
            "            return h.redirect_to(u'{}.edit'.format(package_type), id=id)",
            "        context = self._prepare()",
            "        try:",
            "            get_action(u'package_delete')(context, {u'id': id})",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except NotAuthorized:",
            "            return base.abort(",
            "                403,",
            "                _(u'Unauthorized to delete package %s') % u''",
            "            )",
            "",
            "        h.flash_notice(_(u'Dataset has been deleted.'))",
            "        return h.redirect_to(package_type + u'.search')",
            "",
            "    def get(self, package_type: str, id: str) -> Union[Response, str]:",
            "        context = self._prepare()",
            "        try:",
            "            pkg_dict = get_action(u'package_show')(context, {u'id': id})",
            "        except NotFound:",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        except NotAuthorized:",
            "            return base.abort(",
            "                403,",
            "                _(u'Unauthorized to delete package %s') % u''",
            "            )",
            "",
            "        dataset_type = pkg_dict[u'type'] or package_type",
            "",
            "        # TODO: remove",
            "        g.pkg_dict = pkg_dict",
            "",
            "        return base.render(",
            "            u'package/confirm_delete.html', {",
            "                u'pkg_dict': pkg_dict,",
            "                u'dataset_type': dataset_type",
            "            }",
            "        )",
            "",
            "",
            "def follow(package_type: str, id: str) -> Response:",
            "    \"\"\"Start following this dataset.",
            "    \"\"\"",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict = {u'id': id}",
            "    try:",
            "        get_action(u'follow_dataset')(context, data_dict)",
            "        package_dict = get_action(u'package_show')(context, data_dict)",
            "        id = package_dict['name']",
            "    except ValidationError as e:",
            "        error_message = (e.message or e.error_summary or e.error_dict)",
            "        h.flash_error(error_message)",
            "    except NotAuthorized as e:",
            "        h.flash_error(e.message)",
            "    else:",
            "        h.flash_success(",
            "            _(u\"You are now following {0}\").format(package_dict[u'title'])",
            "        )",
            "",
            "    return h.redirect_to(u'{}.read'.format(package_type), id=id)",
            "",
            "",
            "def unfollow(package_type: str, id: str) -> Union[Response, str]:",
            "    \"\"\"Stop following this dataset.",
            "    \"\"\"",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'auth_user_obj': current_user",
            "    })",
            "    data_dict = {u'id': id}",
            "    try:",
            "        get_action(u'unfollow_dataset')(context, data_dict)",
            "        package_dict = get_action(u'package_show')(context, data_dict)",
            "        id = package_dict['name']",
            "    except ValidationError as e:",
            "        error_message = (e.message or e.error_summary or e.error_dict)",
            "        h.flash_error(error_message)",
            "    except NotFound as e:",
            "        error_message = e.message or ''",
            "        base.abort(404, _(error_message))",
            "    except NotAuthorized as e:",
            "        error_message = e.message or ''",
            "        base.abort(403, _(error_message))",
            "    else:",
            "        h.flash_success(",
            "            _(u\"You are no longer following {0}\").format(",
            "                package_dict[u'title']",
            "            )",
            "        )",
            "",
            "    return h.redirect_to(u'{}.read'.format(package_type), id=id)",
            "",
            "",
            "def followers(package_type: str,",
            "              id: Optional[str] = None) -> Union[Response, str]:",
            "    context = cast(Context, {",
            "        u'model': model,",
            "        u'session': model.Session,",
            "        u'user': current_user.name,",
            "        u'for_view': True,",
            "        u'auth_user_obj': current_user",
            "    })",
            "",
            "    data_dict = {u'id': id}",
            "    try:",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        pkg = context[u'package']",
            "        followers = get_action(u'dataset_follower_list')(",
            "            context, {",
            "                u'id': pkg_dict[u'id']",
            "            }",
            "        )",
            "",
            "        dataset_type = pkg.type or package_type",
            "    except NotFound:",
            "        return base.abort(404, _(u'Dataset not found'))",
            "    except NotAuthorized:",
            "        return base.abort(403, _(u'Unauthorized to read package %s') % id)",
            "",
            "    # TODO: remove",
            "    g.pkg_dict = pkg_dict",
            "    g.pkg = pkg",
            "    g.followers = followers",
            "",
            "    return base.render(",
            "        u'package/followers.html', {",
            "            u'dataset_type': dataset_type,",
            "            u'pkg_dict': pkg_dict,",
            "            u'pkg': pkg,",
            "            u'followers': followers",
            "        }",
            "    )",
            "",
            "",
            "class GroupView(MethodView):",
            "    def _prepare(self, id: str) -> tuple[Context, dict[str, Any]]:",
            "        context = cast(Context, {",
            "            u'model': model,",
            "            u'session': model.Session,",
            "            u'user': current_user.name,",
            "            u'for_view': True,",
            "            u'auth_user_obj': current_user,",
            "            u'use_cache': False",
            "        })",
            "",
            "        try:",
            "            pkg_dict = get_action(u'package_show')(context, {u'id': id})",
            "        except (NotFound, NotAuthorized):",
            "            return base.abort(404, _(u'Dataset not found'))",
            "        return context, pkg_dict",
            "",
            "    def post(self, package_type: str, id: str) -> Response:",
            "        context = self._prepare(id)[0]",
            "        new_group = request.form.get(u'group_added')",
            "        if new_group:",
            "            data_dict = {",
            "                u\"id\": new_group,",
            "                u\"object\": id,",
            "                u\"object_type\": u'package',",
            "                u\"capacity\": u'public'",
            "            }",
            "            try:",
            "                get_action(u'member_create')(context, data_dict)",
            "            except NotFound:",
            "                return base.abort(404, _(u'Group not found'))",
            "",
            "        removed_group = None",
            "        for param in request.form:",
            "            if param.startswith(u'group_remove'):",
            "                removed_group = param.split(u'.')[-1]",
            "                break",
            "        if removed_group:",
            "            data_dict = {",
            "                u\"id\": removed_group,",
            "                u\"object\": id,",
            "                u\"object_type\": u'package'",
            "            }",
            "",
            "            try:",
            "                get_action(u'member_delete')(context, data_dict)",
            "            except NotFound:",
            "                return base.abort(404, _(u'Group not found'))",
            "        return h.redirect_to(u'{}.groups'.format(package_type), id=id)",
            "",
            "    def get(self, package_type: str, id: str) -> str:",
            "        context, pkg_dict = self._prepare(id)",
            "        dataset_type = pkg_dict[u'type'] or package_type",
            "        context[u'is_member'] = True",
            "        users_groups = get_action(u'group_list_authz')(context, {u'id': id})",
            "",
            "        pkg_group_ids = set(",
            "            group[u'id'] for group in pkg_dict.get(u'groups', [])",
            "        )",
            "",
            "        user_group_ids = set(group[u'id'] for group in users_groups)",
            "",
            "        group_dropdown = [[group[u'id'], group[u'display_name']]",
            "                          for group in users_groups",
            "                          if group[u'id'] not in pkg_group_ids]",
            "",
            "        for group in pkg_dict.get(u'groups', []):",
            "            group[u'user_member'] = (group[u'id'] in user_group_ids)",
            "",
            "        # TODO: remove",
            "        g.pkg_dict = pkg_dict",
            "        g.group_dropdown = group_dropdown",
            "",
            "        return base.render(",
            "            u'package/group_list.html', {",
            "                u'dataset_type': dataset_type,",
            "                u'pkg_dict': pkg_dict,",
            "                u'group_dropdown': group_dropdown",
            "            }",
            "        )",
            "",
            "",
            "def collaborators_read(package_type: str, id: str) -> Union[Response, str]:  # noqa",
            "    context = cast(Context, {u'model': model, u'user': current_user.name})",
            "    data_dict = {u'id': id}",
            "",
            "    try:",
            "        check_access(u'package_collaborator_list', context, data_dict)",
            "        # needed to ckan_extend package/edit_base.html",
            "        pkg_dict = get_action(u'package_show')(context, data_dict)",
            "    except NotAuthorized:",
            "        message = _(u'Unauthorized to read collaborators {}').format(id)",
            "        return base.abort(401, message)",
            "    except NotFound:",
            "        return base.abort(404, _(u'Dataset not found'))",
            "",
            "    return base.render(u'package/collaborators/collaborators.html', {",
            "        u'pkg_dict': pkg_dict})",
            "",
            "",
            "def collaborator_delete(package_type: str,",
            "                        id: str, user_id: str) -> Union[Response, str]:  # noqa",
            "    context: Context = {'user': current_user.name}",
            "",
            "    if u'cancel' in request.form:",
            "        return h.redirect_to(u'{}.collaborators_read'",
            "                             .format(package_type), id=id)",
            "",
            "    try:",
            "        if request.method == u'POST':",
            "            get_action(u'package_collaborator_delete')(context, {",
            "                u'id': id,",
            "                u'user_id': user_id",
            "            })",
            "        user_dict = logic.get_action(u'user_show')(context, {u'id': user_id})",
            "    except NotAuthorized:",
            "        message = _(u'Unauthorized to delete collaborators {}').format(id)",
            "        return base.abort(401, _(message))",
            "    except NotFound as e:",
            "        return base.abort(404, _(e.message))",
            "",
            "    if request.method == u'POST':",
            "        h.flash_success(_(u'User removed from collaborators'))",
            "",
            "        return h.redirect_to(u'dataset.collaborators_read', id=id)",
            "",
            "    # TODO: Remove",
            "    # ckan 2.9: Adding variables that were removed from c object for",
            "    # compatibility with templates in existing extensions",
            "    g.user_dict = user_dict",
            "    g.user_id = user_id",
            "    g.package_id = id",
            "",
            "    extra_vars = {",
            "        u\"user_id\": user_id,",
            "        u\"user_dict\": user_dict,",
            "        u\"package_id\": id,",
            "        u\"package_type\": package_type",
            "    }",
            "    return base.render(",
            "        u'package/collaborators/confirm_delete.html', extra_vars)",
            "",
            "",
            "class CollaboratorEditView(MethodView):",
            "",
            "    def post(self, package_type: str, id: str) -> Response:  # noqa",
            "        context = cast(Context, {u'model': model, u'user': current_user.name})",
            "",
            "        try:",
            "            form_dict = logic.clean_dict(",
            "                dict_fns.unflatten(",
            "                    logic.tuplize_dict(",
            "                        logic.parse_params(request.form))))",
            "",
            "            user = get_action(u'user_show')(",
            "                context, {u'id': form_dict[u'username']}",
            "            )",
            "",
            "            data_dict: dict[str, Any] = {",
            "                u'id': id,",
            "                u'user_id': user[u'id'],",
            "                u'capacity': form_dict[u'capacity']",
            "            }",
            "",
            "            get_action(u'package_collaborator_create')(",
            "                context, data_dict)",
            "",
            "        except dict_fns.DataError:",
            "            return base.abort(400, _(u'Integrity Error'))",
            "        except NotAuthorized:",
            "            message = _(u'Unauthorized to edit collaborators {}').format(id)",
            "            return base.abort(401, _(message))",
            "        except NotFound:",
            "            h.flash_error(_('User not found'))",
            "            return h.redirect_to(u'dataset.new_collaborator', id=id)",
            "        except ValidationError as e:",
            "            h.flash_error(e.error_summary)",
            "            return h.redirect_to(u'dataset.new_collaborator', id=id)",
            "        else:",
            "            h.flash_success(_(u'User added to collaborators'))",
            "",
            "        return h.redirect_to(u'dataset.collaborators_read', id=id)",
            "",
            "    def get(self, package_type: str, id: str) -> Union[Response, str]:  # noqa",
            "        context = cast(Context, {u'model': model, u'user': current_user.name})",
            "        data_dict = {u'id': id}",
            "",
            "        try:",
            "            check_access(u'package_collaborator_list', context, data_dict)",
            "            # needed to ckan_extend package/edit_base.html",
            "            pkg_dict = get_action(u'package_show')(context, data_dict)",
            "        except NotAuthorized:",
            "            message = u'Unauthorized to read collaborators {}'.format(id)",
            "            return base.abort(401, _(message))",
            "        except NotFound:",
            "            return base.abort(404, _(u'Resource not found'))",
            "",
            "        user = request.args.get(u'user_id')",
            "        user_capacity = u'member'",
            "",
            "        if user:",
            "            collaborators = get_action(u'package_collaborator_list')(",
            "                context, data_dict)",
            "            for c in collaborators:",
            "                if c[u'user_id'] == user:",
            "                    user_capacity = c[u'capacity']",
            "            user = get_action(u'user_show')(context, {u'id': user})",
            "",
            "        capacities: list[dict[str, str]] = []",
            "        if authz.check_config_permission(u'allow_admin_collaborators'):",
            "            capacities.append({u'name': u'admin', u'value': u'admin'})",
            "        capacities.extend([",
            "            {u'name': u'editor', u'value': u'editor'},",
            "            {u'name': u'member', u'value': u'member'}",
            "        ])",
            "",
            "        extra_vars: dict[str, Any] = {",
            "            u'capacities': capacities,",
            "            u'user_capacity': user_capacity,",
            "            u'user': user,",
            "            u'pkg_dict': pkg_dict,",
            "        }",
            "",
            "        return base.render(",
            "            u'package/collaborators/collaborator_new.html', extra_vars)",
            "",
            "",
            "def register_dataset_plugin_rules(blueprint: Blueprint):",
            "    blueprint.add_url_rule(u'/', view_func=search, strict_slashes=False)",
            "    blueprint.add_url_rule(u'/new', view_func=CreateView.as_view(str(u'new')))",
            "    blueprint.add_url_rule(u'/<id>', view_func=read)",
            "    blueprint.add_url_rule(u'/resources/<id>', view_func=resources)",
            "    blueprint.add_url_rule(",
            "        u'/edit/<id>', view_func=EditView.as_view(str(u'edit'))",
            "    )",
            "    blueprint.add_url_rule(",
            "        u'/delete/<id>', view_func=DeleteView.as_view(str(u'delete'))",
            "    )",
            "    blueprint.add_url_rule(",
            "        u'/follow/<id>', view_func=follow, methods=(u'POST', )",
            "    )",
            "    blueprint.add_url_rule(",
            "        u'/unfollow/<id>', view_func=unfollow, methods=(u'POST', )",
            "    )",
            "    blueprint.add_url_rule(u'/followers/<id>', view_func=followers)",
            "    blueprint.add_url_rule(",
            "        u'/groups/<id>', view_func=GroupView.as_view(str(u'groups'))",
            "    )",
            "",
            "    if authz.check_config_permission(u'allow_dataset_collaborators'):",
            "        blueprint.add_url_rule(",
            "            rule=u'/collaborators/<id>',",
            "            view_func=collaborators_read,",
            "            methods=['GET', ]",
            "        )",
            "",
            "        blueprint.add_url_rule(",
            "            rule=u'/collaborators/<id>/new',",
            "            view_func=CollaboratorEditView.as_view(str(u'new_collaborator')),",
            "            methods=[u'GET', u'POST', ]",
            "        )",
            "",
            "        blueprint.add_url_rule(",
            "            rule=u'/collaborators/<id>/delete/<user_id>',",
            "            view_func=collaborator_delete, methods=['POST', 'GET']",
            "        )",
            "",
            "",
            "register_dataset_plugin_rules(dataset)",
            "# remove this when we improve blueprint registration to be explicit:",
            "dataset.auto_register = False  # type: ignore"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "29": [],
            "351": [
                "search"
            ]
        },
        "addLocation": []
    }
}