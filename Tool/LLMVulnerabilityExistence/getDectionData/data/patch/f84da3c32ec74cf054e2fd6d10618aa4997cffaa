{
    "synapse/events/validator.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     CANONICALJSON_MIN_INT,"
            },
            "1": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     validate_canonicaljson,"
            },
            "2": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " )"
            },
            "3": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from synapse.federation.federation_server import server_matches_acl_event"
            },
            "4": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " from synapse.http.servlet import validate_json_object"
            },
            "5": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " from synapse.rest.models import RequestBodyModel"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+from synapse.storage.controllers.state import server_acl_evaluator_from_event"
            },
            "7": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " from synapse.types import EventID, JsonDict, RoomID, StrCollection, UserID"
            },
            "8": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 47,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "             self._validate_retention(event)"
            },
            "11": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 107,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "         elif event.type == EventTypes.ServerACL:"
            },
            "13": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if not server_matches_acl_event(config.server.server_name, event):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+            server_acl_evaluator = server_acl_evaluator_from_event(event)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+            if not server_acl_evaluator.server_matches_acl_event("
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+                config.server.server_name"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+            ):"
            },
            "18": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "                 raise SynapseError("
            },
            "19": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "                     400, \"Can't create an ACL event that denies the local server\""
            },
            "20": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "                 )"
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import collections.abc",
            "from typing import TYPE_CHECKING, List, Type, Union, cast",
            "",
            "import jsonschema",
            "",
            "from synapse._pydantic_compat import HAS_PYDANTIC_V2",
            "",
            "if TYPE_CHECKING or HAS_PYDANTIC_V2:",
            "    from pydantic.v1 import Field, StrictBool, StrictStr",
            "else:",
            "    from pydantic import Field, StrictBool, StrictStr",
            "",
            "from synapse.api.constants import (",
            "    MAX_ALIAS_LENGTH,",
            "    EventContentFields,",
            "    EventTypes,",
            "    Membership,",
            ")",
            "from synapse.api.errors import Codes, SynapseError",
            "from synapse.api.room_versions import EventFormatVersions",
            "from synapse.config.homeserver import HomeServerConfig",
            "from synapse.events import EventBase",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.utils import (",
            "    CANONICALJSON_MAX_INT,",
            "    CANONICALJSON_MIN_INT,",
            "    validate_canonicaljson,",
            ")",
            "from synapse.federation.federation_server import server_matches_acl_event",
            "from synapse.http.servlet import validate_json_object",
            "from synapse.rest.models import RequestBodyModel",
            "from synapse.types import EventID, JsonDict, RoomID, StrCollection, UserID",
            "",
            "",
            "class EventValidator:",
            "    def validate_new(self, event: EventBase, config: HomeServerConfig) -> None:",
            "        \"\"\"Validates the event has roughly the right format",
            "",
            "        Suitable for checking a locally-created event. It has stricter checks than",
            "        is appropriate for an event received over federation (for which, see",
            "        event_auth.validate_event_for_room_version)",
            "",
            "        Args:",
            "            event: The event to validate.",
            "            config: The homeserver's configuration.",
            "        \"\"\"",
            "        self.validate_builder(event)",
            "",
            "        if event.format_version == EventFormatVersions.ROOM_V1_V2:",
            "            EventID.from_string(event.event_id)",
            "",
            "        required = [",
            "            \"auth_events\",",
            "            \"content\",",
            "            \"hashes\",",
            "            \"origin\",",
            "            \"prev_events\",",
            "            \"sender\",",
            "            \"type\",",
            "        ]",
            "",
            "        for k in required:",
            "            if k not in event:",
            "                raise SynapseError(400, \"Event does not have key %s\" % (k,))",
            "",
            "        # Check that the following keys have string values",
            "        event_strings = [\"origin\"]",
            "",
            "        for s in event_strings:",
            "            if not isinstance(getattr(event, s), str):",
            "                raise SynapseError(400, \"'%s' not a string type\" % (s,))",
            "",
            "        # Depending on the room version, ensure the data is spec compliant JSON.",
            "        if event.room_version.strict_canonicaljson:",
            "            # Note that only the client controlled portion of the event is",
            "            # checked, since we trust the portions of the event we created.",
            "            validate_canonicaljson(event.content)",
            "",
            "        if event.type == EventTypes.Aliases:",
            "            if \"aliases\" in event.content:",
            "                for alias in event.content[\"aliases\"]:",
            "                    if len(alias) > MAX_ALIAS_LENGTH:",
            "                        raise SynapseError(",
            "                            400,",
            "                            (",
            "                                \"Can't create aliases longer than\"",
            "                                \" %d characters\" % (MAX_ALIAS_LENGTH,)",
            "                            ),",
            "                            Codes.INVALID_PARAM,",
            "                        )",
            "",
            "        elif event.type == EventTypes.Retention:",
            "            self._validate_retention(event)",
            "",
            "        elif event.type == EventTypes.ServerACL:",
            "            if not server_matches_acl_event(config.server.server_name, event):",
            "                raise SynapseError(",
            "                    400, \"Can't create an ACL event that denies the local server\"",
            "                )",
            "",
            "        elif event.type == EventTypes.PowerLevels:",
            "            try:",
            "                jsonschema.validate(",
            "                    instance=event.content,",
            "                    schema=POWER_LEVELS_SCHEMA,",
            "                    cls=POWER_LEVELS_VALIDATOR,",
            "                )",
            "            except jsonschema.ValidationError as e:",
            "                if e.path:",
            "                    # example: \"users_default\": '0' is not of type 'integer'",
            "                    # cast safety: path entries can be integers, if we fail to validate",
            "                    # items in an array. However, the POWER_LEVELS_SCHEMA doesn't expect",
            "                    # to see any arrays.",
            "                    message = (",
            "                        '\"' + cast(str, e.path[-1]) + '\": ' + e.message  # noqa: B306",
            "                    )",
            "                    # jsonschema.ValidationError.message is a valid attribute",
            "                else:",
            "                    # example: '0' is not of type 'integer'",
            "                    message = e.message  # noqa: B306",
            "                    # jsonschema.ValidationError.message is a valid attribute",
            "",
            "                raise SynapseError(",
            "                    code=400,",
            "                    msg=message,",
            "                    errcode=Codes.BAD_JSON,",
            "                )",
            "",
            "        # If the event contains a mentions key, validate it.",
            "        if EventContentFields.MENTIONS in event.content:",
            "            validate_json_object(event.content[EventContentFields.MENTIONS], Mentions)",
            "",
            "    def _validate_retention(self, event: EventBase) -> None:",
            "        \"\"\"Checks that an event that defines the retention policy for a room respects the",
            "        format enforced by the spec.",
            "",
            "        Args:",
            "            event: The event to validate.",
            "        \"\"\"",
            "        if not event.is_state():",
            "            raise SynapseError(code=400, msg=\"must be a state event\")",
            "",
            "        min_lifetime = event.content.get(\"min_lifetime\")",
            "        max_lifetime = event.content.get(\"max_lifetime\")",
            "",
            "        if min_lifetime is not None:",
            "            if type(min_lifetime) is not int:  # noqa: E721",
            "                raise SynapseError(",
            "                    code=400,",
            "                    msg=\"'min_lifetime' must be an integer\",",
            "                    errcode=Codes.BAD_JSON,",
            "                )",
            "",
            "        if max_lifetime is not None:",
            "            if type(max_lifetime) is not int:  # noqa: E721",
            "                raise SynapseError(",
            "                    code=400,",
            "                    msg=\"'max_lifetime' must be an integer\",",
            "                    errcode=Codes.BAD_JSON,",
            "                )",
            "",
            "        if (",
            "            min_lifetime is not None",
            "            and max_lifetime is not None",
            "            and min_lifetime > max_lifetime",
            "        ):",
            "            raise SynapseError(",
            "                code=400,",
            "                msg=\"'min_lifetime' can't be greater than 'max_lifetime\",",
            "                errcode=Codes.BAD_JSON,",
            "            )",
            "",
            "    def validate_builder(self, event: Union[EventBase, EventBuilder]) -> None:",
            "        \"\"\"Validates that the builder/event has roughly the right format. Only",
            "        checks values that we expect a proto event to have, rather than all the",
            "        fields an event would have",
            "        \"\"\"",
            "",
            "        strings = [\"room_id\", \"sender\", \"type\"]",
            "",
            "        if hasattr(event, \"state_key\"):",
            "            strings.append(\"state_key\")",
            "",
            "        for s in strings:",
            "            if not isinstance(getattr(event, s), str):",
            "                raise SynapseError(400, \"Not '%s' a string type\" % (s,))",
            "",
            "        RoomID.from_string(event.room_id)",
            "        UserID.from_string(event.sender)",
            "",
            "        if event.type == EventTypes.Message:",
            "            strings = [\"body\", \"msgtype\"]",
            "",
            "            self._ensure_strings(event.content, strings)",
            "",
            "        elif event.type == EventTypes.Topic:",
            "            self._ensure_strings(event.content, [\"topic\"])",
            "            self._ensure_state_event(event)",
            "        elif event.type == EventTypes.Name:",
            "            self._ensure_strings(event.content, [\"name\"])",
            "            self._ensure_state_event(event)",
            "        elif event.type == EventTypes.Member:",
            "            if \"membership\" not in event.content:",
            "                raise SynapseError(400, \"Content has not membership key\")",
            "",
            "            if event.content[\"membership\"] not in Membership.LIST:",
            "                raise SynapseError(400, \"Invalid membership key\")",
            "",
            "            self._ensure_state_event(event)",
            "        elif event.type == EventTypes.Tombstone:",
            "            if \"replacement_room\" not in event.content:",
            "                raise SynapseError(400, \"Content has no replacement_room key\")",
            "",
            "            if event.content[\"replacement_room\"] == event.room_id:",
            "                raise SynapseError(",
            "                    400, \"Tombstone cannot reference the room it was sent in\"",
            "                )",
            "",
            "            self._ensure_state_event(event)",
            "",
            "    def _ensure_strings(self, d: JsonDict, keys: StrCollection) -> None:",
            "        for s in keys:",
            "            if s not in d:",
            "                raise SynapseError(400, \"'%s' not in content\" % (s,))",
            "            if not isinstance(d[s], str):",
            "                raise SynapseError(400, \"'%s' not a string type\" % (s,))",
            "",
            "    def _ensure_state_event(self, event: Union[EventBase, EventBuilder]) -> None:",
            "        if not event.is_state():",
            "            raise SynapseError(400, \"'%s' must be state events\" % (event.type,))",
            "",
            "",
            "POWER_LEVELS_SCHEMA = {",
            "    \"type\": \"object\",",
            "    \"properties\": {",
            "        \"ban\": {\"$ref\": \"#/definitions/int\"},",
            "        \"events\": {\"$ref\": \"#/definitions/objectOfInts\"},",
            "        \"events_default\": {\"$ref\": \"#/definitions/int\"},",
            "        \"invite\": {\"$ref\": \"#/definitions/int\"},",
            "        \"kick\": {\"$ref\": \"#/definitions/int\"},",
            "        \"notifications\": {\"$ref\": \"#/definitions/objectOfInts\"},",
            "        \"redact\": {\"$ref\": \"#/definitions/int\"},",
            "        \"state_default\": {\"$ref\": \"#/definitions/int\"},",
            "        \"users\": {\"$ref\": \"#/definitions/objectOfInts\"},",
            "        \"users_default\": {\"$ref\": \"#/definitions/int\"},",
            "    },",
            "    \"definitions\": {",
            "        \"int\": {",
            "            \"type\": \"integer\",",
            "            \"minimum\": CANONICALJSON_MIN_INT,",
            "            \"maximum\": CANONICALJSON_MAX_INT,",
            "        },",
            "        \"objectOfInts\": {",
            "            \"type\": \"object\",",
            "            \"additionalProperties\": {\"$ref\": \"#/definitions/int\"},",
            "        },",
            "    },",
            "}",
            "",
            "",
            "class Mentions(RequestBodyModel):",
            "    user_ids: List[StrictStr] = Field(default_factory=list)",
            "    room: StrictBool = False",
            "",
            "",
            "# This could return something newer than Draft 7, but that's the current \"latest\"",
            "# validator.",
            "def _create_validator(schema: JsonDict) -> Type[jsonschema.Draft7Validator]:",
            "    validator = jsonschema.validators.validator_for(schema)",
            "",
            "    # by default jsonschema does not consider a immutabledict to be an object so",
            "    # we need to use a custom type checker",
            "    # https://python-jsonschema.readthedocs.io/en/stable/validate/?highlight=object#validating-with-additional-types",
            "    type_checker = validator.TYPE_CHECKER.redefine(",
            "        \"object\", lambda checker, thing: isinstance(thing, collections.abc.Mapping)",
            "    )",
            "",
            "    return jsonschema.validators.extend(validator, type_checker=type_checker)",
            "",
            "",
            "POWER_LEVELS_VALIDATOR = _create_validator(POWER_LEVELS_SCHEMA)"
        ],
        "afterPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import collections.abc",
            "from typing import TYPE_CHECKING, List, Type, Union, cast",
            "",
            "import jsonschema",
            "",
            "from synapse._pydantic_compat import HAS_PYDANTIC_V2",
            "",
            "if TYPE_CHECKING or HAS_PYDANTIC_V2:",
            "    from pydantic.v1 import Field, StrictBool, StrictStr",
            "else:",
            "    from pydantic import Field, StrictBool, StrictStr",
            "",
            "from synapse.api.constants import (",
            "    MAX_ALIAS_LENGTH,",
            "    EventContentFields,",
            "    EventTypes,",
            "    Membership,",
            ")",
            "from synapse.api.errors import Codes, SynapseError",
            "from synapse.api.room_versions import EventFormatVersions",
            "from synapse.config.homeserver import HomeServerConfig",
            "from synapse.events import EventBase",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.utils import (",
            "    CANONICALJSON_MAX_INT,",
            "    CANONICALJSON_MIN_INT,",
            "    validate_canonicaljson,",
            ")",
            "from synapse.http.servlet import validate_json_object",
            "from synapse.rest.models import RequestBodyModel",
            "from synapse.storage.controllers.state import server_acl_evaluator_from_event",
            "from synapse.types import EventID, JsonDict, RoomID, StrCollection, UserID",
            "",
            "",
            "class EventValidator:",
            "    def validate_new(self, event: EventBase, config: HomeServerConfig) -> None:",
            "        \"\"\"Validates the event has roughly the right format",
            "",
            "        Suitable for checking a locally-created event. It has stricter checks than",
            "        is appropriate for an event received over federation (for which, see",
            "        event_auth.validate_event_for_room_version)",
            "",
            "        Args:",
            "            event: The event to validate.",
            "            config: The homeserver's configuration.",
            "        \"\"\"",
            "        self.validate_builder(event)",
            "",
            "        if event.format_version == EventFormatVersions.ROOM_V1_V2:",
            "            EventID.from_string(event.event_id)",
            "",
            "        required = [",
            "            \"auth_events\",",
            "            \"content\",",
            "            \"hashes\",",
            "            \"origin\",",
            "            \"prev_events\",",
            "            \"sender\",",
            "            \"type\",",
            "        ]",
            "",
            "        for k in required:",
            "            if k not in event:",
            "                raise SynapseError(400, \"Event does not have key %s\" % (k,))",
            "",
            "        # Check that the following keys have string values",
            "        event_strings = [\"origin\"]",
            "",
            "        for s in event_strings:",
            "            if not isinstance(getattr(event, s), str):",
            "                raise SynapseError(400, \"'%s' not a string type\" % (s,))",
            "",
            "        # Depending on the room version, ensure the data is spec compliant JSON.",
            "        if event.room_version.strict_canonicaljson:",
            "            # Note that only the client controlled portion of the event is",
            "            # checked, since we trust the portions of the event we created.",
            "            validate_canonicaljson(event.content)",
            "",
            "        if event.type == EventTypes.Aliases:",
            "            if \"aliases\" in event.content:",
            "                for alias in event.content[\"aliases\"]:",
            "                    if len(alias) > MAX_ALIAS_LENGTH:",
            "                        raise SynapseError(",
            "                            400,",
            "                            (",
            "                                \"Can't create aliases longer than\"",
            "                                \" %d characters\" % (MAX_ALIAS_LENGTH,)",
            "                            ),",
            "                            Codes.INVALID_PARAM,",
            "                        )",
            "",
            "        elif event.type == EventTypes.Retention:",
            "            self._validate_retention(event)",
            "",
            "        elif event.type == EventTypes.ServerACL:",
            "            server_acl_evaluator = server_acl_evaluator_from_event(event)",
            "            if not server_acl_evaluator.server_matches_acl_event(",
            "                config.server.server_name",
            "            ):",
            "                raise SynapseError(",
            "                    400, \"Can't create an ACL event that denies the local server\"",
            "                )",
            "",
            "        elif event.type == EventTypes.PowerLevels:",
            "            try:",
            "                jsonschema.validate(",
            "                    instance=event.content,",
            "                    schema=POWER_LEVELS_SCHEMA,",
            "                    cls=POWER_LEVELS_VALIDATOR,",
            "                )",
            "            except jsonschema.ValidationError as e:",
            "                if e.path:",
            "                    # example: \"users_default\": '0' is not of type 'integer'",
            "                    # cast safety: path entries can be integers, if we fail to validate",
            "                    # items in an array. However, the POWER_LEVELS_SCHEMA doesn't expect",
            "                    # to see any arrays.",
            "                    message = (",
            "                        '\"' + cast(str, e.path[-1]) + '\": ' + e.message  # noqa: B306",
            "                    )",
            "                    # jsonschema.ValidationError.message is a valid attribute",
            "                else:",
            "                    # example: '0' is not of type 'integer'",
            "                    message = e.message  # noqa: B306",
            "                    # jsonschema.ValidationError.message is a valid attribute",
            "",
            "                raise SynapseError(",
            "                    code=400,",
            "                    msg=message,",
            "                    errcode=Codes.BAD_JSON,",
            "                )",
            "",
            "        # If the event contains a mentions key, validate it.",
            "        if EventContentFields.MENTIONS in event.content:",
            "            validate_json_object(event.content[EventContentFields.MENTIONS], Mentions)",
            "",
            "    def _validate_retention(self, event: EventBase) -> None:",
            "        \"\"\"Checks that an event that defines the retention policy for a room respects the",
            "        format enforced by the spec.",
            "",
            "        Args:",
            "            event: The event to validate.",
            "        \"\"\"",
            "        if not event.is_state():",
            "            raise SynapseError(code=400, msg=\"must be a state event\")",
            "",
            "        min_lifetime = event.content.get(\"min_lifetime\")",
            "        max_lifetime = event.content.get(\"max_lifetime\")",
            "",
            "        if min_lifetime is not None:",
            "            if type(min_lifetime) is not int:  # noqa: E721",
            "                raise SynapseError(",
            "                    code=400,",
            "                    msg=\"'min_lifetime' must be an integer\",",
            "                    errcode=Codes.BAD_JSON,",
            "                )",
            "",
            "        if max_lifetime is not None:",
            "            if type(max_lifetime) is not int:  # noqa: E721",
            "                raise SynapseError(",
            "                    code=400,",
            "                    msg=\"'max_lifetime' must be an integer\",",
            "                    errcode=Codes.BAD_JSON,",
            "                )",
            "",
            "        if (",
            "            min_lifetime is not None",
            "            and max_lifetime is not None",
            "            and min_lifetime > max_lifetime",
            "        ):",
            "            raise SynapseError(",
            "                code=400,",
            "                msg=\"'min_lifetime' can't be greater than 'max_lifetime\",",
            "                errcode=Codes.BAD_JSON,",
            "            )",
            "",
            "    def validate_builder(self, event: Union[EventBase, EventBuilder]) -> None:",
            "        \"\"\"Validates that the builder/event has roughly the right format. Only",
            "        checks values that we expect a proto event to have, rather than all the",
            "        fields an event would have",
            "        \"\"\"",
            "",
            "        strings = [\"room_id\", \"sender\", \"type\"]",
            "",
            "        if hasattr(event, \"state_key\"):",
            "            strings.append(\"state_key\")",
            "",
            "        for s in strings:",
            "            if not isinstance(getattr(event, s), str):",
            "                raise SynapseError(400, \"Not '%s' a string type\" % (s,))",
            "",
            "        RoomID.from_string(event.room_id)",
            "        UserID.from_string(event.sender)",
            "",
            "        if event.type == EventTypes.Message:",
            "            strings = [\"body\", \"msgtype\"]",
            "",
            "            self._ensure_strings(event.content, strings)",
            "",
            "        elif event.type == EventTypes.Topic:",
            "            self._ensure_strings(event.content, [\"topic\"])",
            "            self._ensure_state_event(event)",
            "        elif event.type == EventTypes.Name:",
            "            self._ensure_strings(event.content, [\"name\"])",
            "            self._ensure_state_event(event)",
            "        elif event.type == EventTypes.Member:",
            "            if \"membership\" not in event.content:",
            "                raise SynapseError(400, \"Content has not membership key\")",
            "",
            "            if event.content[\"membership\"] not in Membership.LIST:",
            "                raise SynapseError(400, \"Invalid membership key\")",
            "",
            "            self._ensure_state_event(event)",
            "        elif event.type == EventTypes.Tombstone:",
            "            if \"replacement_room\" not in event.content:",
            "                raise SynapseError(400, \"Content has no replacement_room key\")",
            "",
            "            if event.content[\"replacement_room\"] == event.room_id:",
            "                raise SynapseError(",
            "                    400, \"Tombstone cannot reference the room it was sent in\"",
            "                )",
            "",
            "            self._ensure_state_event(event)",
            "",
            "    def _ensure_strings(self, d: JsonDict, keys: StrCollection) -> None:",
            "        for s in keys:",
            "            if s not in d:",
            "                raise SynapseError(400, \"'%s' not in content\" % (s,))",
            "            if not isinstance(d[s], str):",
            "                raise SynapseError(400, \"'%s' not a string type\" % (s,))",
            "",
            "    def _ensure_state_event(self, event: Union[EventBase, EventBuilder]) -> None:",
            "        if not event.is_state():",
            "            raise SynapseError(400, \"'%s' must be state events\" % (event.type,))",
            "",
            "",
            "POWER_LEVELS_SCHEMA = {",
            "    \"type\": \"object\",",
            "    \"properties\": {",
            "        \"ban\": {\"$ref\": \"#/definitions/int\"},",
            "        \"events\": {\"$ref\": \"#/definitions/objectOfInts\"},",
            "        \"events_default\": {\"$ref\": \"#/definitions/int\"},",
            "        \"invite\": {\"$ref\": \"#/definitions/int\"},",
            "        \"kick\": {\"$ref\": \"#/definitions/int\"},",
            "        \"notifications\": {\"$ref\": \"#/definitions/objectOfInts\"},",
            "        \"redact\": {\"$ref\": \"#/definitions/int\"},",
            "        \"state_default\": {\"$ref\": \"#/definitions/int\"},",
            "        \"users\": {\"$ref\": \"#/definitions/objectOfInts\"},",
            "        \"users_default\": {\"$ref\": \"#/definitions/int\"},",
            "    },",
            "    \"definitions\": {",
            "        \"int\": {",
            "            \"type\": \"integer\",",
            "            \"minimum\": CANONICALJSON_MIN_INT,",
            "            \"maximum\": CANONICALJSON_MAX_INT,",
            "        },",
            "        \"objectOfInts\": {",
            "            \"type\": \"object\",",
            "            \"additionalProperties\": {\"$ref\": \"#/definitions/int\"},",
            "        },",
            "    },",
            "}",
            "",
            "",
            "class Mentions(RequestBodyModel):",
            "    user_ids: List[StrictStr] = Field(default_factory=list)",
            "    room: StrictBool = False",
            "",
            "",
            "# This could return something newer than Draft 7, but that's the current \"latest\"",
            "# validator.",
            "def _create_validator(schema: JsonDict) -> Type[jsonschema.Draft7Validator]:",
            "    validator = jsonschema.validators.validator_for(schema)",
            "",
            "    # by default jsonschema does not consider a immutabledict to be an object so",
            "    # we need to use a custom type checker",
            "    # https://python-jsonschema.readthedocs.io/en/stable/validate/?highlight=object#validating-with-additional-types",
            "    type_checker = validator.TYPE_CHECKER.redefine(",
            "        \"object\", lambda checker, thing: isinstance(thing, collections.abc.Mapping)",
            "    )",
            "",
            "    return jsonschema.validators.extend(validator, type_checker=type_checker)",
            "",
            "",
            "POWER_LEVELS_VALIDATOR = _create_validator(POWER_LEVELS_SCHEMA)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "42": [],
            "109": [
                "EventValidator",
                "validate_new"
            ]
        },
        "addLocation": []
    },
    "synapse/federation/federation_server.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": "     Union,"
            },
            "1": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " )"
            },
            "2": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from matrix_common.regex import glob_to_regex"
            },
            "4": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from prometheus_client import Counter, Gauge, Histogram"
            },
            "5": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from twisted.internet.abstract import isIPAddress"
            },
            "7": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from twisted.python import failure"
            },
            "8": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from synapse.api.constants import ("
            },
            "10": {
                "beforePatchRowNumber": 1324,
                "afterPatchRowNumber": 1322,
                "PatchRowcode": "         Raises:"
            },
            "11": {
                "beforePatchRowNumber": 1325,
                "afterPatchRowNumber": 1323,
                "PatchRowcode": "             AuthError if the server does not match the ACL"
            },
            "12": {
                "beforePatchRowNumber": 1326,
                "afterPatchRowNumber": 1324,
                "PatchRowcode": "         \"\"\""
            },
            "13": {
                "beforePatchRowNumber": 1327,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        acl_event = await self._storage_controllers.state.get_current_state_event("
            },
            "14": {
                "beforePatchRowNumber": 1328,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            room_id, EventTypes.ServerACL, \"\""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1325,
                "PatchRowcode": "+        server_acl_evaluator = ("
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1326,
                "PatchRowcode": "+            await self._storage_controllers.state.get_server_acl_for_room(room_id)"
            },
            "17": {
                "beforePatchRowNumber": 1329,
                "afterPatchRowNumber": 1327,
                "PatchRowcode": "         )"
            },
            "18": {
                "beforePatchRowNumber": 1330,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not acl_event or server_matches_acl_event(server_name, acl_event):"
            },
            "19": {
                "beforePatchRowNumber": 1331,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return"
            },
            "20": {
                "beforePatchRowNumber": 1332,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "21": {
                "beforePatchRowNumber": 1333,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        raise AuthError(code=403, msg=\"Server is banned from room\")"
            },
            "22": {
                "beforePatchRowNumber": 1334,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "23": {
                "beforePatchRowNumber": 1335,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "24": {
                "beforePatchRowNumber": 1336,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:"
            },
            "25": {
                "beforePatchRowNumber": 1337,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\"Check if the given server is allowed by the ACL event"
            },
            "26": {
                "beforePatchRowNumber": 1338,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "27": {
                "beforePatchRowNumber": 1339,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Args:"
            },
            "28": {
                "beforePatchRowNumber": 1340,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        server_name: name of server, without any port part"
            },
            "29": {
                "beforePatchRowNumber": 1341,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        acl_event: m.room.server_acl event"
            },
            "30": {
                "beforePatchRowNumber": 1342,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "31": {
                "beforePatchRowNumber": 1343,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    Returns:"
            },
            "32": {
                "beforePatchRowNumber": 1344,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        True if this server is allowed by the ACLs"
            },
            "33": {
                "beforePatchRowNumber": 1345,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    \"\"\""
            },
            "34": {
                "beforePatchRowNumber": 1346,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)"
            },
            "35": {
                "beforePatchRowNumber": 1347,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "36": {
                "beforePatchRowNumber": 1348,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # first of all, check if literal IPs are blocked, and if so, whether the"
            },
            "37": {
                "beforePatchRowNumber": 1349,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # server name is a literal IP"
            },
            "38": {
                "beforePatchRowNumber": 1350,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)"
            },
            "39": {
                "beforePatchRowNumber": 1351,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not isinstance(allow_ip_literals, bool):"
            },
            "40": {
                "beforePatchRowNumber": 1352,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")"
            },
            "41": {
                "beforePatchRowNumber": 1353,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        allow_ip_literals = True"
            },
            "42": {
                "beforePatchRowNumber": 1354,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not allow_ip_literals:"
            },
            "43": {
                "beforePatchRowNumber": 1355,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # check for ipv6 literals. These start with '['."
            },
            "44": {
                "beforePatchRowNumber": 1356,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if server_name[0] == \"[\":"
            },
            "45": {
                "beforePatchRowNumber": 1357,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return False"
            },
            "46": {
                "beforePatchRowNumber": 1358,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "47": {
                "beforePatchRowNumber": 1359,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # check for ipv4 literals. We can just lift the routine from twisted."
            },
            "48": {
                "beforePatchRowNumber": 1360,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if isIPAddress(server_name):"
            },
            "49": {
                "beforePatchRowNumber": 1361,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return False"
            },
            "50": {
                "beforePatchRowNumber": 1362,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "51": {
                "beforePatchRowNumber": 1363,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # next,  check the deny list"
            },
            "52": {
                "beforePatchRowNumber": 1364,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    deny = acl_event.content.get(\"deny\", [])"
            },
            "53": {
                "beforePatchRowNumber": 1365,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not isinstance(deny, (list, tuple)):"
            },
            "54": {
                "beforePatchRowNumber": 1366,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        logger.warning(\"Ignoring non-list deny ACL %s\", deny)"
            },
            "55": {
                "beforePatchRowNumber": 1367,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        deny = []"
            },
            "56": {
                "beforePatchRowNumber": 1368,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for e in deny:"
            },
            "57": {
                "beforePatchRowNumber": 1369,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if _acl_entry_matches(server_name, e):"
            },
            "58": {
                "beforePatchRowNumber": 1370,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # logger.info(\"%s matched deny rule %s\", server_name, e)"
            },
            "59": {
                "beforePatchRowNumber": 1371,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return False"
            },
            "60": {
                "beforePatchRowNumber": 1372,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "61": {
                "beforePatchRowNumber": 1373,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # then the allow list."
            },
            "62": {
                "beforePatchRowNumber": 1374,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    allow = acl_event.content.get(\"allow\", [])"
            },
            "63": {
                "beforePatchRowNumber": 1375,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not isinstance(allow, (list, tuple)):"
            },
            "64": {
                "beforePatchRowNumber": 1376,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        logger.warning(\"Ignoring non-list allow ACL %s\", allow)"
            },
            "65": {
                "beforePatchRowNumber": 1377,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        allow = []"
            },
            "66": {
                "beforePatchRowNumber": 1378,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    for e in allow:"
            },
            "67": {
                "beforePatchRowNumber": 1379,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if _acl_entry_matches(server_name, e):"
            },
            "68": {
                "beforePatchRowNumber": 1380,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # logger.info(\"%s matched allow rule %s\", server_name, e)"
            },
            "69": {
                "beforePatchRowNumber": 1381,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return True"
            },
            "70": {
                "beforePatchRowNumber": 1382,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "71": {
                "beforePatchRowNumber": 1383,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # everything else should be rejected."
            },
            "72": {
                "beforePatchRowNumber": 1384,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    # logger.info(\"%s fell through\", server_name)"
            },
            "73": {
                "beforePatchRowNumber": 1385,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return False"
            },
            "74": {
                "beforePatchRowNumber": 1386,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "75": {
                "beforePatchRowNumber": 1387,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "76": {
                "beforePatchRowNumber": 1388,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:"
            },
            "77": {
                "beforePatchRowNumber": 1389,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    if not isinstance(acl_entry, str):"
            },
            "78": {
                "beforePatchRowNumber": 1390,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        logger.warning("
            },
            "79": {
                "beforePatchRowNumber": 1391,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)"
            },
            "80": {
                "beforePatchRowNumber": 1392,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "81": {
                "beforePatchRowNumber": 1393,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return False"
            },
            "82": {
                "beforePatchRowNumber": 1394,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    regex = glob_to_regex(acl_entry)"
            },
            "83": {
                "beforePatchRowNumber": 1395,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return bool(regex.match(server_name))"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1328,
                "PatchRowcode": "+        if server_acl_evaluator and not server_acl_evaluator.server_matches_acl_event("
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1329,
                "PatchRowcode": "+            server_name"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1330,
                "PatchRowcode": "+        ):"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1331,
                "PatchRowcode": "+            raise AuthError(code=403, msg=\"Server is banned from room\")"
            },
            "88": {
                "beforePatchRowNumber": 1396,
                "afterPatchRowNumber": 1332,
                "PatchRowcode": " "
            },
            "89": {
                "beforePatchRowNumber": 1397,
                "afterPatchRowNumber": 1333,
                "PatchRowcode": " "
            },
            "90": {
                "beforePatchRowNumber": 1398,
                "afterPatchRowNumber": 1334,
                "PatchRowcode": " class FederationHandlerRegistry:"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019-2021 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from matrix_common.regex import glob_to_regex",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.internet.abstract import isIPAddress",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import (",
            "    Direction,",
            "    EduTypes,",
            "    EventContentFields,",
            "    EventTypes,",
            "    Membership,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_base import (",
            "    FederationBase,",
            "    InvalidEventSignatureError,",
            "    event_from_pdu_json,",
            ")",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.handlers.worker_lock import NEW_EVENT_DURING_PURGE_LOCK_NAME",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    log_kv,",
            "    set_tag,",
            "    start_active_span_from_edu,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import wrap_as_background_process",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.storage.databases.main.lock import Lock",
            "from synapse.storage.databases.main.roommember import extract_heroes_from_room_summary",
            "from synapse.storage.roommember import MemberSummary",
            "from synapse.types import JsonDict, StateMap, get_domain_from_id",
            "from synapse.util import unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute, gather_results",
            "from synapse.util.caches.response_cache import ResponseCache",
            "from synapse.util.stringutils import parse_server_name",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\",",
            "    \"Time taken to process an event\",",
            ")",
            "",
            "last_pdu_ts_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_time\",",
            "    \"The timestamp of the last PDU which was successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "# The name of the lock to use when process events in a room received over",
            "# federation.",
            "_INBOUND_EVENT_HANDLING_LOCK_NAME = \"federation_inbound_pdu\"",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.server_name = hs.hostname",
            "        self.handler = hs.get_federation_handler()",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self.state = hs.get_state_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._room_member_handler = hs.get_room_member_handler()",
            "        self._e2e_keys_handler = hs.get_e2e_keys_handler()",
            "        self._worker_lock_handler = hs.get_worker_locks_handler()",
            "",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "",
            "        # origins that we are currently processing a transaction from.",
            "        # a dict from origin to txn id.",
            "        self._active_transactions: Dict[str, str] = {}",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"fed_txn_handler\", timeout_ms=30000",
            "        )",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache: ResponseCache[",
            "            Tuple[str, Optional[str]]",
            "        ] = ResponseCache(hs.get_clock(), \"state_resp\", timeout_ms=30000)",
            "        self._state_ids_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"state_ids_resp\", timeout_ms=30000",
            "        )",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.config.federation.federation_metrics_domains",
            "        )",
            "",
            "        self._room_prejoin_state_types = hs.config.api.room_prejoin_state",
            "",
            "        # Whether we have started handling old events in the staging area.",
            "        self._started_handling_of_staged_events = False",
            "",
            "    @wrap_as_background_process(\"_handle_old_staged_events\")",
            "    async def _handle_old_staged_events(self) -> None:",
            "        \"\"\"Handle old staged events by fetching all rooms that have staged",
            "        events and start the processing of each of those rooms.",
            "        \"\"\"",
            "",
            "        # Get all the rooms IDs with staged events.",
            "        room_ids = await self.store.get_all_rooms_with_staged_incoming_events()",
            "",
            "        # We then shuffle them so that if there are multiple instances doing",
            "        # this work they're less likely to collide.",
            "        random.shuffle(room_ids)",
            "",
            "        for room_id in room_ids:",
            "            room_version = await self.store.get_room_version(room_id)",
            "",
            "            # Try and acquire the processing lock for the room, if we get it start a",
            "            # background process for handling the events in the room.",
            "            lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if lock:",
            "                logger.info(\"Handling old staged inbound events in %s\", room_id)",
            "                self._process_incoming_pdus_in_room_inner(",
            "                    room_id,",
            "                    room_version,",
            "                    lock,",
            "                )",
            "",
            "            # We pause a bit so that we don't start handling all rooms at once.",
            "            await self._clock.sleep(random.uniform(0, 0.1))",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_dict_from_pdus(pdus)",
            "",
            "        return 200, res",
            "",
            "    async def on_timestamp_to_event_request(",
            "        self, origin: str, room_id: str, timestamp: int, direction: Direction",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"When we receive a federated `/timestamp_to_event` request,",
            "        handle all of the logic for validating and fetching the event.",
            "",
            "        Args:",
            "            origin: The server we received the event from",
            "            room_id: Room to fetch the event from",
            "            timestamp: The point in time (inclusive) we should navigate from in",
            "                the given direction to find the closest event.",
            "            direction: indicates whether we should navigate forward",
            "                or backward from the given timestamp to find the closest event.",
            "",
            "        Returns:",
            "            Tuple indicating the response status code and dictionary response",
            "            body including `event_id`.",
            "        \"\"\"",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            # We only try to fetch data from the local database",
            "            event_id = await self.store.get_event_id_for_timestamp(",
            "                room_id, timestamp, direction",
            "            )",
            "            if event_id:",
            "                event = await self.store.get_event(",
            "                    event_id, allow_none=False, allow_rejected=False",
            "                )",
            "",
            "                return 200, {",
            "                    \"event_id\": event_id,",
            "                    \"origin_server_ts\": event.origin_server_ts,",
            "                }",
            "",
            "        raise SynapseError(",
            "            404,",
            "            \"Unable to find event from %s in direction %s\" % (timestamp, direction),",
            "            errcode=Codes.NOT_FOUND,",
            "        )",
            "",
            "    async def on_incoming_transaction(",
            "        self,",
            "        origin: str,",
            "        transaction_id: str,",
            "        destination: str,",
            "        transaction_data: JsonDict,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # If we receive a transaction we should make sure that kick off handling",
            "        # any old events in the staging area.",
            "        if not self._started_handling_of_staged_events:",
            "            self._started_handling_of_staged_events = True",
            "            self._handle_old_staged_events()",
            "",
            "            # Start a periodic check for old staged events. This is to handle",
            "            # the case where locks time out, e.g. if another process gets killed",
            "            # without dropping its locks.",
            "            self._clock.looping_call(self._handle_old_staged_events, 60 * 1000)",
            "",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(",
            "            transaction_id=transaction_id,",
            "            destination=destination,",
            "            origin=origin,",
            "            origin_server_ts=transaction_data.get(\"origin_server_ts\"),  # type: ignore[arg-type]",
            "            pdus=transaction_data.get(\"pdus\"),",
            "            edus=transaction_data.get(\"edus\"),",
            "        )",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # Reject malformed transactions early: reject if too many PDUs/EDUs",
            "        if len(transaction.pdus) > 50 or len(transaction.edus) > 100:",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "            return 400, {}",
            "",
            "        # we only process one transaction from each origin at a time. We need to do",
            "        # this check here, rather than in _on_incoming_transaction_inner so that we",
            "        # don't cache the rejection in _transaction_resp_cache (so that if the txn",
            "        # arrives again later, we can process it).",
            "        current_transaction = self._active_transactions.get(origin)",
            "        if current_transaction and current_transaction != transaction_id:",
            "            logger.warning(",
            "                \"Received another txn %s from %s while still processing %s\",",
            "                transaction_id,",
            "                origin,",
            "                current_transaction,",
            "            )",
            "            return 429, {",
            "                \"errcode\": Codes.UNKNOWN,",
            "                \"error\": \"Too many concurrent transactions\",",
            "            }",
            "",
            "        # CRITICAL SECTION: we must now not await until we populate _active_transactions",
            "        # in _on_incoming_transaction_inner.",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # CRITICAL SECTION: the first thing we must do (before awaiting) is",
            "        # add an entry to _active_transactions.",
            "        assert origin not in self._active_transactions",
            "        self._active_transactions[origin] = transaction.transaction_id",
            "",
            "        try:",
            "            result = await self._handle_incoming_transaction(",
            "                origin, transaction, request_time",
            "            )",
            "            return result",
            "        finally:",
            "            del self._active_transactions[origin]",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        existing_response = await self.transaction_actions.have_responded(",
            "            origin, transaction",
            "        )",
            "",
            "        if existing_response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,",
            "            )",
            "            return existing_response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            gather_results(",
            "                (",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ),",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room: Dict[str, List[EventBase]] = {}",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str) -> None:",
            "            with nested_logging_context(room_id):",
            "                logger.debug(\"Processing PDUs for %s\", room_id)",
            "",
            "                try:",
            "                    await self.check_server_matches_acl(origin_host, room_id)",
            "                except AuthError as e:",
            "                    logger.warning(",
            "                        \"Ignoring PDUs for room %s from banned server\", room_id",
            "                    )",
            "                    for pdu in pdus_by_room[room_id]:",
            "                        event_id = pdu.event_id",
            "                        pdu_results[event_id] = e.error_dict(self.hs.config)",
            "                    return",
            "",
            "                for pdu in pdus_by_room[room_id]:",
            "                    pdu_results[pdu.event_id] = await process_pdu(pdu)",
            "",
            "        async def process_pdu(pdu: EventBase) -> JsonDict:",
            "            \"\"\"",
            "            Processes a pushed PDU sent to us via a `/send` transaction",
            "",
            "            Returns:",
            "                JsonDict representing a \"PDU Processing Result\" that will be bundled up",
            "                with the other processed PDU's in the `/send` transaction and sent back",
            "                to remote homeserver.",
            "            \"\"\"",
            "            event_id = pdu.event_id",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    await self._handle_received_pdu(origin, pdu)",
            "                    return {}",
            "                except FederationError as e:",
            "                    logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                    return {\"error\": str(e)}",
            "                except Exception as e:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "                    return {\"error\": str(e)}",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            last_pdu_ts_metric.labels(server_name=origin).set(newest_pdu_ts / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction) -> None:",
            "        \"\"\"Process the EDUs in a received transaction.\"\"\"",
            "",
            "        async def _process_edu(edu_dict: JsonDict) -> None:",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            transaction.edus,",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            resp = await self._state_resp_cache.wrap(",
            "                (room_id, event_id),",
            "                self._on_context_state_request_compute,",
            "                room_id,",
            "                event_id,",
            "            )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id),",
            "            self._on_state_ids_request_compute,",
            "            room_id,",
            "            event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _on_state_ids_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> JsonDict:",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(room_id, state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": list(auth_chain_ids)}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        pdus: Collection[EventBase]",
            "        event_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        pdus = await self.store.get_events_as_list(event_ids)",
            "",
            "        auth_chain = await self.store.get_auth_chain(",
            "            room_id, [pdu.event_id for pdu in pdus]",
            "        )",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_dict_from_pdus([pdu])",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        # Refuse the request if that room has seen too many joins recently.",
            "        # This is in addition to the HS-level rate limiting applied by",
            "        # BaseFederationServlet.",
            "        # type-ignore: mypy doesn't seem able to deduce the type of the limiter(!?)",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {pdu.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "        caller_supports_partial_state: bool = False,",
            "    ) -> Dict[str, Any]:",
            "        set_tag(",
            "            SynapseTags.SEND_JOIN_RESPONSE_IS_PARTIAL_STATE,",
            "            caller_supports_partial_state,",
            "        )",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "",
            "        event, context = await self._on_send_membership_event(",
            "            origin, content, Membership.JOIN, room_id",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_event_ids: Collection[str]",
            "        servers_in_room: Optional[Collection[str]]",
            "        if caller_supports_partial_state:",
            "            summary = await self.store.get_room_summary(room_id)",
            "            state_event_ids = _get_event_ids_for_partial_state_join(",
            "                event, prev_state_ids, summary",
            "            )",
            "            servers_in_room = await self.state.get_hosts_in_room_at_events(",
            "                room_id, event_ids=event.prev_event_ids()",
            "            )",
            "        else:",
            "            state_event_ids = prev_state_ids.values()",
            "            servers_in_room = None",
            "",
            "        auth_chain_event_ids = await self.store.get_auth_chain_ids(",
            "            room_id, state_event_ids",
            "        )",
            "",
            "        # if the caller has opted in, we can omit any auth_chain events which are",
            "        # already in state_event_ids",
            "        if caller_supports_partial_state:",
            "            auth_chain_event_ids.difference_update(state_event_ids)",
            "",
            "        auth_chain_events = await self.store.get_events_as_list(auth_chain_event_ids)",
            "        state_events = await self.store.get_events_as_list(state_event_ids)",
            "",
            "        # we try to do all the async stuff before this point, so that time_now is as",
            "        # accurate as possible.",
            "        time_now = self._clock.time_msec()",
            "        event_json = event.get_pdu_json(time_now)",
            "        resp = {",
            "            \"event\": event_json,",
            "            \"state\": [p.get_pdu_json(time_now) for p in state_events],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in auth_chain_events],",
            "            \"members_omitted\": caller_supports_partial_state,",
            "        }",
            "",
            "        if servers_in_room is not None:",
            "            resp[\"servers_in_room\"] = list(servers_in_room)",
            "",
            "        return resp",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(",
            "        self, origin: str, content: JsonDict, room_id: str",
            "    ) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "        await self._on_send_membership_event(origin, content, Membership.LEAVE, room_id)",
            "        return {}",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> JsonDict:",
            "        \"\"\"We've received a /make_knock/ request, so we create a partial knock",
            "        event for the room and hand that back, along with the room version, to the knocking",
            "        homeserver. We do *not* persist or process this event until the other server has",
            "        signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "            supported_versions: The room versions supported by the requesting server.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # Before we do anything: check if the room is partial-stated.",
            "            # Note that at the time this check was added, `on_make_knock_request` would",
            "            # block due to https://github.com/matrix-org/synapse/issues/12997.",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_knock right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        # Check that this room version is supported by the remote homeserver",
            "        if room_version.identifier not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version.identifier, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version.identifier)",
            "",
            "        # Check that this room supports knocking as defined by its room version",
            "        if not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        pdu = await self.handler.on_make_knock_request(origin, room_id, user_id)",
            "        return {",
            "            \"event\": pdu.get_templated_pdu_json(),",
            "            \"room_version\": room_version.identifier,",
            "        }",
            "",
            "    async def on_send_knock_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "    ) -> Dict[str, List[JsonDict]]:",
            "        \"\"\"",
            "        We have received a knock event for a room. Verify and send the event into the room",
            "        on the knocking homeserver's behalf. Then reply with some stripped state from the",
            "        room for the knockee.",
            "",
            "        Args:",
            "            origin: The remote homeserver of the knocking user.",
            "            content: The content of the request.",
            "            room_id: The ID of the room to knock on.",
            "",
            "        Returns:",
            "            The stripped room state.",
            "        \"\"\"",
            "        _, context = await self._on_send_membership_event(",
            "            origin, content, Membership.KNOCK, room_id",
            "        )",
            "",
            "        # Retrieve stripped state events from the room and send them back to the remote",
            "        # server. This will allow the remote server's clients to display information",
            "        # related to the room while the knock request is pending.",
            "        stripped_room_state = (",
            "            await self.store.get_stripped_room_state_from_event_context(",
            "                context, self._room_prejoin_state_types",
            "            )",
            "        )",
            "        return {",
            "            \"knock_room_state\": stripped_room_state,",
            "            # Since v1.37, Synapse incorrectly used \"knock_state_events\" for this field.",
            "            # Thus, we also populate a 'knock_state_events' with the same content to",
            "            # support old instances.",
            "            # See https://github.com/matrix-org/synapse/issues/14088.",
            "            \"knock_state_events\": stripped_room_state,",
            "        }",
            "",
            "    async def _on_send_membership_event(",
            "        self, origin: str, content: JsonDict, membership_type: str, room_id: str",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Handle an on_send_{join,leave,knock} request",
            "",
            "        Does some preliminary validation before passing the request on to the",
            "        federation handler.",
            "",
            "        Args:",
            "            origin: The (authenticated) requesting server",
            "            content: The body of the send_* request - a complete membership event",
            "            membership_type: The expected membership type (join or leave, depending",
            "                on the endpoint)",
            "            room_id: The room_id from the request, to be validated against the room_id",
            "                in the event",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            SynapseError if there is a problem with the request, including things like",
            "               the room_id not matching or the event not being authorized.",
            "        \"\"\"",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        if content[\"room_id\"] != room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room ID in body does not match that in request path\",",
            "                Codes.BAD_JSON,",
            "            )",
            "",
            "        # Note that get_room_version throws if the room does not exist here.",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /send_join, /send_knock or /send_leave.",
            "            # This is because we will not be able to provide the server list (for partial",
            "            # joins) or the full state (for full joins).",
            "            # Return a 404 as we would if we weren't in the room at all.",
            "            logger.info(",
            "                f\"Rejecting /send_{membership_type} to %s because it's a partial state room\",",
            "                room_id,",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                f\"Unable to handle /send_{membership_type} right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        if membership_type == Membership.KNOCK and not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        event = event_from_pdu_json(content, room_version)",
            "",
            "        if event.type != EventTypes.Member or not event.is_state():",
            "            raise SynapseError(400, \"Not an m.room.member event\", Codes.BAD_JSON)",
            "",
            "        if event.content.get(\"membership\") != membership_type:",
            "            raise SynapseError(400, \"Not a %s event\" % membership_type, Codes.BAD_JSON)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, event.room_id)",
            "",
            "        logger.debug(\"_on_send_membership_event: pdu sigs: %s\", event.signatures)",
            "",
            "        # Sign the event since we're vouching on behalf of the remote server that",
            "        # the event is valid to be sent into the room. Currently this is only done",
            "        # if the user is being joined via restricted join rules.",
            "        if (",
            "            room_version.restricted_join_rule",
            "            and event.membership == Membership.JOIN",
            "            and EventContentFields.AUTHORISING_USER in event.content",
            "        ):",
            "            # We can only authorise our own users.",
            "            authorising_server = get_domain_from_id(",
            "                event.content[EventContentFields.AUTHORISING_USER]",
            "            )",
            "            if not self._is_mine_server_name(authorising_server):",
            "                raise SynapseError(",
            "                    400,",
            "                    f\"Cannot authorise membership event for {authorising_server}. We can only authorise requests from our own homeserver\",",
            "                )",
            "",
            "            event.signatures.update(",
            "                compute_event_signature(",
            "                    room_version,",
            "                    event.get_pdu_json(),",
            "                    self.hs.hostname,",
            "                    self.hs.signing_key,",
            "                )",
            "            )",
            "",
            "        try:",
            "            event = await self._check_sigs_and_hash(room_version, event)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {event.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "",
            "        try:",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were persisting the event.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated during `on_send_membership_event`, trying again.\",",
            "                room_id,",
            "            )",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, query: List[Tuple[str, str, str, int]], always_include_fallback_keys: bool",
            "    ) -> Dict[str, Any]:",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self._e2e_keys_handler.claim_local_one_time_keys(",
            "            query, always_include_fallback_keys=always_include_fallback_keys",
            "        )",
            "",
            "        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}",
            "        for result in results:",
            "            for user_id, device_keys in result.items():",
            "                for device_id, keys in device_keys.items():",
            "                    for key_id, key in keys.items():",
            "                        json_result.setdefault(user_id, {}).setdefault(device_id, {})[",
            "                            key_id",
            "                        ] = key",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_dict_from_pdus(self, pdu_list: List[EventBase]) -> JsonDict:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            # Just need a dummy transaction ID and destination since it won't be used.",
            "            transaction_id=\"\",",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=\"\",",
            "        ).get_dict()",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            logger.warning(\"event id %s: %s\", pdu.event_id, e)",
            "            raise FederationError(\"ERROR\", 403, str(e), affected=pdu.event_id)",
            "",
            "        if await self._spam_checker_module_callbacks.should_drop_federated_event(pdu):",
            "            logger.warning(",
            "                \"Unstaged federated event contains spam, dropping %s\", pdu.event_id",
            "            )",
            "            return",
            "",
            "        # Add the event to our staging area",
            "        await self.store.insert_received_event_to_staging(origin, pdu)",
            "",
            "        # Try and acquire the processing lock for the room, if we get it start a",
            "        # background process for handling the events in the room.",
            "        lock = await self.store.try_acquire_lock(",
            "            _INBOUND_EVENT_HANDLING_LOCK_NAME, pdu.room_id",
            "        )",
            "        if lock:",
            "            self._process_incoming_pdus_in_room_inner(",
            "                pdu.room_id, room_version, lock, origin, pdu",
            "            )",
            "",
            "    async def _get_next_nonspam_staged_event_for_room(",
            "        self, room_id: str, room_version: RoomVersion",
            "    ) -> Optional[Tuple[str, EventBase]]:",
            "        \"\"\"Fetch the first non-spam event from staging queue.",
            "",
            "        Args:",
            "            room_id: the room to fetch the first non-spam event in.",
            "            room_version: the version of the room.",
            "",
            "        Returns:",
            "            The first non-spam event in that room.",
            "        \"\"\"",
            "",
            "        while True:",
            "            # We need to do this check outside the lock to avoid a race between",
            "            # a new event being inserted by another instance and it attempting",
            "            # to acquire the lock.",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if next is None:",
            "                return None",
            "",
            "            origin, event = next",
            "",
            "            if await self._spam_checker_module_callbacks.should_drop_federated_event(",
            "                event",
            "            ):",
            "                logger.warning(",
            "                    \"Staged federated event contains spam, dropping %s\",",
            "                    event.event_id,",
            "                )",
            "                continue",
            "",
            "            return next",
            "",
            "    @wrap_as_background_process(\"_process_incoming_pdus_in_room_inner\")",
            "    async def _process_incoming_pdus_in_room_inner(",
            "        self,",
            "        room_id: str,",
            "        room_version: RoomVersion,",
            "        lock: Lock,",
            "        latest_origin: Optional[str] = None,",
            "        latest_event: Optional[EventBase] = None,",
            "    ) -> None:",
            "        \"\"\"Process events in the staging area for the given room.",
            "",
            "        The latest_origin and latest_event args are the latest origin and event",
            "        received (or None to simply pull the next event from the database).",
            "        \"\"\"",
            "",
            "        # The common path is for the event we just received be the only event in",
            "        # the room, so instead of pulling the event out of the DB and parsing",
            "        # the event we just pull out the next event ID and check if that matches.",
            "        if latest_event is not None and latest_origin is not None:",
            "            result = await self.store.get_next_staged_event_id_for_room(room_id)",
            "            if result is None:",
            "                latest_origin = None",
            "                latest_event = None",
            "            else:",
            "                next_origin, next_event_id = result",
            "                if (",
            "                    next_origin != latest_origin",
            "                    or next_event_id != latest_event.event_id",
            "                ):",
            "                    latest_origin = None",
            "                    latest_event = None",
            "",
            "        if latest_origin is None or latest_event is None:",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "            if not next:",
            "                await lock.release()",
            "                return",
            "",
            "            origin, event = next",
            "        else:",
            "            origin = latest_origin",
            "            event = latest_event",
            "",
            "        # We loop round until there are no more events in the room in the",
            "        # staging area, or we fail to get the lock (which means another process",
            "        # has started processing).",
            "        while True:",
            "            async with lock:",
            "                logger.info(\"handling received PDU in room %s: %s\", room_id, event)",
            "                try:",
            "                    with nested_logging_context(event.event_id):",
            "                        # We're taking out a lock within a lock, which could",
            "                        # lead to deadlocks if we're not careful. However, it is",
            "                        # safe on this occasion as we only ever take a write",
            "                        # lock when deleting a room, which we would never do",
            "                        # while holding the `_INBOUND_EVENT_HANDLING_LOCK_NAME`",
            "                        # lock.",
            "                        async with self._worker_lock_handler.acquire_read_write_lock(",
            "                            NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "                        ):",
            "                            await self._federation_event_handler.on_receive_pdu(",
            "                                origin, event",
            "                            )",
            "                except FederationError as e:",
            "                    # XXX: Ideally we'd inform the remote we failed to process",
            "                    # the event, but we can't return an error in the transaction",
            "                    # response (as we've already responded).",
            "                    logger.warning(\"Error handling PDU %s: %s\", event.event_id, e)",
            "                except Exception:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event.event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "",
            "                received_ts = await self.store.remove_received_event_from_staging(",
            "                    origin, event.event_id",
            "                )",
            "                if received_ts is not None:",
            "                    pdu_process_time.observe(",
            "                        (self._clock.time_msec() - received_ts) / 1000",
            "                    )",
            "",
            "            next = await self._get_next_nonspam_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if not next:",
            "                break",
            "",
            "            origin, event = next",
            "",
            "            # Prune the event queue if it's getting large.",
            "            #",
            "            # We do this *after* handling the first event as the common case is",
            "            # that the queue is empty (/has the single event in), and so there's",
            "            # no need to do this check.",
            "            pruned = await self.store.prune_staged_events_in_room(room_id, room_version)",
            "            if pruned:",
            "                # If we have pruned the queue check we need to refetch the next",
            "                # event to handle.",
            "                next = await self.store.get_next_staged_event_for_room(",
            "                    room_id, room_version",
            "                )",
            "                if not next:",
            "                    break",
            "",
            "                origin, event = next",
            "",
            "            new_lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if not new_lock:",
            "                return",
            "            lock = new_lock",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ) -> None:",
            "        await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict) -> None:",
            "        await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str) -> None:",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        acl_event = await self._storage_controllers.state.get_current_state_event(",
            "            room_id, EventTypes.ServerACL, \"\"",
            "        )",
            "        if not acl_event or server_matches_acl_event(server_name, acl_event):",
            "            return",
            "",
            "        raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "def server_matches_acl_event(server_name: str, acl_event: EventBase) -> bool:",
            "    \"\"\"Check if the given server is allowed by the ACL event",
            "",
            "    Args:",
            "        server_name: name of server, without any port part",
            "        acl_event: m.room.server_acl event",
            "",
            "    Returns:",
            "        True if this server is allowed by the ACLs",
            "    \"\"\"",
            "    logger.debug(\"Checking %s against acl %s\", server_name, acl_event.content)",
            "",
            "    # first of all, check if literal IPs are blocked, and if so, whether the",
            "    # server name is a literal IP",
            "    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)",
            "    if not isinstance(allow_ip_literals, bool):",
            "        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")",
            "        allow_ip_literals = True",
            "    if not allow_ip_literals:",
            "        # check for ipv6 literals. These start with '['.",
            "        if server_name[0] == \"[\":",
            "            return False",
            "",
            "        # check for ipv4 literals. We can just lift the routine from twisted.",
            "        if isIPAddress(server_name):",
            "            return False",
            "",
            "    # next,  check the deny list",
            "    deny = acl_event.content.get(\"deny\", [])",
            "    if not isinstance(deny, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list deny ACL %s\", deny)",
            "        deny = []",
            "    for e in deny:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched deny rule %s\", server_name, e)",
            "            return False",
            "",
            "    # then the allow list.",
            "    allow = acl_event.content.get(\"allow\", [])",
            "    if not isinstance(allow, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list allow ACL %s\", allow)",
            "        allow = []",
            "    for e in allow:",
            "        if _acl_entry_matches(server_name, e):",
            "            # logger.info(\"%s matched allow rule %s\", server_name, e)",
            "            return True",
            "",
            "    # everything else should be rejected.",
            "    # logger.info(\"%s fell through\", server_name)",
            "    return False",
            "",
            "",
            "def _acl_entry_matches(server_name: str, acl_entry: Any) -> bool:",
            "    if not isinstance(acl_entry, str):",
            "        logger.warning(",
            "            \"Ignoring non-str ACL entry '%s' (is %s)\", acl_entry, type(acl_entry)",
            "        )",
            "        return False",
            "    regex = glob_to_regex(acl_entry)",
            "    return bool(regex.match(server_name))",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers: Dict[str, Callable[[str, dict], Awaitable[None]]] = {}",
            "        self.query_handlers: Dict[str, Callable[[dict], Awaitable[JsonDict]]] = {}",
            "",
            "        # Map from type to instance names that we should route EDU handling to.",
            "        # We randomly choose one instance from the list to route to for each new",
            "        # EDU received.",
            "        self._edu_type_to_instance: Dict[str, List[str]] = {}",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], Awaitable[JsonDict]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instances_for_edu(",
            "        self, edu_type: str, instance_names: List[str]",
            "    ) -> None:",
            "        \"\"\"Register that the EDU handler is on multiple instances.\"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_names",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict) -> None:",
            "        if not self.config.server.use_presence and edu_type == EduTypes.PRESENCE:",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        instances = self._edu_type_to_instance.get(edu_type, [\"master\"])",
            "        if self._instance_name not in instances:",
            "            # Pick an instance randomly so that we don't overload one.",
            "            route_to = random.choice(instances)",
            "",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict) -> JsonDict:",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))",
            "",
            "",
            "def _get_event_ids_for_partial_state_join(",
            "    join_event: EventBase,",
            "    prev_state_ids: StateMap[str],",
            "    summary: Mapping[str, MemberSummary],",
            ") -> Collection[str]:",
            "    \"\"\"Calculate state to be returned in a partial_state send_join",
            "",
            "    Args:",
            "        join_event: the join event being send_joined",
            "        prev_state_ids: the event ids of the state before the join",
            "",
            "    Returns:",
            "        the event ids to be returned",
            "    \"\"\"",
            "",
            "    # return all non-member events",
            "    state_event_ids = {",
            "        event_id",
            "        for (event_type, state_key), event_id in prev_state_ids.items()",
            "        if event_type != EventTypes.Member",
            "    }",
            "",
            "    # we also need the current state of the current user (it's going to",
            "    # be an auth event for the new join, so we may as well return it)",
            "    current_membership_event_id = prev_state_ids.get(",
            "        (EventTypes.Member, join_event.state_key)",
            "    )",
            "    if current_membership_event_id is not None:",
            "        state_event_ids.add(current_membership_event_id)",
            "",
            "    name_id = prev_state_ids.get((EventTypes.Name, \"\"))",
            "    canonical_alias_id = prev_state_ids.get((EventTypes.CanonicalAlias, \"\"))",
            "    if not name_id and not canonical_alias_id:",
            "        # Also include the hero members of the room (for DM rooms without a title).",
            "        # To do this properly, we should select the correct subset of membership events",
            "        # from `prev_state_ids`. Instead, we are lazier and use the (cached)",
            "        # `get_room_summary` function, which is based on the current state of the room.",
            "        # This introduces races; we choose to ignore them because a) they should be rare",
            "        # and b) even if it's wrong, joining servers will get the full state eventually.",
            "        heroes = extract_heroes_from_room_summary(summary, join_event.state_key)",
            "        for hero in heroes:",
            "            membership_event_id = prev_state_ids.get((EventTypes.Member, hero))",
            "            if membership_event_id:",
            "                state_event_ids.add(membership_event_id)",
            "",
            "    return state_event_ids"
        ],
        "afterPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019-2021 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import (",
            "    Direction,",
            "    EduTypes,",
            "    EventContentFields,",
            "    EventTypes,",
            "    Membership,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_base import (",
            "    FederationBase,",
            "    InvalidEventSignatureError,",
            "    event_from_pdu_json,",
            ")",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.handlers.worker_lock import NEW_EVENT_DURING_PURGE_LOCK_NAME",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    log_kv,",
            "    set_tag,",
            "    start_active_span_from_edu,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import wrap_as_background_process",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.storage.databases.main.lock import Lock",
            "from synapse.storage.databases.main.roommember import extract_heroes_from_room_summary",
            "from synapse.storage.roommember import MemberSummary",
            "from synapse.types import JsonDict, StateMap, get_domain_from_id",
            "from synapse.util import unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute, gather_results",
            "from synapse.util.caches.response_cache import ResponseCache",
            "from synapse.util.stringutils import parse_server_name",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\",",
            "    \"Time taken to process an event\",",
            ")",
            "",
            "last_pdu_ts_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_time\",",
            "    \"The timestamp of the last PDU which was successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "# The name of the lock to use when process events in a room received over",
            "# federation.",
            "_INBOUND_EVENT_HANDLING_LOCK_NAME = \"federation_inbound_pdu\"",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.server_name = hs.hostname",
            "        self.handler = hs.get_federation_handler()",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self.state = hs.get_state_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._room_member_handler = hs.get_room_member_handler()",
            "        self._e2e_keys_handler = hs.get_e2e_keys_handler()",
            "        self._worker_lock_handler = hs.get_worker_locks_handler()",
            "",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "",
            "        # origins that we are currently processing a transaction from.",
            "        # a dict from origin to txn id.",
            "        self._active_transactions: Dict[str, str] = {}",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"fed_txn_handler\", timeout_ms=30000",
            "        )",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache: ResponseCache[",
            "            Tuple[str, Optional[str]]",
            "        ] = ResponseCache(hs.get_clock(), \"state_resp\", timeout_ms=30000)",
            "        self._state_ids_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"state_ids_resp\", timeout_ms=30000",
            "        )",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.config.federation.federation_metrics_domains",
            "        )",
            "",
            "        self._room_prejoin_state_types = hs.config.api.room_prejoin_state",
            "",
            "        # Whether we have started handling old events in the staging area.",
            "        self._started_handling_of_staged_events = False",
            "",
            "    @wrap_as_background_process(\"_handle_old_staged_events\")",
            "    async def _handle_old_staged_events(self) -> None:",
            "        \"\"\"Handle old staged events by fetching all rooms that have staged",
            "        events and start the processing of each of those rooms.",
            "        \"\"\"",
            "",
            "        # Get all the rooms IDs with staged events.",
            "        room_ids = await self.store.get_all_rooms_with_staged_incoming_events()",
            "",
            "        # We then shuffle them so that if there are multiple instances doing",
            "        # this work they're less likely to collide.",
            "        random.shuffle(room_ids)",
            "",
            "        for room_id in room_ids:",
            "            room_version = await self.store.get_room_version(room_id)",
            "",
            "            # Try and acquire the processing lock for the room, if we get it start a",
            "            # background process for handling the events in the room.",
            "            lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if lock:",
            "                logger.info(\"Handling old staged inbound events in %s\", room_id)",
            "                self._process_incoming_pdus_in_room_inner(",
            "                    room_id,",
            "                    room_version,",
            "                    lock,",
            "                )",
            "",
            "            # We pause a bit so that we don't start handling all rooms at once.",
            "            await self._clock.sleep(random.uniform(0, 0.1))",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_dict_from_pdus(pdus)",
            "",
            "        return 200, res",
            "",
            "    async def on_timestamp_to_event_request(",
            "        self, origin: str, room_id: str, timestamp: int, direction: Direction",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"When we receive a federated `/timestamp_to_event` request,",
            "        handle all of the logic for validating and fetching the event.",
            "",
            "        Args:",
            "            origin: The server we received the event from",
            "            room_id: Room to fetch the event from",
            "            timestamp: The point in time (inclusive) we should navigate from in",
            "                the given direction to find the closest event.",
            "            direction: indicates whether we should navigate forward",
            "                or backward from the given timestamp to find the closest event.",
            "",
            "        Returns:",
            "            Tuple indicating the response status code and dictionary response",
            "            body including `event_id`.",
            "        \"\"\"",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            # We only try to fetch data from the local database",
            "            event_id = await self.store.get_event_id_for_timestamp(",
            "                room_id, timestamp, direction",
            "            )",
            "            if event_id:",
            "                event = await self.store.get_event(",
            "                    event_id, allow_none=False, allow_rejected=False",
            "                )",
            "",
            "                return 200, {",
            "                    \"event_id\": event_id,",
            "                    \"origin_server_ts\": event.origin_server_ts,",
            "                }",
            "",
            "        raise SynapseError(",
            "            404,",
            "            \"Unable to find event from %s in direction %s\" % (timestamp, direction),",
            "            errcode=Codes.NOT_FOUND,",
            "        )",
            "",
            "    async def on_incoming_transaction(",
            "        self,",
            "        origin: str,",
            "        transaction_id: str,",
            "        destination: str,",
            "        transaction_data: JsonDict,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # If we receive a transaction we should make sure that kick off handling",
            "        # any old events in the staging area.",
            "        if not self._started_handling_of_staged_events:",
            "            self._started_handling_of_staged_events = True",
            "            self._handle_old_staged_events()",
            "",
            "            # Start a periodic check for old staged events. This is to handle",
            "            # the case where locks time out, e.g. if another process gets killed",
            "            # without dropping its locks.",
            "            self._clock.looping_call(self._handle_old_staged_events, 60 * 1000)",
            "",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(",
            "            transaction_id=transaction_id,",
            "            destination=destination,",
            "            origin=origin,",
            "            origin_server_ts=transaction_data.get(\"origin_server_ts\"),  # type: ignore[arg-type]",
            "            pdus=transaction_data.get(\"pdus\"),",
            "            edus=transaction_data.get(\"edus\"),",
            "        )",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # Reject malformed transactions early: reject if too many PDUs/EDUs",
            "        if len(transaction.pdus) > 50 or len(transaction.edus) > 100:",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "            return 400, {}",
            "",
            "        # we only process one transaction from each origin at a time. We need to do",
            "        # this check here, rather than in _on_incoming_transaction_inner so that we",
            "        # don't cache the rejection in _transaction_resp_cache (so that if the txn",
            "        # arrives again later, we can process it).",
            "        current_transaction = self._active_transactions.get(origin)",
            "        if current_transaction and current_transaction != transaction_id:",
            "            logger.warning(",
            "                \"Received another txn %s from %s while still processing %s\",",
            "                transaction_id,",
            "                origin,",
            "                current_transaction,",
            "            )",
            "            return 429, {",
            "                \"errcode\": Codes.UNKNOWN,",
            "                \"error\": \"Too many concurrent transactions\",",
            "            }",
            "",
            "        # CRITICAL SECTION: we must now not await until we populate _active_transactions",
            "        # in _on_incoming_transaction_inner.",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # CRITICAL SECTION: the first thing we must do (before awaiting) is",
            "        # add an entry to _active_transactions.",
            "        assert origin not in self._active_transactions",
            "        self._active_transactions[origin] = transaction.transaction_id",
            "",
            "        try:",
            "            result = await self._handle_incoming_transaction(",
            "                origin, transaction, request_time",
            "            )",
            "            return result",
            "        finally:",
            "            del self._active_transactions[origin]",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        existing_response = await self.transaction_actions.have_responded(",
            "            origin, transaction",
            "        )",
            "",
            "        if existing_response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,",
            "            )",
            "            return existing_response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            gather_results(",
            "                (",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ),",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room: Dict[str, List[EventBase]] = {}",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str) -> None:",
            "            with nested_logging_context(room_id):",
            "                logger.debug(\"Processing PDUs for %s\", room_id)",
            "",
            "                try:",
            "                    await self.check_server_matches_acl(origin_host, room_id)",
            "                except AuthError as e:",
            "                    logger.warning(",
            "                        \"Ignoring PDUs for room %s from banned server\", room_id",
            "                    )",
            "                    for pdu in pdus_by_room[room_id]:",
            "                        event_id = pdu.event_id",
            "                        pdu_results[event_id] = e.error_dict(self.hs.config)",
            "                    return",
            "",
            "                for pdu in pdus_by_room[room_id]:",
            "                    pdu_results[pdu.event_id] = await process_pdu(pdu)",
            "",
            "        async def process_pdu(pdu: EventBase) -> JsonDict:",
            "            \"\"\"",
            "            Processes a pushed PDU sent to us via a `/send` transaction",
            "",
            "            Returns:",
            "                JsonDict representing a \"PDU Processing Result\" that will be bundled up",
            "                with the other processed PDU's in the `/send` transaction and sent back",
            "                to remote homeserver.",
            "            \"\"\"",
            "            event_id = pdu.event_id",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    await self._handle_received_pdu(origin, pdu)",
            "                    return {}",
            "                except FederationError as e:",
            "                    logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                    return {\"error\": str(e)}",
            "                except Exception as e:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "                    return {\"error\": str(e)}",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            last_pdu_ts_metric.labels(server_name=origin).set(newest_pdu_ts / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction) -> None:",
            "        \"\"\"Process the EDUs in a received transaction.\"\"\"",
            "",
            "        async def _process_edu(edu_dict: JsonDict) -> None:",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            transaction.edus,",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            resp = await self._state_resp_cache.wrap(",
            "                (room_id, event_id),",
            "                self._on_context_state_request_compute,",
            "                room_id,",
            "                event_id,",
            "            )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id),",
            "            self._on_state_ids_request_compute,",
            "            room_id,",
            "            event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _on_state_ids_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> JsonDict:",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(room_id, state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": list(auth_chain_ids)}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        pdus: Collection[EventBase]",
            "        event_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        pdus = await self.store.get_events_as_list(event_ids)",
            "",
            "        auth_chain = await self.store.get_auth_chain(",
            "            room_id, [pdu.event_id for pdu in pdus]",
            "        )",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_dict_from_pdus([pdu])",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        # Refuse the request if that room has seen too many joins recently.",
            "        # This is in addition to the HS-level rate limiting applied by",
            "        # BaseFederationServlet.",
            "        # type-ignore: mypy doesn't seem able to deduce the type of the limiter(!?)",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {pdu.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "        caller_supports_partial_state: bool = False,",
            "    ) -> Dict[str, Any]:",
            "        set_tag(",
            "            SynapseTags.SEND_JOIN_RESPONSE_IS_PARTIAL_STATE,",
            "            caller_supports_partial_state,",
            "        )",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "",
            "        event, context = await self._on_send_membership_event(",
            "            origin, content, Membership.JOIN, room_id",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_event_ids: Collection[str]",
            "        servers_in_room: Optional[Collection[str]]",
            "        if caller_supports_partial_state:",
            "            summary = await self.store.get_room_summary(room_id)",
            "            state_event_ids = _get_event_ids_for_partial_state_join(",
            "                event, prev_state_ids, summary",
            "            )",
            "            servers_in_room = await self.state.get_hosts_in_room_at_events(",
            "                room_id, event_ids=event.prev_event_ids()",
            "            )",
            "        else:",
            "            state_event_ids = prev_state_ids.values()",
            "            servers_in_room = None",
            "",
            "        auth_chain_event_ids = await self.store.get_auth_chain_ids(",
            "            room_id, state_event_ids",
            "        )",
            "",
            "        # if the caller has opted in, we can omit any auth_chain events which are",
            "        # already in state_event_ids",
            "        if caller_supports_partial_state:",
            "            auth_chain_event_ids.difference_update(state_event_ids)",
            "",
            "        auth_chain_events = await self.store.get_events_as_list(auth_chain_event_ids)",
            "        state_events = await self.store.get_events_as_list(state_event_ids)",
            "",
            "        # we try to do all the async stuff before this point, so that time_now is as",
            "        # accurate as possible.",
            "        time_now = self._clock.time_msec()",
            "        event_json = event.get_pdu_json(time_now)",
            "        resp = {",
            "            \"event\": event_json,",
            "            \"state\": [p.get_pdu_json(time_now) for p in state_events],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in auth_chain_events],",
            "            \"members_omitted\": caller_supports_partial_state,",
            "        }",
            "",
            "        if servers_in_room is not None:",
            "            resp[\"servers_in_room\"] = list(servers_in_room)",
            "",
            "        return resp",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(",
            "        self, origin: str, content: JsonDict, room_id: str",
            "    ) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "        await self._on_send_membership_event(origin, content, Membership.LEAVE, room_id)",
            "        return {}",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> JsonDict:",
            "        \"\"\"We've received a /make_knock/ request, so we create a partial knock",
            "        event for the room and hand that back, along with the room version, to the knocking",
            "        homeserver. We do *not* persist or process this event until the other server has",
            "        signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "            supported_versions: The room versions supported by the requesting server.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # Before we do anything: check if the room is partial-stated.",
            "            # Note that at the time this check was added, `on_make_knock_request` would",
            "            # block due to https://github.com/matrix-org/synapse/issues/12997.",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_knock right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        # Check that this room version is supported by the remote homeserver",
            "        if room_version.identifier not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version.identifier, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version.identifier)",
            "",
            "        # Check that this room supports knocking as defined by its room version",
            "        if not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        pdu = await self.handler.on_make_knock_request(origin, room_id, user_id)",
            "        return {",
            "            \"event\": pdu.get_templated_pdu_json(),",
            "            \"room_version\": room_version.identifier,",
            "        }",
            "",
            "    async def on_send_knock_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "    ) -> Dict[str, List[JsonDict]]:",
            "        \"\"\"",
            "        We have received a knock event for a room. Verify and send the event into the room",
            "        on the knocking homeserver's behalf. Then reply with some stripped state from the",
            "        room for the knockee.",
            "",
            "        Args:",
            "            origin: The remote homeserver of the knocking user.",
            "            content: The content of the request.",
            "            room_id: The ID of the room to knock on.",
            "",
            "        Returns:",
            "            The stripped room state.",
            "        \"\"\"",
            "        _, context = await self._on_send_membership_event(",
            "            origin, content, Membership.KNOCK, room_id",
            "        )",
            "",
            "        # Retrieve stripped state events from the room and send them back to the remote",
            "        # server. This will allow the remote server's clients to display information",
            "        # related to the room while the knock request is pending.",
            "        stripped_room_state = (",
            "            await self.store.get_stripped_room_state_from_event_context(",
            "                context, self._room_prejoin_state_types",
            "            )",
            "        )",
            "        return {",
            "            \"knock_room_state\": stripped_room_state,",
            "            # Since v1.37, Synapse incorrectly used \"knock_state_events\" for this field.",
            "            # Thus, we also populate a 'knock_state_events' with the same content to",
            "            # support old instances.",
            "            # See https://github.com/matrix-org/synapse/issues/14088.",
            "            \"knock_state_events\": stripped_room_state,",
            "        }",
            "",
            "    async def _on_send_membership_event(",
            "        self, origin: str, content: JsonDict, membership_type: str, room_id: str",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Handle an on_send_{join,leave,knock} request",
            "",
            "        Does some preliminary validation before passing the request on to the",
            "        federation handler.",
            "",
            "        Args:",
            "            origin: The (authenticated) requesting server",
            "            content: The body of the send_* request - a complete membership event",
            "            membership_type: The expected membership type (join or leave, depending",
            "                on the endpoint)",
            "            room_id: The room_id from the request, to be validated against the room_id",
            "                in the event",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            SynapseError if there is a problem with the request, including things like",
            "               the room_id not matching or the event not being authorized.",
            "        \"\"\"",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        if content[\"room_id\"] != room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room ID in body does not match that in request path\",",
            "                Codes.BAD_JSON,",
            "            )",
            "",
            "        # Note that get_room_version throws if the room does not exist here.",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /send_join, /send_knock or /send_leave.",
            "            # This is because we will not be able to provide the server list (for partial",
            "            # joins) or the full state (for full joins).",
            "            # Return a 404 as we would if we weren't in the room at all.",
            "            logger.info(",
            "                f\"Rejecting /send_{membership_type} to %s because it's a partial state room\",",
            "                room_id,",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                f\"Unable to handle /send_{membership_type} right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        if membership_type == Membership.KNOCK and not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        event = event_from_pdu_json(content, room_version)",
            "",
            "        if event.type != EventTypes.Member or not event.is_state():",
            "            raise SynapseError(400, \"Not an m.room.member event\", Codes.BAD_JSON)",
            "",
            "        if event.content.get(\"membership\") != membership_type:",
            "            raise SynapseError(400, \"Not a %s event\" % membership_type, Codes.BAD_JSON)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, event.room_id)",
            "",
            "        logger.debug(\"_on_send_membership_event: pdu sigs: %s\", event.signatures)",
            "",
            "        # Sign the event since we're vouching on behalf of the remote server that",
            "        # the event is valid to be sent into the room. Currently this is only done",
            "        # if the user is being joined via restricted join rules.",
            "        if (",
            "            room_version.restricted_join_rule",
            "            and event.membership == Membership.JOIN",
            "            and EventContentFields.AUTHORISING_USER in event.content",
            "        ):",
            "            # We can only authorise our own users.",
            "            authorising_server = get_domain_from_id(",
            "                event.content[EventContentFields.AUTHORISING_USER]",
            "            )",
            "            if not self._is_mine_server_name(authorising_server):",
            "                raise SynapseError(",
            "                    400,",
            "                    f\"Cannot authorise membership event for {authorising_server}. We can only authorise requests from our own homeserver\",",
            "                )",
            "",
            "            event.signatures.update(",
            "                compute_event_signature(",
            "                    room_version,",
            "                    event.get_pdu_json(),",
            "                    self.hs.hostname,",
            "                    self.hs.signing_key,",
            "                )",
            "            )",
            "",
            "        try:",
            "            event = await self._check_sigs_and_hash(room_version, event)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {event.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "",
            "        try:",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were persisting the event.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated during `on_send_membership_event`, trying again.\",",
            "                room_id,",
            "            )",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, query: List[Tuple[str, str, str, int]], always_include_fallback_keys: bool",
            "    ) -> Dict[str, Any]:",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self._e2e_keys_handler.claim_local_one_time_keys(",
            "            query, always_include_fallback_keys=always_include_fallback_keys",
            "        )",
            "",
            "        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}",
            "        for result in results:",
            "            for user_id, device_keys in result.items():",
            "                for device_id, keys in device_keys.items():",
            "                    for key_id, key in keys.items():",
            "                        json_result.setdefault(user_id, {}).setdefault(device_id, {})[",
            "                            key_id",
            "                        ] = key",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_dict_from_pdus(self, pdu_list: List[EventBase]) -> JsonDict:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            # Just need a dummy transaction ID and destination since it won't be used.",
            "            transaction_id=\"\",",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=\"\",",
            "        ).get_dict()",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            logger.warning(\"event id %s: %s\", pdu.event_id, e)",
            "            raise FederationError(\"ERROR\", 403, str(e), affected=pdu.event_id)",
            "",
            "        if await self._spam_checker_module_callbacks.should_drop_federated_event(pdu):",
            "            logger.warning(",
            "                \"Unstaged federated event contains spam, dropping %s\", pdu.event_id",
            "            )",
            "            return",
            "",
            "        # Add the event to our staging area",
            "        await self.store.insert_received_event_to_staging(origin, pdu)",
            "",
            "        # Try and acquire the processing lock for the room, if we get it start a",
            "        # background process for handling the events in the room.",
            "        lock = await self.store.try_acquire_lock(",
            "            _INBOUND_EVENT_HANDLING_LOCK_NAME, pdu.room_id",
            "        )",
            "        if lock:",
            "            self._process_incoming_pdus_in_room_inner(",
            "                pdu.room_id, room_version, lock, origin, pdu",
            "            )",
            "",
            "    async def _get_next_nonspam_staged_event_for_room(",
            "        self, room_id: str, room_version: RoomVersion",
            "    ) -> Optional[Tuple[str, EventBase]]:",
            "        \"\"\"Fetch the first non-spam event from staging queue.",
            "",
            "        Args:",
            "            room_id: the room to fetch the first non-spam event in.",
            "            room_version: the version of the room.",
            "",
            "        Returns:",
            "            The first non-spam event in that room.",
            "        \"\"\"",
            "",
            "        while True:",
            "            # We need to do this check outside the lock to avoid a race between",
            "            # a new event being inserted by another instance and it attempting",
            "            # to acquire the lock.",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if next is None:",
            "                return None",
            "",
            "            origin, event = next",
            "",
            "            if await self._spam_checker_module_callbacks.should_drop_federated_event(",
            "                event",
            "            ):",
            "                logger.warning(",
            "                    \"Staged federated event contains spam, dropping %s\",",
            "                    event.event_id,",
            "                )",
            "                continue",
            "",
            "            return next",
            "",
            "    @wrap_as_background_process(\"_process_incoming_pdus_in_room_inner\")",
            "    async def _process_incoming_pdus_in_room_inner(",
            "        self,",
            "        room_id: str,",
            "        room_version: RoomVersion,",
            "        lock: Lock,",
            "        latest_origin: Optional[str] = None,",
            "        latest_event: Optional[EventBase] = None,",
            "    ) -> None:",
            "        \"\"\"Process events in the staging area for the given room.",
            "",
            "        The latest_origin and latest_event args are the latest origin and event",
            "        received (or None to simply pull the next event from the database).",
            "        \"\"\"",
            "",
            "        # The common path is for the event we just received be the only event in",
            "        # the room, so instead of pulling the event out of the DB and parsing",
            "        # the event we just pull out the next event ID and check if that matches.",
            "        if latest_event is not None and latest_origin is not None:",
            "            result = await self.store.get_next_staged_event_id_for_room(room_id)",
            "            if result is None:",
            "                latest_origin = None",
            "                latest_event = None",
            "            else:",
            "                next_origin, next_event_id = result",
            "                if (",
            "                    next_origin != latest_origin",
            "                    or next_event_id != latest_event.event_id",
            "                ):",
            "                    latest_origin = None",
            "                    latest_event = None",
            "",
            "        if latest_origin is None or latest_event is None:",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "            if not next:",
            "                await lock.release()",
            "                return",
            "",
            "            origin, event = next",
            "        else:",
            "            origin = latest_origin",
            "            event = latest_event",
            "",
            "        # We loop round until there are no more events in the room in the",
            "        # staging area, or we fail to get the lock (which means another process",
            "        # has started processing).",
            "        while True:",
            "            async with lock:",
            "                logger.info(\"handling received PDU in room %s: %s\", room_id, event)",
            "                try:",
            "                    with nested_logging_context(event.event_id):",
            "                        # We're taking out a lock within a lock, which could",
            "                        # lead to deadlocks if we're not careful. However, it is",
            "                        # safe on this occasion as we only ever take a write",
            "                        # lock when deleting a room, which we would never do",
            "                        # while holding the `_INBOUND_EVENT_HANDLING_LOCK_NAME`",
            "                        # lock.",
            "                        async with self._worker_lock_handler.acquire_read_write_lock(",
            "                            NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "                        ):",
            "                            await self._federation_event_handler.on_receive_pdu(",
            "                                origin, event",
            "                            )",
            "                except FederationError as e:",
            "                    # XXX: Ideally we'd inform the remote we failed to process",
            "                    # the event, but we can't return an error in the transaction",
            "                    # response (as we've already responded).",
            "                    logger.warning(\"Error handling PDU %s: %s\", event.event_id, e)",
            "                except Exception:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event.event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "",
            "                received_ts = await self.store.remove_received_event_from_staging(",
            "                    origin, event.event_id",
            "                )",
            "                if received_ts is not None:",
            "                    pdu_process_time.observe(",
            "                        (self._clock.time_msec() - received_ts) / 1000",
            "                    )",
            "",
            "            next = await self._get_next_nonspam_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if not next:",
            "                break",
            "",
            "            origin, event = next",
            "",
            "            # Prune the event queue if it's getting large.",
            "            #",
            "            # We do this *after* handling the first event as the common case is",
            "            # that the queue is empty (/has the single event in), and so there's",
            "            # no need to do this check.",
            "            pruned = await self.store.prune_staged_events_in_room(room_id, room_version)",
            "            if pruned:",
            "                # If we have pruned the queue check we need to refetch the next",
            "                # event to handle.",
            "                next = await self.store.get_next_staged_event_for_room(",
            "                    room_id, room_version",
            "                )",
            "                if not next:",
            "                    break",
            "",
            "                origin, event = next",
            "",
            "            new_lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if not new_lock:",
            "                return",
            "            lock = new_lock",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ) -> None:",
            "        await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict) -> None:",
            "        await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str) -> None:",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        server_acl_evaluator = (",
            "            await self._storage_controllers.state.get_server_acl_for_room(room_id)",
            "        )",
            "        if server_acl_evaluator and not server_acl_evaluator.server_matches_acl_event(",
            "            server_name",
            "        ):",
            "            raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers: Dict[str, Callable[[str, dict], Awaitable[None]]] = {}",
            "        self.query_handlers: Dict[str, Callable[[dict], Awaitable[JsonDict]]] = {}",
            "",
            "        # Map from type to instance names that we should route EDU handling to.",
            "        # We randomly choose one instance from the list to route to for each new",
            "        # EDU received.",
            "        self._edu_type_to_instance: Dict[str, List[str]] = {}",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], Awaitable[JsonDict]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instances_for_edu(",
            "        self, edu_type: str, instance_names: List[str]",
            "    ) -> None:",
            "        \"\"\"Register that the EDU handler is on multiple instances.\"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_names",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict) -> None:",
            "        if not self.config.server.use_presence and edu_type == EduTypes.PRESENCE:",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        instances = self._edu_type_to_instance.get(edu_type, [\"master\"])",
            "        if self._instance_name not in instances:",
            "            # Pick an instance randomly so that we don't overload one.",
            "            route_to = random.choice(instances)",
            "",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict) -> JsonDict:",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))",
            "",
            "",
            "def _get_event_ids_for_partial_state_join(",
            "    join_event: EventBase,",
            "    prev_state_ids: StateMap[str],",
            "    summary: Mapping[str, MemberSummary],",
            ") -> Collection[str]:",
            "    \"\"\"Calculate state to be returned in a partial_state send_join",
            "",
            "    Args:",
            "        join_event: the join event being send_joined",
            "        prev_state_ids: the event ids of the state before the join",
            "",
            "    Returns:",
            "        the event ids to be returned",
            "    \"\"\"",
            "",
            "    # return all non-member events",
            "    state_event_ids = {",
            "        event_id",
            "        for (event_type, state_key), event_id in prev_state_ids.items()",
            "        if event_type != EventTypes.Member",
            "    }",
            "",
            "    # we also need the current state of the current user (it's going to",
            "    # be an auth event for the new join, so we may as well return it)",
            "    current_membership_event_id = prev_state_ids.get(",
            "        (EventTypes.Member, join_event.state_key)",
            "    )",
            "    if current_membership_event_id is not None:",
            "        state_event_ids.add(current_membership_event_id)",
            "",
            "    name_id = prev_state_ids.get((EventTypes.Name, \"\"))",
            "    canonical_alias_id = prev_state_ids.get((EventTypes.CanonicalAlias, \"\"))",
            "    if not name_id and not canonical_alias_id:",
            "        # Also include the hero members of the room (for DM rooms without a title).",
            "        # To do this properly, we should select the correct subset of membership events",
            "        # from `prev_state_ids`. Instead, we are lazier and use the (cached)",
            "        # `get_room_summary` function, which is based on the current state of the room.",
            "        # This introduces races; we choose to ignore them because a) they should be rare",
            "        # and b) even if it's wrong, joining servers will get the full state eventually.",
            "        heroes = extract_heroes_from_room_summary(summary, join_event.state_key)",
            "        for hero in heroes:",
            "            membership_event_id = prev_state_ids.get((EventTypes.Member, hero))",
            "            if membership_event_id:",
            "                state_event_ids.add(membership_event_id)",
            "",
            "    return state_event_ids"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "32": [],
            "35": [],
            "1327": [
                "FederationServer"
            ],
            "1328": [
                "FederationServer"
            ],
            "1330": [
                "FederationServer"
            ],
            "1331": [
                "FederationServer"
            ],
            "1332": [
                "FederationServer"
            ],
            "1333": [
                "FederationServer"
            ],
            "1334": [],
            "1335": [],
            "1336": [
                "server_matches_acl_event"
            ],
            "1337": [
                "server_matches_acl_event"
            ],
            "1338": [
                "server_matches_acl_event"
            ],
            "1339": [
                "server_matches_acl_event"
            ],
            "1340": [
                "server_matches_acl_event"
            ],
            "1341": [
                "server_matches_acl_event"
            ],
            "1342": [
                "server_matches_acl_event"
            ],
            "1343": [
                "server_matches_acl_event"
            ],
            "1344": [
                "server_matches_acl_event"
            ],
            "1345": [
                "server_matches_acl_event"
            ],
            "1346": [
                "server_matches_acl_event"
            ],
            "1347": [
                "server_matches_acl_event"
            ],
            "1348": [
                "server_matches_acl_event"
            ],
            "1349": [
                "server_matches_acl_event"
            ],
            "1350": [
                "server_matches_acl_event"
            ],
            "1351": [
                "server_matches_acl_event"
            ],
            "1352": [
                "server_matches_acl_event"
            ],
            "1353": [
                "server_matches_acl_event"
            ],
            "1354": [
                "server_matches_acl_event"
            ],
            "1355": [
                "server_matches_acl_event"
            ],
            "1356": [
                "server_matches_acl_event"
            ],
            "1357": [
                "server_matches_acl_event"
            ],
            "1358": [
                "server_matches_acl_event"
            ],
            "1359": [
                "server_matches_acl_event"
            ],
            "1360": [
                "server_matches_acl_event"
            ],
            "1361": [
                "server_matches_acl_event"
            ],
            "1362": [
                "server_matches_acl_event"
            ],
            "1363": [
                "server_matches_acl_event"
            ],
            "1364": [
                "server_matches_acl_event"
            ],
            "1365": [
                "server_matches_acl_event"
            ],
            "1366": [
                "server_matches_acl_event"
            ],
            "1367": [
                "server_matches_acl_event"
            ],
            "1368": [
                "server_matches_acl_event"
            ],
            "1369": [
                "server_matches_acl_event"
            ],
            "1370": [
                "server_matches_acl_event"
            ],
            "1371": [
                "server_matches_acl_event"
            ],
            "1372": [
                "server_matches_acl_event"
            ],
            "1373": [
                "server_matches_acl_event"
            ],
            "1374": [
                "server_matches_acl_event"
            ],
            "1375": [
                "server_matches_acl_event"
            ],
            "1376": [
                "server_matches_acl_event"
            ],
            "1377": [
                "server_matches_acl_event"
            ],
            "1378": [
                "server_matches_acl_event"
            ],
            "1379": [
                "server_matches_acl_event"
            ],
            "1380": [
                "server_matches_acl_event"
            ],
            "1381": [
                "server_matches_acl_event"
            ],
            "1382": [
                "server_matches_acl_event"
            ],
            "1383": [
                "server_matches_acl_event"
            ],
            "1384": [
                "server_matches_acl_event"
            ],
            "1385": [
                "server_matches_acl_event"
            ],
            "1386": [],
            "1387": [],
            "1388": [
                "_acl_entry_matches"
            ],
            "1389": [
                "_acl_entry_matches"
            ],
            "1390": [
                "_acl_entry_matches"
            ],
            "1391": [
                "_acl_entry_matches"
            ],
            "1392": [
                "_acl_entry_matches"
            ],
            "1393": [
                "_acl_entry_matches"
            ],
            "1394": [
                "_acl_entry_matches"
            ],
            "1395": [
                "_acl_entry_matches"
            ]
        },
        "addLocation": []
    },
    "synapse/handlers/federation_event.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2342,
                "afterPatchRowNumber": 2342,
                "PatchRowcode": "             # TODO retrieve the previous state, and exclude join -> join transitions"
            },
            "1": {
                "beforePatchRowNumber": 2343,
                "afterPatchRowNumber": 2343,
                "PatchRowcode": "             self._notifier.notify_user_joined_room(event.event_id, event.room_id)"
            },
            "2": {
                "beforePatchRowNumber": 2344,
                "afterPatchRowNumber": 2344,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2345,
                "PatchRowcode": "+        # If this is a server ACL event, clear the cache in the storage controller."
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2346,
                "PatchRowcode": "+        if event.type == EventTypes.ServerACL:"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2347,
                "PatchRowcode": "+            self._state_storage_controller.get_server_acl_for_room.invalidate("
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2348,
                "PatchRowcode": "+                (event.room_id,)"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2349,
                "PatchRowcode": "+            )"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2350,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": 2345,
                "afterPatchRowNumber": 2351,
                "PatchRowcode": "     def _sanity_check_event(self, ev: EventBase) -> None:"
            },
            "10": {
                "beforePatchRowNumber": 2346,
                "afterPatchRowNumber": 2352,
                "PatchRowcode": "         \"\"\""
            },
            "11": {
                "beforePatchRowNumber": 2347,
                "afterPatchRowNumber": 2353,
                "PatchRowcode": "         Do some early sanity checks of a received event"
            }
        },
        "frontPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import collections",
            "import itertools",
            "import logging",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Collection,",
            "    Container,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Sequence,",
            "    Set,",
            "    Tuple,",
            ")",
            "",
            "from prometheus_client import Counter, Histogram",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    Membership,",
            "    RejectedReason,",
            "    RoomEncryptionAlgorithms,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    EventSizeError,",
            "    FederationError,",
            "    FederationPullAttemptBackoffError,",
            "    HttpResponseException,",
            "    PartialStateConflictError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions",
            "from synapse.event_auth import (",
            "    auth_types_for_event,",
            "    check_state_dependent_auth_rules,",
            "    check_state_independent_auth_rules,",
            "    validate_event_for_room_version,",
            ")",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext, UnpersistedEventContextBase",
            "from synapse.federation.federation_client import InvalidResponseError, PulledPduInfo",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    set_tag,",
            "    start_active_span,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.devices import (",
            "    ReplicationMultiUserDevicesResyncRestServlet,",
            ")",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEventsRestServlet,",
            ")",
            "from synapse.state import StateResolutionStore",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    PersistedEventPosition,",
            "    RoomStreamToken,",
            "    StateMap,",
            "    StrCollection,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.iterutils import batch_iter, partition",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import shortstr",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "soft_failed_event_counter = Counter(",
            "    \"synapse_federation_soft_failed_events_total\",",
            "    \"Events received over federation that we marked as soft_failed\",",
            ")",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_after_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_after_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.25,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        25.0,",
            "        30.0,",
            "        40.0,",
            "        50.0,",
            "        60.0,",
            "        80.0,",
            "        100.0,",
            "        120.0,",
            "        150.0,",
            "        180.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "class FederationEventHandler:",
            "    \"\"\"Handles events that originated from federation.",
            "",
            "    Responsible for handing incoming events and passing them on to the rest",
            "    of the homeserver (including auth and state conflict resolutions)",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._clock = hs.get_clock()",
            "        self._store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "",
            "        self._state_handler = hs.get_state_handler()",
            "        self._event_creation_handler = hs.get_event_creation_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._message_handler = hs.get_message_handler()",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "        self._state_resolution_handler = hs.get_state_resolution_handler()",
            "        # avoid a circular dependency by deferring execution here",
            "        self._get_room_member_handler = hs.get_room_member_handler",
            "",
            "        self._federation_client = hs.get_federation_client()",
            "        self._third_party_event_rules = (",
            "            hs.get_module_api_callbacks().third_party_event_rules",
            "        )",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._is_mine_server_name = hs.is_mine_server_name",
            "        self._server_name = hs.hostname",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        self._config = hs.config",
            "        self._ephemeral_messages_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)",
            "        if hs.config.worker.worker_app:",
            "            self._multi_user_device_resync = (",
            "                ReplicationMultiUserDevicesResyncRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._device_list_updater = hs.get_device_handler().device_list_updater",
            "",
            "        # When joining a room we need to queue any events for that room up.",
            "        # For each room, a list of (pdu, origin) tuples.",
            "        # TODO: replace this with something more elegant, probably based around the",
            "        # federation event staging area.",
            "        self.room_queues: Dict[str, List[Tuple[EventBase, str]]] = {}",
            "",
            "        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")",
            "",
            "    async def on_receive_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received via a federation /send/ transaction",
            "",
            "        Args:",
            "            origin: server which initiated the /send/ transaction. Will",
            "                be used to fetch missing events or state.",
            "            pdu: received PDU",
            "        \"\"\"",
            "",
            "        # We should never see any outliers here.",
            "        assert not pdu.internal_metadata.outlier",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        # We reprocess pdus when we have seen them only as outliers",
            "        existing = await self._store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        # FIXME: Currently we fetch an event again when we already have it",
            "        # if it has been marked as an outlier.",
            "        if existing:",
            "            if not existing.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received event %s which we have already seen\", event_id",
            "                )",
            "                return",
            "            if pdu.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received outlier %s which we already have as an outlier\",",
            "                    event_id,",
            "                )",
            "                return",
            "            logger.info(\"De-outliering event %s\", event_id)",
            "",
            "        # do some initial sanity-checking of the event. In particular, make",
            "        # sure it doesn't have hundreds of prev_events or auth_events, which",
            "        # could cause a huge state resolution or cascade of event fetches.",
            "        try:",
            "            self._sanity_check_event(pdu)",
            "        except SynapseError as err:",
            "            logger.warning(\"Received event failed sanity checks\")",
            "            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)",
            "",
            "        # If we are currently in the process of joining this room, then we",
            "        # queue up events for later processing.",
            "        if room_id in self.room_queues:",
            "            logger.info(",
            "                \"Queuing PDU from %s for now: join in progress\",",
            "                origin,",
            "            )",
            "            self.room_queues[room_id].append((pdu, origin))",
            "            return",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        #",
            "        # Note that if we were never in the room then we would have already",
            "        # dropped the event, since we wouldn't know the room version.",
            "        is_in_room = await self._event_auth_handler.is_host_in_room(",
            "            room_id, self._server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Ignoring PDU from %s as we're not in the room\",",
            "                origin,",
            "            )",
            "            return None",
            "",
            "        # Try to fetch any missing prev events to fill in gaps in the graph",
            "        prevs = set(pdu.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        if missing_prevs:",
            "            # We only backfill backwards to the min depth.",
            "            min_depth = await self._store.get_min_depth(pdu.room_id)",
            "            logger.debug(\"min_depth: %d\", min_depth)",
            "",
            "            if min_depth is not None and pdu.depth > min_depth:",
            "                # If we're missing stuff, ensure we only fetch stuff one",
            "                # at a time.",
            "                logger.info(",
            "                    \"Acquiring room lock to fetch %d missing prev_events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                async with self._room_pdu_linearizer.queue(pdu.room_id):",
            "                    logger.info(",
            "                        \"Acquired room lock to fetch %d missing prev_events\",",
            "                        len(missing_prevs),",
            "                    )",
            "",
            "                    try:",
            "                        await self._get_missing_events_for_pdu(",
            "                            origin, pdu, prevs, min_depth",
            "                        )",
            "                    except Exception as e:",
            "                        raise Exception(",
            "                            \"Error fetching missing prev_events for %s: %s\"",
            "                            % (event_id, e)",
            "                        ) from e",
            "",
            "                # Update the set of things we've seen after trying to",
            "                # fetch the missing stuff",
            "                seen = await self._store.have_events_in_timeline(prevs)",
            "                missing_prevs = prevs - seen",
            "",
            "                if not missing_prevs:",
            "                    logger.info(\"Found all missing prev_events\")",
            "",
            "            if missing_prevs:",
            "                # since this event was pushed to us, it is possible for it to",
            "                # become the only forward-extremity in the room, and we would then",
            "                # trust its state to be the state for the whole room. This is very",
            "                # bad. Further, if the event was pushed to us, there is no excuse",
            "                # for us not to have all the prev_events. (XXX: apart from",
            "                # min_depth?)",
            "                #",
            "                # We therefore reject any such events.",
            "                logger.warning(",
            "                    \"Rejecting: failed to fetch %d prev events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                raise FederationError(",
            "                    \"ERROR\",",
            "                    403,",
            "                    (",
            "                        \"Your server isn't divulging details about prev_events \"",
            "                        \"referenced in this event.\"",
            "                    ),",
            "                    affected=pdu.event_id,",
            "                )",
            "",
            "        try:",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were processing the PDU.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated while processing the PDU, trying again.\",",
            "                room_id,",
            "            )",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "",
            "    async def on_send_membership_event(",
            "        self, origin: str, event: EventBase",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"",
            "        We have received a join/leave/knock event for a room via send_join/leave/knock.",
            "",
            "        Verify that event and send it into the room on the remote homeserver's behalf.",
            "",
            "        This is quite similar to on_receive_pdu, with the following principal",
            "        differences:",
            "          * only membership events are permitted (and only events with",
            "            sender==state_key -- ie, no kicks or bans)",
            "          * *We* send out the event on behalf of the remote server.",
            "          * We enforce the membership restrictions of restricted rooms.",
            "          * Rejected events result in an exception rather than being stored.",
            "",
            "        There are also other differences, however it is not clear if these are by",
            "        design or omission. In particular, we do not attempt to backfill any missing",
            "        prev_events.",
            "",
            "        Args:",
            "            origin: The homeserver of the remote (joining/invited/knocking) user.",
            "            event: The member event that has been signed by the remote homeserver.",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            RuntimeError if any prev_events are missing",
            "            SynapseError if the event is not accepted into the room",
            "            PartialStateConflictError if the room was un-partial stated in between",
            "                computing the state at the event and persisting it. The caller should",
            "                retry exactly once in this case.",
            "        \"\"\"",
            "        logger.debug(",
            "            \"on_send_membership_event: Got event: %s, signatures: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got send_membership request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        if event.sender != event.state_key:",
            "            raise SynapseError(400, \"state_key and sender must match\", Codes.BAD_JSON)",
            "",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # Send this event on behalf of the other server.",
            "        #",
            "        # The remote server isn't a full participant in the room at this point, so",
            "        # may not have an up-to-date list of the other homeservers participating in",
            "        # the room, so we send it on their behalf.",
            "        event.internal_metadata.send_on_behalf_of = origin",
            "",
            "        context = await self._state_handler.compute_event_context(event)",
            "        await self._check_event_auth(origin, event, context)",
            "        if context.rejected:",
            "            raise SynapseError(",
            "                403, f\"{event.membership} event was rejected\", Codes.FORBIDDEN",
            "            )",
            "",
            "        # for joins, we need to check the restrictions of restricted rooms",
            "        if event.membership == Membership.JOIN:",
            "            await self.check_join_restrictions(context, event)",
            "",
            "        # for knock events, we run the third-party event rules. It's not entirely clear",
            "        # why we don't do this for other sorts of membership events.",
            "        if event.membership == Membership.KNOCK:",
            "            event_allowed, _ = await self._third_party_event_rules.check_event_allowed(",
            "                event, context",
            "            )",
            "            if not event_allowed:",
            "                logger.info(\"Sending of knock %s forbidden by third-party rules\", event)",
            "                raise SynapseError(",
            "                    403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "                )",
            "",
            "        # all looks good, we can persist the event.",
            "",
            "        # First, precalculate the joined hosts so that the federation sender doesn't",
            "        # need to.",
            "        await self._event_creation_handler.cache_joined_hosts_for_events(",
            "            [(event, context)]",
            "        )",
            "",
            "        await self._check_for_soft_fail(event, context=context, origin=origin)",
            "        await self._run_push_actions_and_persist_event(event, context)",
            "        return event, context",
            "",
            "    async def check_join_restrictions(",
            "        self,",
            "        context: UnpersistedEventContextBase,",
            "        event: EventBase,",
            "    ) -> None:",
            "        \"\"\"Check that restrictions in restricted join rules are matched",
            "",
            "        Called when we receive a join event via send_join.",
            "",
            "        Raises an auth error if the restrictions are not matched.",
            "        \"\"\"",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        # Check if the user is already in the room or invited to the room.",
            "        user_id = event.state_key",
            "        prev_member_event_id = prev_state_ids.get((EventTypes.Member, user_id), None)",
            "        prev_membership = None",
            "        if prev_member_event_id:",
            "            prev_member_event = await self._store.get_event(prev_member_event_id)",
            "            prev_membership = prev_member_event.membership",
            "",
            "        # Check if the member should be allowed access via membership in a space.",
            "        await self._event_auth_handler.check_restricted_join_rules(",
            "            prev_state_ids,",
            "            event.room_version,",
            "            user_id,",
            "            prev_membership,",
            "        )",
            "",
            "    @trace",
            "    async def process_remote_join(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        auth_events: List[EventBase],",
            "        state: List[EventBase],",
            "        event: EventBase,",
            "        room_version: RoomVersion,",
            "        partial_state: bool,",
            "    ) -> int:",
            "        \"\"\"Persists the events returned by a send_join",
            "",
            "        Checks the auth chain is valid (and passes auth checks) for the",
            "        state and event. Then persists all of the events.",
            "        Notifies about the persisted events where appropriate.",
            "",
            "        Args:",
            "            origin: Where the events came from",
            "            room_id:",
            "            auth_events",
            "            state",
            "            event",
            "            room_version: The room version we expect this room to have, and",
            "                will raise if it doesn't match the version in the create event.",
            "            partial_state: True if the state omits non-critical membership events",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            SynapseError if the response is in some way invalid.",
            "            PartialStateConflictError if the homeserver is already in the room and it",
            "                has been un-partial stated.",
            "        \"\"\"",
            "        create_event = None",
            "        for e in state:",
            "            if (e.type, e.state_key) == (EventTypes.Create, \"\"):",
            "                create_event = e",
            "                break",
            "",
            "        if create_event is None:",
            "            # If the state doesn't have a create event then the room is",
            "            # invalid, and it would fail auth checks anyway.",
            "            raise SynapseError(400, \"No create event in state\")",
            "",
            "        room_version_id = create_event.content.get(",
            "            \"room_version\", RoomVersions.V1.identifier",
            "        )",
            "",
            "        if room_version.identifier != room_version_id:",
            "            raise SynapseError(400, \"Room version mismatch\")",
            "",
            "        # persist the auth chain and state events.",
            "        #",
            "        # any invalid events here will be marked as rejected, and we'll carry on.",
            "        #",
            "        # any events whose auth events are missing (ie, not in the send_join response,",
            "        # and not already in our db) will just be ignored. This is correct behaviour,",
            "        # because the reason that auth_events are missing might be due to us being",
            "        # unable to validate their signatures. The fact that we can't validate their",
            "        # signatures right now doesn't mean that we will *never* be able to, so it",
            "        # is premature to reject them.",
            "        #",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state)",
            "        )",
            "",
            "        # and now persist the join event itself.",
            "        logger.info(",
            "            \"Peristing join-via-remote %s (partial_state: %s)\", event, partial_state",
            "        )",
            "        with nested_logging_context(suffix=event.event_id):",
            "            if partial_state:",
            "                # When handling a second partial state join into a partial state room,",
            "                # the returned state will exclude the membership from the first join. To",
            "                # preserve prior memberships, we try to compute the partial state before",
            "                # the event ourselves if we know about any of the prev events.",
            "                #",
            "                # When we don't know about any of the prev events, it's fine to just use",
            "                # the returned state, since the new join will create a new forward",
            "                # extremity, and leave the forward extremity containing our prior",
            "                # memberships alone.",
            "                prev_event_ids = set(event.prev_event_ids())",
            "                seen_event_ids = await self._store.have_events_in_timeline(",
            "                    prev_event_ids",
            "                )",
            "                missing_event_ids = prev_event_ids - seen_event_ids",
            "",
            "                state_maps_to_resolve: List[StateMap[str]] = []",
            "",
            "                # Fetch the state after the prev events that we know about.",
            "                state_maps_to_resolve.extend(",
            "                    (",
            "                        await self._state_storage_controller.get_state_groups_ids(",
            "                            room_id, seen_event_ids, await_full_state=False",
            "                        )",
            "                    ).values()",
            "                )",
            "",
            "                # When there are prev events we do not have the state for, we state",
            "                # resolve with the state returned by the remote homeserver.",
            "                if missing_event_ids or len(state_maps_to_resolve) == 0:",
            "                    state_maps_to_resolve.append(",
            "                        {(e.type, e.state_key): e.event_id for e in state}",
            "                    )",
            "",
            "                state_ids_before_event = (",
            "                    await self._state_resolution_handler.resolve_events_with_store(",
            "                        event.room_id,",
            "                        room_version.identifier,",
            "                        state_maps_to_resolve,",
            "                        event_map=None,",
            "                        state_res_store=StateResolutionStore(self._store),",
            "                    )",
            "                )",
            "            else:",
            "                state_ids_before_event = {",
            "                    (e.type, e.state_key): e.event_id for e in state",
            "                }",
            "",
            "            context = await self._state_handler.compute_event_context(",
            "                event,",
            "                state_ids_before_event=state_ids_before_event,",
            "                partial_state=partial_state,",
            "            )",
            "",
            "            await self._check_event_auth(origin, event, context)",
            "            if context.rejected:",
            "                raise SynapseError(403, \"Join event was rejected\")",
            "",
            "            # the remote server is responsible for sending our join event to the rest",
            "            # of the federation. Indeed, attempting to do so will result in problems",
            "            # when we try to look up the state before the join (to get the server list)",
            "            # and discover that we do not have it.",
            "            event.internal_metadata.proactively_send = False",
            "",
            "            stream_id_after_persist = await self.persist_events_and_notify(",
            "                room_id, [(event, context)]",
            "            )",
            "",
            "            return stream_id_after_persist",
            "",
            "    async def update_state_for_partial_state_event(",
            "        self, destination: str, event: EventBase",
            "    ) -> None:",
            "        \"\"\"Recalculate the state at an event as part of a de-partial-stating process",
            "",
            "        Args:",
            "            destination: server to request full state from",
            "            event: partial-state event to be de-partial-stated",
            "",
            "        Raises:",
            "            FederationPullAttemptBackoffError if we are are deliberately not attempting",
            "                to pull the given event over federation because we've already done so",
            "                recently and are backing off.",
            "            FederationError if we fail to request state from the remote server.",
            "        \"\"\"",
            "        logger.info(\"Updating state for %s\", event.event_id)",
            "        with nested_logging_context(suffix=event.event_id):",
            "            # if we have all the event's prev_events, then we can work out the",
            "            # state based on their states. Otherwise, we request it from the destination",
            "            # server.",
            "            #",
            "            # This is the same operation as we do when we receive a regular event",
            "            # over federation.",
            "            context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                destination, event",
            "            )",
            "            if context.partial_state:",
            "                # this can happen if some or all of the event's prev_events still have",
            "                # partial state. We were careful to only pick events from the db without",
            "                # partial-state prev events, so that implies that a prev event has",
            "                # been persisted (with partial state) since we did the query.",
            "                #",
            "                # So, let's just ignore `event` for now; when we re-run the db query",
            "                # we should instead get its partial-state prev event, which we will",
            "                # de-partial-state, and then come back to event.",
            "                logger.warning(",
            "                    \"%s still has prev_events with partial state: can't de-partial-state it yet\",",
            "                    event.event_id,",
            "                )",
            "                return",
            "",
            "            # since the state at this event has changed, we should now re-evaluate",
            "            # whether it should have been rejected. We must already have all of the",
            "            # auth events (from last time we went round this path), so there is no",
            "            # need to pass the origin.",
            "            await self._check_event_auth(None, event, context)",
            "",
            "            await self._store.update_state_for_partial_state_event(event, context)",
            "            self._state_storage_controller.notify_event_un_partial_stated(",
            "                event.event_id",
            "            )",
            "            # Notify that there's a new row in the un_partial_stated_events stream.",
            "            self._notifier.notify_replication()",
            "",
            "    @trace",
            "    async def backfill(",
            "        self, dest: str, room_id: str, limit: int, extremities: StrCollection",
            "    ) -> None:",
            "        \"\"\"Trigger a backfill request to `dest` for the given `room_id`",
            "",
            "        This will attempt to get more events from the remote. If the other side",
            "        has no new events to offer, this will return an empty list.",
            "",
            "        As the events are received, we check their signatures, and also do some",
            "        sanity-checking on them. If any of the backfilled events are invalid,",
            "        this method throws a SynapseError.",
            "",
            "        We might also raise an InvalidResponseError if the response from the remote",
            "        server is just bogus.",
            "",
            "        TODO: make this more useful to distinguish failures of the remote",
            "        server from invalid events (there is probably no point in trying to",
            "        re-fetch invalid events from every other HS in the room.)",
            "        \"\"\"",
            "        if self._is_mine_server_name(dest):",
            "            raise SynapseError(400, \"Can't backfill from self.\")",
            "",
            "        events = await self._federation_client.backfill(",
            "            dest, room_id, limit=limit, extremities=extremities",
            "        )",
            "",
            "        if not events:",
            "            return",
            "",
            "        with backfill_processing_after_timer.time():",
            "            # if there are any events in the wrong room, the remote server is buggy and",
            "            # should not be trusted.",
            "            for ev in events:",
            "                if ev.room_id != room_id:",
            "                    raise InvalidResponseError(",
            "                        f\"Remote server {dest} returned event {ev.event_id} which is in \"",
            "                        f\"room {ev.room_id}, when we were backfilling in {room_id}\"",
            "                    )",
            "",
            "            await self._process_pulled_events(",
            "                dest,",
            "                events,",
            "                backfilled=True,",
            "            )",
            "",
            "    @trace",
            "    async def _get_missing_events_for_pdu(",
            "        self, origin: str, pdu: EventBase, prevs: Set[str], min_depth: int",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            origin: Origin of the pdu. Will be called to get the missing events",
            "            pdu: received pdu",
            "            prevs: List of event ids which we are missing",
            "            min_depth: Minimum depth of events to return.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "",
            "        if not prevs - seen:",
            "            return",
            "",
            "        latest_frozen = await self._store.get_latest_event_ids_in_room(room_id)",
            "",
            "        # We add the prev events that we have seen to the latest",
            "        # list to ensure the remote server doesn't give them to us",
            "        latest = seen | latest_frozen",
            "",
            "        logger.info(",
            "            \"Requesting missing events between %s and %s\",",
            "            shortstr(latest),",
            "            event_id,",
            "        )",
            "",
            "        # XXX: we set timeout to 10s to help workaround",
            "        # https://github.com/matrix-org/synapse/issues/1733.",
            "        # The reason is to avoid holding the linearizer lock",
            "        # whilst processing inbound /send transactions, causing",
            "        # FDs to stack up and block other inbound transactions",
            "        # which empirically can currently take up to 30 minutes.",
            "        #",
            "        # N.B. this explicitly disables retry attempts.",
            "        #",
            "        # N.B. this also increases our chances of falling back to",
            "        # fetching fresh state for the room if the missing event",
            "        # can't be found, which slightly reduces our security.",
            "        # it may also increase our DAG extremity count for the room,",
            "        # causing additional state resolution?  See #1760.",
            "        # However, fetching state doesn't hold the linearizer lock",
            "        # apparently.",
            "        #",
            "        # see https://github.com/matrix-org/synapse/pull/1744",
            "        #",
            "        # ----",
            "        #",
            "        # Update richvdh 2018/09/18: There are a number of problems with timing this",
            "        # request out aggressively on the client side:",
            "        #",
            "        # - it plays badly with the server-side rate-limiter, which starts tarpitting you",
            "        #   if you send too many requests at once, so you end up with the server carefully",
            "        #   working through the backlog of your requests, which you have already timed",
            "        #   out.",
            "        #",
            "        # - for this request in particular, we now (as of",
            "        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the",
            "        #   server can't produce a plausible-looking set of prev_events - so we becone",
            "        #   much more likely to reject the event.",
            "        #",
            "        # - contrary to what it says above, we do *not* fall back to fetching fresh state",
            "        #   for the room if get_missing_events times out. Rather, we give up processing",
            "        #   the PDU whose prevs we are missing, which then makes it much more likely that",
            "        #   we'll end up back here for the *next* PDU in the list, which exacerbates the",
            "        #   problem.",
            "        #",
            "        # - the aggressive 10s timeout was introduced to deal with incoming federation",
            "        #   requests taking 8 hours to process. It's not entirely clear why that was going",
            "        #   on; certainly there were other issues causing traffic storms which are now",
            "        #   resolved, and I think in any case we may be more sensible about our locking",
            "        #   now. We're *certainly* more sensible about our logging.",
            "        #",
            "        # All that said: Let's try increasing the timeout to 60s and see what happens.",
            "",
            "        try:",
            "            missing_events = await self._federation_client.get_missing_events(",
            "                origin,",
            "                room_id,",
            "                earliest_events_ids=list(latest),",
            "                latest_events=[pdu],",
            "                limit=10,",
            "                min_depth=min_depth,",
            "                timeout=60000,",
            "            )",
            "        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:",
            "            # We failed to get the missing events, but since we need to handle",
            "            # the case of `get_missing_events` not returning the necessary",
            "            # events anyway, it is safe to simply log the error and continue.",
            "            logger.warning(\"Failed to get prev_events: %s\", e)",
            "            return",
            "",
            "        logger.info(\"Got %d prev_events\", len(missing_events))",
            "        await self._process_pulled_events(origin, missing_events, backfilled=False)",
            "",
            "    @trace",
            "    async def _process_pulled_events(",
            "        self, origin: str, events: Collection[EventBase], backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a batch of events we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the events, persists the received events,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the events have already had their signatures and hashes checked.",
            "",
            "        Params:",
            "            origin: The server we received these events from",
            "            events: The received events.",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str([event.event_id for event in events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(events)),",
            "        )",
            "        set_tag(SynapseTags.FUNC_ARG_PREFIX + \"backfilled\", str(backfilled))",
            "        logger.debug(",
            "            \"processing pulled backfilled=%s events=%s\",",
            "            backfilled,",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        # Check if we already any of these have these events.",
            "        # Note: we currently make a lookup in the database directly here rather than",
            "        # checking the event cache, due to:",
            "        # https://github.com/matrix-org/synapse/issues/13476",
            "        existing_events_map = await self._store._get_events_from_db(",
            "            [event.event_id for event in events]",
            "        )",
            "",
            "        new_events: List[EventBase] = []",
            "        for event in events:",
            "            event_id = event.event_id",
            "",
            "            # If we've already seen this event ID...",
            "            if event_id in existing_events_map:",
            "                existing_event = existing_events_map[event_id]",
            "",
            "                # ...and the event itself was not previously stored as an outlier...",
            "                if not existing_event.event.internal_metadata.is_outlier():",
            "                    # ...then there's no need to persist it. We have it already.",
            "                    logger.info(",
            "                        \"_process_pulled_event: Ignoring received event %s which we \"",
            "                        \"have already seen\",",
            "                        event.event_id,",
            "                    )",
            "                    continue",
            "",
            "                # While we have seen this event before, it was stored as an outlier.",
            "                # We'll now persist it as a non-outlier.",
            "                logger.info(\"De-outliering event %s\", event_id)",
            "",
            "            # Continue on with the events that are new to us.",
            "            new_events.append(event)",
            "",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"new_events.length\",",
            "            str(len(new_events)),",
            "        )",
            "",
            "        @trace",
            "        async def _process_new_pulled_events(new_events: Collection[EventBase]) -> None:",
            "            # We want to sort these by depth so we process them and tell clients about",
            "            # them in order. It's also more efficient to backfill this way (`depth`",
            "            # ascending) because one backfill event is likely to be the `prev_event` of",
            "            # the next event we're going to process.",
            "            sorted_events = sorted(new_events, key=lambda x: x.depth)",
            "            for ev in sorted_events:",
            "                with nested_logging_context(ev.event_id):",
            "                    await self._process_pulled_event(origin, ev, backfilled=backfilled)",
            "",
            "        # Check if we've already tried to process these events at some point in the",
            "        # past. We aren't concerned with the expontntial backoff here, just whether it",
            "        # has failed to be processed before.",
            "        event_ids_with_failed_pull_attempts = (",
            "            await self._store.get_event_ids_with_failed_pull_attempts(",
            "                [event.event_id for event in new_events]",
            "            )",
            "        )",
            "",
            "        events_with_failed_pull_attempts, fresh_events = partition(",
            "            new_events, lambda e: e.event_id in event_ids_with_failed_pull_attempts",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"events_with_failed_pull_attempts\",",
            "            str(event_ids_with_failed_pull_attempts),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"events_with_failed_pull_attempts.length\",",
            "            str(len(events_with_failed_pull_attempts)),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"fresh_events\",",
            "            str([event.event_id for event in fresh_events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"fresh_events.length\",",
            "            str(len(fresh_events)),",
            "        )",
            "",
            "        # Process previously failed backfill events in the background to not waste",
            "        # time on something that is likely to fail again.",
            "        if len(events_with_failed_pull_attempts) > 0:",
            "            run_as_background_process(",
            "                \"_process_new_pulled_events_with_failed_pull_attempts\",",
            "                _process_new_pulled_events,",
            "                events_with_failed_pull_attempts,",
            "            )",
            "",
            "        # We can optimistically try to process and wait for the event to be fully",
            "        # persisted if we've never tried before.",
            "        if len(fresh_events) > 0:",
            "            await _process_new_pulled_events(fresh_events)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _process_pulled_event(",
            "        self, origin: str, event: EventBase, backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a single event that we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the event, persists the received event,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the event has already had its signatures and hashes checked.",
            "",
            "        This is somewhat equivalent to on_receive_pdu, but applies somewhat different",
            "        logic in the case that we are missing prev_events (in particular, it just",
            "        requests the state at that point, rather than triggering a get_missing_events) -",
            "        so is appropriate when we have pulled the event from a remote server, rather",
            "        than having it pushed to us.",
            "",
            "        Params:",
            "            origin: The server we received this event from",
            "            events: The received event",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        logger.info(\"Processing pulled event %s\", event)",
            "",
            "        # This function should not be used to persist outliers (use something",
            "        # else) because this does a bunch of operations that aren't necessary",
            "        # (extra work; in particular, it makes sure we have all the prev_events",
            "        # and resolves the state across those prev events). If you happen to run",
            "        # into a situation where the event you're trying to process/backfill is",
            "        # marked as an `outlier`, then you should update that spot to return an",
            "        # `EventBase` copy that doesn't have `outlier` flag set.",
            "        #",
            "        # `EventBase` is used to represent both an event we have not yet",
            "        # persisted, and one that we have persisted and now keep in the cache.",
            "        # In an ideal world this method would only be called with the first type",
            "        # of event, but it turns out that's not actually the case and for",
            "        # example, you could get an event from cache that is marked as an",
            "        # `outlier` (fix up that spot though).",
            "        assert not event.internal_metadata.is_outlier(), (",
            "            \"Outlier event passed to _process_pulled_event. \"",
            "            \"To persist an event as a non-outlier, make sure to pass in a copy without `event.internal_metadata.outlier = true`.\"",
            "        )",
            "",
            "        event_id = event.event_id",
            "",
            "        try:",
            "            self._sanity_check_event(event)",
            "        except SynapseError as err:",
            "            logger.warning(\"Event %s failed sanity check: %s\", event_id, err)",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(err)",
            "            )",
            "            return",
            "",
            "        try:",
            "            try:",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "            except PartialStateConflictError:",
            "                # The room was un-partial stated while we were processing the event.",
            "                # Try once more, with full state this time.",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "",
            "                # We ought to have full state now, barring some unlikely race where we left and",
            "                # rejoned the room in the background.",
            "                if context.partial_state:",
            "                    raise AssertionError(",
            "                        f\"Event {event.event_id} still has a partial resolved state \"",
            "                        f\"after room {event.room_id} was un-partial stated\"",
            "                    )",
            "",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "        except FederationPullAttemptBackoffError as exc:",
            "            # Log a warning about why we failed to process the event (the error message",
            "            # for `FederationPullAttemptBackoffError` is pretty good)",
            "            logger.warning(\"_process_pulled_event: %s\", exc)",
            "            # We do not record a failed pull attempt when we backoff fetching a missing",
            "            # `prev_event` because not being able to fetch the `prev_events` just means",
            "            # we won't be able to de-outlier the pulled event. But we can still use an",
            "            # `outlier` in the state/auth chain for another event. So we shouldn't stop",
            "            # a downstream event from trying to pull it.",
            "            #",
            "            # This avoids a cascade of backoff for all events in the DAG downstream from",
            "            # one event backoff upstream.",
            "        except FederationError as e:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(e)",
            "            )",
            "",
            "            if e.code == 403:",
            "                logger.warning(\"Pulled event %s failed history check.\", event_id)",
            "            else:",
            "                raise",
            "",
            "    @trace",
            "    async def _compute_event_context_with_maybe_missing_prevs(",
            "        self, dest: str, event: EventBase",
            "    ) -> EventContext:",
            "        \"\"\"Build an EventContext structure for a non-outlier event whose prev_events may",
            "        be missing.",
            "",
            "        This is used when we have pulled a batch of events from a remote server, and may",
            "        not have all the prev_events.",
            "",
            "        To build an EventContext, we need to calculate the state before the event. If we",
            "        already have all the prev_events for `event`, we can simply use the state after",
            "        the prev_events to calculate the state before `event`.",
            "",
            "        Otherwise, the missing prevs become new backwards extremities, and we fall back",
            "        to asking the remote server for the state after each missing `prev_event`,",
            "        and resolving across them.",
            "",
            "        That's ok provided we then resolve the state against other bits of the DAG",
            "        before using it - in other words, that the received event `event` is not going",
            "        to become the only forwards_extremity in the room (which will ensure that you",
            "        can't just take over a room by sending an event, withholding its prev_events,",
            "        and declaring yourself to be an admin in the subsequent state request).",
            "",
            "        In other words: we should only call this method if `event` has been *pulled*",
            "        as part of a batch of missing prev events, or similar.",
            "",
            "        Params:",
            "            dest: the remote server to ask for state at the missing prevs. Typically,",
            "                this will be the server we got `event` from.",
            "            event: an event to check for missing prevs.",
            "",
            "        Returns:",
            "            The event context.",
            "",
            "        Raises:",
            "            FederationPullAttemptBackoffError if we are are deliberately not attempting",
            "                to pull one of the given event's `prev_event`s over federation because",
            "                we've already done so recently and are backing off.",
            "            FederationError if we fail to get the state from the remote server after any",
            "                missing `prev_event`s.",
            "        \"\"\"",
            "        room_id = event.room_id",
            "        event_id = event.event_id",
            "",
            "        prevs = set(event.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        # If we've already recently attempted to pull this missing event, don't",
            "        # try it again so soon. Since we have to fetch all of the prev_events, we can",
            "        # bail early here if we find any to ignore.",
            "        prevs_with_pull_backoff = (",
            "            await self._store.get_event_ids_to_not_pull_from_backoff(",
            "                room_id, missing_prevs",
            "            )",
            "        )",
            "        if len(prevs_with_pull_backoff) > 0:",
            "            raise FederationPullAttemptBackoffError(",
            "                event_ids=prevs_with_pull_backoff.keys(),",
            "                message=(",
            "                    f\"While computing context for event={event_id}, not attempting to \"",
            "                    f\"pull missing prev_events={list(prevs_with_pull_backoff.keys())} \"",
            "                    \"because we already tried to pull recently (backing off).\"",
            "                ),",
            "                retry_after_ms=(",
            "                    max(prevs_with_pull_backoff.values()) - self._clock.time_msec()",
            "                ),",
            "            )",
            "",
            "        if not missing_prevs:",
            "            return await self._state_handler.compute_event_context(event)",
            "",
            "        logger.info(",
            "            \"Event %s is missing prev_events %s: calculating state for a \"",
            "            \"backwards extremity\",",
            "            event_id,",
            "            shortstr(missing_prevs),",
            "        )",
            "        # Calculate the state after each of the previous events, and",
            "        # resolve them to find the correct state at the current event.",
            "",
            "        try:",
            "            # Determine whether we may be about to retrieve partial state",
            "            # Events may be un-partial stated right after we compute the partial state",
            "            # flag, but that's okay, as long as the flag errs on the conservative side.",
            "            partial_state_flags = await self._store.get_partial_state_events(seen)",
            "            partial_state = any(partial_state_flags.values())",
            "",
            "            # Get the state of the events we know about",
            "            ours = await self._state_storage_controller.get_state_groups_ids(",
            "                room_id, seen, await_full_state=False",
            "            )",
            "",
            "            # state_maps is a list of mappings from (type, state_key) to event_id",
            "            state_maps: List[StateMap[str]] = list(ours.values())",
            "",
            "            # we don't need this any more, let's delete it.",
            "            del ours",
            "",
            "            # Ask the remote server for the states we don't",
            "            # know about",
            "            for p in missing_prevs:",
            "                logger.info(\"Requesting state after missing prev_event %s\", p)",
            "",
            "                with nested_logging_context(p):",
            "                    # note that if any of the missing prevs share missing state or",
            "                    # auth events, the requests to fetch those events are deduped",
            "                    # by the get_pdu_cache in federation_client.",
            "                    remote_state_map = (",
            "                        await self._get_state_ids_after_missing_prev_event(",
            "                            dest, room_id, p",
            "                        )",
            "                    )",
            "",
            "                    state_maps.append(remote_state_map)",
            "",
            "            room_version = await self._store.get_room_version_id(room_id)",
            "            state_map = await self._state_resolution_handler.resolve_events_with_store(",
            "                room_id,",
            "                room_version,",
            "                state_maps,",
            "                event_map={event_id: event},",
            "                state_res_store=StateResolutionStore(self._store),",
            "            )",
            "",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Error attempting to resolve state at missing prev_events: %s\", e",
            "            )",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                403,",
            "                \"We can't get valid state history.\",",
            "                affected=event_id,",
            "            )",
            "        return await self._state_handler.compute_event_context(",
            "            event, state_ids_before_event=state_map, partial_state=partial_state",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_ids_after_missing_prev_event(",
            "        self,",
            "        destination: str,",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> StateMap[str]:",
            "        \"\"\"Requests all of the room state at a given event from a remote homeserver.",
            "",
            "        Args:",
            "            destination: The remote homeserver to query for the state.",
            "            room_id: The id of the room we're interested in.",
            "            event_id: The id of the event we want the state at.",
            "",
            "        Returns:",
            "            The event ids of the state *after* the given event.",
            "",
            "        Raises:",
            "            InvalidResponseError: if the remote homeserver's response contains fields",
            "                of the wrong type.",
            "        \"\"\"",
            "",
            "        # It would be better if we could query the difference from our known",
            "        # state to the given `event_id` so the sending server doesn't have to",
            "        # send as much and we don't have to process as many events. For example",
            "        # in a room like #matrix:matrix.org, we get 200k events (77k state_events, 122k",
            "        # auth_events) from this call.",
            "        #",
            "        # Tracked by https://github.com/matrix-org/synapse/issues/13618",
            "        (",
            "            state_event_ids,",
            "            auth_event_ids,",
            "        ) = await self._federation_client.get_room_state_ids(",
            "            destination, room_id, event_id=event_id",
            "        )",
            "",
            "        logger.debug(",
            "            \"state_ids returned %i state events, %i auth events\",",
            "            len(state_event_ids),",
            "            len(auth_event_ids),",
            "        )",
            "",
            "        # Start by checking events we already have in the DB",
            "        desired_events = set(state_event_ids)",
            "        desired_events.add(event_id)",
            "        logger.debug(\"Fetching %i events from cache/store\", len(desired_events))",
            "        have_events = await self._store.have_seen_events(room_id, desired_events)",
            "",
            "        missing_desired_event_ids = desired_events - have_events",
            "        logger.debug(",
            "            \"We are missing %i events (got %i)\",",
            "            len(missing_desired_event_ids),",
            "            len(have_events),",
            "        )",
            "",
            "        # We probably won't need most of the auth events, so let's just check which",
            "        # we have for now, rather than thrashing the event cache with them all",
            "        # unnecessarily.",
            "",
            "        # TODO: we probably won't actually need all of the auth events, since we",
            "        #   already have a bunch of the state events. It would be nice if the",
            "        #   federation api gave us a way of finding out which we actually need.",
            "",
            "        missing_auth_event_ids = set(auth_event_ids) - have_events",
            "        missing_auth_event_ids.difference_update(",
            "            await self._store.have_seen_events(room_id, missing_auth_event_ids)",
            "        )",
            "        logger.debug(\"We are also missing %i auth events\", len(missing_auth_event_ids))",
            "",
            "        missing_event_ids = missing_desired_event_ids | missing_auth_event_ids",
            "",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids\",",
            "            str(missing_auth_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids.length\",",
            "            str(len(missing_auth_event_ids)),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids\",",
            "            str(missing_desired_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids.length\",",
            "            str(len(missing_desired_event_ids)),",
            "        )",
            "",
            "        # Making an individual request for each of 1000s of events has a lot of",
            "        # overhead. On the other hand, we don't really want to fetch all of the events",
            "        # if we already have most of them.",
            "        #",
            "        # As an arbitrary heuristic, if we are missing more than 10% of the events, then",
            "        # we fetch the whole state.",
            "        #",
            "        # TODO: might it be better to have an API which lets us do an aggregate event",
            "        #   request",
            "        if (len(missing_event_ids) * 10) >= len(auth_event_ids) + len(state_event_ids):",
            "            logger.debug(\"Requesting complete state from remote\")",
            "            await self._get_state_and_persist(destination, room_id, event_id)",
            "        else:",
            "            logger.debug(\"Fetching %i events from remote\", len(missing_event_ids))",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=missing_event_ids",
            "            )",
            "",
            "        # We now need to fill out the state map, which involves fetching the",
            "        # type and state key for each event ID in the state.",
            "        state_map = {}",
            "",
            "        event_metadata = await self._store.get_metadata_for_events(state_event_ids)",
            "        for state_event_id, metadata in event_metadata.items():",
            "            if metadata.room_id != room_id:",
            "                # This is a bogus situation, but since we may only discover it a long time",
            "                # after it happened, we try our best to carry on, by just omitting the",
            "                # bad events from the returned state set.",
            "                #",
            "                # This can happen if a remote server claims that the state or",
            "                # auth_events at an event in room A are actually events in room B",
            "                logger.warning(",
            "                    \"Remote server %s claims event %s in room %s is an auth/state \"",
            "                    \"event in room %s\",",
            "                    destination,",
            "                    state_event_id,",
            "                    metadata.room_id,",
            "                    room_id,",
            "                )",
            "                continue",
            "",
            "            if metadata.state_key is None:",
            "                logger.warning(",
            "                    \"Remote server gave us non-state event in state: %s\", state_event_id",
            "                )",
            "                continue",
            "",
            "            state_map[(metadata.event_type, metadata.state_key)] = state_event_id",
            "",
            "        # if we couldn't get the prev event in question, that's a problem.",
            "        remote_event = await self._store.get_event(",
            "            event_id,",
            "            allow_none=True,",
            "            allow_rejected=True,",
            "            redact_behaviour=EventRedactBehaviour.as_is,",
            "        )",
            "        if not remote_event:",
            "            raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))",
            "",
            "        # missing state at that event is a warning, not a blocker",
            "        # XXX: this doesn't sound right? it means that we'll end up with incomplete",
            "        #   state.",
            "        failed_to_fetch = desired_events - event_metadata.keys()",
            "        # `event_id` could be missing from `event_metadata` because it's not necessarily",
            "        # a state event. We've already checked that we've fetched it above.",
            "        failed_to_fetch.discard(event_id)",
            "        if failed_to_fetch:",
            "            logger.warning(",
            "                \"Failed to fetch missing state events for %s %s\",",
            "                event_id,",
            "                failed_to_fetch,",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch\",",
            "                str(failed_to_fetch),",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch.length\",",
            "                str(len(failed_to_fetch)),",
            "            )",
            "",
            "        if remote_event.is_state() and remote_event.rejected_reason is None:",
            "            state_map[",
            "                (remote_event.type, remote_event.state_key)",
            "            ] = remote_event.event_id",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_and_persist(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"Get the complete room state at a given event, and persist any new events",
            "        as outliers\"\"\"",
            "        room_version = await self._store.get_room_version(room_id)",
            "        auth_events, state_events = await self._federation_client.get_room_state(",
            "            destination, room_id, event_id=event_id, room_version=room_version",
            "        )",
            "        logger.info(\"/state returned %i events\", len(auth_events) + len(state_events))",
            "",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state_events)",
            "        )",
            "",
            "        # we also need the event itself.",
            "        if not await self._store.have_seen_event(room_id, event_id):",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=(event_id,)",
            "            )",
            "",
            "    @trace",
            "    async def _process_received_pdu(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        backfilled: bool = False,",
            "    ) -> None:",
            "        \"\"\"Called when we have a new non-outlier event.",
            "",
            "        This is called when we have a new event to add to the room DAG. This can be",
            "        due to:",
            "           * events received directly via a /send request",
            "           * events retrieved via get_missing_events after a /send request",
            "           * events backfilled after a client request.",
            "",
            "        It's not currently used for events received from incoming send_{join,knock,leave}",
            "        requests (which go via on_send_membership_event), nor for joins created by a",
            "        remote join dance (which go via process_remote_join).",
            "",
            "        We need to do auth checks and put it through the StateHandler.",
            "",
            "        Args:",
            "            origin: server sending the event",
            "",
            "            event: event to be persisted",
            "",
            "            context: The `EventContext` to persist the event with.",
            "",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "",
            "        PartialStateConflictError: if the room was un-partial stated in between",
            "            computing the state at the event and persisting it. The caller should",
            "            recompute `context` and retry exactly once when this happens.",
            "        \"\"\"",
            "        logger.debug(\"Processing event: %s\", event)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        try:",
            "            await self._check_event_auth(origin, event, context)",
            "        except AuthError as e:",
            "            # This happens only if we couldn't find the auth events. We'll already have",
            "            # logged a warning, so now we just convert to a FederationError.",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)",
            "",
            "        if not backfilled and not context.rejected:",
            "            # For new (non-backfilled and non-outlier) events we check if the event",
            "            # passes auth based on the current state. If it doesn't then we",
            "            # \"soft-fail\" the event.",
            "            await self._check_for_soft_fail(event, context=context, origin=origin)",
            "",
            "        await self._run_push_actions_and_persist_event(event, context, backfilled)",
            "",
            "        if backfilled or context.rejected:",
            "            return",
            "",
            "        await self._maybe_kick_guest_users(event)",
            "",
            "        # For encrypted messages we check that we know about the sending device,",
            "        # if we don't then we mark the device cache for that user as stale.",
            "        if event.type == EventTypes.Encrypted:",
            "            device_id = event.content.get(\"device_id\")",
            "            sender_key = event.content.get(\"sender_key\")",
            "",
            "            cached_devices = await self._store.get_cached_devices_for_user(event.sender)",
            "",
            "            resync = False  # Whether we should resync device lists.",
            "",
            "            device = None",
            "            if device_id is not None:",
            "                device = cached_devices.get(device_id)",
            "                if device is None:",
            "                    logger.info(",
            "                        \"Received event from remote device not in our cache: %s %s\",",
            "                        event.sender,",
            "                        device_id,",
            "                    )",
            "                    resync = True",
            "",
            "            # We also check if the `sender_key` matches what we expect.",
            "            if sender_key is not None:",
            "                # Figure out what sender key we're expecting. If we know the",
            "                # device and recognize the algorithm then we can work out the",
            "                # exact key to expect. Otherwise check it matches any key we",
            "                # have for that device.",
            "",
            "                current_keys: Container[str] = []",
            "",
            "                if device:",
            "                    keys = device.get(\"keys\", {}).get(\"keys\", {})",
            "",
            "                    if (",
            "                        event.content.get(\"algorithm\")",
            "                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2",
            "                    ):",
            "                        # For this algorithm we expect a curve25519 key.",
            "                        key_name = \"curve25519:%s\" % (device_id,)",
            "                        current_keys = [keys.get(key_name)]",
            "                    else:",
            "                        # We don't know understand the algorithm, so we just",
            "                        # check it matches a key for the device.",
            "                        current_keys = keys.values()",
            "                elif device_id:",
            "                    # We don't have any keys for the device ID.",
            "                    pass",
            "                else:",
            "                    # The event didn't include a device ID, so we just look for",
            "                    # keys across all devices.",
            "                    current_keys = [",
            "                        key",
            "                        for device in cached_devices.values()",
            "                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()",
            "                    ]",
            "",
            "                # We now check that the sender key matches (one of) the expected",
            "                # keys.",
            "                if sender_key not in current_keys:",
            "                    logger.info(",
            "                        \"Received event from remote device with unexpected sender key: %s %s: %s\",",
            "                        event.sender,",
            "                        device_id or \"<no device_id>\",",
            "                        sender_key,",
            "                    )",
            "                    resync = True",
            "",
            "            if resync:",
            "                run_as_background_process(",
            "                    \"resync_device_due_to_pdu\",",
            "                    self._resync_device,",
            "                    event.sender,",
            "                )",
            "",
            "    async def _resync_device(self, sender: str) -> None:",
            "        \"\"\"We have detected that the device list for the given user may be out",
            "        of sync, so we try and resync them.",
            "        \"\"\"",
            "",
            "        try:",
            "            await self._store.mark_remote_users_device_caches_as_stale((sender,))",
            "",
            "            # Immediately attempt a resync in the background",
            "            if self._config.worker.worker_app:",
            "                await self._multi_user_device_resync(user_ids=[sender])",
            "            else:",
            "                await self._device_list_updater.multi_user_device_resync(",
            "                    user_ids=[sender]",
            "                )",
            "        except Exception:",
            "            logger.exception(\"Failed to resync device for %s\", sender)",
            "",
            "    async def backfill_event_id(",
            "        self, destinations: StrCollection, room_id: str, event_id: str",
            "    ) -> PulledPduInfo:",
            "        \"\"\"Backfill a single event and persist it as a non-outlier which means",
            "        we also pull in all of the state and auth events necessary for it.",
            "",
            "        Args:",
            "            destination: The homeserver to pull the given event_id from.",
            "            room_id: The room where the event is from.",
            "            event_id: The event ID to backfill.",
            "",
            "        Raises:",
            "            FederationError if we are unable to find the event from the destination",
            "        \"\"\"",
            "        logger.info(\"backfill_event_id: event_id=%s\", event_id)",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        pulled_pdu_info = await self._federation_client.get_pdu(",
            "            destinations,",
            "            event_id,",
            "            room_version,",
            "        )",
            "",
            "        if not pulled_pdu_info:",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                404,",
            "                f\"Unable to find event_id={event_id} from remote servers to backfill.\",",
            "                affected=event_id,",
            "            )",
            "",
            "        # Persist the event we just fetched, including pulling all of the state",
            "        # and auth events to de-outlier it. This also sets up the necessary",
            "        # `state_groups` for the event.",
            "        await self._process_pulled_events(",
            "            pulled_pdu_info.pull_origin,",
            "            [pulled_pdu_info.pdu],",
            "            # Prevent notifications going to clients",
            "            backfilled=True,",
            "        )",
            "",
            "        return pulled_pdu_info",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_events_and_persist(",
            "        self, destination: str, room_id: str, event_ids: StrCollection",
            "    ) -> None:",
            "        \"\"\"Fetch the given events from a server, and persist them as outliers.",
            "",
            "        This function *does not* recursively get missing auth events of the",
            "        newly fetched events. Callers must include in the `event_ids` argument",
            "        any missing events from the auth chain.",
            "",
            "        Logs a warning if we can't find the given event.",
            "        \"\"\"",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        events: List[EventBase] = []",
            "",
            "        async def get_event(event_id: str) -> None:",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    pulled_pdu_info = await self._federation_client.get_pdu(",
            "                        [destination],",
            "                        event_id,",
            "                        room_version,",
            "                    )",
            "                    if pulled_pdu_info is None:",
            "                        logger.warning(",
            "                            \"Server %s didn't return event %s\",",
            "                            destination,",
            "                            event_id,",
            "                        )",
            "                        return",
            "                    events.append(pulled_pdu_info.pdu)",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"Error fetching missing state/auth event %s: %s %s\",",
            "                        event_id,",
            "                        type(e),",
            "                        e,",
            "                    )",
            "",
            "        await concurrently_execute(get_event, event_ids, 5)",
            "        logger.info(\"Fetched %i events of %i requested\", len(events), len(event_ids))",
            "        await self._auth_and_persist_outliers(room_id, events)",
            "",
            "    @trace",
            "    async def _auth_and_persist_outliers(",
            "        self, room_id: str, events: Iterable[EventBase]",
            "    ) -> None:",
            "        \"\"\"Persist a batch of outlier events fetched from remote servers.",
            "",
            "        We first sort the events to make sure that we process each event's auth_events",
            "        before the event itself.",
            "",
            "        We then mark the events as outliers, persist them to the database, and, where",
            "        appropriate (eg, an invite), awake the notifier.",
            "",
            "        Params:",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            events: the events that have been fetched",
            "        \"\"\"",
            "        event_map = {event.event_id: event for event in events}",
            "",
            "        event_ids = event_map.keys()",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str(event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(event_ids)),",
            "        )",
            "",
            "        # filter out any events we have already seen. This might happen because",
            "        # the events were eagerly pushed to us (eg, during a room join), or because",
            "        # another thread has raced against us since we decided to request the event.",
            "        #",
            "        # This is just an optimisation, so it doesn't need to be watertight - the event",
            "        # persister does another round of deduplication.",
            "        seen_remotes = await self._store.have_seen_events(room_id, event_map.keys())",
            "        for s in seen_remotes:",
            "            event_map.pop(s, None)",
            "",
            "        # XXX: it might be possible to kick this process off in parallel with fetching",
            "        # the events.",
            "        while event_map:",
            "            # build a list of events whose auth events are not in the queue.",
            "            roots = tuple(",
            "                ev",
            "                for ev in event_map.values()",
            "                if not any(aid in event_map for aid in ev.auth_event_ids())",
            "            )",
            "",
            "            if not roots:",
            "                # if *none* of the remaining events are ready, that means",
            "                # we have a loop. This either means a bug in our logic, or that",
            "                # somebody has managed to create a loop (which requires finding a",
            "                # hash collision in room v2 and later).",
            "                logger.warning(",
            "                    \"Loop found in auth events while fetching missing state/auth \"",
            "                    \"events: %s\",",
            "                    shortstr(event_map.keys()),",
            "                )",
            "                return",
            "",
            "            logger.info(",
            "                \"Persisting %i of %i remaining outliers: %s\",",
            "                len(roots),",
            "                len(event_map),",
            "                shortstr(e.event_id for e in roots),",
            "            )",
            "",
            "            await self._auth_and_persist_outliers_inner(room_id, roots)",
            "",
            "            for ev in roots:",
            "                del event_map[ev.event_id]",
            "",
            "    async def _auth_and_persist_outliers_inner(",
            "        self, room_id: str, fetched_events: Collection[EventBase]",
            "    ) -> None:",
            "        \"\"\"Helper for _auth_and_persist_outliers",
            "",
            "        Persists a batch of events where we have (theoretically) already persisted all",
            "        of their auth events.",
            "",
            "        Marks the events as outliers, auths them, persists them to the database, and,",
            "        where appropriate (eg, an invite), awakes the notifier.",
            "",
            "        Params:",
            "            origin: where the events came from",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            fetched_events: the events to persist",
            "        \"\"\"",
            "        # get all the auth events for all the events in this batch. By now, they should",
            "        # have been persisted.",
            "        auth_events = {",
            "            aid for event in fetched_events for aid in event.auth_event_ids()",
            "        }",
            "        persisted_events = await self._store.get_events(",
            "            auth_events,",
            "            allow_rejected=True,",
            "        )",
            "",
            "        events_and_contexts_to_persist: List[Tuple[EventBase, EventContext]] = []",
            "",
            "        async def prep(event: EventBase) -> None:",
            "            with nested_logging_context(suffix=event.event_id):",
            "                auth = []",
            "                for auth_event_id in event.auth_event_ids():",
            "                    ae = persisted_events.get(auth_event_id)",
            "                    if not ae:",
            "                        # the fact we can't find the auth event doesn't mean it doesn't",
            "                        # exist, which means it is premature to reject `event`. Instead we",
            "                        # just ignore it for now.",
            "                        logger.warning(",
            "                            \"Dropping event %s, which relies on auth_event %s, which could not be found\",",
            "                            event,",
            "                            auth_event_id,",
            "                        )",
            "                        return",
            "                    auth.append(ae)",
            "",
            "                # we're not bothering about room state, so flag the event as an outlier.",
            "                event.internal_metadata.outlier = True",
            "",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    await check_state_independent_auth_rules(self._store, event)",
            "                    check_state_dependent_auth_rules(event, auth)",
            "                except AuthError as e:",
            "                    logger.warning(\"Rejecting %r because %s\", event, e)",
            "                    context.rejected = RejectedReason.AUTH_ERROR",
            "                except EventSizeError as e:",
            "                    if e.unpersistable:",
            "                        # This event is completely unpersistable.",
            "                        raise e",
            "                    # Otherwise, we are somewhat lenient and just persist the event",
            "                    # as rejected, for moderate compatibility with older Synapse",
            "                    # versions.",
            "                    logger.warning(\"While validating received event %r: %s\", event, e)",
            "                    context.rejected = RejectedReason.OVERSIZED_EVENT",
            "",
            "            events_and_contexts_to_persist.append((event, context))",
            "",
            "        for event in fetched_events:",
            "            await prep(event)",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            events_and_contexts_to_persist,",
            "            # Mark these events backfilled as they're historic events that will",
            "            # eventually be backfilled. For example, missing events we fetch",
            "            # during backfill should be marked as backfilled as well.",
            "            backfilled=True,",
            "        )",
            "",
            "    @trace",
            "    async def _check_event_auth(",
            "        self, origin: Optional[str], event: EventBase, context: EventContext",
            "    ) -> None:",
            "        \"\"\"",
            "        Checks whether an event should be rejected (for failing auth checks).",
            "",
            "        Args:",
            "            origin: The host the event originates from. This is used to fetch",
            "               any missing auth events. It can be set to None, but only if we are",
            "               sure that we already have all the auth events.",
            "            event: The event itself.",
            "            context:",
            "                The event context.",
            "",
            "        Raises:",
            "            AuthError if we were unable to find copies of the event's auth events.",
            "               (Most other failures just cause us to set `context.rejected`.)",
            "        \"\"\"",
            "        # This method should only be used for non-outliers",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # first of all, check that the event itself is valid.",
            "        try:",
            "            validate_event_for_room_version(event)",
            "        except AuthError as e:",
            "            logger.warning(\"While validating received event %r: %s\", event, e)",
            "            # TODO: use a different rejected reason here?",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "        except EventSizeError as e:",
            "            if e.unpersistable:",
            "                # This event is completely unpersistable.",
            "                raise e",
            "            # Otherwise, we are somewhat lenient and just persist the event",
            "            # as rejected, for moderate compatibility with older Synapse",
            "            # versions.",
            "            logger.warning(\"While validating received event %r: %s\", event, e)",
            "            context.rejected = RejectedReason.OVERSIZED_EVENT",
            "            return",
            "",
            "        # next, check that we have all of the event's auth events.",
            "        #",
            "        # Note that this can raise AuthError, which we want to propagate to the",
            "        # caller rather than swallow with `context.rejected` (since we cannot be",
            "        # certain that there is a permanent problem with the event).",
            "        claimed_auth_events = await self._load_or_fetch_auth_events_for_event(",
            "            origin, event",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events\",",
            "            str([ev.event_id for ev in claimed_auth_events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events.length\",",
            "            str(len(claimed_auth_events)),",
            "        )",
            "",
            "        # ... and check that the event passes auth at those auth events.",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   4. Passes authorization rules based on the event\u2019s auth events,",
            "        #      otherwise it is rejected.",
            "        try:",
            "            await check_state_independent_auth_rules(self._store, event)",
            "            check_state_dependent_auth_rules(event, claimed_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against auth_events: %s\", event, e",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "",
            "        # now check the auth rules pass against the room state before the event",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   5. Passes authorization rules based on the state before the event,",
            "        #      otherwise it is rejected.",
            "        #",
            "        # ... however, if we only have partial state for the room, then there is a good",
            "        # chance that we'll be missing some of the state needed to auth the new event.",
            "        # So, we state-resolve the auth events that we are given against the state that",
            "        # we know about, which ensures things like bans are applied. (Note that we'll",
            "        # already have checked we have all the auth events, in",
            "        # _load_or_fetch_auth_events_for_event above)",
            "        if context.partial_state:",
            "            room_version = await self._store.get_room_version_id(event.room_id)",
            "",
            "            local_state_id_map = await context.get_prev_state_ids()",
            "            claimed_auth_events_id_map = {",
            "                (ev.type, ev.state_key): ev.event_id for ev in claimed_auth_events",
            "            }",
            "",
            "            state_for_auth_id_map = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    [local_state_id_map, claimed_auth_events_id_map],",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "            state_for_auth_id_map = await context.get_prev_state_ids(",
            "                StateFilter.from_types(event_types)",
            "            )",
            "",
            "        calculated_auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "            event, state_for_auth_id_map, for_verification=True",
            "        )",
            "",
            "        # if those are the same, we're done here.",
            "        if collections.Counter(event.auth_event_ids()) == collections.Counter(",
            "            calculated_auth_event_ids",
            "        ):",
            "            return",
            "",
            "        # otherwise, re-run the auth checks based on what we calculated.",
            "        calculated_auth_events = await self._store.get_events_as_list(",
            "            calculated_auth_event_ids",
            "        )",
            "",
            "        # log the differences",
            "",
            "        claimed_auth_event_map = {(e.type, e.state_key): e for e in claimed_auth_events}",
            "        calculated_auth_event_map = {",
            "            (e.type, e.state_key): e for e in calculated_auth_events",
            "        }",
            "        logger.info(",
            "            \"event's auth_events are different to our calculated auth_events. \"",
            "            \"Claimed but not calculated: %s. Calculated but not claimed: %s\",",
            "            [",
            "                ev",
            "                for k, ev in claimed_auth_event_map.items()",
            "                if k not in calculated_auth_event_map",
            "                or calculated_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "            [",
            "                ev",
            "                for k, ev in calculated_auth_event_map.items()",
            "                if k not in claimed_auth_event_map",
            "                or claimed_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, calculated_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against room state before the event: %s\",",
            "                event,",
            "                e,",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "    @trace",
            "    async def _maybe_kick_guest_users(self, event: EventBase) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state = await self._storage_controllers.state.get_current_state(",
            "            event.room_id",
            "        )",
            "        current_state_list = list(current_state.values())",
            "        await self._get_room_member_handler().kick_guest_users(current_state_list)",
            "",
            "    async def _check_for_soft_fail(",
            "        self,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        origin: str,",
            "    ) -> None:",
            "        \"\"\"Checks if we should soft fail the event; if so, marks the event as",
            "        such.",
            "",
            "        Does nothing for events in rooms with partial state, since we may not have an",
            "        accurate membership event for the sender in the current state.",
            "",
            "        Args:",
            "            event",
            "            context: The `EventContext` which we are about to persist the event with.",
            "            origin: The host the event originates from.",
            "        \"\"\"",
            "        if await self._store.is_partial_state_room(event.room_id):",
            "            # We might not know the sender's membership in the current state, so don't",
            "            # soft fail anything. Even if we do have a membership for the sender in the",
            "            # current state, it may have been derived from state resolution between",
            "            # partial and full state and may not be accurate.",
            "            return",
            "",
            "        extrem_ids = await self._store.get_latest_event_ids_in_room(event.room_id)",
            "        prev_event_ids = set(event.prev_event_ids())",
            "",
            "        if extrem_ids == prev_event_ids:",
            "            # If they're the same then the current state is the same as the",
            "            # state at the event, so no point rechecking auth for soft fail.",
            "            return",
            "",
            "        room_version = await self._store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        # The event types we want to pull from the \"current\" state.",
            "        auth_types = auth_types_for_event(room_version_obj, event)",
            "",
            "        # Calculate the \"current state\".",
            "        seen_event_ids = await self._store.have_events_in_timeline(prev_event_ids)",
            "        has_missing_prevs = bool(prev_event_ids - seen_event_ids)",
            "        if has_missing_prevs:",
            "            # We don't have all the prev_events of this event, which means we have a",
            "            # gap in the graph, and the new event is going to become a new backwards",
            "            # extremity.",
            "            #",
            "            # In this case we want to be a little careful as we might have been",
            "            # down for a while and have an incorrect view of the current state,",
            "            # however we still want to do checks as gaps are easy to",
            "            # maliciously manufacture.",
            "            #",
            "            # So we use a \"current state\" that is actually a state",
            "            # resolution across the current forward extremities and the",
            "            # given state at the event. This should correctly handle cases",
            "            # like bans, especially with state res v2.",
            "",
            "            state_sets_d = await self._state_storage_controller.get_state_groups_ids(",
            "                event.room_id, extrem_ids",
            "            )",
            "            state_sets: List[StateMap[str]] = list(state_sets_d.values())",
            "            state_ids = await context.get_prev_state_ids()",
            "            state_sets.append(state_ids)",
            "            current_state_ids = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    state_sets,",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            current_state_ids = (",
            "                await self._state_storage_controller.get_current_state_ids(",
            "                    event.room_id, StateFilter.from_types(auth_types)",
            "                )",
            "            )",
            "",
            "        logger.debug(",
            "            \"Doing soft-fail check for %s: state %s\",",
            "            event.event_id,",
            "            current_state_ids,",
            "        )",
            "",
            "        # Now check if event pass auth against said current state",
            "        current_state_ids_list = [",
            "            e for k, e in current_state_ids.items() if k in auth_types",
            "        ]",
            "        current_auth_events = await self._store.get_events_as_list(",
            "            current_state_ids_list",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, current_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"Soft-failing %r (from %s) because %s\",",
            "                event,",
            "                e,",
            "                origin,",
            "                extra={",
            "                    \"room_id\": event.room_id,",
            "                    \"mxid\": event.sender,",
            "                    \"hs\": origin,",
            "                },",
            "            )",
            "            soft_failed_event_counter.inc()",
            "            event.internal_metadata.soft_failed = True",
            "",
            "    async def _load_or_fetch_auth_events_for_event(",
            "        self, destination: Optional[str], event: EventBase",
            "    ) -> Collection[EventBase]:",
            "        \"\"\"Fetch this event's auth_events, from database or remote",
            "",
            "        Loads any of the auth_events that we already have from the database/cache. If",
            "        there are any that are missing, calls /event_auth to get the complete auth",
            "        chain for the event (and then attempts to load the auth_events again).",
            "",
            "        If any of the auth_events cannot be found, raises an AuthError. This can happen",
            "        for a number of reasons; eg: the events don't exist, or we were unable to talk",
            "        to `destination`, or we couldn't validate the signature on the event (which",
            "        in turn has multiple potential causes).",
            "",
            "        Args:",
            "            destination: where to send the /event_auth request. Typically the server",
            "               that sent us `event` in the first place.",
            "",
            "               If this is None, no attempt is made to load any missing auth events:",
            "               rather, an AssertionError is raised if there are any missing events.",
            "",
            "            event: the event whose auth_events we want",
            "",
            "        Returns:",
            "            all of the events listed in `event.auth_events_ids`, after deduplication",
            "",
            "        Raises:",
            "            AssertionError if some auth events were missing and no `destination` was",
            "            supplied.",
            "",
            "            AuthError if we were unable to fetch the auth_events for any reason.",
            "        \"\"\"",
            "        event_auth_event_ids = set(event.auth_event_ids())",
            "        event_auth_events = await self._store.get_events(",
            "            event_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids = event_auth_event_ids.difference(",
            "            event_auth_events.keys()",
            "        )",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "        if destination is None:",
            "            # this shouldn't happen: destination must be set unless we know we have already",
            "            # persisted the auth events.",
            "            raise AssertionError(",
            "                \"_load_or_fetch_auth_events_for_event() called with no destination for \"",
            "                \"an event with missing auth_events\"",
            "            )",
            "",
            "        logger.info(",
            "            \"Event %s refers to unknown auth events %s: fetching auth chain\",",
            "            event,",
            "            missing_auth_event_ids,",
            "        )",
            "        try:",
            "            await self._get_remote_auth_chain_for_event(",
            "                destination, event.room_id, event.event_id",
            "            )",
            "        except Exception as e:",
            "            logger.warning(\"Failed to get auth chain for %s: %s\", event, e)",
            "            # in this case, it's very likely we still won't have all the auth",
            "            # events - but we pick that up below.",
            "",
            "        # try to fetch the auth events we missed list time.",
            "        extra_auth_events = await self._store.get_events(",
            "            missing_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids.difference_update(extra_auth_events.keys())",
            "        event_auth_events.update(extra_auth_events)",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "",
            "        # we still don't have all the auth events.",
            "        logger.warning(",
            "            \"Missing auth events for %s: %s\",",
            "            event,",
            "            shortstr(missing_auth_event_ids),",
            "        )",
            "        # the fact we can't find the auth event doesn't mean it doesn't",
            "        # exist, which means it is premature to store `event` as rejected.",
            "        # instead we raise an AuthError, which will make the caller ignore it.",
            "        raise AuthError(code=HTTPStatus.FORBIDDEN, msg=\"Auth events could not be found\")",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_remote_auth_chain_for_event(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"If we are missing some of an event's auth events, attempt to request them",
            "",
            "        Args:",
            "            destination: where to fetch the auth tree from",
            "            room_id: the room in which we are lacking auth events",
            "            event_id: the event for which we are lacking auth events",
            "        \"\"\"",
            "        try:",
            "            remote_events = await self._federation_client.get_event_auth(",
            "                destination, room_id, event_id",
            "            )",
            "",
            "        except RequestSendFailed as e1:",
            "            # The other side isn't around or doesn't implement the",
            "            # endpoint, so lets just bail out.",
            "            logger.info(\"Failed to get event auth from remote: %s\", e1)",
            "            return",
            "",
            "        logger.info(\"/event_auth returned %i events\", len(remote_events))",
            "",
            "        # `event` may be returned, but we should not yet process it.",
            "        remote_auth_events = (e for e in remote_events if e.event_id != event_id)",
            "",
            "        await self._auth_and_persist_outliers(room_id, remote_auth_events)",
            "",
            "    @trace",
            "    async def _run_push_actions_and_persist_event(",
            "        self, event: EventBase, context: EventContext, backfilled: bool = False",
            "    ) -> None:",
            "        \"\"\"Run the push actions for a received event, and persist it.",
            "",
            "        Args:",
            "            event: The event itself.",
            "            context: The event context.",
            "            backfilled: True if the event was backfilled.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "        # this method should not be called on outliers (those code paths call",
            "        # persist_events_and_notify directly.)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        if not backfilled and not context.rejected:",
            "            min_depth = await self._store.get_min_depth(event.room_id)",
            "            if min_depth is None or min_depth > event.depth:",
            "                # XXX richvdh 2021/10/07: I don't really understand what this",
            "                # condition is doing. I think it's trying not to send pushes",
            "                # for events that predate our join - but that's not really what",
            "                # min_depth means, and anyway ancient events are a more general",
            "                # problem.",
            "                #",
            "                # for now I'm just going to log about it.",
            "                logger.info(",
            "                    \"Skipping push actions for old event with depth %s < %s\",",
            "                    event.depth,",
            "                    min_depth,",
            "                )",
            "            else:",
            "                await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "                    [(event, context)]",
            "                )",
            "",
            "        try:",
            "            await self.persist_events_and_notify(",
            "                event.room_id, [(event, context)], backfilled=backfilled",
            "            )",
            "        except Exception:",
            "            await self._store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def persist_events_and_notify(",
            "        self,",
            "        room_id: str,",
            "        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],",
            "        backfilled: bool = False,",
            "    ) -> int:",
            "        \"\"\"Persists events and tells the notifier/pushers about them, if",
            "        necessary.",
            "",
            "        Args:",
            "            room_id: The room ID of events being persisted.",
            "            event_and_contexts: Sequence of events with their associated",
            "                context that should be persisted. All events must belong to",
            "                the same room.",
            "            backfilled: Whether these events are a result of",
            "                backfilling or not",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        if not event_and_contexts:",
            "            return self._store.get_room_max_stream_ordering()",
            "",
            "        instance = self._config.worker.events_shard_config.get_instance(room_id)",
            "        if instance != self._instance_name:",
            "            # Limit the number of events sent over replication. We choose 200",
            "            # here as that is what we default to in `max_request_body_size(..)`",
            "            result = {}",
            "            try:",
            "                for batch in batch_iter(event_and_contexts, 200):",
            "                    result = await self._send_events(",
            "                        instance_name=instance,",
            "                        store=self._store,",
            "                        room_id=room_id,",
            "                        event_and_contexts=batch,",
            "                        backfilled=backfilled,",
            "                    )",
            "            except SynapseError as e:",
            "                if e.code == HTTPStatus.CONFLICT:",
            "                    raise PartialStateConflictError()",
            "                raise",
            "            return result[\"max_stream_id\"]",
            "        else:",
            "            assert self._storage_controllers.persistence",
            "",
            "            # Note that this returns the events that were persisted, which may not be",
            "            # the same as were passed in if some were deduplicated due to transaction IDs.",
            "            (",
            "                events,",
            "                max_stream_token,",
            "            ) = await self._storage_controllers.persistence.persist_events(",
            "                event_and_contexts, backfilled=backfilled",
            "            )",
            "",
            "            # After persistence we always need to notify replication there may",
            "            # be new data.",
            "            self._notifier.notify_replication()",
            "",
            "            if self._ephemeral_messages_enabled:",
            "                for event in events:",
            "                    # If there's an expiry timestamp on the event, schedule its expiry.",
            "                    self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            if not backfilled:  # Never notify for backfilled events",
            "                with start_active_span(\"notify_persisted_events\"):",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids\",",
            "                        str([ev.event_id for ev in events]),",
            "                    )",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids.length\",",
            "                        str(len(events)),",
            "                    )",
            "                    for event in events:",
            "                        await self._notify_persisted_event(event, max_stream_token)",
            "",
            "            return max_stream_token.stream",
            "",
            "    async def _notify_persisted_event(",
            "        self, event: EventBase, max_stream_token: RoomStreamToken",
            "    ) -> None:",
            "        \"\"\"Checks to see if notifier/pushers should be notified about the",
            "        event or not.",
            "",
            "        Args:",
            "            event:",
            "            max_stream_token: The max_stream_id returned by persist_events",
            "        \"\"\"",
            "",
            "        extra_users = []",
            "        if event.type == EventTypes.Member:",
            "            target_user_id = event.state_key",
            "",
            "            # We notify for memberships if its an invite for one of our",
            "            # users",
            "            if event.internal_metadata.is_outlier():",
            "                if event.membership != Membership.INVITE:",
            "                    if not self._is_mine_id(target_user_id):",
            "                        return",
            "",
            "            target_user = UserID.from_string(target_user_id)",
            "            extra_users.append(target_user)",
            "        elif event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        # the event has been persisted so it should have a stream ordering.",
            "        assert event.internal_metadata.stream_ordering",
            "",
            "        event_pos = PersistedEventPosition(",
            "            self._instance_name, event.internal_metadata.stream_ordering",
            "        )",
            "        await self._notifier.on_new_room_events(",
            "            [(event, event_pos)], max_stream_token, extra_users=extra_users",
            "        )",
            "",
            "        if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "            # TODO retrieve the previous state, and exclude join -> join transitions",
            "            self._notifier.notify_user_joined_room(event.event_id, event.room_id)",
            "",
            "    def _sanity_check_event(self, ev: EventBase) -> None:",
            "        \"\"\"",
            "        Do some early sanity checks of a received event",
            "",
            "        In particular, checks it doesn't have an excessive number of",
            "        prev_events or auth_events, which could cause a huge state resolution",
            "        or cascade of event fetches.",
            "",
            "        Args:",
            "            ev: event to be checked",
            "",
            "        Raises:",
            "            SynapseError if the event does not pass muster",
            "        \"\"\"",
            "        if len(ev.prev_event_ids()) > 20:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i prev_events\",",
            "                ev.event_id,",
            "                len(ev.prev_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")",
            "",
            "        if len(ev.auth_event_ids()) > 10:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i auth_events\",",
            "                ev.event_id,",
            "                len(ev.auth_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")"
        ],
        "afterPatchFile": [
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import collections",
            "import itertools",
            "import logging",
            "from http import HTTPStatus",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Collection,",
            "    Container,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Optional,",
            "    Sequence,",
            "    Set,",
            "    Tuple,",
            ")",
            "",
            "from prometheus_client import Counter, Histogram",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    Membership,",
            "    RejectedReason,",
            "    RoomEncryptionAlgorithms,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    EventSizeError,",
            "    FederationError,",
            "    FederationPullAttemptBackoffError,",
            "    HttpResponseException,",
            "    PartialStateConflictError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion, RoomVersions",
            "from synapse.event_auth import (",
            "    auth_types_for_event,",
            "    check_state_dependent_auth_rules,",
            "    check_state_independent_auth_rules,",
            "    validate_event_for_room_version,",
            ")",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext, UnpersistedEventContextBase",
            "from synapse.federation.federation_client import InvalidResponseError, PulledPduInfo",
            "from synapse.logging.context import nested_logging_context",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    set_tag,",
            "    start_active_span,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.devices import (",
            "    ReplicationMultiUserDevicesResyncRestServlet,",
            ")",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEventsRestServlet,",
            ")",
            "from synapse.state import StateResolutionStore",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    PersistedEventPosition,",
            "    RoomStreamToken,",
            "    StateMap,",
            "    StrCollection,",
            "    UserID,",
            "    get_domain_from_id,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.iterutils import batch_iter, partition",
            "from synapse.util.retryutils import NotRetryingDestination",
            "from synapse.util.stringutils import shortstr",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "soft_failed_event_counter = Counter(",
            "    \"synapse_federation_soft_failed_events_total\",",
            "    \"Events received over federation that we marked as soft_failed\",",
            ")",
            "",
            "# Added to debug performance and track progress on optimizations",
            "backfill_processing_after_timer = Histogram(",
            "    \"synapse_federation_backfill_processing_after_time_seconds\",",
            "    \"sec\",",
            "    [],",
            "    buckets=(",
            "        0.1,",
            "        0.25,",
            "        0.5,",
            "        1.0,",
            "        2.5,",
            "        5.0,",
            "        7.5,",
            "        10.0,",
            "        15.0,",
            "        20.0,",
            "        25.0,",
            "        30.0,",
            "        40.0,",
            "        50.0,",
            "        60.0,",
            "        80.0,",
            "        100.0,",
            "        120.0,",
            "        150.0,",
            "        180.0,",
            "        \"+Inf\",",
            "    ),",
            ")",
            "",
            "",
            "class FederationEventHandler:",
            "    \"\"\"Handles events that originated from federation.",
            "",
            "    Responsible for handing incoming events and passing them on to the rest",
            "    of the homeserver (including auth and state conflict resolutions)",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self._clock = hs.get_clock()",
            "        self._store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "",
            "        self._state_handler = hs.get_state_handler()",
            "        self._event_creation_handler = hs.get_event_creation_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._message_handler = hs.get_message_handler()",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "        self._state_resolution_handler = hs.get_state_resolution_handler()",
            "        # avoid a circular dependency by deferring execution here",
            "        self._get_room_member_handler = hs.get_room_member_handler",
            "",
            "        self._federation_client = hs.get_federation_client()",
            "        self._third_party_event_rules = (",
            "            hs.get_module_api_callbacks().third_party_event_rules",
            "        )",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._is_mine_server_name = hs.is_mine_server_name",
            "        self._server_name = hs.hostname",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        self._config = hs.config",
            "        self._ephemeral_messages_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._send_events = ReplicationFederationSendEventsRestServlet.make_client(hs)",
            "        if hs.config.worker.worker_app:",
            "            self._multi_user_device_resync = (",
            "                ReplicationMultiUserDevicesResyncRestServlet.make_client(hs)",
            "            )",
            "        else:",
            "            self._device_list_updater = hs.get_device_handler().device_list_updater",
            "",
            "        # When joining a room we need to queue any events for that room up.",
            "        # For each room, a list of (pdu, origin) tuples.",
            "        # TODO: replace this with something more elegant, probably based around the",
            "        # federation event staging area.",
            "        self.room_queues: Dict[str, List[Tuple[EventBase, str]]] = {}",
            "",
            "        self._room_pdu_linearizer = Linearizer(\"fed_room_pdu\")",
            "",
            "    async def on_receive_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received via a federation /send/ transaction",
            "",
            "        Args:",
            "            origin: server which initiated the /send/ transaction. Will",
            "                be used to fetch missing events or state.",
            "            pdu: received PDU",
            "        \"\"\"",
            "",
            "        # We should never see any outliers here.",
            "        assert not pdu.internal_metadata.outlier",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        # We reprocess pdus when we have seen them only as outliers",
            "        existing = await self._store.get_event(",
            "            event_id, allow_none=True, allow_rejected=True",
            "        )",
            "",
            "        # FIXME: Currently we fetch an event again when we already have it",
            "        # if it has been marked as an outlier.",
            "        if existing:",
            "            if not existing.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received event %s which we have already seen\", event_id",
            "                )",
            "                return",
            "            if pdu.internal_metadata.is_outlier():",
            "                logger.info(",
            "                    \"Ignoring received outlier %s which we already have as an outlier\",",
            "                    event_id,",
            "                )",
            "                return",
            "            logger.info(\"De-outliering event %s\", event_id)",
            "",
            "        # do some initial sanity-checking of the event. In particular, make",
            "        # sure it doesn't have hundreds of prev_events or auth_events, which",
            "        # could cause a huge state resolution or cascade of event fetches.",
            "        try:",
            "            self._sanity_check_event(pdu)",
            "        except SynapseError as err:",
            "            logger.warning(\"Received event failed sanity checks\")",
            "            raise FederationError(\"ERROR\", err.code, err.msg, affected=pdu.event_id)",
            "",
            "        # If we are currently in the process of joining this room, then we",
            "        # queue up events for later processing.",
            "        if room_id in self.room_queues:",
            "            logger.info(",
            "                \"Queuing PDU from %s for now: join in progress\",",
            "                origin,",
            "            )",
            "            self.room_queues[room_id].append((pdu, origin))",
            "            return",
            "",
            "        # If we're not in the room just ditch the event entirely. This is",
            "        # probably an old server that has come back and thinks we're still in",
            "        # the room (or we've been rejoined to the room by a state reset).",
            "        #",
            "        # Note that if we were never in the room then we would have already",
            "        # dropped the event, since we wouldn't know the room version.",
            "        is_in_room = await self._event_auth_handler.is_host_in_room(",
            "            room_id, self._server_name",
            "        )",
            "        if not is_in_room:",
            "            logger.info(",
            "                \"Ignoring PDU from %s as we're not in the room\",",
            "                origin,",
            "            )",
            "            return None",
            "",
            "        # Try to fetch any missing prev events to fill in gaps in the graph",
            "        prevs = set(pdu.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        if missing_prevs:",
            "            # We only backfill backwards to the min depth.",
            "            min_depth = await self._store.get_min_depth(pdu.room_id)",
            "            logger.debug(\"min_depth: %d\", min_depth)",
            "",
            "            if min_depth is not None and pdu.depth > min_depth:",
            "                # If we're missing stuff, ensure we only fetch stuff one",
            "                # at a time.",
            "                logger.info(",
            "                    \"Acquiring room lock to fetch %d missing prev_events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                async with self._room_pdu_linearizer.queue(pdu.room_id):",
            "                    logger.info(",
            "                        \"Acquired room lock to fetch %d missing prev_events\",",
            "                        len(missing_prevs),",
            "                    )",
            "",
            "                    try:",
            "                        await self._get_missing_events_for_pdu(",
            "                            origin, pdu, prevs, min_depth",
            "                        )",
            "                    except Exception as e:",
            "                        raise Exception(",
            "                            \"Error fetching missing prev_events for %s: %s\"",
            "                            % (event_id, e)",
            "                        ) from e",
            "",
            "                # Update the set of things we've seen after trying to",
            "                # fetch the missing stuff",
            "                seen = await self._store.have_events_in_timeline(prevs)",
            "                missing_prevs = prevs - seen",
            "",
            "                if not missing_prevs:",
            "                    logger.info(\"Found all missing prev_events\")",
            "",
            "            if missing_prevs:",
            "                # since this event was pushed to us, it is possible for it to",
            "                # become the only forward-extremity in the room, and we would then",
            "                # trust its state to be the state for the whole room. This is very",
            "                # bad. Further, if the event was pushed to us, there is no excuse",
            "                # for us not to have all the prev_events. (XXX: apart from",
            "                # min_depth?)",
            "                #",
            "                # We therefore reject any such events.",
            "                logger.warning(",
            "                    \"Rejecting: failed to fetch %d prev events: %s\",",
            "                    len(missing_prevs),",
            "                    shortstr(missing_prevs),",
            "                )",
            "                raise FederationError(",
            "                    \"ERROR\",",
            "                    403,",
            "                    (",
            "                        \"Your server isn't divulging details about prev_events \"",
            "                        \"referenced in this event.\"",
            "                    ),",
            "                    affected=pdu.event_id,",
            "                )",
            "",
            "        try:",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were processing the PDU.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated while processing the PDU, trying again.\",",
            "                room_id,",
            "            )",
            "            context = await self._state_handler.compute_event_context(pdu)",
            "            await self._process_received_pdu(origin, pdu, context)",
            "",
            "    async def on_send_membership_event(",
            "        self, origin: str, event: EventBase",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"",
            "        We have received a join/leave/knock event for a room via send_join/leave/knock.",
            "",
            "        Verify that event and send it into the room on the remote homeserver's behalf.",
            "",
            "        This is quite similar to on_receive_pdu, with the following principal",
            "        differences:",
            "          * only membership events are permitted (and only events with",
            "            sender==state_key -- ie, no kicks or bans)",
            "          * *We* send out the event on behalf of the remote server.",
            "          * We enforce the membership restrictions of restricted rooms.",
            "          * Rejected events result in an exception rather than being stored.",
            "",
            "        There are also other differences, however it is not clear if these are by",
            "        design or omission. In particular, we do not attempt to backfill any missing",
            "        prev_events.",
            "",
            "        Args:",
            "            origin: The homeserver of the remote (joining/invited/knocking) user.",
            "            event: The member event that has been signed by the remote homeserver.",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            RuntimeError if any prev_events are missing",
            "            SynapseError if the event is not accepted into the room",
            "            PartialStateConflictError if the room was un-partial stated in between",
            "                computing the state at the event and persisting it. The caller should",
            "                retry exactly once in this case.",
            "        \"\"\"",
            "        logger.debug(",
            "            \"on_send_membership_event: Got event: %s, signatures: %s\",",
            "            event.event_id,",
            "            event.signatures,",
            "        )",
            "",
            "        if get_domain_from_id(event.sender) != origin:",
            "            logger.info(",
            "                \"Got send_membership request for user %r from different origin %s\",",
            "                event.sender,",
            "                origin,",
            "            )",
            "            raise SynapseError(403, \"User not from origin\", Codes.FORBIDDEN)",
            "",
            "        if event.sender != event.state_key:",
            "            raise SynapseError(400, \"state_key and sender must match\", Codes.BAD_JSON)",
            "",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # Send this event on behalf of the other server.",
            "        #",
            "        # The remote server isn't a full participant in the room at this point, so",
            "        # may not have an up-to-date list of the other homeservers participating in",
            "        # the room, so we send it on their behalf.",
            "        event.internal_metadata.send_on_behalf_of = origin",
            "",
            "        context = await self._state_handler.compute_event_context(event)",
            "        await self._check_event_auth(origin, event, context)",
            "        if context.rejected:",
            "            raise SynapseError(",
            "                403, f\"{event.membership} event was rejected\", Codes.FORBIDDEN",
            "            )",
            "",
            "        # for joins, we need to check the restrictions of restricted rooms",
            "        if event.membership == Membership.JOIN:",
            "            await self.check_join_restrictions(context, event)",
            "",
            "        # for knock events, we run the third-party event rules. It's not entirely clear",
            "        # why we don't do this for other sorts of membership events.",
            "        if event.membership == Membership.KNOCK:",
            "            event_allowed, _ = await self._third_party_event_rules.check_event_allowed(",
            "                event, context",
            "            )",
            "            if not event_allowed:",
            "                logger.info(\"Sending of knock %s forbidden by third-party rules\", event)",
            "                raise SynapseError(",
            "                    403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "                )",
            "",
            "        # all looks good, we can persist the event.",
            "",
            "        # First, precalculate the joined hosts so that the federation sender doesn't",
            "        # need to.",
            "        await self._event_creation_handler.cache_joined_hosts_for_events(",
            "            [(event, context)]",
            "        )",
            "",
            "        await self._check_for_soft_fail(event, context=context, origin=origin)",
            "        await self._run_push_actions_and_persist_event(event, context)",
            "        return event, context",
            "",
            "    async def check_join_restrictions(",
            "        self,",
            "        context: UnpersistedEventContextBase,",
            "        event: EventBase,",
            "    ) -> None:",
            "        \"\"\"Check that restrictions in restricted join rules are matched",
            "",
            "        Called when we receive a join event via send_join.",
            "",
            "        Raises an auth error if the restrictions are not matched.",
            "        \"\"\"",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        # Check if the user is already in the room or invited to the room.",
            "        user_id = event.state_key",
            "        prev_member_event_id = prev_state_ids.get((EventTypes.Member, user_id), None)",
            "        prev_membership = None",
            "        if prev_member_event_id:",
            "            prev_member_event = await self._store.get_event(prev_member_event_id)",
            "            prev_membership = prev_member_event.membership",
            "",
            "        # Check if the member should be allowed access via membership in a space.",
            "        await self._event_auth_handler.check_restricted_join_rules(",
            "            prev_state_ids,",
            "            event.room_version,",
            "            user_id,",
            "            prev_membership,",
            "        )",
            "",
            "    @trace",
            "    async def process_remote_join(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        auth_events: List[EventBase],",
            "        state: List[EventBase],",
            "        event: EventBase,",
            "        room_version: RoomVersion,",
            "        partial_state: bool,",
            "    ) -> int:",
            "        \"\"\"Persists the events returned by a send_join",
            "",
            "        Checks the auth chain is valid (and passes auth checks) for the",
            "        state and event. Then persists all of the events.",
            "        Notifies about the persisted events where appropriate.",
            "",
            "        Args:",
            "            origin: Where the events came from",
            "            room_id:",
            "            auth_events",
            "            state",
            "            event",
            "            room_version: The room version we expect this room to have, and",
            "                will raise if it doesn't match the version in the create event.",
            "            partial_state: True if the state omits non-critical membership events",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            SynapseError if the response is in some way invalid.",
            "            PartialStateConflictError if the homeserver is already in the room and it",
            "                has been un-partial stated.",
            "        \"\"\"",
            "        create_event = None",
            "        for e in state:",
            "            if (e.type, e.state_key) == (EventTypes.Create, \"\"):",
            "                create_event = e",
            "                break",
            "",
            "        if create_event is None:",
            "            # If the state doesn't have a create event then the room is",
            "            # invalid, and it would fail auth checks anyway.",
            "            raise SynapseError(400, \"No create event in state\")",
            "",
            "        room_version_id = create_event.content.get(",
            "            \"room_version\", RoomVersions.V1.identifier",
            "        )",
            "",
            "        if room_version.identifier != room_version_id:",
            "            raise SynapseError(400, \"Room version mismatch\")",
            "",
            "        # persist the auth chain and state events.",
            "        #",
            "        # any invalid events here will be marked as rejected, and we'll carry on.",
            "        #",
            "        # any events whose auth events are missing (ie, not in the send_join response,",
            "        # and not already in our db) will just be ignored. This is correct behaviour,",
            "        # because the reason that auth_events are missing might be due to us being",
            "        # unable to validate their signatures. The fact that we can't validate their",
            "        # signatures right now doesn't mean that we will *never* be able to, so it",
            "        # is premature to reject them.",
            "        #",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state)",
            "        )",
            "",
            "        # and now persist the join event itself.",
            "        logger.info(",
            "            \"Peristing join-via-remote %s (partial_state: %s)\", event, partial_state",
            "        )",
            "        with nested_logging_context(suffix=event.event_id):",
            "            if partial_state:",
            "                # When handling a second partial state join into a partial state room,",
            "                # the returned state will exclude the membership from the first join. To",
            "                # preserve prior memberships, we try to compute the partial state before",
            "                # the event ourselves if we know about any of the prev events.",
            "                #",
            "                # When we don't know about any of the prev events, it's fine to just use",
            "                # the returned state, since the new join will create a new forward",
            "                # extremity, and leave the forward extremity containing our prior",
            "                # memberships alone.",
            "                prev_event_ids = set(event.prev_event_ids())",
            "                seen_event_ids = await self._store.have_events_in_timeline(",
            "                    prev_event_ids",
            "                )",
            "                missing_event_ids = prev_event_ids - seen_event_ids",
            "",
            "                state_maps_to_resolve: List[StateMap[str]] = []",
            "",
            "                # Fetch the state after the prev events that we know about.",
            "                state_maps_to_resolve.extend(",
            "                    (",
            "                        await self._state_storage_controller.get_state_groups_ids(",
            "                            room_id, seen_event_ids, await_full_state=False",
            "                        )",
            "                    ).values()",
            "                )",
            "",
            "                # When there are prev events we do not have the state for, we state",
            "                # resolve with the state returned by the remote homeserver.",
            "                if missing_event_ids or len(state_maps_to_resolve) == 0:",
            "                    state_maps_to_resolve.append(",
            "                        {(e.type, e.state_key): e.event_id for e in state}",
            "                    )",
            "",
            "                state_ids_before_event = (",
            "                    await self._state_resolution_handler.resolve_events_with_store(",
            "                        event.room_id,",
            "                        room_version.identifier,",
            "                        state_maps_to_resolve,",
            "                        event_map=None,",
            "                        state_res_store=StateResolutionStore(self._store),",
            "                    )",
            "                )",
            "            else:",
            "                state_ids_before_event = {",
            "                    (e.type, e.state_key): e.event_id for e in state",
            "                }",
            "",
            "            context = await self._state_handler.compute_event_context(",
            "                event,",
            "                state_ids_before_event=state_ids_before_event,",
            "                partial_state=partial_state,",
            "            )",
            "",
            "            await self._check_event_auth(origin, event, context)",
            "            if context.rejected:",
            "                raise SynapseError(403, \"Join event was rejected\")",
            "",
            "            # the remote server is responsible for sending our join event to the rest",
            "            # of the federation. Indeed, attempting to do so will result in problems",
            "            # when we try to look up the state before the join (to get the server list)",
            "            # and discover that we do not have it.",
            "            event.internal_metadata.proactively_send = False",
            "",
            "            stream_id_after_persist = await self.persist_events_and_notify(",
            "                room_id, [(event, context)]",
            "            )",
            "",
            "            return stream_id_after_persist",
            "",
            "    async def update_state_for_partial_state_event(",
            "        self, destination: str, event: EventBase",
            "    ) -> None:",
            "        \"\"\"Recalculate the state at an event as part of a de-partial-stating process",
            "",
            "        Args:",
            "            destination: server to request full state from",
            "            event: partial-state event to be de-partial-stated",
            "",
            "        Raises:",
            "            FederationPullAttemptBackoffError if we are are deliberately not attempting",
            "                to pull the given event over federation because we've already done so",
            "                recently and are backing off.",
            "            FederationError if we fail to request state from the remote server.",
            "        \"\"\"",
            "        logger.info(\"Updating state for %s\", event.event_id)",
            "        with nested_logging_context(suffix=event.event_id):",
            "            # if we have all the event's prev_events, then we can work out the",
            "            # state based on their states. Otherwise, we request it from the destination",
            "            # server.",
            "            #",
            "            # This is the same operation as we do when we receive a regular event",
            "            # over federation.",
            "            context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                destination, event",
            "            )",
            "            if context.partial_state:",
            "                # this can happen if some or all of the event's prev_events still have",
            "                # partial state. We were careful to only pick events from the db without",
            "                # partial-state prev events, so that implies that a prev event has",
            "                # been persisted (with partial state) since we did the query.",
            "                #",
            "                # So, let's just ignore `event` for now; when we re-run the db query",
            "                # we should instead get its partial-state prev event, which we will",
            "                # de-partial-state, and then come back to event.",
            "                logger.warning(",
            "                    \"%s still has prev_events with partial state: can't de-partial-state it yet\",",
            "                    event.event_id,",
            "                )",
            "                return",
            "",
            "            # since the state at this event has changed, we should now re-evaluate",
            "            # whether it should have been rejected. We must already have all of the",
            "            # auth events (from last time we went round this path), so there is no",
            "            # need to pass the origin.",
            "            await self._check_event_auth(None, event, context)",
            "",
            "            await self._store.update_state_for_partial_state_event(event, context)",
            "            self._state_storage_controller.notify_event_un_partial_stated(",
            "                event.event_id",
            "            )",
            "            # Notify that there's a new row in the un_partial_stated_events stream.",
            "            self._notifier.notify_replication()",
            "",
            "    @trace",
            "    async def backfill(",
            "        self, dest: str, room_id: str, limit: int, extremities: StrCollection",
            "    ) -> None:",
            "        \"\"\"Trigger a backfill request to `dest` for the given `room_id`",
            "",
            "        This will attempt to get more events from the remote. If the other side",
            "        has no new events to offer, this will return an empty list.",
            "",
            "        As the events are received, we check their signatures, and also do some",
            "        sanity-checking on them. If any of the backfilled events are invalid,",
            "        this method throws a SynapseError.",
            "",
            "        We might also raise an InvalidResponseError if the response from the remote",
            "        server is just bogus.",
            "",
            "        TODO: make this more useful to distinguish failures of the remote",
            "        server from invalid events (there is probably no point in trying to",
            "        re-fetch invalid events from every other HS in the room.)",
            "        \"\"\"",
            "        if self._is_mine_server_name(dest):",
            "            raise SynapseError(400, \"Can't backfill from self.\")",
            "",
            "        events = await self._federation_client.backfill(",
            "            dest, room_id, limit=limit, extremities=extremities",
            "        )",
            "",
            "        if not events:",
            "            return",
            "",
            "        with backfill_processing_after_timer.time():",
            "            # if there are any events in the wrong room, the remote server is buggy and",
            "            # should not be trusted.",
            "            for ev in events:",
            "                if ev.room_id != room_id:",
            "                    raise InvalidResponseError(",
            "                        f\"Remote server {dest} returned event {ev.event_id} which is in \"",
            "                        f\"room {ev.room_id}, when we were backfilling in {room_id}\"",
            "                    )",
            "",
            "            await self._process_pulled_events(",
            "                dest,",
            "                events,",
            "                backfilled=True,",
            "            )",
            "",
            "    @trace",
            "    async def _get_missing_events_for_pdu(",
            "        self, origin: str, pdu: EventBase, prevs: Set[str], min_depth: int",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            origin: Origin of the pdu. Will be called to get the missing events",
            "            pdu: received pdu",
            "            prevs: List of event ids which we are missing",
            "            min_depth: Minimum depth of events to return.",
            "        \"\"\"",
            "",
            "        room_id = pdu.room_id",
            "        event_id = pdu.event_id",
            "",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "",
            "        if not prevs - seen:",
            "            return",
            "",
            "        latest_frozen = await self._store.get_latest_event_ids_in_room(room_id)",
            "",
            "        # We add the prev events that we have seen to the latest",
            "        # list to ensure the remote server doesn't give them to us",
            "        latest = seen | latest_frozen",
            "",
            "        logger.info(",
            "            \"Requesting missing events between %s and %s\",",
            "            shortstr(latest),",
            "            event_id,",
            "        )",
            "",
            "        # XXX: we set timeout to 10s to help workaround",
            "        # https://github.com/matrix-org/synapse/issues/1733.",
            "        # The reason is to avoid holding the linearizer lock",
            "        # whilst processing inbound /send transactions, causing",
            "        # FDs to stack up and block other inbound transactions",
            "        # which empirically can currently take up to 30 minutes.",
            "        #",
            "        # N.B. this explicitly disables retry attempts.",
            "        #",
            "        # N.B. this also increases our chances of falling back to",
            "        # fetching fresh state for the room if the missing event",
            "        # can't be found, which slightly reduces our security.",
            "        # it may also increase our DAG extremity count for the room,",
            "        # causing additional state resolution?  See #1760.",
            "        # However, fetching state doesn't hold the linearizer lock",
            "        # apparently.",
            "        #",
            "        # see https://github.com/matrix-org/synapse/pull/1744",
            "        #",
            "        # ----",
            "        #",
            "        # Update richvdh 2018/09/18: There are a number of problems with timing this",
            "        # request out aggressively on the client side:",
            "        #",
            "        # - it plays badly with the server-side rate-limiter, which starts tarpitting you",
            "        #   if you send too many requests at once, so you end up with the server carefully",
            "        #   working through the backlog of your requests, which you have already timed",
            "        #   out.",
            "        #",
            "        # - for this request in particular, we now (as of",
            "        #   https://github.com/matrix-org/synapse/pull/3456) reject any PDUs where the",
            "        #   server can't produce a plausible-looking set of prev_events - so we becone",
            "        #   much more likely to reject the event.",
            "        #",
            "        # - contrary to what it says above, we do *not* fall back to fetching fresh state",
            "        #   for the room if get_missing_events times out. Rather, we give up processing",
            "        #   the PDU whose prevs we are missing, which then makes it much more likely that",
            "        #   we'll end up back here for the *next* PDU in the list, which exacerbates the",
            "        #   problem.",
            "        #",
            "        # - the aggressive 10s timeout was introduced to deal with incoming federation",
            "        #   requests taking 8 hours to process. It's not entirely clear why that was going",
            "        #   on; certainly there were other issues causing traffic storms which are now",
            "        #   resolved, and I think in any case we may be more sensible about our locking",
            "        #   now. We're *certainly* more sensible about our logging.",
            "        #",
            "        # All that said: Let's try increasing the timeout to 60s and see what happens.",
            "",
            "        try:",
            "            missing_events = await self._federation_client.get_missing_events(",
            "                origin,",
            "                room_id,",
            "                earliest_events_ids=list(latest),",
            "                latest_events=[pdu],",
            "                limit=10,",
            "                min_depth=min_depth,",
            "                timeout=60000,",
            "            )",
            "        except (RequestSendFailed, HttpResponseException, NotRetryingDestination) as e:",
            "            # We failed to get the missing events, but since we need to handle",
            "            # the case of `get_missing_events` not returning the necessary",
            "            # events anyway, it is safe to simply log the error and continue.",
            "            logger.warning(\"Failed to get prev_events: %s\", e)",
            "            return",
            "",
            "        logger.info(\"Got %d prev_events\", len(missing_events))",
            "        await self._process_pulled_events(origin, missing_events, backfilled=False)",
            "",
            "    @trace",
            "    async def _process_pulled_events(",
            "        self, origin: str, events: Collection[EventBase], backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a batch of events we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the events, persists the received events,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the events have already had their signatures and hashes checked.",
            "",
            "        Params:",
            "            origin: The server we received these events from",
            "            events: The received events.",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str([event.event_id for event in events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(events)),",
            "        )",
            "        set_tag(SynapseTags.FUNC_ARG_PREFIX + \"backfilled\", str(backfilled))",
            "        logger.debug(",
            "            \"processing pulled backfilled=%s events=%s\",",
            "            backfilled,",
            "            [",
            "                \"event_id=%s,depth=%d,body=%s,prevs=%s\\n\"",
            "                % (",
            "                    event.event_id,",
            "                    event.depth,",
            "                    event.content.get(\"body\", event.type),",
            "                    event.prev_event_ids(),",
            "                )",
            "                for event in events",
            "            ],",
            "        )",
            "",
            "        # Check if we already any of these have these events.",
            "        # Note: we currently make a lookup in the database directly here rather than",
            "        # checking the event cache, due to:",
            "        # https://github.com/matrix-org/synapse/issues/13476",
            "        existing_events_map = await self._store._get_events_from_db(",
            "            [event.event_id for event in events]",
            "        )",
            "",
            "        new_events: List[EventBase] = []",
            "        for event in events:",
            "            event_id = event.event_id",
            "",
            "            # If we've already seen this event ID...",
            "            if event_id in existing_events_map:",
            "                existing_event = existing_events_map[event_id]",
            "",
            "                # ...and the event itself was not previously stored as an outlier...",
            "                if not existing_event.event.internal_metadata.is_outlier():",
            "                    # ...then there's no need to persist it. We have it already.",
            "                    logger.info(",
            "                        \"_process_pulled_event: Ignoring received event %s which we \"",
            "                        \"have already seen\",",
            "                        event.event_id,",
            "                    )",
            "                    continue",
            "",
            "                # While we have seen this event before, it was stored as an outlier.",
            "                # We'll now persist it as a non-outlier.",
            "                logger.info(\"De-outliering event %s\", event_id)",
            "",
            "            # Continue on with the events that are new to us.",
            "            new_events.append(event)",
            "",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"new_events.length\",",
            "            str(len(new_events)),",
            "        )",
            "",
            "        @trace",
            "        async def _process_new_pulled_events(new_events: Collection[EventBase]) -> None:",
            "            # We want to sort these by depth so we process them and tell clients about",
            "            # them in order. It's also more efficient to backfill this way (`depth`",
            "            # ascending) because one backfill event is likely to be the `prev_event` of",
            "            # the next event we're going to process.",
            "            sorted_events = sorted(new_events, key=lambda x: x.depth)",
            "            for ev in sorted_events:",
            "                with nested_logging_context(ev.event_id):",
            "                    await self._process_pulled_event(origin, ev, backfilled=backfilled)",
            "",
            "        # Check if we've already tried to process these events at some point in the",
            "        # past. We aren't concerned with the expontntial backoff here, just whether it",
            "        # has failed to be processed before.",
            "        event_ids_with_failed_pull_attempts = (",
            "            await self._store.get_event_ids_with_failed_pull_attempts(",
            "                [event.event_id for event in new_events]",
            "            )",
            "        )",
            "",
            "        events_with_failed_pull_attempts, fresh_events = partition(",
            "            new_events, lambda e: e.event_id in event_ids_with_failed_pull_attempts",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"events_with_failed_pull_attempts\",",
            "            str(event_ids_with_failed_pull_attempts),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"events_with_failed_pull_attempts.length\",",
            "            str(len(events_with_failed_pull_attempts)),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"fresh_events\",",
            "            str([event.event_id for event in fresh_events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"fresh_events.length\",",
            "            str(len(fresh_events)),",
            "        )",
            "",
            "        # Process previously failed backfill events in the background to not waste",
            "        # time on something that is likely to fail again.",
            "        if len(events_with_failed_pull_attempts) > 0:",
            "            run_as_background_process(",
            "                \"_process_new_pulled_events_with_failed_pull_attempts\",",
            "                _process_new_pulled_events,",
            "                events_with_failed_pull_attempts,",
            "            )",
            "",
            "        # We can optimistically try to process and wait for the event to be fully",
            "        # persisted if we've never tried before.",
            "        if len(fresh_events) > 0:",
            "            await _process_new_pulled_events(fresh_events)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _process_pulled_event(",
            "        self, origin: str, event: EventBase, backfilled: bool",
            "    ) -> None:",
            "        \"\"\"Process a single event that we have pulled from a remote server",
            "",
            "        Pulls in any events required to auth the event, persists the received event,",
            "        and notifies clients, if appropriate.",
            "",
            "        Assumes the event has already had its signatures and hashes checked.",
            "",
            "        This is somewhat equivalent to on_receive_pdu, but applies somewhat different",
            "        logic in the case that we are missing prev_events (in particular, it just",
            "        requests the state at that point, rather than triggering a get_missing_events) -",
            "        so is appropriate when we have pulled the event from a remote server, rather",
            "        than having it pushed to us.",
            "",
            "        Params:",
            "            origin: The server we received this event from",
            "            events: The received event",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "        \"\"\"",
            "        logger.info(\"Processing pulled event %s\", event)",
            "",
            "        # This function should not be used to persist outliers (use something",
            "        # else) because this does a bunch of operations that aren't necessary",
            "        # (extra work; in particular, it makes sure we have all the prev_events",
            "        # and resolves the state across those prev events). If you happen to run",
            "        # into a situation where the event you're trying to process/backfill is",
            "        # marked as an `outlier`, then you should update that spot to return an",
            "        # `EventBase` copy that doesn't have `outlier` flag set.",
            "        #",
            "        # `EventBase` is used to represent both an event we have not yet",
            "        # persisted, and one that we have persisted and now keep in the cache.",
            "        # In an ideal world this method would only be called with the first type",
            "        # of event, but it turns out that's not actually the case and for",
            "        # example, you could get an event from cache that is marked as an",
            "        # `outlier` (fix up that spot though).",
            "        assert not event.internal_metadata.is_outlier(), (",
            "            \"Outlier event passed to _process_pulled_event. \"",
            "            \"To persist an event as a non-outlier, make sure to pass in a copy without `event.internal_metadata.outlier = true`.\"",
            "        )",
            "",
            "        event_id = event.event_id",
            "",
            "        try:",
            "            self._sanity_check_event(event)",
            "        except SynapseError as err:",
            "            logger.warning(\"Event %s failed sanity check: %s\", event_id, err)",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(err)",
            "            )",
            "            return",
            "",
            "        try:",
            "            try:",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "            except PartialStateConflictError:",
            "                # The room was un-partial stated while we were processing the event.",
            "                # Try once more, with full state this time.",
            "                context = await self._compute_event_context_with_maybe_missing_prevs(",
            "                    origin, event",
            "                )",
            "",
            "                # We ought to have full state now, barring some unlikely race where we left and",
            "                # rejoned the room in the background.",
            "                if context.partial_state:",
            "                    raise AssertionError(",
            "                        f\"Event {event.event_id} still has a partial resolved state \"",
            "                        f\"after room {event.room_id} was un-partial stated\"",
            "                    )",
            "",
            "                await self._process_received_pdu(",
            "                    origin,",
            "                    event,",
            "                    context,",
            "                    backfilled=backfilled,",
            "                )",
            "        except FederationPullAttemptBackoffError as exc:",
            "            # Log a warning about why we failed to process the event (the error message",
            "            # for `FederationPullAttemptBackoffError` is pretty good)",
            "            logger.warning(\"_process_pulled_event: %s\", exc)",
            "            # We do not record a failed pull attempt when we backoff fetching a missing",
            "            # `prev_event` because not being able to fetch the `prev_events` just means",
            "            # we won't be able to de-outlier the pulled event. But we can still use an",
            "            # `outlier` in the state/auth chain for another event. So we shouldn't stop",
            "            # a downstream event from trying to pull it.",
            "            #",
            "            # This avoids a cascade of backoff for all events in the DAG downstream from",
            "            # one event backoff upstream.",
            "        except FederationError as e:",
            "            await self._store.record_event_failed_pull_attempt(",
            "                event.room_id, event_id, str(e)",
            "            )",
            "",
            "            if e.code == 403:",
            "                logger.warning(\"Pulled event %s failed history check.\", event_id)",
            "            else:",
            "                raise",
            "",
            "    @trace",
            "    async def _compute_event_context_with_maybe_missing_prevs(",
            "        self, dest: str, event: EventBase",
            "    ) -> EventContext:",
            "        \"\"\"Build an EventContext structure for a non-outlier event whose prev_events may",
            "        be missing.",
            "",
            "        This is used when we have pulled a batch of events from a remote server, and may",
            "        not have all the prev_events.",
            "",
            "        To build an EventContext, we need to calculate the state before the event. If we",
            "        already have all the prev_events for `event`, we can simply use the state after",
            "        the prev_events to calculate the state before `event`.",
            "",
            "        Otherwise, the missing prevs become new backwards extremities, and we fall back",
            "        to asking the remote server for the state after each missing `prev_event`,",
            "        and resolving across them.",
            "",
            "        That's ok provided we then resolve the state against other bits of the DAG",
            "        before using it - in other words, that the received event `event` is not going",
            "        to become the only forwards_extremity in the room (which will ensure that you",
            "        can't just take over a room by sending an event, withholding its prev_events,",
            "        and declaring yourself to be an admin in the subsequent state request).",
            "",
            "        In other words: we should only call this method if `event` has been *pulled*",
            "        as part of a batch of missing prev events, or similar.",
            "",
            "        Params:",
            "            dest: the remote server to ask for state at the missing prevs. Typically,",
            "                this will be the server we got `event` from.",
            "            event: an event to check for missing prevs.",
            "",
            "        Returns:",
            "            The event context.",
            "",
            "        Raises:",
            "            FederationPullAttemptBackoffError if we are are deliberately not attempting",
            "                to pull one of the given event's `prev_event`s over federation because",
            "                we've already done so recently and are backing off.",
            "            FederationError if we fail to get the state from the remote server after any",
            "                missing `prev_event`s.",
            "        \"\"\"",
            "        room_id = event.room_id",
            "        event_id = event.event_id",
            "",
            "        prevs = set(event.prev_event_ids())",
            "        seen = await self._store.have_events_in_timeline(prevs)",
            "        missing_prevs = prevs - seen",
            "",
            "        # If we've already recently attempted to pull this missing event, don't",
            "        # try it again so soon. Since we have to fetch all of the prev_events, we can",
            "        # bail early here if we find any to ignore.",
            "        prevs_with_pull_backoff = (",
            "            await self._store.get_event_ids_to_not_pull_from_backoff(",
            "                room_id, missing_prevs",
            "            )",
            "        )",
            "        if len(prevs_with_pull_backoff) > 0:",
            "            raise FederationPullAttemptBackoffError(",
            "                event_ids=prevs_with_pull_backoff.keys(),",
            "                message=(",
            "                    f\"While computing context for event={event_id}, not attempting to \"",
            "                    f\"pull missing prev_events={list(prevs_with_pull_backoff.keys())} \"",
            "                    \"because we already tried to pull recently (backing off).\"",
            "                ),",
            "                retry_after_ms=(",
            "                    max(prevs_with_pull_backoff.values()) - self._clock.time_msec()",
            "                ),",
            "            )",
            "",
            "        if not missing_prevs:",
            "            return await self._state_handler.compute_event_context(event)",
            "",
            "        logger.info(",
            "            \"Event %s is missing prev_events %s: calculating state for a \"",
            "            \"backwards extremity\",",
            "            event_id,",
            "            shortstr(missing_prevs),",
            "        )",
            "        # Calculate the state after each of the previous events, and",
            "        # resolve them to find the correct state at the current event.",
            "",
            "        try:",
            "            # Determine whether we may be about to retrieve partial state",
            "            # Events may be un-partial stated right after we compute the partial state",
            "            # flag, but that's okay, as long as the flag errs on the conservative side.",
            "            partial_state_flags = await self._store.get_partial_state_events(seen)",
            "            partial_state = any(partial_state_flags.values())",
            "",
            "            # Get the state of the events we know about",
            "            ours = await self._state_storage_controller.get_state_groups_ids(",
            "                room_id, seen, await_full_state=False",
            "            )",
            "",
            "            # state_maps is a list of mappings from (type, state_key) to event_id",
            "            state_maps: List[StateMap[str]] = list(ours.values())",
            "",
            "            # we don't need this any more, let's delete it.",
            "            del ours",
            "",
            "            # Ask the remote server for the states we don't",
            "            # know about",
            "            for p in missing_prevs:",
            "                logger.info(\"Requesting state after missing prev_event %s\", p)",
            "",
            "                with nested_logging_context(p):",
            "                    # note that if any of the missing prevs share missing state or",
            "                    # auth events, the requests to fetch those events are deduped",
            "                    # by the get_pdu_cache in federation_client.",
            "                    remote_state_map = (",
            "                        await self._get_state_ids_after_missing_prev_event(",
            "                            dest, room_id, p",
            "                        )",
            "                    )",
            "",
            "                    state_maps.append(remote_state_map)",
            "",
            "            room_version = await self._store.get_room_version_id(room_id)",
            "            state_map = await self._state_resolution_handler.resolve_events_with_store(",
            "                room_id,",
            "                room_version,",
            "                state_maps,",
            "                event_map={event_id: event},",
            "                state_res_store=StateResolutionStore(self._store),",
            "            )",
            "",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Error attempting to resolve state at missing prev_events: %s\", e",
            "            )",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                403,",
            "                \"We can't get valid state history.\",",
            "                affected=event_id,",
            "            )",
            "        return await self._state_handler.compute_event_context(",
            "            event, state_ids_before_event=state_map, partial_state=partial_state",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_ids_after_missing_prev_event(",
            "        self,",
            "        destination: str,",
            "        room_id: str,",
            "        event_id: str,",
            "    ) -> StateMap[str]:",
            "        \"\"\"Requests all of the room state at a given event from a remote homeserver.",
            "",
            "        Args:",
            "            destination: The remote homeserver to query for the state.",
            "            room_id: The id of the room we're interested in.",
            "            event_id: The id of the event we want the state at.",
            "",
            "        Returns:",
            "            The event ids of the state *after* the given event.",
            "",
            "        Raises:",
            "            InvalidResponseError: if the remote homeserver's response contains fields",
            "                of the wrong type.",
            "        \"\"\"",
            "",
            "        # It would be better if we could query the difference from our known",
            "        # state to the given `event_id` so the sending server doesn't have to",
            "        # send as much and we don't have to process as many events. For example",
            "        # in a room like #matrix:matrix.org, we get 200k events (77k state_events, 122k",
            "        # auth_events) from this call.",
            "        #",
            "        # Tracked by https://github.com/matrix-org/synapse/issues/13618",
            "        (",
            "            state_event_ids,",
            "            auth_event_ids,",
            "        ) = await self._federation_client.get_room_state_ids(",
            "            destination, room_id, event_id=event_id",
            "        )",
            "",
            "        logger.debug(",
            "            \"state_ids returned %i state events, %i auth events\",",
            "            len(state_event_ids),",
            "            len(auth_event_ids),",
            "        )",
            "",
            "        # Start by checking events we already have in the DB",
            "        desired_events = set(state_event_ids)",
            "        desired_events.add(event_id)",
            "        logger.debug(\"Fetching %i events from cache/store\", len(desired_events))",
            "        have_events = await self._store.have_seen_events(room_id, desired_events)",
            "",
            "        missing_desired_event_ids = desired_events - have_events",
            "        logger.debug(",
            "            \"We are missing %i events (got %i)\",",
            "            len(missing_desired_event_ids),",
            "            len(have_events),",
            "        )",
            "",
            "        # We probably won't need most of the auth events, so let's just check which",
            "        # we have for now, rather than thrashing the event cache with them all",
            "        # unnecessarily.",
            "",
            "        # TODO: we probably won't actually need all of the auth events, since we",
            "        #   already have a bunch of the state events. It would be nice if the",
            "        #   federation api gave us a way of finding out which we actually need.",
            "",
            "        missing_auth_event_ids = set(auth_event_ids) - have_events",
            "        missing_auth_event_ids.difference_update(",
            "            await self._store.have_seen_events(room_id, missing_auth_event_ids)",
            "        )",
            "        logger.debug(\"We are also missing %i auth events\", len(missing_auth_event_ids))",
            "",
            "        missing_event_ids = missing_desired_event_ids | missing_auth_event_ids",
            "",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids\",",
            "            str(missing_auth_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_auth_event_ids.length\",",
            "            str(len(missing_auth_event_ids)),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids\",",
            "            str(missing_desired_event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"missing_desired_event_ids.length\",",
            "            str(len(missing_desired_event_ids)),",
            "        )",
            "",
            "        # Making an individual request for each of 1000s of events has a lot of",
            "        # overhead. On the other hand, we don't really want to fetch all of the events",
            "        # if we already have most of them.",
            "        #",
            "        # As an arbitrary heuristic, if we are missing more than 10% of the events, then",
            "        # we fetch the whole state.",
            "        #",
            "        # TODO: might it be better to have an API which lets us do an aggregate event",
            "        #   request",
            "        if (len(missing_event_ids) * 10) >= len(auth_event_ids) + len(state_event_ids):",
            "            logger.debug(\"Requesting complete state from remote\")",
            "            await self._get_state_and_persist(destination, room_id, event_id)",
            "        else:",
            "            logger.debug(\"Fetching %i events from remote\", len(missing_event_ids))",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=missing_event_ids",
            "            )",
            "",
            "        # We now need to fill out the state map, which involves fetching the",
            "        # type and state key for each event ID in the state.",
            "        state_map = {}",
            "",
            "        event_metadata = await self._store.get_metadata_for_events(state_event_ids)",
            "        for state_event_id, metadata in event_metadata.items():",
            "            if metadata.room_id != room_id:",
            "                # This is a bogus situation, but since we may only discover it a long time",
            "                # after it happened, we try our best to carry on, by just omitting the",
            "                # bad events from the returned state set.",
            "                #",
            "                # This can happen if a remote server claims that the state or",
            "                # auth_events at an event in room A are actually events in room B",
            "                logger.warning(",
            "                    \"Remote server %s claims event %s in room %s is an auth/state \"",
            "                    \"event in room %s\",",
            "                    destination,",
            "                    state_event_id,",
            "                    metadata.room_id,",
            "                    room_id,",
            "                )",
            "                continue",
            "",
            "            if metadata.state_key is None:",
            "                logger.warning(",
            "                    \"Remote server gave us non-state event in state: %s\", state_event_id",
            "                )",
            "                continue",
            "",
            "            state_map[(metadata.event_type, metadata.state_key)] = state_event_id",
            "",
            "        # if we couldn't get the prev event in question, that's a problem.",
            "        remote_event = await self._store.get_event(",
            "            event_id,",
            "            allow_none=True,",
            "            allow_rejected=True,",
            "            redact_behaviour=EventRedactBehaviour.as_is,",
            "        )",
            "        if not remote_event:",
            "            raise Exception(\"Unable to get missing prev_event %s\" % (event_id,))",
            "",
            "        # missing state at that event is a warning, not a blocker",
            "        # XXX: this doesn't sound right? it means that we'll end up with incomplete",
            "        #   state.",
            "        failed_to_fetch = desired_events - event_metadata.keys()",
            "        # `event_id` could be missing from `event_metadata` because it's not necessarily",
            "        # a state event. We've already checked that we've fetched it above.",
            "        failed_to_fetch.discard(event_id)",
            "        if failed_to_fetch:",
            "            logger.warning(",
            "                \"Failed to fetch missing state events for %s %s\",",
            "                event_id,",
            "                failed_to_fetch,",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch\",",
            "                str(failed_to_fetch),",
            "            )",
            "            set_tag(",
            "                SynapseTags.RESULT_PREFIX + \"failed_to_fetch.length\",",
            "                str(len(failed_to_fetch)),",
            "            )",
            "",
            "        if remote_event.is_state() and remote_event.rejected_reason is None:",
            "            state_map[",
            "                (remote_event.type, remote_event.state_key)",
            "            ] = remote_event.event_id",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_and_persist(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"Get the complete room state at a given event, and persist any new events",
            "        as outliers\"\"\"",
            "        room_version = await self._store.get_room_version(room_id)",
            "        auth_events, state_events = await self._federation_client.get_room_state(",
            "            destination, room_id, event_id=event_id, room_version=room_version",
            "        )",
            "        logger.info(\"/state returned %i events\", len(auth_events) + len(state_events))",
            "",
            "        await self._auth_and_persist_outliers(",
            "            room_id, itertools.chain(auth_events, state_events)",
            "        )",
            "",
            "        # we also need the event itself.",
            "        if not await self._store.have_seen_event(room_id, event_id):",
            "            await self._get_events_and_persist(",
            "                destination=destination, room_id=room_id, event_ids=(event_id,)",
            "            )",
            "",
            "    @trace",
            "    async def _process_received_pdu(",
            "        self,",
            "        origin: str,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        backfilled: bool = False,",
            "    ) -> None:",
            "        \"\"\"Called when we have a new non-outlier event.",
            "",
            "        This is called when we have a new event to add to the room DAG. This can be",
            "        due to:",
            "           * events received directly via a /send request",
            "           * events retrieved via get_missing_events after a /send request",
            "           * events backfilled after a client request.",
            "",
            "        It's not currently used for events received from incoming send_{join,knock,leave}",
            "        requests (which go via on_send_membership_event), nor for joins created by a",
            "        remote join dance (which go via process_remote_join).",
            "",
            "        We need to do auth checks and put it through the StateHandler.",
            "",
            "        Args:",
            "            origin: server sending the event",
            "",
            "            event: event to be persisted",
            "",
            "            context: The `EventContext` to persist the event with.",
            "",
            "            backfilled: True if this is part of a historical batch of events (inhibits",
            "                notification to clients, and validation of device keys.)",
            "",
            "        PartialStateConflictError: if the room was un-partial stated in between",
            "            computing the state at the event and persisting it. The caller should",
            "            recompute `context` and retry exactly once when this happens.",
            "        \"\"\"",
            "        logger.debug(\"Processing event: %s\", event)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        try:",
            "            await self._check_event_auth(origin, event, context)",
            "        except AuthError as e:",
            "            # This happens only if we couldn't find the auth events. We'll already have",
            "            # logged a warning, so now we just convert to a FederationError.",
            "            raise FederationError(\"ERROR\", e.code, e.msg, affected=event.event_id)",
            "",
            "        if not backfilled and not context.rejected:",
            "            # For new (non-backfilled and non-outlier) events we check if the event",
            "            # passes auth based on the current state. If it doesn't then we",
            "            # \"soft-fail\" the event.",
            "            await self._check_for_soft_fail(event, context=context, origin=origin)",
            "",
            "        await self._run_push_actions_and_persist_event(event, context, backfilled)",
            "",
            "        if backfilled or context.rejected:",
            "            return",
            "",
            "        await self._maybe_kick_guest_users(event)",
            "",
            "        # For encrypted messages we check that we know about the sending device,",
            "        # if we don't then we mark the device cache for that user as stale.",
            "        if event.type == EventTypes.Encrypted:",
            "            device_id = event.content.get(\"device_id\")",
            "            sender_key = event.content.get(\"sender_key\")",
            "",
            "            cached_devices = await self._store.get_cached_devices_for_user(event.sender)",
            "",
            "            resync = False  # Whether we should resync device lists.",
            "",
            "            device = None",
            "            if device_id is not None:",
            "                device = cached_devices.get(device_id)",
            "                if device is None:",
            "                    logger.info(",
            "                        \"Received event from remote device not in our cache: %s %s\",",
            "                        event.sender,",
            "                        device_id,",
            "                    )",
            "                    resync = True",
            "",
            "            # We also check if the `sender_key` matches what we expect.",
            "            if sender_key is not None:",
            "                # Figure out what sender key we're expecting. If we know the",
            "                # device and recognize the algorithm then we can work out the",
            "                # exact key to expect. Otherwise check it matches any key we",
            "                # have for that device.",
            "",
            "                current_keys: Container[str] = []",
            "",
            "                if device:",
            "                    keys = device.get(\"keys\", {}).get(\"keys\", {})",
            "",
            "                    if (",
            "                        event.content.get(\"algorithm\")",
            "                        == RoomEncryptionAlgorithms.MEGOLM_V1_AES_SHA2",
            "                    ):",
            "                        # For this algorithm we expect a curve25519 key.",
            "                        key_name = \"curve25519:%s\" % (device_id,)",
            "                        current_keys = [keys.get(key_name)]",
            "                    else:",
            "                        # We don't know understand the algorithm, so we just",
            "                        # check it matches a key for the device.",
            "                        current_keys = keys.values()",
            "                elif device_id:",
            "                    # We don't have any keys for the device ID.",
            "                    pass",
            "                else:",
            "                    # The event didn't include a device ID, so we just look for",
            "                    # keys across all devices.",
            "                    current_keys = [",
            "                        key",
            "                        for device in cached_devices.values()",
            "                        for key in device.get(\"keys\", {}).get(\"keys\", {}).values()",
            "                    ]",
            "",
            "                # We now check that the sender key matches (one of) the expected",
            "                # keys.",
            "                if sender_key not in current_keys:",
            "                    logger.info(",
            "                        \"Received event from remote device with unexpected sender key: %s %s: %s\",",
            "                        event.sender,",
            "                        device_id or \"<no device_id>\",",
            "                        sender_key,",
            "                    )",
            "                    resync = True",
            "",
            "            if resync:",
            "                run_as_background_process(",
            "                    \"resync_device_due_to_pdu\",",
            "                    self._resync_device,",
            "                    event.sender,",
            "                )",
            "",
            "    async def _resync_device(self, sender: str) -> None:",
            "        \"\"\"We have detected that the device list for the given user may be out",
            "        of sync, so we try and resync them.",
            "        \"\"\"",
            "",
            "        try:",
            "            await self._store.mark_remote_users_device_caches_as_stale((sender,))",
            "",
            "            # Immediately attempt a resync in the background",
            "            if self._config.worker.worker_app:",
            "                await self._multi_user_device_resync(user_ids=[sender])",
            "            else:",
            "                await self._device_list_updater.multi_user_device_resync(",
            "                    user_ids=[sender]",
            "                )",
            "        except Exception:",
            "            logger.exception(\"Failed to resync device for %s\", sender)",
            "",
            "    async def backfill_event_id(",
            "        self, destinations: StrCollection, room_id: str, event_id: str",
            "    ) -> PulledPduInfo:",
            "        \"\"\"Backfill a single event and persist it as a non-outlier which means",
            "        we also pull in all of the state and auth events necessary for it.",
            "",
            "        Args:",
            "            destination: The homeserver to pull the given event_id from.",
            "            room_id: The room where the event is from.",
            "            event_id: The event ID to backfill.",
            "",
            "        Raises:",
            "            FederationError if we are unable to find the event from the destination",
            "        \"\"\"",
            "        logger.info(\"backfill_event_id: event_id=%s\", event_id)",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        pulled_pdu_info = await self._federation_client.get_pdu(",
            "            destinations,",
            "            event_id,",
            "            room_version,",
            "        )",
            "",
            "        if not pulled_pdu_info:",
            "            raise FederationError(",
            "                \"ERROR\",",
            "                404,",
            "                f\"Unable to find event_id={event_id} from remote servers to backfill.\",",
            "                affected=event_id,",
            "            )",
            "",
            "        # Persist the event we just fetched, including pulling all of the state",
            "        # and auth events to de-outlier it. This also sets up the necessary",
            "        # `state_groups` for the event.",
            "        await self._process_pulled_events(",
            "            pulled_pdu_info.pull_origin,",
            "            [pulled_pdu_info.pdu],",
            "            # Prevent notifications going to clients",
            "            backfilled=True,",
            "        )",
            "",
            "        return pulled_pdu_info",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_events_and_persist(",
            "        self, destination: str, room_id: str, event_ids: StrCollection",
            "    ) -> None:",
            "        \"\"\"Fetch the given events from a server, and persist them as outliers.",
            "",
            "        This function *does not* recursively get missing auth events of the",
            "        newly fetched events. Callers must include in the `event_ids` argument",
            "        any missing events from the auth chain.",
            "",
            "        Logs a warning if we can't find the given event.",
            "        \"\"\"",
            "",
            "        room_version = await self._store.get_room_version(room_id)",
            "",
            "        events: List[EventBase] = []",
            "",
            "        async def get_event(event_id: str) -> None:",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    pulled_pdu_info = await self._federation_client.get_pdu(",
            "                        [destination],",
            "                        event_id,",
            "                        room_version,",
            "                    )",
            "                    if pulled_pdu_info is None:",
            "                        logger.warning(",
            "                            \"Server %s didn't return event %s\",",
            "                            destination,",
            "                            event_id,",
            "                        )",
            "                        return",
            "                    events.append(pulled_pdu_info.pdu)",
            "",
            "                except Exception as e:",
            "                    logger.warning(",
            "                        \"Error fetching missing state/auth event %s: %s %s\",",
            "                        event_id,",
            "                        type(e),",
            "                        e,",
            "                    )",
            "",
            "        await concurrently_execute(get_event, event_ids, 5)",
            "        logger.info(\"Fetched %i events of %i requested\", len(events), len(event_ids))",
            "        await self._auth_and_persist_outliers(room_id, events)",
            "",
            "    @trace",
            "    async def _auth_and_persist_outliers(",
            "        self, room_id: str, events: Iterable[EventBase]",
            "    ) -> None:",
            "        \"\"\"Persist a batch of outlier events fetched from remote servers.",
            "",
            "        We first sort the events to make sure that we process each event's auth_events",
            "        before the event itself.",
            "",
            "        We then mark the events as outliers, persist them to the database, and, where",
            "        appropriate (eg, an invite), awake the notifier.",
            "",
            "        Params:",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            events: the events that have been fetched",
            "        \"\"\"",
            "        event_map = {event.event_id: event for event in events}",
            "",
            "        event_ids = event_map.keys()",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids\",",
            "            str(event_ids),",
            "        )",
            "        set_tag(",
            "            SynapseTags.FUNC_ARG_PREFIX + \"event_ids.length\",",
            "            str(len(event_ids)),",
            "        )",
            "",
            "        # filter out any events we have already seen. This might happen because",
            "        # the events were eagerly pushed to us (eg, during a room join), or because",
            "        # another thread has raced against us since we decided to request the event.",
            "        #",
            "        # This is just an optimisation, so it doesn't need to be watertight - the event",
            "        # persister does another round of deduplication.",
            "        seen_remotes = await self._store.have_seen_events(room_id, event_map.keys())",
            "        for s in seen_remotes:",
            "            event_map.pop(s, None)",
            "",
            "        # XXX: it might be possible to kick this process off in parallel with fetching",
            "        # the events.",
            "        while event_map:",
            "            # build a list of events whose auth events are not in the queue.",
            "            roots = tuple(",
            "                ev",
            "                for ev in event_map.values()",
            "                if not any(aid in event_map for aid in ev.auth_event_ids())",
            "            )",
            "",
            "            if not roots:",
            "                # if *none* of the remaining events are ready, that means",
            "                # we have a loop. This either means a bug in our logic, or that",
            "                # somebody has managed to create a loop (which requires finding a",
            "                # hash collision in room v2 and later).",
            "                logger.warning(",
            "                    \"Loop found in auth events while fetching missing state/auth \"",
            "                    \"events: %s\",",
            "                    shortstr(event_map.keys()),",
            "                )",
            "                return",
            "",
            "            logger.info(",
            "                \"Persisting %i of %i remaining outliers: %s\",",
            "                len(roots),",
            "                len(event_map),",
            "                shortstr(e.event_id for e in roots),",
            "            )",
            "",
            "            await self._auth_and_persist_outliers_inner(room_id, roots)",
            "",
            "            for ev in roots:",
            "                del event_map[ev.event_id]",
            "",
            "    async def _auth_and_persist_outliers_inner(",
            "        self, room_id: str, fetched_events: Collection[EventBase]",
            "    ) -> None:",
            "        \"\"\"Helper for _auth_and_persist_outliers",
            "",
            "        Persists a batch of events where we have (theoretically) already persisted all",
            "        of their auth events.",
            "",
            "        Marks the events as outliers, auths them, persists them to the database, and,",
            "        where appropriate (eg, an invite), awakes the notifier.",
            "",
            "        Params:",
            "            origin: where the events came from",
            "            room_id: the room that the events are meant to be in (though this has",
            "               not yet been checked)",
            "            fetched_events: the events to persist",
            "        \"\"\"",
            "        # get all the auth events for all the events in this batch. By now, they should",
            "        # have been persisted.",
            "        auth_events = {",
            "            aid for event in fetched_events for aid in event.auth_event_ids()",
            "        }",
            "        persisted_events = await self._store.get_events(",
            "            auth_events,",
            "            allow_rejected=True,",
            "        )",
            "",
            "        events_and_contexts_to_persist: List[Tuple[EventBase, EventContext]] = []",
            "",
            "        async def prep(event: EventBase) -> None:",
            "            with nested_logging_context(suffix=event.event_id):",
            "                auth = []",
            "                for auth_event_id in event.auth_event_ids():",
            "                    ae = persisted_events.get(auth_event_id)",
            "                    if not ae:",
            "                        # the fact we can't find the auth event doesn't mean it doesn't",
            "                        # exist, which means it is premature to reject `event`. Instead we",
            "                        # just ignore it for now.",
            "                        logger.warning(",
            "                            \"Dropping event %s, which relies on auth_event %s, which could not be found\",",
            "                            event,",
            "                            auth_event_id,",
            "                        )",
            "                        return",
            "                    auth.append(ae)",
            "",
            "                # we're not bothering about room state, so flag the event as an outlier.",
            "                event.internal_metadata.outlier = True",
            "",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    await check_state_independent_auth_rules(self._store, event)",
            "                    check_state_dependent_auth_rules(event, auth)",
            "                except AuthError as e:",
            "                    logger.warning(\"Rejecting %r because %s\", event, e)",
            "                    context.rejected = RejectedReason.AUTH_ERROR",
            "                except EventSizeError as e:",
            "                    if e.unpersistable:",
            "                        # This event is completely unpersistable.",
            "                        raise e",
            "                    # Otherwise, we are somewhat lenient and just persist the event",
            "                    # as rejected, for moderate compatibility with older Synapse",
            "                    # versions.",
            "                    logger.warning(\"While validating received event %r: %s\", event, e)",
            "                    context.rejected = RejectedReason.OVERSIZED_EVENT",
            "",
            "            events_and_contexts_to_persist.append((event, context))",
            "",
            "        for event in fetched_events:",
            "            await prep(event)",
            "",
            "        await self.persist_events_and_notify(",
            "            room_id,",
            "            events_and_contexts_to_persist,",
            "            # Mark these events backfilled as they're historic events that will",
            "            # eventually be backfilled. For example, missing events we fetch",
            "            # during backfill should be marked as backfilled as well.",
            "            backfilled=True,",
            "        )",
            "",
            "    @trace",
            "    async def _check_event_auth(",
            "        self, origin: Optional[str], event: EventBase, context: EventContext",
            "    ) -> None:",
            "        \"\"\"",
            "        Checks whether an event should be rejected (for failing auth checks).",
            "",
            "        Args:",
            "            origin: The host the event originates from. This is used to fetch",
            "               any missing auth events. It can be set to None, but only if we are",
            "               sure that we already have all the auth events.",
            "            event: The event itself.",
            "            context:",
            "                The event context.",
            "",
            "        Raises:",
            "            AuthError if we were unable to find copies of the event's auth events.",
            "               (Most other failures just cause us to set `context.rejected`.)",
            "        \"\"\"",
            "        # This method should only be used for non-outliers",
            "        assert not event.internal_metadata.outlier",
            "",
            "        # first of all, check that the event itself is valid.",
            "        try:",
            "            validate_event_for_room_version(event)",
            "        except AuthError as e:",
            "            logger.warning(\"While validating received event %r: %s\", event, e)",
            "            # TODO: use a different rejected reason here?",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "        except EventSizeError as e:",
            "            if e.unpersistable:",
            "                # This event is completely unpersistable.",
            "                raise e",
            "            # Otherwise, we are somewhat lenient and just persist the event",
            "            # as rejected, for moderate compatibility with older Synapse",
            "            # versions.",
            "            logger.warning(\"While validating received event %r: %s\", event, e)",
            "            context.rejected = RejectedReason.OVERSIZED_EVENT",
            "            return",
            "",
            "        # next, check that we have all of the event's auth events.",
            "        #",
            "        # Note that this can raise AuthError, which we want to propagate to the",
            "        # caller rather than swallow with `context.rejected` (since we cannot be",
            "        # certain that there is a permanent problem with the event).",
            "        claimed_auth_events = await self._load_or_fetch_auth_events_for_event(",
            "            origin, event",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events\",",
            "            str([ev.event_id for ev in claimed_auth_events]),",
            "        )",
            "        set_tag(",
            "            SynapseTags.RESULT_PREFIX + \"claimed_auth_events.length\",",
            "            str(len(claimed_auth_events)),",
            "        )",
            "",
            "        # ... and check that the event passes auth at those auth events.",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   4. Passes authorization rules based on the event\u2019s auth events,",
            "        #      otherwise it is rejected.",
            "        try:",
            "            await check_state_independent_auth_rules(self._store, event)",
            "            check_state_dependent_auth_rules(event, claimed_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against auth_events: %s\", event, e",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "            return",
            "",
            "        # now check the auth rules pass against the room state before the event",
            "        # https://spec.matrix.org/v1.3/server-server-api/#checks-performed-on-receipt-of-a-pdu:",
            "        #   5. Passes authorization rules based on the state before the event,",
            "        #      otherwise it is rejected.",
            "        #",
            "        # ... however, if we only have partial state for the room, then there is a good",
            "        # chance that we'll be missing some of the state needed to auth the new event.",
            "        # So, we state-resolve the auth events that we are given against the state that",
            "        # we know about, which ensures things like bans are applied. (Note that we'll",
            "        # already have checked we have all the auth events, in",
            "        # _load_or_fetch_auth_events_for_event above)",
            "        if context.partial_state:",
            "            room_version = await self._store.get_room_version_id(event.room_id)",
            "",
            "            local_state_id_map = await context.get_prev_state_ids()",
            "            claimed_auth_events_id_map = {",
            "                (ev.type, ev.state_key): ev.event_id for ev in claimed_auth_events",
            "            }",
            "",
            "            state_for_auth_id_map = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    [local_state_id_map, claimed_auth_events_id_map],",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "            state_for_auth_id_map = await context.get_prev_state_ids(",
            "                StateFilter.from_types(event_types)",
            "            )",
            "",
            "        calculated_auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "            event, state_for_auth_id_map, for_verification=True",
            "        )",
            "",
            "        # if those are the same, we're done here.",
            "        if collections.Counter(event.auth_event_ids()) == collections.Counter(",
            "            calculated_auth_event_ids",
            "        ):",
            "            return",
            "",
            "        # otherwise, re-run the auth checks based on what we calculated.",
            "        calculated_auth_events = await self._store.get_events_as_list(",
            "            calculated_auth_event_ids",
            "        )",
            "",
            "        # log the differences",
            "",
            "        claimed_auth_event_map = {(e.type, e.state_key): e for e in claimed_auth_events}",
            "        calculated_auth_event_map = {",
            "            (e.type, e.state_key): e for e in calculated_auth_events",
            "        }",
            "        logger.info(",
            "            \"event's auth_events are different to our calculated auth_events. \"",
            "            \"Claimed but not calculated: %s. Calculated but not claimed: %s\",",
            "            [",
            "                ev",
            "                for k, ev in claimed_auth_event_map.items()",
            "                if k not in calculated_auth_event_map",
            "                or calculated_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "            [",
            "                ev",
            "                for k, ev in calculated_auth_event_map.items()",
            "                if k not in claimed_auth_event_map",
            "                or claimed_auth_event_map[k].event_id != ev.event_id",
            "            ],",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, calculated_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"While checking auth of %r against room state before the event: %s\",",
            "                event,",
            "                e,",
            "            )",
            "            context.rejected = RejectedReason.AUTH_ERROR",
            "",
            "    @trace",
            "    async def _maybe_kick_guest_users(self, event: EventBase) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state = await self._storage_controllers.state.get_current_state(",
            "            event.room_id",
            "        )",
            "        current_state_list = list(current_state.values())",
            "        await self._get_room_member_handler().kick_guest_users(current_state_list)",
            "",
            "    async def _check_for_soft_fail(",
            "        self,",
            "        event: EventBase,",
            "        context: EventContext,",
            "        origin: str,",
            "    ) -> None:",
            "        \"\"\"Checks if we should soft fail the event; if so, marks the event as",
            "        such.",
            "",
            "        Does nothing for events in rooms with partial state, since we may not have an",
            "        accurate membership event for the sender in the current state.",
            "",
            "        Args:",
            "            event",
            "            context: The `EventContext` which we are about to persist the event with.",
            "            origin: The host the event originates from.",
            "        \"\"\"",
            "        if await self._store.is_partial_state_room(event.room_id):",
            "            # We might not know the sender's membership in the current state, so don't",
            "            # soft fail anything. Even if we do have a membership for the sender in the",
            "            # current state, it may have been derived from state resolution between",
            "            # partial and full state and may not be accurate.",
            "            return",
            "",
            "        extrem_ids = await self._store.get_latest_event_ids_in_room(event.room_id)",
            "        prev_event_ids = set(event.prev_event_ids())",
            "",
            "        if extrem_ids == prev_event_ids:",
            "            # If they're the same then the current state is the same as the",
            "            # state at the event, so no point rechecking auth for soft fail.",
            "            return",
            "",
            "        room_version = await self._store.get_room_version_id(event.room_id)",
            "        room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "        # The event types we want to pull from the \"current\" state.",
            "        auth_types = auth_types_for_event(room_version_obj, event)",
            "",
            "        # Calculate the \"current state\".",
            "        seen_event_ids = await self._store.have_events_in_timeline(prev_event_ids)",
            "        has_missing_prevs = bool(prev_event_ids - seen_event_ids)",
            "        if has_missing_prevs:",
            "            # We don't have all the prev_events of this event, which means we have a",
            "            # gap in the graph, and the new event is going to become a new backwards",
            "            # extremity.",
            "            #",
            "            # In this case we want to be a little careful as we might have been",
            "            # down for a while and have an incorrect view of the current state,",
            "            # however we still want to do checks as gaps are easy to",
            "            # maliciously manufacture.",
            "            #",
            "            # So we use a \"current state\" that is actually a state",
            "            # resolution across the current forward extremities and the",
            "            # given state at the event. This should correctly handle cases",
            "            # like bans, especially with state res v2.",
            "",
            "            state_sets_d = await self._state_storage_controller.get_state_groups_ids(",
            "                event.room_id, extrem_ids",
            "            )",
            "            state_sets: List[StateMap[str]] = list(state_sets_d.values())",
            "            state_ids = await context.get_prev_state_ids()",
            "            state_sets.append(state_ids)",
            "            current_state_ids = (",
            "                await self._state_resolution_handler.resolve_events_with_store(",
            "                    event.room_id,",
            "                    room_version,",
            "                    state_sets,",
            "                    event_map=None,",
            "                    state_res_store=StateResolutionStore(self._store),",
            "                )",
            "            )",
            "        else:",
            "            current_state_ids = (",
            "                await self._state_storage_controller.get_current_state_ids(",
            "                    event.room_id, StateFilter.from_types(auth_types)",
            "                )",
            "            )",
            "",
            "        logger.debug(",
            "            \"Doing soft-fail check for %s: state %s\",",
            "            event.event_id,",
            "            current_state_ids,",
            "        )",
            "",
            "        # Now check if event pass auth against said current state",
            "        current_state_ids_list = [",
            "            e for k, e in current_state_ids.items() if k in auth_types",
            "        ]",
            "        current_auth_events = await self._store.get_events_as_list(",
            "            current_state_ids_list",
            "        )",
            "",
            "        try:",
            "            check_state_dependent_auth_rules(event, current_auth_events)",
            "        except AuthError as e:",
            "            logger.warning(",
            "                \"Soft-failing %r (from %s) because %s\",",
            "                event,",
            "                e,",
            "                origin,",
            "                extra={",
            "                    \"room_id\": event.room_id,",
            "                    \"mxid\": event.sender,",
            "                    \"hs\": origin,",
            "                },",
            "            )",
            "            soft_failed_event_counter.inc()",
            "            event.internal_metadata.soft_failed = True",
            "",
            "    async def _load_or_fetch_auth_events_for_event(",
            "        self, destination: Optional[str], event: EventBase",
            "    ) -> Collection[EventBase]:",
            "        \"\"\"Fetch this event's auth_events, from database or remote",
            "",
            "        Loads any of the auth_events that we already have from the database/cache. If",
            "        there are any that are missing, calls /event_auth to get the complete auth",
            "        chain for the event (and then attempts to load the auth_events again).",
            "",
            "        If any of the auth_events cannot be found, raises an AuthError. This can happen",
            "        for a number of reasons; eg: the events don't exist, or we were unable to talk",
            "        to `destination`, or we couldn't validate the signature on the event (which",
            "        in turn has multiple potential causes).",
            "",
            "        Args:",
            "            destination: where to send the /event_auth request. Typically the server",
            "               that sent us `event` in the first place.",
            "",
            "               If this is None, no attempt is made to load any missing auth events:",
            "               rather, an AssertionError is raised if there are any missing events.",
            "",
            "            event: the event whose auth_events we want",
            "",
            "        Returns:",
            "            all of the events listed in `event.auth_events_ids`, after deduplication",
            "",
            "        Raises:",
            "            AssertionError if some auth events were missing and no `destination` was",
            "            supplied.",
            "",
            "            AuthError if we were unable to fetch the auth_events for any reason.",
            "        \"\"\"",
            "        event_auth_event_ids = set(event.auth_event_ids())",
            "        event_auth_events = await self._store.get_events(",
            "            event_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids = event_auth_event_ids.difference(",
            "            event_auth_events.keys()",
            "        )",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "        if destination is None:",
            "            # this shouldn't happen: destination must be set unless we know we have already",
            "            # persisted the auth events.",
            "            raise AssertionError(",
            "                \"_load_or_fetch_auth_events_for_event() called with no destination for \"",
            "                \"an event with missing auth_events\"",
            "            )",
            "",
            "        logger.info(",
            "            \"Event %s refers to unknown auth events %s: fetching auth chain\",",
            "            event,",
            "            missing_auth_event_ids,",
            "        )",
            "        try:",
            "            await self._get_remote_auth_chain_for_event(",
            "                destination, event.room_id, event.event_id",
            "            )",
            "        except Exception as e:",
            "            logger.warning(\"Failed to get auth chain for %s: %s\", event, e)",
            "            # in this case, it's very likely we still won't have all the auth",
            "            # events - but we pick that up below.",
            "",
            "        # try to fetch the auth events we missed list time.",
            "        extra_auth_events = await self._store.get_events(",
            "            missing_auth_event_ids, allow_rejected=True",
            "        )",
            "        missing_auth_event_ids.difference_update(extra_auth_events.keys())",
            "        event_auth_events.update(extra_auth_events)",
            "        if not missing_auth_event_ids:",
            "            return event_auth_events.values()",
            "",
            "        # we still don't have all the auth events.",
            "        logger.warning(",
            "            \"Missing auth events for %s: %s\",",
            "            event,",
            "            shortstr(missing_auth_event_ids),",
            "        )",
            "        # the fact we can't find the auth event doesn't mean it doesn't",
            "        # exist, which means it is premature to store `event` as rejected.",
            "        # instead we raise an AuthError, which will make the caller ignore it.",
            "        raise AuthError(code=HTTPStatus.FORBIDDEN, msg=\"Auth events could not be found\")",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_remote_auth_chain_for_event(",
            "        self, destination: str, room_id: str, event_id: str",
            "    ) -> None:",
            "        \"\"\"If we are missing some of an event's auth events, attempt to request them",
            "",
            "        Args:",
            "            destination: where to fetch the auth tree from",
            "            room_id: the room in which we are lacking auth events",
            "            event_id: the event for which we are lacking auth events",
            "        \"\"\"",
            "        try:",
            "            remote_events = await self._federation_client.get_event_auth(",
            "                destination, room_id, event_id",
            "            )",
            "",
            "        except RequestSendFailed as e1:",
            "            # The other side isn't around or doesn't implement the",
            "            # endpoint, so lets just bail out.",
            "            logger.info(\"Failed to get event auth from remote: %s\", e1)",
            "            return",
            "",
            "        logger.info(\"/event_auth returned %i events\", len(remote_events))",
            "",
            "        # `event` may be returned, but we should not yet process it.",
            "        remote_auth_events = (e for e in remote_events if e.event_id != event_id)",
            "",
            "        await self._auth_and_persist_outliers(room_id, remote_auth_events)",
            "",
            "    @trace",
            "    async def _run_push_actions_and_persist_event(",
            "        self, event: EventBase, context: EventContext, backfilled: bool = False",
            "    ) -> None:",
            "        \"\"\"Run the push actions for a received event, and persist it.",
            "",
            "        Args:",
            "            event: The event itself.",
            "            context: The event context.",
            "            backfilled: True if the event was backfilled.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "        # this method should not be called on outliers (those code paths call",
            "        # persist_events_and_notify directly.)",
            "        assert not event.internal_metadata.outlier",
            "",
            "        if not backfilled and not context.rejected:",
            "            min_depth = await self._store.get_min_depth(event.room_id)",
            "            if min_depth is None or min_depth > event.depth:",
            "                # XXX richvdh 2021/10/07: I don't really understand what this",
            "                # condition is doing. I think it's trying not to send pushes",
            "                # for events that predate our join - but that's not really what",
            "                # min_depth means, and anyway ancient events are a more general",
            "                # problem.",
            "                #",
            "                # for now I'm just going to log about it.",
            "                logger.info(",
            "                    \"Skipping push actions for old event with depth %s < %s\",",
            "                    event.depth,",
            "                    min_depth,",
            "                )",
            "            else:",
            "                await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "                    [(event, context)]",
            "                )",
            "",
            "        try:",
            "            await self.persist_events_and_notify(",
            "                event.room_id, [(event, context)], backfilled=backfilled",
            "            )",
            "        except Exception:",
            "            await self._store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def persist_events_and_notify(",
            "        self,",
            "        room_id: str,",
            "        event_and_contexts: Sequence[Tuple[EventBase, EventContext]],",
            "        backfilled: bool = False,",
            "    ) -> int:",
            "        \"\"\"Persists events and tells the notifier/pushers about them, if",
            "        necessary.",
            "",
            "        Args:",
            "            room_id: The room ID of events being persisted.",
            "            event_and_contexts: Sequence of events with their associated",
            "                context that should be persisted. All events must belong to",
            "                the same room.",
            "            backfilled: Whether these events are a result of",
            "                backfilling or not",
            "",
            "        Returns:",
            "            The stream ID after which all events have been persisted.",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        if not event_and_contexts:",
            "            return self._store.get_room_max_stream_ordering()",
            "",
            "        instance = self._config.worker.events_shard_config.get_instance(room_id)",
            "        if instance != self._instance_name:",
            "            # Limit the number of events sent over replication. We choose 200",
            "            # here as that is what we default to in `max_request_body_size(..)`",
            "            result = {}",
            "            try:",
            "                for batch in batch_iter(event_and_contexts, 200):",
            "                    result = await self._send_events(",
            "                        instance_name=instance,",
            "                        store=self._store,",
            "                        room_id=room_id,",
            "                        event_and_contexts=batch,",
            "                        backfilled=backfilled,",
            "                    )",
            "            except SynapseError as e:",
            "                if e.code == HTTPStatus.CONFLICT:",
            "                    raise PartialStateConflictError()",
            "                raise",
            "            return result[\"max_stream_id\"]",
            "        else:",
            "            assert self._storage_controllers.persistence",
            "",
            "            # Note that this returns the events that were persisted, which may not be",
            "            # the same as were passed in if some were deduplicated due to transaction IDs.",
            "            (",
            "                events,",
            "                max_stream_token,",
            "            ) = await self._storage_controllers.persistence.persist_events(",
            "                event_and_contexts, backfilled=backfilled",
            "            )",
            "",
            "            # After persistence we always need to notify replication there may",
            "            # be new data.",
            "            self._notifier.notify_replication()",
            "",
            "            if self._ephemeral_messages_enabled:",
            "                for event in events:",
            "                    # If there's an expiry timestamp on the event, schedule its expiry.",
            "                    self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            if not backfilled:  # Never notify for backfilled events",
            "                with start_active_span(\"notify_persisted_events\"):",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids\",",
            "                        str([ev.event_id for ev in events]),",
            "                    )",
            "                    set_tag(",
            "                        SynapseTags.RESULT_PREFIX + \"event_ids.length\",",
            "                        str(len(events)),",
            "                    )",
            "                    for event in events:",
            "                        await self._notify_persisted_event(event, max_stream_token)",
            "",
            "            return max_stream_token.stream",
            "",
            "    async def _notify_persisted_event(",
            "        self, event: EventBase, max_stream_token: RoomStreamToken",
            "    ) -> None:",
            "        \"\"\"Checks to see if notifier/pushers should be notified about the",
            "        event or not.",
            "",
            "        Args:",
            "            event:",
            "            max_stream_token: The max_stream_id returned by persist_events",
            "        \"\"\"",
            "",
            "        extra_users = []",
            "        if event.type == EventTypes.Member:",
            "            target_user_id = event.state_key",
            "",
            "            # We notify for memberships if its an invite for one of our",
            "            # users",
            "            if event.internal_metadata.is_outlier():",
            "                if event.membership != Membership.INVITE:",
            "                    if not self._is_mine_id(target_user_id):",
            "                        return",
            "",
            "            target_user = UserID.from_string(target_user_id)",
            "            extra_users.append(target_user)",
            "        elif event.internal_metadata.is_outlier():",
            "            return",
            "",
            "        # the event has been persisted so it should have a stream ordering.",
            "        assert event.internal_metadata.stream_ordering",
            "",
            "        event_pos = PersistedEventPosition(",
            "            self._instance_name, event.internal_metadata.stream_ordering",
            "        )",
            "        await self._notifier.on_new_room_events(",
            "            [(event, event_pos)], max_stream_token, extra_users=extra_users",
            "        )",
            "",
            "        if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "            # TODO retrieve the previous state, and exclude join -> join transitions",
            "            self._notifier.notify_user_joined_room(event.event_id, event.room_id)",
            "",
            "        # If this is a server ACL event, clear the cache in the storage controller.",
            "        if event.type == EventTypes.ServerACL:",
            "            self._state_storage_controller.get_server_acl_for_room.invalidate(",
            "                (event.room_id,)",
            "            )",
            "",
            "    def _sanity_check_event(self, ev: EventBase) -> None:",
            "        \"\"\"",
            "        Do some early sanity checks of a received event",
            "",
            "        In particular, checks it doesn't have an excessive number of",
            "        prev_events or auth_events, which could cause a huge state resolution",
            "        or cascade of event fetches.",
            "",
            "        Args:",
            "            ev: event to be checked",
            "",
            "        Raises:",
            "            SynapseError if the event does not pass muster",
            "        \"\"\"",
            "        if len(ev.prev_event_ids()) > 20:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i prev_events\",",
            "                ev.event_id,",
            "                len(ev.prev_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many prev_events\")",
            "",
            "        if len(ev.auth_event_ids()) > 10:",
            "            logger.warning(",
            "                \"Rejecting event %s which has %i auth_events\",",
            "                ev.event_id,",
            "                len(ev.auth_event_ids()),",
            "            )",
            "            raise SynapseError(HTTPStatus.BAD_REQUEST, \"Too many auth_events\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.handlers.federation_event.FederationEventHandler.self"
        ]
    },
    "synapse/handlers/message.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1730,
                "afterPatchRowNumber": 1730,
                "PatchRowcode": "                         event.event_id, event.room_id"
            },
            "1": {
                "beforePatchRowNumber": 1731,
                "afterPatchRowNumber": 1731,
                "PatchRowcode": "                     )"
            },
            "2": {
                "beforePatchRowNumber": 1732,
                "afterPatchRowNumber": 1732,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1733,
                "PatchRowcode": "+            if event.type == EventTypes.ServerACL:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1734,
                "PatchRowcode": "+                self._storage_controllers.state.get_server_acl_for_room.invalidate("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1735,
                "PatchRowcode": "+                    (event.room_id,)"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1736,
                "PatchRowcode": "+                )"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1737,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": 1733,
                "afterPatchRowNumber": 1738,
                "PatchRowcode": "             await self._maybe_kick_guest_users(event, context)"
            },
            "9": {
                "beforePatchRowNumber": 1734,
                "afterPatchRowNumber": 1739,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 1735,
                "afterPatchRowNumber": 1740,
                "PatchRowcode": "             if event.type == EventTypes.CanonicalAlias:"
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017-2018 New Vector Ltd",
            "# Copyright 2019-2020 The Matrix.org Foundation C.I.C.",
            "# Copyrignt 2020 Sorunome",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from http import HTTPStatus",
            "from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Tuple",
            "",
            "from canonicaljson import encode_canonical_json",
            "",
            "from twisted.internet.interfaces import IDelayedCall",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    HistoryVisibility,",
            "    Membership,",
            "    RelationTypes,",
            "    UserTypes,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    ConsentNotGivenError,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    ShadowBanError,",
            "    SynapseError,",
            "    UnstableSpecAuthError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS",
            "from synapse.api.urls import ConsentURIBuilder",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase, relation_from_event",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.snapshot import EventContext, UnpersistedEventContextBase",
            "from synapse.events.utils import SerializeEventConfig, maybe_upsert_event_field",
            "from synapse.events.validator import EventValidator",
            "from synapse.handlers.directory import DirectoryHandler",
            "from synapse.handlers.worker_lock import NEW_EVENT_DURING_PURGE_LOCK_NAME",
            "from synapse.logging import opentracing",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.send_event import ReplicationSendEventRestServlet",
            "from synapse.replication.http.send_events import ReplicationSendEventsRestServlet",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    PersistedEventPosition,",
            "    Requester,",
            "    RoomAlias,",
            "    StateMap,",
            "    StreamToken,",
            "    UserID,",
            "    create_requester,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util import json_decoder, json_encoder, log_failure, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, gather_results",
            "from synapse.util.caches.expiringcache import ExpiringCache",
            "from synapse.util.metrics import measure_func",
            "from synapse.visibility import get_effective_room_visibility_from_state",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class MessageHandler:",
            "    \"\"\"Contains some read only APIs to get state about a room\"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.auth = hs.get_auth()",
            "        self.clock = hs.get_clock()",
            "        self.state = hs.get_state_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        # The scheduled call to self._expire_event. None if no call is currently",
            "        # scheduled.",
            "        self._scheduled_expiry: Optional[IDelayedCall] = None",
            "",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"_schedule_next_expiry\", self._schedule_next_expiry",
            "            )",
            "",
            "    async def get_room_data(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        event_type: str,",
            "        state_key: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get data from a room.",
            "",
            "        Args:",
            "            requester: The user who did the request.",
            "            room_id",
            "            event_type",
            "            state_key",
            "        Returns:",
            "            The path data content.",
            "        Raises:",
            "            SynapseError or AuthError if the user is not in the room",
            "        \"\"\"",
            "        (",
            "            membership,",
            "            membership_event_id,",
            "        ) = await self.auth.check_user_in_room_or_world_readable(",
            "            room_id, requester, allow_departed_users=True",
            "        )",
            "",
            "        if membership == Membership.JOIN:",
            "            data = await self._storage_controllers.state.get_current_state_event(",
            "                room_id, event_type, state_key",
            "            )",
            "        elif membership == Membership.LEAVE:",
            "            key = (event_type, state_key)",
            "            # If the membership is not JOIN, then the event ID should exist.",
            "            assert (",
            "                membership_event_id is not None",
            "            ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "            room_state = await self._state_storage_controller.get_state_for_events(",
            "                [membership_event_id], StateFilter.from_types([key])",
            "            )",
            "            data = room_state[membership_event_id].get(key)",
            "        else:",
            "            # check_user_in_room_or_world_readable, if it doesn't raise an AuthError, should",
            "            # only ever return a Membership.JOIN/LEAVE object",
            "            #",
            "            # Safeguard in case it returned something else",
            "            logger.error(",
            "                \"Attempted to retrieve data from a room for a user that has never been in it. \"",
            "                \"This should not have happened.\"",
            "            )",
            "            raise UnstableSpecAuthError(",
            "                403,",
            "                \"User not in room\",",
            "                errcode=Codes.NOT_JOINED,",
            "            )",
            "",
            "        return data",
            "",
            "    async def get_state_events(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        at_token: Optional[StreamToken] = None,",
            "    ) -> List[dict]:",
            "        \"\"\"Retrieve all state events for a given room. If the user is",
            "        joined to the room then return the current state. If the user has",
            "        left the room return the state events from when they left. If an explicit",
            "        'at' parameter is passed, return the state events as of that event, if",
            "        visible.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "            state_filter: The state filter used to fetch state from the database.",
            "            at_token: the stream token of the at which we are requesting",
            "                the stats. If the user is not allowed to view the state as of that",
            "                stream token, we raise a 403 SynapseError. If None, returns the current",
            "                state based on the current_state_events table.",
            "        Returns:",
            "            A list of dicts representing state events. [{}, {}, {}]",
            "        Raises:",
            "            NotFoundError (404) if the at token does not yield an event",
            "",
            "            AuthError (403) if the user doesn't have permission to view",
            "            members of this room.",
            "        \"\"\"",
            "        state_filter = state_filter or StateFilter.all()",
            "        user_id = requester.user.to_string()",
            "",
            "        if at_token:",
            "            last_event_id = (",
            "                await self.store.get_last_event_in_room_before_stream_ordering(",
            "                    room_id,",
            "                    end_token=at_token.room_key,",
            "                )",
            "            )",
            "",
            "            if not last_event_id:",
            "                raise NotFoundError(\"Can't find event for token %s\" % (at_token,))",
            "",
            "            if not await self._user_can_see_state_at_event(",
            "                user_id, room_id, last_event_id",
            "            ):",
            "                raise AuthError(",
            "                    403,",
            "                    \"User %s not allowed to view events in room %s at token %s\"",
            "                    % (user_id, room_id, at_token),",
            "                )",
            "",
            "            room_state_events = (",
            "                await self._state_storage_controller.get_state_for_events(",
            "                    [last_event_id], state_filter=state_filter",
            "                )",
            "            )",
            "            room_state: Mapping[Any, EventBase] = room_state_events[last_event_id]",
            "        else:",
            "            (",
            "                membership,",
            "                membership_event_id,",
            "            ) = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "",
            "            if membership == Membership.JOIN:",
            "                state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                    room_id, state_filter=state_filter",
            "                )",
            "                room_state = await self.store.get_events(state_ids.values())",
            "            elif membership == Membership.LEAVE:",
            "                # If the membership is not JOIN, then the event ID should exist.",
            "                assert (",
            "                    membership_event_id is not None",
            "                ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "                room_state_events = (",
            "                    await self._state_storage_controller.get_state_for_events(",
            "                        [membership_event_id], state_filter=state_filter",
            "                    )",
            "                )",
            "                room_state = room_state_events[membership_event_id]",
            "",
            "        events = self._event_serializer.serialize_events(",
            "            room_state.values(),",
            "            self.clock.time_msec(),",
            "            config=SerializeEventConfig(requester=requester),",
            "        )",
            "        return events",
            "",
            "    async def _user_can_see_state_at_event(",
            "        self, user_id: str, room_id: str, event_id: str",
            "    ) -> bool:",
            "        # check whether the user was in the room, and the history visibility,",
            "        # at that time.",
            "        state_map = await self._state_storage_controller.get_state_for_event(",
            "            event_id,",
            "            StateFilter.from_types(",
            "                [",
            "                    (EventTypes.Member, user_id),",
            "                    (EventTypes.RoomHistoryVisibility, \"\"),",
            "                ]",
            "            ),",
            "        )",
            "",
            "        membership = None",
            "        membership_event = state_map.get((EventTypes.Member, user_id))",
            "        if membership_event:",
            "            membership = membership_event.membership",
            "",
            "        # if the user was a member of the room at the time of the event,",
            "        # they can see it.",
            "        if membership == Membership.JOIN:",
            "            return True",
            "",
            "        # otherwise, it depends on the history visibility.",
            "        visibility = get_effective_room_visibility_from_state(state_map)",
            "",
            "        if visibility == HistoryVisibility.JOINED:",
            "            # we weren't a member at the time of the event, so we can't see this event.",
            "            return False",
            "",
            "        # otherwise *invited* is good enough",
            "        if membership == Membership.INVITE:",
            "            return True",
            "",
            "        if visibility == HistoryVisibility.INVITED:",
            "            # we weren't invited, so we can't see this event.",
            "            return False",
            "",
            "        if visibility == HistoryVisibility.WORLD_READABLE:",
            "            return True",
            "",
            "        # So it's SHARED, and the user was not a member at the time. The user cannot",
            "        # see history, unless they have *subsequently* joined the room.",
            "        #",
            "        # XXX: if the user has subsequently joined and then left again,",
            "        # ideally we would share history up to the point they left. But",
            "        # we don't know when they left. We just treat it as though they",
            "        # never joined, and restrict access.",
            "",
            "        (",
            "            current_membership,",
            "            _,",
            "        ) = await self.store.get_local_current_membership_for_user_in_room(",
            "            user_id, event_id",
            "        )",
            "        return current_membership == Membership.JOIN",
            "",
            "    async def get_joined_members(self, requester: Requester, room_id: str) -> dict:",
            "        \"\"\"Get all the joined members in the room and their profile information.",
            "",
            "        If the user has left the room return the state events from when they left.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "        Returns:",
            "            A dict of user_id to profile info",
            "        \"\"\"",
            "        if not requester.app_service:",
            "            # We check AS auth after fetching the room membership, as it",
            "            # requires us to pull out all joined members anyway.",
            "            membership, _ = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "            if membership != Membership.JOIN:",
            "                raise SynapseError(",
            "                    code=403,",
            "                    errcode=Codes.FORBIDDEN,",
            "                    msg=\"Getting joined members while not being a current member of the room is forbidden.\",",
            "                )",
            "",
            "        users_with_profile = (",
            "            await self._state_storage_controller.get_users_in_room_with_profiles(",
            "                room_id",
            "            )",
            "        )",
            "",
            "        # If this is an AS, double check that they are allowed to see the members.",
            "        # This can either be because the AS user is in the room or because there",
            "        # is a user in the room that the AS is \"interested in\"",
            "        if (",
            "            requester.app_service",
            "            and requester.user.to_string() not in users_with_profile",
            "        ):",
            "            for uid in users_with_profile:",
            "                if requester.app_service.is_interested_in_user(uid):",
            "                    break",
            "            else:",
            "                # Loop fell through, AS has no interested users in room",
            "                raise UnstableSpecAuthError(",
            "                    403,",
            "                    \"Appservice not in room\",",
            "                    errcode=Codes.NOT_JOINED,",
            "                )",
            "",
            "        return {",
            "            user_id: {",
            "                \"avatar_url\": profile.avatar_url,",
            "                \"display_name\": profile.display_name,",
            "            }",
            "            for user_id, profile in users_with_profile.items()",
            "        }",
            "",
            "    def maybe_schedule_expiry(self, event: EventBase) -> None:",
            "        \"\"\"Schedule the expiry of an event if there's not already one scheduled,",
            "        or if the one running is for an event that will expire after the provided",
            "        timestamp.",
            "",
            "        This function needs to invalidate the event cache, which is only possible on",
            "        the master process, and therefore needs to be run on there.",
            "",
            "        Args:",
            "            event: The event to schedule the expiry of.",
            "        \"\"\"",
            "",
            "        expiry_ts = event.content.get(EventContentFields.SELF_DESTRUCT_AFTER)",
            "        if type(expiry_ts) is not int or event.is_state():  # noqa: E721",
            "            return",
            "",
            "        # _schedule_expiry_for_event won't actually schedule anything if there's already",
            "        # a task scheduled for a timestamp that's sooner than the provided one.",
            "        self._schedule_expiry_for_event(event.event_id, expiry_ts)",
            "",
            "    async def _schedule_next_expiry(self) -> None:",
            "        \"\"\"Retrieve the ID and the expiry timestamp of the next event to be expired,",
            "        and schedule an expiry task for it.",
            "",
            "        If there's no event left to expire, set _expiry_scheduled to None so that a",
            "        future call to save_expiry_ts can schedule a new expiry task.",
            "        \"\"\"",
            "        # Try to get the expiry timestamp of the next event to expire.",
            "        res = await self.store.get_next_event_to_expire()",
            "        if res:",
            "            event_id, expiry_ts = res",
            "            self._schedule_expiry_for_event(event_id, expiry_ts)",
            "",
            "    def _schedule_expiry_for_event(self, event_id: str, expiry_ts: int) -> None:",
            "        \"\"\"Schedule an expiry task for the provided event if there's not already one",
            "        scheduled at a timestamp that's sooner than the provided one.",
            "",
            "        Args:",
            "            event_id: The ID of the event to expire.",
            "            expiry_ts: The timestamp at which to expire the event.",
            "        \"\"\"",
            "        if self._scheduled_expiry:",
            "            # If the provided timestamp refers to a time before the scheduled time of the",
            "            # next expiry task, cancel that task and reschedule it for this timestamp.",
            "            next_scheduled_expiry_ts = self._scheduled_expiry.getTime() * 1000",
            "            if expiry_ts < next_scheduled_expiry_ts:",
            "                self._scheduled_expiry.cancel()",
            "            else:",
            "                return",
            "",
            "        # Figure out how many seconds we need to wait before expiring the event.",
            "        now_ms = self.clock.time_msec()",
            "        delay = (expiry_ts - now_ms) / 1000",
            "",
            "        # callLater doesn't support negative delays, so trim the delay to 0 if we're",
            "        # in that case.",
            "        if delay < 0:",
            "            delay = 0",
            "",
            "        logger.info(\"Scheduling expiry for event %s in %.3fs\", event_id, delay)",
            "",
            "        self._scheduled_expiry = self.clock.call_later(",
            "            delay,",
            "            run_as_background_process,",
            "            \"_expire_event\",",
            "            self._expire_event,",
            "            event_id,",
            "        )",
            "",
            "    async def _expire_event(self, event_id: str) -> None:",
            "        \"\"\"Retrieve and expire an event that needs to be expired from the database.",
            "",
            "        If the event doesn't exist in the database, log it and delete the expiry date",
            "        from the database (so that we don't try to expire it again).",
            "        \"\"\"",
            "        assert self._ephemeral_events_enabled",
            "",
            "        self._scheduled_expiry = None",
            "",
            "        logger.info(\"Expiring event %s\", event_id)",
            "",
            "        try:",
            "            # Expire the event if we know about it. This function also deletes the expiry",
            "            # date from the database in the same database transaction.",
            "            await self.store.expire_event(event_id)",
            "        except Exception as e:",
            "            logger.error(\"Could not expire event %s: %r\", event_id, e)",
            "",
            "        # Schedule the expiry of the next event to expire.",
            "        await self._schedule_next_expiry()",
            "",
            "",
            "# The duration (in ms) after which rooms should be removed",
            "# `_rooms_to_exclude_from_dummy_event_insertion` (with the effect that we will try",
            "# to generate a dummy event for them once more)",
            "#",
            "_DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY = 7 * 24 * 60 * 60 * 1000",
            "",
            "",
            "class EventCreationHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "        self.auth_blocking = hs.get_auth_blocking()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.state = hs.get_state_handler()",
            "        self.clock = hs.get_clock()",
            "        self.validator = EventValidator()",
            "        self.profile_handler = hs.get_profile_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self.server_name = hs.hostname",
            "        self.notifier = hs.get_notifier()",
            "        self.config = hs.config",
            "        self.require_membership_for_aliases = (",
            "            hs.config.server.require_membership_for_aliases",
            "        )",
            "        self._events_shard_config = self.config.worker.events_shard_config",
            "        self._instance_name = hs.get_instance_name()",
            "        self._notifier = hs.get_notifier()",
            "        self._worker_lock_handler = hs.get_worker_locks_handler()",
            "",
            "        self.room_prejoin_state_types = self.hs.config.api.room_prejoin_state",
            "",
            "        self.membership_types_to_include_profile_data_in = {",
            "            Membership.JOIN,",
            "            Membership.KNOCK,",
            "        }",
            "        if self.hs.config.server.include_profile_data_on_invite:",
            "            self.membership_types_to_include_profile_data_in.add(Membership.INVITE)",
            "",
            "        self.send_event = ReplicationSendEventRestServlet.make_client(hs)",
            "        self.send_events = ReplicationSendEventsRestServlet.make_client(hs)",
            "",
            "        self.request_ratelimiter = hs.get_request_ratelimiter()",
            "",
            "        # We limit concurrent event creation for a room to 1. This prevents state resolution",
            "        # from occurring when sending bursts of events to a local room",
            "        self.limiter = Linearizer(max_count=1, name=\"room_event_creation_limit\")",
            "",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self._third_party_event_rules = (",
            "            self.hs.get_module_api_callbacks().third_party_event_rules",
            "        )",
            "",
            "        self._block_events_without_consent_error = (",
            "            self.config.consent.block_events_without_consent_error",
            "        )",
            "",
            "        # we need to construct a ConsentURIBuilder here, as it checks that the necessary",
            "        # config options, but *only* if we have a configuration for which we are",
            "        # going to need it.",
            "        if self._block_events_without_consent_error:",
            "            self._consent_uri_builder = ConsentURIBuilder(self.config)",
            "",
            "        # Rooms which should be excluded from dummy insertion. (For instance,",
            "        # those without local users who can send events into the room).",
            "        #",
            "        # map from room id to time-of-last-attempt.",
            "        #",
            "        self._rooms_to_exclude_from_dummy_event_insertion: Dict[str, int] = {}",
            "        # The number of forward extremeities before a dummy event is sent.",
            "        self._dummy_events_threshold = hs.config.server.dummy_events_threshold",
            "",
            "        if (",
            "            self.config.worker.run_background_tasks",
            "            and self.config.server.cleanup_extremities_with_dummy_events",
            "        ):",
            "            self.clock.looping_call(",
            "                lambda: run_as_background_process(",
            "                    \"send_dummy_events_to_fill_extremities\",",
            "                    self._send_dummy_events_to_fill_extremities,",
            "                ),",
            "                5 * 60 * 1000,",
            "            )",
            "",
            "        self._message_handler = hs.get_message_handler()",
            "",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._external_cache = hs.get_external_cache()",
            "",
            "        # Stores the state groups we've recently added to the joined hosts",
            "        # external cache. Note that the timeout must be significantly less than",
            "        # the TTL on the external cache.",
            "        self._external_cache_joined_hosts_updates: Optional[ExpiringCache] = None",
            "        if self._external_cache.is_enabled():",
            "            self._external_cache_joined_hosts_updates = ExpiringCache(",
            "                \"_external_cache_joined_hosts_updates\",",
            "                self.clock,",
            "                expiry_ms=30 * 60 * 1000,",
            "            )",
            "",
            "    async def create_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        txn_id: Optional[str] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        require_consent: bool = True,",
            "        outlier: bool = False,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        \"\"\"",
            "        Given a dict from a client, create a new event. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events.",
            "",
            "        Creates an FrozenEvent object, filling out auth_events, prev_events,",
            "        etc.",
            "",
            "        Adds display names to Join membership events.",
            "",
            "        Args:",
            "            requester",
            "            event_dict: An entire event",
            "            txn_id",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases (previously useful for MSC2716).",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "                If non-None, prev_event_ids must also be provided.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This was previously used particularly",
            "                by the MSC2716 /batch_send endpoint. This should normally be left as",
            "                None, which will cause the auth_event_ids to be calculated based on the",
            "                room state at the prev_events.",
            "",
            "            require_consent: Whether to check if the requester has",
            "                consented to the privacy policy.",
            "",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Raises:",
            "            ResourceLimitError if server is blocked to some resource being",
            "            exceeded",
            "",
            "        Returns:",
            "            Tuple of created event, Context",
            "        \"\"\"",
            "        await self.auth_blocking.check_auth_blocking(requester=requester)",
            "",
            "        if event_dict[\"type\"] == EventTypes.Create and event_dict[\"state_key\"] == \"\":",
            "            room_version_id = event_dict[\"content\"][\"room_version\"]",
            "            maybe_room_version_obj = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "            if not maybe_room_version_obj:",
            "                # this can happen if support is withdrawn for a room version",
            "                raise UnsupportedRoomVersionError(room_version_id)",
            "            room_version_obj = maybe_room_version_obj",
            "        else:",
            "            try:",
            "                room_version_obj = await self.store.get_room_version(",
            "                    event_dict[\"room_id\"]",
            "                )",
            "            except NotFoundError:",
            "                raise AuthError(403, \"Unknown room\")",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        self.validator.validate_builder(builder)",
            "",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            target = UserID.from_string(builder.state_key)",
            "",
            "            if membership in self.membership_types_to_include_profile_data_in:",
            "                # If event doesn't include a display name, add one.",
            "                profile = self.profile_handler",
            "                content = builder.content",
            "",
            "                try:",
            "                    if \"displayname\" not in content:",
            "                        displayname = await profile.get_displayname(target)",
            "                        if displayname is not None:",
            "                            content[\"displayname\"] = displayname",
            "                    if \"avatar_url\" not in content:",
            "                        avatar_url = await profile.get_avatar_url(target)",
            "                        if avatar_url is not None:",
            "                            content[\"avatar_url\"] = avatar_url",
            "                except Exception as e:",
            "                    logger.info(",
            "                        \"Failed to get profile information for %r: %s\", target, e",
            "                    )",
            "",
            "        is_exempt = await self._is_exempt_from_privacy_policy(builder, requester)",
            "        if require_consent and not is_exempt:",
            "            await self.assert_accepted_privacy_policy(requester)",
            "",
            "        # Save the access token ID, the device ID and the transaction ID in the event",
            "        # internal metadata. This is useful to determine if we should echo the",
            "        # transaction_id in events.",
            "        # See `synapse.events.utils.EventClientSerializer.serialize_event`",
            "        if requester.access_token_id is not None:",
            "            builder.internal_metadata.token_id = requester.access_token_id",
            "",
            "        if requester.device_id is not None:",
            "            builder.internal_metadata.device_id = requester.device_id",
            "",
            "        if txn_id is not None:",
            "            builder.internal_metadata.txn_id = txn_id",
            "",
            "        builder.internal_metadata.outlier = outlier",
            "",
            "        event, unpersisted_context = await self.create_new_client_event(",
            "            builder=builder,",
            "            requester=requester,",
            "            allow_no_prev_events=allow_no_prev_events,",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=auth_event_ids,",
            "            state_event_ids=state_event_ids,",
            "            depth=depth,",
            "            state_map=state_map,",
            "            for_batch=for_batch,",
            "            current_state_group=current_state_group,",
            "        )",
            "",
            "        # In an ideal world we wouldn't need the second part of this condition. However,",
            "        # this behaviour isn't spec'd yet, meaning we should be able to deactivate this",
            "        # behaviour. Another reason is that this code is also evaluated each time a new",
            "        # m.room.aliases event is created, which includes hitting a /directory route.",
            "        # Therefore not including this condition here would render the similar one in",
            "        # synapse.handlers.directory pointless.",
            "        if builder.type == EventTypes.Aliases and self.require_membership_for_aliases:",
            "            # Ideally we'd do the membership check in event_auth.check(), which",
            "            # describes a spec'd algorithm for authenticating events received over",
            "            # federation as well as those created locally. As of room v3, aliases events",
            "            # can be created by users that are not in the room, therefore we have to",
            "            # tolerate them in event_auth.check().",
            "            if for_batch:",
            "                assert state_map is not None",
            "                prev_event_id = state_map.get((EventTypes.Member, event.sender))",
            "            else:",
            "                prev_state_ids = await unpersisted_context.get_prev_state_ids(",
            "                    StateFilter.from_types([(EventTypes.Member, event.sender)])",
            "                )",
            "                prev_event_id = prev_state_ids.get((EventTypes.Member, event.sender))",
            "            prev_event = (",
            "                await self.store.get_event(prev_event_id, allow_none=True)",
            "                if prev_event_id",
            "                else None",
            "            )",
            "            if not prev_event or prev_event.membership != Membership.JOIN:",
            "                logger.warning(",
            "                    (",
            "                        \"Attempt to send `m.room.aliases` in room %s by user %s but\"",
            "                        \" membership is %s\"",
            "                    ),",
            "                    event.room_id,",
            "                    event.sender,",
            "                    prev_event.membership if prev_event else None,",
            "                )",
            "",
            "                raise AuthError(",
            "                    403, \"You must be in the room to create an alias for it\"",
            "                )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "        return event, unpersisted_context",
            "",
            "    async def _is_exempt_from_privacy_policy(",
            "        self, builder: EventBuilder, requester: Requester",
            "    ) -> bool:",
            "        \"\"\" \"Determine if an event to be sent is exempt from having to consent",
            "        to the privacy policy",
            "",
            "        Args:",
            "            builder: event being created",
            "            requester: user requesting this event",
            "",
            "        Returns:",
            "            true if the event can be sent without the user consenting",
            "        \"\"\"",
            "        # the only thing the user can do is join the server notices room.",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            if membership == Membership.JOIN:",
            "                return await self.store.is_server_notice_room(builder.room_id)",
            "            elif membership == Membership.LEAVE:",
            "                # the user is always allowed to leave (but not kick people)",
            "                return builder.state_key == requester.user.to_string()",
            "        return False",
            "",
            "    async def assert_accepted_privacy_policy(self, requester: Requester) -> None:",
            "        \"\"\"Check if a user has accepted the privacy policy",
            "",
            "        Called when the given user is about to do something that requires",
            "        privacy consent. We see if the user is exempt and otherwise check that",
            "        they have given consent. If they have not, a ConsentNotGiven error is",
            "        raised.",
            "",
            "        Args:",
            "            requester: The user making the request",
            "",
            "        Returns:",
            "            Returns normally if the user has consented or is exempt",
            "",
            "        Raises:",
            "            ConsentNotGivenError: if the user has not given consent yet",
            "        \"\"\"",
            "        if self._block_events_without_consent_error is None:",
            "            return",
            "",
            "        # exempt AS users from needing consent",
            "        if requester.app_service is not None:",
            "            return",
            "",
            "        user_id = requester.authenticated_entity",
            "        if not user_id.startswith(\"@\"):",
            "            # The authenticated entity might not be a user, e.g. if it's the",
            "            # server puppetting the user.",
            "            return",
            "",
            "        user = UserID.from_string(user_id)",
            "",
            "        # exempt the system notices user",
            "        if (",
            "            self.config.servernotices.server_notices_mxid is not None",
            "            and user_id == self.config.servernotices.server_notices_mxid",
            "        ):",
            "            return",
            "",
            "        u = await self.store.get_user_by_id(user_id)",
            "        assert u is not None",
            "        if u.user_type in (UserTypes.SUPPORT, UserTypes.BOT):",
            "            # support and bot users are not required to consent",
            "            return",
            "        if u.appservice_id is not None:",
            "            # users registered by an appservice are exempt",
            "            return",
            "        if u.consent_version == self.config.consent.user_consent_version:",
            "            return",
            "",
            "        consent_uri = self._consent_uri_builder.build_user_consent_uri(user.localpart)",
            "        msg = self._block_events_without_consent_error % {\"consent_uri\": consent_uri}",
            "        raise ConsentNotGivenError(msg=msg, consent_uri=consent_uri)",
            "",
            "    async def deduplicate_state_event(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"",
            "        Checks whether event is in the latest resolved state in context.",
            "",
            "        Args:",
            "            event: The event to check for duplication.",
            "            context: The event context.",
            "",
            "        Returns:",
            "            The previous version of the event is returned, if it is found in the",
            "            event context. Otherwise, None is returned.",
            "        \"\"\"",
            "        if event.internal_metadata.is_outlier():",
            "            # This can happen due to out of band memberships",
            "            return None",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(event.type, event.state_key)])",
            "        )",
            "        prev_event_id = prev_state_ids.get((event.type, event.state_key))",
            "        if not prev_event_id:",
            "            return None",
            "        prev_event = await self.store.get_event(prev_event_id, allow_none=True)",
            "        if not prev_event:",
            "            return None",
            "",
            "        if prev_event and event.user_id == prev_event.user_id:",
            "            prev_content = encode_canonical_json(prev_event.content)",
            "            next_content = encode_canonical_json(event.content)",
            "            if prev_content == next_content:",
            "                return prev_event",
            "        return None",
            "",
            "    async def get_event_id_from_transaction(",
            "        self,",
            "        requester: Requester,",
            "        txn_id: str,",
            "        room_id: str,",
            "    ) -> Optional[str]:",
            "        \"\"\"For the given transaction ID and room ID, check if there is a matching event ID.",
            "",
            "        Args:",
            "            requester: The requester making the request in the context of which we want",
            "                to fetch the event.",
            "            txn_id: The transaction ID.",
            "            room_id: The room ID.",
            "",
            "        Returns:",
            "            An event ID if one could be found, None otherwise.",
            "        \"\"\"",
            "        existing_event_id = None",
            "",
            "        # According to the spec, transactions are scoped to a user's device ID.",
            "        if requester.device_id:",
            "            existing_event_id = (",
            "                await self.store.get_event_id_from_transaction_id_and_device_id(",
            "                    room_id,",
            "                    requester.user.to_string(),",
            "                    requester.device_id,",
            "                    txn_id,",
            "                )",
            "            )",
            "            if existing_event_id:",
            "                return existing_event_id",
            "",
            "        return existing_event_id",
            "",
            "    async def get_event_from_transaction(",
            "        self,",
            "        requester: Requester,",
            "        txn_id: str,",
            "        room_id: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"For the given transaction ID and room ID, check if there is a matching event.",
            "        If so, fetch it and return it.",
            "",
            "        Args:",
            "            requester: The requester making the request in the context of which we want",
            "                to fetch the event.",
            "            txn_id: The transaction ID.",
            "            room_id: The room ID.",
            "",
            "        Returns:",
            "            An event if one could be found, None otherwise.",
            "        \"\"\"",
            "        existing_event_id = await self.get_event_id_from_transaction(",
            "            requester, txn_id, room_id",
            "        )",
            "        if existing_event_id:",
            "            return await self.store.get_event(existing_event_id)",
            "        return None",
            "",
            "    async def create_and_send_nonmember_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        ratelimit: bool = True,",
            "        txn_id: Optional[str] = None,",
            "        ignore_shadow_ban: bool = False,",
            "        outlier: bool = False,",
            "        depth: Optional[int] = None,",
            "    ) -> Tuple[EventBase, int]:",
            "        \"\"\"",
            "        Creates an event, then sends it.",
            "",
            "        See self.create_event and self.handle_new_client_event.",
            "",
            "        Args:",
            "            requester: The requester sending the event.",
            "            event_dict: An entire event.",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases (previously useful for MSC2716).",
            "            prev_event_ids:",
            "                The event IDs to use as the prev events.",
            "                Should normally be left as None to automatically request them",
            "                from the database.",
            "            state_event_ids:",
            "                The full state at a given event. This was previously used particularly",
            "                by the MSC2716 /batch_send endpoint. This should normally be left as",
            "                None, which will cause the auth_event_ids to be calculated based on the",
            "                room state at the prev_events.",
            "            ratelimit: Whether to rate limit this send.",
            "            txn_id: The transaction ID.",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "        Returns:",
            "            The event, and its stream ordering (if deduplication happened,",
            "            the previous, duplicate event).",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "        \"\"\"",
            "",
            "        if event_dict[\"type\"] == EventTypes.Member:",
            "            raise SynapseError(",
            "                500, \"Tried to send member event through non-member codepath\"",
            "            )",
            "",
            "        if not ignore_shadow_ban and requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        if ratelimit:",
            "            await self.request_ratelimiter.ratelimit(requester, update=False)",
            "",
            "        # We limit the number of concurrent event sends in a room so that we",
            "        # don't fork the DAG too much. If we don't limit then we can end up in",
            "        # a situation where event persistence can't keep up, causing",
            "        # extremities to pile up, which in turn leads to state resolution",
            "        # taking longer.",
            "        room_id = event_dict[\"room_id\"]",
            "        async with self.limiter.queue(room_id):",
            "            if txn_id:",
            "                event = await self.get_event_from_transaction(",
            "                    requester, txn_id, room_id",
            "                )",
            "                if event:",
            "                    # we know it was persisted, so must have a stream ordering",
            "                    assert event.internal_metadata.stream_ordering",
            "                    return (",
            "                        event,",
            "                        event.internal_metadata.stream_ordering,",
            "                    )",
            "",
            "        async with self._worker_lock_handler.acquire_read_write_lock(",
            "            NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "        ):",
            "            return await self._create_and_send_nonmember_event_locked(",
            "                requester=requester,",
            "                event_dict=event_dict,",
            "                allow_no_prev_events=allow_no_prev_events,",
            "                prev_event_ids=prev_event_ids,",
            "                state_event_ids=state_event_ids,",
            "                ratelimit=ratelimit,",
            "                txn_id=txn_id,",
            "                ignore_shadow_ban=ignore_shadow_ban,",
            "                outlier=outlier,",
            "                depth=depth,",
            "            )",
            "",
            "    async def _create_and_send_nonmember_event_locked(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        ratelimit: bool = True,",
            "        txn_id: Optional[str] = None,",
            "        ignore_shadow_ban: bool = False,",
            "        outlier: bool = False,",
            "        depth: Optional[int] = None,",
            "    ) -> Tuple[EventBase, int]:",
            "        room_id = event_dict[\"room_id\"]",
            "",
            "        # If we don't have any prev event IDs specified then we need to",
            "        # check that the host is in the room (as otherwise populating the",
            "        # prev events will fail), at which point we may as well check the",
            "        # local user is in the room.",
            "        if not prev_event_ids:",
            "            user_id = requester.user.to_string()",
            "            is_user_in_room = await self.store.check_local_user_in_room(",
            "                user_id, room_id",
            "            )",
            "            if not is_user_in_room:",
            "                raise AuthError(403, f\"User {user_id} not in room {room_id}\")",
            "",
            "        # Try several times, it could fail with PartialStateConflictError",
            "        # in handle_new_client_event, cf comment in except block.",
            "        max_retries = 5",
            "        for i in range(max_retries):",
            "            try:",
            "                event, unpersisted_context = await self.create_event(",
            "                    requester,",
            "                    event_dict,",
            "                    txn_id=txn_id,",
            "                    allow_no_prev_events=allow_no_prev_events,",
            "                    prev_event_ids=prev_event_ids,",
            "                    state_event_ids=state_event_ids,",
            "                    outlier=outlier,",
            "                    depth=depth,",
            "                )",
            "                context = await unpersisted_context.persist(event)",
            "",
            "                assert self.hs.is_mine_id(event.sender), \"User must be our own: %s\" % (",
            "                    event.sender,",
            "                )",
            "",
            "                spam_check_result = (",
            "                    await self._spam_checker_module_callbacks.check_event_for_spam(",
            "                        event",
            "                    )",
            "                )",
            "                if spam_check_result != self._spam_checker_module_callbacks.NOT_SPAM:",
            "                    if isinstance(spam_check_result, tuple):",
            "                        try:",
            "                            [code, dict] = spam_check_result",
            "                            raise SynapseError(",
            "                                403,",
            "                                \"This message had been rejected as probable spam\",",
            "                                code,",
            "                                dict,",
            "                            )",
            "                        except ValueError:",
            "                            logger.error(",
            "                                \"Spam-check module returned invalid error value. Expecting [code, dict], got %s\",",
            "                                spam_check_result,",
            "                            )",
            "",
            "                            raise SynapseError(",
            "                                403,",
            "                                \"This message has been rejected as probable spam\",",
            "                                Codes.FORBIDDEN,",
            "                            )",
            "",
            "                    # Backwards compatibility: if the return value is not an error code, it",
            "                    # means the module returned an error message to be included in the",
            "                    # SynapseError (which is now deprecated).",
            "                    raise SynapseError(",
            "                        403,",
            "                        spam_check_result,",
            "                        Codes.FORBIDDEN,",
            "                    )",
            "",
            "                ev = await self.handle_new_client_event(",
            "                    requester=requester,",
            "                    events_and_context=[(event, context)],",
            "                    ratelimit=ratelimit,",
            "                    ignore_shadow_ban=ignore_shadow_ban,",
            "                )",
            "",
            "                break",
            "            except PartialStateConflictError as e:",
            "                # Persisting couldn't happen because the room got un-partial stated",
            "                # in the meantime and context needs to be recomputed, so let's do so.",
            "                if i == max_retries - 1:",
            "                    raise e",
            "                pass",
            "",
            "        # we know it was persisted, so must have a stream ordering",
            "        assert ev.internal_metadata.stream_ordering",
            "        return ev, ev.internal_metadata.stream_ordering",
            "",
            "    @measure_func(\"create_new_client_event\")",
            "    async def create_new_client_event(",
            "        self,",
            "        builder: EventBuilder,",
            "        requester: Optional[Requester] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        \"\"\"Create a new event for a local client. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events. Note that this returns an UnpersistedEventContext,",
            "        which must be converted to an EventContext before it can be sent to the DB.",
            "",
            "        Args:",
            "            builder:",
            "            requester:",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases (previously useful for MSC2716).",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This was previously used particularly",
            "                by the MSC2716 /batch_send endpoint. This should normally be left as",
            "                None, which will cause the auth_event_ids to be calculated based on the",
            "                room state at the prev_events.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Returns:",
            "            Tuple of created event, UnpersistedEventContext",
            "        \"\"\"",
            "        # Strip down the state_event_ids to only what we need to auth the event.",
            "        # For example, we don't need extra m.room.member that don't match event.sender",
            "        if state_event_ids is not None:",
            "            # Do a quick check to make sure that prev_event_ids is present to",
            "            # make the type-checking around `builder.build` happy.",
            "            # prev_event_ids could be an empty array though.",
            "            assert prev_event_ids is not None",
            "",
            "            temp_event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=state_event_ids,",
            "                depth=depth,",
            "            )",
            "            state_events = await self.store.get_events_as_list(state_event_ids)",
            "            # Create a StateMap[str]",
            "            current_state_ids = {",
            "                (e.type, e.state_key): e.event_id for e in state_events",
            "            }",
            "            # Actually strip down and only use the necessary auth events",
            "            auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "                event=temp_event,",
            "                current_state_ids=current_state_ids,",
            "                for_verification=False,",
            "            )",
            "",
            "        if prev_event_ids is not None:",
            "            assert (",
            "                len(prev_event_ids) <= 10",
            "            ), \"Attempting to create an event with %i prev_events\" % (",
            "                len(prev_event_ids),",
            "            )",
            "        else:",
            "            prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        # Do a quick sanity check here, rather than waiting until we've created the",
            "        # event and then try to auth it (which fails with a somewhat confusing \"No",
            "        # create event in auth events\")",
            "        if allow_no_prev_events:",
            "            # We allow events with no `prev_events` but it better have some `auth_events`",
            "            assert (",
            "                builder.type == EventTypes.Create",
            "                # Allow an event to have empty list of prev_event_ids",
            "                # only if it has auth_event_ids.",
            "                or auth_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events or auth_event_ids\"",
            "        else:",
            "            # we now ought to have some prev_events (unless it's a create event).",
            "            assert (",
            "                builder.type == EventTypes.Create or prev_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events\"",
            "",
            "        if for_batch:",
            "            assert prev_event_ids is not None",
            "            assert state_map is not None",
            "            auth_ids = self._event_auth_handler.compute_auth_events(builder, state_map)",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids, auth_event_ids=auth_ids, depth=depth",
            "            )",
            "",
            "            context: UnpersistedEventContextBase = (",
            "                await self.state.calculate_context_info(",
            "                    event,",
            "                    state_ids_before_event=state_map,",
            "                    partial_state=False,",
            "                    state_group_before_event=current_state_group,",
            "                )",
            "            )",
            "",
            "        else:",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=auth_event_ids,",
            "                depth=depth,",
            "            )",
            "",
            "            # Pass on the outlier property from the builder to the event",
            "            # after it is created",
            "            if builder.internal_metadata.outlier:",
            "                event.internal_metadata.outlier = True",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "            else:",
            "                context = await self.state.calculate_context_info(event)",
            "",
            "        if requester:",
            "            context.app_service = requester.app_service",
            "",
            "        res, new_content = await self._third_party_event_rules.check_event_allowed(",
            "            event, context",
            "        )",
            "        if res is False:",
            "            logger.info(",
            "                \"Event %s forbidden by third-party rules\",",
            "                event,",
            "            )",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "        elif new_content is not None:",
            "            # the third-party rules want to replace the event. We'll need to build a new",
            "            # event.",
            "            event, context = await self._rebuild_event_after_third_party_rules(",
            "                new_content, event",
            "            )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "        await self._validate_event_relation(event)",
            "        logger.debug(\"Created event %s\", event.event_id)",
            "",
            "        return event, context",
            "",
            "    async def _validate_event_relation(self, event: EventBase) -> None:",
            "        \"\"\"",
            "        Ensure the relation data on a new event is not bogus.",
            "",
            "        Args:",
            "            event: The event being created.",
            "",
            "        Raises:",
            "            SynapseError if the event is invalid.",
            "        \"\"\"",
            "",
            "        relation = relation_from_event(event)",
            "        if not relation:",
            "            return",
            "",
            "        parent_event = await self.store.get_event(relation.parent_id, allow_none=True)",
            "        if parent_event:",
            "            # And in the same room.",
            "            if parent_event.room_id != event.room_id:",
            "                raise SynapseError(400, \"Relations must be in the same room\")",
            "",
            "        else:",
            "            # There must be some reason that the client knows the event exists,",
            "            # see if there are existing relations. If so, assume everything is fine.",
            "            if not await self.store.event_is_target_of_relation(relation.parent_id):",
            "                # Otherwise, the client can't know about the parent event!",
            "                raise SynapseError(400, \"Can't send relation to unknown event\")",
            "",
            "        # If this event is an annotation then we check that that the sender",
            "        # can't annotate the same way twice (e.g. stops users from liking an",
            "        # event multiple times).",
            "        if relation.rel_type == RelationTypes.ANNOTATION:",
            "            aggregation_key = relation.aggregation_key",
            "",
            "            if aggregation_key is None:",
            "                raise SynapseError(400, \"Missing aggregation key\")",
            "",
            "            if len(aggregation_key) > 500:",
            "                raise SynapseError(400, \"Aggregation key is too long\")",
            "",
            "            already_exists = await self.store.has_user_annotated_event(",
            "                relation.parent_id, event.type, aggregation_key, event.sender",
            "            )",
            "            if already_exists:",
            "                raise SynapseError(",
            "                    400,",
            "                    \"Can't send same reaction twice\",",
            "                    errcode=Codes.DUPLICATE_ANNOTATION,",
            "                )",
            "",
            "        # Don't attempt to start a thread if the parent event is a relation.",
            "        elif relation.rel_type == RelationTypes.THREAD:",
            "            if await self.store.event_includes_relation(relation.parent_id):",
            "                raise SynapseError(",
            "                    400, \"Cannot start threads from an event with a relation\"",
            "                )",
            "",
            "    @measure_func(\"handle_new_client_event\")",
            "    async def handle_new_client_event(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "        ignore_shadow_ban: bool = False,",
            "    ) -> EventBase:",
            "        \"\"\"Processes new events. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        This includes deduplicating, checking auth, persisting,",
            "        notifying users, sending to remote servers, etc.",
            "",
            "        If called from a worker will hit out to the master process for final",
            "        processing.",
            "",
            "        Args:",
            "            requester",
            "            events_and_context: A list of one or more tuples of event, context to be persisted",
            "            ratelimit",
            "            extra_users: Any extra users to notify about event",
            "",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "",
            "        Return:",
            "            If the event was deduplicated, the previous, duplicate, event. Otherwise,",
            "            `event`.",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "            PartialStateConflictError if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            # we don't apply shadow-banning to membership events here. Invites are blocked",
            "            # higher up the stack, and we allow shadow-banned users to send join and leave",
            "            # events as normal.",
            "            if (",
            "                event.type != EventTypes.Member",
            "                and not ignore_shadow_ban",
            "                and requester.shadow_banned",
            "            ):",
            "                # We randomly sleep a bit just to annoy the requester.",
            "                await self.clock.sleep(random.randint(1, 10))",
            "                raise ShadowBanError()",
            "",
            "            if event.is_state():",
            "                prev_event = await self.deduplicate_state_event(event, context)",
            "                if prev_event is not None:",
            "                    logger.info(",
            "                        \"Not bothering to persist state event %s duplicated by %s\",",
            "                        event.event_id,",
            "                        prev_event.event_id,",
            "                    )",
            "                    return prev_event",
            "",
            "            if event.internal_metadata.is_out_of_band_membership():",
            "                # the only sort of out-of-band-membership events we expect to see here are",
            "                # invite rejections and rescinded knocks that we have generated ourselves.",
            "                assert event.type == EventTypes.Member",
            "                assert event.content[\"membership\"] == Membership.LEAVE",
            "            else:",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    # If we are persisting a batch of events the event(s) needed to auth the",
            "                    # current event may be part of the batch and will not be in the DB yet",
            "                    event_id_to_event = {e.event_id: e for e, _ in events_and_context}",
            "                    batched_auth_events = {}",
            "                    for event_id in event.auth_event_ids():",
            "                        auth_event = event_id_to_event.get(event_id)",
            "                        if auth_event:",
            "                            batched_auth_events[event_id] = auth_event",
            "                    await self._event_auth_handler.check_auth_rules_from_context(",
            "                        event, batched_auth_events",
            "                    )",
            "                except AuthError as err:",
            "                    logger.warning(\"Denying new event %r because %s\", event, err)",
            "                    raise err",
            "",
            "            # Ensure that we can round trip before trying to persist in db",
            "            try:",
            "                dump = json_encoder.encode(event.content)",
            "                json_decoder.decode(dump)",
            "            except Exception:",
            "                logger.exception(\"Failed to encode content: %r\", event.content)",
            "                raise",
            "",
            "        # We now persist the event (and update the cache in parallel, since we",
            "        # don't want to block on it).",
            "        #",
            "        # Note: mypy gets confused if we inline dl and check with twisted#11770.",
            "        # Some kind of bug in mypy's deduction?",
            "        deferreds = (",
            "            run_in_background(",
            "                self._persist_events,",
            "                requester=requester,",
            "                events_and_context=events_and_context,",
            "                ratelimit=ratelimit,",
            "                extra_users=extra_users,",
            "            ),",
            "            run_in_background(",
            "                self.cache_joined_hosts_for_events, events_and_context",
            "            ).addErrback(log_failure, \"cache_joined_hosts_for_event failed\"),",
            "        )",
            "        result, _ = await make_deferred_yieldable(",
            "            gather_results(deferreds, consumeErrors=True)",
            "        ).addErrback(unwrapFirstError)",
            "",
            "        return result",
            "",
            "    async def _persist_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Actually persists new events. Should only be called by",
            "        `handle_new_client_event`, and see its docstring for documentation of",
            "        the arguments. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "",
            "        await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "            events_and_context",
            "        )",
            "",
            "        try:",
            "            # If we're a worker we need to hit out to the master.",
            "            first_event, _ = events_and_context[0]",
            "            writer_instance = self._events_shard_config.get_instance(",
            "                first_event.room_id",
            "            )",
            "            if writer_instance != self._instance_name:",
            "                try:",
            "                    result = await self.send_events(",
            "                        instance_name=writer_instance,",
            "                        events_and_context=events_and_context,",
            "                        store=self.store,",
            "                        requester=requester,",
            "                        ratelimit=ratelimit,",
            "                        extra_users=extra_users,",
            "                    )",
            "                except SynapseError as e:",
            "                    if e.code == HTTPStatus.CONFLICT:",
            "                        raise PartialStateConflictError()",
            "                    raise",
            "                stream_id = result[\"stream_id\"]",
            "                event_id = result[\"event_id\"]",
            "",
            "                # If we batch persisted events we return the last persisted event, otherwise",
            "                # we return the one event that was persisted",
            "                event, _ = events_and_context[-1]",
            "",
            "                if event_id != event.event_id:",
            "                    # If we get a different event back then it means that its",
            "                    # been de-duplicated, so we replace the given event with the",
            "                    # one already persisted.",
            "                    event = await self.store.get_event(event_id)",
            "                else:",
            "                    # If we newly persisted the event then we need to update its",
            "                    # stream_ordering entry manually (as it was persisted on",
            "                    # another worker).",
            "                    event.internal_metadata.stream_ordering = stream_id",
            "                return event",
            "",
            "            event = await self.persist_and_notify_client_events(",
            "                requester,",
            "                events_and_context,",
            "                ratelimit=ratelimit,",
            "                extra_users=extra_users,",
            "            )",
            "",
            "            return event",
            "        except Exception:",
            "            for event, _ in events_and_context:",
            "                # Ensure that we actually remove the entries in the push actions",
            "                # staging area, if we calculated them.",
            "                await self.store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def cache_joined_hosts_for_events(",
            "        self, events_and_context: List[Tuple[EventBase, EventContext]]",
            "    ) -> None:",
            "        \"\"\"Precalculate the joined hosts at each of the given events, when using Redis, so that",
            "        external federation senders don't have to recalculate it themselves.",
            "        \"\"\"",
            "",
            "        if not self._external_cache.is_enabled():",
            "            return",
            "",
            "        # If external cache is enabled we should always have this.",
            "        assert self._external_cache_joined_hosts_updates is not None",
            "",
            "        for event, event_context in events_and_context:",
            "            if event_context.partial_state:",
            "                # To populate the cache for a partial-state event, we either have to",
            "                # block until full state, which the code below does, or change the",
            "                # meaning of cache values to be the list of hosts to which we plan to",
            "                # send events and calculate that instead.",
            "                #",
            "                # The federation senders don't use the external cache when sending",
            "                # events in partial-state rooms anyway, so let's not bother populating",
            "                # the cache.",
            "                continue",
            "",
            "            # We actually store two mappings, event ID -> prev state group,",
            "            # state group -> joined hosts, which is much more space efficient",
            "            # than event ID -> joined hosts.",
            "            #",
            "            # Note: We have to cache event ID -> prev state group, as we don't",
            "            # store that in the DB.",
            "            #",
            "            # Note: We set the state group -> joined hosts cache if it hasn't been",
            "            # set for a while, so that the expiry time is reset.",
            "",
            "            state_entry = await self.state.resolve_state_groups_for_events(",
            "                event.room_id, event_ids=event.prev_event_ids()",
            "            )",
            "",
            "            if state_entry.state_group:",
            "                await self._external_cache.set(",
            "                    \"event_to_prev_state_group\",",
            "                    event.event_id,",
            "                    state_entry.state_group,",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                if state_entry.state_group in self._external_cache_joined_hosts_updates:",
            "                    return",
            "",
            "                with opentracing.start_active_span(\"get_joined_hosts\"):",
            "                    joined_hosts = (",
            "                        await self._storage_controllers.state.get_joined_hosts(",
            "                            event.room_id, state_entry",
            "                        )",
            "                    )",
            "",
            "                # Note that the expiry times must be larger than the expiry time in",
            "                # _external_cache_joined_hosts_updates.",
            "                await self._external_cache.set(",
            "                    \"get_joined_hosts\",",
            "                    str(state_entry.state_group),",
            "                    list(joined_hosts),",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                self._external_cache_joined_hosts_updates[",
            "                    state_entry.state_group",
            "                ] = None",
            "",
            "    async def _validate_canonical_alias(",
            "        self,",
            "        directory_handler: DirectoryHandler,",
            "        room_alias_str: str,",
            "        expected_room_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Ensure that the given room alias points to the expected room ID.",
            "",
            "        Args:",
            "            directory_handler: The directory handler object.",
            "            room_alias_str: The room alias to check.",
            "            expected_room_id: The room ID that the alias should point to.",
            "        \"\"\"",
            "        room_alias = RoomAlias.from_string(room_alias_str)",
            "        try:",
            "            mapping = await directory_handler.get_association(room_alias)",
            "        except SynapseError as e:",
            "            # Turn M_NOT_FOUND errors into M_BAD_ALIAS errors.",
            "            if e.errcode == Codes.NOT_FOUND:",
            "                raise SynapseError(",
            "                    400,",
            "                    \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                    Codes.BAD_ALIAS,",
            "                )",
            "            raise",
            "",
            "        if mapping[\"room_id\"] != expected_room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                Codes.BAD_ALIAS,",
            "            )",
            "",
            "    async def persist_and_notify_client_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Called when we have fully built the events, have already",
            "        calculated the push actions for the events, and checked auth.",
            "",
            "        This should only be run on the instance in charge of persisting events.",
            "",
            "        Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        Returns:",
            "            The persisted event, if one event is passed in, or the last event in the",
            "            list in the case of batch persisting. If only one event was persisted, the",
            "            returned event may be different than the given event if it was de-duplicated",
            "            (e.g. because we had already persisted an event with the same transaction ID.)",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            assert self._events_shard_config.should_handle(",
            "                self._instance_name, event.room_id",
            "            )",
            "",
            "            if ratelimit:",
            "                # We check if this is a room admin redacting an event so that we",
            "                # can apply different ratelimiting. We do this by simply checking",
            "                # it's not a self-redaction (to avoid having to look up whether the",
            "                # user is actually admin or not).",
            "                is_admin_redaction = False",
            "                if event.type == EventTypes.Redaction:",
            "                    assert event.redacts is not None",
            "",
            "                    original_event = await self.store.get_event(",
            "                        event.redacts,",
            "                        redact_behaviour=EventRedactBehaviour.as_is,",
            "                        get_prev_content=False,",
            "                        allow_rejected=False,",
            "                        allow_none=True,",
            "                    )",
            "",
            "                    is_admin_redaction = bool(",
            "                        original_event and event.sender != original_event.sender",
            "                    )",
            "",
            "                await self.request_ratelimiter.ratelimit(",
            "                    requester, is_admin_redaction=is_admin_redaction",
            "                )",
            "",
            "            # run checks/actions on event based on type",
            "            if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "                (",
            "                    current_membership,",
            "                    _,",
            "                ) = await self.store.get_local_current_membership_for_user_in_room(",
            "                    event.state_key, event.room_id",
            "                )",
            "                if current_membership != Membership.JOIN:",
            "                    self._notifier.notify_user_joined_room(",
            "                        event.event_id, event.room_id",
            "                    )",
            "",
            "            await self._maybe_kick_guest_users(event, context)",
            "",
            "            if event.type == EventTypes.CanonicalAlias:",
            "                # Validate a newly added alias or newly added alt_aliases.",
            "",
            "                original_alias = None",
            "                original_alt_aliases: object = []",
            "",
            "                original_event_id = event.unsigned.get(\"replaces_state\")",
            "                if original_event_id:",
            "                    original_alias_event = await self.store.get_event(original_event_id)",
            "",
            "                    if original_alias_event:",
            "                        original_alias = original_alias_event.content.get(\"alias\", None)",
            "                        original_alt_aliases = original_alias_event.content.get(",
            "                            \"alt_aliases\", []",
            "                        )",
            "",
            "                # Check the alias is currently valid (if it has changed).",
            "                room_alias_str = event.content.get(\"alias\", None)",
            "                directory_handler = self.hs.get_directory_handler()",
            "                if room_alias_str and room_alias_str != original_alias:",
            "                    await self._validate_canonical_alias(",
            "                        directory_handler, room_alias_str, event.room_id",
            "                    )",
            "",
            "                # Check that alt_aliases is the proper form.",
            "                alt_aliases = event.content.get(\"alt_aliases\", [])",
            "                if not isinstance(alt_aliases, (list, tuple)):",
            "                    raise SynapseError(",
            "                        400,",
            "                        \"The alt_aliases property must be a list.\",",
            "                        Codes.INVALID_PARAM,",
            "                    )",
            "",
            "                # If the old version of alt_aliases is of an unknown form,",
            "                # completely replace it.",
            "                if not isinstance(original_alt_aliases, (list, tuple)):",
            "                    # TODO: check that the original_alt_aliases' entries are all strings",
            "                    original_alt_aliases = []",
            "",
            "                # Check that each alias is currently valid.",
            "                new_alt_aliases = set(alt_aliases) - set(original_alt_aliases)",
            "                if new_alt_aliases:",
            "                    for alias_str in new_alt_aliases:",
            "                        await self._validate_canonical_alias(",
            "                            directory_handler, alias_str, event.room_id",
            "                        )",
            "",
            "            federation_handler = self.hs.get_federation_handler()",
            "",
            "            if event.type == EventTypes.Member:",
            "                if event.content[\"membership\"] == Membership.INVITE:",
            "                    maybe_upsert_event_field(",
            "                        event,",
            "                        event.unsigned,",
            "                        \"invite_room_state\",",
            "                        await self.store.get_stripped_room_state_from_event_context(",
            "                            context,",
            "                            self.room_prejoin_state_types,",
            "                            membership_user_id=event.sender,",
            "                        ),",
            "                    )",
            "",
            "                    invitee = UserID.from_string(event.state_key)",
            "                    if not self.hs.is_mine(invitee):",
            "                        # TODO: Can we add signature from remote server in a nicer",
            "                        # way? If we have been invited by a remote server, we need",
            "                        # to get them to sign the event.",
            "",
            "                        returned_invite = await federation_handler.send_invite(",
            "                            invitee.domain, event",
            "                        )",
            "                        event.unsigned.pop(\"room_state\", None)",
            "",
            "                        # TODO: Make sure the signatures actually are correct.",
            "                        event.signatures.update(returned_invite.signatures)",
            "",
            "                if event.content[\"membership\"] == Membership.KNOCK:",
            "                    maybe_upsert_event_field(",
            "                        event,",
            "                        event.unsigned,",
            "                        \"knock_room_state\",",
            "                        await self.store.get_stripped_room_state_from_event_context(",
            "                            context,",
            "                            self.room_prejoin_state_types,",
            "                        ),",
            "                    )",
            "",
            "            if event.type == EventTypes.Redaction:",
            "                assert event.redacts is not None",
            "",
            "                original_event = await self.store.get_event(",
            "                    event.redacts,",
            "                    redact_behaviour=EventRedactBehaviour.as_is,",
            "                    get_prev_content=False,",
            "                    allow_rejected=False,",
            "                    allow_none=True,",
            "                )",
            "",
            "                room_version = await self.store.get_room_version_id(event.room_id)",
            "                room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "                # we can make some additional checks now if we have the original event.",
            "                if original_event:",
            "                    if original_event.type == EventTypes.Create:",
            "                        raise AuthError(403, \"Redacting create events is not permitted\")",
            "",
            "                    if original_event.room_id != event.room_id:",
            "                        raise SynapseError(",
            "                            400, \"Cannot redact event from a different room\"",
            "                        )",
            "",
            "                    if original_event.type == EventTypes.ServerACL:",
            "                        raise AuthError(",
            "                            403, \"Redacting server ACL events is not permitted\"",
            "                        )",
            "",
            "                event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "                prev_state_ids = await context.get_prev_state_ids(",
            "                    StateFilter.from_types(event_types)",
            "                )",
            "",
            "                auth_events_ids = self._event_auth_handler.compute_auth_events(",
            "                    event, prev_state_ids, for_verification=True",
            "                )",
            "                auth_events_map = await self.store.get_events(auth_events_ids)",
            "                auth_events = {",
            "                    (e.type, e.state_key): e for e in auth_events_map.values()",
            "                }",
            "",
            "                if event_auth.check_redaction(",
            "                    room_version_obj, event, auth_events=auth_events",
            "                ):",
            "                    # this user doesn't have 'redact' rights, so we need to do some more",
            "                    # checks on the original event. Let's start by checking the original",
            "                    # event exists.",
            "                    if not original_event:",
            "                        raise NotFoundError(",
            "                            \"Could not find event %s\" % (event.redacts,)",
            "                        )",
            "",
            "                    if event.user_id != original_event.user_id:",
            "                        raise AuthError(",
            "                            403, \"You don't have permission to redact events\"",
            "                        )",
            "",
            "                    # all the checks are done.",
            "                    event.internal_metadata.recheck_redaction = False",
            "",
            "            if event.type == EventTypes.Create:",
            "                prev_state_ids = await context.get_prev_state_ids()",
            "                if prev_state_ids:",
            "                    raise AuthError(403, \"Changing the room create event is forbidden\")",
            "",
            "        assert self._storage_controllers.persistence is not None",
            "        (",
            "            persisted_events,",
            "            max_stream_token,",
            "        ) = await self._storage_controllers.persistence.persist_events(",
            "            events_and_context,",
            "        )",
            "",
            "        events_and_pos = []",
            "        for event in persisted_events:",
            "            if self._ephemeral_events_enabled:",
            "                # If there's an expiry timestamp on the event, schedule its expiry.",
            "                self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            stream_ordering = event.internal_metadata.stream_ordering",
            "            assert stream_ordering is not None",
            "            pos = PersistedEventPosition(self._instance_name, stream_ordering)",
            "            events_and_pos.append((event, pos))",
            "",
            "            if event.type == EventTypes.Message:",
            "                # We don't want to block sending messages on any presence code. This",
            "                # matters as sometimes presence code can take a while.",
            "                run_as_background_process(",
            "                    \"bump_presence_active_time\",",
            "                    self._bump_active_time,",
            "                    requester.user,",
            "                    requester.device_id,",
            "                )",
            "",
            "        async def _notify() -> None:",
            "            try:",
            "                await self.notifier.on_new_room_events(",
            "                    events_and_pos, max_stream_token, extra_users=extra_users",
            "                )",
            "            except Exception:",
            "                logger.exception(\"Error notifying about new room events\")",
            "",
            "        run_in_background(_notify)",
            "",
            "        return persisted_events[-1]",
            "",
            "    async def _maybe_kick_guest_users(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state_ids = await context.get_current_state_ids()",
            "",
            "        # since this is a client-generated event, it cannot be an outlier and we must",
            "        # therefore have the state ids.",
            "        assert current_state_ids is not None",
            "        current_state_dict = await self.store.get_events(",
            "            list(current_state_ids.values())",
            "        )",
            "        current_state = list(current_state_dict.values())",
            "        logger.info(\"maybe_kick_guest_users %r\", current_state)",
            "        await self.hs.get_room_member_handler().kick_guest_users(current_state)",
            "",
            "    async def _bump_active_time(self, user: UserID, device_id: Optional[str]) -> None:",
            "        try:",
            "            presence = self.hs.get_presence_handler()",
            "            await presence.bump_presence_active_time(user, device_id)",
            "        except Exception:",
            "            logger.exception(\"Error bumping presence active time\")",
            "",
            "    async def _send_dummy_events_to_fill_extremities(self) -> None:",
            "        \"\"\"Background task to send dummy events into rooms that have a large",
            "        number of extremities",
            "        \"\"\"",
            "        self._expire_rooms_to_exclude_from_dummy_event_insertion()",
            "        room_ids = await self.store.get_rooms_with_many_extremities(",
            "            min_count=self._dummy_events_threshold,",
            "            limit=5,",
            "            room_id_filter=self._rooms_to_exclude_from_dummy_event_insertion.keys(),",
            "        )",
            "",
            "        for room_id in room_ids:",
            "            async with self._worker_lock_handler.acquire_read_write_lock(",
            "                NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "            ):",
            "                dummy_event_sent = await self._send_dummy_event_for_room(room_id)",
            "",
            "            if not dummy_event_sent:",
            "                # Did not find a valid user in the room, so remove from future attempts",
            "                # Exclusion is time limited, so the room will be rechecked in the future",
            "                # dependent on _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s. Will exclude it from \"",
            "                    \"future attempts until cache expires\" % (room_id,)",
            "                )",
            "                now = self.clock.time_msec()",
            "                self._rooms_to_exclude_from_dummy_event_insertion[room_id] = now",
            "",
            "    async def _send_dummy_event_for_room(self, room_id: str) -> bool:",
            "        \"\"\"Attempt to send a dummy event for the given room.",
            "",
            "        Args:",
            "            room_id: room to try to send an event from",
            "",
            "        Returns:",
            "            True if a dummy event was successfully sent. False if no user was able",
            "            to send an event.",
            "        \"\"\"",
            "",
            "        # For each room we need to find a joined member we can use to send",
            "        # the dummy event with.",
            "        members = await self.store.get_local_users_in_room(room_id)",
            "        for user_id in members:",
            "            requester = create_requester(user_id, authenticated_entity=self.server_name)",
            "            try:",
            "                # Try several times, it could fail with PartialStateConflictError",
            "                # in handle_new_client_event, cf comment in except block.",
            "                max_retries = 5",
            "                for i in range(max_retries):",
            "                    try:",
            "                        event, unpersisted_context = await self.create_event(",
            "                            requester,",
            "                            {",
            "                                \"type\": EventTypes.Dummy,",
            "                                \"content\": {},",
            "                                \"room_id\": room_id,",
            "                                \"sender\": user_id,",
            "                            },",
            "                        )",
            "                        context = await unpersisted_context.persist(event)",
            "",
            "                        event.internal_metadata.proactively_send = False",
            "",
            "                        # Since this is a dummy-event it is OK if it is sent by a",
            "                        # shadow-banned user.",
            "                        await self.handle_new_client_event(",
            "                            requester,",
            "                            events_and_context=[(event, context)],",
            "                            ratelimit=False,",
            "                            ignore_shadow_ban=True,",
            "                        )",
            "",
            "                        break",
            "                    except PartialStateConflictError as e:",
            "                        # Persisting couldn't happen because the room got un-partial stated",
            "                        # in the meantime and context needs to be recomputed, so let's do so.",
            "                        if i == max_retries - 1:",
            "                            raise e",
            "                        pass",
            "                return True",
            "            except AuthError:",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s for user %s due to \"",
            "                    \"lack of power. Will try another user\" % (room_id, user_id)",
            "                )",
            "        return False",
            "",
            "    def _expire_rooms_to_exclude_from_dummy_event_insertion(self) -> None:",
            "        expire_before = self.clock.time_msec() - _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "        to_expire = set()",
            "        for room_id, time in self._rooms_to_exclude_from_dummy_event_insertion.items():",
            "            if time < expire_before:",
            "                to_expire.add(room_id)",
            "        for room_id in to_expire:",
            "            logger.debug(",
            "                \"Expiring room id %s from dummy event insertion exclusion cache\",",
            "                room_id,",
            "            )",
            "            del self._rooms_to_exclude_from_dummy_event_insertion[room_id]",
            "",
            "    async def _rebuild_event_after_third_party_rules(",
            "        self, third_party_result: dict, original_event: EventBase",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        # the third_party_event_rules want to replace the event.",
            "        # we do some basic checks, and then return the replacement event.",
            "",
            "        # Construct a new EventBuilder and validate it, which helps with the",
            "        # rest of these checks.",
            "        try:",
            "            builder = self.event_builder_factory.for_room_version(",
            "                original_event.room_version, third_party_result",
            "            )",
            "            self.validator.validate_builder(builder)",
            "        except SynapseError as e:",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \" + e.msg,",
            "            )",
            "",
            "        immutable_fields = [",
            "            # changing the room is going to break things: we've already checked that the",
            "            # room exists, and are holding a concurrency limiter token for that room.",
            "            # Also, we might need to use a different room version.",
            "            \"room_id\",",
            "            # changing the type or state key might work, but we'd need to check that the",
            "            # calling functions aren't making assumptions about them.",
            "            \"type\",",
            "            \"state_key\",",
            "        ]",
            "",
            "        for k in immutable_fields:",
            "            if getattr(builder, k, None) != original_event.get(k):",
            "                raise Exception(",
            "                    \"Third party rules module created an invalid event: \"",
            "                    \"cannot change field \" + k",
            "                )",
            "",
            "        # check that the new sender belongs to this HS",
            "        if not self.hs.is_mine_id(builder.sender):",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \"",
            "                \"invalid sender \" + builder.sender",
            "            )",
            "",
            "        # copy over the original internal metadata",
            "        for k, v in original_event.internal_metadata.get_dict().items():",
            "            setattr(builder.internal_metadata, k, v)",
            "",
            "        # modules can send new state events, so we re-calculate the auth events just in",
            "        # case.",
            "        prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        event = await builder.build(",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=None,",
            "        )",
            "",
            "        # we rebuild the event context, to be on the safe side. If nothing else,",
            "        # delta_ids might need an update.",
            "        context = await self.state.calculate_context_info(event)",
            "",
            "        return event, context"
        ],
        "afterPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017-2018 New Vector Ltd",
            "# Copyright 2019-2020 The Matrix.org Foundation C.I.C.",
            "# Copyrignt 2020 Sorunome",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from http import HTTPStatus",
            "from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Tuple",
            "",
            "from canonicaljson import encode_canonical_json",
            "",
            "from twisted.internet.interfaces import IDelayedCall",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    HistoryVisibility,",
            "    Membership,",
            "    RelationTypes,",
            "    UserTypes,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    ConsentNotGivenError,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    ShadowBanError,",
            "    SynapseError,",
            "    UnstableSpecAuthError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS",
            "from synapse.api.urls import ConsentURIBuilder",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase, relation_from_event",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.snapshot import EventContext, UnpersistedEventContextBase",
            "from synapse.events.utils import SerializeEventConfig, maybe_upsert_event_field",
            "from synapse.events.validator import EventValidator",
            "from synapse.handlers.directory import DirectoryHandler",
            "from synapse.handlers.worker_lock import NEW_EVENT_DURING_PURGE_LOCK_NAME",
            "from synapse.logging import opentracing",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.send_event import ReplicationSendEventRestServlet",
            "from synapse.replication.http.send_events import ReplicationSendEventsRestServlet",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    PersistedEventPosition,",
            "    Requester,",
            "    RoomAlias,",
            "    StateMap,",
            "    StreamToken,",
            "    UserID,",
            "    create_requester,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util import json_decoder, json_encoder, log_failure, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, gather_results",
            "from synapse.util.caches.expiringcache import ExpiringCache",
            "from synapse.util.metrics import measure_func",
            "from synapse.visibility import get_effective_room_visibility_from_state",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class MessageHandler:",
            "    \"\"\"Contains some read only APIs to get state about a room\"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.auth = hs.get_auth()",
            "        self.clock = hs.get_clock()",
            "        self.state = hs.get_state_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        # The scheduled call to self._expire_event. None if no call is currently",
            "        # scheduled.",
            "        self._scheduled_expiry: Optional[IDelayedCall] = None",
            "",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"_schedule_next_expiry\", self._schedule_next_expiry",
            "            )",
            "",
            "    async def get_room_data(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        event_type: str,",
            "        state_key: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get data from a room.",
            "",
            "        Args:",
            "            requester: The user who did the request.",
            "            room_id",
            "            event_type",
            "            state_key",
            "        Returns:",
            "            The path data content.",
            "        Raises:",
            "            SynapseError or AuthError if the user is not in the room",
            "        \"\"\"",
            "        (",
            "            membership,",
            "            membership_event_id,",
            "        ) = await self.auth.check_user_in_room_or_world_readable(",
            "            room_id, requester, allow_departed_users=True",
            "        )",
            "",
            "        if membership == Membership.JOIN:",
            "            data = await self._storage_controllers.state.get_current_state_event(",
            "                room_id, event_type, state_key",
            "            )",
            "        elif membership == Membership.LEAVE:",
            "            key = (event_type, state_key)",
            "            # If the membership is not JOIN, then the event ID should exist.",
            "            assert (",
            "                membership_event_id is not None",
            "            ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "            room_state = await self._state_storage_controller.get_state_for_events(",
            "                [membership_event_id], StateFilter.from_types([key])",
            "            )",
            "            data = room_state[membership_event_id].get(key)",
            "        else:",
            "            # check_user_in_room_or_world_readable, if it doesn't raise an AuthError, should",
            "            # only ever return a Membership.JOIN/LEAVE object",
            "            #",
            "            # Safeguard in case it returned something else",
            "            logger.error(",
            "                \"Attempted to retrieve data from a room for a user that has never been in it. \"",
            "                \"This should not have happened.\"",
            "            )",
            "            raise UnstableSpecAuthError(",
            "                403,",
            "                \"User not in room\",",
            "                errcode=Codes.NOT_JOINED,",
            "            )",
            "",
            "        return data",
            "",
            "    async def get_state_events(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        at_token: Optional[StreamToken] = None,",
            "    ) -> List[dict]:",
            "        \"\"\"Retrieve all state events for a given room. If the user is",
            "        joined to the room then return the current state. If the user has",
            "        left the room return the state events from when they left. If an explicit",
            "        'at' parameter is passed, return the state events as of that event, if",
            "        visible.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "            state_filter: The state filter used to fetch state from the database.",
            "            at_token: the stream token of the at which we are requesting",
            "                the stats. If the user is not allowed to view the state as of that",
            "                stream token, we raise a 403 SynapseError. If None, returns the current",
            "                state based on the current_state_events table.",
            "        Returns:",
            "            A list of dicts representing state events. [{}, {}, {}]",
            "        Raises:",
            "            NotFoundError (404) if the at token does not yield an event",
            "",
            "            AuthError (403) if the user doesn't have permission to view",
            "            members of this room.",
            "        \"\"\"",
            "        state_filter = state_filter or StateFilter.all()",
            "        user_id = requester.user.to_string()",
            "",
            "        if at_token:",
            "            last_event_id = (",
            "                await self.store.get_last_event_in_room_before_stream_ordering(",
            "                    room_id,",
            "                    end_token=at_token.room_key,",
            "                )",
            "            )",
            "",
            "            if not last_event_id:",
            "                raise NotFoundError(\"Can't find event for token %s\" % (at_token,))",
            "",
            "            if not await self._user_can_see_state_at_event(",
            "                user_id, room_id, last_event_id",
            "            ):",
            "                raise AuthError(",
            "                    403,",
            "                    \"User %s not allowed to view events in room %s at token %s\"",
            "                    % (user_id, room_id, at_token),",
            "                )",
            "",
            "            room_state_events = (",
            "                await self._state_storage_controller.get_state_for_events(",
            "                    [last_event_id], state_filter=state_filter",
            "                )",
            "            )",
            "            room_state: Mapping[Any, EventBase] = room_state_events[last_event_id]",
            "        else:",
            "            (",
            "                membership,",
            "                membership_event_id,",
            "            ) = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "",
            "            if membership == Membership.JOIN:",
            "                state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                    room_id, state_filter=state_filter",
            "                )",
            "                room_state = await self.store.get_events(state_ids.values())",
            "            elif membership == Membership.LEAVE:",
            "                # If the membership is not JOIN, then the event ID should exist.",
            "                assert (",
            "                    membership_event_id is not None",
            "                ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "                room_state_events = (",
            "                    await self._state_storage_controller.get_state_for_events(",
            "                        [membership_event_id], state_filter=state_filter",
            "                    )",
            "                )",
            "                room_state = room_state_events[membership_event_id]",
            "",
            "        events = self._event_serializer.serialize_events(",
            "            room_state.values(),",
            "            self.clock.time_msec(),",
            "            config=SerializeEventConfig(requester=requester),",
            "        )",
            "        return events",
            "",
            "    async def _user_can_see_state_at_event(",
            "        self, user_id: str, room_id: str, event_id: str",
            "    ) -> bool:",
            "        # check whether the user was in the room, and the history visibility,",
            "        # at that time.",
            "        state_map = await self._state_storage_controller.get_state_for_event(",
            "            event_id,",
            "            StateFilter.from_types(",
            "                [",
            "                    (EventTypes.Member, user_id),",
            "                    (EventTypes.RoomHistoryVisibility, \"\"),",
            "                ]",
            "            ),",
            "        )",
            "",
            "        membership = None",
            "        membership_event = state_map.get((EventTypes.Member, user_id))",
            "        if membership_event:",
            "            membership = membership_event.membership",
            "",
            "        # if the user was a member of the room at the time of the event,",
            "        # they can see it.",
            "        if membership == Membership.JOIN:",
            "            return True",
            "",
            "        # otherwise, it depends on the history visibility.",
            "        visibility = get_effective_room_visibility_from_state(state_map)",
            "",
            "        if visibility == HistoryVisibility.JOINED:",
            "            # we weren't a member at the time of the event, so we can't see this event.",
            "            return False",
            "",
            "        # otherwise *invited* is good enough",
            "        if membership == Membership.INVITE:",
            "            return True",
            "",
            "        if visibility == HistoryVisibility.INVITED:",
            "            # we weren't invited, so we can't see this event.",
            "            return False",
            "",
            "        if visibility == HistoryVisibility.WORLD_READABLE:",
            "            return True",
            "",
            "        # So it's SHARED, and the user was not a member at the time. The user cannot",
            "        # see history, unless they have *subsequently* joined the room.",
            "        #",
            "        # XXX: if the user has subsequently joined and then left again,",
            "        # ideally we would share history up to the point they left. But",
            "        # we don't know when they left. We just treat it as though they",
            "        # never joined, and restrict access.",
            "",
            "        (",
            "            current_membership,",
            "            _,",
            "        ) = await self.store.get_local_current_membership_for_user_in_room(",
            "            user_id, event_id",
            "        )",
            "        return current_membership == Membership.JOIN",
            "",
            "    async def get_joined_members(self, requester: Requester, room_id: str) -> dict:",
            "        \"\"\"Get all the joined members in the room and their profile information.",
            "",
            "        If the user has left the room return the state events from when they left.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "        Returns:",
            "            A dict of user_id to profile info",
            "        \"\"\"",
            "        if not requester.app_service:",
            "            # We check AS auth after fetching the room membership, as it",
            "            # requires us to pull out all joined members anyway.",
            "            membership, _ = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "            if membership != Membership.JOIN:",
            "                raise SynapseError(",
            "                    code=403,",
            "                    errcode=Codes.FORBIDDEN,",
            "                    msg=\"Getting joined members while not being a current member of the room is forbidden.\",",
            "                )",
            "",
            "        users_with_profile = (",
            "            await self._state_storage_controller.get_users_in_room_with_profiles(",
            "                room_id",
            "            )",
            "        )",
            "",
            "        # If this is an AS, double check that they are allowed to see the members.",
            "        # This can either be because the AS user is in the room or because there",
            "        # is a user in the room that the AS is \"interested in\"",
            "        if (",
            "            requester.app_service",
            "            and requester.user.to_string() not in users_with_profile",
            "        ):",
            "            for uid in users_with_profile:",
            "                if requester.app_service.is_interested_in_user(uid):",
            "                    break",
            "            else:",
            "                # Loop fell through, AS has no interested users in room",
            "                raise UnstableSpecAuthError(",
            "                    403,",
            "                    \"Appservice not in room\",",
            "                    errcode=Codes.NOT_JOINED,",
            "                )",
            "",
            "        return {",
            "            user_id: {",
            "                \"avatar_url\": profile.avatar_url,",
            "                \"display_name\": profile.display_name,",
            "            }",
            "            for user_id, profile in users_with_profile.items()",
            "        }",
            "",
            "    def maybe_schedule_expiry(self, event: EventBase) -> None:",
            "        \"\"\"Schedule the expiry of an event if there's not already one scheduled,",
            "        or if the one running is for an event that will expire after the provided",
            "        timestamp.",
            "",
            "        This function needs to invalidate the event cache, which is only possible on",
            "        the master process, and therefore needs to be run on there.",
            "",
            "        Args:",
            "            event: The event to schedule the expiry of.",
            "        \"\"\"",
            "",
            "        expiry_ts = event.content.get(EventContentFields.SELF_DESTRUCT_AFTER)",
            "        if type(expiry_ts) is not int or event.is_state():  # noqa: E721",
            "            return",
            "",
            "        # _schedule_expiry_for_event won't actually schedule anything if there's already",
            "        # a task scheduled for a timestamp that's sooner than the provided one.",
            "        self._schedule_expiry_for_event(event.event_id, expiry_ts)",
            "",
            "    async def _schedule_next_expiry(self) -> None:",
            "        \"\"\"Retrieve the ID and the expiry timestamp of the next event to be expired,",
            "        and schedule an expiry task for it.",
            "",
            "        If there's no event left to expire, set _expiry_scheduled to None so that a",
            "        future call to save_expiry_ts can schedule a new expiry task.",
            "        \"\"\"",
            "        # Try to get the expiry timestamp of the next event to expire.",
            "        res = await self.store.get_next_event_to_expire()",
            "        if res:",
            "            event_id, expiry_ts = res",
            "            self._schedule_expiry_for_event(event_id, expiry_ts)",
            "",
            "    def _schedule_expiry_for_event(self, event_id: str, expiry_ts: int) -> None:",
            "        \"\"\"Schedule an expiry task for the provided event if there's not already one",
            "        scheduled at a timestamp that's sooner than the provided one.",
            "",
            "        Args:",
            "            event_id: The ID of the event to expire.",
            "            expiry_ts: The timestamp at which to expire the event.",
            "        \"\"\"",
            "        if self._scheduled_expiry:",
            "            # If the provided timestamp refers to a time before the scheduled time of the",
            "            # next expiry task, cancel that task and reschedule it for this timestamp.",
            "            next_scheduled_expiry_ts = self._scheduled_expiry.getTime() * 1000",
            "            if expiry_ts < next_scheduled_expiry_ts:",
            "                self._scheduled_expiry.cancel()",
            "            else:",
            "                return",
            "",
            "        # Figure out how many seconds we need to wait before expiring the event.",
            "        now_ms = self.clock.time_msec()",
            "        delay = (expiry_ts - now_ms) / 1000",
            "",
            "        # callLater doesn't support negative delays, so trim the delay to 0 if we're",
            "        # in that case.",
            "        if delay < 0:",
            "            delay = 0",
            "",
            "        logger.info(\"Scheduling expiry for event %s in %.3fs\", event_id, delay)",
            "",
            "        self._scheduled_expiry = self.clock.call_later(",
            "            delay,",
            "            run_as_background_process,",
            "            \"_expire_event\",",
            "            self._expire_event,",
            "            event_id,",
            "        )",
            "",
            "    async def _expire_event(self, event_id: str) -> None:",
            "        \"\"\"Retrieve and expire an event that needs to be expired from the database.",
            "",
            "        If the event doesn't exist in the database, log it and delete the expiry date",
            "        from the database (so that we don't try to expire it again).",
            "        \"\"\"",
            "        assert self._ephemeral_events_enabled",
            "",
            "        self._scheduled_expiry = None",
            "",
            "        logger.info(\"Expiring event %s\", event_id)",
            "",
            "        try:",
            "            # Expire the event if we know about it. This function also deletes the expiry",
            "            # date from the database in the same database transaction.",
            "            await self.store.expire_event(event_id)",
            "        except Exception as e:",
            "            logger.error(\"Could not expire event %s: %r\", event_id, e)",
            "",
            "        # Schedule the expiry of the next event to expire.",
            "        await self._schedule_next_expiry()",
            "",
            "",
            "# The duration (in ms) after which rooms should be removed",
            "# `_rooms_to_exclude_from_dummy_event_insertion` (with the effect that we will try",
            "# to generate a dummy event for them once more)",
            "#",
            "_DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY = 7 * 24 * 60 * 60 * 1000",
            "",
            "",
            "class EventCreationHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "        self.auth_blocking = hs.get_auth_blocking()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.state = hs.get_state_handler()",
            "        self.clock = hs.get_clock()",
            "        self.validator = EventValidator()",
            "        self.profile_handler = hs.get_profile_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self.server_name = hs.hostname",
            "        self.notifier = hs.get_notifier()",
            "        self.config = hs.config",
            "        self.require_membership_for_aliases = (",
            "            hs.config.server.require_membership_for_aliases",
            "        )",
            "        self._events_shard_config = self.config.worker.events_shard_config",
            "        self._instance_name = hs.get_instance_name()",
            "        self._notifier = hs.get_notifier()",
            "        self._worker_lock_handler = hs.get_worker_locks_handler()",
            "",
            "        self.room_prejoin_state_types = self.hs.config.api.room_prejoin_state",
            "",
            "        self.membership_types_to_include_profile_data_in = {",
            "            Membership.JOIN,",
            "            Membership.KNOCK,",
            "        }",
            "        if self.hs.config.server.include_profile_data_on_invite:",
            "            self.membership_types_to_include_profile_data_in.add(Membership.INVITE)",
            "",
            "        self.send_event = ReplicationSendEventRestServlet.make_client(hs)",
            "        self.send_events = ReplicationSendEventsRestServlet.make_client(hs)",
            "",
            "        self.request_ratelimiter = hs.get_request_ratelimiter()",
            "",
            "        # We limit concurrent event creation for a room to 1. This prevents state resolution",
            "        # from occurring when sending bursts of events to a local room",
            "        self.limiter = Linearizer(max_count=1, name=\"room_event_creation_limit\")",
            "",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self._third_party_event_rules = (",
            "            self.hs.get_module_api_callbacks().third_party_event_rules",
            "        )",
            "",
            "        self._block_events_without_consent_error = (",
            "            self.config.consent.block_events_without_consent_error",
            "        )",
            "",
            "        # we need to construct a ConsentURIBuilder here, as it checks that the necessary",
            "        # config options, but *only* if we have a configuration for which we are",
            "        # going to need it.",
            "        if self._block_events_without_consent_error:",
            "            self._consent_uri_builder = ConsentURIBuilder(self.config)",
            "",
            "        # Rooms which should be excluded from dummy insertion. (For instance,",
            "        # those without local users who can send events into the room).",
            "        #",
            "        # map from room id to time-of-last-attempt.",
            "        #",
            "        self._rooms_to_exclude_from_dummy_event_insertion: Dict[str, int] = {}",
            "        # The number of forward extremeities before a dummy event is sent.",
            "        self._dummy_events_threshold = hs.config.server.dummy_events_threshold",
            "",
            "        if (",
            "            self.config.worker.run_background_tasks",
            "            and self.config.server.cleanup_extremities_with_dummy_events",
            "        ):",
            "            self.clock.looping_call(",
            "                lambda: run_as_background_process(",
            "                    \"send_dummy_events_to_fill_extremities\",",
            "                    self._send_dummy_events_to_fill_extremities,",
            "                ),",
            "                5 * 60 * 1000,",
            "            )",
            "",
            "        self._message_handler = hs.get_message_handler()",
            "",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._external_cache = hs.get_external_cache()",
            "",
            "        # Stores the state groups we've recently added to the joined hosts",
            "        # external cache. Note that the timeout must be significantly less than",
            "        # the TTL on the external cache.",
            "        self._external_cache_joined_hosts_updates: Optional[ExpiringCache] = None",
            "        if self._external_cache.is_enabled():",
            "            self._external_cache_joined_hosts_updates = ExpiringCache(",
            "                \"_external_cache_joined_hosts_updates\",",
            "                self.clock,",
            "                expiry_ms=30 * 60 * 1000,",
            "            )",
            "",
            "    async def create_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        txn_id: Optional[str] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        require_consent: bool = True,",
            "        outlier: bool = False,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        \"\"\"",
            "        Given a dict from a client, create a new event. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events.",
            "",
            "        Creates an FrozenEvent object, filling out auth_events, prev_events,",
            "        etc.",
            "",
            "        Adds display names to Join membership events.",
            "",
            "        Args:",
            "            requester",
            "            event_dict: An entire event",
            "            txn_id",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases (previously useful for MSC2716).",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "                If non-None, prev_event_ids must also be provided.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This was previously used particularly",
            "                by the MSC2716 /batch_send endpoint. This should normally be left as",
            "                None, which will cause the auth_event_ids to be calculated based on the",
            "                room state at the prev_events.",
            "",
            "            require_consent: Whether to check if the requester has",
            "                consented to the privacy policy.",
            "",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Raises:",
            "            ResourceLimitError if server is blocked to some resource being",
            "            exceeded",
            "",
            "        Returns:",
            "            Tuple of created event, Context",
            "        \"\"\"",
            "        await self.auth_blocking.check_auth_blocking(requester=requester)",
            "",
            "        if event_dict[\"type\"] == EventTypes.Create and event_dict[\"state_key\"] == \"\":",
            "            room_version_id = event_dict[\"content\"][\"room_version\"]",
            "            maybe_room_version_obj = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "            if not maybe_room_version_obj:",
            "                # this can happen if support is withdrawn for a room version",
            "                raise UnsupportedRoomVersionError(room_version_id)",
            "            room_version_obj = maybe_room_version_obj",
            "        else:",
            "            try:",
            "                room_version_obj = await self.store.get_room_version(",
            "                    event_dict[\"room_id\"]",
            "                )",
            "            except NotFoundError:",
            "                raise AuthError(403, \"Unknown room\")",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        self.validator.validate_builder(builder)",
            "",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            target = UserID.from_string(builder.state_key)",
            "",
            "            if membership in self.membership_types_to_include_profile_data_in:",
            "                # If event doesn't include a display name, add one.",
            "                profile = self.profile_handler",
            "                content = builder.content",
            "",
            "                try:",
            "                    if \"displayname\" not in content:",
            "                        displayname = await profile.get_displayname(target)",
            "                        if displayname is not None:",
            "                            content[\"displayname\"] = displayname",
            "                    if \"avatar_url\" not in content:",
            "                        avatar_url = await profile.get_avatar_url(target)",
            "                        if avatar_url is not None:",
            "                            content[\"avatar_url\"] = avatar_url",
            "                except Exception as e:",
            "                    logger.info(",
            "                        \"Failed to get profile information for %r: %s\", target, e",
            "                    )",
            "",
            "        is_exempt = await self._is_exempt_from_privacy_policy(builder, requester)",
            "        if require_consent and not is_exempt:",
            "            await self.assert_accepted_privacy_policy(requester)",
            "",
            "        # Save the access token ID, the device ID and the transaction ID in the event",
            "        # internal metadata. This is useful to determine if we should echo the",
            "        # transaction_id in events.",
            "        # See `synapse.events.utils.EventClientSerializer.serialize_event`",
            "        if requester.access_token_id is not None:",
            "            builder.internal_metadata.token_id = requester.access_token_id",
            "",
            "        if requester.device_id is not None:",
            "            builder.internal_metadata.device_id = requester.device_id",
            "",
            "        if txn_id is not None:",
            "            builder.internal_metadata.txn_id = txn_id",
            "",
            "        builder.internal_metadata.outlier = outlier",
            "",
            "        event, unpersisted_context = await self.create_new_client_event(",
            "            builder=builder,",
            "            requester=requester,",
            "            allow_no_prev_events=allow_no_prev_events,",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=auth_event_ids,",
            "            state_event_ids=state_event_ids,",
            "            depth=depth,",
            "            state_map=state_map,",
            "            for_batch=for_batch,",
            "            current_state_group=current_state_group,",
            "        )",
            "",
            "        # In an ideal world we wouldn't need the second part of this condition. However,",
            "        # this behaviour isn't spec'd yet, meaning we should be able to deactivate this",
            "        # behaviour. Another reason is that this code is also evaluated each time a new",
            "        # m.room.aliases event is created, which includes hitting a /directory route.",
            "        # Therefore not including this condition here would render the similar one in",
            "        # synapse.handlers.directory pointless.",
            "        if builder.type == EventTypes.Aliases and self.require_membership_for_aliases:",
            "            # Ideally we'd do the membership check in event_auth.check(), which",
            "            # describes a spec'd algorithm for authenticating events received over",
            "            # federation as well as those created locally. As of room v3, aliases events",
            "            # can be created by users that are not in the room, therefore we have to",
            "            # tolerate them in event_auth.check().",
            "            if for_batch:",
            "                assert state_map is not None",
            "                prev_event_id = state_map.get((EventTypes.Member, event.sender))",
            "            else:",
            "                prev_state_ids = await unpersisted_context.get_prev_state_ids(",
            "                    StateFilter.from_types([(EventTypes.Member, event.sender)])",
            "                )",
            "                prev_event_id = prev_state_ids.get((EventTypes.Member, event.sender))",
            "            prev_event = (",
            "                await self.store.get_event(prev_event_id, allow_none=True)",
            "                if prev_event_id",
            "                else None",
            "            )",
            "            if not prev_event or prev_event.membership != Membership.JOIN:",
            "                logger.warning(",
            "                    (",
            "                        \"Attempt to send `m.room.aliases` in room %s by user %s but\"",
            "                        \" membership is %s\"",
            "                    ),",
            "                    event.room_id,",
            "                    event.sender,",
            "                    prev_event.membership if prev_event else None,",
            "                )",
            "",
            "                raise AuthError(",
            "                    403, \"You must be in the room to create an alias for it\"",
            "                )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "        return event, unpersisted_context",
            "",
            "    async def _is_exempt_from_privacy_policy(",
            "        self, builder: EventBuilder, requester: Requester",
            "    ) -> bool:",
            "        \"\"\" \"Determine if an event to be sent is exempt from having to consent",
            "        to the privacy policy",
            "",
            "        Args:",
            "            builder: event being created",
            "            requester: user requesting this event",
            "",
            "        Returns:",
            "            true if the event can be sent without the user consenting",
            "        \"\"\"",
            "        # the only thing the user can do is join the server notices room.",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            if membership == Membership.JOIN:",
            "                return await self.store.is_server_notice_room(builder.room_id)",
            "            elif membership == Membership.LEAVE:",
            "                # the user is always allowed to leave (but not kick people)",
            "                return builder.state_key == requester.user.to_string()",
            "        return False",
            "",
            "    async def assert_accepted_privacy_policy(self, requester: Requester) -> None:",
            "        \"\"\"Check if a user has accepted the privacy policy",
            "",
            "        Called when the given user is about to do something that requires",
            "        privacy consent. We see if the user is exempt and otherwise check that",
            "        they have given consent. If they have not, a ConsentNotGiven error is",
            "        raised.",
            "",
            "        Args:",
            "            requester: The user making the request",
            "",
            "        Returns:",
            "            Returns normally if the user has consented or is exempt",
            "",
            "        Raises:",
            "            ConsentNotGivenError: if the user has not given consent yet",
            "        \"\"\"",
            "        if self._block_events_without_consent_error is None:",
            "            return",
            "",
            "        # exempt AS users from needing consent",
            "        if requester.app_service is not None:",
            "            return",
            "",
            "        user_id = requester.authenticated_entity",
            "        if not user_id.startswith(\"@\"):",
            "            # The authenticated entity might not be a user, e.g. if it's the",
            "            # server puppetting the user.",
            "            return",
            "",
            "        user = UserID.from_string(user_id)",
            "",
            "        # exempt the system notices user",
            "        if (",
            "            self.config.servernotices.server_notices_mxid is not None",
            "            and user_id == self.config.servernotices.server_notices_mxid",
            "        ):",
            "            return",
            "",
            "        u = await self.store.get_user_by_id(user_id)",
            "        assert u is not None",
            "        if u.user_type in (UserTypes.SUPPORT, UserTypes.BOT):",
            "            # support and bot users are not required to consent",
            "            return",
            "        if u.appservice_id is not None:",
            "            # users registered by an appservice are exempt",
            "            return",
            "        if u.consent_version == self.config.consent.user_consent_version:",
            "            return",
            "",
            "        consent_uri = self._consent_uri_builder.build_user_consent_uri(user.localpart)",
            "        msg = self._block_events_without_consent_error % {\"consent_uri\": consent_uri}",
            "        raise ConsentNotGivenError(msg=msg, consent_uri=consent_uri)",
            "",
            "    async def deduplicate_state_event(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"",
            "        Checks whether event is in the latest resolved state in context.",
            "",
            "        Args:",
            "            event: The event to check for duplication.",
            "            context: The event context.",
            "",
            "        Returns:",
            "            The previous version of the event is returned, if it is found in the",
            "            event context. Otherwise, None is returned.",
            "        \"\"\"",
            "        if event.internal_metadata.is_outlier():",
            "            # This can happen due to out of band memberships",
            "            return None",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(event.type, event.state_key)])",
            "        )",
            "        prev_event_id = prev_state_ids.get((event.type, event.state_key))",
            "        if not prev_event_id:",
            "            return None",
            "        prev_event = await self.store.get_event(prev_event_id, allow_none=True)",
            "        if not prev_event:",
            "            return None",
            "",
            "        if prev_event and event.user_id == prev_event.user_id:",
            "            prev_content = encode_canonical_json(prev_event.content)",
            "            next_content = encode_canonical_json(event.content)",
            "            if prev_content == next_content:",
            "                return prev_event",
            "        return None",
            "",
            "    async def get_event_id_from_transaction(",
            "        self,",
            "        requester: Requester,",
            "        txn_id: str,",
            "        room_id: str,",
            "    ) -> Optional[str]:",
            "        \"\"\"For the given transaction ID and room ID, check if there is a matching event ID.",
            "",
            "        Args:",
            "            requester: The requester making the request in the context of which we want",
            "                to fetch the event.",
            "            txn_id: The transaction ID.",
            "            room_id: The room ID.",
            "",
            "        Returns:",
            "            An event ID if one could be found, None otherwise.",
            "        \"\"\"",
            "        existing_event_id = None",
            "",
            "        # According to the spec, transactions are scoped to a user's device ID.",
            "        if requester.device_id:",
            "            existing_event_id = (",
            "                await self.store.get_event_id_from_transaction_id_and_device_id(",
            "                    room_id,",
            "                    requester.user.to_string(),",
            "                    requester.device_id,",
            "                    txn_id,",
            "                )",
            "            )",
            "            if existing_event_id:",
            "                return existing_event_id",
            "",
            "        return existing_event_id",
            "",
            "    async def get_event_from_transaction(",
            "        self,",
            "        requester: Requester,",
            "        txn_id: str,",
            "        room_id: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"For the given transaction ID and room ID, check if there is a matching event.",
            "        If so, fetch it and return it.",
            "",
            "        Args:",
            "            requester: The requester making the request in the context of which we want",
            "                to fetch the event.",
            "            txn_id: The transaction ID.",
            "            room_id: The room ID.",
            "",
            "        Returns:",
            "            An event if one could be found, None otherwise.",
            "        \"\"\"",
            "        existing_event_id = await self.get_event_id_from_transaction(",
            "            requester, txn_id, room_id",
            "        )",
            "        if existing_event_id:",
            "            return await self.store.get_event(existing_event_id)",
            "        return None",
            "",
            "    async def create_and_send_nonmember_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        ratelimit: bool = True,",
            "        txn_id: Optional[str] = None,",
            "        ignore_shadow_ban: bool = False,",
            "        outlier: bool = False,",
            "        depth: Optional[int] = None,",
            "    ) -> Tuple[EventBase, int]:",
            "        \"\"\"",
            "        Creates an event, then sends it.",
            "",
            "        See self.create_event and self.handle_new_client_event.",
            "",
            "        Args:",
            "            requester: The requester sending the event.",
            "            event_dict: An entire event.",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases (previously useful for MSC2716).",
            "            prev_event_ids:",
            "                The event IDs to use as the prev events.",
            "                Should normally be left as None to automatically request them",
            "                from the database.",
            "            state_event_ids:",
            "                The full state at a given event. This was previously used particularly",
            "                by the MSC2716 /batch_send endpoint. This should normally be left as",
            "                None, which will cause the auth_event_ids to be calculated based on the",
            "                room state at the prev_events.",
            "            ratelimit: Whether to rate limit this send.",
            "            txn_id: The transaction ID.",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "        Returns:",
            "            The event, and its stream ordering (if deduplication happened,",
            "            the previous, duplicate event).",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "        \"\"\"",
            "",
            "        if event_dict[\"type\"] == EventTypes.Member:",
            "            raise SynapseError(",
            "                500, \"Tried to send member event through non-member codepath\"",
            "            )",
            "",
            "        if not ignore_shadow_ban and requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        if ratelimit:",
            "            await self.request_ratelimiter.ratelimit(requester, update=False)",
            "",
            "        # We limit the number of concurrent event sends in a room so that we",
            "        # don't fork the DAG too much. If we don't limit then we can end up in",
            "        # a situation where event persistence can't keep up, causing",
            "        # extremities to pile up, which in turn leads to state resolution",
            "        # taking longer.",
            "        room_id = event_dict[\"room_id\"]",
            "        async with self.limiter.queue(room_id):",
            "            if txn_id:",
            "                event = await self.get_event_from_transaction(",
            "                    requester, txn_id, room_id",
            "                )",
            "                if event:",
            "                    # we know it was persisted, so must have a stream ordering",
            "                    assert event.internal_metadata.stream_ordering",
            "                    return (",
            "                        event,",
            "                        event.internal_metadata.stream_ordering,",
            "                    )",
            "",
            "        async with self._worker_lock_handler.acquire_read_write_lock(",
            "            NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "        ):",
            "            return await self._create_and_send_nonmember_event_locked(",
            "                requester=requester,",
            "                event_dict=event_dict,",
            "                allow_no_prev_events=allow_no_prev_events,",
            "                prev_event_ids=prev_event_ids,",
            "                state_event_ids=state_event_ids,",
            "                ratelimit=ratelimit,",
            "                txn_id=txn_id,",
            "                ignore_shadow_ban=ignore_shadow_ban,",
            "                outlier=outlier,",
            "                depth=depth,",
            "            )",
            "",
            "    async def _create_and_send_nonmember_event_locked(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        ratelimit: bool = True,",
            "        txn_id: Optional[str] = None,",
            "        ignore_shadow_ban: bool = False,",
            "        outlier: bool = False,",
            "        depth: Optional[int] = None,",
            "    ) -> Tuple[EventBase, int]:",
            "        room_id = event_dict[\"room_id\"]",
            "",
            "        # If we don't have any prev event IDs specified then we need to",
            "        # check that the host is in the room (as otherwise populating the",
            "        # prev events will fail), at which point we may as well check the",
            "        # local user is in the room.",
            "        if not prev_event_ids:",
            "            user_id = requester.user.to_string()",
            "            is_user_in_room = await self.store.check_local_user_in_room(",
            "                user_id, room_id",
            "            )",
            "            if not is_user_in_room:",
            "                raise AuthError(403, f\"User {user_id} not in room {room_id}\")",
            "",
            "        # Try several times, it could fail with PartialStateConflictError",
            "        # in handle_new_client_event, cf comment in except block.",
            "        max_retries = 5",
            "        for i in range(max_retries):",
            "            try:",
            "                event, unpersisted_context = await self.create_event(",
            "                    requester,",
            "                    event_dict,",
            "                    txn_id=txn_id,",
            "                    allow_no_prev_events=allow_no_prev_events,",
            "                    prev_event_ids=prev_event_ids,",
            "                    state_event_ids=state_event_ids,",
            "                    outlier=outlier,",
            "                    depth=depth,",
            "                )",
            "                context = await unpersisted_context.persist(event)",
            "",
            "                assert self.hs.is_mine_id(event.sender), \"User must be our own: %s\" % (",
            "                    event.sender,",
            "                )",
            "",
            "                spam_check_result = (",
            "                    await self._spam_checker_module_callbacks.check_event_for_spam(",
            "                        event",
            "                    )",
            "                )",
            "                if spam_check_result != self._spam_checker_module_callbacks.NOT_SPAM:",
            "                    if isinstance(spam_check_result, tuple):",
            "                        try:",
            "                            [code, dict] = spam_check_result",
            "                            raise SynapseError(",
            "                                403,",
            "                                \"This message had been rejected as probable spam\",",
            "                                code,",
            "                                dict,",
            "                            )",
            "                        except ValueError:",
            "                            logger.error(",
            "                                \"Spam-check module returned invalid error value. Expecting [code, dict], got %s\",",
            "                                spam_check_result,",
            "                            )",
            "",
            "                            raise SynapseError(",
            "                                403,",
            "                                \"This message has been rejected as probable spam\",",
            "                                Codes.FORBIDDEN,",
            "                            )",
            "",
            "                    # Backwards compatibility: if the return value is not an error code, it",
            "                    # means the module returned an error message to be included in the",
            "                    # SynapseError (which is now deprecated).",
            "                    raise SynapseError(",
            "                        403,",
            "                        spam_check_result,",
            "                        Codes.FORBIDDEN,",
            "                    )",
            "",
            "                ev = await self.handle_new_client_event(",
            "                    requester=requester,",
            "                    events_and_context=[(event, context)],",
            "                    ratelimit=ratelimit,",
            "                    ignore_shadow_ban=ignore_shadow_ban,",
            "                )",
            "",
            "                break",
            "            except PartialStateConflictError as e:",
            "                # Persisting couldn't happen because the room got un-partial stated",
            "                # in the meantime and context needs to be recomputed, so let's do so.",
            "                if i == max_retries - 1:",
            "                    raise e",
            "                pass",
            "",
            "        # we know it was persisted, so must have a stream ordering",
            "        assert ev.internal_metadata.stream_ordering",
            "        return ev, ev.internal_metadata.stream_ordering",
            "",
            "    @measure_func(\"create_new_client_event\")",
            "    async def create_new_client_event(",
            "        self,",
            "        builder: EventBuilder,",
            "        requester: Optional[Requester] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        \"\"\"Create a new event for a local client. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events. Note that this returns an UnpersistedEventContext,",
            "        which must be converted to an EventContext before it can be sent to the DB.",
            "",
            "        Args:",
            "            builder:",
            "            requester:",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases (previously useful for MSC2716).",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This was previously used particularly",
            "                by the MSC2716 /batch_send endpoint. This should normally be left as",
            "                None, which will cause the auth_event_ids to be calculated based on the",
            "                room state at the prev_events.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Returns:",
            "            Tuple of created event, UnpersistedEventContext",
            "        \"\"\"",
            "        # Strip down the state_event_ids to only what we need to auth the event.",
            "        # For example, we don't need extra m.room.member that don't match event.sender",
            "        if state_event_ids is not None:",
            "            # Do a quick check to make sure that prev_event_ids is present to",
            "            # make the type-checking around `builder.build` happy.",
            "            # prev_event_ids could be an empty array though.",
            "            assert prev_event_ids is not None",
            "",
            "            temp_event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=state_event_ids,",
            "                depth=depth,",
            "            )",
            "            state_events = await self.store.get_events_as_list(state_event_ids)",
            "            # Create a StateMap[str]",
            "            current_state_ids = {",
            "                (e.type, e.state_key): e.event_id for e in state_events",
            "            }",
            "            # Actually strip down and only use the necessary auth events",
            "            auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "                event=temp_event,",
            "                current_state_ids=current_state_ids,",
            "                for_verification=False,",
            "            )",
            "",
            "        if prev_event_ids is not None:",
            "            assert (",
            "                len(prev_event_ids) <= 10",
            "            ), \"Attempting to create an event with %i prev_events\" % (",
            "                len(prev_event_ids),",
            "            )",
            "        else:",
            "            prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        # Do a quick sanity check here, rather than waiting until we've created the",
            "        # event and then try to auth it (which fails with a somewhat confusing \"No",
            "        # create event in auth events\")",
            "        if allow_no_prev_events:",
            "            # We allow events with no `prev_events` but it better have some `auth_events`",
            "            assert (",
            "                builder.type == EventTypes.Create",
            "                # Allow an event to have empty list of prev_event_ids",
            "                # only if it has auth_event_ids.",
            "                or auth_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events or auth_event_ids\"",
            "        else:",
            "            # we now ought to have some prev_events (unless it's a create event).",
            "            assert (",
            "                builder.type == EventTypes.Create or prev_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events\"",
            "",
            "        if for_batch:",
            "            assert prev_event_ids is not None",
            "            assert state_map is not None",
            "            auth_ids = self._event_auth_handler.compute_auth_events(builder, state_map)",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids, auth_event_ids=auth_ids, depth=depth",
            "            )",
            "",
            "            context: UnpersistedEventContextBase = (",
            "                await self.state.calculate_context_info(",
            "                    event,",
            "                    state_ids_before_event=state_map,",
            "                    partial_state=False,",
            "                    state_group_before_event=current_state_group,",
            "                )",
            "            )",
            "",
            "        else:",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=auth_event_ids,",
            "                depth=depth,",
            "            )",
            "",
            "            # Pass on the outlier property from the builder to the event",
            "            # after it is created",
            "            if builder.internal_metadata.outlier:",
            "                event.internal_metadata.outlier = True",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "            else:",
            "                context = await self.state.calculate_context_info(event)",
            "",
            "        if requester:",
            "            context.app_service = requester.app_service",
            "",
            "        res, new_content = await self._third_party_event_rules.check_event_allowed(",
            "            event, context",
            "        )",
            "        if res is False:",
            "            logger.info(",
            "                \"Event %s forbidden by third-party rules\",",
            "                event,",
            "            )",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "        elif new_content is not None:",
            "            # the third-party rules want to replace the event. We'll need to build a new",
            "            # event.",
            "            event, context = await self._rebuild_event_after_third_party_rules(",
            "                new_content, event",
            "            )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "        await self._validate_event_relation(event)",
            "        logger.debug(\"Created event %s\", event.event_id)",
            "",
            "        return event, context",
            "",
            "    async def _validate_event_relation(self, event: EventBase) -> None:",
            "        \"\"\"",
            "        Ensure the relation data on a new event is not bogus.",
            "",
            "        Args:",
            "            event: The event being created.",
            "",
            "        Raises:",
            "            SynapseError if the event is invalid.",
            "        \"\"\"",
            "",
            "        relation = relation_from_event(event)",
            "        if not relation:",
            "            return",
            "",
            "        parent_event = await self.store.get_event(relation.parent_id, allow_none=True)",
            "        if parent_event:",
            "            # And in the same room.",
            "            if parent_event.room_id != event.room_id:",
            "                raise SynapseError(400, \"Relations must be in the same room\")",
            "",
            "        else:",
            "            # There must be some reason that the client knows the event exists,",
            "            # see if there are existing relations. If so, assume everything is fine.",
            "            if not await self.store.event_is_target_of_relation(relation.parent_id):",
            "                # Otherwise, the client can't know about the parent event!",
            "                raise SynapseError(400, \"Can't send relation to unknown event\")",
            "",
            "        # If this event is an annotation then we check that that the sender",
            "        # can't annotate the same way twice (e.g. stops users from liking an",
            "        # event multiple times).",
            "        if relation.rel_type == RelationTypes.ANNOTATION:",
            "            aggregation_key = relation.aggregation_key",
            "",
            "            if aggregation_key is None:",
            "                raise SynapseError(400, \"Missing aggregation key\")",
            "",
            "            if len(aggregation_key) > 500:",
            "                raise SynapseError(400, \"Aggregation key is too long\")",
            "",
            "            already_exists = await self.store.has_user_annotated_event(",
            "                relation.parent_id, event.type, aggregation_key, event.sender",
            "            )",
            "            if already_exists:",
            "                raise SynapseError(",
            "                    400,",
            "                    \"Can't send same reaction twice\",",
            "                    errcode=Codes.DUPLICATE_ANNOTATION,",
            "                )",
            "",
            "        # Don't attempt to start a thread if the parent event is a relation.",
            "        elif relation.rel_type == RelationTypes.THREAD:",
            "            if await self.store.event_includes_relation(relation.parent_id):",
            "                raise SynapseError(",
            "                    400, \"Cannot start threads from an event with a relation\"",
            "                )",
            "",
            "    @measure_func(\"handle_new_client_event\")",
            "    async def handle_new_client_event(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "        ignore_shadow_ban: bool = False,",
            "    ) -> EventBase:",
            "        \"\"\"Processes new events. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        This includes deduplicating, checking auth, persisting,",
            "        notifying users, sending to remote servers, etc.",
            "",
            "        If called from a worker will hit out to the master process for final",
            "        processing.",
            "",
            "        Args:",
            "            requester",
            "            events_and_context: A list of one or more tuples of event, context to be persisted",
            "            ratelimit",
            "            extra_users: Any extra users to notify about event",
            "",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "",
            "        Return:",
            "            If the event was deduplicated, the previous, duplicate, event. Otherwise,",
            "            `event`.",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "            PartialStateConflictError if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            # we don't apply shadow-banning to membership events here. Invites are blocked",
            "            # higher up the stack, and we allow shadow-banned users to send join and leave",
            "            # events as normal.",
            "            if (",
            "                event.type != EventTypes.Member",
            "                and not ignore_shadow_ban",
            "                and requester.shadow_banned",
            "            ):",
            "                # We randomly sleep a bit just to annoy the requester.",
            "                await self.clock.sleep(random.randint(1, 10))",
            "                raise ShadowBanError()",
            "",
            "            if event.is_state():",
            "                prev_event = await self.deduplicate_state_event(event, context)",
            "                if prev_event is not None:",
            "                    logger.info(",
            "                        \"Not bothering to persist state event %s duplicated by %s\",",
            "                        event.event_id,",
            "                        prev_event.event_id,",
            "                    )",
            "                    return prev_event",
            "",
            "            if event.internal_metadata.is_out_of_band_membership():",
            "                # the only sort of out-of-band-membership events we expect to see here are",
            "                # invite rejections and rescinded knocks that we have generated ourselves.",
            "                assert event.type == EventTypes.Member",
            "                assert event.content[\"membership\"] == Membership.LEAVE",
            "            else:",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    # If we are persisting a batch of events the event(s) needed to auth the",
            "                    # current event may be part of the batch and will not be in the DB yet",
            "                    event_id_to_event = {e.event_id: e for e, _ in events_and_context}",
            "                    batched_auth_events = {}",
            "                    for event_id in event.auth_event_ids():",
            "                        auth_event = event_id_to_event.get(event_id)",
            "                        if auth_event:",
            "                            batched_auth_events[event_id] = auth_event",
            "                    await self._event_auth_handler.check_auth_rules_from_context(",
            "                        event, batched_auth_events",
            "                    )",
            "                except AuthError as err:",
            "                    logger.warning(\"Denying new event %r because %s\", event, err)",
            "                    raise err",
            "",
            "            # Ensure that we can round trip before trying to persist in db",
            "            try:",
            "                dump = json_encoder.encode(event.content)",
            "                json_decoder.decode(dump)",
            "            except Exception:",
            "                logger.exception(\"Failed to encode content: %r\", event.content)",
            "                raise",
            "",
            "        # We now persist the event (and update the cache in parallel, since we",
            "        # don't want to block on it).",
            "        #",
            "        # Note: mypy gets confused if we inline dl and check with twisted#11770.",
            "        # Some kind of bug in mypy's deduction?",
            "        deferreds = (",
            "            run_in_background(",
            "                self._persist_events,",
            "                requester=requester,",
            "                events_and_context=events_and_context,",
            "                ratelimit=ratelimit,",
            "                extra_users=extra_users,",
            "            ),",
            "            run_in_background(",
            "                self.cache_joined_hosts_for_events, events_and_context",
            "            ).addErrback(log_failure, \"cache_joined_hosts_for_event failed\"),",
            "        )",
            "        result, _ = await make_deferred_yieldable(",
            "            gather_results(deferreds, consumeErrors=True)",
            "        ).addErrback(unwrapFirstError)",
            "",
            "        return result",
            "",
            "    async def _persist_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Actually persists new events. Should only be called by",
            "        `handle_new_client_event`, and see its docstring for documentation of",
            "        the arguments. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "",
            "        await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "            events_and_context",
            "        )",
            "",
            "        try:",
            "            # If we're a worker we need to hit out to the master.",
            "            first_event, _ = events_and_context[0]",
            "            writer_instance = self._events_shard_config.get_instance(",
            "                first_event.room_id",
            "            )",
            "            if writer_instance != self._instance_name:",
            "                try:",
            "                    result = await self.send_events(",
            "                        instance_name=writer_instance,",
            "                        events_and_context=events_and_context,",
            "                        store=self.store,",
            "                        requester=requester,",
            "                        ratelimit=ratelimit,",
            "                        extra_users=extra_users,",
            "                    )",
            "                except SynapseError as e:",
            "                    if e.code == HTTPStatus.CONFLICT:",
            "                        raise PartialStateConflictError()",
            "                    raise",
            "                stream_id = result[\"stream_id\"]",
            "                event_id = result[\"event_id\"]",
            "",
            "                # If we batch persisted events we return the last persisted event, otherwise",
            "                # we return the one event that was persisted",
            "                event, _ = events_and_context[-1]",
            "",
            "                if event_id != event.event_id:",
            "                    # If we get a different event back then it means that its",
            "                    # been de-duplicated, so we replace the given event with the",
            "                    # one already persisted.",
            "                    event = await self.store.get_event(event_id)",
            "                else:",
            "                    # If we newly persisted the event then we need to update its",
            "                    # stream_ordering entry manually (as it was persisted on",
            "                    # another worker).",
            "                    event.internal_metadata.stream_ordering = stream_id",
            "                return event",
            "",
            "            event = await self.persist_and_notify_client_events(",
            "                requester,",
            "                events_and_context,",
            "                ratelimit=ratelimit,",
            "                extra_users=extra_users,",
            "            )",
            "",
            "            return event",
            "        except Exception:",
            "            for event, _ in events_and_context:",
            "                # Ensure that we actually remove the entries in the push actions",
            "                # staging area, if we calculated them.",
            "                await self.store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def cache_joined_hosts_for_events(",
            "        self, events_and_context: List[Tuple[EventBase, EventContext]]",
            "    ) -> None:",
            "        \"\"\"Precalculate the joined hosts at each of the given events, when using Redis, so that",
            "        external federation senders don't have to recalculate it themselves.",
            "        \"\"\"",
            "",
            "        if not self._external_cache.is_enabled():",
            "            return",
            "",
            "        # If external cache is enabled we should always have this.",
            "        assert self._external_cache_joined_hosts_updates is not None",
            "",
            "        for event, event_context in events_and_context:",
            "            if event_context.partial_state:",
            "                # To populate the cache for a partial-state event, we either have to",
            "                # block until full state, which the code below does, or change the",
            "                # meaning of cache values to be the list of hosts to which we plan to",
            "                # send events and calculate that instead.",
            "                #",
            "                # The federation senders don't use the external cache when sending",
            "                # events in partial-state rooms anyway, so let's not bother populating",
            "                # the cache.",
            "                continue",
            "",
            "            # We actually store two mappings, event ID -> prev state group,",
            "            # state group -> joined hosts, which is much more space efficient",
            "            # than event ID -> joined hosts.",
            "            #",
            "            # Note: We have to cache event ID -> prev state group, as we don't",
            "            # store that in the DB.",
            "            #",
            "            # Note: We set the state group -> joined hosts cache if it hasn't been",
            "            # set for a while, so that the expiry time is reset.",
            "",
            "            state_entry = await self.state.resolve_state_groups_for_events(",
            "                event.room_id, event_ids=event.prev_event_ids()",
            "            )",
            "",
            "            if state_entry.state_group:",
            "                await self._external_cache.set(",
            "                    \"event_to_prev_state_group\",",
            "                    event.event_id,",
            "                    state_entry.state_group,",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                if state_entry.state_group in self._external_cache_joined_hosts_updates:",
            "                    return",
            "",
            "                with opentracing.start_active_span(\"get_joined_hosts\"):",
            "                    joined_hosts = (",
            "                        await self._storage_controllers.state.get_joined_hosts(",
            "                            event.room_id, state_entry",
            "                        )",
            "                    )",
            "",
            "                # Note that the expiry times must be larger than the expiry time in",
            "                # _external_cache_joined_hosts_updates.",
            "                await self._external_cache.set(",
            "                    \"get_joined_hosts\",",
            "                    str(state_entry.state_group),",
            "                    list(joined_hosts),",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                self._external_cache_joined_hosts_updates[",
            "                    state_entry.state_group",
            "                ] = None",
            "",
            "    async def _validate_canonical_alias(",
            "        self,",
            "        directory_handler: DirectoryHandler,",
            "        room_alias_str: str,",
            "        expected_room_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Ensure that the given room alias points to the expected room ID.",
            "",
            "        Args:",
            "            directory_handler: The directory handler object.",
            "            room_alias_str: The room alias to check.",
            "            expected_room_id: The room ID that the alias should point to.",
            "        \"\"\"",
            "        room_alias = RoomAlias.from_string(room_alias_str)",
            "        try:",
            "            mapping = await directory_handler.get_association(room_alias)",
            "        except SynapseError as e:",
            "            # Turn M_NOT_FOUND errors into M_BAD_ALIAS errors.",
            "            if e.errcode == Codes.NOT_FOUND:",
            "                raise SynapseError(",
            "                    400,",
            "                    \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                    Codes.BAD_ALIAS,",
            "                )",
            "            raise",
            "",
            "        if mapping[\"room_id\"] != expected_room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                Codes.BAD_ALIAS,",
            "            )",
            "",
            "    async def persist_and_notify_client_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Called when we have fully built the events, have already",
            "        calculated the push actions for the events, and checked auth.",
            "",
            "        This should only be run on the instance in charge of persisting events.",
            "",
            "        Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        Returns:",
            "            The persisted event, if one event is passed in, or the last event in the",
            "            list in the case of batch persisting. If only one event was persisted, the",
            "            returned event may be different than the given event if it was de-duplicated",
            "            (e.g. because we had already persisted an event with the same transaction ID.)",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            assert self._events_shard_config.should_handle(",
            "                self._instance_name, event.room_id",
            "            )",
            "",
            "            if ratelimit:",
            "                # We check if this is a room admin redacting an event so that we",
            "                # can apply different ratelimiting. We do this by simply checking",
            "                # it's not a self-redaction (to avoid having to look up whether the",
            "                # user is actually admin or not).",
            "                is_admin_redaction = False",
            "                if event.type == EventTypes.Redaction:",
            "                    assert event.redacts is not None",
            "",
            "                    original_event = await self.store.get_event(",
            "                        event.redacts,",
            "                        redact_behaviour=EventRedactBehaviour.as_is,",
            "                        get_prev_content=False,",
            "                        allow_rejected=False,",
            "                        allow_none=True,",
            "                    )",
            "",
            "                    is_admin_redaction = bool(",
            "                        original_event and event.sender != original_event.sender",
            "                    )",
            "",
            "                await self.request_ratelimiter.ratelimit(",
            "                    requester, is_admin_redaction=is_admin_redaction",
            "                )",
            "",
            "            # run checks/actions on event based on type",
            "            if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "                (",
            "                    current_membership,",
            "                    _,",
            "                ) = await self.store.get_local_current_membership_for_user_in_room(",
            "                    event.state_key, event.room_id",
            "                )",
            "                if current_membership != Membership.JOIN:",
            "                    self._notifier.notify_user_joined_room(",
            "                        event.event_id, event.room_id",
            "                    )",
            "",
            "            if event.type == EventTypes.ServerACL:",
            "                self._storage_controllers.state.get_server_acl_for_room.invalidate(",
            "                    (event.room_id,)",
            "                )",
            "",
            "            await self._maybe_kick_guest_users(event, context)",
            "",
            "            if event.type == EventTypes.CanonicalAlias:",
            "                # Validate a newly added alias or newly added alt_aliases.",
            "",
            "                original_alias = None",
            "                original_alt_aliases: object = []",
            "",
            "                original_event_id = event.unsigned.get(\"replaces_state\")",
            "                if original_event_id:",
            "                    original_alias_event = await self.store.get_event(original_event_id)",
            "",
            "                    if original_alias_event:",
            "                        original_alias = original_alias_event.content.get(\"alias\", None)",
            "                        original_alt_aliases = original_alias_event.content.get(",
            "                            \"alt_aliases\", []",
            "                        )",
            "",
            "                # Check the alias is currently valid (if it has changed).",
            "                room_alias_str = event.content.get(\"alias\", None)",
            "                directory_handler = self.hs.get_directory_handler()",
            "                if room_alias_str and room_alias_str != original_alias:",
            "                    await self._validate_canonical_alias(",
            "                        directory_handler, room_alias_str, event.room_id",
            "                    )",
            "",
            "                # Check that alt_aliases is the proper form.",
            "                alt_aliases = event.content.get(\"alt_aliases\", [])",
            "                if not isinstance(alt_aliases, (list, tuple)):",
            "                    raise SynapseError(",
            "                        400,",
            "                        \"The alt_aliases property must be a list.\",",
            "                        Codes.INVALID_PARAM,",
            "                    )",
            "",
            "                # If the old version of alt_aliases is of an unknown form,",
            "                # completely replace it.",
            "                if not isinstance(original_alt_aliases, (list, tuple)):",
            "                    # TODO: check that the original_alt_aliases' entries are all strings",
            "                    original_alt_aliases = []",
            "",
            "                # Check that each alias is currently valid.",
            "                new_alt_aliases = set(alt_aliases) - set(original_alt_aliases)",
            "                if new_alt_aliases:",
            "                    for alias_str in new_alt_aliases:",
            "                        await self._validate_canonical_alias(",
            "                            directory_handler, alias_str, event.room_id",
            "                        )",
            "",
            "            federation_handler = self.hs.get_federation_handler()",
            "",
            "            if event.type == EventTypes.Member:",
            "                if event.content[\"membership\"] == Membership.INVITE:",
            "                    maybe_upsert_event_field(",
            "                        event,",
            "                        event.unsigned,",
            "                        \"invite_room_state\",",
            "                        await self.store.get_stripped_room_state_from_event_context(",
            "                            context,",
            "                            self.room_prejoin_state_types,",
            "                            membership_user_id=event.sender,",
            "                        ),",
            "                    )",
            "",
            "                    invitee = UserID.from_string(event.state_key)",
            "                    if not self.hs.is_mine(invitee):",
            "                        # TODO: Can we add signature from remote server in a nicer",
            "                        # way? If we have been invited by a remote server, we need",
            "                        # to get them to sign the event.",
            "",
            "                        returned_invite = await federation_handler.send_invite(",
            "                            invitee.domain, event",
            "                        )",
            "                        event.unsigned.pop(\"room_state\", None)",
            "",
            "                        # TODO: Make sure the signatures actually are correct.",
            "                        event.signatures.update(returned_invite.signatures)",
            "",
            "                if event.content[\"membership\"] == Membership.KNOCK:",
            "                    maybe_upsert_event_field(",
            "                        event,",
            "                        event.unsigned,",
            "                        \"knock_room_state\",",
            "                        await self.store.get_stripped_room_state_from_event_context(",
            "                            context,",
            "                            self.room_prejoin_state_types,",
            "                        ),",
            "                    )",
            "",
            "            if event.type == EventTypes.Redaction:",
            "                assert event.redacts is not None",
            "",
            "                original_event = await self.store.get_event(",
            "                    event.redacts,",
            "                    redact_behaviour=EventRedactBehaviour.as_is,",
            "                    get_prev_content=False,",
            "                    allow_rejected=False,",
            "                    allow_none=True,",
            "                )",
            "",
            "                room_version = await self.store.get_room_version_id(event.room_id)",
            "                room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "                # we can make some additional checks now if we have the original event.",
            "                if original_event:",
            "                    if original_event.type == EventTypes.Create:",
            "                        raise AuthError(403, \"Redacting create events is not permitted\")",
            "",
            "                    if original_event.room_id != event.room_id:",
            "                        raise SynapseError(",
            "                            400, \"Cannot redact event from a different room\"",
            "                        )",
            "",
            "                    if original_event.type == EventTypes.ServerACL:",
            "                        raise AuthError(",
            "                            403, \"Redacting server ACL events is not permitted\"",
            "                        )",
            "",
            "                event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "                prev_state_ids = await context.get_prev_state_ids(",
            "                    StateFilter.from_types(event_types)",
            "                )",
            "",
            "                auth_events_ids = self._event_auth_handler.compute_auth_events(",
            "                    event, prev_state_ids, for_verification=True",
            "                )",
            "                auth_events_map = await self.store.get_events(auth_events_ids)",
            "                auth_events = {",
            "                    (e.type, e.state_key): e for e in auth_events_map.values()",
            "                }",
            "",
            "                if event_auth.check_redaction(",
            "                    room_version_obj, event, auth_events=auth_events",
            "                ):",
            "                    # this user doesn't have 'redact' rights, so we need to do some more",
            "                    # checks on the original event. Let's start by checking the original",
            "                    # event exists.",
            "                    if not original_event:",
            "                        raise NotFoundError(",
            "                            \"Could not find event %s\" % (event.redacts,)",
            "                        )",
            "",
            "                    if event.user_id != original_event.user_id:",
            "                        raise AuthError(",
            "                            403, \"You don't have permission to redact events\"",
            "                        )",
            "",
            "                    # all the checks are done.",
            "                    event.internal_metadata.recheck_redaction = False",
            "",
            "            if event.type == EventTypes.Create:",
            "                prev_state_ids = await context.get_prev_state_ids()",
            "                if prev_state_ids:",
            "                    raise AuthError(403, \"Changing the room create event is forbidden\")",
            "",
            "        assert self._storage_controllers.persistence is not None",
            "        (",
            "            persisted_events,",
            "            max_stream_token,",
            "        ) = await self._storage_controllers.persistence.persist_events(",
            "            events_and_context,",
            "        )",
            "",
            "        events_and_pos = []",
            "        for event in persisted_events:",
            "            if self._ephemeral_events_enabled:",
            "                # If there's an expiry timestamp on the event, schedule its expiry.",
            "                self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            stream_ordering = event.internal_metadata.stream_ordering",
            "            assert stream_ordering is not None",
            "            pos = PersistedEventPosition(self._instance_name, stream_ordering)",
            "            events_and_pos.append((event, pos))",
            "",
            "            if event.type == EventTypes.Message:",
            "                # We don't want to block sending messages on any presence code. This",
            "                # matters as sometimes presence code can take a while.",
            "                run_as_background_process(",
            "                    \"bump_presence_active_time\",",
            "                    self._bump_active_time,",
            "                    requester.user,",
            "                    requester.device_id,",
            "                )",
            "",
            "        async def _notify() -> None:",
            "            try:",
            "                await self.notifier.on_new_room_events(",
            "                    events_and_pos, max_stream_token, extra_users=extra_users",
            "                )",
            "            except Exception:",
            "                logger.exception(\"Error notifying about new room events\")",
            "",
            "        run_in_background(_notify)",
            "",
            "        return persisted_events[-1]",
            "",
            "    async def _maybe_kick_guest_users(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state_ids = await context.get_current_state_ids()",
            "",
            "        # since this is a client-generated event, it cannot be an outlier and we must",
            "        # therefore have the state ids.",
            "        assert current_state_ids is not None",
            "        current_state_dict = await self.store.get_events(",
            "            list(current_state_ids.values())",
            "        )",
            "        current_state = list(current_state_dict.values())",
            "        logger.info(\"maybe_kick_guest_users %r\", current_state)",
            "        await self.hs.get_room_member_handler().kick_guest_users(current_state)",
            "",
            "    async def _bump_active_time(self, user: UserID, device_id: Optional[str]) -> None:",
            "        try:",
            "            presence = self.hs.get_presence_handler()",
            "            await presence.bump_presence_active_time(user, device_id)",
            "        except Exception:",
            "            logger.exception(\"Error bumping presence active time\")",
            "",
            "    async def _send_dummy_events_to_fill_extremities(self) -> None:",
            "        \"\"\"Background task to send dummy events into rooms that have a large",
            "        number of extremities",
            "        \"\"\"",
            "        self._expire_rooms_to_exclude_from_dummy_event_insertion()",
            "        room_ids = await self.store.get_rooms_with_many_extremities(",
            "            min_count=self._dummy_events_threshold,",
            "            limit=5,",
            "            room_id_filter=self._rooms_to_exclude_from_dummy_event_insertion.keys(),",
            "        )",
            "",
            "        for room_id in room_ids:",
            "            async with self._worker_lock_handler.acquire_read_write_lock(",
            "                NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "            ):",
            "                dummy_event_sent = await self._send_dummy_event_for_room(room_id)",
            "",
            "            if not dummy_event_sent:",
            "                # Did not find a valid user in the room, so remove from future attempts",
            "                # Exclusion is time limited, so the room will be rechecked in the future",
            "                # dependent on _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s. Will exclude it from \"",
            "                    \"future attempts until cache expires\" % (room_id,)",
            "                )",
            "                now = self.clock.time_msec()",
            "                self._rooms_to_exclude_from_dummy_event_insertion[room_id] = now",
            "",
            "    async def _send_dummy_event_for_room(self, room_id: str) -> bool:",
            "        \"\"\"Attempt to send a dummy event for the given room.",
            "",
            "        Args:",
            "            room_id: room to try to send an event from",
            "",
            "        Returns:",
            "            True if a dummy event was successfully sent. False if no user was able",
            "            to send an event.",
            "        \"\"\"",
            "",
            "        # For each room we need to find a joined member we can use to send",
            "        # the dummy event with.",
            "        members = await self.store.get_local_users_in_room(room_id)",
            "        for user_id in members:",
            "            requester = create_requester(user_id, authenticated_entity=self.server_name)",
            "            try:",
            "                # Try several times, it could fail with PartialStateConflictError",
            "                # in handle_new_client_event, cf comment in except block.",
            "                max_retries = 5",
            "                for i in range(max_retries):",
            "                    try:",
            "                        event, unpersisted_context = await self.create_event(",
            "                            requester,",
            "                            {",
            "                                \"type\": EventTypes.Dummy,",
            "                                \"content\": {},",
            "                                \"room_id\": room_id,",
            "                                \"sender\": user_id,",
            "                            },",
            "                        )",
            "                        context = await unpersisted_context.persist(event)",
            "",
            "                        event.internal_metadata.proactively_send = False",
            "",
            "                        # Since this is a dummy-event it is OK if it is sent by a",
            "                        # shadow-banned user.",
            "                        await self.handle_new_client_event(",
            "                            requester,",
            "                            events_and_context=[(event, context)],",
            "                            ratelimit=False,",
            "                            ignore_shadow_ban=True,",
            "                        )",
            "",
            "                        break",
            "                    except PartialStateConflictError as e:",
            "                        # Persisting couldn't happen because the room got un-partial stated",
            "                        # in the meantime and context needs to be recomputed, so let's do so.",
            "                        if i == max_retries - 1:",
            "                            raise e",
            "                        pass",
            "                return True",
            "            except AuthError:",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s for user %s due to \"",
            "                    \"lack of power. Will try another user\" % (room_id, user_id)",
            "                )",
            "        return False",
            "",
            "    def _expire_rooms_to_exclude_from_dummy_event_insertion(self) -> None:",
            "        expire_before = self.clock.time_msec() - _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "        to_expire = set()",
            "        for room_id, time in self._rooms_to_exclude_from_dummy_event_insertion.items():",
            "            if time < expire_before:",
            "                to_expire.add(room_id)",
            "        for room_id in to_expire:",
            "            logger.debug(",
            "                \"Expiring room id %s from dummy event insertion exclusion cache\",",
            "                room_id,",
            "            )",
            "            del self._rooms_to_exclude_from_dummy_event_insertion[room_id]",
            "",
            "    async def _rebuild_event_after_third_party_rules(",
            "        self, third_party_result: dict, original_event: EventBase",
            "    ) -> Tuple[EventBase, UnpersistedEventContextBase]:",
            "        # the third_party_event_rules want to replace the event.",
            "        # we do some basic checks, and then return the replacement event.",
            "",
            "        # Construct a new EventBuilder and validate it, which helps with the",
            "        # rest of these checks.",
            "        try:",
            "            builder = self.event_builder_factory.for_room_version(",
            "                original_event.room_version, third_party_result",
            "            )",
            "            self.validator.validate_builder(builder)",
            "        except SynapseError as e:",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \" + e.msg,",
            "            )",
            "",
            "        immutable_fields = [",
            "            # changing the room is going to break things: we've already checked that the",
            "            # room exists, and are holding a concurrency limiter token for that room.",
            "            # Also, we might need to use a different room version.",
            "            \"room_id\",",
            "            # changing the type or state key might work, but we'd need to check that the",
            "            # calling functions aren't making assumptions about them.",
            "            \"type\",",
            "            \"state_key\",",
            "        ]",
            "",
            "        for k in immutable_fields:",
            "            if getattr(builder, k, None) != original_event.get(k):",
            "                raise Exception(",
            "                    \"Third party rules module created an invalid event: \"",
            "                    \"cannot change field \" + k",
            "                )",
            "",
            "        # check that the new sender belongs to this HS",
            "        if not self.hs.is_mine_id(builder.sender):",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \"",
            "                \"invalid sender \" + builder.sender",
            "            )",
            "",
            "        # copy over the original internal metadata",
            "        for k, v in original_event.internal_metadata.get_dict().items():",
            "            setattr(builder.internal_metadata, k, v)",
            "",
            "        # modules can send new state events, so we re-calculate the auth events just in",
            "        # case.",
            "        prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        event = await builder.build(",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=None,",
            "        )",
            "",
            "        # we rebuild the event context, to be on the safe side. If nothing else,",
            "        # delta_ids might need an update.",
            "        context = await self.state.calculate_context_info(event)",
            "",
            "        return event, context"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.handlers.message.EventCreationHandler.self"
        ]
    },
    "synapse/replication/tcp/client.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": 205,
                "PatchRowcode": "                     self.notifier.notify_user_joined_room("
            },
            "1": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": 206,
                "PatchRowcode": "                         row.data.event_id, row.data.room_id"
            },
            "2": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": 207,
                "PatchRowcode": "                     )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 208,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 209,
                "PatchRowcode": "+                # If this is a server ACL event, clear the cache in the storage controller."
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 210,
                "PatchRowcode": "+                if row.data.type == EventTypes.ServerACL:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+                    self._state_storage_controller.get_server_acl_for_room.invalidate("
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 212,
                "PatchRowcode": "+                        (row.data.room_id,)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 213,
                "PatchRowcode": "+                    )"
            },
            "9": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "         elif stream_name == UnPartialStatedRoomStream.NAME:"
            },
            "10": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "             for row in rows:"
            },
            "11": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 216,
                "PatchRowcode": "                 assert isinstance(row, UnPartialStatedRoomStreamRow)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2017 Vector Creations Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "\"\"\"A replication client for use by synapse workers.",
            "\"\"\"",
            "import logging",
            "from typing import TYPE_CHECKING, Dict, Iterable, Optional, Set, Tuple",
            "",
            "from sortedcontainers import SortedList",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.defer import Deferred",
            "",
            "from synapse.api.constants import EventTypes, Membership, ReceiptTypes",
            "from synapse.federation import send_queue",
            "from synapse.federation.sender import FederationSender",
            "from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.tcp.streams import (",
            "    AccountDataStream,",
            "    DeviceListsStream,",
            "    PushersStream,",
            "    PushRulesStream,",
            "    ReceiptsStream,",
            "    ToDeviceStream,",
            "    TypingStream,",
            "    UnPartialStatedEventStream,",
            "    UnPartialStatedRoomStream,",
            ")",
            "from synapse.replication.tcp.streams.events import (",
            "    EventsStream,",
            "    EventsStreamEventRow,",
            "    EventsStreamRow,",
            ")",
            "from synapse.replication.tcp.streams.partial_state import (",
            "    UnPartialStatedEventStreamRow,",
            "    UnPartialStatedRoomStreamRow,",
            ")",
            "from synapse.types import PersistedEventPosition, ReadReceipt, StreamKeyType, UserID",
            "from synapse.util.async_helpers import Linearizer, timeout_deferred",
            "from synapse.util.metrics import Measure",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# How long we allow callers to wait for replication updates before timing out.",
            "_WAIT_FOR_REPLICATION_TIMEOUT_SECONDS = 5",
            "",
            "",
            "class ReplicationDataHandler:",
            "    \"\"\"Handles incoming stream updates from replication.",
            "",
            "    This instance notifies the data store about updates. Can be subclassed",
            "    to handle updates in additional ways.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self.notifier = hs.get_notifier()",
            "        self._reactor = hs.get_reactor()",
            "        self._clock = hs.get_clock()",
            "        self._streams = hs.get_replication_streams()",
            "        self._instance_name = hs.get_instance_name()",
            "        self._typing_handler = hs.get_typing_handler()",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self._notify_pushers = hs.config.worker.start_pushers",
            "        self._pusher_pool = hs.get_pusherpool()",
            "        self._presence_handler = hs.get_presence_handler()",
            "",
            "        self.send_handler: Optional[FederationSenderHandler] = None",
            "        if hs.should_send_federation():",
            "            self.send_handler = FederationSenderHandler(hs)",
            "",
            "        # Map from stream and instance to list of deferreds waiting for the stream to",
            "        # arrive at a particular position. The lists are sorted by stream position.",
            "        self._streams_to_waiters: Dict[",
            "            Tuple[str, str], SortedList[Tuple[int, Deferred]]",
            "        ] = {}",
            "",
            "    async def on_rdata(",
            "        self, stream_name: str, instance_name: str, token: int, rows: list",
            "    ) -> None:",
            "        \"\"\"Called to handle a batch of replication data with a given stream token.",
            "",
            "        By default, this just pokes the data store. Can be overridden in subclasses to",
            "        handle more.",
            "",
            "        Args:",
            "            stream_name: name of the replication stream for this batch of rows",
            "            instance_name: the instance that wrote the rows.",
            "            token: stream token for this batch of rows",
            "            rows: a list of Stream.ROW_TYPE objects as returned by Stream.parse_row.",
            "        \"\"\"",
            "        self.store.process_replication_rows(stream_name, instance_name, token, rows)",
            "        # NOTE: this must be called after process_replication_rows to ensure any",
            "        # cache invalidations are first handled before any stream ID advances.",
            "        self.store.process_replication_position(stream_name, instance_name, token)",
            "",
            "        if self.send_handler:",
            "            await self.send_handler.process_replication_rows(stream_name, token, rows)",
            "",
            "        if stream_name == TypingStream.NAME:",
            "            self._typing_handler.process_replication_rows(token, rows)",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.TYPING, token, rooms=[row.room_id for row in rows]",
            "            )",
            "        elif stream_name == PushRulesStream.NAME:",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.PUSH_RULES, token, users=[row.user_id for row in rows]",
            "            )",
            "        elif stream_name in AccountDataStream.NAME:",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.ACCOUNT_DATA, token, users=[row.user_id for row in rows]",
            "            )",
            "        elif stream_name == ReceiptsStream.NAME:",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.RECEIPT, token, rooms=[row.room_id for row in rows]",
            "            )",
            "            await self._pusher_pool.on_new_receipts(",
            "                token, token, {row.room_id for row in rows}",
            "            )",
            "        elif stream_name == ToDeviceStream.NAME:",
            "            entities = [row.entity for row in rows if row.entity.startswith(\"@\")]",
            "            if entities:",
            "                self.notifier.on_new_event(",
            "                    StreamKeyType.TO_DEVICE, token, users=entities",
            "                )",
            "        elif stream_name == DeviceListsStream.NAME:",
            "            all_room_ids: Set[str] = set()",
            "            for row in rows:",
            "                if row.entity.startswith(\"@\") and not row.is_signature:",
            "                    room_ids = await self.store.get_rooms_for_user(row.entity)",
            "                    all_room_ids.update(room_ids)",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.DEVICE_LIST, token, rooms=all_room_ids",
            "            )",
            "        elif stream_name == PushersStream.NAME:",
            "            for row in rows:",
            "                if row.deleted:",
            "                    self.stop_pusher(row.user_id, row.app_id, row.pushkey)",
            "                else:",
            "                    await self.process_pusher_change(",
            "                        row.user_id, row.app_id, row.pushkey",
            "                    )",
            "        elif stream_name == EventsStream.NAME:",
            "            # We shouldn't get multiple rows per token for events stream, so",
            "            # we don't need to optimise this for multiple rows.",
            "            for row in rows:",
            "                if row.type != EventsStreamEventRow.TypeId:",
            "                    # The row's data is an `EventsStreamCurrentStateRow`.",
            "                    # When we recompute the current state of a room based on forward",
            "                    # extremities (see `update_current_state`), no new events are",
            "                    # persisted, so we must poke the replication callbacks ourselves.",
            "                    # This functionality is used when finishing up a partial state join.",
            "                    self.notifier.notify_replication()",
            "                    continue",
            "                assert isinstance(row, EventsStreamRow)",
            "                assert isinstance(row.data, EventsStreamEventRow)",
            "",
            "                if row.data.rejected:",
            "                    continue",
            "",
            "                extra_users: Tuple[UserID, ...] = ()",
            "                if row.data.type == EventTypes.Member and row.data.state_key:",
            "                    extra_users = (UserID.from_string(row.data.state_key),)",
            "",
            "                max_token = self.store.get_room_max_token()",
            "                event_pos = PersistedEventPosition(instance_name, token)",
            "                event_entry = self.notifier.create_pending_room_event_entry(",
            "                    event_pos,",
            "                    extra_users,",
            "                    row.data.room_id,",
            "                    row.data.type,",
            "                    row.data.state_key,",
            "                    row.data.membership,",
            "                )",
            "                await self.notifier.notify_new_room_events(",
            "                    [(event_entry, row.data.event_id)], max_token",
            "                )",
            "",
            "                # If this event is a join, make a note of it so we have an accurate",
            "                # cross-worker room rate limit.",
            "                # TODO: Erik said we should exclude rows that came from ex_outliers",
            "                #  here, but I don't see how we can determine that. I guess we could",
            "                #  add a flag to row.data?",
            "                if (",
            "                    row.data.type == EventTypes.Member",
            "                    and row.data.membership == Membership.JOIN",
            "                    and not row.data.outlier",
            "                ):",
            "                    # TODO retrieve the previous state, and exclude join -> join transitions",
            "                    self.notifier.notify_user_joined_room(",
            "                        row.data.event_id, row.data.room_id",
            "                    )",
            "        elif stream_name == UnPartialStatedRoomStream.NAME:",
            "            for row in rows:",
            "                assert isinstance(row, UnPartialStatedRoomStreamRow)",
            "",
            "                # Wake up any tasks waiting for the room to be un-partial-stated.",
            "                self._state_storage_controller.notify_room_un_partial_stated(",
            "                    row.room_id",
            "                )",
            "                await self.notifier.on_un_partial_stated_room(row.room_id, token)",
            "        elif stream_name == UnPartialStatedEventStream.NAME:",
            "            for row in rows:",
            "                assert isinstance(row, UnPartialStatedEventStreamRow)",
            "",
            "                # Wake up any tasks waiting for the event to be un-partial-stated.",
            "                self._state_storage_controller.notify_event_un_partial_stated(",
            "                    row.event_id",
            "                )",
            "",
            "        await self._presence_handler.process_replication_rows(",
            "            stream_name, instance_name, token, rows",
            "        )",
            "",
            "        # Notify any waiting deferreds. The list is ordered by position so we",
            "        # just iterate through the list until we reach a position that is",
            "        # greater than the received row position.",
            "        waiting_list = self._streams_to_waiters.get((stream_name, instance_name))",
            "        if not waiting_list:",
            "            return",
            "",
            "        # Index of first item with a position after the current token, i.e we",
            "        # have called all deferreds before this index. If not overwritten by",
            "        # loop below means either a) no items in list so no-op or b) all items",
            "        # in list were called and so the list should be cleared. Setting it to",
            "        # `len(list)` works for both cases.",
            "        index_of_first_deferred_not_called = len(waiting_list)",
            "",
            "        # We don't fire the deferreds until after we finish iterating over the",
            "        # list, to avoid the list changing when we fire the deferreds.",
            "        deferreds_to_callback = []",
            "",
            "        for idx, (position, deferred) in enumerate(waiting_list):",
            "            if position <= token:",
            "                deferreds_to_callback.append(deferred)",
            "            else:",
            "                # The list is sorted by position so we don't need to continue",
            "                # checking any further entries in the list.",
            "                index_of_first_deferred_not_called = idx",
            "                break",
            "",
            "        # Drop all entries in the waiting list that were called in the above",
            "        # loop. (This maintains the order so no need to resort)",
            "        del waiting_list[:index_of_first_deferred_not_called]",
            "",
            "        for deferred in deferreds_to_callback:",
            "            try:",
            "                with PreserveLoggingContext():",
            "                    deferred.callback(None)",
            "            except Exception:",
            "                # The deferred has been cancelled or timed out.",
            "                pass",
            "",
            "    async def on_position(",
            "        self, stream_name: str, instance_name: str, token: int",
            "    ) -> None:",
            "        await self.on_rdata(stream_name, instance_name, token, [])",
            "",
            "        # We poke the generic \"replication\" notifier to wake anything up that",
            "        # may be streaming.",
            "        self.notifier.notify_replication()",
            "",
            "    def on_remote_server_up(self, server: str) -> None:",
            "        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"",
            "",
            "        # Let's wake up the transaction queue for the server in case we have",
            "        # pending stuff to send to it.",
            "        if self.send_handler:",
            "            self.send_handler.wake_destination(server)",
            "",
            "    async def wait_for_stream_position(",
            "        self,",
            "        instance_name: str,",
            "        stream_name: str,",
            "        position: int,",
            "    ) -> None:",
            "        \"\"\"Wait until this instance has received updates up to and including",
            "        the given stream position.",
            "",
            "        Args:",
            "            instance_name",
            "            stream_name",
            "            position",
            "        \"\"\"",
            "",
            "        if instance_name == self._instance_name:",
            "            # We don't get told about updates written by this process, and",
            "            # anyway in that case we don't need to wait.",
            "            return",
            "",
            "        current_position = self._streams[stream_name].current_token(instance_name)",
            "        if position <= current_position:",
            "            # We're already past the position",
            "            return",
            "",
            "        # Create a new deferred that times out after N seconds, as we don't want",
            "        # to wedge here forever.",
            "        deferred: \"Deferred[None]\" = Deferred()",
            "        deferred = timeout_deferred(",
            "            deferred, _WAIT_FOR_REPLICATION_TIMEOUT_SECONDS, self._reactor",
            "        )",
            "",
            "        waiting_list = self._streams_to_waiters.setdefault(",
            "            (stream_name, instance_name), SortedList(key=lambda t: t[0])",
            "        )",
            "",
            "        waiting_list.add((position, deferred))",
            "",
            "        # We measure here to get in flight counts and average waiting time.",
            "        with Measure(self._clock, \"repl.wait_for_stream_position\"):",
            "            logger.info(",
            "                \"Waiting for repl stream %r to reach %s (%s); currently at: %s\",",
            "                stream_name,",
            "                position,",
            "                instance_name,",
            "                current_position,",
            "            )",
            "            try:",
            "                await make_deferred_yieldable(deferred)",
            "            except defer.TimeoutError:",
            "                logger.error(",
            "                    \"Timed out waiting for repl stream %r to reach %s (%s)\"",
            "                    \"; currently at: %s\",",
            "                    stream_name,",
            "                    position,",
            "                    instance_name,",
            "                    self._streams[stream_name].current_token(instance_name),",
            "                )",
            "                return",
            "",
            "            logger.info(",
            "                \"Finished waiting for repl stream %r to reach %s (%s)\",",
            "                stream_name,",
            "                position,",
            "                instance_name,",
            "            )",
            "",
            "    def stop_pusher(self, user_id: str, app_id: str, pushkey: str) -> None:",
            "        if not self._notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        pushers_for_user = self._pusher_pool.pushers.get(user_id, {})",
            "        pusher = pushers_for_user.pop(key, None)",
            "        if pusher is None:",
            "            return",
            "        logger.info(\"Stopping pusher %r / %r\", user_id, key)",
            "        pusher.on_stop()",
            "",
            "    async def process_pusher_change(",
            "        self, user_id: str, app_id: str, pushkey: str",
            "    ) -> None:",
            "        if not self._notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        logger.info(\"Starting pusher %r / %r\", user_id, key)",
            "        await self._pusher_pool.process_pusher_change_by_id(app_id, pushkey, user_id)",
            "",
            "",
            "class FederationSenderHandler:",
            "    \"\"\"Processes the fedration replication stream",
            "",
            "    This class is only instantiate on the worker responsible for sending outbound",
            "    federation transactions. It receives rows from the replication stream and forwards",
            "    the appropriate entries to the FederationSender class.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        assert hs.should_send_federation()",
            "",
            "        self.store = hs.get_datastores().main",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._hs = hs",
            "",
            "        # We need to make a temporary value to ensure that mypy picks up the",
            "        # right type. We know we should have a federation sender instance since",
            "        # `should_send_federation` is True.",
            "        sender = hs.get_federation_sender()",
            "        assert isinstance(sender, FederationSender)",
            "        self.federation_sender = sender",
            "",
            "        # Stores the latest position in the federation stream we've gotten up",
            "        # to. This is always set before we use it.",
            "        self.federation_position: Optional[int] = None",
            "",
            "        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")",
            "",
            "    def wake_destination(self, server: str) -> None:",
            "        self.federation_sender.wake_destination(server)",
            "",
            "    async def process_replication_rows(",
            "        self, stream_name: str, token: int, rows: list",
            "    ) -> None:",
            "        # The federation stream contains things that we want to send out, e.g.",
            "        # presence, typing, etc.",
            "        if stream_name == \"federation\":",
            "            await send_queue.process_rows_for_federation(self.federation_sender, rows)",
            "            await self.update_token(token)",
            "",
            "        # ... and when new receipts happen",
            "        elif stream_name == ReceiptsStream.NAME:",
            "            await self._on_new_receipts(rows)",
            "",
            "        # ... as well as device updates and messages",
            "        elif stream_name == DeviceListsStream.NAME:",
            "            # The entities are either user IDs (starting with '@') whose devices",
            "            # have changed, or remote servers that we need to tell about",
            "            # changes.",
            "            hosts = {",
            "                row.entity",
            "                for row in rows",
            "                if not row.entity.startswith(\"@\") and not row.is_signature",
            "            }",
            "            await self.federation_sender.send_device_messages(hosts, immediate=False)",
            "",
            "        elif stream_name == ToDeviceStream.NAME:",
            "            # The to_device stream includes stuff to be pushed to both local",
            "            # clients and remote servers, so we ignore entities that start with",
            "            # '@' (since they'll be local users rather than destinations).",
            "            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}",
            "            await self.federation_sender.send_device_messages(hosts)",
            "",
            "    async def _on_new_receipts(",
            "        self, rows: Iterable[ReceiptsStream.ReceiptsStreamRow]",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            rows: new receipts to be processed",
            "        \"\"\"",
            "        for receipt in rows:",
            "            # we only want to send on receipts for our own users",
            "            if not self._is_mine_id(receipt.user_id):",
            "                continue",
            "            # Private read receipts never get sent over federation.",
            "            if receipt.receipt_type == ReceiptTypes.READ_PRIVATE:",
            "                continue",
            "            receipt_info = ReadReceipt(",
            "                receipt.room_id,",
            "                receipt.receipt_type,",
            "                receipt.user_id,",
            "                [receipt.event_id],",
            "                thread_id=receipt.thread_id,",
            "                data=receipt.data,",
            "            )",
            "            await self.federation_sender.send_read_receipt(receipt_info)",
            "",
            "    async def update_token(self, token: int) -> None:",
            "        \"\"\"Update the record of where we have processed to in the federation stream.",
            "",
            "        Called after we have processed a an update received over replication. Sends",
            "        a FEDERATION_ACK back to the master, and stores the token that we have processed",
            "         in `federation_stream_position` so that we can restart where we left off.",
            "        \"\"\"",
            "        self.federation_position = token",
            "",
            "        # We save and send the ACK to master asynchronously, so we don't block",
            "        # processing on persistence. We don't need to do this operation for",
            "        # every single RDATA we receive, we just need to do it periodically.",
            "",
            "        if self._fed_position_linearizer.is_queued(None):",
            "            # There is already a task queued up to save and send the token, so",
            "            # no need to queue up another task.",
            "            return",
            "",
            "        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)",
            "",
            "    async def _save_and_send_ack(self) -> None:",
            "        \"\"\"Save the current federation position in the database and send an ACK",
            "        to master with where we're up to.",
            "        \"\"\"",
            "        # We should only be calling this once we've got a token.",
            "        assert self.federation_position is not None",
            "",
            "        try:",
            "            # We linearize here to ensure we don't have races updating the token",
            "            #",
            "            # XXX this appears to be redundant, since the ReplicationCommandHandler",
            "            # has a linearizer which ensures that we only process one line of",
            "            # replication data at a time. Should we remove it, or is it doing useful",
            "            # service for robustness? Or could we replace it with an assertion that",
            "            # we're not being re-entered?",
            "",
            "            async with self._fed_position_linearizer.queue(None):",
            "                # We persist and ack the same position, so we take a copy of it",
            "                # here as otherwise it can get modified from underneath us.",
            "                current_position = self.federation_position",
            "",
            "                await self.store.update_federation_out_pos(",
            "                    \"federation\", current_position",
            "                )",
            "",
            "                # We ACK this token over replication so that the master can drop",
            "                # its in memory queues",
            "                self._hs.get_replication_command_handler().send_federation_ack(",
            "                    current_position",
            "                )",
            "        except Exception:",
            "            logger.exception(\"Error updating federation stream position\")"
        ],
        "afterPatchFile": [
            "# Copyright 2017 Vector Creations Ltd",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "\"\"\"A replication client for use by synapse workers.",
            "\"\"\"",
            "import logging",
            "from typing import TYPE_CHECKING, Dict, Iterable, Optional, Set, Tuple",
            "",
            "from sortedcontainers import SortedList",
            "",
            "from twisted.internet import defer",
            "from twisted.internet.defer import Deferred",
            "",
            "from synapse.api.constants import EventTypes, Membership, ReceiptTypes",
            "from synapse.federation import send_queue",
            "from synapse.federation.sender import FederationSender",
            "from synapse.logging.context import PreserveLoggingContext, make_deferred_yieldable",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.tcp.streams import (",
            "    AccountDataStream,",
            "    DeviceListsStream,",
            "    PushersStream,",
            "    PushRulesStream,",
            "    ReceiptsStream,",
            "    ToDeviceStream,",
            "    TypingStream,",
            "    UnPartialStatedEventStream,",
            "    UnPartialStatedRoomStream,",
            ")",
            "from synapse.replication.tcp.streams.events import (",
            "    EventsStream,",
            "    EventsStreamEventRow,",
            "    EventsStreamRow,",
            ")",
            "from synapse.replication.tcp.streams.partial_state import (",
            "    UnPartialStatedEventStreamRow,",
            "    UnPartialStatedRoomStreamRow,",
            ")",
            "from synapse.types import PersistedEventPosition, ReadReceipt, StreamKeyType, UserID",
            "from synapse.util.async_helpers import Linearizer, timeout_deferred",
            "from synapse.util.metrics import Measure",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "# How long we allow callers to wait for replication updates before timing out.",
            "_WAIT_FOR_REPLICATION_TIMEOUT_SECONDS = 5",
            "",
            "",
            "class ReplicationDataHandler:",
            "    \"\"\"Handles incoming stream updates from replication.",
            "",
            "    This instance notifies the data store about updates. Can be subclassed",
            "    to handle updates in additional ways.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self.notifier = hs.get_notifier()",
            "        self._reactor = hs.get_reactor()",
            "        self._clock = hs.get_clock()",
            "        self._streams = hs.get_replication_streams()",
            "        self._instance_name = hs.get_instance_name()",
            "        self._typing_handler = hs.get_typing_handler()",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self._notify_pushers = hs.config.worker.start_pushers",
            "        self._pusher_pool = hs.get_pusherpool()",
            "        self._presence_handler = hs.get_presence_handler()",
            "",
            "        self.send_handler: Optional[FederationSenderHandler] = None",
            "        if hs.should_send_federation():",
            "            self.send_handler = FederationSenderHandler(hs)",
            "",
            "        # Map from stream and instance to list of deferreds waiting for the stream to",
            "        # arrive at a particular position. The lists are sorted by stream position.",
            "        self._streams_to_waiters: Dict[",
            "            Tuple[str, str], SortedList[Tuple[int, Deferred]]",
            "        ] = {}",
            "",
            "    async def on_rdata(",
            "        self, stream_name: str, instance_name: str, token: int, rows: list",
            "    ) -> None:",
            "        \"\"\"Called to handle a batch of replication data with a given stream token.",
            "",
            "        By default, this just pokes the data store. Can be overridden in subclasses to",
            "        handle more.",
            "",
            "        Args:",
            "            stream_name: name of the replication stream for this batch of rows",
            "            instance_name: the instance that wrote the rows.",
            "            token: stream token for this batch of rows",
            "            rows: a list of Stream.ROW_TYPE objects as returned by Stream.parse_row.",
            "        \"\"\"",
            "        self.store.process_replication_rows(stream_name, instance_name, token, rows)",
            "        # NOTE: this must be called after process_replication_rows to ensure any",
            "        # cache invalidations are first handled before any stream ID advances.",
            "        self.store.process_replication_position(stream_name, instance_name, token)",
            "",
            "        if self.send_handler:",
            "            await self.send_handler.process_replication_rows(stream_name, token, rows)",
            "",
            "        if stream_name == TypingStream.NAME:",
            "            self._typing_handler.process_replication_rows(token, rows)",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.TYPING, token, rooms=[row.room_id for row in rows]",
            "            )",
            "        elif stream_name == PushRulesStream.NAME:",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.PUSH_RULES, token, users=[row.user_id for row in rows]",
            "            )",
            "        elif stream_name in AccountDataStream.NAME:",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.ACCOUNT_DATA, token, users=[row.user_id for row in rows]",
            "            )",
            "        elif stream_name == ReceiptsStream.NAME:",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.RECEIPT, token, rooms=[row.room_id for row in rows]",
            "            )",
            "            await self._pusher_pool.on_new_receipts(",
            "                token, token, {row.room_id for row in rows}",
            "            )",
            "        elif stream_name == ToDeviceStream.NAME:",
            "            entities = [row.entity for row in rows if row.entity.startswith(\"@\")]",
            "            if entities:",
            "                self.notifier.on_new_event(",
            "                    StreamKeyType.TO_DEVICE, token, users=entities",
            "                )",
            "        elif stream_name == DeviceListsStream.NAME:",
            "            all_room_ids: Set[str] = set()",
            "            for row in rows:",
            "                if row.entity.startswith(\"@\") and not row.is_signature:",
            "                    room_ids = await self.store.get_rooms_for_user(row.entity)",
            "                    all_room_ids.update(room_ids)",
            "            self.notifier.on_new_event(",
            "                StreamKeyType.DEVICE_LIST, token, rooms=all_room_ids",
            "            )",
            "        elif stream_name == PushersStream.NAME:",
            "            for row in rows:",
            "                if row.deleted:",
            "                    self.stop_pusher(row.user_id, row.app_id, row.pushkey)",
            "                else:",
            "                    await self.process_pusher_change(",
            "                        row.user_id, row.app_id, row.pushkey",
            "                    )",
            "        elif stream_name == EventsStream.NAME:",
            "            # We shouldn't get multiple rows per token for events stream, so",
            "            # we don't need to optimise this for multiple rows.",
            "            for row in rows:",
            "                if row.type != EventsStreamEventRow.TypeId:",
            "                    # The row's data is an `EventsStreamCurrentStateRow`.",
            "                    # When we recompute the current state of a room based on forward",
            "                    # extremities (see `update_current_state`), no new events are",
            "                    # persisted, so we must poke the replication callbacks ourselves.",
            "                    # This functionality is used when finishing up a partial state join.",
            "                    self.notifier.notify_replication()",
            "                    continue",
            "                assert isinstance(row, EventsStreamRow)",
            "                assert isinstance(row.data, EventsStreamEventRow)",
            "",
            "                if row.data.rejected:",
            "                    continue",
            "",
            "                extra_users: Tuple[UserID, ...] = ()",
            "                if row.data.type == EventTypes.Member and row.data.state_key:",
            "                    extra_users = (UserID.from_string(row.data.state_key),)",
            "",
            "                max_token = self.store.get_room_max_token()",
            "                event_pos = PersistedEventPosition(instance_name, token)",
            "                event_entry = self.notifier.create_pending_room_event_entry(",
            "                    event_pos,",
            "                    extra_users,",
            "                    row.data.room_id,",
            "                    row.data.type,",
            "                    row.data.state_key,",
            "                    row.data.membership,",
            "                )",
            "                await self.notifier.notify_new_room_events(",
            "                    [(event_entry, row.data.event_id)], max_token",
            "                )",
            "",
            "                # If this event is a join, make a note of it so we have an accurate",
            "                # cross-worker room rate limit.",
            "                # TODO: Erik said we should exclude rows that came from ex_outliers",
            "                #  here, but I don't see how we can determine that. I guess we could",
            "                #  add a flag to row.data?",
            "                if (",
            "                    row.data.type == EventTypes.Member",
            "                    and row.data.membership == Membership.JOIN",
            "                    and not row.data.outlier",
            "                ):",
            "                    # TODO retrieve the previous state, and exclude join -> join transitions",
            "                    self.notifier.notify_user_joined_room(",
            "                        row.data.event_id, row.data.room_id",
            "                    )",
            "",
            "                # If this is a server ACL event, clear the cache in the storage controller.",
            "                if row.data.type == EventTypes.ServerACL:",
            "                    self._state_storage_controller.get_server_acl_for_room.invalidate(",
            "                        (row.data.room_id,)",
            "                    )",
            "        elif stream_name == UnPartialStatedRoomStream.NAME:",
            "            for row in rows:",
            "                assert isinstance(row, UnPartialStatedRoomStreamRow)",
            "",
            "                # Wake up any tasks waiting for the room to be un-partial-stated.",
            "                self._state_storage_controller.notify_room_un_partial_stated(",
            "                    row.room_id",
            "                )",
            "                await self.notifier.on_un_partial_stated_room(row.room_id, token)",
            "        elif stream_name == UnPartialStatedEventStream.NAME:",
            "            for row in rows:",
            "                assert isinstance(row, UnPartialStatedEventStreamRow)",
            "",
            "                # Wake up any tasks waiting for the event to be un-partial-stated.",
            "                self._state_storage_controller.notify_event_un_partial_stated(",
            "                    row.event_id",
            "                )",
            "",
            "        await self._presence_handler.process_replication_rows(",
            "            stream_name, instance_name, token, rows",
            "        )",
            "",
            "        # Notify any waiting deferreds. The list is ordered by position so we",
            "        # just iterate through the list until we reach a position that is",
            "        # greater than the received row position.",
            "        waiting_list = self._streams_to_waiters.get((stream_name, instance_name))",
            "        if not waiting_list:",
            "            return",
            "",
            "        # Index of first item with a position after the current token, i.e we",
            "        # have called all deferreds before this index. If not overwritten by",
            "        # loop below means either a) no items in list so no-op or b) all items",
            "        # in list were called and so the list should be cleared. Setting it to",
            "        # `len(list)` works for both cases.",
            "        index_of_first_deferred_not_called = len(waiting_list)",
            "",
            "        # We don't fire the deferreds until after we finish iterating over the",
            "        # list, to avoid the list changing when we fire the deferreds.",
            "        deferreds_to_callback = []",
            "",
            "        for idx, (position, deferred) in enumerate(waiting_list):",
            "            if position <= token:",
            "                deferreds_to_callback.append(deferred)",
            "            else:",
            "                # The list is sorted by position so we don't need to continue",
            "                # checking any further entries in the list.",
            "                index_of_first_deferred_not_called = idx",
            "                break",
            "",
            "        # Drop all entries in the waiting list that were called in the above",
            "        # loop. (This maintains the order so no need to resort)",
            "        del waiting_list[:index_of_first_deferred_not_called]",
            "",
            "        for deferred in deferreds_to_callback:",
            "            try:",
            "                with PreserveLoggingContext():",
            "                    deferred.callback(None)",
            "            except Exception:",
            "                # The deferred has been cancelled or timed out.",
            "                pass",
            "",
            "    async def on_position(",
            "        self, stream_name: str, instance_name: str, token: int",
            "    ) -> None:",
            "        await self.on_rdata(stream_name, instance_name, token, [])",
            "",
            "        # We poke the generic \"replication\" notifier to wake anything up that",
            "        # may be streaming.",
            "        self.notifier.notify_replication()",
            "",
            "    def on_remote_server_up(self, server: str) -> None:",
            "        \"\"\"Called when get a new REMOTE_SERVER_UP command.\"\"\"",
            "",
            "        # Let's wake up the transaction queue for the server in case we have",
            "        # pending stuff to send to it.",
            "        if self.send_handler:",
            "            self.send_handler.wake_destination(server)",
            "",
            "    async def wait_for_stream_position(",
            "        self,",
            "        instance_name: str,",
            "        stream_name: str,",
            "        position: int,",
            "    ) -> None:",
            "        \"\"\"Wait until this instance has received updates up to and including",
            "        the given stream position.",
            "",
            "        Args:",
            "            instance_name",
            "            stream_name",
            "            position",
            "        \"\"\"",
            "",
            "        if instance_name == self._instance_name:",
            "            # We don't get told about updates written by this process, and",
            "            # anyway in that case we don't need to wait.",
            "            return",
            "",
            "        current_position = self._streams[stream_name].current_token(instance_name)",
            "        if position <= current_position:",
            "            # We're already past the position",
            "            return",
            "",
            "        # Create a new deferred that times out after N seconds, as we don't want",
            "        # to wedge here forever.",
            "        deferred: \"Deferred[None]\" = Deferred()",
            "        deferred = timeout_deferred(",
            "            deferred, _WAIT_FOR_REPLICATION_TIMEOUT_SECONDS, self._reactor",
            "        )",
            "",
            "        waiting_list = self._streams_to_waiters.setdefault(",
            "            (stream_name, instance_name), SortedList(key=lambda t: t[0])",
            "        )",
            "",
            "        waiting_list.add((position, deferred))",
            "",
            "        # We measure here to get in flight counts and average waiting time.",
            "        with Measure(self._clock, \"repl.wait_for_stream_position\"):",
            "            logger.info(",
            "                \"Waiting for repl stream %r to reach %s (%s); currently at: %s\",",
            "                stream_name,",
            "                position,",
            "                instance_name,",
            "                current_position,",
            "            )",
            "            try:",
            "                await make_deferred_yieldable(deferred)",
            "            except defer.TimeoutError:",
            "                logger.error(",
            "                    \"Timed out waiting for repl stream %r to reach %s (%s)\"",
            "                    \"; currently at: %s\",",
            "                    stream_name,",
            "                    position,",
            "                    instance_name,",
            "                    self._streams[stream_name].current_token(instance_name),",
            "                )",
            "                return",
            "",
            "            logger.info(",
            "                \"Finished waiting for repl stream %r to reach %s (%s)\",",
            "                stream_name,",
            "                position,",
            "                instance_name,",
            "            )",
            "",
            "    def stop_pusher(self, user_id: str, app_id: str, pushkey: str) -> None:",
            "        if not self._notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        pushers_for_user = self._pusher_pool.pushers.get(user_id, {})",
            "        pusher = pushers_for_user.pop(key, None)",
            "        if pusher is None:",
            "            return",
            "        logger.info(\"Stopping pusher %r / %r\", user_id, key)",
            "        pusher.on_stop()",
            "",
            "    async def process_pusher_change(",
            "        self, user_id: str, app_id: str, pushkey: str",
            "    ) -> None:",
            "        if not self._notify_pushers:",
            "            return",
            "",
            "        key = \"%s:%s\" % (app_id, pushkey)",
            "        logger.info(\"Starting pusher %r / %r\", user_id, key)",
            "        await self._pusher_pool.process_pusher_change_by_id(app_id, pushkey, user_id)",
            "",
            "",
            "class FederationSenderHandler:",
            "    \"\"\"Processes the fedration replication stream",
            "",
            "    This class is only instantiate on the worker responsible for sending outbound",
            "    federation transactions. It receives rows from the replication stream and forwards",
            "    the appropriate entries to the FederationSender class.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        assert hs.should_send_federation()",
            "",
            "        self.store = hs.get_datastores().main",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._hs = hs",
            "",
            "        # We need to make a temporary value to ensure that mypy picks up the",
            "        # right type. We know we should have a federation sender instance since",
            "        # `should_send_federation` is True.",
            "        sender = hs.get_federation_sender()",
            "        assert isinstance(sender, FederationSender)",
            "        self.federation_sender = sender",
            "",
            "        # Stores the latest position in the federation stream we've gotten up",
            "        # to. This is always set before we use it.",
            "        self.federation_position: Optional[int] = None",
            "",
            "        self._fed_position_linearizer = Linearizer(name=\"_fed_position_linearizer\")",
            "",
            "    def wake_destination(self, server: str) -> None:",
            "        self.federation_sender.wake_destination(server)",
            "",
            "    async def process_replication_rows(",
            "        self, stream_name: str, token: int, rows: list",
            "    ) -> None:",
            "        # The federation stream contains things that we want to send out, e.g.",
            "        # presence, typing, etc.",
            "        if stream_name == \"federation\":",
            "            await send_queue.process_rows_for_federation(self.federation_sender, rows)",
            "            await self.update_token(token)",
            "",
            "        # ... and when new receipts happen",
            "        elif stream_name == ReceiptsStream.NAME:",
            "            await self._on_new_receipts(rows)",
            "",
            "        # ... as well as device updates and messages",
            "        elif stream_name == DeviceListsStream.NAME:",
            "            # The entities are either user IDs (starting with '@') whose devices",
            "            # have changed, or remote servers that we need to tell about",
            "            # changes.",
            "            hosts = {",
            "                row.entity",
            "                for row in rows",
            "                if not row.entity.startswith(\"@\") and not row.is_signature",
            "            }",
            "            await self.federation_sender.send_device_messages(hosts, immediate=False)",
            "",
            "        elif stream_name == ToDeviceStream.NAME:",
            "            # The to_device stream includes stuff to be pushed to both local",
            "            # clients and remote servers, so we ignore entities that start with",
            "            # '@' (since they'll be local users rather than destinations).",
            "            hosts = {row.entity for row in rows if not row.entity.startswith(\"@\")}",
            "            await self.federation_sender.send_device_messages(hosts)",
            "",
            "    async def _on_new_receipts(",
            "        self, rows: Iterable[ReceiptsStream.ReceiptsStreamRow]",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            rows: new receipts to be processed",
            "        \"\"\"",
            "        for receipt in rows:",
            "            # we only want to send on receipts for our own users",
            "            if not self._is_mine_id(receipt.user_id):",
            "                continue",
            "            # Private read receipts never get sent over federation.",
            "            if receipt.receipt_type == ReceiptTypes.READ_PRIVATE:",
            "                continue",
            "            receipt_info = ReadReceipt(",
            "                receipt.room_id,",
            "                receipt.receipt_type,",
            "                receipt.user_id,",
            "                [receipt.event_id],",
            "                thread_id=receipt.thread_id,",
            "                data=receipt.data,",
            "            )",
            "            await self.federation_sender.send_read_receipt(receipt_info)",
            "",
            "    async def update_token(self, token: int) -> None:",
            "        \"\"\"Update the record of where we have processed to in the federation stream.",
            "",
            "        Called after we have processed a an update received over replication. Sends",
            "        a FEDERATION_ACK back to the master, and stores the token that we have processed",
            "         in `federation_stream_position` so that we can restart where we left off.",
            "        \"\"\"",
            "        self.federation_position = token",
            "",
            "        # We save and send the ACK to master asynchronously, so we don't block",
            "        # processing on persistence. We don't need to do this operation for",
            "        # every single RDATA we receive, we just need to do it periodically.",
            "",
            "        if self._fed_position_linearizer.is_queued(None):",
            "            # There is already a task queued up to save and send the token, so",
            "            # no need to queue up another task.",
            "            return",
            "",
            "        run_as_background_process(\"_save_and_send_ack\", self._save_and_send_ack)",
            "",
            "    async def _save_and_send_ack(self) -> None:",
            "        \"\"\"Save the current federation position in the database and send an ACK",
            "        to master with where we're up to.",
            "        \"\"\"",
            "        # We should only be calling this once we've got a token.",
            "        assert self.federation_position is not None",
            "",
            "        try:",
            "            # We linearize here to ensure we don't have races updating the token",
            "            #",
            "            # XXX this appears to be redundant, since the ReplicationCommandHandler",
            "            # has a linearizer which ensures that we only process one line of",
            "            # replication data at a time. Should we remove it, or is it doing useful",
            "            # service for robustness? Or could we replace it with an assertion that",
            "            # we're not being re-entered?",
            "",
            "            async with self._fed_position_linearizer.queue(None):",
            "                # We persist and ack the same position, so we take a copy of it",
            "                # here as otherwise it can get modified from underneath us.",
            "                current_position = self.federation_position",
            "",
            "                await self.store.update_federation_out_pos(",
            "                    \"federation\", current_position",
            "                )",
            "",
            "                # We ACK this token over replication so that the master can drop",
            "                # its in memory queues",
            "                self._hs.get_replication_command_handler().send_federation_ack(",
            "                    current_position",
            "                )",
            "        except Exception:",
            "            logger.exception(\"Error updating federation stream position\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.replication.tcp.client.ReplicationDataHandler.self"
        ]
    },
    "synapse/storage/controllers/state.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "     PartialCurrentStateTracker,"
            },
            "1": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "     PartialStateEventsTracker,"
            },
            "2": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+from synapse.synapse_rust.acl import ServerAclEvaluator"
            },
            "4": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " from synapse.types import MutableStateMap, StateMap, get_domain_from_id"
            },
            "5": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 42,
                "PatchRowcode": " from synapse.types.state import StateFilter"
            },
            "6": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 43,
                "PatchRowcode": " from synapse.util.async_helpers import Linearizer"
            },
            "7": {
                "beforePatchRowNumber": 501,
                "afterPatchRowNumber": 502,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 502,
                "afterPatchRowNumber": 503,
                "PatchRowcode": "         return event.content.get(\"alias\")"
            },
            "9": {
                "beforePatchRowNumber": 503,
                "afterPatchRowNumber": 504,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 505,
                "PatchRowcode": "+    @cached()"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 506,
                "PatchRowcode": "+    async def get_server_acl_for_room("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 507,
                "PatchRowcode": "+        self, room_id: str"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 508,
                "PatchRowcode": "+    ) -> Optional[ServerAclEvaluator]:"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 509,
                "PatchRowcode": "+        \"\"\"Get the server ACL evaluator for room, if any"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 510,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 511,
                "PatchRowcode": "+        This does up-front parsing of the content to ignore bad data and pre-compile"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 512,
                "PatchRowcode": "+        regular expressions."
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 513,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 514,
                "PatchRowcode": "+        Args:"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 515,
                "PatchRowcode": "+            room_id: The room ID"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 516,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 517,
                "PatchRowcode": "+        Returns:"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 518,
                "PatchRowcode": "+            The server ACL evaluator, if any"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 519,
                "PatchRowcode": "+        \"\"\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 520,
                "PatchRowcode": "+"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 521,
                "PatchRowcode": "+        acl_event = await self.get_current_state_event("
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 522,
                "PatchRowcode": "+            room_id, EventTypes.ServerACL, \"\""
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 523,
                "PatchRowcode": "+        )"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 524,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 525,
                "PatchRowcode": "+        if not acl_event:"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 526,
                "PatchRowcode": "+            return None"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 527,
                "PatchRowcode": "+"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 528,
                "PatchRowcode": "+        return server_acl_evaluator_from_event(acl_event)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 529,
                "PatchRowcode": "+"
            },
            "35": {
                "beforePatchRowNumber": 504,
                "afterPatchRowNumber": 530,
                "PatchRowcode": "     @trace"
            },
            "36": {
                "beforePatchRowNumber": 505,
                "afterPatchRowNumber": 531,
                "PatchRowcode": "     @tag_args"
            },
            "37": {
                "beforePatchRowNumber": 506,
                "afterPatchRowNumber": 532,
                "PatchRowcode": "     async def get_current_state_deltas("
            },
            "38": {
                "beforePatchRowNumber": 760,
                "afterPatchRowNumber": 786,
                "PatchRowcode": "                 cache.state_group = object()"
            },
            "39": {
                "beforePatchRowNumber": 761,
                "afterPatchRowNumber": 787,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 762,
                "afterPatchRowNumber": 788,
                "PatchRowcode": "         return frozenset(cache.hosts_to_joined_users)"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 789,
                "PatchRowcode": "+"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 790,
                "PatchRowcode": "+"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 791,
                "PatchRowcode": "+def server_acl_evaluator_from_event(acl_event: EventBase) -> \"ServerAclEvaluator\":"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 792,
                "PatchRowcode": "+    \"\"\""
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 793,
                "PatchRowcode": "+    Create a ServerAclEvaluator from a m.room.server_acl event's content."
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 794,
                "PatchRowcode": "+"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 795,
                "PatchRowcode": "+    This does up-front parsing of the content to ignore bad data. It then creates"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 796,
                "PatchRowcode": "+    the ServerAclEvaluator which will pre-compile regular expressions from the globs."
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 797,
                "PatchRowcode": "+    \"\"\""
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 798,
                "PatchRowcode": "+"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 799,
                "PatchRowcode": "+    # first of all, parse if literal IPs are blocked."
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 800,
                "PatchRowcode": "+    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 801,
                "PatchRowcode": "+    if not isinstance(allow_ip_literals, bool):"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 802,
                "PatchRowcode": "+        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 803,
                "PatchRowcode": "+        allow_ip_literals = True"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 804,
                "PatchRowcode": "+"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 805,
                "PatchRowcode": "+    # next, parse the deny list by ignoring any non-strings."
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 806,
                "PatchRowcode": "+    deny = acl_event.content.get(\"deny\", [])"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 807,
                "PatchRowcode": "+    if not isinstance(deny, (list, tuple)):"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 808,
                "PatchRowcode": "+        logger.warning(\"Ignoring non-list deny ACL %s\", deny)"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 809,
                "PatchRowcode": "+        deny = []"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 810,
                "PatchRowcode": "+    else:"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 811,
                "PatchRowcode": "+        deny = [s for s in deny if isinstance(s, str)]"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 812,
                "PatchRowcode": "+"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 813,
                "PatchRowcode": "+    # then the allow list."
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 814,
                "PatchRowcode": "+    allow = acl_event.content.get(\"allow\", [])"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 815,
                "PatchRowcode": "+    if not isinstance(allow, (list, tuple)):"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 816,
                "PatchRowcode": "+        logger.warning(\"Ignoring non-list allow ACL %s\", allow)"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 817,
                "PatchRowcode": "+        allow = []"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 818,
                "PatchRowcode": "+    else:"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 819,
                "PatchRowcode": "+        allow = [s for s in allow if isinstance(s, str)]"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 820,
                "PatchRowcode": "+"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 821,
                "PatchRowcode": "+    return ServerAclEvaluator(allow_ip_literals, allow, deny)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2022 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from itertools import chain",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    AbstractSet,",
            "    Any,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    FrozenSet,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from synapse.api.constants import EventTypes, Membership",
            "from synapse.events import EventBase",
            "from synapse.logging.opentracing import tag_args, trace",
            "from synapse.storage.roommember import ProfileInfo",
            "from synapse.storage.util.partial_state_events_tracker import (",
            "    PartialCurrentStateTracker,",
            "    PartialStateEventsTracker,",
            ")",
            "from synapse.types import MutableStateMap, StateMap, get_domain_from_id",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.caches import intern_string",
            "from synapse.util.caches.descriptors import cached",
            "from synapse.util.cancellation import cancellable",
            "from synapse.util.metrics import Measure",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "    from synapse.state import _StateCacheEntry",
            "    from synapse.storage.databases import Databases",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class StateStorageController:",
            "    \"\"\"High level interface to fetching state for an event, or the current state",
            "    in a room.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\", stores: \"Databases\"):",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._clock = hs.get_clock()",
            "        self.stores = stores",
            "        self._partial_state_events_tracker = PartialStateEventsTracker(stores.main)",
            "        self._partial_state_room_tracker = PartialCurrentStateTracker(stores.main)",
            "",
            "        # Used by `_get_joined_hosts` to ensure only one thing mutates the cache",
            "        # at a time. Keyed by room_id.",
            "        self._joined_host_linearizer = Linearizer(\"_JoinedHostsCache\")",
            "",
            "    def notify_event_un_partial_stated(self, event_id: str) -> None:",
            "        self._partial_state_events_tracker.notify_un_partial_stated(event_id)",
            "",
            "    def notify_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Notify that the room no longer has any partial state.",
            "",
            "        Must be called after `DataStore.clear_partial_state_room`",
            "        \"\"\"",
            "        self._partial_state_room_tracker.notify_un_partial_stated(room_id)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_group_delta(",
            "        self, state_group: int",
            "    ) -> Tuple[Optional[int], Optional[StateMap[str]]]:",
            "        \"\"\"Given a state group try to return a previous group and a delta between",
            "        the old and the new.",
            "",
            "        Args:",
            "            state_group: The state group used to retrieve state deltas.",
            "",
            "        Returns:",
            "            A tuple of the previous group and a state map of the event IDs which",
            "            make up the delta between the old and new state groups.",
            "        \"\"\"",
            "",
            "        state_group_delta = await self.stores.state.get_state_group_delta(state_group)",
            "        return state_group_delta.prev_group, state_group_delta.delta_ids",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_groups_ids(",
            "        self, _room_id: str, event_ids: Collection[str], await_full_state: bool = True",
            "    ) -> Dict[int, MutableStateMap[str]]:",
            "        \"\"\"Get the event IDs of all the state for the state groups for the given events",
            "",
            "        Args:",
            "            _room_id: id of the room for these events",
            "            event_ids: ids of the events",
            "            await_full_state: if `True`, will block if we do not yet have complete",
            "               state at these events.",
            "",
            "        Returns:",
            "            dict of state_group_id -> (dict of (type, state_key) -> event id)",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "               (ie they are outliers or unknown)",
            "        \"\"\"",
            "        if not event_ids:",
            "            return {}",
            "",
            "        event_to_groups = await self.get_state_group_for_events(",
            "            event_ids, await_full_state=await_full_state",
            "        )",
            "",
            "        groups = set(event_to_groups.values())",
            "        group_to_state = await self.stores.state._get_state_for_groups(groups)",
            "",
            "        return group_to_state",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_group(",
            "        self, state_group: int, state_filter: Optional[StateFilter] = None",
            "    ) -> StateMap[str]:",
            "        \"\"\"Get the event IDs of all the state in the given state group",
            "",
            "        Args:",
            "            state_group: A state group for which we want to get the state IDs.",
            "            state_filter: specifies the type of state event to fetch from DB, example: EventTypes.JoinRules",
            "",
            "        Returns:",
            "            Resolves to a map of (type, state_key) -> event_id",
            "        \"\"\"",
            "        group_to_state = await self.get_state_for_groups((state_group,), state_filter)",
            "",
            "        return group_to_state[state_group]",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_groups(",
            "        self, room_id: str, event_ids: Collection[str]",
            "    ) -> Dict[int, List[EventBase]]:",
            "        \"\"\"Get the state groups for the given list of event_ids",
            "",
            "        Args:",
            "            room_id: ID of the room for these events.",
            "            event_ids: The event IDs to retrieve state for.",
            "",
            "        Returns:",
            "            dict of state_group_id -> list of state events.",
            "        \"\"\"",
            "        if not event_ids:",
            "            return {}",
            "",
            "        group_to_ids = await self.get_state_groups_ids(room_id, event_ids)",
            "",
            "        state_event_map = await self.stores.main.get_events(",
            "            [",
            "                ev_id",
            "                for group_ids in group_to_ids.values()",
            "                for ev_id in group_ids.values()",
            "            ],",
            "            get_prev_content=False,",
            "        )",
            "",
            "        return {",
            "            group: [",
            "                state_event_map[v]",
            "                for v in event_id_map.values()",
            "                if v in state_event_map",
            "            ]",
            "            for group, event_id_map in group_to_ids.items()",
            "        }",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_groups_from_groups(",
            "        self, groups: List[int], state_filter: StateFilter",
            "    ) -> Dict[int, StateMap[str]]:",
            "        \"\"\"Returns the state groups for a given set of groups, filtering on",
            "        types of state events.",
            "",
            "        Args:",
            "            groups: list of state group IDs to query",
            "            state_filter: The state filter used to fetch state",
            "                from the database.",
            "",
            "        Returns:",
            "            Dict of state group to state map.",
            "        \"\"\"",
            "",
            "        return await self.stores.state._get_state_groups_from_groups(",
            "            groups, state_filter",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_for_events(",
            "        self, event_ids: Collection[str], state_filter: Optional[StateFilter] = None",
            "    ) -> Dict[str, StateMap[EventBase]]:",
            "        \"\"\"Given a list of event_ids and type tuples, return a list of state",
            "        dicts for each event.",
            "",
            "        Args:",
            "            event_ids: The events to fetch the state of.",
            "            state_filter: The state filter used to fetch state.",
            "",
            "        Returns:",
            "            A dict of (event_id) -> (type, state_key) -> [state_events]",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "               (ie they are outliers or unknown)",
            "        \"\"\"",
            "        await_full_state = True",
            "        if state_filter and not state_filter.must_await_full_state(self._is_mine_id):",
            "            await_full_state = False",
            "",
            "        event_to_groups = await self.get_state_group_for_events(",
            "            event_ids, await_full_state=await_full_state",
            "        )",
            "",
            "        groups = set(event_to_groups.values())",
            "        group_to_state = await self.stores.state._get_state_for_groups(",
            "            groups, state_filter or StateFilter.all()",
            "        )",
            "",
            "        state_event_map = await self.stores.main.get_events(",
            "            [ev_id for sd in group_to_state.values() for ev_id in sd.values()],",
            "            get_prev_content=False,",
            "        )",
            "",
            "        event_to_state = {",
            "            event_id: {",
            "                k: state_event_map[v]",
            "                for k, v in group_to_state[group].items()",
            "                if v in state_event_map",
            "            }",
            "            for event_id, group in event_to_groups.items()",
            "        }",
            "",
            "        return {event: event_to_state[event] for event in event_ids}",
            "",
            "    @trace",
            "    @tag_args",
            "    @cancellable",
            "    async def get_state_ids_for_events(",
            "        self,",
            "        event_ids: Collection[str],",
            "        state_filter: Optional[StateFilter] = None,",
            "        await_full_state: bool = True,",
            "    ) -> Dict[str, StateMap[str]]:",
            "        \"\"\"",
            "        Get the state dicts corresponding to a list of events, containing the event_ids",
            "        of the state events (as opposed to the events themselves)",
            "",
            "        Args:",
            "            event_ids: events whose state should be returned",
            "            state_filter: The state filter used to fetch state from the database.",
            "            await_full_state: if `True`, will block if we do not yet have complete state",
            "                at these events and `state_filter` is not satisfied by partial state.",
            "                Defaults to `True`.",
            "",
            "        Returns:",
            "            A dict from event_id -> (type, state_key) -> event_id",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "                (ie they are outliers or unknown)",
            "        \"\"\"",
            "        if (",
            "            await_full_state",
            "            and state_filter",
            "            and not state_filter.must_await_full_state(self._is_mine_id)",
            "        ):",
            "            # Full state is not required if the state filter is restrictive enough.",
            "            await_full_state = False",
            "",
            "        event_to_groups = await self.get_state_group_for_events(",
            "            event_ids, await_full_state=await_full_state",
            "        )",
            "",
            "        groups = set(event_to_groups.values())",
            "        group_to_state = await self.stores.state._get_state_for_groups(",
            "            groups, state_filter or StateFilter.all()",
            "        )",
            "",
            "        event_to_state = {",
            "            event_id: group_to_state[group]",
            "            for event_id, group in event_to_groups.items()",
            "        }",
            "",
            "        return {event: event_to_state[event] for event in event_ids}",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_for_event(",
            "        self, event_id: str, state_filter: Optional[StateFilter] = None",
            "    ) -> StateMap[EventBase]:",
            "        \"\"\"",
            "        Get the state dict corresponding to a particular event",
            "",
            "        Args:",
            "            event_id: event whose state should be returned",
            "            state_filter: The state filter used to fetch state from the database.",
            "",
            "        Returns:",
            "            A dict from (type, state_key) -> state_event",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for the event (ie it is an",
            "                outlier or is unknown)",
            "        \"\"\"",
            "        state_map = await self.get_state_for_events(",
            "            [event_id], state_filter or StateFilter.all()",
            "        )",
            "        return state_map[event_id]",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_event(",
            "        self,",
            "        event_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        await_full_state: bool = True,",
            "    ) -> StateMap[str]:",
            "        \"\"\"",
            "        Get the state dict corresponding to a particular event",
            "",
            "        Args:",
            "            event_id: event whose state should be returned",
            "            state_filter: The state filter used to fetch state from the database.",
            "            await_full_state: if `True`, will block if we do not yet have complete state",
            "                at the event and `state_filter` is not satisfied by partial state.",
            "                Defaults to `True`.",
            "",
            "        Returns:",
            "            A dict from (type, state_key) -> state_event_id",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for the event (ie it is an",
            "                outlier or is unknown)",
            "        \"\"\"",
            "        state_map = await self.get_state_ids_for_events(",
            "            [event_id],",
            "            state_filter or StateFilter.all(),",
            "            await_full_state=await_full_state,",
            "        )",
            "        return state_map[event_id]",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_for_groups(",
            "        self, groups: Iterable[int], state_filter: Optional[StateFilter] = None",
            "    ) -> Dict[int, MutableStateMap[str]]:",
            "        \"\"\"Gets the state at each of a list of state groups, optionally",
            "        filtering by type/state_key",
            "",
            "        Args:",
            "            groups: list of state groups for which we want to get the state.",
            "            state_filter: The state filter used to fetch state.",
            "                from the database.",
            "",
            "        Returns:",
            "            Dict of state group to state map.",
            "        \"\"\"",
            "        return await self.stores.state._get_state_for_groups(",
            "            groups, state_filter or StateFilter.all()",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    @cancellable",
            "    async def get_state_group_for_events(",
            "        self,",
            "        event_ids: Collection[str],",
            "        await_full_state: bool = True,",
            "    ) -> Mapping[str, int]:",
            "        \"\"\"Returns mapping event_id -> state_group",
            "",
            "        Args:",
            "            event_ids: events to get state groups for",
            "            await_full_state: if true, will block if we do not yet have complete",
            "               state at these events.",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "               (ie. they are outliers or unknown)",
            "        \"\"\"",
            "        if await_full_state:",
            "            await self._partial_state_events_tracker.await_full_state(event_ids)",
            "",
            "        return await self.stores.main._get_state_group_for_events(event_ids)",
            "",
            "    async def store_state_group(",
            "        self,",
            "        event_id: str,",
            "        room_id: str,",
            "        prev_group: Optional[int],",
            "        delta_ids: Optional[StateMap[str]],",
            "        current_state_ids: Optional[StateMap[str]],",
            "    ) -> int:",
            "        \"\"\"Store a new set of state, returning a newly assigned state group.",
            "",
            "        Args:",
            "            event_id: The event ID for which the state was calculated.",
            "            room_id: ID of the room for which the state was calculated.",
            "            prev_group: A previous state group for the room, optional.",
            "            delta_ids: The delta between state at `prev_group` and",
            "                `current_state_ids`, if `prev_group` was given. Same format as",
            "                `current_state_ids`.",
            "            current_state_ids: The state to store. Map of (type, state_key)",
            "                to event_id.",
            "",
            "        Returns:",
            "            The state group ID",
            "        \"\"\"",
            "        return await self.stores.state.store_state_group(",
            "            event_id, room_id, prev_group, delta_ids, current_state_ids",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    @cancellable",
            "    async def get_current_state_ids(",
            "        self,",
            "        room_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        await_full_state: bool = True,",
            "        on_invalidate: Optional[Callable[[], None]] = None,",
            "    ) -> StateMap[str]:",
            "        \"\"\"Get the current state event ids for a room based on the",
            "        current_state_events table.",
            "",
            "        If a state filter is given (that is not `StateFilter.all()`) the query",
            "        result is *not* cached.",
            "",
            "        Args:",
            "            room_id: The room to get the state IDs of. state_filter: The state",
            "            filter used to fetch state from the",
            "                database.",
            "            await_full_state: if true, will block if we do not yet have complete",
            "               state for the room.",
            "            on_invalidate: Callback for when the `get_current_state_ids` cache",
            "                for the room gets invalidated.",
            "",
            "        Returns:",
            "            The current state of the room.",
            "        \"\"\"",
            "        if await_full_state and (",
            "            not state_filter or state_filter.must_await_full_state(self._is_mine_id)",
            "        ):",
            "            await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        if state_filter and not state_filter.is_full():",
            "            return await self.stores.main.get_partial_filtered_current_state_ids(",
            "                room_id, state_filter",
            "            )",
            "        else:",
            "            return await self.stores.main.get_partial_current_state_ids(",
            "                room_id, on_invalidate=on_invalidate",
            "            )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_canonical_alias_for_room(self, room_id: str) -> Optional[str]:",
            "        \"\"\"Get canonical alias for room, if any",
            "",
            "        Args:",
            "            room_id: The room ID",
            "",
            "        Returns:",
            "            The canonical alias, if any",
            "        \"\"\"",
            "",
            "        state = await self.get_current_state_ids(",
            "            room_id, StateFilter.from_types([(EventTypes.CanonicalAlias, \"\")])",
            "        )",
            "",
            "        event_id = state.get((EventTypes.CanonicalAlias, \"\"))",
            "        if not event_id:",
            "            return None",
            "",
            "        event = await self.stores.main.get_event(event_id, allow_none=True)",
            "        if not event:",
            "            return None",
            "",
            "        return event.content.get(\"alias\")",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_state_deltas(",
            "        self, prev_stream_id: int, max_stream_id: int",
            "    ) -> Tuple[int, List[Dict[str, Any]]]:",
            "        \"\"\"Fetch a list of room state changes since the given stream id",
            "",
            "        Each entry in the result contains the following fields:",
            "            - stream_id (int)",
            "            - room_id (str)",
            "            - type (str): event type",
            "            - state_key (str):",
            "            - event_id (str|None): new event_id for this state key. None if the",
            "                state has been deleted.",
            "            - prev_event_id (str|None): previous event_id for this state key. None",
            "                if it's new state.",
            "",
            "        Args:",
            "            prev_stream_id: point to get changes since (exclusive)",
            "            max_stream_id: the point that we know has been correctly persisted",
            "               - ie, an upper limit to return changes from.",
            "",
            "        Returns:",
            "            A tuple consisting of:",
            "               - the stream id which these results go up to",
            "               - list of current_state_delta_stream rows. If it is empty, we are",
            "                 up to date.",
            "        \"\"\"",
            "        # FIXME(faster_joins): what do we do here?",
            "        #   https://github.com/matrix-org/synapse/issues/13008",
            "",
            "        return await self.stores.main.get_partial_current_state_deltas(",
            "            prev_stream_id, max_stream_id",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_state(",
            "        self, room_id: str, state_filter: Optional[StateFilter] = None",
            "    ) -> StateMap[EventBase]:",
            "        \"\"\"Same as `get_current_state_ids` but also fetches the events\"\"\"",
            "        state_map_ids = await self.get_current_state_ids(room_id, state_filter)",
            "",
            "        event_map = await self.stores.main.get_events(list(state_map_ids.values()))",
            "",
            "        state_map = {}",
            "        for key, event_id in state_map_ids.items():",
            "            event = event_map.get(event_id)",
            "            if event:",
            "                state_map[key] = event",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_state_event(",
            "        self, room_id: str, event_type: str, state_key: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get the current state event for the given type/state_key.\"\"\"",
            "",
            "        key = (event_type, state_key)",
            "        state_map = await self.get_current_state(",
            "            room_id, StateFilter.from_types((key,))",
            "        )",
            "        return state_map.get(key)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_hosts_in_room(self, room_id: str) -> AbstractSet[str]:",
            "        \"\"\"Get current hosts in room based on current state.",
            "",
            "        Blocks until we have full state for the given room. This only happens for rooms",
            "        with partial state.",
            "        \"\"\"",
            "",
            "        await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        return await self.stores.main.get_current_hosts_in_room(room_id)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_hosts_in_room_ordered(self, room_id: str) -> Tuple[str, ...]:",
            "        \"\"\"Get current hosts in room based on current state.",
            "",
            "        Blocks until we have full state for the given room. This only happens for rooms",
            "        with partial state.",
            "",
            "        Returns:",
            "            A list of hosts in the room, sorted by longest in the room first. (aka.",
            "            sorted by join with the lowest depth first).",
            "        \"\"\"",
            "",
            "        await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        return await self.stores.main.get_current_hosts_in_room_ordered(room_id)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_hosts_in_room_or_partial_state_approximation(",
            "        self, room_id: str",
            "    ) -> Collection[str]:",
            "        \"\"\"Get approximation of current hosts in room based on current state.",
            "",
            "        For rooms with full state, this is equivalent to `get_current_hosts_in_room`,",
            "        with the same order of results.",
            "",
            "        For rooms with partial state, no blocking occurs. Instead, the list of hosts",
            "        in the room at the time of joining is combined with the list of hosts which",
            "        joined the room afterwards. The returned list may include hosts that are not",
            "        actually in the room and exclude hosts that are in the room, since we may",
            "        calculate state incorrectly during the partial state phase. The order of results",
            "        is arbitrary for rooms with partial state.",
            "        \"\"\"",
            "        # We have to read this list first to mitigate races with un-partial stating.",
            "        hosts_at_join = await self.stores.main.get_partial_state_servers_at_join(",
            "            room_id",
            "        )",
            "        if hosts_at_join is None:",
            "            hosts_at_join = frozenset()",
            "",
            "        hosts_from_state = await self.stores.main.get_current_hosts_in_room(room_id)",
            "",
            "        hosts = set(hosts_at_join)",
            "        hosts.update(hosts_from_state)",
            "",
            "        return hosts",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_users_in_room_with_profiles(",
            "        self, room_id: str",
            "    ) -> Mapping[str, ProfileInfo]:",
            "        \"\"\"",
            "        Get the current users in the room with their profiles.",
            "        If the room is currently partial-stated, this will block until the room has",
            "        full state.",
            "        \"\"\"",
            "        await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        return await self.stores.main.get_users_in_room_with_profiles(room_id)",
            "",
            "    async def get_joined_hosts(",
            "        self, room_id: str, state_entry: \"_StateCacheEntry\"",
            "    ) -> FrozenSet[str]:",
            "        state_group: Union[object, int] = state_entry.state_group",
            "        if not state_group:",
            "            # If state_group is None it means it has yet to be assigned a",
            "            # state group, i.e. we need to make sure that calls with a state_group",
            "            # of None don't hit previous cached calls with a None state_group.",
            "            # To do this we set the state_group to a new object as object() != object()",
            "            state_group = object()",
            "",
            "        assert state_group is not None",
            "        with Measure(self._clock, \"get_joined_hosts\"):",
            "            return await self._get_joined_hosts(",
            "                room_id, state_group, state_entry=state_entry",
            "            )",
            "",
            "    @cached(num_args=2, max_entries=10000, iterable=True)",
            "    async def _get_joined_hosts(",
            "        self,",
            "        room_id: str,",
            "        state_group: Union[object, int],",
            "        state_entry: \"_StateCacheEntry\",",
            "    ) -> FrozenSet[str]:",
            "        # We don't use `state_group`, it's there so that we can cache based on",
            "        # it. However, its important that its never None, since two",
            "        # current_state's with a state_group of None are likely to be different.",
            "        #",
            "        # The `state_group` must match the `state_entry.state_group` (if not None).",
            "        assert state_group is not None",
            "        assert state_entry.state_group is None or state_entry.state_group == state_group",
            "",
            "        # We use a secondary cache of previous work to allow us to build up the",
            "        # joined hosts for the given state group based on previous state groups.",
            "        #",
            "        # We cache one object per room containing the results of the last state",
            "        # group we got joined hosts for. The idea is that generally",
            "        # `get_joined_hosts` is called with the \"current\" state group for the",
            "        # room, and so consecutive calls will be for consecutive state groups",
            "        # which point to the previous state group.",
            "        cache = await self.stores.main._get_joined_hosts_cache(room_id)",
            "",
            "        # If the state group in the cache matches, we already have the data we need.",
            "        if state_entry.state_group == cache.state_group:",
            "            return frozenset(cache.hosts_to_joined_users)",
            "",
            "        # Since we'll mutate the cache we need to lock.",
            "        async with self._joined_host_linearizer.queue(room_id):",
            "            if state_entry.state_group == cache.state_group:",
            "                # Same state group, so nothing to do. We've already checked for",
            "                # this above, but the cache may have changed while waiting on",
            "                # the lock.",
            "                pass",
            "            elif state_entry.prev_group == cache.state_group:",
            "                # The cached work is for the previous state group, so we work out",
            "                # the delta.",
            "                assert state_entry.delta_ids is not None",
            "                for (typ, state_key), event_id in state_entry.delta_ids.items():",
            "                    if typ != EventTypes.Member:",
            "                        continue",
            "",
            "                    host = intern_string(get_domain_from_id(state_key))",
            "                    user_id = state_key",
            "                    known_joins = cache.hosts_to_joined_users.setdefault(host, set())",
            "",
            "                    event = await self.stores.main.get_event(event_id)",
            "                    if event.membership == Membership.JOIN:",
            "                        known_joins.add(user_id)",
            "                    else:",
            "                        known_joins.discard(user_id)",
            "",
            "                        if not known_joins:",
            "                            cache.hosts_to_joined_users.pop(host, None)",
            "            else:",
            "                # The cache doesn't match the state group or prev state group,",
            "                # so we calculate the result from first principles.",
            "                #",
            "                # We need to fetch all hosts joined to the room according to `state` by",
            "                # inspecting all join memberships in `state`. However, if the `state` is",
            "                # relatively recent then many of its events are likely to be held in",
            "                # the current state of the room, which is easily available and likely",
            "                # cached.",
            "                #",
            "                # We therefore compute the set of `state` events not in the",
            "                # current state and only fetch those.",
            "                current_memberships = (",
            "                    await self.stores.main._get_approximate_current_memberships_in_room(",
            "                        room_id",
            "                    )",
            "                )",
            "                unknown_state_events = {}",
            "                joined_users_in_current_state = []",
            "",
            "                state = await state_entry.get_state(",
            "                    self, StateFilter.from_types([(EventTypes.Member, None)])",
            "                )",
            "",
            "                for (type, state_key), event_id in state.items():",
            "                    if event_id not in current_memberships:",
            "                        unknown_state_events[type, state_key] = event_id",
            "                    elif current_memberships[event_id] == Membership.JOIN:",
            "                        joined_users_in_current_state.append(state_key)",
            "",
            "                joined_user_ids = await self.stores.main.get_joined_user_ids_from_state(",
            "                    room_id, unknown_state_events",
            "                )",
            "",
            "                cache.hosts_to_joined_users = {}",
            "                for user_id in chain(joined_user_ids, joined_users_in_current_state):",
            "                    host = intern_string(get_domain_from_id(user_id))",
            "                    cache.hosts_to_joined_users.setdefault(host, set()).add(user_id)",
            "",
            "            if state_entry.state_group:",
            "                cache.state_group = state_entry.state_group",
            "            else:",
            "                cache.state_group = object()",
            "",
            "        return frozenset(cache.hosts_to_joined_users)"
        ],
        "afterPatchFile": [
            "# Copyright 2022 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from itertools import chain",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    AbstractSet,",
            "    Any,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    FrozenSet,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from synapse.api.constants import EventTypes, Membership",
            "from synapse.events import EventBase",
            "from synapse.logging.opentracing import tag_args, trace",
            "from synapse.storage.roommember import ProfileInfo",
            "from synapse.storage.util.partial_state_events_tracker import (",
            "    PartialCurrentStateTracker,",
            "    PartialStateEventsTracker,",
            ")",
            "from synapse.synapse_rust.acl import ServerAclEvaluator",
            "from synapse.types import MutableStateMap, StateMap, get_domain_from_id",
            "from synapse.types.state import StateFilter",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.caches import intern_string",
            "from synapse.util.caches.descriptors import cached",
            "from synapse.util.cancellation import cancellable",
            "from synapse.util.metrics import Measure",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "    from synapse.state import _StateCacheEntry",
            "    from synapse.storage.databases import Databases",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class StateStorageController:",
            "    \"\"\"High level interface to fetching state for an event, or the current state",
            "    in a room.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\", stores: \"Databases\"):",
            "        self._is_mine_id = hs.is_mine_id",
            "        self._clock = hs.get_clock()",
            "        self.stores = stores",
            "        self._partial_state_events_tracker = PartialStateEventsTracker(stores.main)",
            "        self._partial_state_room_tracker = PartialCurrentStateTracker(stores.main)",
            "",
            "        # Used by `_get_joined_hosts` to ensure only one thing mutates the cache",
            "        # at a time. Keyed by room_id.",
            "        self._joined_host_linearizer = Linearizer(\"_JoinedHostsCache\")",
            "",
            "    def notify_event_un_partial_stated(self, event_id: str) -> None:",
            "        self._partial_state_events_tracker.notify_un_partial_stated(event_id)",
            "",
            "    def notify_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Notify that the room no longer has any partial state.",
            "",
            "        Must be called after `DataStore.clear_partial_state_room`",
            "        \"\"\"",
            "        self._partial_state_room_tracker.notify_un_partial_stated(room_id)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_group_delta(",
            "        self, state_group: int",
            "    ) -> Tuple[Optional[int], Optional[StateMap[str]]]:",
            "        \"\"\"Given a state group try to return a previous group and a delta between",
            "        the old and the new.",
            "",
            "        Args:",
            "            state_group: The state group used to retrieve state deltas.",
            "",
            "        Returns:",
            "            A tuple of the previous group and a state map of the event IDs which",
            "            make up the delta between the old and new state groups.",
            "        \"\"\"",
            "",
            "        state_group_delta = await self.stores.state.get_state_group_delta(state_group)",
            "        return state_group_delta.prev_group, state_group_delta.delta_ids",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_groups_ids(",
            "        self, _room_id: str, event_ids: Collection[str], await_full_state: bool = True",
            "    ) -> Dict[int, MutableStateMap[str]]:",
            "        \"\"\"Get the event IDs of all the state for the state groups for the given events",
            "",
            "        Args:",
            "            _room_id: id of the room for these events",
            "            event_ids: ids of the events",
            "            await_full_state: if `True`, will block if we do not yet have complete",
            "               state at these events.",
            "",
            "        Returns:",
            "            dict of state_group_id -> (dict of (type, state_key) -> event id)",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "               (ie they are outliers or unknown)",
            "        \"\"\"",
            "        if not event_ids:",
            "            return {}",
            "",
            "        event_to_groups = await self.get_state_group_for_events(",
            "            event_ids, await_full_state=await_full_state",
            "        )",
            "",
            "        groups = set(event_to_groups.values())",
            "        group_to_state = await self.stores.state._get_state_for_groups(groups)",
            "",
            "        return group_to_state",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_group(",
            "        self, state_group: int, state_filter: Optional[StateFilter] = None",
            "    ) -> StateMap[str]:",
            "        \"\"\"Get the event IDs of all the state in the given state group",
            "",
            "        Args:",
            "            state_group: A state group for which we want to get the state IDs.",
            "            state_filter: specifies the type of state event to fetch from DB, example: EventTypes.JoinRules",
            "",
            "        Returns:",
            "            Resolves to a map of (type, state_key) -> event_id",
            "        \"\"\"",
            "        group_to_state = await self.get_state_for_groups((state_group,), state_filter)",
            "",
            "        return group_to_state[state_group]",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_groups(",
            "        self, room_id: str, event_ids: Collection[str]",
            "    ) -> Dict[int, List[EventBase]]:",
            "        \"\"\"Get the state groups for the given list of event_ids",
            "",
            "        Args:",
            "            room_id: ID of the room for these events.",
            "            event_ids: The event IDs to retrieve state for.",
            "",
            "        Returns:",
            "            dict of state_group_id -> list of state events.",
            "        \"\"\"",
            "        if not event_ids:",
            "            return {}",
            "",
            "        group_to_ids = await self.get_state_groups_ids(room_id, event_ids)",
            "",
            "        state_event_map = await self.stores.main.get_events(",
            "            [",
            "                ev_id",
            "                for group_ids in group_to_ids.values()",
            "                for ev_id in group_ids.values()",
            "            ],",
            "            get_prev_content=False,",
            "        )",
            "",
            "        return {",
            "            group: [",
            "                state_event_map[v]",
            "                for v in event_id_map.values()",
            "                if v in state_event_map",
            "            ]",
            "            for group, event_id_map in group_to_ids.items()",
            "        }",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _get_state_groups_from_groups(",
            "        self, groups: List[int], state_filter: StateFilter",
            "    ) -> Dict[int, StateMap[str]]:",
            "        \"\"\"Returns the state groups for a given set of groups, filtering on",
            "        types of state events.",
            "",
            "        Args:",
            "            groups: list of state group IDs to query",
            "            state_filter: The state filter used to fetch state",
            "                from the database.",
            "",
            "        Returns:",
            "            Dict of state group to state map.",
            "        \"\"\"",
            "",
            "        return await self.stores.state._get_state_groups_from_groups(",
            "            groups, state_filter",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_for_events(",
            "        self, event_ids: Collection[str], state_filter: Optional[StateFilter] = None",
            "    ) -> Dict[str, StateMap[EventBase]]:",
            "        \"\"\"Given a list of event_ids and type tuples, return a list of state",
            "        dicts for each event.",
            "",
            "        Args:",
            "            event_ids: The events to fetch the state of.",
            "            state_filter: The state filter used to fetch state.",
            "",
            "        Returns:",
            "            A dict of (event_id) -> (type, state_key) -> [state_events]",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "               (ie they are outliers or unknown)",
            "        \"\"\"",
            "        await_full_state = True",
            "        if state_filter and not state_filter.must_await_full_state(self._is_mine_id):",
            "            await_full_state = False",
            "",
            "        event_to_groups = await self.get_state_group_for_events(",
            "            event_ids, await_full_state=await_full_state",
            "        )",
            "",
            "        groups = set(event_to_groups.values())",
            "        group_to_state = await self.stores.state._get_state_for_groups(",
            "            groups, state_filter or StateFilter.all()",
            "        )",
            "",
            "        state_event_map = await self.stores.main.get_events(",
            "            [ev_id for sd in group_to_state.values() for ev_id in sd.values()],",
            "            get_prev_content=False,",
            "        )",
            "",
            "        event_to_state = {",
            "            event_id: {",
            "                k: state_event_map[v]",
            "                for k, v in group_to_state[group].items()",
            "                if v in state_event_map",
            "            }",
            "            for event_id, group in event_to_groups.items()",
            "        }",
            "",
            "        return {event: event_to_state[event] for event in event_ids}",
            "",
            "    @trace",
            "    @tag_args",
            "    @cancellable",
            "    async def get_state_ids_for_events(",
            "        self,",
            "        event_ids: Collection[str],",
            "        state_filter: Optional[StateFilter] = None,",
            "        await_full_state: bool = True,",
            "    ) -> Dict[str, StateMap[str]]:",
            "        \"\"\"",
            "        Get the state dicts corresponding to a list of events, containing the event_ids",
            "        of the state events (as opposed to the events themselves)",
            "",
            "        Args:",
            "            event_ids: events whose state should be returned",
            "            state_filter: The state filter used to fetch state from the database.",
            "            await_full_state: if `True`, will block if we do not yet have complete state",
            "                at these events and `state_filter` is not satisfied by partial state.",
            "                Defaults to `True`.",
            "",
            "        Returns:",
            "            A dict from event_id -> (type, state_key) -> event_id",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "                (ie they are outliers or unknown)",
            "        \"\"\"",
            "        if (",
            "            await_full_state",
            "            and state_filter",
            "            and not state_filter.must_await_full_state(self._is_mine_id)",
            "        ):",
            "            # Full state is not required if the state filter is restrictive enough.",
            "            await_full_state = False",
            "",
            "        event_to_groups = await self.get_state_group_for_events(",
            "            event_ids, await_full_state=await_full_state",
            "        )",
            "",
            "        groups = set(event_to_groups.values())",
            "        group_to_state = await self.stores.state._get_state_for_groups(",
            "            groups, state_filter or StateFilter.all()",
            "        )",
            "",
            "        event_to_state = {",
            "            event_id: group_to_state[group]",
            "            for event_id, group in event_to_groups.items()",
            "        }",
            "",
            "        return {event: event_to_state[event] for event in event_ids}",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_for_event(",
            "        self, event_id: str, state_filter: Optional[StateFilter] = None",
            "    ) -> StateMap[EventBase]:",
            "        \"\"\"",
            "        Get the state dict corresponding to a particular event",
            "",
            "        Args:",
            "            event_id: event whose state should be returned",
            "            state_filter: The state filter used to fetch state from the database.",
            "",
            "        Returns:",
            "            A dict from (type, state_key) -> state_event",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for the event (ie it is an",
            "                outlier or is unknown)",
            "        \"\"\"",
            "        state_map = await self.get_state_for_events(",
            "            [event_id], state_filter or StateFilter.all()",
            "        )",
            "        return state_map[event_id]",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_ids_for_event(",
            "        self,",
            "        event_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        await_full_state: bool = True,",
            "    ) -> StateMap[str]:",
            "        \"\"\"",
            "        Get the state dict corresponding to a particular event",
            "",
            "        Args:",
            "            event_id: event whose state should be returned",
            "            state_filter: The state filter used to fetch state from the database.",
            "            await_full_state: if `True`, will block if we do not yet have complete state",
            "                at the event and `state_filter` is not satisfied by partial state.",
            "                Defaults to `True`.",
            "",
            "        Returns:",
            "            A dict from (type, state_key) -> state_event_id",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for the event (ie it is an",
            "                outlier or is unknown)",
            "        \"\"\"",
            "        state_map = await self.get_state_ids_for_events(",
            "            [event_id],",
            "            state_filter or StateFilter.all(),",
            "            await_full_state=await_full_state,",
            "        )",
            "        return state_map[event_id]",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_state_for_groups(",
            "        self, groups: Iterable[int], state_filter: Optional[StateFilter] = None",
            "    ) -> Dict[int, MutableStateMap[str]]:",
            "        \"\"\"Gets the state at each of a list of state groups, optionally",
            "        filtering by type/state_key",
            "",
            "        Args:",
            "            groups: list of state groups for which we want to get the state.",
            "            state_filter: The state filter used to fetch state.",
            "                from the database.",
            "",
            "        Returns:",
            "            Dict of state group to state map.",
            "        \"\"\"",
            "        return await self.stores.state._get_state_for_groups(",
            "            groups, state_filter or StateFilter.all()",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    @cancellable",
            "    async def get_state_group_for_events(",
            "        self,",
            "        event_ids: Collection[str],",
            "        await_full_state: bool = True,",
            "    ) -> Mapping[str, int]:",
            "        \"\"\"Returns mapping event_id -> state_group",
            "",
            "        Args:",
            "            event_ids: events to get state groups for",
            "            await_full_state: if true, will block if we do not yet have complete",
            "               state at these events.",
            "",
            "        Raises:",
            "            RuntimeError if we don't have a state group for one or more of the events",
            "               (ie. they are outliers or unknown)",
            "        \"\"\"",
            "        if await_full_state:",
            "            await self._partial_state_events_tracker.await_full_state(event_ids)",
            "",
            "        return await self.stores.main._get_state_group_for_events(event_ids)",
            "",
            "    async def store_state_group(",
            "        self,",
            "        event_id: str,",
            "        room_id: str,",
            "        prev_group: Optional[int],",
            "        delta_ids: Optional[StateMap[str]],",
            "        current_state_ids: Optional[StateMap[str]],",
            "    ) -> int:",
            "        \"\"\"Store a new set of state, returning a newly assigned state group.",
            "",
            "        Args:",
            "            event_id: The event ID for which the state was calculated.",
            "            room_id: ID of the room for which the state was calculated.",
            "            prev_group: A previous state group for the room, optional.",
            "            delta_ids: The delta between state at `prev_group` and",
            "                `current_state_ids`, if `prev_group` was given. Same format as",
            "                `current_state_ids`.",
            "            current_state_ids: The state to store. Map of (type, state_key)",
            "                to event_id.",
            "",
            "        Returns:",
            "            The state group ID",
            "        \"\"\"",
            "        return await self.stores.state.store_state_group(",
            "            event_id, room_id, prev_group, delta_ids, current_state_ids",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    @cancellable",
            "    async def get_current_state_ids(",
            "        self,",
            "        room_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        await_full_state: bool = True,",
            "        on_invalidate: Optional[Callable[[], None]] = None,",
            "    ) -> StateMap[str]:",
            "        \"\"\"Get the current state event ids for a room based on the",
            "        current_state_events table.",
            "",
            "        If a state filter is given (that is not `StateFilter.all()`) the query",
            "        result is *not* cached.",
            "",
            "        Args:",
            "            room_id: The room to get the state IDs of. state_filter: The state",
            "            filter used to fetch state from the",
            "                database.",
            "            await_full_state: if true, will block if we do not yet have complete",
            "               state for the room.",
            "            on_invalidate: Callback for when the `get_current_state_ids` cache",
            "                for the room gets invalidated.",
            "",
            "        Returns:",
            "            The current state of the room.",
            "        \"\"\"",
            "        if await_full_state and (",
            "            not state_filter or state_filter.must_await_full_state(self._is_mine_id)",
            "        ):",
            "            await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        if state_filter and not state_filter.is_full():",
            "            return await self.stores.main.get_partial_filtered_current_state_ids(",
            "                room_id, state_filter",
            "            )",
            "        else:",
            "            return await self.stores.main.get_partial_current_state_ids(",
            "                room_id, on_invalidate=on_invalidate",
            "            )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_canonical_alias_for_room(self, room_id: str) -> Optional[str]:",
            "        \"\"\"Get canonical alias for room, if any",
            "",
            "        Args:",
            "            room_id: The room ID",
            "",
            "        Returns:",
            "            The canonical alias, if any",
            "        \"\"\"",
            "",
            "        state = await self.get_current_state_ids(",
            "            room_id, StateFilter.from_types([(EventTypes.CanonicalAlias, \"\")])",
            "        )",
            "",
            "        event_id = state.get((EventTypes.CanonicalAlias, \"\"))",
            "        if not event_id:",
            "            return None",
            "",
            "        event = await self.stores.main.get_event(event_id, allow_none=True)",
            "        if not event:",
            "            return None",
            "",
            "        return event.content.get(\"alias\")",
            "",
            "    @cached()",
            "    async def get_server_acl_for_room(",
            "        self, room_id: str",
            "    ) -> Optional[ServerAclEvaluator]:",
            "        \"\"\"Get the server ACL evaluator for room, if any",
            "",
            "        This does up-front parsing of the content to ignore bad data and pre-compile",
            "        regular expressions.",
            "",
            "        Args:",
            "            room_id: The room ID",
            "",
            "        Returns:",
            "            The server ACL evaluator, if any",
            "        \"\"\"",
            "",
            "        acl_event = await self.get_current_state_event(",
            "            room_id, EventTypes.ServerACL, \"\"",
            "        )",
            "",
            "        if not acl_event:",
            "            return None",
            "",
            "        return server_acl_evaluator_from_event(acl_event)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_state_deltas(",
            "        self, prev_stream_id: int, max_stream_id: int",
            "    ) -> Tuple[int, List[Dict[str, Any]]]:",
            "        \"\"\"Fetch a list of room state changes since the given stream id",
            "",
            "        Each entry in the result contains the following fields:",
            "            - stream_id (int)",
            "            - room_id (str)",
            "            - type (str): event type",
            "            - state_key (str):",
            "            - event_id (str|None): new event_id for this state key. None if the",
            "                state has been deleted.",
            "            - prev_event_id (str|None): previous event_id for this state key. None",
            "                if it's new state.",
            "",
            "        Args:",
            "            prev_stream_id: point to get changes since (exclusive)",
            "            max_stream_id: the point that we know has been correctly persisted",
            "               - ie, an upper limit to return changes from.",
            "",
            "        Returns:",
            "            A tuple consisting of:",
            "               - the stream id which these results go up to",
            "               - list of current_state_delta_stream rows. If it is empty, we are",
            "                 up to date.",
            "        \"\"\"",
            "        # FIXME(faster_joins): what do we do here?",
            "        #   https://github.com/matrix-org/synapse/issues/13008",
            "",
            "        return await self.stores.main.get_partial_current_state_deltas(",
            "            prev_stream_id, max_stream_id",
            "        )",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_state(",
            "        self, room_id: str, state_filter: Optional[StateFilter] = None",
            "    ) -> StateMap[EventBase]:",
            "        \"\"\"Same as `get_current_state_ids` but also fetches the events\"\"\"",
            "        state_map_ids = await self.get_current_state_ids(room_id, state_filter)",
            "",
            "        event_map = await self.stores.main.get_events(list(state_map_ids.values()))",
            "",
            "        state_map = {}",
            "        for key, event_id in state_map_ids.items():",
            "            event = event_map.get(event_id)",
            "            if event:",
            "                state_map[key] = event",
            "",
            "        return state_map",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_state_event(",
            "        self, room_id: str, event_type: str, state_key: str",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get the current state event for the given type/state_key.\"\"\"",
            "",
            "        key = (event_type, state_key)",
            "        state_map = await self.get_current_state(",
            "            room_id, StateFilter.from_types((key,))",
            "        )",
            "        return state_map.get(key)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_hosts_in_room(self, room_id: str) -> AbstractSet[str]:",
            "        \"\"\"Get current hosts in room based on current state.",
            "",
            "        Blocks until we have full state for the given room. This only happens for rooms",
            "        with partial state.",
            "        \"\"\"",
            "",
            "        await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        return await self.stores.main.get_current_hosts_in_room(room_id)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_hosts_in_room_ordered(self, room_id: str) -> Tuple[str, ...]:",
            "        \"\"\"Get current hosts in room based on current state.",
            "",
            "        Blocks until we have full state for the given room. This only happens for rooms",
            "        with partial state.",
            "",
            "        Returns:",
            "            A list of hosts in the room, sorted by longest in the room first. (aka.",
            "            sorted by join with the lowest depth first).",
            "        \"\"\"",
            "",
            "        await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        return await self.stores.main.get_current_hosts_in_room_ordered(room_id)",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_current_hosts_in_room_or_partial_state_approximation(",
            "        self, room_id: str",
            "    ) -> Collection[str]:",
            "        \"\"\"Get approximation of current hosts in room based on current state.",
            "",
            "        For rooms with full state, this is equivalent to `get_current_hosts_in_room`,",
            "        with the same order of results.",
            "",
            "        For rooms with partial state, no blocking occurs. Instead, the list of hosts",
            "        in the room at the time of joining is combined with the list of hosts which",
            "        joined the room afterwards. The returned list may include hosts that are not",
            "        actually in the room and exclude hosts that are in the room, since we may",
            "        calculate state incorrectly during the partial state phase. The order of results",
            "        is arbitrary for rooms with partial state.",
            "        \"\"\"",
            "        # We have to read this list first to mitigate races with un-partial stating.",
            "        hosts_at_join = await self.stores.main.get_partial_state_servers_at_join(",
            "            room_id",
            "        )",
            "        if hosts_at_join is None:",
            "            hosts_at_join = frozenset()",
            "",
            "        hosts_from_state = await self.stores.main.get_current_hosts_in_room(room_id)",
            "",
            "        hosts = set(hosts_at_join)",
            "        hosts.update(hosts_from_state)",
            "",
            "        return hosts",
            "",
            "    @trace",
            "    @tag_args",
            "    async def get_users_in_room_with_profiles(",
            "        self, room_id: str",
            "    ) -> Mapping[str, ProfileInfo]:",
            "        \"\"\"",
            "        Get the current users in the room with their profiles.",
            "        If the room is currently partial-stated, this will block until the room has",
            "        full state.",
            "        \"\"\"",
            "        await self._partial_state_room_tracker.await_full_state(room_id)",
            "",
            "        return await self.stores.main.get_users_in_room_with_profiles(room_id)",
            "",
            "    async def get_joined_hosts(",
            "        self, room_id: str, state_entry: \"_StateCacheEntry\"",
            "    ) -> FrozenSet[str]:",
            "        state_group: Union[object, int] = state_entry.state_group",
            "        if not state_group:",
            "            # If state_group is None it means it has yet to be assigned a",
            "            # state group, i.e. we need to make sure that calls with a state_group",
            "            # of None don't hit previous cached calls with a None state_group.",
            "            # To do this we set the state_group to a new object as object() != object()",
            "            state_group = object()",
            "",
            "        assert state_group is not None",
            "        with Measure(self._clock, \"get_joined_hosts\"):",
            "            return await self._get_joined_hosts(",
            "                room_id, state_group, state_entry=state_entry",
            "            )",
            "",
            "    @cached(num_args=2, max_entries=10000, iterable=True)",
            "    async def _get_joined_hosts(",
            "        self,",
            "        room_id: str,",
            "        state_group: Union[object, int],",
            "        state_entry: \"_StateCacheEntry\",",
            "    ) -> FrozenSet[str]:",
            "        # We don't use `state_group`, it's there so that we can cache based on",
            "        # it. However, its important that its never None, since two",
            "        # current_state's with a state_group of None are likely to be different.",
            "        #",
            "        # The `state_group` must match the `state_entry.state_group` (if not None).",
            "        assert state_group is not None",
            "        assert state_entry.state_group is None or state_entry.state_group == state_group",
            "",
            "        # We use a secondary cache of previous work to allow us to build up the",
            "        # joined hosts for the given state group based on previous state groups.",
            "        #",
            "        # We cache one object per room containing the results of the last state",
            "        # group we got joined hosts for. The idea is that generally",
            "        # `get_joined_hosts` is called with the \"current\" state group for the",
            "        # room, and so consecutive calls will be for consecutive state groups",
            "        # which point to the previous state group.",
            "        cache = await self.stores.main._get_joined_hosts_cache(room_id)",
            "",
            "        # If the state group in the cache matches, we already have the data we need.",
            "        if state_entry.state_group == cache.state_group:",
            "            return frozenset(cache.hosts_to_joined_users)",
            "",
            "        # Since we'll mutate the cache we need to lock.",
            "        async with self._joined_host_linearizer.queue(room_id):",
            "            if state_entry.state_group == cache.state_group:",
            "                # Same state group, so nothing to do. We've already checked for",
            "                # this above, but the cache may have changed while waiting on",
            "                # the lock.",
            "                pass",
            "            elif state_entry.prev_group == cache.state_group:",
            "                # The cached work is for the previous state group, so we work out",
            "                # the delta.",
            "                assert state_entry.delta_ids is not None",
            "                for (typ, state_key), event_id in state_entry.delta_ids.items():",
            "                    if typ != EventTypes.Member:",
            "                        continue",
            "",
            "                    host = intern_string(get_domain_from_id(state_key))",
            "                    user_id = state_key",
            "                    known_joins = cache.hosts_to_joined_users.setdefault(host, set())",
            "",
            "                    event = await self.stores.main.get_event(event_id)",
            "                    if event.membership == Membership.JOIN:",
            "                        known_joins.add(user_id)",
            "                    else:",
            "                        known_joins.discard(user_id)",
            "",
            "                        if not known_joins:",
            "                            cache.hosts_to_joined_users.pop(host, None)",
            "            else:",
            "                # The cache doesn't match the state group or prev state group,",
            "                # so we calculate the result from first principles.",
            "                #",
            "                # We need to fetch all hosts joined to the room according to `state` by",
            "                # inspecting all join memberships in `state`. However, if the `state` is",
            "                # relatively recent then many of its events are likely to be held in",
            "                # the current state of the room, which is easily available and likely",
            "                # cached.",
            "                #",
            "                # We therefore compute the set of `state` events not in the",
            "                # current state and only fetch those.",
            "                current_memberships = (",
            "                    await self.stores.main._get_approximate_current_memberships_in_room(",
            "                        room_id",
            "                    )",
            "                )",
            "                unknown_state_events = {}",
            "                joined_users_in_current_state = []",
            "",
            "                state = await state_entry.get_state(",
            "                    self, StateFilter.from_types([(EventTypes.Member, None)])",
            "                )",
            "",
            "                for (type, state_key), event_id in state.items():",
            "                    if event_id not in current_memberships:",
            "                        unknown_state_events[type, state_key] = event_id",
            "                    elif current_memberships[event_id] == Membership.JOIN:",
            "                        joined_users_in_current_state.append(state_key)",
            "",
            "                joined_user_ids = await self.stores.main.get_joined_user_ids_from_state(",
            "                    room_id, unknown_state_events",
            "                )",
            "",
            "                cache.hosts_to_joined_users = {}",
            "                for user_id in chain(joined_user_ids, joined_users_in_current_state):",
            "                    host = intern_string(get_domain_from_id(user_id))",
            "                    cache.hosts_to_joined_users.setdefault(host, set()).add(user_id)",
            "",
            "            if state_entry.state_group:",
            "                cache.state_group = state_entry.state_group",
            "            else:",
            "                cache.state_group = object()",
            "",
            "        return frozenset(cache.hosts_to_joined_users)",
            "",
            "",
            "def server_acl_evaluator_from_event(acl_event: EventBase) -> \"ServerAclEvaluator\":",
            "    \"\"\"",
            "    Create a ServerAclEvaluator from a m.room.server_acl event's content.",
            "",
            "    This does up-front parsing of the content to ignore bad data. It then creates",
            "    the ServerAclEvaluator which will pre-compile regular expressions from the globs.",
            "    \"\"\"",
            "",
            "    # first of all, parse if literal IPs are blocked.",
            "    allow_ip_literals = acl_event.content.get(\"allow_ip_literals\", True)",
            "    if not isinstance(allow_ip_literals, bool):",
            "        logger.warning(\"Ignoring non-bool allow_ip_literals flag\")",
            "        allow_ip_literals = True",
            "",
            "    # next, parse the deny list by ignoring any non-strings.",
            "    deny = acl_event.content.get(\"deny\", [])",
            "    if not isinstance(deny, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list deny ACL %s\", deny)",
            "        deny = []",
            "    else:",
            "        deny = [s for s in deny if isinstance(s, str)]",
            "",
            "    # then the allow list.",
            "    allow = acl_event.content.get(\"allow\", [])",
            "    if not isinstance(allow, (list, tuple)):",
            "        logger.warning(\"Ignoring non-list allow ACL %s\", allow)",
            "        allow = []",
            "    else:",
            "        allow = [s for s in allow if isinstance(s, str)]",
            "",
            "    return ServerAclEvaluator(allow_ip_literals, allow, deny)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.storage.controllers.state.StateStorageController.self",
            "tensorflow.python.ops.image_ops_test.RGBToHSVTest"
        ]
    }
}