{
    "mindsdb/integrations/handlers/chromadb_handler/chromadb_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+import ast"
            },
            "1": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " import sys"
            },
            "2": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import List, Optional"
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "4": {
                "beforePatchRowNumber": 329,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "         # ensure metadata is a dict, convert to dict if it is a string"
            },
            "5": {
                "beforePatchRowNumber": 330,
                "afterPatchRowNumber": 331,
                "PatchRowcode": "         if data.get(TableField.METADATA.value) is not None:"
            },
            "6": {
                "beforePatchRowNumber": 331,
                "afterPatchRowNumber": 332,
                "PatchRowcode": "             data[TableField.METADATA.value] = data[TableField.METADATA.value].apply("
            },
            "7": {
                "beforePatchRowNumber": 332,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                lambda x: x if isinstance(x, dict) else eval(x)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 333,
                "PatchRowcode": "+                lambda x: x if isinstance(x, dict) else ast.literal_eval(x)"
            },
            "9": {
                "beforePatchRowNumber": 333,
                "afterPatchRowNumber": 334,
                "PatchRowcode": "             )"
            },
            "10": {
                "beforePatchRowNumber": 334,
                "afterPatchRowNumber": 335,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 335,
                "afterPatchRowNumber": 336,
                "PatchRowcode": "         # convert to dict"
            }
        },
        "frontPatchFile": [
            "import sys",
            "from typing import List, Optional",
            "",
            "import pandas as pd",
            "",
            "from mindsdb.integrations.handlers.chromadb_handler.settings import ChromaHandlerConfig",
            "from mindsdb.integrations.libs.response import RESPONSE_TYPE",
            "from mindsdb.integrations.libs.response import HandlerResponse",
            "from mindsdb.integrations.libs.response import HandlerResponse as Response",
            "from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse",
            "from mindsdb.integrations.libs.vectordatabase_handler import (",
            "    FilterCondition,",
            "    FilterOperator,",
            "    TableField,",
            "    VectorStoreHandler,",
            ")",
            "from mindsdb.interfaces.storage.model_fs import HandlerStorage",
            "from mindsdb.utilities import log",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "def get_chromadb():",
            "    \"\"\"",
            "    Import and return the chromadb module, using pysqlite3 if available.",
            "    this is a hack to make chromadb work with pysqlite3 instead of sqlite3 for cloud usage",
            "    see https://docs.trychroma.com/troubleshooting#sqlite",
            "    \"\"\"",
            "",
            "    # if we are using python 3.10 or above, we don't need pysqlite",
            "    if sys.hexversion < 0x30A0000:",
            "        try:",
            "            __import__(\"pysqlite3\")",
            "            sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")",
            "        except ImportError:",
            "            logger.warn(",
            "                \"Python version < 3.10 and pysqlite3 is not installed. ChromaDB may not work without solving one of these: https://docs.trychroma.com/troubleshooting#sqlite\"",
            "            )  # noqa: E501",
            "",
            "    try:",
            "        import chromadb",
            "",
            "        return chromadb",
            "    except ImportError:",
            "        raise ImportError(\"Failed to import chromadb.\")",
            "",
            "",
            "class ChromaDBHandler(VectorStoreHandler):",
            "    \"\"\"This handler handles connection and execution of the ChromaDB statements.\"\"\"",
            "",
            "    name = \"chromadb\"",
            "",
            "    def __init__(self, name: str, **kwargs):",
            "        super().__init__(name)",
            "        self.handler_storage = HandlerStorage(kwargs.get(\"integration_id\"))",
            "        self._client = None",
            "        self.persist_directory = None",
            "        self.is_connected = False",
            "",
            "        config = self.validate_connection_parameters(name, **kwargs)",
            "",
            "        self._client_config = {",
            "            \"chroma_server_host\": config.host,",
            "            \"chroma_server_http_port\": config.port,",
            "            \"persist_directory\": self.persist_directory,",
            "        }",
            "",
            "        self.connect()",
            "",
            "    def validate_connection_parameters(self, name, **kwargs):",
            "        \"\"\"",
            "        Validate the connection parameters.",
            "        \"\"\"",
            "",
            "        _config = kwargs.get(\"connection_data\")",
            "        _config[\"vector_store\"] = name",
            "",
            "        config = ChromaHandlerConfig(**_config)",
            "",
            "        if config.persist_directory and not self.handler_storage.is_temporal:",
            "            # get full persistence directory from handler storage",
            "            self.persist_directory = self.handler_storage.folder_get(",
            "                config.persist_directory",
            "            )",
            "",
            "        return config",
            "",
            "    def _get_client(self):",
            "        client_config = self._client_config",
            "        if client_config is None:",
            "            raise Exception(\"Client config is not set!\")",
            "",
            "        chromadb = get_chromadb()",
            "",
            "        # decide the client type to be used, either persistent or httpclient",
            "        if client_config[\"persist_directory\"] is not None:",
            "            return chromadb.PersistentClient(path=client_config[\"persist_directory\"])",
            "        else:",
            "            return chromadb.HttpClient(",
            "                host=client_config[\"chroma_server_host\"],",
            "                port=client_config[\"chroma_server_http_port\"],",
            "            )",
            "",
            "    def _sync(self):",
            "        # if handler storage is used: sync on every change write operation",
            "        if self.persist_directory:",
            "            self.handler_storage.folder_sync(self.persist_directory)",
            "",
            "    def __del__(self):",
            "        \"\"\"Close the database connection.\"\"\"",
            "",
            "        if self.is_connected is True:",
            "            self._sync()",
            "",
            "            self.disconnect()",
            "",
            "    def connect(self):",
            "        \"\"\"Connect to a ChromaDB database.\"\"\"",
            "        if self.is_connected is True:",
            "            return self._client",
            "",
            "        try:",
            "            self._client = self._get_client()",
            "            self.is_connected = True",
            "            return self._client",
            "        except Exception as e:",
            "            self.is_connected = False",
            "            raise Exception(f\"Error connecting to ChromaDB client, {e}!\")",
            "",
            "    def disconnect(self):",
            "        \"\"\"Close the database connection.\"\"\"",
            "",
            "        if self.is_connected is False:",
            "            return",
            "",
            "        self._client = None",
            "        self.is_connected = False",
            "",
            "    def check_connection(self):",
            "        \"\"\"Check the connection to the ChromaDB database.\"\"\"",
            "        response_code = StatusResponse(False)",
            "        need_to_close = self.is_connected is False",
            "",
            "        try:",
            "            self._client.heartbeat()",
            "            response_code.success = True",
            "        except Exception as e:",
            "            logger.error(f\"Error connecting to ChromaDB , {e}!\")",
            "            response_code.error_message = str(e)",
            "        finally:",
            "            if response_code.success is True and need_to_close:",
            "                self.disconnect()",
            "            if response_code.success is False and self.is_connected is True:",
            "                self.is_connected = False",
            "",
            "        return response_code",
            "",
            "    def _get_chromadb_operator(self, operator: FilterOperator) -> str:",
            "        mapping = {",
            "            FilterOperator.EQUAL: \"$eq\",",
            "            FilterOperator.NOT_EQUAL: \"$ne\",",
            "            FilterOperator.LESS_THAN: \"$lt\",",
            "            FilterOperator.LESS_THAN_OR_EQUAL: \"$lte\",",
            "            FilterOperator.GREATER_THAN: \"$gt\",",
            "            FilterOperator.GREATER_THAN_OR_EQUAL: \"$gte\",",
            "        }",
            "",
            "        if operator not in mapping:",
            "            raise Exception(f\"Operator {operator} is not supported by ChromaDB!\")",
            "",
            "        return mapping[operator]",
            "",
            "    def _translate_metadata_condition(",
            "        self, conditions: List[FilterCondition]",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Translate a list of FilterCondition objects a dict that can be used by ChromaDB.",
            "        E.g.,",
            "        [",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.LESS_THAN,",
            "                value=\"2020-01-01\",",
            "            ),",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.GREATER_THAN,",
            "                value=\"2019-01-01\",",
            "            )",
            "        ]",
            "        -->",
            "        {",
            "            \"$and\": [",
            "                {\"created_at\": {\"$lt\": \"2020-01-01\"}},",
            "                {\"created_at\": {\"$gt\": \"2019-01-01\"}}",
            "            ]",
            "        }",
            "        \"\"\"",
            "        # we ignore all non-metadata conditions",
            "        if conditions is None:",
            "            return None",
            "        metadata_conditions = [",
            "            condition",
            "            for condition in conditions",
            "            if condition.column.startswith(TableField.METADATA.value)",
            "        ]",
            "        if len(metadata_conditions) == 0:",
            "            return None",
            "",
            "        # we translate each metadata condition into a dict",
            "        chroma_db_conditions = []",
            "        for condition in metadata_conditions:",
            "            metadata_key = condition.column.split(\".\")[-1]",
            "            chroma_db_conditions.append(",
            "                {",
            "                    metadata_key: {",
            "                        self._get_chromadb_operator(condition.op): condition.value",
            "                    }",
            "                }",
            "            )",
            "",
            "        # we combine all metadata conditions into a single dict",
            "        metadata_condition = (",
            "            {\"$and\": chroma_db_conditions}",
            "            if len(chroma_db_conditions) > 1",
            "            else chroma_db_conditions[0]",
            "        )",
            "        return metadata_condition",
            "",
            "    def select(",
            "        self,",
            "        table_name: str,",
            "        columns: List[str] = None,",
            "        conditions: List[FilterCondition] = None,",
            "        offset: int = None,",
            "        limit: int = None,",
            "    ) -> pd.DataFrame:",
            "        collection = self._client.get_collection(table_name)",
            "        filters = self._translate_metadata_condition(conditions)",
            "",
            "        include = [\"metadatas\", \"documents\", \"embeddings\"]",
            "",
            "        # check if embedding vector filter is present",
            "        vector_filter = (",
            "            []",
            "            if conditions is None",
            "            else [",
            "                condition",
            "                for condition in conditions",
            "                if condition.column == TableField.EMBEDDINGS.value",
            "            ]",
            "        )",
            "",
            "        if len(vector_filter) > 0:",
            "            vector_filter = vector_filter[0]",
            "        else:",
            "            vector_filter = None",
            "        id_filters = []",
            "        if conditions is not None:",
            "            for condition in conditions:",
            "                if condition.column != TableField.ID.value:",
            "                    continue",
            "                if condition.op == FilterOperator.EQUAL:",
            "                    id_filters.append(condition.value)",
            "                elif condition.op == FilterOperator.IN:",
            "                    id_filters.extend(condition.value)",
            "",
            "        if vector_filter is not None:",
            "            # similarity search",
            "            query_payload = {",
            "                \"where\": filters,",
            "                \"query_embeddings\": vector_filter.value",
            "                if vector_filter is not None",
            "                else None,",
            "                \"include\": include + [\"distances\"],",
            "            }",
            "            if limit is not None:",
            "                query_payload[\"n_results\"] = limit",
            "",
            "            result = collection.query(**query_payload)",
            "            ids = result[\"ids\"][0]",
            "            documents = result[\"documents\"][0]",
            "            metadatas = result[\"metadatas\"][0]",
            "            distances = result[\"distances\"][0]",
            "            embeddings = result[\"embeddings\"][0]",
            "",
            "        else:",
            "            # general get query",
            "            result = collection.get(",
            "                ids=id_filters,",
            "                where=filters,",
            "                limit=limit,",
            "                offset=offset,",
            "                include=include,",
            "            )",
            "            ids = result[\"ids\"]",
            "            documents = result[\"documents\"]",
            "            metadatas = result[\"metadatas\"]",
            "            embeddings = result[\"embeddings\"]",
            "            distances = None",
            "",
            "        # project based on columns",
            "        payload = {",
            "            TableField.ID.value: ids,",
            "            TableField.CONTENT.value: documents,",
            "            TableField.METADATA.value: metadatas,",
            "            TableField.EMBEDDINGS.value: embeddings,",
            "        }",
            "",
            "        if columns is not None:",
            "            payload = {column: payload[column] for column in columns}",
            "",
            "        # always include distance",
            "        if distances is not None:",
            "            payload[TableField.DISTANCE.value] = distances",
            "        return pd.DataFrame(payload)",
            "",
            "    def insert(self, table_name: str, data: pd.DataFrame):",
            "        \"\"\"",
            "        Insert data into the ChromaDB database.",
            "        \"\"\"",
            "",
            "        collection = self._client.get_or_create_collection(name=table_name)",
            "",
            "        # drop columns with all None values",
            "",
            "        data.dropna(axis=1, inplace=True)",
            "",
            "        # ensure metadata is a dict, convert to dict if it is a string",
            "        if data.get(TableField.METADATA.value) is not None:",
            "            data[TableField.METADATA.value] = data[TableField.METADATA.value].apply(",
            "                lambda x: x if isinstance(x, dict) else eval(x)",
            "            )",
            "",
            "        # convert to dict",
            "",
            "        data = data.to_dict(orient=\"list\")",
            "",
            "        collection.upsert(",
            "            ids=data[TableField.ID.value],",
            "            documents=data.get(TableField.CONTENT.value),",
            "            embeddings=data[TableField.EMBEDDINGS.value],",
            "            metadatas=data.get(TableField.METADATA.value),",
            "        )",
            "        self._sync()",
            "",
            "    def upsert(self, table_name: str, data: pd.DataFrame):",
            "        return self.insert(table_name, data)",
            "",
            "    def update(",
            "        self,",
            "        table_name: str,",
            "        data: pd.DataFrame,",
            "        key_columns: List[str] = None,",
            "    ):",
            "        \"\"\"",
            "        Update data in the ChromaDB database.",
            "        \"\"\"",
            "        collection = self._client.get_collection(table_name)",
            "",
            "        # drop columns with all None values",
            "",
            "        data.dropna(axis=1, inplace=True)",
            "",
            "        data = data.to_dict(orient=\"list\")",
            "",
            "        collection.update(",
            "            ids=data[TableField.ID.value],",
            "            documents=data.get(TableField.CONTENT.value),",
            "            embeddings=data[TableField.EMBEDDINGS.value],",
            "            metadatas=data.get(TableField.METADATA.value),",
            "        )",
            "        self._sync()",
            "",
            "    def delete(",
            "        self, table_name: str, conditions: List[FilterCondition] = None",
            "    ):",
            "        filters = self._translate_metadata_condition(conditions)",
            "        # get id filters",
            "        id_filters = [",
            "            condition.value",
            "            for condition in conditions",
            "            if condition.column == TableField.ID.value",
            "        ] or None",
            "",
            "        if filters is None and id_filters is None:",
            "            raise Exception(\"Delete query must have at least one condition!\")",
            "        collection = self._client.get_collection(table_name)",
            "        collection.delete(ids=id_filters, where=filters)",
            "        self._sync()",
            "",
            "    def create_table(self, table_name: str, if_not_exists=True):",
            "        \"\"\"",
            "        Create a collection with the given name in the ChromaDB database.",
            "        \"\"\"",
            "        self._client.create_collection(table_name, get_or_create=if_not_exists)",
            "        self._sync()",
            "",
            "    def drop_table(self, table_name: str, if_exists=True):",
            "        \"\"\"",
            "        Delete a collection from the ChromaDB database.",
            "        \"\"\"",
            "        try:",
            "            self._client.delete_collection(table_name)",
            "            self._sync()",
            "        except ValueError:",
            "            if if_exists:",
            "                return",
            "            else:",
            "                raise Exception(f\"Collection {table_name} does not exist!\")",
            "",
            "    def get_tables(self) -> HandlerResponse:",
            "        \"\"\"",
            "        Get the list of collections in the ChromaDB database.",
            "        \"\"\"",
            "        collections = self._client.list_collections()",
            "        collections_name = pd.DataFrame(",
            "            columns=[\"table_name\"],",
            "            data=[collection.name for collection in collections],",
            "        )",
            "        return Response(resp_type=RESPONSE_TYPE.TABLE, data_frame=collections_name)",
            "",
            "    def get_columns(self, table_name: str) -> HandlerResponse:",
            "        # check if collection exists",
            "        try:",
            "            _ = self._client.get_collection(table_name)",
            "        except ValueError:",
            "            return Response(",
            "                resp_type=RESPONSE_TYPE.ERROR,",
            "                error_message=f\"Table {table_name} does not exist!\",",
            "            )",
            "        return super().get_columns(table_name)"
        ],
        "afterPatchFile": [
            "import ast",
            "import sys",
            "from typing import List, Optional",
            "",
            "import pandas as pd",
            "",
            "from mindsdb.integrations.handlers.chromadb_handler.settings import ChromaHandlerConfig",
            "from mindsdb.integrations.libs.response import RESPONSE_TYPE",
            "from mindsdb.integrations.libs.response import HandlerResponse",
            "from mindsdb.integrations.libs.response import HandlerResponse as Response",
            "from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse",
            "from mindsdb.integrations.libs.vectordatabase_handler import (",
            "    FilterCondition,",
            "    FilterOperator,",
            "    TableField,",
            "    VectorStoreHandler,",
            ")",
            "from mindsdb.interfaces.storage.model_fs import HandlerStorage",
            "from mindsdb.utilities import log",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "def get_chromadb():",
            "    \"\"\"",
            "    Import and return the chromadb module, using pysqlite3 if available.",
            "    this is a hack to make chromadb work with pysqlite3 instead of sqlite3 for cloud usage",
            "    see https://docs.trychroma.com/troubleshooting#sqlite",
            "    \"\"\"",
            "",
            "    # if we are using python 3.10 or above, we don't need pysqlite",
            "    if sys.hexversion < 0x30A0000:",
            "        try:",
            "            __import__(\"pysqlite3\")",
            "            sys.modules[\"sqlite3\"] = sys.modules.pop(\"pysqlite3\")",
            "        except ImportError:",
            "            logger.warn(",
            "                \"Python version < 3.10 and pysqlite3 is not installed. ChromaDB may not work without solving one of these: https://docs.trychroma.com/troubleshooting#sqlite\"",
            "            )  # noqa: E501",
            "",
            "    try:",
            "        import chromadb",
            "",
            "        return chromadb",
            "    except ImportError:",
            "        raise ImportError(\"Failed to import chromadb.\")",
            "",
            "",
            "class ChromaDBHandler(VectorStoreHandler):",
            "    \"\"\"This handler handles connection and execution of the ChromaDB statements.\"\"\"",
            "",
            "    name = \"chromadb\"",
            "",
            "    def __init__(self, name: str, **kwargs):",
            "        super().__init__(name)",
            "        self.handler_storage = HandlerStorage(kwargs.get(\"integration_id\"))",
            "        self._client = None",
            "        self.persist_directory = None",
            "        self.is_connected = False",
            "",
            "        config = self.validate_connection_parameters(name, **kwargs)",
            "",
            "        self._client_config = {",
            "            \"chroma_server_host\": config.host,",
            "            \"chroma_server_http_port\": config.port,",
            "            \"persist_directory\": self.persist_directory,",
            "        }",
            "",
            "        self.connect()",
            "",
            "    def validate_connection_parameters(self, name, **kwargs):",
            "        \"\"\"",
            "        Validate the connection parameters.",
            "        \"\"\"",
            "",
            "        _config = kwargs.get(\"connection_data\")",
            "        _config[\"vector_store\"] = name",
            "",
            "        config = ChromaHandlerConfig(**_config)",
            "",
            "        if config.persist_directory and not self.handler_storage.is_temporal:",
            "            # get full persistence directory from handler storage",
            "            self.persist_directory = self.handler_storage.folder_get(",
            "                config.persist_directory",
            "            )",
            "",
            "        return config",
            "",
            "    def _get_client(self):",
            "        client_config = self._client_config",
            "        if client_config is None:",
            "            raise Exception(\"Client config is not set!\")",
            "",
            "        chromadb = get_chromadb()",
            "",
            "        # decide the client type to be used, either persistent or httpclient",
            "        if client_config[\"persist_directory\"] is not None:",
            "            return chromadb.PersistentClient(path=client_config[\"persist_directory\"])",
            "        else:",
            "            return chromadb.HttpClient(",
            "                host=client_config[\"chroma_server_host\"],",
            "                port=client_config[\"chroma_server_http_port\"],",
            "            )",
            "",
            "    def _sync(self):",
            "        # if handler storage is used: sync on every change write operation",
            "        if self.persist_directory:",
            "            self.handler_storage.folder_sync(self.persist_directory)",
            "",
            "    def __del__(self):",
            "        \"\"\"Close the database connection.\"\"\"",
            "",
            "        if self.is_connected is True:",
            "            self._sync()",
            "",
            "            self.disconnect()",
            "",
            "    def connect(self):",
            "        \"\"\"Connect to a ChromaDB database.\"\"\"",
            "        if self.is_connected is True:",
            "            return self._client",
            "",
            "        try:",
            "            self._client = self._get_client()",
            "            self.is_connected = True",
            "            return self._client",
            "        except Exception as e:",
            "            self.is_connected = False",
            "            raise Exception(f\"Error connecting to ChromaDB client, {e}!\")",
            "",
            "    def disconnect(self):",
            "        \"\"\"Close the database connection.\"\"\"",
            "",
            "        if self.is_connected is False:",
            "            return",
            "",
            "        self._client = None",
            "        self.is_connected = False",
            "",
            "    def check_connection(self):",
            "        \"\"\"Check the connection to the ChromaDB database.\"\"\"",
            "        response_code = StatusResponse(False)",
            "        need_to_close = self.is_connected is False",
            "",
            "        try:",
            "            self._client.heartbeat()",
            "            response_code.success = True",
            "        except Exception as e:",
            "            logger.error(f\"Error connecting to ChromaDB , {e}!\")",
            "            response_code.error_message = str(e)",
            "        finally:",
            "            if response_code.success is True and need_to_close:",
            "                self.disconnect()",
            "            if response_code.success is False and self.is_connected is True:",
            "                self.is_connected = False",
            "",
            "        return response_code",
            "",
            "    def _get_chromadb_operator(self, operator: FilterOperator) -> str:",
            "        mapping = {",
            "            FilterOperator.EQUAL: \"$eq\",",
            "            FilterOperator.NOT_EQUAL: \"$ne\",",
            "            FilterOperator.LESS_THAN: \"$lt\",",
            "            FilterOperator.LESS_THAN_OR_EQUAL: \"$lte\",",
            "            FilterOperator.GREATER_THAN: \"$gt\",",
            "            FilterOperator.GREATER_THAN_OR_EQUAL: \"$gte\",",
            "        }",
            "",
            "        if operator not in mapping:",
            "            raise Exception(f\"Operator {operator} is not supported by ChromaDB!\")",
            "",
            "        return mapping[operator]",
            "",
            "    def _translate_metadata_condition(",
            "        self, conditions: List[FilterCondition]",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Translate a list of FilterCondition objects a dict that can be used by ChromaDB.",
            "        E.g.,",
            "        [",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.LESS_THAN,",
            "                value=\"2020-01-01\",",
            "            ),",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.GREATER_THAN,",
            "                value=\"2019-01-01\",",
            "            )",
            "        ]",
            "        -->",
            "        {",
            "            \"$and\": [",
            "                {\"created_at\": {\"$lt\": \"2020-01-01\"}},",
            "                {\"created_at\": {\"$gt\": \"2019-01-01\"}}",
            "            ]",
            "        }",
            "        \"\"\"",
            "        # we ignore all non-metadata conditions",
            "        if conditions is None:",
            "            return None",
            "        metadata_conditions = [",
            "            condition",
            "            for condition in conditions",
            "            if condition.column.startswith(TableField.METADATA.value)",
            "        ]",
            "        if len(metadata_conditions) == 0:",
            "            return None",
            "",
            "        # we translate each metadata condition into a dict",
            "        chroma_db_conditions = []",
            "        for condition in metadata_conditions:",
            "            metadata_key = condition.column.split(\".\")[-1]",
            "            chroma_db_conditions.append(",
            "                {",
            "                    metadata_key: {",
            "                        self._get_chromadb_operator(condition.op): condition.value",
            "                    }",
            "                }",
            "            )",
            "",
            "        # we combine all metadata conditions into a single dict",
            "        metadata_condition = (",
            "            {\"$and\": chroma_db_conditions}",
            "            if len(chroma_db_conditions) > 1",
            "            else chroma_db_conditions[0]",
            "        )",
            "        return metadata_condition",
            "",
            "    def select(",
            "        self,",
            "        table_name: str,",
            "        columns: List[str] = None,",
            "        conditions: List[FilterCondition] = None,",
            "        offset: int = None,",
            "        limit: int = None,",
            "    ) -> pd.DataFrame:",
            "        collection = self._client.get_collection(table_name)",
            "        filters = self._translate_metadata_condition(conditions)",
            "",
            "        include = [\"metadatas\", \"documents\", \"embeddings\"]",
            "",
            "        # check if embedding vector filter is present",
            "        vector_filter = (",
            "            []",
            "            if conditions is None",
            "            else [",
            "                condition",
            "                for condition in conditions",
            "                if condition.column == TableField.EMBEDDINGS.value",
            "            ]",
            "        )",
            "",
            "        if len(vector_filter) > 0:",
            "            vector_filter = vector_filter[0]",
            "        else:",
            "            vector_filter = None",
            "        id_filters = []",
            "        if conditions is not None:",
            "            for condition in conditions:",
            "                if condition.column != TableField.ID.value:",
            "                    continue",
            "                if condition.op == FilterOperator.EQUAL:",
            "                    id_filters.append(condition.value)",
            "                elif condition.op == FilterOperator.IN:",
            "                    id_filters.extend(condition.value)",
            "",
            "        if vector_filter is not None:",
            "            # similarity search",
            "            query_payload = {",
            "                \"where\": filters,",
            "                \"query_embeddings\": vector_filter.value",
            "                if vector_filter is not None",
            "                else None,",
            "                \"include\": include + [\"distances\"],",
            "            }",
            "            if limit is not None:",
            "                query_payload[\"n_results\"] = limit",
            "",
            "            result = collection.query(**query_payload)",
            "            ids = result[\"ids\"][0]",
            "            documents = result[\"documents\"][0]",
            "            metadatas = result[\"metadatas\"][0]",
            "            distances = result[\"distances\"][0]",
            "            embeddings = result[\"embeddings\"][0]",
            "",
            "        else:",
            "            # general get query",
            "            result = collection.get(",
            "                ids=id_filters,",
            "                where=filters,",
            "                limit=limit,",
            "                offset=offset,",
            "                include=include,",
            "            )",
            "            ids = result[\"ids\"]",
            "            documents = result[\"documents\"]",
            "            metadatas = result[\"metadatas\"]",
            "            embeddings = result[\"embeddings\"]",
            "            distances = None",
            "",
            "        # project based on columns",
            "        payload = {",
            "            TableField.ID.value: ids,",
            "            TableField.CONTENT.value: documents,",
            "            TableField.METADATA.value: metadatas,",
            "            TableField.EMBEDDINGS.value: embeddings,",
            "        }",
            "",
            "        if columns is not None:",
            "            payload = {column: payload[column] for column in columns}",
            "",
            "        # always include distance",
            "        if distances is not None:",
            "            payload[TableField.DISTANCE.value] = distances",
            "        return pd.DataFrame(payload)",
            "",
            "    def insert(self, table_name: str, data: pd.DataFrame):",
            "        \"\"\"",
            "        Insert data into the ChromaDB database.",
            "        \"\"\"",
            "",
            "        collection = self._client.get_or_create_collection(name=table_name)",
            "",
            "        # drop columns with all None values",
            "",
            "        data.dropna(axis=1, inplace=True)",
            "",
            "        # ensure metadata is a dict, convert to dict if it is a string",
            "        if data.get(TableField.METADATA.value) is not None:",
            "            data[TableField.METADATA.value] = data[TableField.METADATA.value].apply(",
            "                lambda x: x if isinstance(x, dict) else ast.literal_eval(x)",
            "            )",
            "",
            "        # convert to dict",
            "",
            "        data = data.to_dict(orient=\"list\")",
            "",
            "        collection.upsert(",
            "            ids=data[TableField.ID.value],",
            "            documents=data.get(TableField.CONTENT.value),",
            "            embeddings=data[TableField.EMBEDDINGS.value],",
            "            metadatas=data.get(TableField.METADATA.value),",
            "        )",
            "        self._sync()",
            "",
            "    def upsert(self, table_name: str, data: pd.DataFrame):",
            "        return self.insert(table_name, data)",
            "",
            "    def update(",
            "        self,",
            "        table_name: str,",
            "        data: pd.DataFrame,",
            "        key_columns: List[str] = None,",
            "    ):",
            "        \"\"\"",
            "        Update data in the ChromaDB database.",
            "        \"\"\"",
            "        collection = self._client.get_collection(table_name)",
            "",
            "        # drop columns with all None values",
            "",
            "        data.dropna(axis=1, inplace=True)",
            "",
            "        data = data.to_dict(orient=\"list\")",
            "",
            "        collection.update(",
            "            ids=data[TableField.ID.value],",
            "            documents=data.get(TableField.CONTENT.value),",
            "            embeddings=data[TableField.EMBEDDINGS.value],",
            "            metadatas=data.get(TableField.METADATA.value),",
            "        )",
            "        self._sync()",
            "",
            "    def delete(",
            "        self, table_name: str, conditions: List[FilterCondition] = None",
            "    ):",
            "        filters = self._translate_metadata_condition(conditions)",
            "        # get id filters",
            "        id_filters = [",
            "            condition.value",
            "            for condition in conditions",
            "            if condition.column == TableField.ID.value",
            "        ] or None",
            "",
            "        if filters is None and id_filters is None:",
            "            raise Exception(\"Delete query must have at least one condition!\")",
            "        collection = self._client.get_collection(table_name)",
            "        collection.delete(ids=id_filters, where=filters)",
            "        self._sync()",
            "",
            "    def create_table(self, table_name: str, if_not_exists=True):",
            "        \"\"\"",
            "        Create a collection with the given name in the ChromaDB database.",
            "        \"\"\"",
            "        self._client.create_collection(table_name, get_or_create=if_not_exists)",
            "        self._sync()",
            "",
            "    def drop_table(self, table_name: str, if_exists=True):",
            "        \"\"\"",
            "        Delete a collection from the ChromaDB database.",
            "        \"\"\"",
            "        try:",
            "            self._client.delete_collection(table_name)",
            "            self._sync()",
            "        except ValueError:",
            "            if if_exists:",
            "                return",
            "            else:",
            "                raise Exception(f\"Collection {table_name} does not exist!\")",
            "",
            "    def get_tables(self) -> HandlerResponse:",
            "        \"\"\"",
            "        Get the list of collections in the ChromaDB database.",
            "        \"\"\"",
            "        collections = self._client.list_collections()",
            "        collections_name = pd.DataFrame(",
            "            columns=[\"table_name\"],",
            "            data=[collection.name for collection in collections],",
            "        )",
            "        return Response(resp_type=RESPONSE_TYPE.TABLE, data_frame=collections_name)",
            "",
            "    def get_columns(self, table_name: str) -> HandlerResponse:",
            "        # check if collection exists",
            "        try:",
            "            _ = self._client.get_collection(table_name)",
            "        except ValueError:",
            "            return Response(",
            "                resp_type=RESPONSE_TYPE.ERROR,",
            "                error_message=f\"Table {table_name} does not exist!\",",
            "            )",
            "        return super().get_columns(table_name)"
        ],
        "action": [
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "332": [
                "ChromaDBHandler",
                "insert"
            ]
        },
        "addLocation": []
    },
    "mindsdb/integrations/handlers/sharepoint_handler/sharepoint_api.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+import ast"
            },
            "1": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " from datetime import datetime, timezone"
            },
            "2": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import Text, List, Dict, Any"
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "4": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 250,
                "PatchRowcode": "         url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/\""
            },
            "5": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "         payload = {}"
            },
            "6": {
                "beforePatchRowNumber": 251,
                "afterPatchRowNumber": 252,
                "PatchRowcode": "         if column:"
            },
            "7": {
                "beforePatchRowNumber": 252,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            column = eval(column)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 253,
                "PatchRowcode": "+            column = ast.literal_eval(column)"
            },
            "9": {
                "beforePatchRowNumber": 253,
                "afterPatchRowNumber": 254,
                "PatchRowcode": "             payload[\"column\"] = column"
            },
            "10": {
                "beforePatchRowNumber": 254,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "         payload[\"displayName\"] = display_name"
            },
            "11": {
                "beforePatchRowNumber": 255,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        payload[\"list\"] = eval(list_template)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 256,
                "PatchRowcode": "+        payload[\"list\"] = ast.literal_eval(list_template)"
            },
            "13": {
                "beforePatchRowNumber": 256,
                "afterPatchRowNumber": 257,
                "PatchRowcode": "         create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)"
            },
            "14": {
                "beforePatchRowNumber": 257,
                "afterPatchRowNumber": 258,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 258,
                "afterPatchRowNumber": 259,
                "PatchRowcode": "     def get_site_columns_by_site("
            },
            "16": {
                "beforePatchRowNumber": 410,
                "afterPatchRowNumber": 411,
                "PatchRowcode": "         url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/\""
            },
            "17": {
                "beforePatchRowNumber": 411,
                "afterPatchRowNumber": 412,
                "PatchRowcode": "         payload = {}"
            },
            "18": {
                "beforePatchRowNumber": 412,
                "afterPatchRowNumber": 413,
                "PatchRowcode": "         if text:"
            },
            "19": {
                "beforePatchRowNumber": 413,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            text = eval(text)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 414,
                "PatchRowcode": "+            text = ast.literal_eval(text)"
            },
            "21": {
                "beforePatchRowNumber": 414,
                "afterPatchRowNumber": 415,
                "PatchRowcode": "             payload[\"text\"] = text"
            },
            "22": {
                "beforePatchRowNumber": 415,
                "afterPatchRowNumber": 416,
                "PatchRowcode": "         payload[\"name\"] = name"
            },
            "23": {
                "beforePatchRowNumber": 416,
                "afterPatchRowNumber": 417,
                "PatchRowcode": "         if enforce_unique_values is not None:"
            },
            "24": {
                "beforePatchRowNumber": 574,
                "afterPatchRowNumber": 575,
                "PatchRowcode": "         url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items/\""
            },
            "25": {
                "beforePatchRowNumber": 575,
                "afterPatchRowNumber": 576,
                "PatchRowcode": "         payload = {}"
            },
            "26": {
                "beforePatchRowNumber": 576,
                "afterPatchRowNumber": 577,
                "PatchRowcode": "         if fields:"
            },
            "27": {
                "beforePatchRowNumber": 577,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            payload[\"fields\"] = eval(fields)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 578,
                "PatchRowcode": "+            payload[\"fields\"] = ast.literal_eval(fields)"
            },
            "29": {
                "beforePatchRowNumber": 578,
                "afterPatchRowNumber": 579,
                "PatchRowcode": "         create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)"
            }
        },
        "frontPatchFile": [
            "from datetime import datetime, timezone",
            "from typing import Text, List, Dict, Any",
            "",
            "from mindsdb.integrations.handlers.sharepoint_handler.utils import (",
            "    bearer_token_request,",
            "    get_an_entity,",
            "    delete_an_entity,",
            "    update_an_entity,",
            "    create_an_entity,",
            ")",
            "",
            "",
            "class SharepointAPI:",
            "    def __init__(",
            "        self, client_id: str = None, client_secret: str = None, tenant_id: str = None",
            "    ):",
            "        self.client_id = client_id",
            "        self.client_secret = client_secret",
            "        self.tenant_id = tenant_id",
            "        self.bearer_token = None",
            "        self.is_connected = False",
            "        self.expiration_time = datetime.now(timezone.utc).timestamp()",
            "",
            "    def get_bearer_token(self) -> None:",
            "        \"\"\"",
            "        Generates new bearer token for the credentials",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        response = bearer_token_request(",
            "            client_id=self.client_id,",
            "            tenant_id=self.tenant_id,",
            "            client_secret=self.client_secret,",
            "        )",
            "        self.bearer_token = response[\"access_token\"]",
            "        self.expiration_time = int(response[\"expires_on\"])",
            "        self.is_connected = True",
            "",
            "    def check_bearer_token_validity(self) -> bool:",
            "        \"\"\"",
            "        Provides information whether a valid bearer token is available or not. Returns true if available",
            "        otherwise false",
            "",
            "        Returns",
            "        bool",
            "        \"\"\"",
            "        if (",
            "            self.is_connected",
            "            and datetime.now(timezone.utc).astimezone().timestamp()",
            "            < self.expiration_time",
            "        ):",
            "            return True",
            "        else:",
            "            return False",
            "",
            "    def disconnect(self) -> None:",
            "        \"\"\"",
            "        Removes bearer token from the sharepoint API class (makes it null)",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        self.bearer_token = None",
            "        self.is_connected = False",
            "",
            "    def get_all_sites(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all sites associated with the account",
            "",
            "        limit: limits the number of site information to be returned",
            "",
            "        Returns",
            "        response: metadata information corresponding to all sites",
            "        \"\"\"",
            "        url = \"https://graph.microsoft.com/v1.0/sites?search=*\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def update_sites(",
            "        self, site_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given sites (site_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_site for every site",
            "        site_dict: A dictionary containing site ids of the sites which are to be updated",
            "        values_to_update: a dictionary which will be used to update the fields of the sites",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for site_entry in site_dict:",
            "            self.update_a_site(",
            "                site_id=site_entry[\"siteId\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_a_site(self, site_id: str, values_to_update: Dict[Text, Any]) -> None:",
            "        \"\"\"",
            "        Updates a site with given values",
            "        site_id: GUID of the site",
            "        values_to_update: a dictionary values which will be used to update the properties of the site",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/\"",
            "        update_an_entity(",
            "            url=url, values_to_update=values_to_update, bearer_token=self.bearer_token",
            "        )",
            "",
            "    def get_lists_by_site(",
            "        self, site_id: str, limit: int = None",
            "    ) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets lists' information corresponding to a site",
            "",
            "        site_id:  GUID of a site",
            "        limit: limits the number of lists for which information is returned",
            "",
            "        Returns",
            "        response: metadata information/ fields corresponding to lists of the site",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def get_all_lists(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all the lists' information assocaited with the account",
            "",
            "        limit: puts a limit to the number of lists returned",
            "",
            "        Returns",
            "        response: returns metadata information regarding all the lists that have been made using that account",
            "        \"\"\"",
            "        sites = self.get_all_sites()",
            "        lists = []",
            "        for site in sites:",
            "            for list_dict in self.get_lists_by_site(site_id=site[\"id\"].split(\",\")[1]):",
            "                list_dict[\"siteName\"] = site[\"name\"]",
            "                list_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]",
            "                lists.append(list_dict)",
            "        if limit:",
            "            lists = lists[:limit]",
            "        return lists",
            "",
            "    def delete_lists(self, list_dict: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Deletes lists for the given site ID and list ID",
            "",
            "        list_dict: a dictionary values containing the list IDs which are to be deleted and",
            "        their corresponding site IDs",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for list_entry in list_dict:",
            "            self.delete_a_list(site_id=list_entry[\"siteId\"], list_id=list_entry[\"id\"])",
            "",
            "    def delete_a_list(self, site_id: str, list_id: str) -> None:",
            "        \"\"\"",
            "        Deletes a list, given its list ID and its site ID",
            "",
            "        site_id: GUID of the site in which the list is present",
            "        list_id: GUID of the list which is to be deleted",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}\"",
            "        delete_an_entity(url=url, bearer_token=self.bearer_token)",
            "",
            "    def update_lists(",
            "        self, list_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given lists (list_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_list for every list",
            "        list_dict: A dictionary containing ids of the list which are to be updated and also their site IDs",
            "        values_to_update: a dictionary which will be used to update the fields of the lists",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for list_entry in list_dict:",
            "            self.update_a_list(",
            "                site_id=list_entry[\"siteId\"],",
            "                list_id=list_entry[\"id\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_a_list(",
            "        self, site_id: str, list_id: str, values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates a list with given values",
            "        list_id: GUID of the list",
            "        site_id: GUID of the site in which the list is present",
            "        values_to_update: a dictionary values which will be used to update the properties of the list",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/\"",
            "        update_an_entity(",
            "            url=url, bearer_token=self.bearer_token, values_to_update=values_to_update",
            "        )",
            "",
            "    def create_lists(self, data: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Creates lists with the information provided in the data parameter",
            "        calls create_a_list for each entry of list metadata dictionary",
            "",
            "        data: parameter which contains information such as the site IDs where the lists would be created",
            "        and their metadata information which will be used to create them",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for entry in data:",
            "            self.create_a_list(",
            "                site_id=entry[\"siteId\"],",
            "                column=entry.get(\"column\"),",
            "                display_name=entry[\"displayName\"],",
            "                list_template=entry[\"list\"],",
            "            )",
            "",
            "    def create_a_list(",
            "        self, site_id: str, list_template: str, display_name: str, column: str = None",
            "    ) -> None:",
            "        \"\"\"",
            "        Creates a list with metadata information provided in the params",
            "",
            "        site_id: GUID of the site where the list is to be created",
            "        list_template: a string which contains the list template information (type of list)",
            "            eg.- \"{'template': 'documentLibrary'}\"",
            "        display_name: the display name of the given list, which will be displayed in the site",
            "        column: specifies the list of columns that should be created for the list",
            "            eg.- \"[{'name': 'Author', 'text': { }},{'name': 'PageCount', 'number': { }}]\"",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/\"",
            "        payload = {}",
            "        if column:",
            "            column = eval(column)",
            "            payload[\"column\"] = column",
            "        payload[\"displayName\"] = display_name",
            "        payload[\"list\"] = eval(list_template)",
            "        create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)",
            "",
            "    def get_site_columns_by_site(",
            "        self, site_id: str, limit: int = None",
            "    ) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets columns' information corresponding to a site",
            "",
            "        site_id:  GUID of a site",
            "        limit: limits the number of columns for which information is returned",
            "",
            "        Returns",
            "        response: metadata information/ fields corresponding to columns of the site",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def get_all_site_columns(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all the columns' information associated with the account",
            "",
            "        limit: puts a limit to the number of columns returned",
            "",
            "        Returns",
            "        response: returns metadata information regarding all the columns that have been made using that account",
            "        \"\"\"",
            "        sites = self.get_all_sites()",
            "        site_columns = []",
            "        for site in sites:",
            "            for site_column_dict in self.get_site_columns_by_site(",
            "                site_id=site[\"id\"].split(\",\")[1]",
            "            ):",
            "                site_column_dict[\"siteName\"] = site[\"name\"]",
            "                site_column_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]",
            "                site_columns.append(site_column_dict)",
            "        if limit:",
            "            site_columns = site_columns[:limit]",
            "        return site_columns",
            "",
            "    def update_site_columns(",
            "        self,",
            "        site_column_dict: List[Dict[Text, Text]],",
            "        values_to_update: Dict[Text, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given columns (site_column_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_site_column for every column",
            "",
            "        site_column_dict: A dictionary containing ids of the column which are to be updated and",
            "        also their site IDs",
            "        values_to_update: a dictionary which will be used to update the fields of the columns",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for site_column_entry in site_column_dict:",
            "            self.update_a_site_column(",
            "                site_id=site_column_entry[\"siteId\"],",
            "                column_id=site_column_entry[\"id\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_a_site_column(",
            "        self, site_id: str, column_id: str, values_to_update: Dict[Text, Any]",
            "    ):",
            "        \"\"\"",
            "        Updates a column with given values",
            "",
            "        column_id: GUID of the column",
            "        site_id: GUID of the site in which the column is present",
            "        values_to_update: a dictionary values which will be used to update the properties of the column",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/{column_id}\"",
            "        update_an_entity(",
            "            url=url, values_to_update=values_to_update, bearer_token=self.bearer_token",
            "        )",
            "",
            "    def delete_site_columns(self, column_dict: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Deletes columns for the given site ID and column ID",
            "",
            "        column_dict: a dictionary values containing the column IDs which are to be deleted and",
            "        their corresponding site IDs",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for column_entry in column_dict:",
            "            self.delete_a_site_columns(",
            "                site_id=column_entry[\"siteId\"], column_id=column_entry[\"id\"]",
            "            )",
            "",
            "    def delete_a_site_columns(self, site_id: str, column_id: str) -> None:",
            "        \"\"\"",
            "        Deletes a column, given its column ID and its site ID",
            "",
            "        site_id: GUID of the site in which the column is present",
            "        column_id: GUID of the column which is to be deleted",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/{column_id}\"",
            "        delete_an_entity(url=url, bearer_token=self.bearer_token)",
            "",
            "    def create_site_columns(self, data: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Creates columns with the information provided in the data parameter",
            "        calls create_a_site_column for each entry of column metadata dictionary",
            "",
            "        data: parameter which contains information such as the site IDs where the columns would be created",
            "        and their metadata information which will be used to create them",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for entry in data:",
            "            self.create_a_site_column(",
            "                site_id=entry[\"siteId\"],",
            "                enforce_unique_values=entry.get(\"enforceUniqueValues\"),",
            "                hidden=entry.get(\"hidden\"),",
            "                indexed=entry.get(\"indexed\"),",
            "                name=entry[\"name\"],",
            "                text=entry.get(\"text\"),",
            "            )",
            "",
            "    def create_a_site_column(",
            "        self,",
            "        site_id: str,",
            "        enforce_unique_values: bool,",
            "        hidden: bool,",
            "        indexed: bool,",
            "        name: str,",
            "        text: str = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Creates a list with metadata information provided in the params",
            "",
            "        site_id: GUID of the site where the column is to be created",
            "        enforced_unique_values: if true, no two list items may have the same value for this column",
            "        hidden: specifies whether the column is displayed in the user interface",
            "        name: the API-facing name of the column as it appears in the fields on a listItem.",
            "        text: details regarding the text values in the column",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/\"",
            "        payload = {}",
            "        if text:",
            "            text = eval(text)",
            "            payload[\"text\"] = text",
            "        payload[\"name\"] = name",
            "        if enforce_unique_values is not None:",
            "            payload[\"enforceUniqueValues\"] = enforce_unique_values",
            "        if hidden is not None:",
            "            payload[\"hidden\"] = hidden",
            "        if indexed is not None:",
            "            payload[\"indexed\"] = indexed",
            "        create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)",
            "",
            "    def get_items_by_sites_and_lists(",
            "        self, site_id: str, list_id: str, limit: int = None",
            "    ) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets items' information corresponding to a site and a list",
            "",
            "        site_id:  GUID of a site",
            "        list_id: GUID of a list",
            "        limit: limits the number of columns for which information is returned",
            "",
            "        Returns",
            "        response: metadata information/ fields corresponding to list-items of the site",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items?expand=fields&select=*\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def get_all_items(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all the items' information associated with the account",
            "",
            "        limit: puts a limit to the number of items returned",
            "",
            "        Returns",
            "        response: returns metadata information regarding all the items that are associated with that account",
            "        \"\"\"",
            "        sites = self.get_all_sites()",
            "        items = []",
            "        for site in sites:",
            "            site_id = site[\"id\"].split(\",\")[1]",
            "            for sharepoint_list in self.get_lists_by_site(site_id=site_id):",
            "                for item_dict in self.get_items_by_sites_and_lists(",
            "                    site_id=site[\"id\"].split(\",\")[1], list_id=sharepoint_list[\"id\"]",
            "                ):",
            "                    item_dict[\"siteName\"] = site[\"name\"]",
            "                    item_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]",
            "                    item_dict[\"listId\"] = sharepoint_list[\"id\"]",
            "                    item_dict[\"list_name\"] = sharepoint_list[\"displayName\"]",
            "                    items.append(item_dict)",
            "        if limit:",
            "            items = items[:limit]",
            "        return items",
            "",
            "    def update_items(",
            "        self, item_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given items (item_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_item for every column",
            "",
            "        item_dict: A dictionary containing ids of the list-items which are to be updated and",
            "        also their site IDs",
            "        values_to_update: a dictionary which will be used to update the fields of the items",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for item_entry in item_dict:",
            "            self.update_an_item(",
            "                site_id=item_entry[\"siteId\"],",
            "                list_id=item_entry[\"listId\"],",
            "                item_id=item_entry[\"id\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_an_item(",
            "        self,",
            "        site_id: str,",
            "        list_id: str,",
            "        item_id: str,",
            "        values_to_update: Dict[Text, Any],",
            "    ):",
            "        \"\"\"",
            "        Updates an item with given values",
            "",
            "        item_id: GUID of the column",
            "        list_id: GUID of the list",
            "        site_id: GUID of the site in which the list is present",
            "        values_to_update: a dictionary values which will be used to update the properties of the list-item",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items/{item_id}\"",
            "        update_an_entity(",
            "            url=url, values_to_update=values_to_update, bearer_token=self.bearer_token",
            "        )",
            "",
            "    def delete_items(self, item_dict: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Deletes items for the given site ID and list ID",
            "",
            "        item_dict: a dictionary values containing the item IDs which are to be deleted,",
            "        their corresponding site IDs and their list IDs",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for item_entry in item_dict:",
            "            self.delete_an_item(",
            "                site_id=item_entry[\"siteId\"],",
            "                list_id=item_entry[\"listId\"],",
            "                item_id=item_entry[\"id\"],",
            "            )",
            "",
            "    def delete_an_item(self, site_id: str, list_id: str, item_id: str) -> None:",
            "        \"\"\"",
            "        Deletes an item, given its item ID, its site ID and its list ID",
            "",
            "        list_id: GUID of the list in which the site is present",
            "        site_id: GUID of the site in which the list is present",
            "        item_id: GUID of the item which is to be deleted",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items/{item_id}\"",
            "        delete_an_entity(url=url, bearer_token=self.bearer_token)",
            "",
            "    def create_items(self, data: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Creates items with the information provided in the data parameter",
            "        calls create_an_item for each entry of item metadata dictionary",
            "",
            "        data: parameter which contains information such as the site IDs and list IDs where the items",
            "        would be created and their metadata information which will be used to create them",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for entry in data:",
            "            self.create_an_item(",
            "                site_id=entry[\"siteId\"],",
            "                list_id=entry[\"listId\"],",
            "                fields=entry.get(\"fields\"),",
            "            )",
            "",
            "    def create_an_item(self, site_id: str, list_id: str, fields: str) -> None:",
            "        \"\"\"",
            "        Creates an item with metadata information provided in the params",
            "",
            "        site_id: GUID of the site where the list id present",
            "        list_id: GUID of the list where the item is to be created",
            "        fields: The values of the columns set on this list item.",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items/\"",
            "        payload = {}",
            "        if fields:",
            "            payload[\"fields\"] = eval(fields)",
            "        create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)"
        ],
        "afterPatchFile": [
            "import ast",
            "from datetime import datetime, timezone",
            "from typing import Text, List, Dict, Any",
            "",
            "from mindsdb.integrations.handlers.sharepoint_handler.utils import (",
            "    bearer_token_request,",
            "    get_an_entity,",
            "    delete_an_entity,",
            "    update_an_entity,",
            "    create_an_entity,",
            ")",
            "",
            "",
            "class SharepointAPI:",
            "    def __init__(",
            "        self, client_id: str = None, client_secret: str = None, tenant_id: str = None",
            "    ):",
            "        self.client_id = client_id",
            "        self.client_secret = client_secret",
            "        self.tenant_id = tenant_id",
            "        self.bearer_token = None",
            "        self.is_connected = False",
            "        self.expiration_time = datetime.now(timezone.utc).timestamp()",
            "",
            "    def get_bearer_token(self) -> None:",
            "        \"\"\"",
            "        Generates new bearer token for the credentials",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        response = bearer_token_request(",
            "            client_id=self.client_id,",
            "            tenant_id=self.tenant_id,",
            "            client_secret=self.client_secret,",
            "        )",
            "        self.bearer_token = response[\"access_token\"]",
            "        self.expiration_time = int(response[\"expires_on\"])",
            "        self.is_connected = True",
            "",
            "    def check_bearer_token_validity(self) -> bool:",
            "        \"\"\"",
            "        Provides information whether a valid bearer token is available or not. Returns true if available",
            "        otherwise false",
            "",
            "        Returns",
            "        bool",
            "        \"\"\"",
            "        if (",
            "            self.is_connected",
            "            and datetime.now(timezone.utc).astimezone().timestamp()",
            "            < self.expiration_time",
            "        ):",
            "            return True",
            "        else:",
            "            return False",
            "",
            "    def disconnect(self) -> None:",
            "        \"\"\"",
            "        Removes bearer token from the sharepoint API class (makes it null)",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        self.bearer_token = None",
            "        self.is_connected = False",
            "",
            "    def get_all_sites(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all sites associated with the account",
            "",
            "        limit: limits the number of site information to be returned",
            "",
            "        Returns",
            "        response: metadata information corresponding to all sites",
            "        \"\"\"",
            "        url = \"https://graph.microsoft.com/v1.0/sites?search=*\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def update_sites(",
            "        self, site_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given sites (site_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_site for every site",
            "        site_dict: A dictionary containing site ids of the sites which are to be updated",
            "        values_to_update: a dictionary which will be used to update the fields of the sites",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for site_entry in site_dict:",
            "            self.update_a_site(",
            "                site_id=site_entry[\"siteId\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_a_site(self, site_id: str, values_to_update: Dict[Text, Any]) -> None:",
            "        \"\"\"",
            "        Updates a site with given values",
            "        site_id: GUID of the site",
            "        values_to_update: a dictionary values which will be used to update the properties of the site",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/\"",
            "        update_an_entity(",
            "            url=url, values_to_update=values_to_update, bearer_token=self.bearer_token",
            "        )",
            "",
            "    def get_lists_by_site(",
            "        self, site_id: str, limit: int = None",
            "    ) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets lists' information corresponding to a site",
            "",
            "        site_id:  GUID of a site",
            "        limit: limits the number of lists for which information is returned",
            "",
            "        Returns",
            "        response: metadata information/ fields corresponding to lists of the site",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def get_all_lists(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all the lists' information assocaited with the account",
            "",
            "        limit: puts a limit to the number of lists returned",
            "",
            "        Returns",
            "        response: returns metadata information regarding all the lists that have been made using that account",
            "        \"\"\"",
            "        sites = self.get_all_sites()",
            "        lists = []",
            "        for site in sites:",
            "            for list_dict in self.get_lists_by_site(site_id=site[\"id\"].split(\",\")[1]):",
            "                list_dict[\"siteName\"] = site[\"name\"]",
            "                list_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]",
            "                lists.append(list_dict)",
            "        if limit:",
            "            lists = lists[:limit]",
            "        return lists",
            "",
            "    def delete_lists(self, list_dict: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Deletes lists for the given site ID and list ID",
            "",
            "        list_dict: a dictionary values containing the list IDs which are to be deleted and",
            "        their corresponding site IDs",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for list_entry in list_dict:",
            "            self.delete_a_list(site_id=list_entry[\"siteId\"], list_id=list_entry[\"id\"])",
            "",
            "    def delete_a_list(self, site_id: str, list_id: str) -> None:",
            "        \"\"\"",
            "        Deletes a list, given its list ID and its site ID",
            "",
            "        site_id: GUID of the site in which the list is present",
            "        list_id: GUID of the list which is to be deleted",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}\"",
            "        delete_an_entity(url=url, bearer_token=self.bearer_token)",
            "",
            "    def update_lists(",
            "        self, list_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given lists (list_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_list for every list",
            "        list_dict: A dictionary containing ids of the list which are to be updated and also their site IDs",
            "        values_to_update: a dictionary which will be used to update the fields of the lists",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for list_entry in list_dict:",
            "            self.update_a_list(",
            "                site_id=list_entry[\"siteId\"],",
            "                list_id=list_entry[\"id\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_a_list(",
            "        self, site_id: str, list_id: str, values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates a list with given values",
            "        list_id: GUID of the list",
            "        site_id: GUID of the site in which the list is present",
            "        values_to_update: a dictionary values which will be used to update the properties of the list",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/\"",
            "        update_an_entity(",
            "            url=url, bearer_token=self.bearer_token, values_to_update=values_to_update",
            "        )",
            "",
            "    def create_lists(self, data: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Creates lists with the information provided in the data parameter",
            "        calls create_a_list for each entry of list metadata dictionary",
            "",
            "        data: parameter which contains information such as the site IDs where the lists would be created",
            "        and their metadata information which will be used to create them",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for entry in data:",
            "            self.create_a_list(",
            "                site_id=entry[\"siteId\"],",
            "                column=entry.get(\"column\"),",
            "                display_name=entry[\"displayName\"],",
            "                list_template=entry[\"list\"],",
            "            )",
            "",
            "    def create_a_list(",
            "        self, site_id: str, list_template: str, display_name: str, column: str = None",
            "    ) -> None:",
            "        \"\"\"",
            "        Creates a list with metadata information provided in the params",
            "",
            "        site_id: GUID of the site where the list is to be created",
            "        list_template: a string which contains the list template information (type of list)",
            "            eg.- \"{'template': 'documentLibrary'}\"",
            "        display_name: the display name of the given list, which will be displayed in the site",
            "        column: specifies the list of columns that should be created for the list",
            "            eg.- \"[{'name': 'Author', 'text': { }},{'name': 'PageCount', 'number': { }}]\"",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/\"",
            "        payload = {}",
            "        if column:",
            "            column = ast.literal_eval(column)",
            "            payload[\"column\"] = column",
            "        payload[\"displayName\"] = display_name",
            "        payload[\"list\"] = ast.literal_eval(list_template)",
            "        create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)",
            "",
            "    def get_site_columns_by_site(",
            "        self, site_id: str, limit: int = None",
            "    ) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets columns' information corresponding to a site",
            "",
            "        site_id:  GUID of a site",
            "        limit: limits the number of columns for which information is returned",
            "",
            "        Returns",
            "        response: metadata information/ fields corresponding to columns of the site",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def get_all_site_columns(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all the columns' information associated with the account",
            "",
            "        limit: puts a limit to the number of columns returned",
            "",
            "        Returns",
            "        response: returns metadata information regarding all the columns that have been made using that account",
            "        \"\"\"",
            "        sites = self.get_all_sites()",
            "        site_columns = []",
            "        for site in sites:",
            "            for site_column_dict in self.get_site_columns_by_site(",
            "                site_id=site[\"id\"].split(\",\")[1]",
            "            ):",
            "                site_column_dict[\"siteName\"] = site[\"name\"]",
            "                site_column_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]",
            "                site_columns.append(site_column_dict)",
            "        if limit:",
            "            site_columns = site_columns[:limit]",
            "        return site_columns",
            "",
            "    def update_site_columns(",
            "        self,",
            "        site_column_dict: List[Dict[Text, Text]],",
            "        values_to_update: Dict[Text, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given columns (site_column_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_site_column for every column",
            "",
            "        site_column_dict: A dictionary containing ids of the column which are to be updated and",
            "        also their site IDs",
            "        values_to_update: a dictionary which will be used to update the fields of the columns",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for site_column_entry in site_column_dict:",
            "            self.update_a_site_column(",
            "                site_id=site_column_entry[\"siteId\"],",
            "                column_id=site_column_entry[\"id\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_a_site_column(",
            "        self, site_id: str, column_id: str, values_to_update: Dict[Text, Any]",
            "    ):",
            "        \"\"\"",
            "        Updates a column with given values",
            "",
            "        column_id: GUID of the column",
            "        site_id: GUID of the site in which the column is present",
            "        values_to_update: a dictionary values which will be used to update the properties of the column",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/{column_id}\"",
            "        update_an_entity(",
            "            url=url, values_to_update=values_to_update, bearer_token=self.bearer_token",
            "        )",
            "",
            "    def delete_site_columns(self, column_dict: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Deletes columns for the given site ID and column ID",
            "",
            "        column_dict: a dictionary values containing the column IDs which are to be deleted and",
            "        their corresponding site IDs",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for column_entry in column_dict:",
            "            self.delete_a_site_columns(",
            "                site_id=column_entry[\"siteId\"], column_id=column_entry[\"id\"]",
            "            )",
            "",
            "    def delete_a_site_columns(self, site_id: str, column_id: str) -> None:",
            "        \"\"\"",
            "        Deletes a column, given its column ID and its site ID",
            "",
            "        site_id: GUID of the site in which the column is present",
            "        column_id: GUID of the column which is to be deleted",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/{column_id}\"",
            "        delete_an_entity(url=url, bearer_token=self.bearer_token)",
            "",
            "    def create_site_columns(self, data: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Creates columns with the information provided in the data parameter",
            "        calls create_a_site_column for each entry of column metadata dictionary",
            "",
            "        data: parameter which contains information such as the site IDs where the columns would be created",
            "        and their metadata information which will be used to create them",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for entry in data:",
            "            self.create_a_site_column(",
            "                site_id=entry[\"siteId\"],",
            "                enforce_unique_values=entry.get(\"enforceUniqueValues\"),",
            "                hidden=entry.get(\"hidden\"),",
            "                indexed=entry.get(\"indexed\"),",
            "                name=entry[\"name\"],",
            "                text=entry.get(\"text\"),",
            "            )",
            "",
            "    def create_a_site_column(",
            "        self,",
            "        site_id: str,",
            "        enforce_unique_values: bool,",
            "        hidden: bool,",
            "        indexed: bool,",
            "        name: str,",
            "        text: str = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Creates a list with metadata information provided in the params",
            "",
            "        site_id: GUID of the site where the column is to be created",
            "        enforced_unique_values: if true, no two list items may have the same value for this column",
            "        hidden: specifies whether the column is displayed in the user interface",
            "        name: the API-facing name of the column as it appears in the fields on a listItem.",
            "        text: details regarding the text values in the column",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/columns/\"",
            "        payload = {}",
            "        if text:",
            "            text = ast.literal_eval(text)",
            "            payload[\"text\"] = text",
            "        payload[\"name\"] = name",
            "        if enforce_unique_values is not None:",
            "            payload[\"enforceUniqueValues\"] = enforce_unique_values",
            "        if hidden is not None:",
            "            payload[\"hidden\"] = hidden",
            "        if indexed is not None:",
            "            payload[\"indexed\"] = indexed",
            "        create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)",
            "",
            "    def get_items_by_sites_and_lists(",
            "        self, site_id: str, list_id: str, limit: int = None",
            "    ) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets items' information corresponding to a site and a list",
            "",
            "        site_id:  GUID of a site",
            "        list_id: GUID of a list",
            "        limit: limits the number of columns for which information is returned",
            "",
            "        Returns",
            "        response: metadata information/ fields corresponding to list-items of the site",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items?expand=fields&select=*\"",
            "        response = get_an_entity(url=url, bearer_token=self.bearer_token)",
            "        if limit:",
            "            response = response[:limit]",
            "        return response",
            "",
            "    def get_all_items(self, limit: int = None) -> List[Dict[Text, Any]]:",
            "        \"\"\"",
            "        Gets all the items' information associated with the account",
            "",
            "        limit: puts a limit to the number of items returned",
            "",
            "        Returns",
            "        response: returns metadata information regarding all the items that are associated with that account",
            "        \"\"\"",
            "        sites = self.get_all_sites()",
            "        items = []",
            "        for site in sites:",
            "            site_id = site[\"id\"].split(\",\")[1]",
            "            for sharepoint_list in self.get_lists_by_site(site_id=site_id):",
            "                for item_dict in self.get_items_by_sites_and_lists(",
            "                    site_id=site[\"id\"].split(\",\")[1], list_id=sharepoint_list[\"id\"]",
            "                ):",
            "                    item_dict[\"siteName\"] = site[\"name\"]",
            "                    item_dict[\"siteId\"] = site[\"id\"].split(\",\")[1]",
            "                    item_dict[\"listId\"] = sharepoint_list[\"id\"]",
            "                    item_dict[\"list_name\"] = sharepoint_list[\"displayName\"]",
            "                    items.append(item_dict)",
            "        if limit:",
            "            items = items[:limit]",
            "        return items",
            "",
            "    def update_items(",
            "        self, item_dict: List[Dict[Text, Text]], values_to_update: Dict[Text, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Updates the given items (item_dict) with the provided values (values_to_update)",
            "        Calls the function update_a_item for every column",
            "",
            "        item_dict: A dictionary containing ids of the list-items which are to be updated and",
            "        also their site IDs",
            "        values_to_update: a dictionary which will be used to update the fields of the items",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for item_entry in item_dict:",
            "            self.update_an_item(",
            "                site_id=item_entry[\"siteId\"],",
            "                list_id=item_entry[\"listId\"],",
            "                item_id=item_entry[\"id\"],",
            "                values_to_update=values_to_update,",
            "            )",
            "",
            "    def update_an_item(",
            "        self,",
            "        site_id: str,",
            "        list_id: str,",
            "        item_id: str,",
            "        values_to_update: Dict[Text, Any],",
            "    ):",
            "        \"\"\"",
            "        Updates an item with given values",
            "",
            "        item_id: GUID of the column",
            "        list_id: GUID of the list",
            "        site_id: GUID of the site in which the list is present",
            "        values_to_update: a dictionary values which will be used to update the properties of the list-item",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items/{item_id}\"",
            "        update_an_entity(",
            "            url=url, values_to_update=values_to_update, bearer_token=self.bearer_token",
            "        )",
            "",
            "    def delete_items(self, item_dict: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Deletes items for the given site ID and list ID",
            "",
            "        item_dict: a dictionary values containing the item IDs which are to be deleted,",
            "        their corresponding site IDs and their list IDs",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for item_entry in item_dict:",
            "            self.delete_an_item(",
            "                site_id=item_entry[\"siteId\"],",
            "                list_id=item_entry[\"listId\"],",
            "                item_id=item_entry[\"id\"],",
            "            )",
            "",
            "    def delete_an_item(self, site_id: str, list_id: str, item_id: str) -> None:",
            "        \"\"\"",
            "        Deletes an item, given its item ID, its site ID and its list ID",
            "",
            "        list_id: GUID of the list in which the site is present",
            "        site_id: GUID of the site in which the list is present",
            "        item_id: GUID of the item which is to be deleted",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items/{item_id}\"",
            "        delete_an_entity(url=url, bearer_token=self.bearer_token)",
            "",
            "    def create_items(self, data: List[Dict[Text, Any]]) -> None:",
            "        \"\"\"",
            "        Creates items with the information provided in the data parameter",
            "        calls create_an_item for each entry of item metadata dictionary",
            "",
            "        data: parameter which contains information such as the site IDs and list IDs where the items",
            "        would be created and their metadata information which will be used to create them",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        for entry in data:",
            "            self.create_an_item(",
            "                site_id=entry[\"siteId\"],",
            "                list_id=entry[\"listId\"],",
            "                fields=entry.get(\"fields\"),",
            "            )",
            "",
            "    def create_an_item(self, site_id: str, list_id: str, fields: str) -> None:",
            "        \"\"\"",
            "        Creates an item with metadata information provided in the params",
            "",
            "        site_id: GUID of the site where the list id present",
            "        list_id: GUID of the list where the item is to be created",
            "        fields: The values of the columns set on this list item.",
            "",
            "        Returns",
            "        None",
            "        \"\"\"",
            "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_id}/items/\"",
            "        payload = {}",
            "        if fields:",
            "            payload[\"fields\"] = ast.literal_eval(fields)",
            "        create_an_entity(url=url, payload=payload, bearer_token=self.bearer_token)"
        ],
        "action": [
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0"
        ],
        "dele_reviseLocation": {
            "252": [
                "SharepointAPI",
                "create_a_list"
            ],
            "255": [
                "SharepointAPI",
                "create_a_list"
            ],
            "413": [
                "SharepointAPI",
                "create_a_site_column"
            ],
            "577": [
                "SharepointAPI",
                "create_an_item"
            ]
        },
        "addLocation": []
    },
    "mindsdb/integrations/handlers/weaviate_handler/weaviate_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+import ast"
            },
            "1": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " from datetime import datetime"
            },
            "2": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import List, Optional"
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "4": {
                "beforePatchRowNumber": 335,
                "afterPatchRowNumber": 336,
                "PatchRowcode": "             # assuming there would be only one vector based search per query"
            },
            "5": {
                "beforePatchRowNumber": 336,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "             vector_filter = vector_filter[0]"
            },
            "6": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "             near_vector = {"
            },
            "7": {
                "beforePatchRowNumber": 338,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"vector\": eval(vector_filter.value)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 339,
                "PatchRowcode": "+                \"vector\": ast.literal_eval(vector_filter.value)"
            },
            "9": {
                "beforePatchRowNumber": 339,
                "afterPatchRowNumber": 340,
                "PatchRowcode": "                 if isinstance(vector_filter.value, str)"
            },
            "10": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "                 else vector_filter.value"
            },
            "11": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": 342,
                "PatchRowcode": "             }"
            }
        },
        "frontPatchFile": [
            "from datetime import datetime",
            "from typing import List, Optional",
            "",
            "import weaviate",
            "from weaviate.embedded import EmbeddedOptions",
            "import pandas as pd",
            "",
            "from mindsdb.integrations.libs.response import RESPONSE_TYPE",
            "from mindsdb.integrations.libs.response import HandlerResponse",
            "from mindsdb.integrations.libs.response import HandlerResponse as Response",
            "from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse",
            "from mindsdb.integrations.libs.vectordatabase_handler import (",
            "    FilterCondition,",
            "    FilterOperator,",
            "    TableField,",
            "    VectorStoreHandler,",
            ")",
            "from mindsdb.utilities import log",
            "from weaviate.util import generate_uuid5",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "class WeaviateDBHandler(VectorStoreHandler):",
            "    \"\"\"This handler handles connection and execution of the Weaviate statements.\"\"\"",
            "",
            "    name = \"weaviate\"",
            "",
            "    def __init__(self, name: str, **kwargs):",
            "        super().__init__(name)",
            "",
            "        self._connection_data = kwargs.get(\"connection_data\")",
            "",
            "        self._client_config = {",
            "            \"weaviate_url\": self._connection_data.get(\"weaviate_url\"),",
            "            \"weaviate_api_key\": self._connection_data.get(\"weaviate_api_key\"),",
            "            \"persistence_directory\": self._connection_data.get(\"persistence_directory\"),",
            "        }",
            "",
            "        if not (",
            "            self._client_config.get(\"weaviate_url\")",
            "            or self._client_config.get(\"persistence_directory\")",
            "        ):",
            "            raise Exception(",
            "                \"Either url or persist_directory is required for weaviate connection!\"",
            "            )",
            "",
            "        self._client = None",
            "        self._embedded_options = None",
            "        self.is_connected = False",
            "        self.connect()",
            "",
            "    def _get_client(self) -> weaviate.Client:",
            "        if not (",
            "            self._client_config",
            "            and (",
            "                self._client_config.get(\"weaviate_url\")",
            "                or self._client_config.get(\"persistence_directory\")",
            "            )",
            "        ):",
            "            raise Exception(\"Client config is not set! or missing parameters\")",
            "",
            "        # decide the client type to be used, either persistent or httpclient",
            "        if self._client_config.get(\"persistence_directory\"):",
            "            self._embedded_options = EmbeddedOptions(",
            "                persistence_data_path=self._client_config.get(\"persistence_directory\")",
            "            )",
            "            return weaviate.Client(embedded_options=self._embedded_options)",
            "        if self._client_config.get(\"weaviate_api_key\"):",
            "            return weaviate.Client(",
            "                url=self._client_config[\"weaviate_url\"],",
            "                auth_client_secret=weaviate.AuthApiKey(",
            "                    api_key=self._client_config[\"weaviate_api_key\"]",
            "                ),",
            "            )",
            "        return weaviate.Client(url=self._client_config[\"weaviate_url\"])",
            "",
            "    def __del__(self):",
            "        self.is_connected = False",
            "        if self._embedded_options:",
            "            self._client._connection.embedded_db.stop()",
            "            del self._embedded_options",
            "        self._embedded_options = None",
            "        self._client._connection.close()",
            "        if self._client:",
            "            del self._client",
            "",
            "    def connect(self):",
            "        \"\"\"Connect to a weaviate database.\"\"\"",
            "        if self.is_connected:",
            "            return self._client",
            "",
            "        try:",
            "            self._client = self._get_client()",
            "            self.is_connected = True",
            "            return self._client",
            "        except Exception as e:",
            "            logger.error(f\"Error connecting to weaviate client, {e}!\")",
            "            self.is_connected = False",
            "",
            "    def disconnect(self):",
            "        \"\"\"Close the database connection.\"\"\"",
            "",
            "        if not self.is_connected:",
            "            return",
            "        if self._embedded_options:",
            "            self._client._connection.embedded_db.stop()",
            "            del self._embedded_options",
            "        del self._client",
            "        self._embedded_options = None",
            "        self._client = None",
            "        self.is_connected = False",
            "",
            "    def check_connection(self):",
            "        \"\"\"Check the connection to the Weaviate database.\"\"\"",
            "        response_code = StatusResponse(False)",
            "",
            "        try:",
            "            if self._client.is_live():",
            "                response_code.success = True",
            "        except Exception as e:",
            "            logger.error(f\"Error connecting to weaviate , {e}!\")",
            "            response_code.error_message = str(e)",
            "        finally:",
            "            if response_code.success and not self.is_connected:",
            "                self.disconnect()",
            "            if not response_code.success and self.is_connected:",
            "                self.is_connected = False",
            "",
            "        return response_code",
            "",
            "    @staticmethod",
            "    def _get_weaviate_operator(operator: FilterOperator) -> str:",
            "        mapping = {",
            "            FilterOperator.EQUAL: \"Equal\",",
            "            FilterOperator.NOT_EQUAL: \"NotEqual\",",
            "            FilterOperator.LESS_THAN: \"LessThan\",",
            "            FilterOperator.LESS_THAN_OR_EQUAL: \"LessThanEqual\",",
            "            FilterOperator.GREATER_THAN: \"GreaterThan\",",
            "            FilterOperator.GREATER_THAN_OR_EQUAL: \"GreaterThanEqual\",",
            "            FilterOperator.IS_NULL: \"IsNull\",",
            "            FilterOperator.LIKE: \"Like\",",
            "        }",
            "",
            "        if operator not in mapping:",
            "            raise Exception(f\"Operator {operator} is not supported by weaviate!\")",
            "",
            "        return mapping[operator]",
            "",
            "    @staticmethod",
            "    def _get_weaviate_value_type(value) -> str:",
            "        # https://github.com/weaviate/weaviate-python-client/blob/c760b1d59b2a222e770d53cc257b1bf993a0a592/weaviate/gql/filter.py#L18",
            "        if isinstance(value, list):",
            "            value_list_types = {",
            "                str: \"valueTextList\",",
            "                int: \"valueIntList\",",
            "                float: \"valueIntList\",",
            "                bool: \"valueBooleanList\",",
            "            }",
            "            if not value:",
            "                raise Exception(\"Empty list is not supported\")",
            "            value_type = value_list_types.get(type(value[0]))",
            "",
            "        else:",
            "            value_primitive_types = {",
            "                str: \"valueText\",",
            "                int: \"valueInt\",",
            "                float: \"valueInt\",",
            "                datetime: \"valueDate\",",
            "                bool: \"valueBoolean\",",
            "            }",
            "            value_type = value_primitive_types.get(type(value))",
            "",
            "        if not value_type:",
            "            raise Exception(f\"Value type {type(value)} is not supported by weaviate!\")",
            "",
            "        return value_type",
            "",
            "    def _translate_condition(",
            "        self,",
            "        table_name: str,",
            "        conditions: List[FilterCondition] = None,",
            "        meta_conditions: List[FilterCondition] = None,",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Translate a list of FilterCondition objects a dict that can be used by Weaviate.",
            "        E.g.,",
            "        [",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.LESS_THAN,",
            "                value=\"2020-01-01\",",
            "            ),",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.GREATER_THAN,",
            "                value=\"2019-01-01\",",
            "            )",
            "        ]",
            "        -->",
            "        {\"operator\": \"And\",",
            "        \"operands\": [",
            "            {",
            "                \"path\": [\"created_at\"],",
            "                \"operator\": \"LessThan\",",
            "                \"valueText\": \"2020-01-01\",",
            "            },",
            "            {",
            "                \"path\": [\"created_at\"],",
            "                \"operator\": \"GreaterThan\",",
            "                \"valueInt\": \"2019-01-01\",",
            "            },",
            "        ]}",
            "        \"\"\"",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        #",
            "        if not (conditions or meta_conditions):",
            "            return None",
            "",
            "        # we translate each condition into a single dict",
            "        #  conditions on columns",
            "        weaviate_conditions = []",
            "        if conditions:",
            "            for condition in conditions:",
            "                column_key = condition.column",
            "                value_type = self._get_weaviate_value_type(condition.value)",
            "                weaviate_conditions.append(",
            "                    {",
            "                        \"path\": [column_key],",
            "                        \"operator\": self._get_weaviate_operator(condition.op),",
            "                        value_type: condition.value,",
            "                    }",
            "                )",
            "        # condition on metadata columns",
            "        if meta_conditions:",
            "            for condition in meta_conditions:",
            "                meta_key = condition.column.split(\".\")[-1]",
            "                value_type = self._get_weaviate_value_type(condition.value)",
            "                weaviate_conditions.append(",
            "                    {",
            "                        \"path\": [",
            "                            \"associatedMetadata\",",
            "                            metadata_table_name,",
            "                            meta_key,",
            "                        ],",
            "                        \"operator\": self._get_weaviate_operator(condition.op),",
            "                        value_type: condition.value,",
            "                    }",
            "                )",
            "",
            "        # we combine all conditions into a single dict",
            "        all_conditions = (",
            "            {\"operator\": \"And\", \"operands\": weaviate_conditions}",
            "            # combining all conditions if there are more than one conditions",
            "            if len(weaviate_conditions) > 1",
            "            # only a single condition",
            "            else weaviate_conditions[0]",
            "        )",
            "        return all_conditions",
            "",
            "    def select(",
            "        self,",
            "        table_name: str,",
            "        columns: List[str] = None,",
            "        conditions: List[FilterCondition] = None,",
            "        offset: int = None,",
            "        limit: int = None,",
            "    ) -> HandlerResponse:",
            "        table_name = table_name.capitalize()",
            "        # columns which we will always provide in the result",
            "        filters = None",
            "        if conditions:",
            "            non_metadata_conditions = [",
            "                condition",
            "                for condition in conditions",
            "                if not condition.column.startswith(TableField.METADATA.value)",
            "                and condition.column != TableField.SEARCH_VECTOR.value",
            "                and condition.column != TableField.EMBEDDINGS.value",
            "            ]",
            "            metadata_conditions = [",
            "                condition",
            "                for condition in conditions",
            "                if condition.column.startswith(TableField.METADATA.value)",
            "            ]",
            "            filters = self._translate_condition(",
            "                table_name,",
            "                non_metadata_conditions if non_metadata_conditions else None,",
            "                metadata_conditions if metadata_conditions else None,",
            "            )",
            "",
            "        # check if embedding vector filter is present",
            "        vector_filter = (",
            "            None",
            "            if not conditions",
            "            else [",
            "                condition",
            "                for condition in conditions",
            "                if condition.column == TableField.SEARCH_VECTOR.value",
            "                or condition.column == TableField.EMBEDDINGS.value",
            "            ]",
            "        )",
            "",
            "        for col in [\"id\", \"embeddings\", \"distance\", \"metadata\"]:",
            "            if col in columns:",
            "                columns.remove(col)",
            "",
            "        metadata_table = table_name.capitalize() + \"_metadata\"",
            "",
            "        metadata_fields = \" \".join(",
            "            [",
            "                prop[\"name\"]",
            "                for prop in self._client.schema.get(metadata_table)[\"properties\"]",
            "            ]",
            "        )",
            "",
            "        # query to get all metadata fields",
            "        metadata_query = (",
            "            f\"associatedMetadata {{ ... on {metadata_table} {{ {metadata_fields} }} }}\"",
            "        )",
            "",
            "        if columns:",
            "            query = self._client.query.get(",
            "                table_name,",
            "                columns + [metadata_query],",
            "            ).with_additional([\"id vector distance\"])",
            "        else:",
            "            query = self._client.query.get(",
            "                table_name,",
            "                [metadata_query],",
            "            ).with_additional([\"id vector distance\"])",
            "        if vector_filter:",
            "            # similarity search",
            "            # assuming the similarity search is on content",
            "            # assuming there would be only one vector based search per query",
            "            vector_filter = vector_filter[0]",
            "            near_vector = {",
            "                \"vector\": eval(vector_filter.value)",
            "                if isinstance(vector_filter.value, str)",
            "                else vector_filter.value",
            "            }",
            "            query = query.with_near_vector(near_vector)",
            "        if filters:",
            "            query = query.with_where(filters)",
            "        if limit:",
            "            query = query.with_limit(limit)",
            "        result = query.do()",
            "        result = result[\"data\"][\"Get\"][table_name.capitalize()]",
            "        ids = [query_obj[\"_additional\"][\"id\"] for query_obj in result]",
            "        contents = [query_obj.get(\"content\") for query_obj in result]",
            "        distances = [",
            "            query_obj.get(\"_additional\").get(\"distance\") for query_obj in result",
            "        ]",
            "        # distances will be null for non vector/embedding query",
            "        vectors = [query_obj.get(\"_additional\").get(\"vector\") for query_obj in result]",
            "        metadatas = [query_obj.get(\"associatedMetadata\")[0] for query_obj in result]",
            "",
            "        payload = {",
            "            TableField.ID.value: ids,",
            "            TableField.CONTENT.value: contents,",
            "            TableField.METADATA.value: metadatas,",
            "            TableField.EMBEDDINGS.value: vectors,",
            "            TableField.DISTANCE.value: distances,",
            "        }",
            "",
            "        if columns:",
            "            payload = {",
            "                column: payload[column]",
            "                for column in columns + [\"id\", \"embeddings\", \"distance\", \"metadata\"]",
            "                if column != TableField.EMBEDDINGS.value",
            "            }",
            "",
            "        # always include distance",
            "        if distances:",
            "            payload[TableField.DISTANCE.value] = distances",
            "        result_df = pd.DataFrame(payload)",
            "        return Response(resp_type=RESPONSE_TYPE.TABLE, data_frame=result_df)",
            "",
            "    def insert(",
            "        self, table_name: str, data: pd.DataFrame, columns: List[str] = None",
            "    ) -> HandlerResponse:",
            "        \"\"\"",
            "        Insert data into the Weaviate database.",
            "        \"\"\"",
            "",
            "        table_name = table_name.capitalize()",
            "",
            "        # drop columns with all None values",
            "",
            "        data.dropna(axis=1, inplace=True)",
            "",
            "        data = data.to_dict(orient=\"records\")",
            "        # parsing the records one by one as we need to update metadata (which has variable columns)",
            "        for record in data:",
            "            metadata_data = record.get(TableField.METADATA.value)",
            "            data_object = {\"content\": record.get(TableField.CONTENT.value)}",
            "            data_obj_id = (",
            "                record[TableField.ID.value]",
            "                if TableField.ID.value in record.keys()",
            "                else generate_uuid5(data_object)",
            "            )",
            "            obj_id = self._client.data_object.create(",
            "                data_object=data_object,",
            "                class_name=table_name,",
            "                vector=record[TableField.EMBEDDINGS.value],",
            "                uuid=data_obj_id,",
            "            )",
            "            if metadata_data:",
            "                meta_id = self.add_metadata(metadata_data, table_name)",
            "                self._client.data_object.reference.add(",
            "                    from_uuid=obj_id,",
            "                    from_property_name=\"associatedMetadata\",",
            "                    to_uuid=meta_id,",
            "                )",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def update(",
            "        self, table_name: str, data: pd.DataFrame, columns: List[str] = None",
            "    ) -> HandlerResponse:",
            "        \"\"\"",
            "        Update data in the weaviate database.",
            "        \"\"\"",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        data_list = data.to_dict(\"records\")",
            "        for row in data_list:",
            "            non_metadata_keys = [",
            "                key",
            "                for key in row.keys()",
            "                if key and not key.startswith(TableField.METADATA.value)",
            "            ]",
            "            metadata_keys = [",
            "                key.split(\".\")[1]",
            "                for key in row.keys()",
            "                if key and key.startswith(TableField.METADATA.value)",
            "            ]",
            "",
            "            id_filter = {\"path\": [\"id\"], \"operator\": \"Equal\", \"valueText\": row[\"id\"]}",
            "            metadata_id_query = f\"associatedMetadata {{ ... on {metadata_table_name} {{ _additional {{ id }} }} }}\"",
            "            result = (",
            "                self._client.query.get(table_name, metadata_id_query)",
            "                .with_additional([\"id\"])",
            "                .with_where(id_filter)",
            "                .do()",
            "            )",
            "",
            "            metadata_id = result[\"data\"][\"Get\"][table_name][0][\"associatedMetadata\"][0][",
            "                \"_additional\"",
            "            ][\"id\"][0]",
            "            # updating table",
            "            self._client.data_object.update(",
            "                uuid=row[\"id\"],",
            "                class_name=table_name,",
            "                data_object={key: row[key] for key in non_metadata_keys},",
            "            )",
            "            # updating metadata",
            "            self._client.data_object.update(",
            "                uuid=metadata_id,",
            "                class_name=metadata_table_name,",
            "                data_object={key: row[key] for key in metadata_keys},",
            "            )",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def delete(",
            "        self, table_name: str, conditions: List[FilterCondition] = None",
            "    ) -> HandlerResponse:",
            "        table_name = table_name.capitalize()",
            "        non_metadata_conditions = [",
            "            condition",
            "            for condition in conditions",
            "            if not condition.column.startswith(TableField.METADATA.value)",
            "            and condition.column != TableField.SEARCH_VECTOR.value",
            "            and condition.column != TableField.EMBEDDINGS.value",
            "        ]",
            "        metadata_conditions = [",
            "            condition",
            "            for condition in conditions",
            "            if condition.column.startswith(TableField.METADATA.value)",
            "        ]",
            "        filters = self._translate_condition(",
            "            table_name,",
            "            non_metadata_conditions if non_metadata_conditions else None,",
            "            metadata_conditions if metadata_conditions else None,",
            "        )",
            "        if not filters:",
            "            raise Exception(\"Delete query must have at least one condition!\")",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        # query to get metadata ids",
            "        metadata_query = f\"associatedMetadata {{ ... on {metadata_table_name} {{ _additional {{ id }} }} }}\"",
            "        result = (",
            "            self._client.query.get(table_name, metadata_query)",
            "            .with_additional([\"id\"])",
            "            .with_where(filters)",
            "            .do()",
            "        )",
            "        result = result[\"data\"][\"Get\"][table_name]",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        table_ids = []",
            "        metadata_ids = []",
            "        for i in result:",
            "            table_ids.append(i[\"_additional\"][\"id\"])",
            "            metadata_ids.append(i[\"associatedMetadata\"][0][\"_additional\"][\"id\"])",
            "        self._client.batch.delete_objects(",
            "            class_name=table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": table_ids,",
            "            },",
            "        )",
            "        self._client.batch.delete_objects(",
            "            class_name=metadata_table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": metadata_ids,",
            "            },",
            "        )",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def create_table(self, table_name: str, if_not_exists=True) -> HandlerResponse:",
            "        \"\"\"",
            "        Create a class with the given name in the weaviate database.",
            "        \"\"\"",
            "        # separate metadata table for each table (as different tables will have different metadata columns)",
            "        # this reduces the query time using metadata but increases the insertion time",
            "        metadata_table_name = table_name + \"_metadata\"",
            "        if not self._client.schema.exists(metadata_table_name):",
            "            self._client.schema.create_class({\"class\": metadata_table_name})",
            "        if not self._client.schema.exists(table_name):",
            "            self._client.schema.create_class(",
            "                {",
            "                    \"class\": table_name,",
            "                    \"properties\": [",
            "                        {\"dataType\": [\"text\"], \"name\": prop[\"name\"]}",
            "                        for prop in self.SCHEMA",
            "                        if prop[\"name\"] != \"id\"",
            "                        and prop[\"name\"] != \"embeddings\"",
            "                        and prop[\"name\"] != \"metadata\"",
            "                    ],",
            "                    \"vectorIndexType\": \"hnsw\",",
            "                }",
            "            )",
            "            add_prop = {",
            "                \"name\": \"associatedMetadata\",",
            "                \"dataType\": [metadata_table_name.capitalize()],",
            "            }",
            "            self._client.schema.property.create(table_name.capitalize(), add_prop)",
            "",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def drop_table(self, table_name: str, if_exists=True) -> HandlerResponse:",
            "        \"\"\"",
            "        Delete a class from the weaviate database.",
            "        \"\"\"",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        table_id_query = self._client.query.get(table_name).with_additional([\"id\"]).do()",
            "        table_ids = [",
            "            i[\"_additional\"][\"id\"] for i in table_id_query[\"data\"][\"Get\"][table_name]",
            "        ]",
            "        metadata_table_id_query = (",
            "            self._client.query.get(metadata_table_name).with_additional([\"id\"]).do()",
            "        )",
            "        metadata_ids = [",
            "            i[\"_additional\"][\"id\"]",
            "            for i in metadata_table_id_query[\"data\"][\"Get\"][metadata_table_name]",
            "        ]",
            "        self._client.batch.delete_objects(",
            "            class_name=table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": table_ids,",
            "            },",
            "        )",
            "        self._client.batch.delete_objects(",
            "            class_name=metadata_table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": metadata_ids,",
            "            },",
            "        )",
            "        try:",
            "            self._client.schema.delete_class(table_name)",
            "            self._client.schema.delete_class(metadata_table_name)",
            "        except ValueError:",
            "            if if_exists:",
            "                return Response(resp_type=RESPONSE_TYPE.OK)",
            "            else:",
            "                return Response(",
            "                    resp_type=RESPONSE_TYPE.ERROR,",
            "                    error_message=f\"Table {table_name} does not exist!\",",
            "                )",
            "",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def get_tables(self) -> HandlerResponse:",
            "        \"\"\"",
            "        Get the list of tables in the Weaviate database.",
            "        \"\"\"",
            "        query_tables = self._client.schema.get()",
            "        tables = []",
            "        if query_tables:",
            "            tables = [table[\"class\"] for table in query_tables[\"classes\"]]",
            "        table_name = pd.DataFrame(",
            "            columns=[\"table_name\"],",
            "            data=tables,",
            "        )",
            "        return Response(resp_type=RESPONSE_TYPE.TABLE, data_frame=table_name)",
            "",
            "    def get_columns(self, table_name: str) -> HandlerResponse:",
            "        table_name = table_name.capitalize()",
            "        # check if table exists",
            "        try:",
            "            table = self._client.schema.get(table_name)",
            "        except ValueError:",
            "            return Response(",
            "                resp_type=RESPONSE_TYPE.ERROR,",
            "                error_message=f\"Table {table_name} does not exist!\",",
            "            )",
            "        data = pd.DataFrame(",
            "            data=[",
            "                {\"COLUMN_NAME\": column[\"name\"], \"DATA_TYPE\": column[\"dataType\"][0]}",
            "                for column in table[\"properties\"]",
            "            ]",
            "        )",
            "        return Response(data_frame=data, resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def add_metadata(self, data: dict, table_name: str):",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        self._client.schema.get(metadata_table_name)",
            "        # getting existing metadata fields",
            "        added_prop_list = [",
            "            prop[\"name\"]",
            "            for prop in self._client.schema.get(metadata_table_name)[\"properties\"]",
            "        ]",
            "        # as metadata columns are not fixed, at every entry, a check takes place for the columns",
            "        for prop in data.keys():",
            "            if prop not in added_prop_list:",
            "                if isinstance(data[prop], int):",
            "                    add_prop = {",
            "                        \"name\": prop,",
            "                        \"dataType\": [\"int\"],",
            "                    }",
            "                elif isinstance(data[prop][0], datetime):",
            "                    add_prop = {",
            "                        \"name\": prop,",
            "                        \"dataType\": [\"date\"],",
            "                    }",
            "                else:",
            "                    add_prop = {",
            "                        \"name\": prop,",
            "                        \"dataType\": [\"string\"],",
            "                    }",
            "                # when a new column is identified, it is added to the metadata table",
            "                self._client.schema.property.create(metadata_table_name, add_prop)",
            "        metadata_id = self._client.data_object.create(",
            "            data_object=data, class_name=table_name.capitalize() + \"_metadata\"",
            "        )",
            "        return metadata_id"
        ],
        "afterPatchFile": [
            "import ast",
            "from datetime import datetime",
            "from typing import List, Optional",
            "",
            "import weaviate",
            "from weaviate.embedded import EmbeddedOptions",
            "import pandas as pd",
            "",
            "from mindsdb.integrations.libs.response import RESPONSE_TYPE",
            "from mindsdb.integrations.libs.response import HandlerResponse",
            "from mindsdb.integrations.libs.response import HandlerResponse as Response",
            "from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse",
            "from mindsdb.integrations.libs.vectordatabase_handler import (",
            "    FilterCondition,",
            "    FilterOperator,",
            "    TableField,",
            "    VectorStoreHandler,",
            ")",
            "from mindsdb.utilities import log",
            "from weaviate.util import generate_uuid5",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "class WeaviateDBHandler(VectorStoreHandler):",
            "    \"\"\"This handler handles connection and execution of the Weaviate statements.\"\"\"",
            "",
            "    name = \"weaviate\"",
            "",
            "    def __init__(self, name: str, **kwargs):",
            "        super().__init__(name)",
            "",
            "        self._connection_data = kwargs.get(\"connection_data\")",
            "",
            "        self._client_config = {",
            "            \"weaviate_url\": self._connection_data.get(\"weaviate_url\"),",
            "            \"weaviate_api_key\": self._connection_data.get(\"weaviate_api_key\"),",
            "            \"persistence_directory\": self._connection_data.get(\"persistence_directory\"),",
            "        }",
            "",
            "        if not (",
            "            self._client_config.get(\"weaviate_url\")",
            "            or self._client_config.get(\"persistence_directory\")",
            "        ):",
            "            raise Exception(",
            "                \"Either url or persist_directory is required for weaviate connection!\"",
            "            )",
            "",
            "        self._client = None",
            "        self._embedded_options = None",
            "        self.is_connected = False",
            "        self.connect()",
            "",
            "    def _get_client(self) -> weaviate.Client:",
            "        if not (",
            "            self._client_config",
            "            and (",
            "                self._client_config.get(\"weaviate_url\")",
            "                or self._client_config.get(\"persistence_directory\")",
            "            )",
            "        ):",
            "            raise Exception(\"Client config is not set! or missing parameters\")",
            "",
            "        # decide the client type to be used, either persistent or httpclient",
            "        if self._client_config.get(\"persistence_directory\"):",
            "            self._embedded_options = EmbeddedOptions(",
            "                persistence_data_path=self._client_config.get(\"persistence_directory\")",
            "            )",
            "            return weaviate.Client(embedded_options=self._embedded_options)",
            "        if self._client_config.get(\"weaviate_api_key\"):",
            "            return weaviate.Client(",
            "                url=self._client_config[\"weaviate_url\"],",
            "                auth_client_secret=weaviate.AuthApiKey(",
            "                    api_key=self._client_config[\"weaviate_api_key\"]",
            "                ),",
            "            )",
            "        return weaviate.Client(url=self._client_config[\"weaviate_url\"])",
            "",
            "    def __del__(self):",
            "        self.is_connected = False",
            "        if self._embedded_options:",
            "            self._client._connection.embedded_db.stop()",
            "            del self._embedded_options",
            "        self._embedded_options = None",
            "        self._client._connection.close()",
            "        if self._client:",
            "            del self._client",
            "",
            "    def connect(self):",
            "        \"\"\"Connect to a weaviate database.\"\"\"",
            "        if self.is_connected:",
            "            return self._client",
            "",
            "        try:",
            "            self._client = self._get_client()",
            "            self.is_connected = True",
            "            return self._client",
            "        except Exception as e:",
            "            logger.error(f\"Error connecting to weaviate client, {e}!\")",
            "            self.is_connected = False",
            "",
            "    def disconnect(self):",
            "        \"\"\"Close the database connection.\"\"\"",
            "",
            "        if not self.is_connected:",
            "            return",
            "        if self._embedded_options:",
            "            self._client._connection.embedded_db.stop()",
            "            del self._embedded_options",
            "        del self._client",
            "        self._embedded_options = None",
            "        self._client = None",
            "        self.is_connected = False",
            "",
            "    def check_connection(self):",
            "        \"\"\"Check the connection to the Weaviate database.\"\"\"",
            "        response_code = StatusResponse(False)",
            "",
            "        try:",
            "            if self._client.is_live():",
            "                response_code.success = True",
            "        except Exception as e:",
            "            logger.error(f\"Error connecting to weaviate , {e}!\")",
            "            response_code.error_message = str(e)",
            "        finally:",
            "            if response_code.success and not self.is_connected:",
            "                self.disconnect()",
            "            if not response_code.success and self.is_connected:",
            "                self.is_connected = False",
            "",
            "        return response_code",
            "",
            "    @staticmethod",
            "    def _get_weaviate_operator(operator: FilterOperator) -> str:",
            "        mapping = {",
            "            FilterOperator.EQUAL: \"Equal\",",
            "            FilterOperator.NOT_EQUAL: \"NotEqual\",",
            "            FilterOperator.LESS_THAN: \"LessThan\",",
            "            FilterOperator.LESS_THAN_OR_EQUAL: \"LessThanEqual\",",
            "            FilterOperator.GREATER_THAN: \"GreaterThan\",",
            "            FilterOperator.GREATER_THAN_OR_EQUAL: \"GreaterThanEqual\",",
            "            FilterOperator.IS_NULL: \"IsNull\",",
            "            FilterOperator.LIKE: \"Like\",",
            "        }",
            "",
            "        if operator not in mapping:",
            "            raise Exception(f\"Operator {operator} is not supported by weaviate!\")",
            "",
            "        return mapping[operator]",
            "",
            "    @staticmethod",
            "    def _get_weaviate_value_type(value) -> str:",
            "        # https://github.com/weaviate/weaviate-python-client/blob/c760b1d59b2a222e770d53cc257b1bf993a0a592/weaviate/gql/filter.py#L18",
            "        if isinstance(value, list):",
            "            value_list_types = {",
            "                str: \"valueTextList\",",
            "                int: \"valueIntList\",",
            "                float: \"valueIntList\",",
            "                bool: \"valueBooleanList\",",
            "            }",
            "            if not value:",
            "                raise Exception(\"Empty list is not supported\")",
            "            value_type = value_list_types.get(type(value[0]))",
            "",
            "        else:",
            "            value_primitive_types = {",
            "                str: \"valueText\",",
            "                int: \"valueInt\",",
            "                float: \"valueInt\",",
            "                datetime: \"valueDate\",",
            "                bool: \"valueBoolean\",",
            "            }",
            "            value_type = value_primitive_types.get(type(value))",
            "",
            "        if not value_type:",
            "            raise Exception(f\"Value type {type(value)} is not supported by weaviate!\")",
            "",
            "        return value_type",
            "",
            "    def _translate_condition(",
            "        self,",
            "        table_name: str,",
            "        conditions: List[FilterCondition] = None,",
            "        meta_conditions: List[FilterCondition] = None,",
            "    ) -> Optional[dict]:",
            "        \"\"\"",
            "        Translate a list of FilterCondition objects a dict that can be used by Weaviate.",
            "        E.g.,",
            "        [",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.LESS_THAN,",
            "                value=\"2020-01-01\",",
            "            ),",
            "            FilterCondition(",
            "                column=\"metadata.created_at\",",
            "                op=FilterOperator.GREATER_THAN,",
            "                value=\"2019-01-01\",",
            "            )",
            "        ]",
            "        -->",
            "        {\"operator\": \"And\",",
            "        \"operands\": [",
            "            {",
            "                \"path\": [\"created_at\"],",
            "                \"operator\": \"LessThan\",",
            "                \"valueText\": \"2020-01-01\",",
            "            },",
            "            {",
            "                \"path\": [\"created_at\"],",
            "                \"operator\": \"GreaterThan\",",
            "                \"valueInt\": \"2019-01-01\",",
            "            },",
            "        ]}",
            "        \"\"\"",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        #",
            "        if not (conditions or meta_conditions):",
            "            return None",
            "",
            "        # we translate each condition into a single dict",
            "        #  conditions on columns",
            "        weaviate_conditions = []",
            "        if conditions:",
            "            for condition in conditions:",
            "                column_key = condition.column",
            "                value_type = self._get_weaviate_value_type(condition.value)",
            "                weaviate_conditions.append(",
            "                    {",
            "                        \"path\": [column_key],",
            "                        \"operator\": self._get_weaviate_operator(condition.op),",
            "                        value_type: condition.value,",
            "                    }",
            "                )",
            "        # condition on metadata columns",
            "        if meta_conditions:",
            "            for condition in meta_conditions:",
            "                meta_key = condition.column.split(\".\")[-1]",
            "                value_type = self._get_weaviate_value_type(condition.value)",
            "                weaviate_conditions.append(",
            "                    {",
            "                        \"path\": [",
            "                            \"associatedMetadata\",",
            "                            metadata_table_name,",
            "                            meta_key,",
            "                        ],",
            "                        \"operator\": self._get_weaviate_operator(condition.op),",
            "                        value_type: condition.value,",
            "                    }",
            "                )",
            "",
            "        # we combine all conditions into a single dict",
            "        all_conditions = (",
            "            {\"operator\": \"And\", \"operands\": weaviate_conditions}",
            "            # combining all conditions if there are more than one conditions",
            "            if len(weaviate_conditions) > 1",
            "            # only a single condition",
            "            else weaviate_conditions[0]",
            "        )",
            "        return all_conditions",
            "",
            "    def select(",
            "        self,",
            "        table_name: str,",
            "        columns: List[str] = None,",
            "        conditions: List[FilterCondition] = None,",
            "        offset: int = None,",
            "        limit: int = None,",
            "    ) -> HandlerResponse:",
            "        table_name = table_name.capitalize()",
            "        # columns which we will always provide in the result",
            "        filters = None",
            "        if conditions:",
            "            non_metadata_conditions = [",
            "                condition",
            "                for condition in conditions",
            "                if not condition.column.startswith(TableField.METADATA.value)",
            "                and condition.column != TableField.SEARCH_VECTOR.value",
            "                and condition.column != TableField.EMBEDDINGS.value",
            "            ]",
            "            metadata_conditions = [",
            "                condition",
            "                for condition in conditions",
            "                if condition.column.startswith(TableField.METADATA.value)",
            "            ]",
            "            filters = self._translate_condition(",
            "                table_name,",
            "                non_metadata_conditions if non_metadata_conditions else None,",
            "                metadata_conditions if metadata_conditions else None,",
            "            )",
            "",
            "        # check if embedding vector filter is present",
            "        vector_filter = (",
            "            None",
            "            if not conditions",
            "            else [",
            "                condition",
            "                for condition in conditions",
            "                if condition.column == TableField.SEARCH_VECTOR.value",
            "                or condition.column == TableField.EMBEDDINGS.value",
            "            ]",
            "        )",
            "",
            "        for col in [\"id\", \"embeddings\", \"distance\", \"metadata\"]:",
            "            if col in columns:",
            "                columns.remove(col)",
            "",
            "        metadata_table = table_name.capitalize() + \"_metadata\"",
            "",
            "        metadata_fields = \" \".join(",
            "            [",
            "                prop[\"name\"]",
            "                for prop in self._client.schema.get(metadata_table)[\"properties\"]",
            "            ]",
            "        )",
            "",
            "        # query to get all metadata fields",
            "        metadata_query = (",
            "            f\"associatedMetadata {{ ... on {metadata_table} {{ {metadata_fields} }} }}\"",
            "        )",
            "",
            "        if columns:",
            "            query = self._client.query.get(",
            "                table_name,",
            "                columns + [metadata_query],",
            "            ).with_additional([\"id vector distance\"])",
            "        else:",
            "            query = self._client.query.get(",
            "                table_name,",
            "                [metadata_query],",
            "            ).with_additional([\"id vector distance\"])",
            "        if vector_filter:",
            "            # similarity search",
            "            # assuming the similarity search is on content",
            "            # assuming there would be only one vector based search per query",
            "            vector_filter = vector_filter[0]",
            "            near_vector = {",
            "                \"vector\": ast.literal_eval(vector_filter.value)",
            "                if isinstance(vector_filter.value, str)",
            "                else vector_filter.value",
            "            }",
            "            query = query.with_near_vector(near_vector)",
            "        if filters:",
            "            query = query.with_where(filters)",
            "        if limit:",
            "            query = query.with_limit(limit)",
            "        result = query.do()",
            "        result = result[\"data\"][\"Get\"][table_name.capitalize()]",
            "        ids = [query_obj[\"_additional\"][\"id\"] for query_obj in result]",
            "        contents = [query_obj.get(\"content\") for query_obj in result]",
            "        distances = [",
            "            query_obj.get(\"_additional\").get(\"distance\") for query_obj in result",
            "        ]",
            "        # distances will be null for non vector/embedding query",
            "        vectors = [query_obj.get(\"_additional\").get(\"vector\") for query_obj in result]",
            "        metadatas = [query_obj.get(\"associatedMetadata\")[0] for query_obj in result]",
            "",
            "        payload = {",
            "            TableField.ID.value: ids,",
            "            TableField.CONTENT.value: contents,",
            "            TableField.METADATA.value: metadatas,",
            "            TableField.EMBEDDINGS.value: vectors,",
            "            TableField.DISTANCE.value: distances,",
            "        }",
            "",
            "        if columns:",
            "            payload = {",
            "                column: payload[column]",
            "                for column in columns + [\"id\", \"embeddings\", \"distance\", \"metadata\"]",
            "                if column != TableField.EMBEDDINGS.value",
            "            }",
            "",
            "        # always include distance",
            "        if distances:",
            "            payload[TableField.DISTANCE.value] = distances",
            "        result_df = pd.DataFrame(payload)",
            "        return Response(resp_type=RESPONSE_TYPE.TABLE, data_frame=result_df)",
            "",
            "    def insert(",
            "        self, table_name: str, data: pd.DataFrame, columns: List[str] = None",
            "    ) -> HandlerResponse:",
            "        \"\"\"",
            "        Insert data into the Weaviate database.",
            "        \"\"\"",
            "",
            "        table_name = table_name.capitalize()",
            "",
            "        # drop columns with all None values",
            "",
            "        data.dropna(axis=1, inplace=True)",
            "",
            "        data = data.to_dict(orient=\"records\")",
            "        # parsing the records one by one as we need to update metadata (which has variable columns)",
            "        for record in data:",
            "            metadata_data = record.get(TableField.METADATA.value)",
            "            data_object = {\"content\": record.get(TableField.CONTENT.value)}",
            "            data_obj_id = (",
            "                record[TableField.ID.value]",
            "                if TableField.ID.value in record.keys()",
            "                else generate_uuid5(data_object)",
            "            )",
            "            obj_id = self._client.data_object.create(",
            "                data_object=data_object,",
            "                class_name=table_name,",
            "                vector=record[TableField.EMBEDDINGS.value],",
            "                uuid=data_obj_id,",
            "            )",
            "            if metadata_data:",
            "                meta_id = self.add_metadata(metadata_data, table_name)",
            "                self._client.data_object.reference.add(",
            "                    from_uuid=obj_id,",
            "                    from_property_name=\"associatedMetadata\",",
            "                    to_uuid=meta_id,",
            "                )",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def update(",
            "        self, table_name: str, data: pd.DataFrame, columns: List[str] = None",
            "    ) -> HandlerResponse:",
            "        \"\"\"",
            "        Update data in the weaviate database.",
            "        \"\"\"",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        data_list = data.to_dict(\"records\")",
            "        for row in data_list:",
            "            non_metadata_keys = [",
            "                key",
            "                for key in row.keys()",
            "                if key and not key.startswith(TableField.METADATA.value)",
            "            ]",
            "            metadata_keys = [",
            "                key.split(\".\")[1]",
            "                for key in row.keys()",
            "                if key and key.startswith(TableField.METADATA.value)",
            "            ]",
            "",
            "            id_filter = {\"path\": [\"id\"], \"operator\": \"Equal\", \"valueText\": row[\"id\"]}",
            "            metadata_id_query = f\"associatedMetadata {{ ... on {metadata_table_name} {{ _additional {{ id }} }} }}\"",
            "            result = (",
            "                self._client.query.get(table_name, metadata_id_query)",
            "                .with_additional([\"id\"])",
            "                .with_where(id_filter)",
            "                .do()",
            "            )",
            "",
            "            metadata_id = result[\"data\"][\"Get\"][table_name][0][\"associatedMetadata\"][0][",
            "                \"_additional\"",
            "            ][\"id\"][0]",
            "            # updating table",
            "            self._client.data_object.update(",
            "                uuid=row[\"id\"],",
            "                class_name=table_name,",
            "                data_object={key: row[key] for key in non_metadata_keys},",
            "            )",
            "            # updating metadata",
            "            self._client.data_object.update(",
            "                uuid=metadata_id,",
            "                class_name=metadata_table_name,",
            "                data_object={key: row[key] for key in metadata_keys},",
            "            )",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def delete(",
            "        self, table_name: str, conditions: List[FilterCondition] = None",
            "    ) -> HandlerResponse:",
            "        table_name = table_name.capitalize()",
            "        non_metadata_conditions = [",
            "            condition",
            "            for condition in conditions",
            "            if not condition.column.startswith(TableField.METADATA.value)",
            "            and condition.column != TableField.SEARCH_VECTOR.value",
            "            and condition.column != TableField.EMBEDDINGS.value",
            "        ]",
            "        metadata_conditions = [",
            "            condition",
            "            for condition in conditions",
            "            if condition.column.startswith(TableField.METADATA.value)",
            "        ]",
            "        filters = self._translate_condition(",
            "            table_name,",
            "            non_metadata_conditions if non_metadata_conditions else None,",
            "            metadata_conditions if metadata_conditions else None,",
            "        )",
            "        if not filters:",
            "            raise Exception(\"Delete query must have at least one condition!\")",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        # query to get metadata ids",
            "        metadata_query = f\"associatedMetadata {{ ... on {metadata_table_name} {{ _additional {{ id }} }} }}\"",
            "        result = (",
            "            self._client.query.get(table_name, metadata_query)",
            "            .with_additional([\"id\"])",
            "            .with_where(filters)",
            "            .do()",
            "        )",
            "        result = result[\"data\"][\"Get\"][table_name]",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        table_ids = []",
            "        metadata_ids = []",
            "        for i in result:",
            "            table_ids.append(i[\"_additional\"][\"id\"])",
            "            metadata_ids.append(i[\"associatedMetadata\"][0][\"_additional\"][\"id\"])",
            "        self._client.batch.delete_objects(",
            "            class_name=table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": table_ids,",
            "            },",
            "        )",
            "        self._client.batch.delete_objects(",
            "            class_name=metadata_table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": metadata_ids,",
            "            },",
            "        )",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def create_table(self, table_name: str, if_not_exists=True) -> HandlerResponse:",
            "        \"\"\"",
            "        Create a class with the given name in the weaviate database.",
            "        \"\"\"",
            "        # separate metadata table for each table (as different tables will have different metadata columns)",
            "        # this reduces the query time using metadata but increases the insertion time",
            "        metadata_table_name = table_name + \"_metadata\"",
            "        if not self._client.schema.exists(metadata_table_name):",
            "            self._client.schema.create_class({\"class\": metadata_table_name})",
            "        if not self._client.schema.exists(table_name):",
            "            self._client.schema.create_class(",
            "                {",
            "                    \"class\": table_name,",
            "                    \"properties\": [",
            "                        {\"dataType\": [\"text\"], \"name\": prop[\"name\"]}",
            "                        for prop in self.SCHEMA",
            "                        if prop[\"name\"] != \"id\"",
            "                        and prop[\"name\"] != \"embeddings\"",
            "                        and prop[\"name\"] != \"metadata\"",
            "                    ],",
            "                    \"vectorIndexType\": \"hnsw\",",
            "                }",
            "            )",
            "            add_prop = {",
            "                \"name\": \"associatedMetadata\",",
            "                \"dataType\": [metadata_table_name.capitalize()],",
            "            }",
            "            self._client.schema.property.create(table_name.capitalize(), add_prop)",
            "",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def drop_table(self, table_name: str, if_exists=True) -> HandlerResponse:",
            "        \"\"\"",
            "        Delete a class from the weaviate database.",
            "        \"\"\"",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        table_id_query = self._client.query.get(table_name).with_additional([\"id\"]).do()",
            "        table_ids = [",
            "            i[\"_additional\"][\"id\"] for i in table_id_query[\"data\"][\"Get\"][table_name]",
            "        ]",
            "        metadata_table_id_query = (",
            "            self._client.query.get(metadata_table_name).with_additional([\"id\"]).do()",
            "        )",
            "        metadata_ids = [",
            "            i[\"_additional\"][\"id\"]",
            "            for i in metadata_table_id_query[\"data\"][\"Get\"][metadata_table_name]",
            "        ]",
            "        self._client.batch.delete_objects(",
            "            class_name=table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": table_ids,",
            "            },",
            "        )",
            "        self._client.batch.delete_objects(",
            "            class_name=metadata_table_name,",
            "            where={",
            "                \"path\": [\"id\"],",
            "                \"operator\": \"ContainsAny\",",
            "                \"valueTextArray\": metadata_ids,",
            "            },",
            "        )",
            "        try:",
            "            self._client.schema.delete_class(table_name)",
            "            self._client.schema.delete_class(metadata_table_name)",
            "        except ValueError:",
            "            if if_exists:",
            "                return Response(resp_type=RESPONSE_TYPE.OK)",
            "            else:",
            "                return Response(",
            "                    resp_type=RESPONSE_TYPE.ERROR,",
            "                    error_message=f\"Table {table_name} does not exist!\",",
            "                )",
            "",
            "        return Response(resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def get_tables(self) -> HandlerResponse:",
            "        \"\"\"",
            "        Get the list of tables in the Weaviate database.",
            "        \"\"\"",
            "        query_tables = self._client.schema.get()",
            "        tables = []",
            "        if query_tables:",
            "            tables = [table[\"class\"] for table in query_tables[\"classes\"]]",
            "        table_name = pd.DataFrame(",
            "            columns=[\"table_name\"],",
            "            data=tables,",
            "        )",
            "        return Response(resp_type=RESPONSE_TYPE.TABLE, data_frame=table_name)",
            "",
            "    def get_columns(self, table_name: str) -> HandlerResponse:",
            "        table_name = table_name.capitalize()",
            "        # check if table exists",
            "        try:",
            "            table = self._client.schema.get(table_name)",
            "        except ValueError:",
            "            return Response(",
            "                resp_type=RESPONSE_TYPE.ERROR,",
            "                error_message=f\"Table {table_name} does not exist!\",",
            "            )",
            "        data = pd.DataFrame(",
            "            data=[",
            "                {\"COLUMN_NAME\": column[\"name\"], \"DATA_TYPE\": column[\"dataType\"][0]}",
            "                for column in table[\"properties\"]",
            "            ]",
            "        )",
            "        return Response(data_frame=data, resp_type=RESPONSE_TYPE.OK)",
            "",
            "    def add_metadata(self, data: dict, table_name: str):",
            "        table_name = table_name.capitalize()",
            "        metadata_table_name = table_name.capitalize() + \"_metadata\"",
            "        self._client.schema.get(metadata_table_name)",
            "        # getting existing metadata fields",
            "        added_prop_list = [",
            "            prop[\"name\"]",
            "            for prop in self._client.schema.get(metadata_table_name)[\"properties\"]",
            "        ]",
            "        # as metadata columns are not fixed, at every entry, a check takes place for the columns",
            "        for prop in data.keys():",
            "            if prop not in added_prop_list:",
            "                if isinstance(data[prop], int):",
            "                    add_prop = {",
            "                        \"name\": prop,",
            "                        \"dataType\": [\"int\"],",
            "                    }",
            "                elif isinstance(data[prop][0], datetime):",
            "                    add_prop = {",
            "                        \"name\": prop,",
            "                        \"dataType\": [\"date\"],",
            "                    }",
            "                else:",
            "                    add_prop = {",
            "                        \"name\": prop,",
            "                        \"dataType\": [\"string\"],",
            "                    }",
            "                # when a new column is identified, it is added to the metadata table",
            "                self._client.schema.property.create(metadata_table_name, add_prop)",
            "        metadata_id = self._client.data_object.create(",
            "            data_object=data, class_name=table_name.capitalize() + \"_metadata\"",
            "        )",
            "        return metadata_id"
        ],
        "action": [
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "338": [
                "WeaviateDBHandler",
                "select"
            ]
        },
        "addLocation": []
    },
    "mindsdb/integrations/libs/vectordatabase_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "             if k == TableField.EMBEDDINGS.value and isinstance(v, str):"
            },
            "1": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "                 # it could be embeddings in string"
            },
            "2": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "                 try:"
            },
            "3": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    v = eval(v)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 245,
                "PatchRowcode": "+                    v = ast.literal_eval(v)"
            },
            "5": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "                 except Exception:"
            },
            "6": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 247,
                "PatchRowcode": "                     pass"
            },
            "7": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 248,
                "PatchRowcode": "             row[k] = v"
            }
        },
        "frontPatchFile": [
            "import ast",
            "import hashlib",
            "from enum import Enum",
            "from typing import List, Optional",
            "",
            "import pandas as pd",
            "from mindsdb_sql.parser.ast import (",
            "    BinaryOperation,",
            "    Constant,",
            "    CreateTable,",
            "    Delete,",
            "    DropTables,",
            "    Insert,",
            "    Select,",
            "    Star,",
            "    Tuple,",
            "    Update,",
            ")",
            "from mindsdb_sql.parser.ast.base import ASTNode",
            "",
            "from mindsdb.integrations.libs.response import RESPONSE_TYPE, HandlerResponse",
            "from mindsdb.utilities import log",
            "from mindsdb.integrations.utilities.sql_utils import conditions_to_filter, FilterCondition, FilterOperator",
            "",
            "from ..utilities.sql_utils import query_traversal",
            "from .base import BaseHandler",
            "",
            "LOG = log.getLogger(__name__)",
            "",
            "",
            "class TableField(Enum):",
            "    \"\"\"",
            "    Enum for table fields.",
            "    \"\"\"",
            "",
            "    ID = \"id\"",
            "    CONTENT = \"content\"",
            "    EMBEDDINGS = \"embeddings\"",
            "    METADATA = \"metadata\"",
            "    SEARCH_VECTOR = \"search_vector\"",
            "    DISTANCE = \"distance\"",
            "",
            "",
            "class VectorStoreHandler(BaseHandler):",
            "    \"\"\"",
            "    Base class for handlers associated to vector databases.",
            "    \"\"\"",
            "",
            "    SCHEMA = [",
            "        {",
            "            \"name\": TableField.ID.value,",
            "            \"data_type\": \"string\",",
            "        },",
            "        {",
            "            \"name\": TableField.CONTENT.value,",
            "            \"data_type\": \"string\",",
            "        },",
            "        {",
            "            \"name\": TableField.EMBEDDINGS.value,",
            "            \"data_type\": \"list\",",
            "        },",
            "        {",
            "            \"name\": TableField.METADATA.value,",
            "            \"data_type\": \"json\",",
            "        },",
            "    ]",
            "",
            "    def validate_connection_parameters(self, name, **kwargs):",
            "        \"\"\"Create validation for input parameters.\"\"\"",
            "",
            "        return NotImplementedError()",
            "",
            "    def __del__(self):",
            "        if self.is_connected is True:",
            "            self.disconnect()",
            "",
            "    def disconnect(self):",
            "        pass",
            "",
            "    def _value_or_self(self, value):",
            "        if isinstance(value, Constant):",
            "            return value.value",
            "        else:",
            "            return value",
            "",
            "    def _extract_conditions(self, where_statement) -> Optional[List[FilterCondition]]:",
            "        conditions = []",
            "        # parse conditions",
            "        if where_statement is not None:",
            "            # dfs to get all binary operators in the where statement",
            "            def _extract_comparison_conditions(node, **kwargs):",
            "                if isinstance(node, BinaryOperation):",
            "                    # if the op is and, continue",
            "                    # TODO: need to handle the OR case",
            "                    if node.op.upper() == \"AND\":",
            "                        return",
            "                    op = FilterOperator(node.op.upper())",
            "                    # unquote the left hand side",
            "                    left_hand = node.args[0].parts[-1].strip(\"`\")",
            "                    if isinstance(node.args[1], Constant):",
            "                        if left_hand == TableField.SEARCH_VECTOR.value:",
            "                            right_hand = ast.literal_eval(node.args[1].value)",
            "                        else:",
            "                            right_hand = node.args[1].value",
            "                    elif isinstance(node.args[1], Tuple):",
            "                        # Constant could be actually a list i.e. [1.2, 3.2]",
            "                        right_hand = [",
            "                            ast.literal_eval(item.value)",
            "                            if isinstance(item, Constant)",
            "                            and not isinstance(item.value, list)",
            "                            else item.value",
            "                            for item in node.args[1].items",
            "                        ]",
            "                    else:",
            "                        raise Exception(f\"Unsupported right hand side: {node.args[1]}\")",
            "                    conditions.append(",
            "                        FilterCondition(column=left_hand, op=op, value=right_hand)",
            "                    )",
            "",
            "            query_traversal(where_statement, _extract_comparison_conditions)",
            "",
            "            # try to treat conditions that are not in TableField as metadata conditions",
            "            for condition in conditions:",
            "                if not self._is_condition_allowed(condition):",
            "                    condition.column = (",
            "                        TableField.METADATA.value + \".\" + condition.column",
            "                    )",
            "",
            "        else:",
            "            conditions = None",
            "",
            "        return conditions",
            "",
            "    def _is_columns_allowed(self, columns: List[str]) -> bool:",
            "        \"\"\"",
            "        Check if columns are allowed.",
            "        \"\"\"",
            "        allowed_columns = set([col[\"name\"] for col in self.SCHEMA])",
            "        return set(columns).issubset(allowed_columns)",
            "",
            "    def _is_condition_allowed(self, condition: FilterCondition) -> bool:",
            "        allowed_field_values = set([field.value for field in TableField])",
            "        if condition.column in allowed_field_values:",
            "            return True",
            "        else:",
            "            # check if column is a metadata column",
            "            if condition.column.startswith(TableField.METADATA.value):",
            "                return True",
            "            else:",
            "                return False",
            "",
            "    def _dispatch_create_table(self, query: CreateTable):",
            "        \"\"\"",
            "        Dispatch create table query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.name.parts[-1]",
            "        if_not_exists = getattr(query, \"if_not_exists\", False)",
            "        return self.create_table(table_name, if_not_exists=if_not_exists)",
            "",
            "    def _dispatch_drop_table(self, query: DropTables):",
            "        \"\"\"",
            "        Dispatch drop table query to the appropriate method.",
            "        \"\"\"",
            "        table_name = query.tables[0].parts[-1]",
            "        if_exists = getattr(query, \"if_exists\", False)",
            "",
            "        return self.drop_table(table_name, if_exists=if_exists)",
            "",
            "    def _dispatch_insert(self, query: Insert):",
            "        \"\"\"",
            "        Dispatch insert query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.table.parts[-1]",
            "        columns = [column.name for column in query.columns]",
            "",
            "        if not self._is_columns_allowed(columns):",
            "            raise Exception(",
            "                f\"Columns {columns} not allowed.\"",
            "                f\"Allowed columns are {[col['name'] for col in self.SCHEMA]}\"",
            "            )",
            "",
            "        # get content column if it is present",
            "        if TableField.CONTENT.value in columns:",
            "            content_col_index = columns.index(\"content\")",
            "            content = [",
            "                self._value_or_self(row[content_col_index]) for row in query.values",
            "            ]",
            "        else:",
            "            content = None",
            "",
            "        # get id column if it is present",
            "        ids = None",
            "        if TableField.ID.value in columns:",
            "            id_col_index = columns.index(\"id\")",
            "            ids = [self._value_or_self(row[id_col_index]) for row in query.values]",
            "        elif TableField.CONTENT.value is None:",
            "            raise Exception(\"Content or id is required!\")",
            "",
            "        # get embeddings column if it is present",
            "        if TableField.EMBEDDINGS.value in columns:",
            "            embeddings_col_index = columns.index(\"embeddings\")",
            "            embeddings = [",
            "                ast.literal_eval(self._value_or_self(row[embeddings_col_index]))",
            "                for row in query.values",
            "            ]",
            "        else:",
            "            raise Exception(\"Embeddings column is required!\")",
            "",
            "        if TableField.METADATA.value in columns:",
            "            metadata_col_index = columns.index(\"metadata\")",
            "            metadata = [",
            "                ast.literal_eval(self._value_or_self(row[metadata_col_index]))",
            "                for row in query.values",
            "            ]",
            "        else:",
            "            metadata = None",
            "",
            "        # create dataframe",
            "        data = {",
            "            TableField.CONTENT.value: content,",
            "            TableField.EMBEDDINGS.value: embeddings,",
            "            TableField.METADATA.value: metadata,",
            "        }",
            "        if ids is not None:",
            "            data[TableField.ID.value] = ids",
            "",
            "        return self.do_upsert(table_name, pd.DataFrame(data))",
            "",
            "    def _dispatch_update(self, query: Update):",
            "        \"\"\"",
            "        Dispatch update query to the appropriate method.",
            "        \"\"\"",
            "        table_name = query.table.parts[-1]",
            "",
            "        row = {}",
            "        for k, v in query.update_columns.items():",
            "            k = k.lower()",
            "            if isinstance(v, Constant):",
            "                v = v.value",
            "            if k == TableField.EMBEDDINGS.value and isinstance(v, str):",
            "                # it could be embeddings in string",
            "                try:",
            "                    v = eval(v)",
            "                except Exception:",
            "                    pass",
            "            row[k] = v",
            "",
            "        filters = conditions_to_filter(query.where)",
            "        row.update(filters)",
            "",
            "        # checks",
            "        if TableField.EMBEDDINGS.value not in row:",
            "            raise Exception(\"Embeddings column is required!\")",
            "",
            "        if TableField.CONTENT.value not in row:",
            "            raise Exception(\"Content is required!\")",
            "",
            "        # store",
            "        df = pd.DataFrame([row])",
            "",
            "        return self.do_upsert(table_name, df)",
            "",
            "    def do_upsert(self, table_name, df):",
            "        # if handler supports it, call upsert method",
            "",
            "        id_col = TableField.ID.value",
            "        content_col = TableField.CONTENT.value",
            "",
            "        def gen_hash(v):",
            "            return hashlib.md5(str(v).encode()).hexdigest()",
            "",
            "        if id_col not in df.columns:",
            "            # generate for all",
            "            df[id_col] = df[content_col].apply(gen_hash)",
            "        else:",
            "            # generate for empty",
            "            for i in range(len(df)):",
            "                if pd.isna(df.loc[i, id_col]):",
            "                    df.loc[i, id_col] = gen_hash(df.loc[i, content_col])",
            "",
            "        # remove duplicated ids",
            "        df = df.drop_duplicates([TableField.ID.value])",
            "",
            "        # id is string TODO is it ok?",
            "        df[id_col] = df[id_col].apply(str)",
            "",
            "        if hasattr(self, 'upsert'):",
            "            self.upsert(table_name, df)",
            "            return",
            "",
            "        # find existing ids",
            "        res = self.select(",
            "            table_name,",
            "            columns=[id_col],",
            "            conditions=[",
            "                FilterCondition(column=id_col, op=FilterOperator.IN, value=list(df[id_col]))",
            "            ]",
            "        )",
            "        existed_ids = list(res[id_col])",
            "",
            "        # update existed",
            "        df_update = df[df[id_col].isin(existed_ids)]",
            "        df_insert = df[~df[id_col].isin(existed_ids)]",
            "",
            "        if not df_update.empty:",
            "            try:",
            "                self.update(table_name, df_update, [id_col])",
            "            except NotImplementedError:",
            "                # not implemented? do it with delete and insert",
            "                conditions = [FilterCondition(",
            "                    column=id_col,",
            "                    op=FilterOperator.IN,",
            "                    value=list(df[id_col])",
            "                )]",
            "                self.delete(table_name, conditions)",
            "                self.insert(table_name, df_update)",
            "        if not df_insert.empty:",
            "            self.insert(table_name, df_insert)",
            "",
            "    def _dispatch_delete(self, query: Delete):",
            "        \"\"\"",
            "        Dispatch delete query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.table.parts[-1]",
            "        where_statement = query.where",
            "        conditions = self._extract_conditions(where_statement)",
            "",
            "        # dispatch delete",
            "        return self.delete(table_name, conditions=conditions)",
            "",
            "    def _dispatch_select(self, query: Select):",
            "        \"\"\"",
            "        Dispatch select query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.from_table.parts[-1]",
            "        # if targets are star, select all columns",
            "        if isinstance(query.targets[0], Star):",
            "            columns = [col[\"name\"] for col in self.SCHEMA]",
            "        else:",
            "            columns = [col.parts[-1] for col in query.targets]",
            "",
            "        if not self._is_columns_allowed(columns):",
            "            raise Exception(",
            "                f\"Columns {columns} not allowed.\"",
            "                f\"Allowed columns are {[col['name'] for col in self.SCHEMA]}\"",
            "            )",
            "",
            "        # check if columns are allowed",
            "        where_statement = query.where",
            "        conditions = self._extract_conditions(where_statement)",
            "",
            "        # get offset and limit",
            "        offset = query.offset.value if query.offset is not None else None",
            "        limit = query.limit.value if query.limit is not None else None",
            "",
            "        # dispatch select",
            "        return self.select(",
            "            table_name,",
            "            columns=columns,",
            "            conditions=conditions,",
            "            offset=offset,",
            "            limit=limit,",
            "        )",
            "",
            "    def _dispatch(self, query: ASTNode) -> HandlerResponse:",
            "        \"\"\"",
            "        Parse and Dispatch query to the appropriate method.",
            "        \"\"\"",
            "        dispatch_router = {",
            "            CreateTable: self._dispatch_create_table,",
            "            DropTables: self._dispatch_drop_table,",
            "            Insert: self._dispatch_insert,",
            "            Update: self._dispatch_update,",
            "            Delete: self._dispatch_delete,",
            "            Select: self._dispatch_select,",
            "        }",
            "        if type(query) in dispatch_router:",
            "            resp = dispatch_router[type(query)](query)",
            "            if resp is not None:",
            "                return HandlerResponse(",
            "                    resp_type=RESPONSE_TYPE.TABLE,",
            "                    data_frame=resp",
            "                )",
            "            else:",
            "                return HandlerResponse(resp_type=RESPONSE_TYPE.OK)",
            "",
            "        else:",
            "            raise NotImplementedError(f\"Query type {type(query)} not implemented.\")",
            "",
            "    def query(self, query: ASTNode) -> HandlerResponse:",
            "        \"\"\"",
            "        Receive query as AST (abstract syntax tree) and act upon it somehow.",
            "",
            "        Args:",
            "            query (ASTNode): sql query represented as AST. May be any kind",
            "                of query: SELECT, INSERT, DELETE, etc",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        return self._dispatch(query)",
            "",
            "    def create_table(self, table_name: str, if_not_exists=True) -> HandlerResponse:",
            "        \"\"\"Create table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            if_not_exists (bool): if True, do nothing if table exists",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def drop_table(self, table_name: str, if_exists=True) -> HandlerResponse:",
            "        \"\"\"Drop table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            if_exists (bool): if True, do nothing if table does not exist",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def insert(",
            "        self, table_name: str, data: pd.DataFrame",
            "    ) -> HandlerResponse:",
            "        \"\"\"Insert data into table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            data (pd.DataFrame): data to insert",
            "            columns (List[str]): columns to insert",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def update(",
            "        self, table_name: str, data: pd.DataFrame, key_columns: List[str] = None",
            "    ):",
            "        \"\"\"Update data in table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            data (pd.DataFrame): data to update",
            "            key_columns (List[str]): key to  to update",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def delete(",
            "        self, table_name: str, conditions: List[FilterCondition] = None",
            "    ) -> HandlerResponse:",
            "        \"\"\"Delete data from table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            conditions (List[FilterCondition]): conditions to delete",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def select(",
            "        self,",
            "        table_name: str,",
            "        columns: List[str] = None,",
            "        conditions: List[FilterCondition] = None,",
            "        offset: int = None,",
            "        limit: int = None,",
            "    ) -> pd.DataFrame:",
            "        \"\"\"Select data from table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            columns (List[str]): columns to select",
            "            conditions (List[FilterCondition]): conditions to select",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def get_columns(self, table_name: str) -> HandlerResponse:",
            "        # return a fixed set of columns",
            "        data = pd.DataFrame(self.SCHEMA)",
            "        data.columns = [\"COLUMN_NAME\", \"DATA_TYPE\"]",
            "        return HandlerResponse(",
            "            resp_type=RESPONSE_TYPE.DATA,",
            "            data_frame=data,",
            "        )"
        ],
        "afterPatchFile": [
            "import ast",
            "import hashlib",
            "from enum import Enum",
            "from typing import List, Optional",
            "",
            "import pandas as pd",
            "from mindsdb_sql.parser.ast import (",
            "    BinaryOperation,",
            "    Constant,",
            "    CreateTable,",
            "    Delete,",
            "    DropTables,",
            "    Insert,",
            "    Select,",
            "    Star,",
            "    Tuple,",
            "    Update,",
            ")",
            "from mindsdb_sql.parser.ast.base import ASTNode",
            "",
            "from mindsdb.integrations.libs.response import RESPONSE_TYPE, HandlerResponse",
            "from mindsdb.utilities import log",
            "from mindsdb.integrations.utilities.sql_utils import conditions_to_filter, FilterCondition, FilterOperator",
            "",
            "from ..utilities.sql_utils import query_traversal",
            "from .base import BaseHandler",
            "",
            "LOG = log.getLogger(__name__)",
            "",
            "",
            "class TableField(Enum):",
            "    \"\"\"",
            "    Enum for table fields.",
            "    \"\"\"",
            "",
            "    ID = \"id\"",
            "    CONTENT = \"content\"",
            "    EMBEDDINGS = \"embeddings\"",
            "    METADATA = \"metadata\"",
            "    SEARCH_VECTOR = \"search_vector\"",
            "    DISTANCE = \"distance\"",
            "",
            "",
            "class VectorStoreHandler(BaseHandler):",
            "    \"\"\"",
            "    Base class for handlers associated to vector databases.",
            "    \"\"\"",
            "",
            "    SCHEMA = [",
            "        {",
            "            \"name\": TableField.ID.value,",
            "            \"data_type\": \"string\",",
            "        },",
            "        {",
            "            \"name\": TableField.CONTENT.value,",
            "            \"data_type\": \"string\",",
            "        },",
            "        {",
            "            \"name\": TableField.EMBEDDINGS.value,",
            "            \"data_type\": \"list\",",
            "        },",
            "        {",
            "            \"name\": TableField.METADATA.value,",
            "            \"data_type\": \"json\",",
            "        },",
            "    ]",
            "",
            "    def validate_connection_parameters(self, name, **kwargs):",
            "        \"\"\"Create validation for input parameters.\"\"\"",
            "",
            "        return NotImplementedError()",
            "",
            "    def __del__(self):",
            "        if self.is_connected is True:",
            "            self.disconnect()",
            "",
            "    def disconnect(self):",
            "        pass",
            "",
            "    def _value_or_self(self, value):",
            "        if isinstance(value, Constant):",
            "            return value.value",
            "        else:",
            "            return value",
            "",
            "    def _extract_conditions(self, where_statement) -> Optional[List[FilterCondition]]:",
            "        conditions = []",
            "        # parse conditions",
            "        if where_statement is not None:",
            "            # dfs to get all binary operators in the where statement",
            "            def _extract_comparison_conditions(node, **kwargs):",
            "                if isinstance(node, BinaryOperation):",
            "                    # if the op is and, continue",
            "                    # TODO: need to handle the OR case",
            "                    if node.op.upper() == \"AND\":",
            "                        return",
            "                    op = FilterOperator(node.op.upper())",
            "                    # unquote the left hand side",
            "                    left_hand = node.args[0].parts[-1].strip(\"`\")",
            "                    if isinstance(node.args[1], Constant):",
            "                        if left_hand == TableField.SEARCH_VECTOR.value:",
            "                            right_hand = ast.literal_eval(node.args[1].value)",
            "                        else:",
            "                            right_hand = node.args[1].value",
            "                    elif isinstance(node.args[1], Tuple):",
            "                        # Constant could be actually a list i.e. [1.2, 3.2]",
            "                        right_hand = [",
            "                            ast.literal_eval(item.value)",
            "                            if isinstance(item, Constant)",
            "                            and not isinstance(item.value, list)",
            "                            else item.value",
            "                            for item in node.args[1].items",
            "                        ]",
            "                    else:",
            "                        raise Exception(f\"Unsupported right hand side: {node.args[1]}\")",
            "                    conditions.append(",
            "                        FilterCondition(column=left_hand, op=op, value=right_hand)",
            "                    )",
            "",
            "            query_traversal(where_statement, _extract_comparison_conditions)",
            "",
            "            # try to treat conditions that are not in TableField as metadata conditions",
            "            for condition in conditions:",
            "                if not self._is_condition_allowed(condition):",
            "                    condition.column = (",
            "                        TableField.METADATA.value + \".\" + condition.column",
            "                    )",
            "",
            "        else:",
            "            conditions = None",
            "",
            "        return conditions",
            "",
            "    def _is_columns_allowed(self, columns: List[str]) -> bool:",
            "        \"\"\"",
            "        Check if columns are allowed.",
            "        \"\"\"",
            "        allowed_columns = set([col[\"name\"] for col in self.SCHEMA])",
            "        return set(columns).issubset(allowed_columns)",
            "",
            "    def _is_condition_allowed(self, condition: FilterCondition) -> bool:",
            "        allowed_field_values = set([field.value for field in TableField])",
            "        if condition.column in allowed_field_values:",
            "            return True",
            "        else:",
            "            # check if column is a metadata column",
            "            if condition.column.startswith(TableField.METADATA.value):",
            "                return True",
            "            else:",
            "                return False",
            "",
            "    def _dispatch_create_table(self, query: CreateTable):",
            "        \"\"\"",
            "        Dispatch create table query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.name.parts[-1]",
            "        if_not_exists = getattr(query, \"if_not_exists\", False)",
            "        return self.create_table(table_name, if_not_exists=if_not_exists)",
            "",
            "    def _dispatch_drop_table(self, query: DropTables):",
            "        \"\"\"",
            "        Dispatch drop table query to the appropriate method.",
            "        \"\"\"",
            "        table_name = query.tables[0].parts[-1]",
            "        if_exists = getattr(query, \"if_exists\", False)",
            "",
            "        return self.drop_table(table_name, if_exists=if_exists)",
            "",
            "    def _dispatch_insert(self, query: Insert):",
            "        \"\"\"",
            "        Dispatch insert query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.table.parts[-1]",
            "        columns = [column.name for column in query.columns]",
            "",
            "        if not self._is_columns_allowed(columns):",
            "            raise Exception(",
            "                f\"Columns {columns} not allowed.\"",
            "                f\"Allowed columns are {[col['name'] for col in self.SCHEMA]}\"",
            "            )",
            "",
            "        # get content column if it is present",
            "        if TableField.CONTENT.value in columns:",
            "            content_col_index = columns.index(\"content\")",
            "            content = [",
            "                self._value_or_self(row[content_col_index]) for row in query.values",
            "            ]",
            "        else:",
            "            content = None",
            "",
            "        # get id column if it is present",
            "        ids = None",
            "        if TableField.ID.value in columns:",
            "            id_col_index = columns.index(\"id\")",
            "            ids = [self._value_or_self(row[id_col_index]) for row in query.values]",
            "        elif TableField.CONTENT.value is None:",
            "            raise Exception(\"Content or id is required!\")",
            "",
            "        # get embeddings column if it is present",
            "        if TableField.EMBEDDINGS.value in columns:",
            "            embeddings_col_index = columns.index(\"embeddings\")",
            "            embeddings = [",
            "                ast.literal_eval(self._value_or_self(row[embeddings_col_index]))",
            "                for row in query.values",
            "            ]",
            "        else:",
            "            raise Exception(\"Embeddings column is required!\")",
            "",
            "        if TableField.METADATA.value in columns:",
            "            metadata_col_index = columns.index(\"metadata\")",
            "            metadata = [",
            "                ast.literal_eval(self._value_or_self(row[metadata_col_index]))",
            "                for row in query.values",
            "            ]",
            "        else:",
            "            metadata = None",
            "",
            "        # create dataframe",
            "        data = {",
            "            TableField.CONTENT.value: content,",
            "            TableField.EMBEDDINGS.value: embeddings,",
            "            TableField.METADATA.value: metadata,",
            "        }",
            "        if ids is not None:",
            "            data[TableField.ID.value] = ids",
            "",
            "        return self.do_upsert(table_name, pd.DataFrame(data))",
            "",
            "    def _dispatch_update(self, query: Update):",
            "        \"\"\"",
            "        Dispatch update query to the appropriate method.",
            "        \"\"\"",
            "        table_name = query.table.parts[-1]",
            "",
            "        row = {}",
            "        for k, v in query.update_columns.items():",
            "            k = k.lower()",
            "            if isinstance(v, Constant):",
            "                v = v.value",
            "            if k == TableField.EMBEDDINGS.value and isinstance(v, str):",
            "                # it could be embeddings in string",
            "                try:",
            "                    v = ast.literal_eval(v)",
            "                except Exception:",
            "                    pass",
            "            row[k] = v",
            "",
            "        filters = conditions_to_filter(query.where)",
            "        row.update(filters)",
            "",
            "        # checks",
            "        if TableField.EMBEDDINGS.value not in row:",
            "            raise Exception(\"Embeddings column is required!\")",
            "",
            "        if TableField.CONTENT.value not in row:",
            "            raise Exception(\"Content is required!\")",
            "",
            "        # store",
            "        df = pd.DataFrame([row])",
            "",
            "        return self.do_upsert(table_name, df)",
            "",
            "    def do_upsert(self, table_name, df):",
            "        # if handler supports it, call upsert method",
            "",
            "        id_col = TableField.ID.value",
            "        content_col = TableField.CONTENT.value",
            "",
            "        def gen_hash(v):",
            "            return hashlib.md5(str(v).encode()).hexdigest()",
            "",
            "        if id_col not in df.columns:",
            "            # generate for all",
            "            df[id_col] = df[content_col].apply(gen_hash)",
            "        else:",
            "            # generate for empty",
            "            for i in range(len(df)):",
            "                if pd.isna(df.loc[i, id_col]):",
            "                    df.loc[i, id_col] = gen_hash(df.loc[i, content_col])",
            "",
            "        # remove duplicated ids",
            "        df = df.drop_duplicates([TableField.ID.value])",
            "",
            "        # id is string TODO is it ok?",
            "        df[id_col] = df[id_col].apply(str)",
            "",
            "        if hasattr(self, 'upsert'):",
            "            self.upsert(table_name, df)",
            "            return",
            "",
            "        # find existing ids",
            "        res = self.select(",
            "            table_name,",
            "            columns=[id_col],",
            "            conditions=[",
            "                FilterCondition(column=id_col, op=FilterOperator.IN, value=list(df[id_col]))",
            "            ]",
            "        )",
            "        existed_ids = list(res[id_col])",
            "",
            "        # update existed",
            "        df_update = df[df[id_col].isin(existed_ids)]",
            "        df_insert = df[~df[id_col].isin(existed_ids)]",
            "",
            "        if not df_update.empty:",
            "            try:",
            "                self.update(table_name, df_update, [id_col])",
            "            except NotImplementedError:",
            "                # not implemented? do it with delete and insert",
            "                conditions = [FilterCondition(",
            "                    column=id_col,",
            "                    op=FilterOperator.IN,",
            "                    value=list(df[id_col])",
            "                )]",
            "                self.delete(table_name, conditions)",
            "                self.insert(table_name, df_update)",
            "        if not df_insert.empty:",
            "            self.insert(table_name, df_insert)",
            "",
            "    def _dispatch_delete(self, query: Delete):",
            "        \"\"\"",
            "        Dispatch delete query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.table.parts[-1]",
            "        where_statement = query.where",
            "        conditions = self._extract_conditions(where_statement)",
            "",
            "        # dispatch delete",
            "        return self.delete(table_name, conditions=conditions)",
            "",
            "    def _dispatch_select(self, query: Select):",
            "        \"\"\"",
            "        Dispatch select query to the appropriate method.",
            "        \"\"\"",
            "        # parse key arguments",
            "        table_name = query.from_table.parts[-1]",
            "        # if targets are star, select all columns",
            "        if isinstance(query.targets[0], Star):",
            "            columns = [col[\"name\"] for col in self.SCHEMA]",
            "        else:",
            "            columns = [col.parts[-1] for col in query.targets]",
            "",
            "        if not self._is_columns_allowed(columns):",
            "            raise Exception(",
            "                f\"Columns {columns} not allowed.\"",
            "                f\"Allowed columns are {[col['name'] for col in self.SCHEMA]}\"",
            "            )",
            "",
            "        # check if columns are allowed",
            "        where_statement = query.where",
            "        conditions = self._extract_conditions(where_statement)",
            "",
            "        # get offset and limit",
            "        offset = query.offset.value if query.offset is not None else None",
            "        limit = query.limit.value if query.limit is not None else None",
            "",
            "        # dispatch select",
            "        return self.select(",
            "            table_name,",
            "            columns=columns,",
            "            conditions=conditions,",
            "            offset=offset,",
            "            limit=limit,",
            "        )",
            "",
            "    def _dispatch(self, query: ASTNode) -> HandlerResponse:",
            "        \"\"\"",
            "        Parse and Dispatch query to the appropriate method.",
            "        \"\"\"",
            "        dispatch_router = {",
            "            CreateTable: self._dispatch_create_table,",
            "            DropTables: self._dispatch_drop_table,",
            "            Insert: self._dispatch_insert,",
            "            Update: self._dispatch_update,",
            "            Delete: self._dispatch_delete,",
            "            Select: self._dispatch_select,",
            "        }",
            "        if type(query) in dispatch_router:",
            "            resp = dispatch_router[type(query)](query)",
            "            if resp is not None:",
            "                return HandlerResponse(",
            "                    resp_type=RESPONSE_TYPE.TABLE,",
            "                    data_frame=resp",
            "                )",
            "            else:",
            "                return HandlerResponse(resp_type=RESPONSE_TYPE.OK)",
            "",
            "        else:",
            "            raise NotImplementedError(f\"Query type {type(query)} not implemented.\")",
            "",
            "    def query(self, query: ASTNode) -> HandlerResponse:",
            "        \"\"\"",
            "        Receive query as AST (abstract syntax tree) and act upon it somehow.",
            "",
            "        Args:",
            "            query (ASTNode): sql query represented as AST. May be any kind",
            "                of query: SELECT, INSERT, DELETE, etc",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        return self._dispatch(query)",
            "",
            "    def create_table(self, table_name: str, if_not_exists=True) -> HandlerResponse:",
            "        \"\"\"Create table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            if_not_exists (bool): if True, do nothing if table exists",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def drop_table(self, table_name: str, if_exists=True) -> HandlerResponse:",
            "        \"\"\"Drop table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            if_exists (bool): if True, do nothing if table does not exist",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def insert(",
            "        self, table_name: str, data: pd.DataFrame",
            "    ) -> HandlerResponse:",
            "        \"\"\"Insert data into table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            data (pd.DataFrame): data to insert",
            "            columns (List[str]): columns to insert",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def update(",
            "        self, table_name: str, data: pd.DataFrame, key_columns: List[str] = None",
            "    ):",
            "        \"\"\"Update data in table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            data (pd.DataFrame): data to update",
            "            key_columns (List[str]): key to  to update",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def delete(",
            "        self, table_name: str, conditions: List[FilterCondition] = None",
            "    ) -> HandlerResponse:",
            "        \"\"\"Delete data from table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            conditions (List[FilterCondition]): conditions to delete",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def select(",
            "        self,",
            "        table_name: str,",
            "        columns: List[str] = None,",
            "        conditions: List[FilterCondition] = None,",
            "        offset: int = None,",
            "        limit: int = None,",
            "    ) -> pd.DataFrame:",
            "        \"\"\"Select data from table",
            "",
            "        Args:",
            "            table_name (str): table name",
            "            columns (List[str]): columns to select",
            "            conditions (List[FilterCondition]): conditions to select",
            "",
            "        Returns:",
            "            HandlerResponse",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def get_columns(self, table_name: str) -> HandlerResponse:",
            "        # return a fixed set of columns",
            "        data = pd.DataFrame(self.SCHEMA)",
            "        data.columns = [\"COLUMN_NAME\", \"DATA_TYPE\"]",
            "        return HandlerResponse(",
            "            resp_type=RESPONSE_TYPE.DATA,",
            "            data_frame=data,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "245": [
                "VectorStoreHandler",
                "_dispatch_update"
            ]
        },
        "addLocation": []
    }
}