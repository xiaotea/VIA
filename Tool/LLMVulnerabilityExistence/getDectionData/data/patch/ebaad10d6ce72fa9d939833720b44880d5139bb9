{
    "setup.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "         \"PyJWT>=2.4.0, <3.0\","
            },
            "1": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "         \"redis\","
            },
            "2": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "         \"selenium>=3.141.0\","
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        \"sshtunnel>=0.4.0, <0.5\","
            },
            "4": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "         \"simplejson>=3.15.0\","
            },
            "5": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "         \"slack_sdk>=3.1.1, <4\","
            },
            "6": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "         \"sqlalchemy>=1.4, <2\","
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import io",
            "import json",
            "import os",
            "import subprocess",
            "import sys",
            "",
            "from setuptools import find_packages, setup",
            "",
            "BASE_DIR = os.path.abspath(os.path.dirname(__file__))",
            "PACKAGE_JSON = os.path.join(BASE_DIR, \"superset-frontend\", \"package.json\")",
            "",
            "with open(PACKAGE_JSON, \"r\") as package_file:",
            "    version_string = json.load(package_file)[\"version\"]",
            "",
            "with io.open(\"README.md\", \"r\", encoding=\"utf-8\") as f:",
            "    long_description = f.read()",
            "",
            "",
            "def get_git_sha() -> str:",
            "    try:",
            "        s = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])",
            "        return s.decode().strip()",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "GIT_SHA = get_git_sha()",
            "version_info = {\"GIT_SHA\": GIT_SHA, \"version\": version_string}",
            "print(\"-==-\" * 15)",
            "print(\"VERSION: \" + version_string)",
            "print(\"GIT SHA: \" + GIT_SHA)",
            "print(\"-==-\" * 15)",
            "",
            "VERSION_INFO_FILE = os.path.join(BASE_DIR, \"superset\", \"static\", \"version_info.json\")",
            "",
            "with open(VERSION_INFO_FILE, \"w\") as version_file:",
            "    json.dump(version_info, version_file)",
            "",
            "",
            "setup(",
            "    name=\"apache-superset\",",
            "    description=\"A modern, enterprise-ready business intelligence web application\",",
            "    long_description=long_description,",
            "    long_description_content_type=\"text/markdown\",",
            "    version=version_string,",
            "    packages=find_packages(),",
            "    include_package_data=True,",
            "    zip_safe=False,",
            "    entry_points={",
            "        \"console_scripts\": [\"superset=superset.cli.main:superset\"],",
            "        # the `postgres` and `postgres+psycopg2://` schemes were removed in SQLAlchemy 1.4",
            "        # add an alias here to prevent breaking existing databases",
            "        \"sqlalchemy.dialects\": [",
            "            \"postgres.psycopg2 = sqlalchemy.dialects.postgresql:dialect\",",
            "            \"postgres = sqlalchemy.dialects.postgresql:dialect\",",
            "        ],",
            "    },",
            "    install_requires=[",
            "        \"backoff>=1.8.0\",",
            "        \"bleach>=3.0.2, <4.0.0\",",
            "        \"cachelib>=0.4.1,<0.5\",",
            "        \"celery>=5.2.2, <6.0.0\",",
            "        \"click>=8.0.3\",",
            "        \"colorama\",",
            "        \"croniter>=0.3.28\",",
            "        \"cron-descriptor\",",
            "        \"cryptography>=3.3.2\",",
            "        \"deprecation>=2.1.0, <2.2.0\",",
            "        \"flask>=2.0.0, <3.0.0\",",
            "        \"flask-appbuilder>=4.1.6, <5.0.0\",",
            "        \"flask-caching>=1.10.0\",",
            "        \"flask-compress\",",
            "        \"flask-talisman\",",
            "        \"flask-migrate\",",
            "        \"flask-wtf\",",
            "        \"func_timeout\",",
            "        \"geopy\",",
            "        \"graphlib-backport\",",
            "        \"gunicorn>=20.1.0\",",
            "        \"hashids>=1.3.1, <2\",",
            "        \"holidays>=0.17.2, <0.18\",",
            "        \"humanize\",",
            "        \"isodate\",",
            "        \"markdown>=3.0\",",
            "        \"msgpack>=1.0.0, <1.1\",",
            "        \"numpy==1.23.5\",",
            "        \"pandas>=1.5.2, <1.6\",",
            "        \"parsedatetime\",",
            "        \"pgsanity\",",
            "        \"polyline\",",
            "        \"pyparsing>=3.0.6, <4\",",
            "        \"python-dateutil\",",
            "        \"python-dotenv\",",
            "        \"python-geohash\",",
            "        \"pyarrow>=10.0.1, <11\",",
            "        \"pyyaml>=5.4\",",
            "        \"PyJWT>=2.4.0, <3.0\",",
            "        \"redis\",",
            "        \"selenium>=3.141.0\",",
            "        \"simplejson>=3.15.0\",",
            "        \"slack_sdk>=3.1.1, <4\",",
            "        \"sqlalchemy>=1.4, <2\",",
            "        \"sqlalchemy-utils>=0.38.3, <0.39\",",
            "        \"sqlparse>=0.4.3, <0.5\",",
            "        \"tabulate>=0.8.9, <0.9\",",
            "        \"typing-extensions>=4, <5\",",
            "        \"wtforms>=2.3.3, <2.4\",",
            "        \"wtforms-json\",",
            "    ],",
            "    extras_require={",
            "        \"athena\": [\"pyathena[pandas]>=2, <3\"],",
            "        \"aurora-data-api\": [\"preset-sqlalchemy-aurora-data-api>=0.2.8,<0.3\"],",
            "        \"bigquery\": [",
            "            \"pandas-gbq>=0.18.1\",",
            "            \"sqlalchemy-bigquery>=1.5.0\",",
            "            \"google-cloud-bigquery>=3.4.0\",",
            "        ],",
            "        \"clickhouse\": [\"clickhouse-connect>=0.4.6, <0.5\"],",
            "        \"cockroachdb\": [\"cockroachdb>=0.3.5, <0.4\"],",
            "        \"cors\": [\"flask-cors>=2.0.0\"],",
            "        \"crate\": [\"crate[sqlalchemy]>=0.26.0, <0.27\"],",
            "        \"databricks\": [",
            "            \"databricks-sql-connector>=2.0.2, <3\",",
            "            \"sqlalchemy-databricks>=0.2.0\",",
            "        ],",
            "        \"db2\": [\"ibm-db-sa>=0.3.5, <0.4\"],",
            "        \"dremio\": [\"sqlalchemy-dremio>=1.1.5, <1.3\"],",
            "        \"drill\": [\"sqlalchemy-drill==0.1.dev\"],",
            "        \"druid\": [\"pydruid>=0.6.5,<0.7\"],",
            "        \"dynamodb\": [\"pydynamodb>=0.4.2\"],",
            "        \"solr\": [\"sqlalchemy-solr >= 0.2.0\"],",
            "        \"elasticsearch\": [\"elasticsearch-dbapi>=0.2.9, <0.3.0\"],",
            "        \"exasol\": [\"sqlalchemy-exasol >= 2.4.0, <3.0\"],",
            "        \"excel\": [\"xlrd>=1.2.0, <1.3\"],",
            "        \"firebird\": [\"sqlalchemy-firebird>=0.7.0, <0.8\"],",
            "        \"firebolt\": [\"firebolt-sqlalchemy>=0.0.1\"],",
            "        \"gsheets\": [\"shillelagh[gsheetsapi]>=1.0.14, <2\"],",
            "        \"hana\": [\"hdbcli==2.4.162\", \"sqlalchemy_hana==0.4.0\"],",
            "        \"hive\": [\"pyhive[hive]>=0.6.5\", \"tableschema\", \"thrift>=0.14.1, <1.0.0\"],",
            "        \"impala\": [\"impyla>0.16.2, <0.17\"],",
            "        \"kusto\": [\"sqlalchemy-kusto>=2.0.0, <3\"],",
            "        \"kylin\": [\"kylinpy>=2.8.1, <2.9\"],",
            "        \"mssql\": [\"pymssql>=2.1.4, <2.2\"],",
            "        \"mysql\": [\"mysqlclient>=2.1.0, <3\"],",
            "        \"oracle\": [\"cx-Oracle>8.0.0, <8.1\"],",
            "        \"pinot\": [\"pinotdb>=0.3.3, <0.4\"],",
            "        \"postgres\": [\"psycopg2-binary==2.9.5\"],",
            "        \"presto\": [\"pyhive[presto]>=0.6.5\"],",
            "        \"trino\": [\"trino>=0.319.0\"],",
            "        \"prophet\": [\"prophet>=1.0.1, <1.1\", \"pystan<3.0\"],",
            "        \"redshift\": [\"sqlalchemy-redshift>=0.8.1, < 0.9\"],",
            "        \"rockset\": [\"rockset>=0.8.10, <0.9\"],",
            "        \"shillelagh\": [",
            "            \"shillelagh[datasetteapi,gsheetsapi,socrata,weatherapi]>=1.1.1, <2\"",
            "        ],",
            "        \"snowflake\": [\"snowflake-sqlalchemy>=1.2.4, <2\"],",
            "        \"spark\": [\"pyhive[hive]>=0.6.5\", \"tableschema\", \"thrift>=0.14.1, <1.0.0\"],",
            "        \"teradata\": [\"teradatasql>=16.20.0.23\"],",
            "        \"thumbnails\": [\"Pillow>=9.3.0, <10.0.0\"],",
            "        \"vertica\": [\"sqlalchemy-vertica-python>=0.5.9, < 0.6\"],",
            "        \"netezza\": [\"nzalchemy>=11.0.2\"],",
            "    },",
            "    python_requires=\"~=3.8\",",
            "    author=\"Apache Software Foundation\",",
            "    author_email=\"dev@superset.apache.org\",",
            "    url=\"https://superset.apache.org/\",",
            "    download_url=\"https://www.apache.org/dist/superset/\" + version_string,",
            "    classifiers=[",
            "        \"Programming Language :: Python :: 3.8\",",
            "        \"Programming Language :: Python :: 3.9\",",
            "        \"Programming Language :: Python :: 3.10\",",
            "        \"Programming Language :: Python :: 3.11\",",
            "    ],",
            ")"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import io",
            "import json",
            "import os",
            "import subprocess",
            "import sys",
            "",
            "from setuptools import find_packages, setup",
            "",
            "BASE_DIR = os.path.abspath(os.path.dirname(__file__))",
            "PACKAGE_JSON = os.path.join(BASE_DIR, \"superset-frontend\", \"package.json\")",
            "",
            "with open(PACKAGE_JSON, \"r\") as package_file:",
            "    version_string = json.load(package_file)[\"version\"]",
            "",
            "with io.open(\"README.md\", \"r\", encoding=\"utf-8\") as f:",
            "    long_description = f.read()",
            "",
            "",
            "def get_git_sha() -> str:",
            "    try:",
            "        s = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])",
            "        return s.decode().strip()",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "GIT_SHA = get_git_sha()",
            "version_info = {\"GIT_SHA\": GIT_SHA, \"version\": version_string}",
            "print(\"-==-\" * 15)",
            "print(\"VERSION: \" + version_string)",
            "print(\"GIT SHA: \" + GIT_SHA)",
            "print(\"-==-\" * 15)",
            "",
            "VERSION_INFO_FILE = os.path.join(BASE_DIR, \"superset\", \"static\", \"version_info.json\")",
            "",
            "with open(VERSION_INFO_FILE, \"w\") as version_file:",
            "    json.dump(version_info, version_file)",
            "",
            "",
            "setup(",
            "    name=\"apache-superset\",",
            "    description=\"A modern, enterprise-ready business intelligence web application\",",
            "    long_description=long_description,",
            "    long_description_content_type=\"text/markdown\",",
            "    version=version_string,",
            "    packages=find_packages(),",
            "    include_package_data=True,",
            "    zip_safe=False,",
            "    entry_points={",
            "        \"console_scripts\": [\"superset=superset.cli.main:superset\"],",
            "        # the `postgres` and `postgres+psycopg2://` schemes were removed in SQLAlchemy 1.4",
            "        # add an alias here to prevent breaking existing databases",
            "        \"sqlalchemy.dialects\": [",
            "            \"postgres.psycopg2 = sqlalchemy.dialects.postgresql:dialect\",",
            "            \"postgres = sqlalchemy.dialects.postgresql:dialect\",",
            "        ],",
            "    },",
            "    install_requires=[",
            "        \"backoff>=1.8.0\",",
            "        \"bleach>=3.0.2, <4.0.0\",",
            "        \"cachelib>=0.4.1,<0.5\",",
            "        \"celery>=5.2.2, <6.0.0\",",
            "        \"click>=8.0.3\",",
            "        \"colorama\",",
            "        \"croniter>=0.3.28\",",
            "        \"cron-descriptor\",",
            "        \"cryptography>=3.3.2\",",
            "        \"deprecation>=2.1.0, <2.2.0\",",
            "        \"flask>=2.0.0, <3.0.0\",",
            "        \"flask-appbuilder>=4.1.6, <5.0.0\",",
            "        \"flask-caching>=1.10.0\",",
            "        \"flask-compress\",",
            "        \"flask-talisman\",",
            "        \"flask-migrate\",",
            "        \"flask-wtf\",",
            "        \"func_timeout\",",
            "        \"geopy\",",
            "        \"graphlib-backport\",",
            "        \"gunicorn>=20.1.0\",",
            "        \"hashids>=1.3.1, <2\",",
            "        \"holidays>=0.17.2, <0.18\",",
            "        \"humanize\",",
            "        \"isodate\",",
            "        \"markdown>=3.0\",",
            "        \"msgpack>=1.0.0, <1.1\",",
            "        \"numpy==1.23.5\",",
            "        \"pandas>=1.5.2, <1.6\",",
            "        \"parsedatetime\",",
            "        \"pgsanity\",",
            "        \"polyline\",",
            "        \"pyparsing>=3.0.6, <4\",",
            "        \"python-dateutil\",",
            "        \"python-dotenv\",",
            "        \"python-geohash\",",
            "        \"pyarrow>=10.0.1, <11\",",
            "        \"pyyaml>=5.4\",",
            "        \"PyJWT>=2.4.0, <3.0\",",
            "        \"redis\",",
            "        \"selenium>=3.141.0\",",
            "        \"sshtunnel>=0.4.0, <0.5\",",
            "        \"simplejson>=3.15.0\",",
            "        \"slack_sdk>=3.1.1, <4\",",
            "        \"sqlalchemy>=1.4, <2\",",
            "        \"sqlalchemy-utils>=0.38.3, <0.39\",",
            "        \"sqlparse>=0.4.3, <0.5\",",
            "        \"tabulate>=0.8.9, <0.9\",",
            "        \"typing-extensions>=4, <5\",",
            "        \"wtforms>=2.3.3, <2.4\",",
            "        \"wtforms-json\",",
            "    ],",
            "    extras_require={",
            "        \"athena\": [\"pyathena[pandas]>=2, <3\"],",
            "        \"aurora-data-api\": [\"preset-sqlalchemy-aurora-data-api>=0.2.8,<0.3\"],",
            "        \"bigquery\": [",
            "            \"pandas-gbq>=0.18.1\",",
            "            \"sqlalchemy-bigquery>=1.5.0\",",
            "            \"google-cloud-bigquery>=3.4.0\",",
            "        ],",
            "        \"clickhouse\": [\"clickhouse-connect>=0.4.6, <0.5\"],",
            "        \"cockroachdb\": [\"cockroachdb>=0.3.5, <0.4\"],",
            "        \"cors\": [\"flask-cors>=2.0.0\"],",
            "        \"crate\": [\"crate[sqlalchemy]>=0.26.0, <0.27\"],",
            "        \"databricks\": [",
            "            \"databricks-sql-connector>=2.0.2, <3\",",
            "            \"sqlalchemy-databricks>=0.2.0\",",
            "        ],",
            "        \"db2\": [\"ibm-db-sa>=0.3.5, <0.4\"],",
            "        \"dremio\": [\"sqlalchemy-dremio>=1.1.5, <1.3\"],",
            "        \"drill\": [\"sqlalchemy-drill==0.1.dev\"],",
            "        \"druid\": [\"pydruid>=0.6.5,<0.7\"],",
            "        \"dynamodb\": [\"pydynamodb>=0.4.2\"],",
            "        \"solr\": [\"sqlalchemy-solr >= 0.2.0\"],",
            "        \"elasticsearch\": [\"elasticsearch-dbapi>=0.2.9, <0.3.0\"],",
            "        \"exasol\": [\"sqlalchemy-exasol >= 2.4.0, <3.0\"],",
            "        \"excel\": [\"xlrd>=1.2.0, <1.3\"],",
            "        \"firebird\": [\"sqlalchemy-firebird>=0.7.0, <0.8\"],",
            "        \"firebolt\": [\"firebolt-sqlalchemy>=0.0.1\"],",
            "        \"gsheets\": [\"shillelagh[gsheetsapi]>=1.0.14, <2\"],",
            "        \"hana\": [\"hdbcli==2.4.162\", \"sqlalchemy_hana==0.4.0\"],",
            "        \"hive\": [\"pyhive[hive]>=0.6.5\", \"tableschema\", \"thrift>=0.14.1, <1.0.0\"],",
            "        \"impala\": [\"impyla>0.16.2, <0.17\"],",
            "        \"kusto\": [\"sqlalchemy-kusto>=2.0.0, <3\"],",
            "        \"kylin\": [\"kylinpy>=2.8.1, <2.9\"],",
            "        \"mssql\": [\"pymssql>=2.1.4, <2.2\"],",
            "        \"mysql\": [\"mysqlclient>=2.1.0, <3\"],",
            "        \"oracle\": [\"cx-Oracle>8.0.0, <8.1\"],",
            "        \"pinot\": [\"pinotdb>=0.3.3, <0.4\"],",
            "        \"postgres\": [\"psycopg2-binary==2.9.5\"],",
            "        \"presto\": [\"pyhive[presto]>=0.6.5\"],",
            "        \"trino\": [\"trino>=0.319.0\"],",
            "        \"prophet\": [\"prophet>=1.0.1, <1.1\", \"pystan<3.0\"],",
            "        \"redshift\": [\"sqlalchemy-redshift>=0.8.1, < 0.9\"],",
            "        \"rockset\": [\"rockset>=0.8.10, <0.9\"],",
            "        \"shillelagh\": [",
            "            \"shillelagh[datasetteapi,gsheetsapi,socrata,weatherapi]>=1.1.1, <2\"",
            "        ],",
            "        \"snowflake\": [\"snowflake-sqlalchemy>=1.2.4, <2\"],",
            "        \"spark\": [\"pyhive[hive]>=0.6.5\", \"tableschema\", \"thrift>=0.14.1, <1.0.0\"],",
            "        \"teradata\": [\"teradatasql>=16.20.0.23\"],",
            "        \"thumbnails\": [\"Pillow>=9.3.0, <10.0.0\"],",
            "        \"vertica\": [\"sqlalchemy-vertica-python>=0.5.9, < 0.6\"],",
            "        \"netezza\": [\"nzalchemy>=11.0.2\"],",
            "    },",
            "    python_requires=\"~=3.8\",",
            "    author=\"Apache Software Foundation\",",
            "    author_email=\"dev@superset.apache.org\",",
            "    url=\"https://superset.apache.org/\",",
            "    download_url=\"https://www.apache.org/dist/superset/\" + version_string,",
            "    classifiers=[",
            "        \"Programming Language :: Python :: 3.8\",",
            "        \"Programming Language :: Python :: 3.9\",",
            "        \"Programming Language :: Python :: 3.10\",",
            "        \"Programming Language :: Python :: 3.11\",",
            "    ],",
            ")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "superset/config.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 476,
                "PatchRowcode": "     \"DRILL_TO_DETAIL\": False,"
            },
            "1": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": 477,
                "PatchRowcode": "     \"DATAPANEL_CLOSED_BY_DEFAULT\": False,"
            },
            "2": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": 478,
                "PatchRowcode": "     \"HORIZONTAL_FILTER_BAR\": False,"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 479,
                "PatchRowcode": "+    # Allow users to enable ssh tunneling when creating a DB."
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 480,
                "PatchRowcode": "+    # Users must check whether the DB engine supports SSH Tunnels"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 481,
                "PatchRowcode": "+    # otherwise enabling this flag won't have any effect on the DB."
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 482,
                "PatchRowcode": "+    \"SSH_TUNNELING\": False,"
            },
            "7": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": 483,
                "PatchRowcode": " }"
            },
            "8": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": 484,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 485,
                "PatchRowcode": "+# ------------------------------"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 486,
                "PatchRowcode": "+# SSH Tunnel"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 487,
                "PatchRowcode": "+# ------------------------------"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 488,
                "PatchRowcode": "+# Allow users to set the host used when connecting to the SSH Tunnel"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 489,
                "PatchRowcode": "+# as localhost and any other alias (0.0.0.0)"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 490,
                "PatchRowcode": "+# ----------------------------------------------------------------------"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 491,
                "PatchRowcode": "+#                             |"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 492,
                "PatchRowcode": "+# -------------+              |    +----------+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 493,
                "PatchRowcode": "+#     LOCAL    |              |    |  REMOTE  | :22 SSH"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 494,
                "PatchRowcode": "+#     CLIENT   | <== SSH ========> |  SERVER  | :8080 web service"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 495,
                "PatchRowcode": "+# -------------+              |    +----------+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 496,
                "PatchRowcode": "+#                             |"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 497,
                "PatchRowcode": "+#                          FIREWALL (only port 22 is open)"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 498,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 499,
                "PatchRowcode": "+# ----------------------------------------------------------------------"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 500,
                "PatchRowcode": "+SSH_TUNNEL_MANAGER_CLASS = \"superset.extensions.ssh.SSHManager\""
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 501,
                "PatchRowcode": "+SSH_TUNNEL_LOCAL_BIND_ADDRESS = \"127.0.0.1\""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 502,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": 481,
                "afterPatchRowNumber": 503,
                "PatchRowcode": " # Feature flags may also be set via 'SUPERSET_FEATURE_' prefixed environment vars."
            },
            "28": {
                "beforePatchRowNumber": 482,
                "afterPatchRowNumber": 504,
                "PatchRowcode": " DEFAULT_FEATURE_FLAGS.update("
            },
            "29": {
                "beforePatchRowNumber": 483,
                "afterPatchRowNumber": 505,
                "PatchRowcode": "     {"
            },
            "30": {
                "beforePatchRowNumber": 1506,
                "afterPatchRowNumber": 1528,
                "PatchRowcode": "     try:"
            },
            "31": {
                "beforePatchRowNumber": 1507,
                "afterPatchRowNumber": 1529,
                "PatchRowcode": "         # pylint: disable=import-error,wildcard-import,unused-wildcard-import"
            },
            "32": {
                "beforePatchRowNumber": 1508,
                "afterPatchRowNumber": 1530,
                "PatchRowcode": "         import superset_config"
            },
            "33": {
                "beforePatchRowNumber": 1509,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        from superset_config import *  # type:ignore"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1531,
                "PatchRowcode": "+        from superset_config import *  # type: ignore"
            },
            "35": {
                "beforePatchRowNumber": 1510,
                "afterPatchRowNumber": 1532,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 1511,
                "afterPatchRowNumber": 1533,
                "PatchRowcode": "         print(f\"Loaded your LOCAL configuration at [{superset_config.__file__}]\")"
            },
            "37": {
                "beforePatchRowNumber": 1512,
                "afterPatchRowNumber": 1534,
                "PatchRowcode": "     except Exception:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"The main config file for Superset",
            "",
            "All configuration in this file can be overridden by providing a superset_config",
            "in your PYTHONPATH as there is a ``from superset_config import *``",
            "at the end of this file.",
            "\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import imp  # pylint: disable=deprecated-module",
            "import importlib.util",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import sys",
            "from collections import OrderedDict",
            "from datetime import timedelta",
            "from email.mime.multipart import MIMEMultipart",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    List,",
            "    Literal,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    TYPE_CHECKING,",
            "    Union,",
            ")",
            "",
            "import pkg_resources",
            "from cachelib.base import BaseCache",
            "from celery.schedules import crontab",
            "from dateutil import tz",
            "from flask import Blueprint",
            "from flask_appbuilder.security.manager import AUTH_DB",
            "from pandas._libs.parsers import STR_NA_VALUES  # pylint: disable=no-name-in-module",
            "",
            "from superset.advanced_data_type.plugins.internet_address import internet_address",
            "from superset.advanced_data_type.plugins.internet_port import internet_port",
            "from superset.advanced_data_type.types import AdvancedDataType",
            "from superset.constants import CHANGE_ME_SECRET_KEY",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.stats_logger import DummyStatsLogger",
            "from superset.superset_typing import CacheConfig",
            "from superset.tasks.types import ExecutorType",
            "from superset.utils.core import is_test, NO_TIME_RANGE, parse_boolean_string",
            "from superset.utils.encrypt import SQLAlchemyUtilsAdapter",
            "from superset.utils.log import DBEventLogger",
            "from superset.utils.logging_configurator import DefaultLoggingConfigurator",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from flask_appbuilder.security.sqla import models",
            "",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.slice import Slice",
            "",
            "# Realtime stats logger, a StatsD implementation exists",
            "STATS_LOGGER = DummyStatsLogger()",
            "EVENT_LOGGER = DBEventLogger()",
            "",
            "SUPERSET_LOG_VIEW = True",
            "",
            "BASE_DIR = pkg_resources.resource_filename(\"superset\", \"\")",
            "if \"SUPERSET_HOME\" in os.environ:",
            "    DATA_DIR = os.environ[\"SUPERSET_HOME\"]",
            "else:",
            "    DATA_DIR = os.path.expanduser(\"~/.superset\")",
            "",
            "# ---------------------------------------------------------",
            "# Superset specific config",
            "# ---------------------------------------------------------",
            "VERSION_INFO_FILE = pkg_resources.resource_filename(",
            "    \"superset\", \"static/version_info.json\"",
            ")",
            "PACKAGE_JSON_FILE = pkg_resources.resource_filename(",
            "    \"superset\", \"static/assets/package.json\"",
            ")",
            "",
            "# Multiple favicons can be specified here. The \"href\" property",
            "# is mandatory, but \"sizes,\" \"type,\" and \"rel\" are optional.",
            "# For example:",
            "# {",
            "#     \"href\":path/to/image.png\",",
            "#     \"sizes\": \"16x16\",",
            "#     \"type\": \"image/png\"",
            "#     \"rel\": \"icon\"",
            "# },",
            "FAVICONS = [{\"href\": \"/static/assets/images/favicon.png\"}]",
            "",
            "",
            "def _try_json_readversion(filepath: str) -> Optional[str]:",
            "    try:",
            "        with open(filepath, \"r\") as f:",
            "            return json.load(f).get(\"version\")",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def _try_json_readsha(filepath: str, length: int) -> Optional[str]:",
            "    try:",
            "        with open(filepath, \"r\") as f:",
            "            return json.load(f).get(\"GIT_SHA\")[:length]",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "#",
            "# If True, we will skip the call to load the logger config found in alembic.init",
            "#",
            "ALEMBIC_SKIP_LOG_CONFIG = False",
            "",
            "# Depending on the context in which this config is loaded, the",
            "# version_info.json file may or may not be available, as it is",
            "# generated on install via setup.py. In the event that we're",
            "# actually running Superset, we will have already installed,",
            "# therefore it WILL exist. When unit tests are running, however,",
            "# it WILL NOT exist, so we fall back to reading package.json",
            "VERSION_STRING = _try_json_readversion(VERSION_INFO_FILE) or _try_json_readversion(",
            "    PACKAGE_JSON_FILE",
            ")",
            "",
            "VERSION_SHA_LENGTH = 8",
            "VERSION_SHA = _try_json_readsha(VERSION_INFO_FILE, VERSION_SHA_LENGTH)",
            "",
            "# Build number is shown in the About section if available. This",
            "# can be replaced at build time to expose build information.",
            "BUILD_NUMBER = None",
            "",
            "# default viz used in chart explorer & SQL Lab explore",
            "DEFAULT_VIZ_TYPE = \"table\"",
            "",
            "# default row limit when requesting chart data",
            "ROW_LIMIT = 50000",
            "# default row limit when requesting samples from datasource in explore view",
            "SAMPLES_ROW_LIMIT = 1000",
            "# max rows retrieved by filter select auto complete",
            "FILTER_SELECT_ROW_LIMIT = 10000",
            "# default time filter in explore",
            "# values may be \"Last day\", \"Last week\", \"<ISO date> : now\", etc.",
            "DEFAULT_TIME_FILTER = NO_TIME_RANGE",
            "",
            "SUPERSET_WEBSERVER_PROTOCOL = \"http\"",
            "SUPERSET_WEBSERVER_ADDRESS = \"0.0.0.0\"",
            "SUPERSET_WEBSERVER_PORT = 8088",
            "",
            "# This is an important setting, and should be lower than your",
            "# [load balancer / proxy / envoy / kong / ...] timeout settings.",
            "# You should also make sure to configure your WSGI server",
            "# (gunicorn, nginx, apache, ...) timeout setting to be <= to this setting",
            "SUPERSET_WEBSERVER_TIMEOUT = int(timedelta(minutes=1).total_seconds())",
            "",
            "# this 2 settings are used by dashboard period force refresh feature",
            "# When user choose auto force refresh frequency",
            "# < SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT",
            "# they will see warning message in the Refresh Interval Modal.",
            "# please check PR #9886",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT = 0",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE = None",
            "",
            "SUPERSET_DASHBOARD_POSITION_DATA_LIMIT = 65535",
            "CUSTOM_SECURITY_MANAGER = None",
            "SQLALCHEMY_TRACK_MODIFICATIONS = False",
            "# ---------------------------------------------------------",
            "",
            "# Your App secret key. Make sure you override it on superset_config.py.",
            "# Use a strong complex alphanumeric string and use a tool to help you generate",
            "# a sufficiently random sequence, ex: openssl rand -base64 42\"",
            "SECRET_KEY = CHANGE_ME_SECRET_KEY",
            "",
            "# The SQLAlchemy connection string.",
            "SQLALCHEMY_DATABASE_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"superset.db\")",
            "# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'",
            "# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp'",
            "",
            "# In order to hook up a custom password store for all SQLACHEMY connections",
            "# implement a function that takes a single argument of type 'sqla.engine.url',",
            "# returns a password and set SQLALCHEMY_CUSTOM_PASSWORD_STORE.",
            "#",
            "# e.g.:",
            "# def lookup_password(url):",
            "#     return 'secret'",
            "# SQLALCHEMY_CUSTOM_PASSWORD_STORE = lookup_password",
            "SQLALCHEMY_CUSTOM_PASSWORD_STORE = None",
            "",
            "#",
            "# The EncryptedFieldTypeAdapter is used whenever we're building SqlAlchemy models",
            "# which include sensitive fields that should be app-encrypted BEFORE sending",
            "# to the DB.",
            "#",
            "# Note: the default impl leverages SqlAlchemyUtils' EncryptedType, which defaults",
            "#  to AesEngine that uses AES-128 under the covers using the app's SECRET_KEY",
            "#  as key material. Do note that AesEngine allows for queryability over the",
            "#  encrypted fields.",
            "#",
            "#  To change the default engine you need to define your own adapter:",
            "#",
            "# e.g.:",
            "#",
            "# class AesGcmEncryptedAdapter(",
            "#     AbstractEncryptedFieldAdapter",
            "# ):",
            "#     def create(",
            "#         self,",
            "#         app_config: Optional[Dict[str, Any]],",
            "#         *args: List[Any],",
            "#         **kwargs: Optional[Dict[str, Any]],",
            "#     ) -> TypeDecorator:",
            "#         if app_config:",
            "#             return EncryptedType(",
            "#                 *args, app_config[\"SECRET_KEY\"], engine=AesGcmEngine, **kwargs",
            "#             )",
            "#         raise Exception(\"Missing app_config kwarg\")",
            "#",
            "#",
            "#  SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = AesGcmEncryptedAdapter",
            "SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = (  # pylint: disable=invalid-name",
            "    SQLAlchemyUtilsAdapter",
            ")",
            "# The limit of queries fetched for query search",
            "QUERY_SEARCH_LIMIT = 1000",
            "",
            "# Flask-WTF flag for CSRF",
            "WTF_CSRF_ENABLED = True",
            "",
            "# Add endpoints that need to be exempt from CSRF protection",
            "WTF_CSRF_EXEMPT_LIST = [",
            "    \"superset.views.core.log\",",
            "    \"superset.views.core.explore_json\",",
            "    \"superset.charts.data.api.data\",",
            "]",
            "",
            "# Whether to run the web server in debug mode or not",
            "DEBUG = os.environ.get(\"FLASK_ENV\") == \"development\"",
            "FLASK_USE_RELOAD = True",
            "",
            "# Enable profiling of Python calls. Turn this on and append ``?_instrument=1``",
            "# to the page to see the call stack.",
            "PROFILING = False",
            "",
            "# Superset allows server-side python stacktraces to be surfaced to the",
            "# user when this feature is on. This may has security implications",
            "# and it's more secure to turn it off in production settings.",
            "SHOW_STACKTRACE = True",
            "",
            "# Use all X-Forwarded headers when ENABLE_PROXY_FIX is True.",
            "# When proxying to a different port, set \"x_port\" to 0 to avoid downstream issues.",
            "ENABLE_PROXY_FIX = False",
            "PROXY_FIX_CONFIG = {\"x_for\": 1, \"x_proto\": 1, \"x_host\": 1, \"x_port\": 1, \"x_prefix\": 1}",
            "",
            "# Configuration for scheduling queries from SQL Lab.",
            "SCHEDULED_QUERIES: Dict[str, Any] = {}",
            "",
            "# ------------------------------",
            "# GLOBALS FOR APP Builder",
            "# ------------------------------",
            "# Uncomment to setup Your App name",
            "APP_NAME = \"Superset\"",
            "",
            "# Specify the App icon",
            "APP_ICON = \"/static/assets/images/superset-logo-horiz.png\"",
            "",
            "# Specify where clicking the logo would take the user",
            "# e.g. setting it to '/' would take the user to '/superset/welcome/'",
            "LOGO_TARGET_PATH = None",
            "",
            "# Specify tooltip that should appear when hovering over the App Icon/Logo",
            "LOGO_TOOLTIP = \"\"",
            "",
            "# Specify any text that should appear to the right of the logo",
            "LOGO_RIGHT_TEXT: Union[Callable[[], str], str] = \"\"",
            "",
            "# Enables SWAGGER UI for superset openapi spec",
            "# ex: http://localhost:8080/swagger/v1",
            "FAB_API_SWAGGER_UI = True",
            "",
            "# Druid query timezone",
            "# tz.tzutc() : Using utc timezone",
            "# tz.tzlocal() : Using local timezone",
            "# tz.gettz('Asia/Shanghai') : Using the time zone with specific name",
            "# [TimeZone List]",
            "# See: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones",
            "# other tz can be overridden by providing a local_config",
            "DRUID_TZ = tz.tzutc()",
            "DRUID_ANALYSIS_TYPES = [\"cardinality\"]",
            "",
            "",
            "# ----------------------------------------------------",
            "# AUTHENTICATION CONFIG",
            "# ----------------------------------------------------",
            "# The authentication type",
            "# AUTH_OID : Is for OpenID",
            "# AUTH_DB : Is for database (username/password)",
            "# AUTH_LDAP : Is for LDAP",
            "# AUTH_REMOTE_USER : Is for using REMOTE_USER from web server",
            "AUTH_TYPE = AUTH_DB",
            "",
            "# Uncomment to setup Full admin role name",
            "# AUTH_ROLE_ADMIN = 'Admin'",
            "",
            "# Uncomment to setup Public role name, no authentication needed",
            "# AUTH_ROLE_PUBLIC = 'Public'",
            "",
            "# Will allow user self registration",
            "# AUTH_USER_REGISTRATION = True",
            "",
            "# The default user self registration role",
            "# AUTH_USER_REGISTRATION_ROLE = \"Public\"",
            "",
            "# When using LDAP Auth, setup the LDAP server",
            "# AUTH_LDAP_SERVER = \"ldap://ldapserver.new\"",
            "",
            "# Uncomment to setup OpenID providers example for OpenID authentication",
            "# OPENID_PROVIDERS = [",
            "#    { 'name': 'Yahoo', 'url': 'https://open.login.yahoo.com/' },",
            "#    { 'name': 'Flickr', 'url': 'https://www.flickr.com/<username>' },",
            "",
            "# ---------------------------------------------------",
            "# Roles config",
            "# ---------------------------------------------------",
            "# Grant public role the same set of permissions as for a selected builtin role.",
            "# This is useful if one wants to enable anonymous users to view",
            "# dashboards. Explicit grant on specific datasets is still required.",
            "PUBLIC_ROLE_LIKE: Optional[str] = None",
            "",
            "# ---------------------------------------------------",
            "# Babel config for translations",
            "# ---------------------------------------------------",
            "# Setup default language",
            "BABEL_DEFAULT_LOCALE = \"en\"",
            "# Your application default translation path",
            "BABEL_DEFAULT_FOLDER = \"superset/translations\"",
            "# The allowed translation for you app",
            "LANGUAGES = {",
            "    \"en\": {\"flag\": \"us\", \"name\": \"English\"},",
            "    \"es\": {\"flag\": \"es\", \"name\": \"Spanish\"},",
            "    \"it\": {\"flag\": \"it\", \"name\": \"Italian\"},",
            "    \"fr\": {\"flag\": \"fr\", \"name\": \"French\"},",
            "    \"zh\": {\"flag\": \"cn\", \"name\": \"Chinese\"},",
            "    \"ja\": {\"flag\": \"jp\", \"name\": \"Japanese\"},",
            "    \"de\": {\"flag\": \"de\", \"name\": \"German\"},",
            "    \"pt\": {\"flag\": \"pt\", \"name\": \"Portuguese\"},",
            "    \"pt_BR\": {\"flag\": \"br\", \"name\": \"Brazilian Portuguese\"},",
            "    \"ru\": {\"flag\": \"ru\", \"name\": \"Russian\"},",
            "    \"ko\": {\"flag\": \"kr\", \"name\": \"Korean\"},",
            "    \"sk\": {\"flag\": \"sk\", \"name\": \"Slovak\"},",
            "    \"sl\": {\"flag\": \"si\", \"name\": \"Slovenian\"},",
            "    \"nl\": {\"flag\": \"nl\", \"name\": \"Dutch\"},",
            "}",
            "# Turning off i18n by default as translation in most languages are",
            "# incomplete and not well maintained.",
            "LANGUAGES = {}",
            "",
            "# ---------------------------------------------------",
            "# Feature flags",
            "# ---------------------------------------------------",
            "# Feature flags that are set by default go here. Their values can be",
            "# overwritten by those specified under FEATURE_FLAGS in superset_config.py",
            "# For example, DEFAULT_FEATURE_FLAGS = { 'FOO': True, 'BAR': False } here",
            "# and FEATURE_FLAGS = { 'BAR': True, 'BAZ': True } in superset_config.py",
            "# will result in combined feature flags of { 'FOO': True, 'BAR': True, 'BAZ': True }",
            "DEFAULT_FEATURE_FLAGS: Dict[str, bool] = {",
            "    # allow dashboard to use sub-domains to send chart request",
            "    # you also need ENABLE_CORS and",
            "    # SUPERSET_WEBSERVER_DOMAINS for list of domains",
            "    \"ALLOW_DASHBOARD_DOMAIN_SHARDING\": True,",
            "    # Experimental feature introducing a client (browser) cache",
            "    \"CLIENT_CACHE\": False,",
            "    \"DISABLE_DATASET_SOURCE_EDIT\": False,",
            "    # When using a recent version of Druid that supports JOINs turn this on",
            "    \"DRUID_JOINS\": False,",
            "    \"DYNAMIC_PLUGINS\": False,",
            "    # With Superset 2.0, we are updating the default so that the legacy datasource",
            "    # editor no longer shows. Currently this is set to false so that the editor",
            "    # option does show, but we will be depreciating it.",
            "    \"DISABLE_LEGACY_DATASOURCE_EDITOR\": True,",
            "    # For some security concerns, you may need to enforce CSRF protection on",
            "    # all query request to explore_json endpoint. In Superset, we use",
            "    # `flask-csrf <https://sjl.bitbucket.io/flask-csrf/>`_ add csrf protection",
            "    # for all POST requests, but this protection doesn't apply to GET method.",
            "    # When ENABLE_EXPLORE_JSON_CSRF_PROTECTION is set to true, your users cannot",
            "    # make GET request to explore_json. explore_json accepts both GET and POST request.",
            "    # See `PR 7935 <https://github.com/apache/superset/pull/7935>`_ for more details.",
            "    \"ENABLE_EXPLORE_JSON_CSRF_PROTECTION\": False,",
            "    \"ENABLE_TEMPLATE_PROCESSING\": False,",
            "    \"ENABLE_TEMPLATE_REMOVE_FILTERS\": False,",
            "    # Allow for javascript controls components",
            "    # this enables programmers to customize certain charts (like the",
            "    # geospatial ones) by inputing javascript in controls. This exposes",
            "    # an XSS security vulnerability",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\": False,",
            "    \"KV_STORE\": False,",
            "    # When this feature is enabled, nested types in Presto will be",
            "    # expanded into extra columns and/or arrays. This is experimental,",
            "    # and doesn't work with all nested types.",
            "    \"PRESTO_EXPAND_DATA\": False,",
            "    # Exposes API endpoint to compute thumbnails",
            "    \"THUMBNAILS\": False,",
            "    \"DASHBOARD_CACHE\": False,",
            "    \"REMOVE_SLICE_LEVEL_LABEL_COLORS\": False,",
            "    \"SHARE_QUERIES_VIA_KV_STORE\": False,",
            "    \"TAGGING_SYSTEM\": False,",
            "    \"SQLLAB_BACKEND_PERSISTENCE\": True,",
            "    \"LISTVIEWS_DEFAULT_CARD_VIEW\": False,",
            "    # When True, this flag allows display of HTML tags in Markdown components",
            "    \"DISPLAY_MARKDOWN_HTML\": True,",
            "    # When True, this escapes HTML (rather than rendering it) in Markdown components",
            "    \"ESCAPE_MARKDOWN_HTML\": False,",
            "    \"DASHBOARD_NATIVE_FILTERS\": True,",
            "    \"DASHBOARD_CROSS_FILTERS\": False,",
            "    # Feature is under active development and breaking changes are expected",
            "    \"DASHBOARD_NATIVE_FILTERS_SET\": False,",
            "    \"DASHBOARD_FILTERS_EXPERIMENTAL\": False,",
            "    \"DASHBOARD_VIRTUALIZATION\": False,",
            "    \"GLOBAL_ASYNC_QUERIES\": False,",
            "    \"VERSIONED_EXPORT\": True,",
            "    \"EMBEDDED_SUPERSET\": False,",
            "    # Enables Alerts and reports new implementation",
            "    \"ALERT_REPORTS\": False,",
            "    \"DASHBOARD_RBAC\": False,",
            "    \"ENABLE_EXPLORE_DRAG_AND_DROP\": True,",
            "    \"ENABLE_FILTER_BOX_MIGRATION\": False,",
            "    \"ENABLE_ADVANCED_DATA_TYPES\": False,",
            "    \"ENABLE_DND_WITH_CLICK_UX\": True,",
            "    # Enabling ALERTS_ATTACH_REPORTS, the system sends email and slack message",
            "    # with screenshot and link",
            "    # Disables ALERTS_ATTACH_REPORTS, the system DOES NOT generate screenshot",
            "    # for report with type 'alert' and sends email and slack message with only link;",
            "    # for report with type 'report' still send with email and slack message with",
            "    # screenshot and link",
            "    \"ALERTS_ATTACH_REPORTS\": True,",
            "    # FORCE_DATABASE_CONNECTIONS_SSL is depreciated.",
            "    \"FORCE_DATABASE_CONNECTIONS_SSL\": False,",
            "    # Enabling ENFORCE_DB_ENCRYPTION_UI forces all database connections to be",
            "    # encrypted before being saved into superset metastore.",
            "    \"ENFORCE_DB_ENCRYPTION_UI\": False,",
            "    # Allow users to export full CSV of table viz type.",
            "    # This could cause the server to run out of memory or compute.",
            "    \"ALLOW_FULL_CSV_EXPORT\": False,",
            "    \"UX_BETA\": False,",
            "    \"GENERIC_CHART_AXES\": False,",
            "    \"ALLOW_ADHOC_SUBQUERY\": False,",
            "    \"USE_ANALAGOUS_COLORS\": False,",
            "    \"DASHBOARD_EDIT_CHART_IN_NEW_TAB\": False,",
            "    # Apply RLS rules to SQL Lab queries. This requires parsing and manipulating the",
            "    # query, and might break queries and/or allow users to bypass RLS. Use with care!",
            "    \"RLS_IN_SQLLAB\": False,",
            "    # Enable caching per impersonation key (e.g username) in a datasource where user",
            "    # impersonation is enabled",
            "    \"CACHE_IMPERSONATION\": False,",
            "    # Enable sharing charts with embedding",
            "    \"EMBEDDABLE_CHARTS\": True,",
            "    \"DRILL_TO_DETAIL\": False,",
            "    \"DATAPANEL_CLOSED_BY_DEFAULT\": False,",
            "    \"HORIZONTAL_FILTER_BAR\": False,",
            "}",
            "",
            "# Feature flags may also be set via 'SUPERSET_FEATURE_' prefixed environment vars.",
            "DEFAULT_FEATURE_FLAGS.update(",
            "    {",
            "        k[len(\"SUPERSET_FEATURE_\") :]: parse_boolean_string(v)",
            "        for k, v in os.environ.items()",
            "        if re.search(r\"^SUPERSET_FEATURE_\\w+\", k)",
            "    }",
            ")",
            "",
            "# This is merely a default.",
            "FEATURE_FLAGS: Dict[str, bool] = {}",
            "",
            "# A function that receives a dict of all feature flags",
            "# (DEFAULT_FEATURE_FLAGS merged with FEATURE_FLAGS)",
            "# can alter it, and returns a similar dict. Note the dict of feature",
            "# flags passed to the function is a deepcopy of the dict in the config,",
            "# and can therefore be mutated without side-effect",
            "#",
            "# GET_FEATURE_FLAGS_FUNC can be used to implement progressive rollouts,",
            "# role-based features, or a full on A/B testing framework.",
            "#",
            "# from flask import g, request",
            "# def GET_FEATURE_FLAGS_FUNC(feature_flags_dict: Dict[str, bool]) -> Dict[str, bool]:",
            "#     if hasattr(g, \"user\") and g.user.is_active:",
            "#         feature_flags_dict['some_feature'] = g.user and g.user.get_id() == 5",
            "#     return feature_flags_dict",
            "GET_FEATURE_FLAGS_FUNC: Optional[Callable[[Dict[str, bool]], Dict[str, bool]]] = None",
            "# A function that receives a feature flag name and an optional default value.",
            "# Has a similar utility to GET_FEATURE_FLAGS_FUNC but it's useful to not force the",
            "# evaluation of all feature flags when just evaluating a single one.",
            "#",
            "# Note that the default `get_feature_flags` will evaluate each feature with this",
            "# callable when the config key is set, so don't use both GET_FEATURE_FLAGS_FUNC",
            "# and IS_FEATURE_ENABLED_FUNC in conjunction.",
            "IS_FEATURE_ENABLED_FUNC: Optional[Callable[[str, Optional[bool]], bool]] = None",
            "# A function that expands/overrides the frontend `bootstrap_data.common` object.",
            "# Can be used to implement custom frontend functionality,",
            "# or dynamically change certain configs.",
            "#",
            "# Values in `bootstrap_data.common` should have these characteristics:",
            "# - They are not specific to a page the user is visiting",
            "# - They do not contain secrets",
            "#",
            "# Takes as a parameter the common bootstrap payload before transformations.",
            "# Returns a dict containing data that should be added or overridden to the payload.",
            "COMMON_BOOTSTRAP_OVERRIDES_FUNC: Callable[",
            "    [Dict[str, Any]], Dict[str, Any]",
            "] = lambda data: {}  # default: empty dict",
            "",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES is used for adding custom categorical color schemes",
            "# example code for \"My custom warm to hot\" color scheme",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES = [",
            "#     {",
            "#         \"id\": 'myVisualizationColors',",
            "#         \"description\": '',",
            "#         \"label\": 'My Visualization Colors',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77',",
            "#          '#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_CATEGORICAL_COLOR_SCHEMES: List[Dict[str, Any]] = []",
            "",
            "# THEME_OVERRIDES is used for adding custom theme to superset",
            "# example code for \"My theme\" custom scheme",
            "# THEME_OVERRIDES = {",
            "#   \"borderRadius\": 4,",
            "#   \"colors\": {",
            "#     \"primary\": {",
            "#       \"base\": 'red',",
            "#     },",
            "#     \"secondary\": {",
            "#       \"base\": 'green',",
            "#     },",
            "#     \"grayscale\": {",
            "#       \"base\": 'orange',",
            "#     }",
            "#   }",
            "# }",
            "",
            "THEME_OVERRIDES: Dict[str, Any] = {}",
            "",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES is used for adding custom sequential color schemes",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES =  [",
            "#     {",
            "#         \"id\": 'warmToHot',",
            "#         \"description\": '',",
            "#         \"isDiverging\": True,",
            "#         \"label\": 'My custom warm to hot',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD',",
            "#          '#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_SEQUENTIAL_COLOR_SCHEMES: List[Dict[str, Any]] = []",
            "",
            "# ---------------------------------------------------",
            "# Thumbnail config (behind feature flag)",
            "# ---------------------------------------------------",
            "# When executing Alerts & Reports or Thumbnails as the Selenium user, this defines",
            "# the username of the account used to render the queries and dashboards/charts",
            "THUMBNAIL_SELENIUM_USER: Optional[str] = \"admin\"",
            "",
            "# To be able to have different thumbnails for different users, use these configs to",
            "# define which user to execute the thumbnails and potentially custom functions for",
            "# calculating thumbnail digests. To have unique thumbnails for all users, use the",
            "# following config:",
            "# THUMBNAIL_EXECUTE_AS = [ExecutorType.CURRENT_USER]",
            "THUMBNAIL_EXECUTE_AS = [ExecutorType.SELENIUM]",
            "",
            "# By default, thumbnail digests are calculated based on various parameters in the",
            "# chart/dashboard metadata, and in the case of user-specific thumbnails, the",
            "# username. To specify a custom digest function, use the following config parameters",
            "# to define callbacks that receive",
            "# 1. the model (dashboard or chart)",
            "# 2. the executor type (e.g. ExecutorType.SELENIUM)",
            "# 3. the executor's username (note, this is the executor as defined by",
            "# `THUMBNAIL_EXECUTE_AS`; the executor is only equal to the currently logged in",
            "# user if the executor type is equal to `ExecutorType.CURRENT_USER`)",
            "# and return the final digest string:",
            "THUMBNAIL_DASHBOARD_DIGEST_FUNC: Optional[",
            "    Callable[[Dashboard, ExecutorType, str], str]",
            "] = None",
            "THUMBNAIL_CHART_DIGEST_FUNC: Optional[Callable[[Slice, ExecutorType, str], str]] = None",
            "",
            "THUMBNAIL_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"NullCache\",",
            "    \"CACHE_NO_NULL_WARNING\": True,",
            "}",
            "",
            "# Time before selenium times out after trying to locate an element on the page and wait",
            "# for that element to load for a screenshot.",
            "SCREENSHOT_LOCATE_WAIT = int(timedelta(seconds=10).total_seconds())",
            "# Time before selenium times out after waiting for all DOM class elements named",
            "# \"loading\" are gone.",
            "SCREENSHOT_LOAD_WAIT = int(timedelta(minutes=1).total_seconds())",
            "# Selenium destroy retries",
            "SCREENSHOT_SELENIUM_RETRIES = 5",
            "# Give selenium an headstart, in seconds",
            "SCREENSHOT_SELENIUM_HEADSTART = 3",
            "# Wait for the chart animation, in seconds",
            "SCREENSHOT_SELENIUM_ANIMATION_WAIT = 5",
            "# Replace unexpected errors in screenshots with real error messages",
            "SCREENSHOT_REPLACE_UNEXPECTED_ERRORS = False",
            "# Max time to wait for error message modal to show up, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_VISIBLE = 5",
            "# Max time to wait for error message modal to close, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_INVISIBLE = 5",
            "",
            "# ---------------------------------------------------",
            "# Image and file configuration",
            "# ---------------------------------------------------",
            "# The file upload folder, when using models with files",
            "UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "UPLOAD_CHUNK_SIZE = 4096",
            "",
            "# The image upload folder, when using models with images",
            "IMG_UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "",
            "# The image upload url, when using models with images",
            "IMG_UPLOAD_URL = \"/static/uploads/\"",
            "# Setup image size default is (300, 200, True)",
            "# IMG_SIZE = (300, 200, True)",
            "",
            "# Default cache timeout, applies to all cache backends unless specifically overridden in",
            "# each cache config.",
            "CACHE_DEFAULT_TIMEOUT = int(timedelta(days=1).total_seconds())",
            "",
            "# Default cache for Superset objects",
            "CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for datasource metadata and query results",
            "DATA_CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for dashboard filter state (`CACHE_TYPE` defaults to `SimpleCache` when",
            "#  running in debug mode unless overridden)",
            "FILTER_STATE_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=90).total_seconds()),",
            "    # should the timeout be reset when retrieving a cached value",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "}",
            "",
            "# Cache for explore form data state (`CACHE_TYPE` defaults to `SimpleCache` when",
            "#  running in debug mode unless overridden)",
            "EXPLORE_FORM_DATA_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=7).total_seconds()),",
            "    # should the timeout be reset when retrieving a cached value",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "}",
            "",
            "# store cache keys by datasource UID (via CacheKey) for custom processing/invalidation",
            "STORE_CACHE_KEYS_IN_METADATA_DB = False",
            "",
            "# CORS Options",
            "ENABLE_CORS = False",
            "CORS_OPTIONS: Dict[Any, Any] = {}",
            "",
            "# Sanitizes the HTML content used in markdowns to allow its rendering in a safe manner.",
            "# Disabling this option is not recommended for security reasons. If you wish to allow",
            "# valid safe elements that are not included in the default sanitization schema, use the",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS configuration.",
            "HTML_SANITIZATION = True",
            "",
            "# Use this configuration to extend the HTML sanitization schema.",
            "# By default we use the Gihtub schema defined in",
            "# https://github.com/syntax-tree/hast-util-sanitize/blob/main/lib/schema.js",
            "# For example, the following configuration would allow the rendering of the",
            "# style attribute for div elements and the ftp protocol in hrefs:",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS = {",
            "#   \"attributes\": {",
            "#     \"div\": [\"style\"],",
            "#   },",
            "#   \"protocols\": {",
            "#     \"href\": [\"ftp\"],",
            "#   }",
            "# }",
            "# Be careful when extending the default schema to avoid XSS attacks.",
            "HTML_SANITIZATION_SCHEMA_EXTENSIONS: Dict[str, Any] = {}",
            "",
            "# Chrome allows up to 6 open connections per domain at a time. When there are more",
            "# than 6 slices in dashboard, a lot of time fetch requests are queued up and wait for",
            "# next available socket. PR #5039 is trying to allow domain sharding for Superset,",
            "# and this feature will be enabled by configuration only (by default Superset",
            "# doesn't allow cross-domain request).",
            "SUPERSET_WEBSERVER_DOMAINS = None",
            "",
            "# Allowed format types for upload on Database view",
            "EXCEL_EXTENSIONS = {\"xlsx\", \"xls\"}",
            "CSV_EXTENSIONS = {\"csv\", \"tsv\", \"txt\"}",
            "COLUMNAR_EXTENSIONS = {\"parquet\", \"zip\"}",
            "ALLOWED_EXTENSIONS = {*EXCEL_EXTENSIONS, *CSV_EXTENSIONS, *COLUMNAR_EXTENSIONS}",
            "",
            "# CSV Options: key/value pairs that will be passed as argument to DataFrame.to_csv",
            "# method.",
            "# note: index option should not be overridden",
            "CSV_EXPORT = {\"encoding\": \"utf-8\"}",
            "",
            "# ---------------------------------------------------",
            "# Time grain configurations",
            "# ---------------------------------------------------",
            "# List of time grains to disable in the application (see list of builtin",
            "# time grains in superset/db_engine_specs/base.py).",
            "# For example: to disable 1 second time grain:",
            "# TIME_GRAIN_DENYLIST = ['PT1S']",
            "TIME_GRAIN_DENYLIST: List[str] = []",
            "",
            "# Additional time grains to be supported using similar definitions as in",
            "# superset/db_engine_specs/base.py.",
            "# For example: To add a new 2 second time grain:",
            "# TIME_GRAIN_ADDONS = {'PT2S': '2 second'}",
            "TIME_GRAIN_ADDONS: Dict[str, str] = {}",
            "",
            "# Implementation of additional time grains per engine.",
            "# The column to be truncated is denoted `{col}` in the expression.",
            "# For example: To implement 2 second time grain on clickhouse engine:",
            "# TIME_GRAIN_ADDON_EXPRESSIONS = {",
            "#     'clickhouse': {",
            "#         'PT2S': 'toDateTime(intDiv(toUInt32(toDateTime({col})), 2)*2)'",
            "#     }",
            "# }",
            "TIME_GRAIN_ADDON_EXPRESSIONS: Dict[str, Dict[str, str]] = {}",
            "",
            "# ---------------------------------------------------",
            "# List of viz_types not allowed in your environment",
            "# For example: Disable pivot table and treemap:",
            "#  VIZ_TYPE_DENYLIST = ['pivot_table', 'treemap']",
            "# ---------------------------------------------------",
            "",
            "VIZ_TYPE_DENYLIST: List[str] = []",
            "",
            "# --------------------------------------------------",
            "# Modules, datasources and middleware to be registered",
            "# --------------------------------------------------",
            "DEFAULT_MODULE_DS_MAP = OrderedDict(",
            "    [",
            "        (\"superset.connectors.sqla.models\", [\"SqlaTable\"]),",
            "    ]",
            ")",
            "ADDITIONAL_MODULE_DS_MAP: Dict[str, List[str]] = {}",
            "ADDITIONAL_MIDDLEWARE: List[Callable[..., Any]] = []",
            "",
            "# 1) https://docs.python-guide.org/writing/logging/",
            "# 2) https://docs.python.org/2/library/logging.config.html",
            "",
            "# Default configurator will consume the LOG_* settings below",
            "LOGGING_CONFIGURATOR = DefaultLoggingConfigurator()",
            "",
            "# Console Log Settings",
            "",
            "LOG_FORMAT = \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\"",
            "LOG_LEVEL = \"DEBUG\"",
            "",
            "# ---------------------------------------------------",
            "# Enable Time Rotate Log Handler",
            "# ---------------------------------------------------",
            "# LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL",
            "",
            "ENABLE_TIME_ROTATE = False",
            "TIME_ROTATE_LOG_LEVEL = \"DEBUG\"",
            "FILENAME = os.path.join(DATA_DIR, \"superset.log\")",
            "ROLLOVER = \"midnight\"",
            "INTERVAL = 1",
            "BACKUP_COUNT = 30",
            "",
            "# Custom logger for auditing queries. This can be used to send ran queries to a",
            "# structured immutable store for auditing purposes. The function is called for",
            "# every query ran, in both SQL Lab and charts/dashboards.",
            "# def QUERY_LOGGER(",
            "#     database,",
            "#     query,",
            "#     schema=None,",
            "#     user=None,  # TODO(john-bodley): Deprecate in 3.0.",
            "#     client=None,",
            "#     security_manager=None,",
            "#     log_params=None,",
            "# ):",
            "#     pass",
            "QUERY_LOGGER = None",
            "",
            "# Set this API key to enable Mapbox visualizations",
            "MAPBOX_API_KEY = os.environ.get(\"MAPBOX_API_KEY\", \"\")",
            "",
            "# Maximum number of rows returned for any analytical database query",
            "SQL_MAX_ROW = 100000",
            "",
            "# Maximum number of rows displayed in SQL Lab UI",
            "# Is set to avoid out of memory/localstorage issues in browsers. Does not affect",
            "# exported CSVs",
            "DISPLAY_MAX_ROW = 10000",
            "",
            "# Default row limit for SQL Lab queries. Is overridden by setting a new limit in",
            "# the SQL Lab UI",
            "DEFAULT_SQLLAB_LIMIT = 1000",
            "",
            "# Adds a warning message on sqllab save query and schedule query modals.",
            "SQLLAB_SAVE_WARNING_MESSAGE = None",
            "SQLLAB_SCHEDULE_WARNING_MESSAGE = None",
            "",
            "# Force refresh while auto-refresh in dashboard",
            "DASHBOARD_AUTO_REFRESH_MODE: Literal[\"fetch\", \"force\"] = \"force\"",
            "# Dashboard auto refresh intervals",
            "DASHBOARD_AUTO_REFRESH_INTERVALS = [",
            "    [0, \"Don't refresh\"],",
            "    [10, \"10 seconds\"],",
            "    [30, \"30 seconds\"],",
            "    [60, \"1 minute\"],",
            "    [300, \"5 minutes\"],",
            "    [1800, \"30 minutes\"],",
            "    [3600, \"1 hour\"],",
            "    [21600, \"6 hours\"],",
            "    [43200, \"12 hours\"],",
            "    [86400, \"24 hours\"],",
            "]",
            "",
            "# Default celery config is to use SQLA as a broker, in a production setting",
            "# you'll want to use a proper broker as specified here:",
            "# http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html",
            "",
            "",
            "class CeleryConfig:  # pylint: disable=too-few-public-methods",
            "    broker_url = \"sqla+sqlite:///celerydb.sqlite\"",
            "    imports = (\"superset.sql_lab\",)",
            "    result_backend = \"db+sqlite:///celery_results.sqlite\"",
            "    worker_log_level = \"DEBUG\"",
            "    worker_prefetch_multiplier = 1",
            "    task_acks_late = False",
            "    task_annotations = {",
            "        \"sql_lab.get_sql_results\": {\"rate_limit\": \"100/s\"},",
            "        \"email_reports.send\": {",
            "            \"rate_limit\": \"1/s\",",
            "            \"time_limit\": int(timedelta(seconds=120).total_seconds()),",
            "            \"soft_time_limit\": int(timedelta(seconds=150).total_seconds()),",
            "            \"ignore_result\": True,",
            "        },",
            "    }",
            "    beat_schedule = {",
            "        \"email_reports.schedule_hourly\": {",
            "            \"task\": \"email_reports.schedule_hourly\",",
            "            \"schedule\": crontab(minute=1, hour=\"*\"),",
            "        },",
            "        \"reports.scheduler\": {",
            "            \"task\": \"reports.scheduler\",",
            "            \"schedule\": crontab(minute=\"*\", hour=\"*\"),",
            "        },",
            "        \"reports.prune_log\": {",
            "            \"task\": \"reports.prune_log\",",
            "            \"schedule\": crontab(minute=0, hour=0),",
            "        },",
            "    }",
            "",
            "",
            "CELERY_CONFIG = CeleryConfig  # pylint: disable=invalid-name",
            "",
            "# Set celery config to None to disable all the above configuration",
            "# CELERY_CONFIG = None",
            "",
            "# Additional static HTTP headers to be served by your Superset server. Note",
            "# Flask-Talisman applies the relevant security HTTP headers.",
            "#",
            "# DEFAULT_HTTP_HEADERS: sets default values for HTTP headers. These may be overridden",
            "# within the app",
            "# OVERRIDE_HTTP_HEADERS: sets override values for HTTP headers. These values will",
            "# override anything set within the app",
            "DEFAULT_HTTP_HEADERS: Dict[str, Any] = {}",
            "OVERRIDE_HTTP_HEADERS: Dict[str, Any] = {}",
            "HTTP_HEADERS: Dict[str, Any] = {}",
            "",
            "# The db id here results in selecting this one as a default in SQL Lab",
            "DEFAULT_DB_ID = None",
            "",
            "# Timeout duration for SQL Lab synchronous queries",
            "SQLLAB_TIMEOUT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Timeout duration for SQL Lab query validation",
            "SQLLAB_VALIDATION_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# SQLLAB_DEFAULT_DBID",
            "SQLLAB_DEFAULT_DBID = None",
            "",
            "# The MAX duration a query can run for before being killed by celery.",
            "SQLLAB_ASYNC_TIME_LIMIT_SEC = int(timedelta(hours=6).total_seconds())",
            "",
            "# Some databases support running EXPLAIN queries that allow users to estimate",
            "# query costs before they run. These EXPLAIN queries should have a small",
            "# timeout.",
            "SQLLAB_QUERY_COST_ESTIMATE_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "# The feature is off by default, and currently only supported in Presto and Postgres.",
            "# It also need to be enabled on a per-database basis, by adding the key/value pair",
            "# `cost_estimate_enabled: true` to the database `extra` attribute.",
            "ESTIMATE_QUERY_COST = False",
            "# The cost returned by the databases is a relative value; in order to map the cost to",
            "# a tangible value you need to define a custom formatter that takes into consideration",
            "# your specific infrastructure. For example, you could analyze queries a posteriori by",
            "# running EXPLAIN on them, and compute a histogram of relative costs to present the",
            "# cost as a percentile:",
            "#",
            "# def postgres_query_cost_formatter(",
            "#     result: List[Dict[str, Any]]",
            "# ) -> List[Dict[str, str]]:",
            "#     # 25, 50, 75% percentiles",
            "#     percentile_costs = [100.0, 1000.0, 10000.0]",
            "#",
            "#     out = []",
            "#     for row in result:",
            "#         relative_cost = row[\"Total cost\"]",
            "#         percentile = bisect.bisect_left(percentile_costs, relative_cost) + 1",
            "#         out.append({",
            "#             \"Relative cost\": relative_cost,",
            "#             \"Percentile\": str(percentile * 25) + \"%\",",
            "#         })",
            "#",
            "#     return out",
            "#",
            "#  Then on define the formatter on the config:",
            "#",
            "# \"QUERY_COST_FORMATTERS_BY_ENGINE\": {\"postgresql\": postgres_query_cost_formatter},",
            "QUERY_COST_FORMATTERS_BY_ENGINE: Dict[",
            "    str, Callable[[List[Dict[str, Any]]], List[Dict[str, Any]]]",
            "] = {}",
            "",
            "# Flag that controls if limit should be enforced on the CTA (create table as queries).",
            "SQLLAB_CTAS_NO_LIMIT = False",
            "",
            "# This allows you to define custom logic around the \"CREATE TABLE AS\" or CTAS feature",
            "# in SQL Lab that defines where the target schema should be for a given user.",
            "# Database `CTAS Schema` has a precedence over this setting.",
            "# Example below returns a username and CTA queries will write tables into the schema",
            "# name `username`",
            "# SQLLAB_CTAS_SCHEMA_NAME_FUNC = lambda database, user, schema, sql: user.username",
            "# This is move involved example where depending on the database you can leverage data",
            "# available to assign schema for the CTA query:",
            "# def compute_schema_name(database: Database, user: User, schema: str, sql: str) -> str:",
            "#     if database.name == 'mysql_payments_slave':",
            "#         return 'tmp_superset_schema'",
            "#     if database.name == 'presto_gold':",
            "#         return user.username",
            "#     if database.name == 'analytics':",
            "#         if 'analytics' in [r.name for r in user.roles]:",
            "#             return 'analytics_cta'",
            "#         else:",
            "#             return f'tmp_{schema}'",
            "# Function accepts database object, user object, schema name and sql that will be run.",
            "SQLLAB_CTAS_SCHEMA_NAME_FUNC: Optional[",
            "    Callable[[Database, models.User, str, str], str]",
            "] = None",
            "",
            "# If enabled, it can be used to store the results of long-running queries",
            "# in SQL Lab by using the \"Run Async\" button/feature",
            "RESULTS_BACKEND: Optional[BaseCache] = None",
            "",
            "# Use PyArrow and MessagePack for async query results serialization,",
            "# rather than JSON. This feature requires additional testing from the",
            "# community before it is fully adopted, so this config option is provided",
            "# in order to disable should breaking issues be discovered.",
            "RESULTS_BACKEND_USE_MSGPACK = True",
            "",
            "# The S3 bucket where you want to store your external hive tables created",
            "# from CSV files. For example, 'companyname-superset'",
            "CSV_TO_HIVE_UPLOAD_S3_BUCKET = None",
            "",
            "# The directory within the bucket specified above that will",
            "# contain all the external tables",
            "CSV_TO_HIVE_UPLOAD_DIRECTORY = \"EXTERNAL_HIVE_TABLES/\"",
            "",
            "",
            "# Function that creates upload directory dynamically based on the",
            "# database used, user and schema provided.",
            "def CSV_TO_HIVE_UPLOAD_DIRECTORY_FUNC(  # pylint: disable=invalid-name",
            "    database: Database,",
            "    user: models.User,  # pylint: disable=unused-argument",
            "    schema: Optional[str],",
            ") -> str:",
            "    # Note the final empty path enforces a trailing slash.",
            "    return os.path.join(",
            "        CSV_TO_HIVE_UPLOAD_DIRECTORY, str(database.id), schema or \"\", \"\"",
            "    )",
            "",
            "",
            "# The namespace within hive where the tables created from",
            "# uploading CSVs will be stored.",
            "UPLOADED_CSV_HIVE_NAMESPACE: Optional[str] = None",
            "",
            "# Function that computes the allowed schemas for the CSV uploads.",
            "# Allowed schemas will be a union of schemas_allowed_for_file_upload",
            "# db configuration and a result of this function.",
            "",
            "# mypy doesn't catch that if case ensures list content being always str",
            "ALLOWED_USER_CSV_SCHEMA_FUNC: Callable[[Database, models.User], List[str]] = (",
            "    lambda database, user: [UPLOADED_CSV_HIVE_NAMESPACE]",
            "    if UPLOADED_CSV_HIVE_NAMESPACE",
            "    else []",
            ")",
            "",
            "# Values that should be treated as nulls for the csv uploads.",
            "CSV_DEFAULT_NA_NAMES = list(STR_NA_VALUES)",
            "",
            "# A dictionary of items that gets merged into the Jinja context for",
            "# SQL Lab. The existing context gets updated with this dictionary,",
            "# meaning values for existing keys get overwritten by the content of this",
            "# dictionary. Exposing functionality through JINJA_CONTEXT_ADDONS has security",
            "# implications as it opens a window for a user to execute untrusted code.",
            "# It's important to make sure that the objects exposed (as well as objects attached",
            "# to those objets) are harmless. We recommend only exposing simple/pure functions that",
            "# return native types.",
            "JINJA_CONTEXT_ADDONS: Dict[str, Callable[..., Any]] = {}",
            "",
            "# A dictionary of macro template processors (by engine) that gets merged into global",
            "# template processors. The existing template processors get updated with this",
            "# dictionary, which means the existing keys get overwritten by the content of this",
            "# dictionary. The customized addons don't necessarily need to use Jinja templating",
            "# language. This allows you to define custom logic to process templates on a per-engine",
            "# basis. Example value = `{\"presto\": CustomPrestoTemplateProcessor}`",
            "CUSTOM_TEMPLATE_PROCESSORS: Dict[str, Type[BaseTemplateProcessor]] = {}",
            "",
            "# Roles that are controlled by the API / Superset and should not be changes",
            "# by humans.",
            "ROBOT_PERMISSION_ROLES = [\"Public\", \"Gamma\", \"Alpha\", \"Admin\", \"sql_lab\"]",
            "",
            "CONFIG_PATH_ENV_VAR = \"SUPERSET_CONFIG_PATH\"",
            "",
            "# If a callable is specified, it will be called at app startup while passing",
            "# a reference to the Flask app. This can be used to alter the Flask app",
            "# in whatever way.",
            "# example: FLASK_APP_MUTATOR = lambda x: x.before_request = f",
            "FLASK_APP_MUTATOR = None",
            "",
            "# Set this to false if you don't want users to be able to request/grant",
            "# datasource access requests from/to other users.",
            "ENABLE_ACCESS_REQUEST = False",
            "",
            "# smtp server configuration",
            "EMAIL_NOTIFICATIONS = False  # all the emails are sent using dryrun",
            "SMTP_HOST = \"localhost\"",
            "SMTP_STARTTLS = True",
            "SMTP_SSL = False",
            "SMTP_USER = \"superset\"",
            "SMTP_PORT = 25",
            "SMTP_PASSWORD = \"superset\"",
            "SMTP_MAIL_FROM = \"superset@superset.com\"",
            "# If True creates a default SSL context with ssl.Purpose.CLIENT_AUTH using the",
            "# default system root CA certificates.",
            "SMTP_SSL_SERVER_AUTH = False",
            "ENABLE_CHUNK_ENCODING = False",
            "",
            "# Whether to bump the logging level to ERROR on the flask_appbuilder package",
            "# Set to False if/when debugging FAB related issues like",
            "# permission management",
            "SILENCE_FAB = True",
            "",
            "FAB_ADD_SECURITY_VIEWS = True",
            "FAB_ADD_SECURITY_PERMISSION_VIEW = False",
            "FAB_ADD_SECURITY_VIEW_MENU_VIEW = False",
            "FAB_ADD_SECURITY_PERMISSION_VIEWS_VIEW = False",
            "",
            "# The link to a page containing common errors and their resolutions",
            "# It will be appended at the bottom of sql_lab errors.",
            "TROUBLESHOOTING_LINK = \"\"",
            "",
            "# CSRF token timeout, set to None for a token that never expires",
            "WTF_CSRF_TIME_LIMIT = int(timedelta(weeks=1).total_seconds())",
            "",
            "# This link should lead to a page with instructions on how to gain access to a",
            "# Datasource. It will be placed at the bottom of permissions errors.",
            "PERMISSION_INSTRUCTIONS_LINK = \"\"",
            "",
            "# Integrate external Blueprints to the app by passing them to your",
            "# configuration. These blueprints will get integrated in the app",
            "BLUEPRINTS: List[Blueprint] = []",
            "",
            "# Provide a callable that receives a tracking_url and returns another",
            "# URL. This is used to translate internal Hadoop job tracker URL",
            "# into a proxied one",
            "",
            "",
            "# Transform SQL query tracking url for Hive and Presto engines. You may also",
            "# access information about the query itself by adding a second parameter",
            "# to your transformer function, e.g.:",
            "#   TRACKING_URL_TRANSFORMER = (",
            "#       lambda url, query: url if is_fresh(query) else None",
            "#   )",
            "TRACKING_URL_TRANSFORMER = lambda url: url",
            "",
            "",
            "# Interval between consecutive polls when using Hive Engine",
            "HIVE_POLL_INTERVAL = int(timedelta(seconds=5).total_seconds())",
            "",
            "# Interval between consecutive polls when using Presto Engine",
            "# See here: https://github.com/dropbox/PyHive/blob/8eb0aeab8ca300f3024655419b93dad926c1a351/pyhive/presto.py#L93  # pylint: disable=line-too-long,useless-suppression",
            "PRESTO_POLL_INTERVAL = int(timedelta(seconds=1).total_seconds())",
            "",
            "# Allow list of custom authentications for each DB engine.",
            "# Example:",
            "# from your.module import AuthClass",
            "# from another.extra import auth_method",
            "#",
            "# ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {",
            "#     \"trino\": {",
            "#         \"custom_auth\": AuthClass,",
            "#         \"another_auth_method\": auth_method,",
            "#     },",
            "# }",
            "ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {}",
            "",
            "# The id of a template dashboard that should be copied to every new user",
            "DASHBOARD_TEMPLATE_ID = None",
            "",
            "# A callable that allows altering the database connection URL and params",
            "# on the fly, at runtime. This allows for things like impersonation or",
            "# arbitrary logic. For instance you can wire different users to",
            "# use different connection parameters, or pass their email address as the",
            "# username. The function receives the connection uri object, connection",
            "# params, the username, and returns the mutated uri and params objects.",
            "# Example:",
            "#   def DB_CONNECTION_MUTATOR(uri, params, username, security_manager, source):",
            "#       user = security_manager.find_user(username=username)",
            "#       if user and user.email:",
            "#           uri.username = user.email",
            "#       return uri, params",
            "#",
            "# Note that the returned uri and params are passed directly to sqlalchemy's",
            "# as such `create_engine(url, **params)`",
            "DB_CONNECTION_MUTATOR = None",
            "",
            "",
            "# A function that intercepts the SQL to be executed and can alter it.",
            "# The use case is can be around adding some sort of comment header",
            "# with information such as the username and worker node information",
            "#",
            "#    def SQL_QUERY_MUTATOR(",
            "#        sql,",
            "#        user_name=user_name,  # TODO(john-bodley): Deprecate in 3.0.",
            "#        security_manager=security_manager,",
            "#        database=database,",
            "#    ):",
            "#        dttm = datetime.now().isoformat()",
            "#        return f\"-- [SQL LAB] {user_name} {dttm}\\n{sql}\"",
            "# For backward compatibility, you can unpack any of the above arguments in your",
            "# function definition, but keep the **kwargs as the last argument to allow new args",
            "# to be added later without any errors.",
            "def SQL_QUERY_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    sql: str, **kwargs: Any",
            ") -> str:",
            "    return sql",
            "",
            "",
            "# This allows for a user to add header data to any outgoing emails. For example,",
            "# if you need to include metadata in the header or you want to change the specifications",
            "# of the email title, header, or sender.",
            "def EMAIL_HEADER_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    msg: MIMEMultipart, **kwargs: Any",
            ") -> MIMEMultipart:",
            "    return msg",
            "",
            "",
            "# Define a list of usernames to be excluded from all dropdown lists of users",
            "# Owners, filters for created_by, etc.",
            "# The users can also be excluded by overriding the get_exclude_users_from_lists method",
            "# in security manager",
            "EXCLUDE_USERS_FROM_LISTS: Optional[List[str]] = None",
            "",
            "# For database connections, this dictionary will remove engines from the available",
            "# list/dropdown if you do not want these dbs to show as available.",
            "# The available list is generated by driver installed, and some engines have multiple",
            "# drivers.",
            "# e.g., DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {\"databricks\": (\"pyhive\", \"pyodbc\")}",
            "DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {}",
            "",
            "# This auth provider is used by background (offline) tasks that need to access",
            "# protected resources. Can be overridden by end users in order to support",
            "# custom auth mechanisms",
            "MACHINE_AUTH_PROVIDER_CLASS = \"superset.utils.machine_auth.MachineAuthProvider\"",
            "",
            "# ---------------------------------------------------",
            "# Alerts & Reports",
            "# ---------------------------------------------------",
            "# Used for Alerts/Reports (Feature flask ALERT_REPORTS) to set the size for the",
            "# sliding cron window size, should be synced with the celery beat config minus 1 second",
            "ALERT_REPORTS_CRON_WINDOW_SIZE = 59",
            "ALERT_REPORTS_WORKING_TIME_OUT_KILL = True",
            "# Which user to attempt to execute Alerts/Reports as. By default,",
            "# use the user defined in the `THUMBNAIL_SELENIUM_USER` config parameter.",
            "# To first try to execute as the creator in the owners list (if present), then fall",
            "# back to the creator, then the last modifier in the owners list (if present), then the",
            "# last modifier, then an owner (giving priority to the last modifier and then the",
            "# creator if either is contained within the list of owners, otherwise the first owner",
            "# will be used) and finally `THUMBNAIL_SELENIUM_USER`, set as follows:",
            "# ALERT_REPORTS_EXECUTE_AS = [",
            "#     ScheduledTaskExecutor.CREATOR_OWNER,",
            "#     ScheduledTaskExecutor.CREATOR,",
            "#     ScheduledTaskExecutor.MODIFIER_OWNER,",
            "#     ScheduledTaskExecutor.MODIFIER,",
            "#     ScheduledTaskExecutor.OWNER,",
            "#     ScheduledTaskExecutor.SELENIUM,",
            "# ]",
            "ALERT_REPORTS_EXECUTE_AS: List[ExecutorType] = [ExecutorType.SELENIUM]",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_TIME_OUT_LAG = int(timedelta(seconds=10).total_seconds())",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG = int(timedelta(seconds=1).total_seconds())",
            "# If set to true no notification is sent, the worker will just log a message.",
            "# Useful for debugging",
            "ALERT_REPORTS_NOTIFICATION_DRY_RUN = False",
            "# Max tries to run queries to prevent false errors caused by transient errors",
            "# being returned to users. Set to a value >1 to enable retries.",
            "ALERT_REPORTS_QUERY_EXECUTION_MAX_TRIES = 1",
            "",
            "# A custom prefix to use on all Alerts & Reports emails",
            "EMAIL_REPORTS_SUBJECT_PREFIX = \"[Report] \"",
            "",
            "# Slack API token for the superset reports, either string or callable",
            "SLACK_API_TOKEN: Optional[Union[Callable[[], str], str]] = None",
            "SLACK_PROXY = None",
            "",
            "# The webdriver to use for generating reports. Use one of the following",
            "# firefox",
            "#   Requires: geckodriver and firefox installations",
            "#   Limitations: can be buggy at times",
            "# chrome:",
            "#   Requires: headless chrome",
            "#   Limitations: unable to generate screenshots of elements",
            "WEBDRIVER_TYPE = \"firefox\"",
            "",
            "# Window size - this will impact the rendering of the data",
            "WEBDRIVER_WINDOW = {",
            "    \"dashboard\": (1600, 2000),",
            "    \"slice\": (3000, 1200),",
            "    \"pixel_density\": 1,",
            "}",
            "",
            "# An optional override to the default auth hook used to provide auth to the",
            "# offline webdriver",
            "WEBDRIVER_AUTH_FUNC = None",
            "",
            "# Any config options to be passed as-is to the webdriver",
            "WEBDRIVER_CONFIGURATION: Dict[Any, Any] = {\"service_log_path\": \"/dev/null\"}",
            "",
            "# Additional args to be passed as arguments to the config object",
            "# Note: these options are Chrome-specific. For FF, these should",
            "# only include the \"--headless\" arg",
            "WEBDRIVER_OPTION_ARGS = [\"--headless\", \"--marionette\"]",
            "",
            "# The base URL to query for accessing the user interface",
            "WEBDRIVER_BASEURL = \"http://0.0.0.0:8080/\"",
            "# The base URL for the email report hyperlinks.",
            "WEBDRIVER_BASEURL_USER_FRIENDLY = WEBDRIVER_BASEURL",
            "# Time selenium will wait for the page to load and render for the email report.",
            "EMAIL_PAGE_RENDER_WAIT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Send user to a link where they can report bugs",
            "BUG_REPORT_URL = None",
            "",
            "# Send user to a link where they can read more about Superset",
            "DOCUMENTATION_URL = None",
            "DOCUMENTATION_TEXT = \"Documentation\"",
            "DOCUMENTATION_ICON = None  # Recommended size: 16x16",
            "",
            "# What is the Last N days relative in the time selector to:",
            "# 'today' means it is midnight (00:00:00) in the local timezone",
            "# 'now' means it is relative to the query issue time",
            "# If both start and end time is set to now, this will make the time",
            "# filter a moving window. By only setting the end time to now,",
            "# start time will be set to midnight, while end will be relative to",
            "# the query issue time.",
            "DEFAULT_RELATIVE_START_TIME = \"today\"",
            "DEFAULT_RELATIVE_END_TIME = \"today\"",
            "",
            "# Configure which SQL validator to use for each engine",
            "SQL_VALIDATORS_BY_ENGINE = {",
            "    \"presto\": \"PrestoDBSQLValidator\",",
            "    \"postgresql\": \"PostgreSQLValidator\",",
            "}",
            "",
            "# A list of preferred databases, in order. These databases will be",
            "# displayed prominently in the \"Add Database\" dialog. You should",
            "# use the \"engine_name\" attribute of the corresponding DB engine spec",
            "# in `superset/db_engine_specs/`.",
            "PREFERRED_DATABASES: List[str] = [",
            "    \"PostgreSQL\",",
            "    \"Presto\",",
            "    \"MySQL\",",
            "    \"SQLite\",",
            "    # etc.",
            "]",
            "# When adding a new database we try to connect to it. Depending on which parameters are",
            "# incorrect this could take a couple minutes, until the SQLAlchemy driver pinging the",
            "# database times out. Instead of relying on the driver timeout we can specify a shorter",
            "# one here.",
            "TEST_DATABASE_CONNECTION_TIMEOUT = timedelta(seconds=30)",
            "",
            "# Enable/disable CSP warning",
            "CONTENT_SECURITY_POLICY_WARNING = True",
            "",
            "# Do you want Talisman enabled?",
            "TALISMAN_ENABLED = False",
            "# If you want Talisman, how do you want it configured??",
            "TALISMAN_CONFIG = {",
            "    \"content_security_policy\": None,",
            "    \"force_https\": True,",
            "    \"force_https_permanent\": False,",
            "}",
            "",
            "# It is possible to customize which tables and roles are featured in the RLS",
            "# dropdown. When set, this dict is assigned to `add_form_query_rel_fields` and",
            "# `edit_form_query_rel_fields` on `RowLevelSecurityFiltersModelView`. Example:",
            "#",
            "# from flask_appbuilder.models.sqla import filters",
            "# RLS_FORM_QUERY_REL_FIELDS = {",
            "#     \"roles\": [[\"name\", filters.FilterStartsWith, \"RlsRole\"]]",
            "#     \"tables\": [[\"table_name\", filters.FilterContains, \"rls\"]]",
            "# }",
            "RLS_FORM_QUERY_REL_FIELDS: Optional[Dict[str, List[List[Any]]]] = None",
            "",
            "#",
            "# Flask session cookie options",
            "#",
            "# See https://flask.palletsprojects.com/en/1.1.x/security/#set-cookie-options",
            "# for details",
            "#",
            "SESSION_COOKIE_HTTPONLY = True  # Prevent cookie from being read by frontend JS?",
            "SESSION_COOKIE_SECURE = False  # Prevent cookie from being transmitted over non-tls?",
            "SESSION_COOKIE_SAMESITE = \"Lax\"  # One of [None, 'None', 'Lax', 'Strict']",
            "",
            "# Cache static resources.",
            "SEND_FILE_MAX_AGE_DEFAULT = int(timedelta(days=365).total_seconds())",
            "",
            "# URI to database storing the example data, points to",
            "# SQLALCHEMY_DATABASE_URI by default if set to `None`",
            "SQLALCHEMY_EXAMPLES_URI = None",
            "",
            "# Optional prefix to be added to all static asset paths when rendering the UI.",
            "# This is useful for hosting assets in an external CDN, for example",
            "STATIC_ASSETS_PREFIX = \"\"",
            "",
            "# Some sqlalchemy connection strings can open Superset to security risks.",
            "# Typically these should not be allowed.",
            "PREVENT_UNSAFE_DB_CONNECTIONS = True",
            "",
            "# Prevents unsafe default endpoints to be registered on datasets.",
            "PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET = True",
            "",
            "# Path used to store SSL certificates that are generated when using custom certs.",
            "# Defaults to temporary directory.",
            "# Example: SSL_CERT_PATH = \"/certs\"",
            "SSL_CERT_PATH: Optional[str] = None",
            "",
            "# SQLA table mutator, every time we fetch the metadata for a certain table",
            "# (superset.connectors.sqla.models.SqlaTable), we call this hook",
            "# to allow mutating the object with this callback.",
            "# This can be used to set any properties of the object based on naming",
            "# conventions and such. You can find examples in the tests.",
            "",
            "SQLA_TABLE_MUTATOR = lambda table: table",
            "",
            "",
            "# Global async query config options.",
            "# Requires GLOBAL_ASYNC_QUERIES feature flag to be enabled.",
            "GLOBAL_ASYNC_QUERIES_REDIS_CONFIG = {",
            "    \"port\": 6379,",
            "    \"host\": \"127.0.0.1\",",
            "    \"password\": \"\",",
            "    \"db\": 0,",
            "    \"ssl\": False,",
            "}",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_PREFIX = \"async-events-\"",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT = 1000",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT_FIREHOSE = 1000000",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_NAME = \"async-token\"",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SECURE = False",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_DOMAIN = None",
            "GLOBAL_ASYNC_QUERIES_JWT_SECRET = \"test-secret-change-me\"",
            "GLOBAL_ASYNC_QUERIES_TRANSPORT = \"polling\"",
            "GLOBAL_ASYNC_QUERIES_POLLING_DELAY = int(",
            "    timedelta(milliseconds=500).total_seconds() * 1000",
            ")",
            "GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL = \"ws://127.0.0.1:8080/\"",
            "",
            "# Embedded config options",
            "GUEST_ROLE_NAME = \"Public\"",
            "GUEST_TOKEN_JWT_SECRET = \"test-guest-secret-change-me\"",
            "GUEST_TOKEN_JWT_ALGO = \"HS256\"",
            "GUEST_TOKEN_HEADER_NAME = \"X-GuestToken\"",
            "GUEST_TOKEN_JWT_EXP_SECONDS = 300  # 5 minutes",
            "# Guest token audience for the embedded superset, either string or callable",
            "GUEST_TOKEN_JWT_AUDIENCE: Optional[Union[Callable[[], str], str]] = None",
            "",
            "# A SQL dataset health check. Note if enabled it is strongly advised that the callable",
            "# be memoized to aid with performance, i.e.,",
            "#",
            "#    @cache_manager.cache.memoize(timeout=0)",
            "#    def DATASET_HEALTH_CHECK(datasource: SqlaTable) -> Optional[str]:",
            "#        if (",
            "#            datasource.sql and",
            "#            len(sql_parse.ParsedQuery(datasource.sql, strip_comments=True).tables) == 1",
            "#        ):",
            "#            return (",
            "#                \"This virtual dataset queries only one table and therefore could be \"",
            "#                \"replaced by querying the table directly.\"",
            "#            )",
            "#",
            "#        return None",
            "#",
            "# Within the FLASK_APP_MUTATOR callable, i.e., once the application and thus cache have",
            "# been initialized it is also necessary to add the following logic to blow the cache for",
            "# all datasources if the callback function changed.",
            "#",
            "#    def FLASK_APP_MUTATOR(app: Flask) -> None:",
            "#        name = \"DATASET_HEALTH_CHECK\"",
            "#        func = app.config[name]",
            "#        code = func.uncached.__code__.co_code",
            "#",
            "#        if cache_manager.cache.get(name) != code:",
            "#            cache_manager.cache.delete_memoized(func)",
            "#            cache_manager.cache.set(name, code, timeout=0)",
            "#",
            "DATASET_HEALTH_CHECK: Optional[Callable[[\"SqlaTable\"], str]] = None",
            "",
            "# Do not show user info or profile in the menu",
            "MENU_HIDE_USER_INFO = False",
            "",
            "# Set to False to only allow viewing own recent activity",
            "# or to disallow users from viewing other users profile page",
            "ENABLE_BROAD_ACTIVITY_ACCESS = True",
            "",
            "# the advanced data type key should correspond to that set in the column metadata",
            "ADVANCED_DATA_TYPES: Dict[str, AdvancedDataType] = {",
            "    \"internet_address\": internet_address,",
            "    \"port\": internet_port,",
            "}",
            "",
            "# By default, the Welcome page features example charts and dashboards. This can be",
            "# changed to show all charts/dashboards the user has access to, or a custom view",
            "# by providing the title and a FAB filter:",
            "# WELCOME_PAGE_LAST_TAB = (",
            "#     \"Xyz\",",
            "#     [{\"col\": 'created_by', \"opr\": 'rel_o_m', \"value\": 10}],",
            "# )",
            "WELCOME_PAGE_LAST_TAB: Union[",
            "    Literal[\"examples\", \"all\"], Tuple[str, List[Dict[str, Any]]]",
            "] = \"examples\"",
            "",
            "# Configuration for environment tag shown on the navbar. Setting 'text' to '' will hide the tag.",
            "# 'color' can either be a hex color code, or a dot-indexed theme color (e.g. error.base)",
            "ENVIRONMENT_TAG_CONFIG = {",
            "    \"variable\": \"FLASK_ENV\",",
            "    \"values\": {",
            "        \"development\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"Development\",",
            "        },",
            "        \"production\": {",
            "            \"color\": \"\",",
            "            \"text\": \"\",",
            "        },",
            "    },",
            "}",
            "",
            "# -------------------------------------------------------------------",
            "# *                WARNING:  STOP EDITING  HERE                    *",
            "# -------------------------------------------------------------------",
            "# Don't add config values below this line since local configs won't be",
            "# able to override them.",
            "if CONFIG_PATH_ENV_VAR in os.environ:",
            "    # Explicitly import config module that is not necessarily in pythonpath; useful",
            "    # for case where app is being executed via pex.",
            "    cfg_path = os.environ[CONFIG_PATH_ENV_VAR]",
            "    try:",
            "        module = sys.modules[__name__]",
            "        override_conf = imp.load_source(\"superset_config\", cfg_path)",
            "        for key in dir(override_conf):",
            "            if key.isupper():",
            "                setattr(module, key, getattr(override_conf, key))",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{cfg_path}]\")",
            "    except Exception:",
            "        logger.exception(",
            "            \"Failed to import config for %s=%s\", CONFIG_PATH_ENV_VAR, cfg_path",
            "        )",
            "        raise",
            "elif importlib.util.find_spec(\"superset_config\") and not is_test():",
            "    try:",
            "        # pylint: disable=import-error,wildcard-import,unused-wildcard-import",
            "        import superset_config",
            "        from superset_config import *  # type:ignore",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{superset_config.__file__}]\")",
            "    except Exception:",
            "        logger.exception(\"Found but failed to import local superset_config\")",
            "        raise"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"The main config file for Superset",
            "",
            "All configuration in this file can be overridden by providing a superset_config",
            "in your PYTHONPATH as there is a ``from superset_config import *``",
            "at the end of this file.",
            "\"\"\"",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import imp  # pylint: disable=deprecated-module",
            "import importlib.util",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import sys",
            "from collections import OrderedDict",
            "from datetime import timedelta",
            "from email.mime.multipart import MIMEMultipart",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    List,",
            "    Literal,",
            "    Optional,",
            "    Set,",
            "    Tuple,",
            "    Type,",
            "    TYPE_CHECKING,",
            "    Union,",
            ")",
            "",
            "import pkg_resources",
            "from cachelib.base import BaseCache",
            "from celery.schedules import crontab",
            "from dateutil import tz",
            "from flask import Blueprint",
            "from flask_appbuilder.security.manager import AUTH_DB",
            "from pandas._libs.parsers import STR_NA_VALUES  # pylint: disable=no-name-in-module",
            "",
            "from superset.advanced_data_type.plugins.internet_address import internet_address",
            "from superset.advanced_data_type.plugins.internet_port import internet_port",
            "from superset.advanced_data_type.types import AdvancedDataType",
            "from superset.constants import CHANGE_ME_SECRET_KEY",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.stats_logger import DummyStatsLogger",
            "from superset.superset_typing import CacheConfig",
            "from superset.tasks.types import ExecutorType",
            "from superset.utils.core import is_test, NO_TIME_RANGE, parse_boolean_string",
            "from superset.utils.encrypt import SQLAlchemyUtilsAdapter",
            "from superset.utils.log import DBEventLogger",
            "from superset.utils.logging_configurator import DefaultLoggingConfigurator",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "if TYPE_CHECKING:",
            "    from flask_appbuilder.security.sqla import models",
            "",
            "    from superset.connectors.sqla.models import SqlaTable",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.slice import Slice",
            "",
            "# Realtime stats logger, a StatsD implementation exists",
            "STATS_LOGGER = DummyStatsLogger()",
            "EVENT_LOGGER = DBEventLogger()",
            "",
            "SUPERSET_LOG_VIEW = True",
            "",
            "BASE_DIR = pkg_resources.resource_filename(\"superset\", \"\")",
            "if \"SUPERSET_HOME\" in os.environ:",
            "    DATA_DIR = os.environ[\"SUPERSET_HOME\"]",
            "else:",
            "    DATA_DIR = os.path.expanduser(\"~/.superset\")",
            "",
            "# ---------------------------------------------------------",
            "# Superset specific config",
            "# ---------------------------------------------------------",
            "VERSION_INFO_FILE = pkg_resources.resource_filename(",
            "    \"superset\", \"static/version_info.json\"",
            ")",
            "PACKAGE_JSON_FILE = pkg_resources.resource_filename(",
            "    \"superset\", \"static/assets/package.json\"",
            ")",
            "",
            "# Multiple favicons can be specified here. The \"href\" property",
            "# is mandatory, but \"sizes,\" \"type,\" and \"rel\" are optional.",
            "# For example:",
            "# {",
            "#     \"href\":path/to/image.png\",",
            "#     \"sizes\": \"16x16\",",
            "#     \"type\": \"image/png\"",
            "#     \"rel\": \"icon\"",
            "# },",
            "FAVICONS = [{\"href\": \"/static/assets/images/favicon.png\"}]",
            "",
            "",
            "def _try_json_readversion(filepath: str) -> Optional[str]:",
            "    try:",
            "        with open(filepath, \"r\") as f:",
            "            return json.load(f).get(\"version\")",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "def _try_json_readsha(filepath: str, length: int) -> Optional[str]:",
            "    try:",
            "        with open(filepath, \"r\") as f:",
            "            return json.load(f).get(\"GIT_SHA\")[:length]",
            "    except Exception:  # pylint: disable=broad-except",
            "        return None",
            "",
            "",
            "#",
            "# If True, we will skip the call to load the logger config found in alembic.init",
            "#",
            "ALEMBIC_SKIP_LOG_CONFIG = False",
            "",
            "# Depending on the context in which this config is loaded, the",
            "# version_info.json file may or may not be available, as it is",
            "# generated on install via setup.py. In the event that we're",
            "# actually running Superset, we will have already installed,",
            "# therefore it WILL exist. When unit tests are running, however,",
            "# it WILL NOT exist, so we fall back to reading package.json",
            "VERSION_STRING = _try_json_readversion(VERSION_INFO_FILE) or _try_json_readversion(",
            "    PACKAGE_JSON_FILE",
            ")",
            "",
            "VERSION_SHA_LENGTH = 8",
            "VERSION_SHA = _try_json_readsha(VERSION_INFO_FILE, VERSION_SHA_LENGTH)",
            "",
            "# Build number is shown in the About section if available. This",
            "# can be replaced at build time to expose build information.",
            "BUILD_NUMBER = None",
            "",
            "# default viz used in chart explorer & SQL Lab explore",
            "DEFAULT_VIZ_TYPE = \"table\"",
            "",
            "# default row limit when requesting chart data",
            "ROW_LIMIT = 50000",
            "# default row limit when requesting samples from datasource in explore view",
            "SAMPLES_ROW_LIMIT = 1000",
            "# max rows retrieved by filter select auto complete",
            "FILTER_SELECT_ROW_LIMIT = 10000",
            "# default time filter in explore",
            "# values may be \"Last day\", \"Last week\", \"<ISO date> : now\", etc.",
            "DEFAULT_TIME_FILTER = NO_TIME_RANGE",
            "",
            "SUPERSET_WEBSERVER_PROTOCOL = \"http\"",
            "SUPERSET_WEBSERVER_ADDRESS = \"0.0.0.0\"",
            "SUPERSET_WEBSERVER_PORT = 8088",
            "",
            "# This is an important setting, and should be lower than your",
            "# [load balancer / proxy / envoy / kong / ...] timeout settings.",
            "# You should also make sure to configure your WSGI server",
            "# (gunicorn, nginx, apache, ...) timeout setting to be <= to this setting",
            "SUPERSET_WEBSERVER_TIMEOUT = int(timedelta(minutes=1).total_seconds())",
            "",
            "# this 2 settings are used by dashboard period force refresh feature",
            "# When user choose auto force refresh frequency",
            "# < SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT",
            "# they will see warning message in the Refresh Interval Modal.",
            "# please check PR #9886",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_LIMIT = 0",
            "SUPERSET_DASHBOARD_PERIODICAL_REFRESH_WARNING_MESSAGE = None",
            "",
            "SUPERSET_DASHBOARD_POSITION_DATA_LIMIT = 65535",
            "CUSTOM_SECURITY_MANAGER = None",
            "SQLALCHEMY_TRACK_MODIFICATIONS = False",
            "# ---------------------------------------------------------",
            "",
            "# Your App secret key. Make sure you override it on superset_config.py.",
            "# Use a strong complex alphanumeric string and use a tool to help you generate",
            "# a sufficiently random sequence, ex: openssl rand -base64 42\"",
            "SECRET_KEY = CHANGE_ME_SECRET_KEY",
            "",
            "# The SQLAlchemy connection string.",
            "SQLALCHEMY_DATABASE_URI = \"sqlite:///\" + os.path.join(DATA_DIR, \"superset.db\")",
            "# SQLALCHEMY_DATABASE_URI = 'mysql://myapp@localhost/myapp'",
            "# SQLALCHEMY_DATABASE_URI = 'postgresql://root:password@localhost/myapp'",
            "",
            "# In order to hook up a custom password store for all SQLACHEMY connections",
            "# implement a function that takes a single argument of type 'sqla.engine.url',",
            "# returns a password and set SQLALCHEMY_CUSTOM_PASSWORD_STORE.",
            "#",
            "# e.g.:",
            "# def lookup_password(url):",
            "#     return 'secret'",
            "# SQLALCHEMY_CUSTOM_PASSWORD_STORE = lookup_password",
            "SQLALCHEMY_CUSTOM_PASSWORD_STORE = None",
            "",
            "#",
            "# The EncryptedFieldTypeAdapter is used whenever we're building SqlAlchemy models",
            "# which include sensitive fields that should be app-encrypted BEFORE sending",
            "# to the DB.",
            "#",
            "# Note: the default impl leverages SqlAlchemyUtils' EncryptedType, which defaults",
            "#  to AesEngine that uses AES-128 under the covers using the app's SECRET_KEY",
            "#  as key material. Do note that AesEngine allows for queryability over the",
            "#  encrypted fields.",
            "#",
            "#  To change the default engine you need to define your own adapter:",
            "#",
            "# e.g.:",
            "#",
            "# class AesGcmEncryptedAdapter(",
            "#     AbstractEncryptedFieldAdapter",
            "# ):",
            "#     def create(",
            "#         self,",
            "#         app_config: Optional[Dict[str, Any]],",
            "#         *args: List[Any],",
            "#         **kwargs: Optional[Dict[str, Any]],",
            "#     ) -> TypeDecorator:",
            "#         if app_config:",
            "#             return EncryptedType(",
            "#                 *args, app_config[\"SECRET_KEY\"], engine=AesGcmEngine, **kwargs",
            "#             )",
            "#         raise Exception(\"Missing app_config kwarg\")",
            "#",
            "#",
            "#  SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = AesGcmEncryptedAdapter",
            "SQLALCHEMY_ENCRYPTED_FIELD_TYPE_ADAPTER = (  # pylint: disable=invalid-name",
            "    SQLAlchemyUtilsAdapter",
            ")",
            "# The limit of queries fetched for query search",
            "QUERY_SEARCH_LIMIT = 1000",
            "",
            "# Flask-WTF flag for CSRF",
            "WTF_CSRF_ENABLED = True",
            "",
            "# Add endpoints that need to be exempt from CSRF protection",
            "WTF_CSRF_EXEMPT_LIST = [",
            "    \"superset.views.core.log\",",
            "    \"superset.views.core.explore_json\",",
            "    \"superset.charts.data.api.data\",",
            "]",
            "",
            "# Whether to run the web server in debug mode or not",
            "DEBUG = os.environ.get(\"FLASK_ENV\") == \"development\"",
            "FLASK_USE_RELOAD = True",
            "",
            "# Enable profiling of Python calls. Turn this on and append ``?_instrument=1``",
            "# to the page to see the call stack.",
            "PROFILING = False",
            "",
            "# Superset allows server-side python stacktraces to be surfaced to the",
            "# user when this feature is on. This may has security implications",
            "# and it's more secure to turn it off in production settings.",
            "SHOW_STACKTRACE = True",
            "",
            "# Use all X-Forwarded headers when ENABLE_PROXY_FIX is True.",
            "# When proxying to a different port, set \"x_port\" to 0 to avoid downstream issues.",
            "ENABLE_PROXY_FIX = False",
            "PROXY_FIX_CONFIG = {\"x_for\": 1, \"x_proto\": 1, \"x_host\": 1, \"x_port\": 1, \"x_prefix\": 1}",
            "",
            "# Configuration for scheduling queries from SQL Lab.",
            "SCHEDULED_QUERIES: Dict[str, Any] = {}",
            "",
            "# ------------------------------",
            "# GLOBALS FOR APP Builder",
            "# ------------------------------",
            "# Uncomment to setup Your App name",
            "APP_NAME = \"Superset\"",
            "",
            "# Specify the App icon",
            "APP_ICON = \"/static/assets/images/superset-logo-horiz.png\"",
            "",
            "# Specify where clicking the logo would take the user",
            "# e.g. setting it to '/' would take the user to '/superset/welcome/'",
            "LOGO_TARGET_PATH = None",
            "",
            "# Specify tooltip that should appear when hovering over the App Icon/Logo",
            "LOGO_TOOLTIP = \"\"",
            "",
            "# Specify any text that should appear to the right of the logo",
            "LOGO_RIGHT_TEXT: Union[Callable[[], str], str] = \"\"",
            "",
            "# Enables SWAGGER UI for superset openapi spec",
            "# ex: http://localhost:8080/swagger/v1",
            "FAB_API_SWAGGER_UI = True",
            "",
            "# Druid query timezone",
            "# tz.tzutc() : Using utc timezone",
            "# tz.tzlocal() : Using local timezone",
            "# tz.gettz('Asia/Shanghai') : Using the time zone with specific name",
            "# [TimeZone List]",
            "# See: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones",
            "# other tz can be overridden by providing a local_config",
            "DRUID_TZ = tz.tzutc()",
            "DRUID_ANALYSIS_TYPES = [\"cardinality\"]",
            "",
            "",
            "# ----------------------------------------------------",
            "# AUTHENTICATION CONFIG",
            "# ----------------------------------------------------",
            "# The authentication type",
            "# AUTH_OID : Is for OpenID",
            "# AUTH_DB : Is for database (username/password)",
            "# AUTH_LDAP : Is for LDAP",
            "# AUTH_REMOTE_USER : Is for using REMOTE_USER from web server",
            "AUTH_TYPE = AUTH_DB",
            "",
            "# Uncomment to setup Full admin role name",
            "# AUTH_ROLE_ADMIN = 'Admin'",
            "",
            "# Uncomment to setup Public role name, no authentication needed",
            "# AUTH_ROLE_PUBLIC = 'Public'",
            "",
            "# Will allow user self registration",
            "# AUTH_USER_REGISTRATION = True",
            "",
            "# The default user self registration role",
            "# AUTH_USER_REGISTRATION_ROLE = \"Public\"",
            "",
            "# When using LDAP Auth, setup the LDAP server",
            "# AUTH_LDAP_SERVER = \"ldap://ldapserver.new\"",
            "",
            "# Uncomment to setup OpenID providers example for OpenID authentication",
            "# OPENID_PROVIDERS = [",
            "#    { 'name': 'Yahoo', 'url': 'https://open.login.yahoo.com/' },",
            "#    { 'name': 'Flickr', 'url': 'https://www.flickr.com/<username>' },",
            "",
            "# ---------------------------------------------------",
            "# Roles config",
            "# ---------------------------------------------------",
            "# Grant public role the same set of permissions as for a selected builtin role.",
            "# This is useful if one wants to enable anonymous users to view",
            "# dashboards. Explicit grant on specific datasets is still required.",
            "PUBLIC_ROLE_LIKE: Optional[str] = None",
            "",
            "# ---------------------------------------------------",
            "# Babel config for translations",
            "# ---------------------------------------------------",
            "# Setup default language",
            "BABEL_DEFAULT_LOCALE = \"en\"",
            "# Your application default translation path",
            "BABEL_DEFAULT_FOLDER = \"superset/translations\"",
            "# The allowed translation for you app",
            "LANGUAGES = {",
            "    \"en\": {\"flag\": \"us\", \"name\": \"English\"},",
            "    \"es\": {\"flag\": \"es\", \"name\": \"Spanish\"},",
            "    \"it\": {\"flag\": \"it\", \"name\": \"Italian\"},",
            "    \"fr\": {\"flag\": \"fr\", \"name\": \"French\"},",
            "    \"zh\": {\"flag\": \"cn\", \"name\": \"Chinese\"},",
            "    \"ja\": {\"flag\": \"jp\", \"name\": \"Japanese\"},",
            "    \"de\": {\"flag\": \"de\", \"name\": \"German\"},",
            "    \"pt\": {\"flag\": \"pt\", \"name\": \"Portuguese\"},",
            "    \"pt_BR\": {\"flag\": \"br\", \"name\": \"Brazilian Portuguese\"},",
            "    \"ru\": {\"flag\": \"ru\", \"name\": \"Russian\"},",
            "    \"ko\": {\"flag\": \"kr\", \"name\": \"Korean\"},",
            "    \"sk\": {\"flag\": \"sk\", \"name\": \"Slovak\"},",
            "    \"sl\": {\"flag\": \"si\", \"name\": \"Slovenian\"},",
            "    \"nl\": {\"flag\": \"nl\", \"name\": \"Dutch\"},",
            "}",
            "# Turning off i18n by default as translation in most languages are",
            "# incomplete and not well maintained.",
            "LANGUAGES = {}",
            "",
            "# ---------------------------------------------------",
            "# Feature flags",
            "# ---------------------------------------------------",
            "# Feature flags that are set by default go here. Their values can be",
            "# overwritten by those specified under FEATURE_FLAGS in superset_config.py",
            "# For example, DEFAULT_FEATURE_FLAGS = { 'FOO': True, 'BAR': False } here",
            "# and FEATURE_FLAGS = { 'BAR': True, 'BAZ': True } in superset_config.py",
            "# will result in combined feature flags of { 'FOO': True, 'BAR': True, 'BAZ': True }",
            "DEFAULT_FEATURE_FLAGS: Dict[str, bool] = {",
            "    # allow dashboard to use sub-domains to send chart request",
            "    # you also need ENABLE_CORS and",
            "    # SUPERSET_WEBSERVER_DOMAINS for list of domains",
            "    \"ALLOW_DASHBOARD_DOMAIN_SHARDING\": True,",
            "    # Experimental feature introducing a client (browser) cache",
            "    \"CLIENT_CACHE\": False,",
            "    \"DISABLE_DATASET_SOURCE_EDIT\": False,",
            "    # When using a recent version of Druid that supports JOINs turn this on",
            "    \"DRUID_JOINS\": False,",
            "    \"DYNAMIC_PLUGINS\": False,",
            "    # With Superset 2.0, we are updating the default so that the legacy datasource",
            "    # editor no longer shows. Currently this is set to false so that the editor",
            "    # option does show, but we will be depreciating it.",
            "    \"DISABLE_LEGACY_DATASOURCE_EDITOR\": True,",
            "    # For some security concerns, you may need to enforce CSRF protection on",
            "    # all query request to explore_json endpoint. In Superset, we use",
            "    # `flask-csrf <https://sjl.bitbucket.io/flask-csrf/>`_ add csrf protection",
            "    # for all POST requests, but this protection doesn't apply to GET method.",
            "    # When ENABLE_EXPLORE_JSON_CSRF_PROTECTION is set to true, your users cannot",
            "    # make GET request to explore_json. explore_json accepts both GET and POST request.",
            "    # See `PR 7935 <https://github.com/apache/superset/pull/7935>`_ for more details.",
            "    \"ENABLE_EXPLORE_JSON_CSRF_PROTECTION\": False,",
            "    \"ENABLE_TEMPLATE_PROCESSING\": False,",
            "    \"ENABLE_TEMPLATE_REMOVE_FILTERS\": False,",
            "    # Allow for javascript controls components",
            "    # this enables programmers to customize certain charts (like the",
            "    # geospatial ones) by inputing javascript in controls. This exposes",
            "    # an XSS security vulnerability",
            "    \"ENABLE_JAVASCRIPT_CONTROLS\": False,",
            "    \"KV_STORE\": False,",
            "    # When this feature is enabled, nested types in Presto will be",
            "    # expanded into extra columns and/or arrays. This is experimental,",
            "    # and doesn't work with all nested types.",
            "    \"PRESTO_EXPAND_DATA\": False,",
            "    # Exposes API endpoint to compute thumbnails",
            "    \"THUMBNAILS\": False,",
            "    \"DASHBOARD_CACHE\": False,",
            "    \"REMOVE_SLICE_LEVEL_LABEL_COLORS\": False,",
            "    \"SHARE_QUERIES_VIA_KV_STORE\": False,",
            "    \"TAGGING_SYSTEM\": False,",
            "    \"SQLLAB_BACKEND_PERSISTENCE\": True,",
            "    \"LISTVIEWS_DEFAULT_CARD_VIEW\": False,",
            "    # When True, this flag allows display of HTML tags in Markdown components",
            "    \"DISPLAY_MARKDOWN_HTML\": True,",
            "    # When True, this escapes HTML (rather than rendering it) in Markdown components",
            "    \"ESCAPE_MARKDOWN_HTML\": False,",
            "    \"DASHBOARD_NATIVE_FILTERS\": True,",
            "    \"DASHBOARD_CROSS_FILTERS\": False,",
            "    # Feature is under active development and breaking changes are expected",
            "    \"DASHBOARD_NATIVE_FILTERS_SET\": False,",
            "    \"DASHBOARD_FILTERS_EXPERIMENTAL\": False,",
            "    \"DASHBOARD_VIRTUALIZATION\": False,",
            "    \"GLOBAL_ASYNC_QUERIES\": False,",
            "    \"VERSIONED_EXPORT\": True,",
            "    \"EMBEDDED_SUPERSET\": False,",
            "    # Enables Alerts and reports new implementation",
            "    \"ALERT_REPORTS\": False,",
            "    \"DASHBOARD_RBAC\": False,",
            "    \"ENABLE_EXPLORE_DRAG_AND_DROP\": True,",
            "    \"ENABLE_FILTER_BOX_MIGRATION\": False,",
            "    \"ENABLE_ADVANCED_DATA_TYPES\": False,",
            "    \"ENABLE_DND_WITH_CLICK_UX\": True,",
            "    # Enabling ALERTS_ATTACH_REPORTS, the system sends email and slack message",
            "    # with screenshot and link",
            "    # Disables ALERTS_ATTACH_REPORTS, the system DOES NOT generate screenshot",
            "    # for report with type 'alert' and sends email and slack message with only link;",
            "    # for report with type 'report' still send with email and slack message with",
            "    # screenshot and link",
            "    \"ALERTS_ATTACH_REPORTS\": True,",
            "    # FORCE_DATABASE_CONNECTIONS_SSL is depreciated.",
            "    \"FORCE_DATABASE_CONNECTIONS_SSL\": False,",
            "    # Enabling ENFORCE_DB_ENCRYPTION_UI forces all database connections to be",
            "    # encrypted before being saved into superset metastore.",
            "    \"ENFORCE_DB_ENCRYPTION_UI\": False,",
            "    # Allow users to export full CSV of table viz type.",
            "    # This could cause the server to run out of memory or compute.",
            "    \"ALLOW_FULL_CSV_EXPORT\": False,",
            "    \"UX_BETA\": False,",
            "    \"GENERIC_CHART_AXES\": False,",
            "    \"ALLOW_ADHOC_SUBQUERY\": False,",
            "    \"USE_ANALAGOUS_COLORS\": False,",
            "    \"DASHBOARD_EDIT_CHART_IN_NEW_TAB\": False,",
            "    # Apply RLS rules to SQL Lab queries. This requires parsing and manipulating the",
            "    # query, and might break queries and/or allow users to bypass RLS. Use with care!",
            "    \"RLS_IN_SQLLAB\": False,",
            "    # Enable caching per impersonation key (e.g username) in a datasource where user",
            "    # impersonation is enabled",
            "    \"CACHE_IMPERSONATION\": False,",
            "    # Enable sharing charts with embedding",
            "    \"EMBEDDABLE_CHARTS\": True,",
            "    \"DRILL_TO_DETAIL\": False,",
            "    \"DATAPANEL_CLOSED_BY_DEFAULT\": False,",
            "    \"HORIZONTAL_FILTER_BAR\": False,",
            "    # Allow users to enable ssh tunneling when creating a DB.",
            "    # Users must check whether the DB engine supports SSH Tunnels",
            "    # otherwise enabling this flag won't have any effect on the DB.",
            "    \"SSH_TUNNELING\": False,",
            "}",
            "",
            "# ------------------------------",
            "# SSH Tunnel",
            "# ------------------------------",
            "# Allow users to set the host used when connecting to the SSH Tunnel",
            "# as localhost and any other alias (0.0.0.0)",
            "# ----------------------------------------------------------------------",
            "#                             |",
            "# -------------+              |    +----------+",
            "#     LOCAL    |              |    |  REMOTE  | :22 SSH",
            "#     CLIENT   | <== SSH ========> |  SERVER  | :8080 web service",
            "# -------------+              |    +----------+",
            "#                             |",
            "#                          FIREWALL (only port 22 is open)",
            "",
            "# ----------------------------------------------------------------------",
            "SSH_TUNNEL_MANAGER_CLASS = \"superset.extensions.ssh.SSHManager\"",
            "SSH_TUNNEL_LOCAL_BIND_ADDRESS = \"127.0.0.1\"",
            "",
            "# Feature flags may also be set via 'SUPERSET_FEATURE_' prefixed environment vars.",
            "DEFAULT_FEATURE_FLAGS.update(",
            "    {",
            "        k[len(\"SUPERSET_FEATURE_\") :]: parse_boolean_string(v)",
            "        for k, v in os.environ.items()",
            "        if re.search(r\"^SUPERSET_FEATURE_\\w+\", k)",
            "    }",
            ")",
            "",
            "# This is merely a default.",
            "FEATURE_FLAGS: Dict[str, bool] = {}",
            "",
            "# A function that receives a dict of all feature flags",
            "# (DEFAULT_FEATURE_FLAGS merged with FEATURE_FLAGS)",
            "# can alter it, and returns a similar dict. Note the dict of feature",
            "# flags passed to the function is a deepcopy of the dict in the config,",
            "# and can therefore be mutated without side-effect",
            "#",
            "# GET_FEATURE_FLAGS_FUNC can be used to implement progressive rollouts,",
            "# role-based features, or a full on A/B testing framework.",
            "#",
            "# from flask import g, request",
            "# def GET_FEATURE_FLAGS_FUNC(feature_flags_dict: Dict[str, bool]) -> Dict[str, bool]:",
            "#     if hasattr(g, \"user\") and g.user.is_active:",
            "#         feature_flags_dict['some_feature'] = g.user and g.user.get_id() == 5",
            "#     return feature_flags_dict",
            "GET_FEATURE_FLAGS_FUNC: Optional[Callable[[Dict[str, bool]], Dict[str, bool]]] = None",
            "# A function that receives a feature flag name and an optional default value.",
            "# Has a similar utility to GET_FEATURE_FLAGS_FUNC but it's useful to not force the",
            "# evaluation of all feature flags when just evaluating a single one.",
            "#",
            "# Note that the default `get_feature_flags` will evaluate each feature with this",
            "# callable when the config key is set, so don't use both GET_FEATURE_FLAGS_FUNC",
            "# and IS_FEATURE_ENABLED_FUNC in conjunction.",
            "IS_FEATURE_ENABLED_FUNC: Optional[Callable[[str, Optional[bool]], bool]] = None",
            "# A function that expands/overrides the frontend `bootstrap_data.common` object.",
            "# Can be used to implement custom frontend functionality,",
            "# or dynamically change certain configs.",
            "#",
            "# Values in `bootstrap_data.common` should have these characteristics:",
            "# - They are not specific to a page the user is visiting",
            "# - They do not contain secrets",
            "#",
            "# Takes as a parameter the common bootstrap payload before transformations.",
            "# Returns a dict containing data that should be added or overridden to the payload.",
            "COMMON_BOOTSTRAP_OVERRIDES_FUNC: Callable[",
            "    [Dict[str, Any]], Dict[str, Any]",
            "] = lambda data: {}  # default: empty dict",
            "",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES is used for adding custom categorical color schemes",
            "# example code for \"My custom warm to hot\" color scheme",
            "# EXTRA_CATEGORICAL_COLOR_SCHEMES = [",
            "#     {",
            "#         \"id\": 'myVisualizationColors',",
            "#         \"description\": '',",
            "#         \"label\": 'My Visualization Colors',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77',",
            "#          '#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_CATEGORICAL_COLOR_SCHEMES: List[Dict[str, Any]] = []",
            "",
            "# THEME_OVERRIDES is used for adding custom theme to superset",
            "# example code for \"My theme\" custom scheme",
            "# THEME_OVERRIDES = {",
            "#   \"borderRadius\": 4,",
            "#   \"colors\": {",
            "#     \"primary\": {",
            "#       \"base\": 'red',",
            "#     },",
            "#     \"secondary\": {",
            "#       \"base\": 'green',",
            "#     },",
            "#     \"grayscale\": {",
            "#       \"base\": 'orange',",
            "#     }",
            "#   }",
            "# }",
            "",
            "THEME_OVERRIDES: Dict[str, Any] = {}",
            "",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES is used for adding custom sequential color schemes",
            "# EXTRA_SEQUENTIAL_COLOR_SCHEMES =  [",
            "#     {",
            "#         \"id\": 'warmToHot',",
            "#         \"description\": '',",
            "#         \"isDiverging\": True,",
            "#         \"label\": 'My custom warm to hot',",
            "#         \"isDefault\": True,",
            "#         \"colors\":",
            "#          ['#552288', '#5AAA46', '#CC7788', '#EEDD55', '#9977BB', '#BBAA44', '#DDCCDD',",
            "#          '#006699', '#009DD9', '#5AAA46', '#44AAAA', '#DDAA77', '#7799BB', '#88AA77']",
            "#     }]",
            "",
            "# This is merely a default",
            "EXTRA_SEQUENTIAL_COLOR_SCHEMES: List[Dict[str, Any]] = []",
            "",
            "# ---------------------------------------------------",
            "# Thumbnail config (behind feature flag)",
            "# ---------------------------------------------------",
            "# When executing Alerts & Reports or Thumbnails as the Selenium user, this defines",
            "# the username of the account used to render the queries and dashboards/charts",
            "THUMBNAIL_SELENIUM_USER: Optional[str] = \"admin\"",
            "",
            "# To be able to have different thumbnails for different users, use these configs to",
            "# define which user to execute the thumbnails and potentially custom functions for",
            "# calculating thumbnail digests. To have unique thumbnails for all users, use the",
            "# following config:",
            "# THUMBNAIL_EXECUTE_AS = [ExecutorType.CURRENT_USER]",
            "THUMBNAIL_EXECUTE_AS = [ExecutorType.SELENIUM]",
            "",
            "# By default, thumbnail digests are calculated based on various parameters in the",
            "# chart/dashboard metadata, and in the case of user-specific thumbnails, the",
            "# username. To specify a custom digest function, use the following config parameters",
            "# to define callbacks that receive",
            "# 1. the model (dashboard or chart)",
            "# 2. the executor type (e.g. ExecutorType.SELENIUM)",
            "# 3. the executor's username (note, this is the executor as defined by",
            "# `THUMBNAIL_EXECUTE_AS`; the executor is only equal to the currently logged in",
            "# user if the executor type is equal to `ExecutorType.CURRENT_USER`)",
            "# and return the final digest string:",
            "THUMBNAIL_DASHBOARD_DIGEST_FUNC: Optional[",
            "    Callable[[Dashboard, ExecutorType, str], str]",
            "] = None",
            "THUMBNAIL_CHART_DIGEST_FUNC: Optional[Callable[[Slice, ExecutorType, str], str]] = None",
            "",
            "THUMBNAIL_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_TYPE\": \"NullCache\",",
            "    \"CACHE_NO_NULL_WARNING\": True,",
            "}",
            "",
            "# Time before selenium times out after trying to locate an element on the page and wait",
            "# for that element to load for a screenshot.",
            "SCREENSHOT_LOCATE_WAIT = int(timedelta(seconds=10).total_seconds())",
            "# Time before selenium times out after waiting for all DOM class elements named",
            "# \"loading\" are gone.",
            "SCREENSHOT_LOAD_WAIT = int(timedelta(minutes=1).total_seconds())",
            "# Selenium destroy retries",
            "SCREENSHOT_SELENIUM_RETRIES = 5",
            "# Give selenium an headstart, in seconds",
            "SCREENSHOT_SELENIUM_HEADSTART = 3",
            "# Wait for the chart animation, in seconds",
            "SCREENSHOT_SELENIUM_ANIMATION_WAIT = 5",
            "# Replace unexpected errors in screenshots with real error messages",
            "SCREENSHOT_REPLACE_UNEXPECTED_ERRORS = False",
            "# Max time to wait for error message modal to show up, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_VISIBLE = 5",
            "# Max time to wait for error message modal to close, in seconds",
            "SCREENSHOT_WAIT_FOR_ERROR_MODAL_INVISIBLE = 5",
            "",
            "# ---------------------------------------------------",
            "# Image and file configuration",
            "# ---------------------------------------------------",
            "# The file upload folder, when using models with files",
            "UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "UPLOAD_CHUNK_SIZE = 4096",
            "",
            "# The image upload folder, when using models with images",
            "IMG_UPLOAD_FOLDER = BASE_DIR + \"/app/static/uploads/\"",
            "",
            "# The image upload url, when using models with images",
            "IMG_UPLOAD_URL = \"/static/uploads/\"",
            "# Setup image size default is (300, 200, True)",
            "# IMG_SIZE = (300, 200, True)",
            "",
            "# Default cache timeout, applies to all cache backends unless specifically overridden in",
            "# each cache config.",
            "CACHE_DEFAULT_TIMEOUT = int(timedelta(days=1).total_seconds())",
            "",
            "# Default cache for Superset objects",
            "CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for datasource metadata and query results",
            "DATA_CACHE_CONFIG: CacheConfig = {\"CACHE_TYPE\": \"NullCache\"}",
            "",
            "# Cache for dashboard filter state (`CACHE_TYPE` defaults to `SimpleCache` when",
            "#  running in debug mode unless overridden)",
            "FILTER_STATE_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=90).total_seconds()),",
            "    # should the timeout be reset when retrieving a cached value",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "}",
            "",
            "# Cache for explore form data state (`CACHE_TYPE` defaults to `SimpleCache` when",
            "#  running in debug mode unless overridden)",
            "EXPLORE_FORM_DATA_CACHE_CONFIG: CacheConfig = {",
            "    \"CACHE_DEFAULT_TIMEOUT\": int(timedelta(days=7).total_seconds()),",
            "    # should the timeout be reset when retrieving a cached value",
            "    \"REFRESH_TIMEOUT_ON_RETRIEVAL\": True,",
            "}",
            "",
            "# store cache keys by datasource UID (via CacheKey) for custom processing/invalidation",
            "STORE_CACHE_KEYS_IN_METADATA_DB = False",
            "",
            "# CORS Options",
            "ENABLE_CORS = False",
            "CORS_OPTIONS: Dict[Any, Any] = {}",
            "",
            "# Sanitizes the HTML content used in markdowns to allow its rendering in a safe manner.",
            "# Disabling this option is not recommended for security reasons. If you wish to allow",
            "# valid safe elements that are not included in the default sanitization schema, use the",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS configuration.",
            "HTML_SANITIZATION = True",
            "",
            "# Use this configuration to extend the HTML sanitization schema.",
            "# By default we use the Gihtub schema defined in",
            "# https://github.com/syntax-tree/hast-util-sanitize/blob/main/lib/schema.js",
            "# For example, the following configuration would allow the rendering of the",
            "# style attribute for div elements and the ftp protocol in hrefs:",
            "# HTML_SANITIZATION_SCHEMA_EXTENSIONS = {",
            "#   \"attributes\": {",
            "#     \"div\": [\"style\"],",
            "#   },",
            "#   \"protocols\": {",
            "#     \"href\": [\"ftp\"],",
            "#   }",
            "# }",
            "# Be careful when extending the default schema to avoid XSS attacks.",
            "HTML_SANITIZATION_SCHEMA_EXTENSIONS: Dict[str, Any] = {}",
            "",
            "# Chrome allows up to 6 open connections per domain at a time. When there are more",
            "# than 6 slices in dashboard, a lot of time fetch requests are queued up and wait for",
            "# next available socket. PR #5039 is trying to allow domain sharding for Superset,",
            "# and this feature will be enabled by configuration only (by default Superset",
            "# doesn't allow cross-domain request).",
            "SUPERSET_WEBSERVER_DOMAINS = None",
            "",
            "# Allowed format types for upload on Database view",
            "EXCEL_EXTENSIONS = {\"xlsx\", \"xls\"}",
            "CSV_EXTENSIONS = {\"csv\", \"tsv\", \"txt\"}",
            "COLUMNAR_EXTENSIONS = {\"parquet\", \"zip\"}",
            "ALLOWED_EXTENSIONS = {*EXCEL_EXTENSIONS, *CSV_EXTENSIONS, *COLUMNAR_EXTENSIONS}",
            "",
            "# CSV Options: key/value pairs that will be passed as argument to DataFrame.to_csv",
            "# method.",
            "# note: index option should not be overridden",
            "CSV_EXPORT = {\"encoding\": \"utf-8\"}",
            "",
            "# ---------------------------------------------------",
            "# Time grain configurations",
            "# ---------------------------------------------------",
            "# List of time grains to disable in the application (see list of builtin",
            "# time grains in superset/db_engine_specs/base.py).",
            "# For example: to disable 1 second time grain:",
            "# TIME_GRAIN_DENYLIST = ['PT1S']",
            "TIME_GRAIN_DENYLIST: List[str] = []",
            "",
            "# Additional time grains to be supported using similar definitions as in",
            "# superset/db_engine_specs/base.py.",
            "# For example: To add a new 2 second time grain:",
            "# TIME_GRAIN_ADDONS = {'PT2S': '2 second'}",
            "TIME_GRAIN_ADDONS: Dict[str, str] = {}",
            "",
            "# Implementation of additional time grains per engine.",
            "# The column to be truncated is denoted `{col}` in the expression.",
            "# For example: To implement 2 second time grain on clickhouse engine:",
            "# TIME_GRAIN_ADDON_EXPRESSIONS = {",
            "#     'clickhouse': {",
            "#         'PT2S': 'toDateTime(intDiv(toUInt32(toDateTime({col})), 2)*2)'",
            "#     }",
            "# }",
            "TIME_GRAIN_ADDON_EXPRESSIONS: Dict[str, Dict[str, str]] = {}",
            "",
            "# ---------------------------------------------------",
            "# List of viz_types not allowed in your environment",
            "# For example: Disable pivot table and treemap:",
            "#  VIZ_TYPE_DENYLIST = ['pivot_table', 'treemap']",
            "# ---------------------------------------------------",
            "",
            "VIZ_TYPE_DENYLIST: List[str] = []",
            "",
            "# --------------------------------------------------",
            "# Modules, datasources and middleware to be registered",
            "# --------------------------------------------------",
            "DEFAULT_MODULE_DS_MAP = OrderedDict(",
            "    [",
            "        (\"superset.connectors.sqla.models\", [\"SqlaTable\"]),",
            "    ]",
            ")",
            "ADDITIONAL_MODULE_DS_MAP: Dict[str, List[str]] = {}",
            "ADDITIONAL_MIDDLEWARE: List[Callable[..., Any]] = []",
            "",
            "# 1) https://docs.python-guide.org/writing/logging/",
            "# 2) https://docs.python.org/2/library/logging.config.html",
            "",
            "# Default configurator will consume the LOG_* settings below",
            "LOGGING_CONFIGURATOR = DefaultLoggingConfigurator()",
            "",
            "# Console Log Settings",
            "",
            "LOG_FORMAT = \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\"",
            "LOG_LEVEL = \"DEBUG\"",
            "",
            "# ---------------------------------------------------",
            "# Enable Time Rotate Log Handler",
            "# ---------------------------------------------------",
            "# LOG_LEVEL = DEBUG, INFO, WARNING, ERROR, CRITICAL",
            "",
            "ENABLE_TIME_ROTATE = False",
            "TIME_ROTATE_LOG_LEVEL = \"DEBUG\"",
            "FILENAME = os.path.join(DATA_DIR, \"superset.log\")",
            "ROLLOVER = \"midnight\"",
            "INTERVAL = 1",
            "BACKUP_COUNT = 30",
            "",
            "# Custom logger for auditing queries. This can be used to send ran queries to a",
            "# structured immutable store for auditing purposes. The function is called for",
            "# every query ran, in both SQL Lab and charts/dashboards.",
            "# def QUERY_LOGGER(",
            "#     database,",
            "#     query,",
            "#     schema=None,",
            "#     user=None,  # TODO(john-bodley): Deprecate in 3.0.",
            "#     client=None,",
            "#     security_manager=None,",
            "#     log_params=None,",
            "# ):",
            "#     pass",
            "QUERY_LOGGER = None",
            "",
            "# Set this API key to enable Mapbox visualizations",
            "MAPBOX_API_KEY = os.environ.get(\"MAPBOX_API_KEY\", \"\")",
            "",
            "# Maximum number of rows returned for any analytical database query",
            "SQL_MAX_ROW = 100000",
            "",
            "# Maximum number of rows displayed in SQL Lab UI",
            "# Is set to avoid out of memory/localstorage issues in browsers. Does not affect",
            "# exported CSVs",
            "DISPLAY_MAX_ROW = 10000",
            "",
            "# Default row limit for SQL Lab queries. Is overridden by setting a new limit in",
            "# the SQL Lab UI",
            "DEFAULT_SQLLAB_LIMIT = 1000",
            "",
            "# Adds a warning message on sqllab save query and schedule query modals.",
            "SQLLAB_SAVE_WARNING_MESSAGE = None",
            "SQLLAB_SCHEDULE_WARNING_MESSAGE = None",
            "",
            "# Force refresh while auto-refresh in dashboard",
            "DASHBOARD_AUTO_REFRESH_MODE: Literal[\"fetch\", \"force\"] = \"force\"",
            "# Dashboard auto refresh intervals",
            "DASHBOARD_AUTO_REFRESH_INTERVALS = [",
            "    [0, \"Don't refresh\"],",
            "    [10, \"10 seconds\"],",
            "    [30, \"30 seconds\"],",
            "    [60, \"1 minute\"],",
            "    [300, \"5 minutes\"],",
            "    [1800, \"30 minutes\"],",
            "    [3600, \"1 hour\"],",
            "    [21600, \"6 hours\"],",
            "    [43200, \"12 hours\"],",
            "    [86400, \"24 hours\"],",
            "]",
            "",
            "# Default celery config is to use SQLA as a broker, in a production setting",
            "# you'll want to use a proper broker as specified here:",
            "# http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html",
            "",
            "",
            "class CeleryConfig:  # pylint: disable=too-few-public-methods",
            "    broker_url = \"sqla+sqlite:///celerydb.sqlite\"",
            "    imports = (\"superset.sql_lab\",)",
            "    result_backend = \"db+sqlite:///celery_results.sqlite\"",
            "    worker_log_level = \"DEBUG\"",
            "    worker_prefetch_multiplier = 1",
            "    task_acks_late = False",
            "    task_annotations = {",
            "        \"sql_lab.get_sql_results\": {\"rate_limit\": \"100/s\"},",
            "        \"email_reports.send\": {",
            "            \"rate_limit\": \"1/s\",",
            "            \"time_limit\": int(timedelta(seconds=120).total_seconds()),",
            "            \"soft_time_limit\": int(timedelta(seconds=150).total_seconds()),",
            "            \"ignore_result\": True,",
            "        },",
            "    }",
            "    beat_schedule = {",
            "        \"email_reports.schedule_hourly\": {",
            "            \"task\": \"email_reports.schedule_hourly\",",
            "            \"schedule\": crontab(minute=1, hour=\"*\"),",
            "        },",
            "        \"reports.scheduler\": {",
            "            \"task\": \"reports.scheduler\",",
            "            \"schedule\": crontab(minute=\"*\", hour=\"*\"),",
            "        },",
            "        \"reports.prune_log\": {",
            "            \"task\": \"reports.prune_log\",",
            "            \"schedule\": crontab(minute=0, hour=0),",
            "        },",
            "    }",
            "",
            "",
            "CELERY_CONFIG = CeleryConfig  # pylint: disable=invalid-name",
            "",
            "# Set celery config to None to disable all the above configuration",
            "# CELERY_CONFIG = None",
            "",
            "# Additional static HTTP headers to be served by your Superset server. Note",
            "# Flask-Talisman applies the relevant security HTTP headers.",
            "#",
            "# DEFAULT_HTTP_HEADERS: sets default values for HTTP headers. These may be overridden",
            "# within the app",
            "# OVERRIDE_HTTP_HEADERS: sets override values for HTTP headers. These values will",
            "# override anything set within the app",
            "DEFAULT_HTTP_HEADERS: Dict[str, Any] = {}",
            "OVERRIDE_HTTP_HEADERS: Dict[str, Any] = {}",
            "HTTP_HEADERS: Dict[str, Any] = {}",
            "",
            "# The db id here results in selecting this one as a default in SQL Lab",
            "DEFAULT_DB_ID = None",
            "",
            "# Timeout duration for SQL Lab synchronous queries",
            "SQLLAB_TIMEOUT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Timeout duration for SQL Lab query validation",
            "SQLLAB_VALIDATION_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "",
            "# SQLLAB_DEFAULT_DBID",
            "SQLLAB_DEFAULT_DBID = None",
            "",
            "# The MAX duration a query can run for before being killed by celery.",
            "SQLLAB_ASYNC_TIME_LIMIT_SEC = int(timedelta(hours=6).total_seconds())",
            "",
            "# Some databases support running EXPLAIN queries that allow users to estimate",
            "# query costs before they run. These EXPLAIN queries should have a small",
            "# timeout.",
            "SQLLAB_QUERY_COST_ESTIMATE_TIMEOUT = int(timedelta(seconds=10).total_seconds())",
            "# The feature is off by default, and currently only supported in Presto and Postgres.",
            "# It also need to be enabled on a per-database basis, by adding the key/value pair",
            "# `cost_estimate_enabled: true` to the database `extra` attribute.",
            "ESTIMATE_QUERY_COST = False",
            "# The cost returned by the databases is a relative value; in order to map the cost to",
            "# a tangible value you need to define a custom formatter that takes into consideration",
            "# your specific infrastructure. For example, you could analyze queries a posteriori by",
            "# running EXPLAIN on them, and compute a histogram of relative costs to present the",
            "# cost as a percentile:",
            "#",
            "# def postgres_query_cost_formatter(",
            "#     result: List[Dict[str, Any]]",
            "# ) -> List[Dict[str, str]]:",
            "#     # 25, 50, 75% percentiles",
            "#     percentile_costs = [100.0, 1000.0, 10000.0]",
            "#",
            "#     out = []",
            "#     for row in result:",
            "#         relative_cost = row[\"Total cost\"]",
            "#         percentile = bisect.bisect_left(percentile_costs, relative_cost) + 1",
            "#         out.append({",
            "#             \"Relative cost\": relative_cost,",
            "#             \"Percentile\": str(percentile * 25) + \"%\",",
            "#         })",
            "#",
            "#     return out",
            "#",
            "#  Then on define the formatter on the config:",
            "#",
            "# \"QUERY_COST_FORMATTERS_BY_ENGINE\": {\"postgresql\": postgres_query_cost_formatter},",
            "QUERY_COST_FORMATTERS_BY_ENGINE: Dict[",
            "    str, Callable[[List[Dict[str, Any]]], List[Dict[str, Any]]]",
            "] = {}",
            "",
            "# Flag that controls if limit should be enforced on the CTA (create table as queries).",
            "SQLLAB_CTAS_NO_LIMIT = False",
            "",
            "# This allows you to define custom logic around the \"CREATE TABLE AS\" or CTAS feature",
            "# in SQL Lab that defines where the target schema should be for a given user.",
            "# Database `CTAS Schema` has a precedence over this setting.",
            "# Example below returns a username and CTA queries will write tables into the schema",
            "# name `username`",
            "# SQLLAB_CTAS_SCHEMA_NAME_FUNC = lambda database, user, schema, sql: user.username",
            "# This is move involved example where depending on the database you can leverage data",
            "# available to assign schema for the CTA query:",
            "# def compute_schema_name(database: Database, user: User, schema: str, sql: str) -> str:",
            "#     if database.name == 'mysql_payments_slave':",
            "#         return 'tmp_superset_schema'",
            "#     if database.name == 'presto_gold':",
            "#         return user.username",
            "#     if database.name == 'analytics':",
            "#         if 'analytics' in [r.name for r in user.roles]:",
            "#             return 'analytics_cta'",
            "#         else:",
            "#             return f'tmp_{schema}'",
            "# Function accepts database object, user object, schema name and sql that will be run.",
            "SQLLAB_CTAS_SCHEMA_NAME_FUNC: Optional[",
            "    Callable[[Database, models.User, str, str], str]",
            "] = None",
            "",
            "# If enabled, it can be used to store the results of long-running queries",
            "# in SQL Lab by using the \"Run Async\" button/feature",
            "RESULTS_BACKEND: Optional[BaseCache] = None",
            "",
            "# Use PyArrow and MessagePack for async query results serialization,",
            "# rather than JSON. This feature requires additional testing from the",
            "# community before it is fully adopted, so this config option is provided",
            "# in order to disable should breaking issues be discovered.",
            "RESULTS_BACKEND_USE_MSGPACK = True",
            "",
            "# The S3 bucket where you want to store your external hive tables created",
            "# from CSV files. For example, 'companyname-superset'",
            "CSV_TO_HIVE_UPLOAD_S3_BUCKET = None",
            "",
            "# The directory within the bucket specified above that will",
            "# contain all the external tables",
            "CSV_TO_HIVE_UPLOAD_DIRECTORY = \"EXTERNAL_HIVE_TABLES/\"",
            "",
            "",
            "# Function that creates upload directory dynamically based on the",
            "# database used, user and schema provided.",
            "def CSV_TO_HIVE_UPLOAD_DIRECTORY_FUNC(  # pylint: disable=invalid-name",
            "    database: Database,",
            "    user: models.User,  # pylint: disable=unused-argument",
            "    schema: Optional[str],",
            ") -> str:",
            "    # Note the final empty path enforces a trailing slash.",
            "    return os.path.join(",
            "        CSV_TO_HIVE_UPLOAD_DIRECTORY, str(database.id), schema or \"\", \"\"",
            "    )",
            "",
            "",
            "# The namespace within hive where the tables created from",
            "# uploading CSVs will be stored.",
            "UPLOADED_CSV_HIVE_NAMESPACE: Optional[str] = None",
            "",
            "# Function that computes the allowed schemas for the CSV uploads.",
            "# Allowed schemas will be a union of schemas_allowed_for_file_upload",
            "# db configuration and a result of this function.",
            "",
            "# mypy doesn't catch that if case ensures list content being always str",
            "ALLOWED_USER_CSV_SCHEMA_FUNC: Callable[[Database, models.User], List[str]] = (",
            "    lambda database, user: [UPLOADED_CSV_HIVE_NAMESPACE]",
            "    if UPLOADED_CSV_HIVE_NAMESPACE",
            "    else []",
            ")",
            "",
            "# Values that should be treated as nulls for the csv uploads.",
            "CSV_DEFAULT_NA_NAMES = list(STR_NA_VALUES)",
            "",
            "# A dictionary of items that gets merged into the Jinja context for",
            "# SQL Lab. The existing context gets updated with this dictionary,",
            "# meaning values for existing keys get overwritten by the content of this",
            "# dictionary. Exposing functionality through JINJA_CONTEXT_ADDONS has security",
            "# implications as it opens a window for a user to execute untrusted code.",
            "# It's important to make sure that the objects exposed (as well as objects attached",
            "# to those objets) are harmless. We recommend only exposing simple/pure functions that",
            "# return native types.",
            "JINJA_CONTEXT_ADDONS: Dict[str, Callable[..., Any]] = {}",
            "",
            "# A dictionary of macro template processors (by engine) that gets merged into global",
            "# template processors. The existing template processors get updated with this",
            "# dictionary, which means the existing keys get overwritten by the content of this",
            "# dictionary. The customized addons don't necessarily need to use Jinja templating",
            "# language. This allows you to define custom logic to process templates on a per-engine",
            "# basis. Example value = `{\"presto\": CustomPrestoTemplateProcessor}`",
            "CUSTOM_TEMPLATE_PROCESSORS: Dict[str, Type[BaseTemplateProcessor]] = {}",
            "",
            "# Roles that are controlled by the API / Superset and should not be changes",
            "# by humans.",
            "ROBOT_PERMISSION_ROLES = [\"Public\", \"Gamma\", \"Alpha\", \"Admin\", \"sql_lab\"]",
            "",
            "CONFIG_PATH_ENV_VAR = \"SUPERSET_CONFIG_PATH\"",
            "",
            "# If a callable is specified, it will be called at app startup while passing",
            "# a reference to the Flask app. This can be used to alter the Flask app",
            "# in whatever way.",
            "# example: FLASK_APP_MUTATOR = lambda x: x.before_request = f",
            "FLASK_APP_MUTATOR = None",
            "",
            "# Set this to false if you don't want users to be able to request/grant",
            "# datasource access requests from/to other users.",
            "ENABLE_ACCESS_REQUEST = False",
            "",
            "# smtp server configuration",
            "EMAIL_NOTIFICATIONS = False  # all the emails are sent using dryrun",
            "SMTP_HOST = \"localhost\"",
            "SMTP_STARTTLS = True",
            "SMTP_SSL = False",
            "SMTP_USER = \"superset\"",
            "SMTP_PORT = 25",
            "SMTP_PASSWORD = \"superset\"",
            "SMTP_MAIL_FROM = \"superset@superset.com\"",
            "# If True creates a default SSL context with ssl.Purpose.CLIENT_AUTH using the",
            "# default system root CA certificates.",
            "SMTP_SSL_SERVER_AUTH = False",
            "ENABLE_CHUNK_ENCODING = False",
            "",
            "# Whether to bump the logging level to ERROR on the flask_appbuilder package",
            "# Set to False if/when debugging FAB related issues like",
            "# permission management",
            "SILENCE_FAB = True",
            "",
            "FAB_ADD_SECURITY_VIEWS = True",
            "FAB_ADD_SECURITY_PERMISSION_VIEW = False",
            "FAB_ADD_SECURITY_VIEW_MENU_VIEW = False",
            "FAB_ADD_SECURITY_PERMISSION_VIEWS_VIEW = False",
            "",
            "# The link to a page containing common errors and their resolutions",
            "# It will be appended at the bottom of sql_lab errors.",
            "TROUBLESHOOTING_LINK = \"\"",
            "",
            "# CSRF token timeout, set to None for a token that never expires",
            "WTF_CSRF_TIME_LIMIT = int(timedelta(weeks=1).total_seconds())",
            "",
            "# This link should lead to a page with instructions on how to gain access to a",
            "# Datasource. It will be placed at the bottom of permissions errors.",
            "PERMISSION_INSTRUCTIONS_LINK = \"\"",
            "",
            "# Integrate external Blueprints to the app by passing them to your",
            "# configuration. These blueprints will get integrated in the app",
            "BLUEPRINTS: List[Blueprint] = []",
            "",
            "# Provide a callable that receives a tracking_url and returns another",
            "# URL. This is used to translate internal Hadoop job tracker URL",
            "# into a proxied one",
            "",
            "",
            "# Transform SQL query tracking url for Hive and Presto engines. You may also",
            "# access information about the query itself by adding a second parameter",
            "# to your transformer function, e.g.:",
            "#   TRACKING_URL_TRANSFORMER = (",
            "#       lambda url, query: url if is_fresh(query) else None",
            "#   )",
            "TRACKING_URL_TRANSFORMER = lambda url: url",
            "",
            "",
            "# Interval between consecutive polls when using Hive Engine",
            "HIVE_POLL_INTERVAL = int(timedelta(seconds=5).total_seconds())",
            "",
            "# Interval between consecutive polls when using Presto Engine",
            "# See here: https://github.com/dropbox/PyHive/blob/8eb0aeab8ca300f3024655419b93dad926c1a351/pyhive/presto.py#L93  # pylint: disable=line-too-long,useless-suppression",
            "PRESTO_POLL_INTERVAL = int(timedelta(seconds=1).total_seconds())",
            "",
            "# Allow list of custom authentications for each DB engine.",
            "# Example:",
            "# from your.module import AuthClass",
            "# from another.extra import auth_method",
            "#",
            "# ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {",
            "#     \"trino\": {",
            "#         \"custom_auth\": AuthClass,",
            "#         \"another_auth_method\": auth_method,",
            "#     },",
            "# }",
            "ALLOWED_EXTRA_AUTHENTICATIONS: Dict[str, Dict[str, Callable[..., Any]]] = {}",
            "",
            "# The id of a template dashboard that should be copied to every new user",
            "DASHBOARD_TEMPLATE_ID = None",
            "",
            "# A callable that allows altering the database connection URL and params",
            "# on the fly, at runtime. This allows for things like impersonation or",
            "# arbitrary logic. For instance you can wire different users to",
            "# use different connection parameters, or pass their email address as the",
            "# username. The function receives the connection uri object, connection",
            "# params, the username, and returns the mutated uri and params objects.",
            "# Example:",
            "#   def DB_CONNECTION_MUTATOR(uri, params, username, security_manager, source):",
            "#       user = security_manager.find_user(username=username)",
            "#       if user and user.email:",
            "#           uri.username = user.email",
            "#       return uri, params",
            "#",
            "# Note that the returned uri and params are passed directly to sqlalchemy's",
            "# as such `create_engine(url, **params)`",
            "DB_CONNECTION_MUTATOR = None",
            "",
            "",
            "# A function that intercepts the SQL to be executed and can alter it.",
            "# The use case is can be around adding some sort of comment header",
            "# with information such as the username and worker node information",
            "#",
            "#    def SQL_QUERY_MUTATOR(",
            "#        sql,",
            "#        user_name=user_name,  # TODO(john-bodley): Deprecate in 3.0.",
            "#        security_manager=security_manager,",
            "#        database=database,",
            "#    ):",
            "#        dttm = datetime.now().isoformat()",
            "#        return f\"-- [SQL LAB] {user_name} {dttm}\\n{sql}\"",
            "# For backward compatibility, you can unpack any of the above arguments in your",
            "# function definition, but keep the **kwargs as the last argument to allow new args",
            "# to be added later without any errors.",
            "def SQL_QUERY_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    sql: str, **kwargs: Any",
            ") -> str:",
            "    return sql",
            "",
            "",
            "# This allows for a user to add header data to any outgoing emails. For example,",
            "# if you need to include metadata in the header or you want to change the specifications",
            "# of the email title, header, or sender.",
            "def EMAIL_HEADER_MUTATOR(  # pylint: disable=invalid-name,unused-argument",
            "    msg: MIMEMultipart, **kwargs: Any",
            ") -> MIMEMultipart:",
            "    return msg",
            "",
            "",
            "# Define a list of usernames to be excluded from all dropdown lists of users",
            "# Owners, filters for created_by, etc.",
            "# The users can also be excluded by overriding the get_exclude_users_from_lists method",
            "# in security manager",
            "EXCLUDE_USERS_FROM_LISTS: Optional[List[str]] = None",
            "",
            "# For database connections, this dictionary will remove engines from the available",
            "# list/dropdown if you do not want these dbs to show as available.",
            "# The available list is generated by driver installed, and some engines have multiple",
            "# drivers.",
            "# e.g., DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {\"databricks\": (\"pyhive\", \"pyodbc\")}",
            "DBS_AVAILABLE_DENYLIST: Dict[str, Set[str]] = {}",
            "",
            "# This auth provider is used by background (offline) tasks that need to access",
            "# protected resources. Can be overridden by end users in order to support",
            "# custom auth mechanisms",
            "MACHINE_AUTH_PROVIDER_CLASS = \"superset.utils.machine_auth.MachineAuthProvider\"",
            "",
            "# ---------------------------------------------------",
            "# Alerts & Reports",
            "# ---------------------------------------------------",
            "# Used for Alerts/Reports (Feature flask ALERT_REPORTS) to set the size for the",
            "# sliding cron window size, should be synced with the celery beat config minus 1 second",
            "ALERT_REPORTS_CRON_WINDOW_SIZE = 59",
            "ALERT_REPORTS_WORKING_TIME_OUT_KILL = True",
            "# Which user to attempt to execute Alerts/Reports as. By default,",
            "# use the user defined in the `THUMBNAIL_SELENIUM_USER` config parameter.",
            "# To first try to execute as the creator in the owners list (if present), then fall",
            "# back to the creator, then the last modifier in the owners list (if present), then the",
            "# last modifier, then an owner (giving priority to the last modifier and then the",
            "# creator if either is contained within the list of owners, otherwise the first owner",
            "# will be used) and finally `THUMBNAIL_SELENIUM_USER`, set as follows:",
            "# ALERT_REPORTS_EXECUTE_AS = [",
            "#     ScheduledTaskExecutor.CREATOR_OWNER,",
            "#     ScheduledTaskExecutor.CREATOR,",
            "#     ScheduledTaskExecutor.MODIFIER_OWNER,",
            "#     ScheduledTaskExecutor.MODIFIER,",
            "#     ScheduledTaskExecutor.OWNER,",
            "#     ScheduledTaskExecutor.SELENIUM,",
            "# ]",
            "ALERT_REPORTS_EXECUTE_AS: List[ExecutorType] = [ExecutorType.SELENIUM]",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_TIME_OUT_LAG = int(timedelta(seconds=10).total_seconds())",
            "# if ALERT_REPORTS_WORKING_TIME_OUT_KILL is True, set a celery hard timeout",
            "# Equal to working timeout + ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG",
            "ALERT_REPORTS_WORKING_SOFT_TIME_OUT_LAG = int(timedelta(seconds=1).total_seconds())",
            "# If set to true no notification is sent, the worker will just log a message.",
            "# Useful for debugging",
            "ALERT_REPORTS_NOTIFICATION_DRY_RUN = False",
            "# Max tries to run queries to prevent false errors caused by transient errors",
            "# being returned to users. Set to a value >1 to enable retries.",
            "ALERT_REPORTS_QUERY_EXECUTION_MAX_TRIES = 1",
            "",
            "# A custom prefix to use on all Alerts & Reports emails",
            "EMAIL_REPORTS_SUBJECT_PREFIX = \"[Report] \"",
            "",
            "# Slack API token for the superset reports, either string or callable",
            "SLACK_API_TOKEN: Optional[Union[Callable[[], str], str]] = None",
            "SLACK_PROXY = None",
            "",
            "# The webdriver to use for generating reports. Use one of the following",
            "# firefox",
            "#   Requires: geckodriver and firefox installations",
            "#   Limitations: can be buggy at times",
            "# chrome:",
            "#   Requires: headless chrome",
            "#   Limitations: unable to generate screenshots of elements",
            "WEBDRIVER_TYPE = \"firefox\"",
            "",
            "# Window size - this will impact the rendering of the data",
            "WEBDRIVER_WINDOW = {",
            "    \"dashboard\": (1600, 2000),",
            "    \"slice\": (3000, 1200),",
            "    \"pixel_density\": 1,",
            "}",
            "",
            "# An optional override to the default auth hook used to provide auth to the",
            "# offline webdriver",
            "WEBDRIVER_AUTH_FUNC = None",
            "",
            "# Any config options to be passed as-is to the webdriver",
            "WEBDRIVER_CONFIGURATION: Dict[Any, Any] = {\"service_log_path\": \"/dev/null\"}",
            "",
            "# Additional args to be passed as arguments to the config object",
            "# Note: these options are Chrome-specific. For FF, these should",
            "# only include the \"--headless\" arg",
            "WEBDRIVER_OPTION_ARGS = [\"--headless\", \"--marionette\"]",
            "",
            "# The base URL to query for accessing the user interface",
            "WEBDRIVER_BASEURL = \"http://0.0.0.0:8080/\"",
            "# The base URL for the email report hyperlinks.",
            "WEBDRIVER_BASEURL_USER_FRIENDLY = WEBDRIVER_BASEURL",
            "# Time selenium will wait for the page to load and render for the email report.",
            "EMAIL_PAGE_RENDER_WAIT = int(timedelta(seconds=30).total_seconds())",
            "",
            "# Send user to a link where they can report bugs",
            "BUG_REPORT_URL = None",
            "",
            "# Send user to a link where they can read more about Superset",
            "DOCUMENTATION_URL = None",
            "DOCUMENTATION_TEXT = \"Documentation\"",
            "DOCUMENTATION_ICON = None  # Recommended size: 16x16",
            "",
            "# What is the Last N days relative in the time selector to:",
            "# 'today' means it is midnight (00:00:00) in the local timezone",
            "# 'now' means it is relative to the query issue time",
            "# If both start and end time is set to now, this will make the time",
            "# filter a moving window. By only setting the end time to now,",
            "# start time will be set to midnight, while end will be relative to",
            "# the query issue time.",
            "DEFAULT_RELATIVE_START_TIME = \"today\"",
            "DEFAULT_RELATIVE_END_TIME = \"today\"",
            "",
            "# Configure which SQL validator to use for each engine",
            "SQL_VALIDATORS_BY_ENGINE = {",
            "    \"presto\": \"PrestoDBSQLValidator\",",
            "    \"postgresql\": \"PostgreSQLValidator\",",
            "}",
            "",
            "# A list of preferred databases, in order. These databases will be",
            "# displayed prominently in the \"Add Database\" dialog. You should",
            "# use the \"engine_name\" attribute of the corresponding DB engine spec",
            "# in `superset/db_engine_specs/`.",
            "PREFERRED_DATABASES: List[str] = [",
            "    \"PostgreSQL\",",
            "    \"Presto\",",
            "    \"MySQL\",",
            "    \"SQLite\",",
            "    # etc.",
            "]",
            "# When adding a new database we try to connect to it. Depending on which parameters are",
            "# incorrect this could take a couple minutes, until the SQLAlchemy driver pinging the",
            "# database times out. Instead of relying on the driver timeout we can specify a shorter",
            "# one here.",
            "TEST_DATABASE_CONNECTION_TIMEOUT = timedelta(seconds=30)",
            "",
            "# Enable/disable CSP warning",
            "CONTENT_SECURITY_POLICY_WARNING = True",
            "",
            "# Do you want Talisman enabled?",
            "TALISMAN_ENABLED = False",
            "# If you want Talisman, how do you want it configured??",
            "TALISMAN_CONFIG = {",
            "    \"content_security_policy\": None,",
            "    \"force_https\": True,",
            "    \"force_https_permanent\": False,",
            "}",
            "",
            "# It is possible to customize which tables and roles are featured in the RLS",
            "# dropdown. When set, this dict is assigned to `add_form_query_rel_fields` and",
            "# `edit_form_query_rel_fields` on `RowLevelSecurityFiltersModelView`. Example:",
            "#",
            "# from flask_appbuilder.models.sqla import filters",
            "# RLS_FORM_QUERY_REL_FIELDS = {",
            "#     \"roles\": [[\"name\", filters.FilterStartsWith, \"RlsRole\"]]",
            "#     \"tables\": [[\"table_name\", filters.FilterContains, \"rls\"]]",
            "# }",
            "RLS_FORM_QUERY_REL_FIELDS: Optional[Dict[str, List[List[Any]]]] = None",
            "",
            "#",
            "# Flask session cookie options",
            "#",
            "# See https://flask.palletsprojects.com/en/1.1.x/security/#set-cookie-options",
            "# for details",
            "#",
            "SESSION_COOKIE_HTTPONLY = True  # Prevent cookie from being read by frontend JS?",
            "SESSION_COOKIE_SECURE = False  # Prevent cookie from being transmitted over non-tls?",
            "SESSION_COOKIE_SAMESITE = \"Lax\"  # One of [None, 'None', 'Lax', 'Strict']",
            "",
            "# Cache static resources.",
            "SEND_FILE_MAX_AGE_DEFAULT = int(timedelta(days=365).total_seconds())",
            "",
            "# URI to database storing the example data, points to",
            "# SQLALCHEMY_DATABASE_URI by default if set to `None`",
            "SQLALCHEMY_EXAMPLES_URI = None",
            "",
            "# Optional prefix to be added to all static asset paths when rendering the UI.",
            "# This is useful for hosting assets in an external CDN, for example",
            "STATIC_ASSETS_PREFIX = \"\"",
            "",
            "# Some sqlalchemy connection strings can open Superset to security risks.",
            "# Typically these should not be allowed.",
            "PREVENT_UNSAFE_DB_CONNECTIONS = True",
            "",
            "# Prevents unsafe default endpoints to be registered on datasets.",
            "PREVENT_UNSAFE_DEFAULT_URLS_ON_DATASET = True",
            "",
            "# Path used to store SSL certificates that are generated when using custom certs.",
            "# Defaults to temporary directory.",
            "# Example: SSL_CERT_PATH = \"/certs\"",
            "SSL_CERT_PATH: Optional[str] = None",
            "",
            "# SQLA table mutator, every time we fetch the metadata for a certain table",
            "# (superset.connectors.sqla.models.SqlaTable), we call this hook",
            "# to allow mutating the object with this callback.",
            "# This can be used to set any properties of the object based on naming",
            "# conventions and such. You can find examples in the tests.",
            "",
            "SQLA_TABLE_MUTATOR = lambda table: table",
            "",
            "",
            "# Global async query config options.",
            "# Requires GLOBAL_ASYNC_QUERIES feature flag to be enabled.",
            "GLOBAL_ASYNC_QUERIES_REDIS_CONFIG = {",
            "    \"port\": 6379,",
            "    \"host\": \"127.0.0.1\",",
            "    \"password\": \"\",",
            "    \"db\": 0,",
            "    \"ssl\": False,",
            "}",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_PREFIX = \"async-events-\"",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT = 1000",
            "GLOBAL_ASYNC_QUERIES_REDIS_STREAM_LIMIT_FIREHOSE = 1000000",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_NAME = \"async-token\"",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_SECURE = False",
            "GLOBAL_ASYNC_QUERIES_JWT_COOKIE_DOMAIN = None",
            "GLOBAL_ASYNC_QUERIES_JWT_SECRET = \"test-secret-change-me\"",
            "GLOBAL_ASYNC_QUERIES_TRANSPORT = \"polling\"",
            "GLOBAL_ASYNC_QUERIES_POLLING_DELAY = int(",
            "    timedelta(milliseconds=500).total_seconds() * 1000",
            ")",
            "GLOBAL_ASYNC_QUERIES_WEBSOCKET_URL = \"ws://127.0.0.1:8080/\"",
            "",
            "# Embedded config options",
            "GUEST_ROLE_NAME = \"Public\"",
            "GUEST_TOKEN_JWT_SECRET = \"test-guest-secret-change-me\"",
            "GUEST_TOKEN_JWT_ALGO = \"HS256\"",
            "GUEST_TOKEN_HEADER_NAME = \"X-GuestToken\"",
            "GUEST_TOKEN_JWT_EXP_SECONDS = 300  # 5 minutes",
            "# Guest token audience for the embedded superset, either string or callable",
            "GUEST_TOKEN_JWT_AUDIENCE: Optional[Union[Callable[[], str], str]] = None",
            "",
            "# A SQL dataset health check. Note if enabled it is strongly advised that the callable",
            "# be memoized to aid with performance, i.e.,",
            "#",
            "#    @cache_manager.cache.memoize(timeout=0)",
            "#    def DATASET_HEALTH_CHECK(datasource: SqlaTable) -> Optional[str]:",
            "#        if (",
            "#            datasource.sql and",
            "#            len(sql_parse.ParsedQuery(datasource.sql, strip_comments=True).tables) == 1",
            "#        ):",
            "#            return (",
            "#                \"This virtual dataset queries only one table and therefore could be \"",
            "#                \"replaced by querying the table directly.\"",
            "#            )",
            "#",
            "#        return None",
            "#",
            "# Within the FLASK_APP_MUTATOR callable, i.e., once the application and thus cache have",
            "# been initialized it is also necessary to add the following logic to blow the cache for",
            "# all datasources if the callback function changed.",
            "#",
            "#    def FLASK_APP_MUTATOR(app: Flask) -> None:",
            "#        name = \"DATASET_HEALTH_CHECK\"",
            "#        func = app.config[name]",
            "#        code = func.uncached.__code__.co_code",
            "#",
            "#        if cache_manager.cache.get(name) != code:",
            "#            cache_manager.cache.delete_memoized(func)",
            "#            cache_manager.cache.set(name, code, timeout=0)",
            "#",
            "DATASET_HEALTH_CHECK: Optional[Callable[[\"SqlaTable\"], str]] = None",
            "",
            "# Do not show user info or profile in the menu",
            "MENU_HIDE_USER_INFO = False",
            "",
            "# Set to False to only allow viewing own recent activity",
            "# or to disallow users from viewing other users profile page",
            "ENABLE_BROAD_ACTIVITY_ACCESS = True",
            "",
            "# the advanced data type key should correspond to that set in the column metadata",
            "ADVANCED_DATA_TYPES: Dict[str, AdvancedDataType] = {",
            "    \"internet_address\": internet_address,",
            "    \"port\": internet_port,",
            "}",
            "",
            "# By default, the Welcome page features example charts and dashboards. This can be",
            "# changed to show all charts/dashboards the user has access to, or a custom view",
            "# by providing the title and a FAB filter:",
            "# WELCOME_PAGE_LAST_TAB = (",
            "#     \"Xyz\",",
            "#     [{\"col\": 'created_by', \"opr\": 'rel_o_m', \"value\": 10}],",
            "# )",
            "WELCOME_PAGE_LAST_TAB: Union[",
            "    Literal[\"examples\", \"all\"], Tuple[str, List[Dict[str, Any]]]",
            "] = \"examples\"",
            "",
            "# Configuration for environment tag shown on the navbar. Setting 'text' to '' will hide the tag.",
            "# 'color' can either be a hex color code, or a dot-indexed theme color (e.g. error.base)",
            "ENVIRONMENT_TAG_CONFIG = {",
            "    \"variable\": \"FLASK_ENV\",",
            "    \"values\": {",
            "        \"development\": {",
            "            \"color\": \"error.base\",",
            "            \"text\": \"Development\",",
            "        },",
            "        \"production\": {",
            "            \"color\": \"\",",
            "            \"text\": \"\",",
            "        },",
            "    },",
            "}",
            "",
            "# -------------------------------------------------------------------",
            "# *                WARNING:  STOP EDITING  HERE                    *",
            "# -------------------------------------------------------------------",
            "# Don't add config values below this line since local configs won't be",
            "# able to override them.",
            "if CONFIG_PATH_ENV_VAR in os.environ:",
            "    # Explicitly import config module that is not necessarily in pythonpath; useful",
            "    # for case where app is being executed via pex.",
            "    cfg_path = os.environ[CONFIG_PATH_ENV_VAR]",
            "    try:",
            "        module = sys.modules[__name__]",
            "        override_conf = imp.load_source(\"superset_config\", cfg_path)",
            "        for key in dir(override_conf):",
            "            if key.isupper():",
            "                setattr(module, key, getattr(override_conf, key))",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{cfg_path}]\")",
            "    except Exception:",
            "        logger.exception(",
            "            \"Failed to import config for %s=%s\", CONFIG_PATH_ENV_VAR, cfg_path",
            "        )",
            "        raise",
            "elif importlib.util.find_spec(\"superset_config\") and not is_test():",
            "    try:",
            "        # pylint: disable=import-error,wildcard-import,unused-wildcard-import",
            "        import superset_config",
            "        from superset_config import *  # type: ignore",
            "",
            "        print(f\"Loaded your LOCAL configuration at [{superset_config.__file__}]\")",
            "    except Exception:",
            "        logger.exception(\"Found but failed to import local superset_config\")",
            "        raise"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1509": []
        },
        "addLocation": []
    },
    "superset/constants.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 139,
                "PatchRowcode": "     \"validate_sql\": \"read\","
            },
            "1": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 140,
                "PatchRowcode": "     \"get_data\": \"read\","
            },
            "2": {
                "beforePatchRowNumber": 141,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "     \"samples\": \"read\","
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+    \"delete_ssh_tunnel\": \"write\","
            },
            "4": {
                "beforePatchRowNumber": 142,
                "afterPatchRowNumber": 143,
                "PatchRowcode": " }"
            },
            "5": {
                "beforePatchRowNumber": 143,
                "afterPatchRowNumber": 144,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 144,
                "afterPatchRowNumber": 145,
                "PatchRowcode": " EXTRA_FORM_DATA_APPEND_KEYS = {"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "# ATTENTION: If you change any constants, make sure to also change utils/common.js",
            "",
            "# string to use when None values *need* to be converted to/from strings",
            "from enum import Enum",
            "",
            "USER_AGENT = \"Apache Superset\"",
            "",
            "NULL_STRING = \"<NULL>\"",
            "EMPTY_STRING = \"<empty string>\"",
            "",
            "CHANGE_ME_SECRET_KEY = \"CHANGE_ME_TO_A_COMPLEX_RANDOM_SECRET\"",
            "",
            "# UUID for the examples database",
            "EXAMPLES_DB_UUID = \"a2dc77af-e654-49bb-b321-40f6b559a1ee\"",
            "",
            "PASSWORD_MASK = \"X\" * 10",
            "",
            "NO_TIME_RANGE = \"No filter\"",
            "",
            "QUERY_CANCEL_KEY = \"cancel_query\"",
            "QUERY_EARLY_CANCEL_KEY = \"early_cancel_query\"",
            "",
            "",
            "class RouteMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Route methods are a FAB concept around ModelView and RestModelView",
            "    classes in FAB. Derivatives can define `include_route_method` and",
            "    `exclude_route_methods` class attribute as a set of methods that",
            "    will or won't get exposed.",
            "",
            "    This class is a collection of static constants to reference common",
            "    route methods, namely the ones defined in the base classes in FAB",
            "    \"\"\"",
            "",
            "    # ModelView specific",
            "    ACTION = \"action\"",
            "    ACTION_POST = \"action_post\"",
            "    ADD = \"add\"",
            "    API_CREATE = \"api_create\"",
            "    API_DELETE = \"api_delete\"",
            "    API_GET = \"api_get\"",
            "    API_READ = \"api_read\"",
            "    API_UPDATE = \"api_update\"",
            "    DELETE = \"delete\"",
            "    DOWNLOAD = \"download\"",
            "    EDIT = \"edit\"",
            "    LIST = \"list\"",
            "    SHOW = \"show\"",
            "    INFO = \"info\"",
            "",
            "    # RestModelView specific",
            "    EXPORT = \"export\"",
            "    IMPORT = \"import_\"",
            "    GET = \"get\"",
            "    GET_LIST = \"get_list\"",
            "    POST = \"post\"",
            "    PUT = \"put\"",
            "    RELATED = \"related\"",
            "    DISTINCT = \"distinct\"",
            "",
            "    # Commonly used sets",
            "    API_SET = {API_CREATE, API_DELETE, API_GET, API_READ, API_UPDATE}",
            "    CRUD_SET = {ADD, LIST, EDIT, DELETE, ACTION_POST, SHOW}",
            "    RELATED_VIEW_SET = {ADD, LIST, EDIT, DELETE}",
            "    REST_MODEL_VIEW_CRUD_SET = {DELETE, GET, GET_LIST, POST, PUT, INFO}",
            "",
            "",
            "MODEL_VIEW_RW_METHOD_PERMISSION_MAP = {",
            "    \"add\": \"write\",",
            "    \"api\": \"read\",",
            "    \"api_column_add\": \"write\",",
            "    \"api_column_edit\": \"write\",",
            "    \"api_create\": \"write\",",
            "    \"api_delete\": \"write\",",
            "    \"api_get\": \"read\",",
            "    \"api_read\": \"read\",",
            "    \"api_readvalues\": \"read\",",
            "    \"api_update\": \"write\",",
            "    \"annotation\": \"read\",",
            "    \"delete\": \"write\",",
            "    \"download\": \"read\",",
            "    \"download_dashboards\": \"read\",",
            "    \"edit\": \"write\",",
            "    \"list\": \"read\",",
            "    \"muldelete\": \"write\",",
            "    \"mulexport\": \"read\",",
            "    \"show\": \"read\",",
            "    \"new\": \"write\",",
            "    \"yaml_export\": \"read\",",
            "    \"refresh\": \"write\",",
            "}",
            "",
            "MODEL_API_RW_METHOD_PERMISSION_MAP = {",
            "    \"bulk_delete\": \"write\",",
            "    \"delete\": \"write\",",
            "    \"distinct\": \"read\",",
            "    \"get\": \"read\",",
            "    \"get_list\": \"read\",",
            "    \"info\": \"read\",",
            "    \"post\": \"write\",",
            "    \"put\": \"write\",",
            "    \"related\": \"read\",",
            "    \"related_objects\": \"read\",",
            "    \"schemas\": \"read\",",
            "    \"select_star\": \"read\",",
            "    \"table_metadata\": \"read\",",
            "    \"table_extra_metadata\": \"read\",",
            "    \"test_connection\": \"read\",",
            "    \"validate_parameters\": \"read\",",
            "    \"favorite_status\": \"read\",",
            "    \"thumbnail\": \"read\",",
            "    \"import_\": \"write\",",
            "    \"refresh\": \"write\",",
            "    \"cache_screenshot\": \"read\",",
            "    \"screenshot\": \"read\",",
            "    \"data\": \"read\",",
            "    \"data_from_cache\": \"read\",",
            "    \"get_charts\": \"read\",",
            "    \"get_datasets\": \"read\",",
            "    \"function_names\": \"read\",",
            "    \"available\": \"read\",",
            "    \"validate_sql\": \"read\",",
            "    \"get_data\": \"read\",",
            "    \"samples\": \"read\",",
            "}",
            "",
            "EXTRA_FORM_DATA_APPEND_KEYS = {",
            "    \"adhoc_filters\",",
            "    \"filters\",",
            "    \"interactive_groupby\",",
            "    \"interactive_highlight\",",
            "    \"interactive_drilldown\",",
            "    \"custom_form_data\",",
            "}",
            "",
            "EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS = {",
            "    \"granularity\": \"granularity\",",
            "    \"granularity_sqla\": \"granularity\",",
            "    \"time_column\": \"time_column\",",
            "    \"time_grain\": \"time_grain\",",
            "    \"time_range\": \"time_range\",",
            "    \"time_grain_sqla\": \"time_grain_sqla\",",
            "}",
            "",
            "EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS = {",
            "    \"relative_start\",",
            "    \"relative_end\",",
            "}",
            "",
            "EXTRA_FORM_DATA_OVERRIDE_KEYS = (",
            "    set(EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS.values())",
            "    | EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS",
            ")",
            "",
            "",
            "class PandasAxis(int, Enum):",
            "    ROW = 0",
            "    COLUMN = 1",
            "",
            "",
            "class PandasPostprocessingCompare(str, Enum):",
            "    DIFF = \"difference\"",
            "    PCT = \"percentage\"",
            "    RAT = \"ratio\"",
            "",
            "",
            "class CacheRegion(str, Enum):",
            "    DEFAULT = \"default\"",
            "    DATA = \"data\"",
            "    THUMBNAIL = \"thumbnail\""
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "# ATTENTION: If you change any constants, make sure to also change utils/common.js",
            "",
            "# string to use when None values *need* to be converted to/from strings",
            "from enum import Enum",
            "",
            "USER_AGENT = \"Apache Superset\"",
            "",
            "NULL_STRING = \"<NULL>\"",
            "EMPTY_STRING = \"<empty string>\"",
            "",
            "CHANGE_ME_SECRET_KEY = \"CHANGE_ME_TO_A_COMPLEX_RANDOM_SECRET\"",
            "",
            "# UUID for the examples database",
            "EXAMPLES_DB_UUID = \"a2dc77af-e654-49bb-b321-40f6b559a1ee\"",
            "",
            "PASSWORD_MASK = \"X\" * 10",
            "",
            "NO_TIME_RANGE = \"No filter\"",
            "",
            "QUERY_CANCEL_KEY = \"cancel_query\"",
            "QUERY_EARLY_CANCEL_KEY = \"early_cancel_query\"",
            "",
            "",
            "class RouteMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Route methods are a FAB concept around ModelView and RestModelView",
            "    classes in FAB. Derivatives can define `include_route_method` and",
            "    `exclude_route_methods` class attribute as a set of methods that",
            "    will or won't get exposed.",
            "",
            "    This class is a collection of static constants to reference common",
            "    route methods, namely the ones defined in the base classes in FAB",
            "    \"\"\"",
            "",
            "    # ModelView specific",
            "    ACTION = \"action\"",
            "    ACTION_POST = \"action_post\"",
            "    ADD = \"add\"",
            "    API_CREATE = \"api_create\"",
            "    API_DELETE = \"api_delete\"",
            "    API_GET = \"api_get\"",
            "    API_READ = \"api_read\"",
            "    API_UPDATE = \"api_update\"",
            "    DELETE = \"delete\"",
            "    DOWNLOAD = \"download\"",
            "    EDIT = \"edit\"",
            "    LIST = \"list\"",
            "    SHOW = \"show\"",
            "    INFO = \"info\"",
            "",
            "    # RestModelView specific",
            "    EXPORT = \"export\"",
            "    IMPORT = \"import_\"",
            "    GET = \"get\"",
            "    GET_LIST = \"get_list\"",
            "    POST = \"post\"",
            "    PUT = \"put\"",
            "    RELATED = \"related\"",
            "    DISTINCT = \"distinct\"",
            "",
            "    # Commonly used sets",
            "    API_SET = {API_CREATE, API_DELETE, API_GET, API_READ, API_UPDATE}",
            "    CRUD_SET = {ADD, LIST, EDIT, DELETE, ACTION_POST, SHOW}",
            "    RELATED_VIEW_SET = {ADD, LIST, EDIT, DELETE}",
            "    REST_MODEL_VIEW_CRUD_SET = {DELETE, GET, GET_LIST, POST, PUT, INFO}",
            "",
            "",
            "MODEL_VIEW_RW_METHOD_PERMISSION_MAP = {",
            "    \"add\": \"write\",",
            "    \"api\": \"read\",",
            "    \"api_column_add\": \"write\",",
            "    \"api_column_edit\": \"write\",",
            "    \"api_create\": \"write\",",
            "    \"api_delete\": \"write\",",
            "    \"api_get\": \"read\",",
            "    \"api_read\": \"read\",",
            "    \"api_readvalues\": \"read\",",
            "    \"api_update\": \"write\",",
            "    \"annotation\": \"read\",",
            "    \"delete\": \"write\",",
            "    \"download\": \"read\",",
            "    \"download_dashboards\": \"read\",",
            "    \"edit\": \"write\",",
            "    \"list\": \"read\",",
            "    \"muldelete\": \"write\",",
            "    \"mulexport\": \"read\",",
            "    \"show\": \"read\",",
            "    \"new\": \"write\",",
            "    \"yaml_export\": \"read\",",
            "    \"refresh\": \"write\",",
            "}",
            "",
            "MODEL_API_RW_METHOD_PERMISSION_MAP = {",
            "    \"bulk_delete\": \"write\",",
            "    \"delete\": \"write\",",
            "    \"distinct\": \"read\",",
            "    \"get\": \"read\",",
            "    \"get_list\": \"read\",",
            "    \"info\": \"read\",",
            "    \"post\": \"write\",",
            "    \"put\": \"write\",",
            "    \"related\": \"read\",",
            "    \"related_objects\": \"read\",",
            "    \"schemas\": \"read\",",
            "    \"select_star\": \"read\",",
            "    \"table_metadata\": \"read\",",
            "    \"table_extra_metadata\": \"read\",",
            "    \"test_connection\": \"read\",",
            "    \"validate_parameters\": \"read\",",
            "    \"favorite_status\": \"read\",",
            "    \"thumbnail\": \"read\",",
            "    \"import_\": \"write\",",
            "    \"refresh\": \"write\",",
            "    \"cache_screenshot\": \"read\",",
            "    \"screenshot\": \"read\",",
            "    \"data\": \"read\",",
            "    \"data_from_cache\": \"read\",",
            "    \"get_charts\": \"read\",",
            "    \"get_datasets\": \"read\",",
            "    \"function_names\": \"read\",",
            "    \"available\": \"read\",",
            "    \"validate_sql\": \"read\",",
            "    \"get_data\": \"read\",",
            "    \"samples\": \"read\",",
            "    \"delete_ssh_tunnel\": \"write\",",
            "}",
            "",
            "EXTRA_FORM_DATA_APPEND_KEYS = {",
            "    \"adhoc_filters\",",
            "    \"filters\",",
            "    \"interactive_groupby\",",
            "    \"interactive_highlight\",",
            "    \"interactive_drilldown\",",
            "    \"custom_form_data\",",
            "}",
            "",
            "EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS = {",
            "    \"granularity\": \"granularity\",",
            "    \"granularity_sqla\": \"granularity\",",
            "    \"time_column\": \"time_column\",",
            "    \"time_grain\": \"time_grain\",",
            "    \"time_range\": \"time_range\",",
            "    \"time_grain_sqla\": \"time_grain_sqla\",",
            "}",
            "",
            "EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS = {",
            "    \"relative_start\",",
            "    \"relative_end\",",
            "}",
            "",
            "EXTRA_FORM_DATA_OVERRIDE_KEYS = (",
            "    set(EXTRA_FORM_DATA_OVERRIDE_REGULAR_MAPPINGS.values())",
            "    | EXTRA_FORM_DATA_OVERRIDE_EXTRA_KEYS",
            ")",
            "",
            "",
            "class PandasAxis(int, Enum):",
            "    ROW = 0",
            "    COLUMN = 1",
            "",
            "",
            "class PandasPostprocessingCompare(str, Enum):",
            "    DIFF = \"difference\"",
            "    PCT = \"percentage\"",
            "    RAT = \"ratio\"",
            "",
            "",
            "class CacheRegion(str, Enum):",
            "    DEFAULT = \"default\"",
            "    DATA = \"data\"",
            "    THUMBNAIL = \"thumbnail\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "superset/databases/api.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "     ValidateSQLRequest,"
            },
            "1": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "     ValidateSQLResponse,"
            },
            "2": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+from superset.databases.ssh_tunnel.commands.delete import DeleteSSHTunnelCommand"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+from superset.databases.ssh_tunnel.commands.exceptions import ("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+    SSHTunnelDeleteFailedError,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+    SSHTunnelNotFoundError,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+)"
            },
            "8": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " from superset.databases.utils import get_table_metadata"
            },
            "9": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 81,
                "PatchRowcode": " from superset.db_engine_specs import get_available_engine_specs"
            },
            "10": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 82,
                "PatchRowcode": " from superset.errors import ErrorLevel, SupersetError, SupersetErrorType"
            },
            "11": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 85,
                "PatchRowcode": " from superset.models.core import Database"
            },
            "12": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 86,
                "PatchRowcode": " from superset.superset_typing import FlaskResponse"
            },
            "13": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 87,
                "PatchRowcode": " from superset.utils.core import error_msg_from_exception, parse_js_uri_path_item"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+from superset.utils.ssh_tunnel import mask_password_info"
            },
            "15": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 89,
                "PatchRowcode": " from superset.views.base import json_errors_response"
            },
            "16": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 90,
                "PatchRowcode": " from superset.views.base_api import ("
            },
            "17": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "     BaseSupersetModelRestApi,"
            },
            "18": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "         \"available\","
            },
            "19": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "         \"validate_parameters\","
            },
            "20": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "         \"validate_sql\","
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        \"delete_ssh_tunnel\","
            },
            "22": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "     }"
            },
            "23": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "     resource_name = \"database\""
            },
            "24": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "     class_permission_name = \"Database\""
            },
            "25": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 226,
                "PatchRowcode": "         ValidateSQLResponse,"
            },
            "26": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 227,
                "PatchRowcode": "     )"
            },
            "27": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": 228,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 229,
                "PatchRowcode": "+    @expose(\"/<int:pk>\", methods=[\"GET\"])"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 230,
                "PatchRowcode": "+    @protect()"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 231,
                "PatchRowcode": "+    @safe"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 232,
                "PatchRowcode": "+    def get(self, pk: int, **kwargs: Any) -> Response:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 233,
                "PatchRowcode": "+        \"\"\"Get a database"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 234,
                "PatchRowcode": "+        ---"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 235,
                "PatchRowcode": "+        get:"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 236,
                "PatchRowcode": "+          description: >-"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 237,
                "PatchRowcode": "+            Get a database"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 238,
                "PatchRowcode": "+          parameters:"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 239,
                "PatchRowcode": "+          - in: path"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 240,
                "PatchRowcode": "+            schema:"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 241,
                "PatchRowcode": "+              type: integer"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 242,
                "PatchRowcode": "+            description: The database id"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 243,
                "PatchRowcode": "+            name: pk"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+          responses:"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 245,
                "PatchRowcode": "+            200:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+              description: Database"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+              content:"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+                application/json:"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 249,
                "PatchRowcode": "+                  schema:"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 250,
                "PatchRowcode": "+                    type: object"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 251,
                "PatchRowcode": "+            400:"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 252,
                "PatchRowcode": "+              $ref: '#/components/responses/400'"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 253,
                "PatchRowcode": "+            401:"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 254,
                "PatchRowcode": "+              $ref: '#/components/responses/401'"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 255,
                "PatchRowcode": "+            422:"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 256,
                "PatchRowcode": "+              $ref: '#/components/responses/422'"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 257,
                "PatchRowcode": "+            500:"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 258,
                "PatchRowcode": "+              $ref: '#/components/responses/500'"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 259,
                "PatchRowcode": "+        \"\"\""
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 260,
                "PatchRowcode": "+        data = self.get_headless(pk, **kwargs)"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 261,
                "PatchRowcode": "+        try:"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 262,
                "PatchRowcode": "+            if ssh_tunnel := DatabaseDAO.get_ssh_tunnel(pk):"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 263,
                "PatchRowcode": "+                payload = data.json"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 264,
                "PatchRowcode": "+                payload[\"result\"][\"ssh_tunnel\"] = ssh_tunnel.data"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 265,
                "PatchRowcode": "+                return payload"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 266,
                "PatchRowcode": "+            return data"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 267,
                "PatchRowcode": "+        except SupersetException as ex:"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 268,
                "PatchRowcode": "+            return self.response(ex.status, message=ex.message)"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 269,
                "PatchRowcode": "+"
            },
            "69": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 270,
                "PatchRowcode": "     @expose(\"/\", methods=[\"POST\"])"
            },
            "70": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 271,
                "PatchRowcode": "     @protect()"
            },
            "71": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "     @safe"
            },
            "72": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 328,
                "PatchRowcode": "             if new_model.driver:"
            },
            "73": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "                 item[\"driver\"] = new_model.driver"
            },
            "74": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 330,
                "PatchRowcode": " "
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 331,
                "PatchRowcode": "+            # Return SSH Tunnel and hide passwords if any"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+            if item.get(\"ssh_tunnel\"):"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 333,
                "PatchRowcode": "+                item[\"ssh_tunnel\"] = mask_password_info("
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 334,
                "PatchRowcode": "+                    new_model.ssh_tunnel  # pylint: disable=no-member"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 335,
                "PatchRowcode": "+                )"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 336,
                "PatchRowcode": "+"
            },
            "81": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": 337,
                "PatchRowcode": "             return self.response(201, id=new_model.id, result=item)"
            },
            "82": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 338,
                "PatchRowcode": "         except DatabaseInvalidError as ex:"
            },
            "83": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "             return self.response_422(message=ex.normalized_messages())"
            },
            "84": {
                "beforePatchRowNumber": 361,
                "afterPatchRowNumber": 415,
                "PatchRowcode": "             item[\"sqlalchemy_uri\"] = changed_model.sqlalchemy_uri"
            },
            "85": {
                "beforePatchRowNumber": 362,
                "afterPatchRowNumber": 416,
                "PatchRowcode": "             if changed_model.parameters:"
            },
            "86": {
                "beforePatchRowNumber": 363,
                "afterPatchRowNumber": 417,
                "PatchRowcode": "                 item[\"parameters\"] = changed_model.parameters"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 418,
                "PatchRowcode": "+            # Return SSH Tunnel and hide passwords if any"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 419,
                "PatchRowcode": "+            if item.get(\"ssh_tunnel\"):"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 420,
                "PatchRowcode": "+                item[\"ssh_tunnel\"] = mask_password_info(changed_model.ssh_tunnel)"
            },
            "90": {
                "beforePatchRowNumber": 364,
                "afterPatchRowNumber": 421,
                "PatchRowcode": "             return self.response(200, id=changed_model.id, result=item)"
            },
            "91": {
                "beforePatchRowNumber": 365,
                "afterPatchRowNumber": 422,
                "PatchRowcode": "         except DatabaseNotFoundError:"
            },
            "92": {
                "beforePatchRowNumber": 366,
                "afterPatchRowNumber": 423,
                "PatchRowcode": "             return self.response_404()"
            },
            "93": {
                "beforePatchRowNumber": 1206,
                "afterPatchRowNumber": 1263,
                "PatchRowcode": "         command = ValidateDatabaseParametersCommand(payload)"
            },
            "94": {
                "beforePatchRowNumber": 1207,
                "afterPatchRowNumber": 1264,
                "PatchRowcode": "         command.run()"
            },
            "95": {
                "beforePatchRowNumber": 1208,
                "afterPatchRowNumber": 1265,
                "PatchRowcode": "         return self.response(200, message=\"OK\")"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1266,
                "PatchRowcode": "+"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1267,
                "PatchRowcode": "+    @expose(\"/<int:pk>/ssh_tunnel/\", methods=[\"DELETE\"])"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1268,
                "PatchRowcode": "+    @protect()"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1269,
                "PatchRowcode": "+    @statsd_metrics"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1270,
                "PatchRowcode": "+    @event_logger.log_this_with_context("
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1271,
                "PatchRowcode": "+        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\""
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1272,
                "PatchRowcode": "+        f\".delete_ssh_tunnel\","
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1273,
                "PatchRowcode": "+        log_to_statsd=False,"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1274,
                "PatchRowcode": "+    )"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1275,
                "PatchRowcode": "+    def delete_ssh_tunnel(self, pk: int) -> Response:"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1276,
                "PatchRowcode": "+        \"\"\"Deletes a SSH Tunnel"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1277,
                "PatchRowcode": "+        ---"
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1278,
                "PatchRowcode": "+        delete:"
            },
            "109": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1279,
                "PatchRowcode": "+          description: >-"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1280,
                "PatchRowcode": "+            Deletes a SSH Tunnel."
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1281,
                "PatchRowcode": "+          parameters:"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1282,
                "PatchRowcode": "+          - in: path"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1283,
                "PatchRowcode": "+            schema:"
            },
            "114": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1284,
                "PatchRowcode": "+              type: integer"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1285,
                "PatchRowcode": "+            name: pk"
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1286,
                "PatchRowcode": "+          responses:"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1287,
                "PatchRowcode": "+            200:"
            },
            "118": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1288,
                "PatchRowcode": "+              description: SSH Tunnel deleted"
            },
            "119": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1289,
                "PatchRowcode": "+              content:"
            },
            "120": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1290,
                "PatchRowcode": "+                application/json:"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1291,
                "PatchRowcode": "+                  schema:"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1292,
                "PatchRowcode": "+                    type: object"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1293,
                "PatchRowcode": "+                    properties:"
            },
            "124": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1294,
                "PatchRowcode": "+                      message:"
            },
            "125": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1295,
                "PatchRowcode": "+                        type: string"
            },
            "126": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1296,
                "PatchRowcode": "+            401:"
            },
            "127": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1297,
                "PatchRowcode": "+              $ref: '#/components/responses/401'"
            },
            "128": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1298,
                "PatchRowcode": "+            403:"
            },
            "129": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1299,
                "PatchRowcode": "+              $ref: '#/components/responses/403'"
            },
            "130": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1300,
                "PatchRowcode": "+            404:"
            },
            "131": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1301,
                "PatchRowcode": "+              $ref: '#/components/responses/404'"
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1302,
                "PatchRowcode": "+            422:"
            },
            "133": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1303,
                "PatchRowcode": "+              $ref: '#/components/responses/422'"
            },
            "134": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1304,
                "PatchRowcode": "+            500:"
            },
            "135": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1305,
                "PatchRowcode": "+              $ref: '#/components/responses/500'"
            },
            "136": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1306,
                "PatchRowcode": "+        \"\"\""
            },
            "137": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1307,
                "PatchRowcode": "+        try:"
            },
            "138": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1308,
                "PatchRowcode": "+            DeleteSSHTunnelCommand(pk).run()"
            },
            "139": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1309,
                "PatchRowcode": "+            return self.response(200, message=\"OK\")"
            },
            "140": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1310,
                "PatchRowcode": "+        except SSHTunnelNotFoundError:"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1311,
                "PatchRowcode": "+            return self.response_404()"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1312,
                "PatchRowcode": "+        except SSHTunnelDeleteFailedError as ex:"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1313,
                "PatchRowcode": "+            logger.error("
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1314,
                "PatchRowcode": "+                \"Error deleting SSH Tunnel %s: %s\","
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1315,
                "PatchRowcode": "+                self.__class__.__name__,"
            },
            "146": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1316,
                "PatchRowcode": "+                str(ex),"
            },
            "147": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1317,
                "PatchRowcode": "+                exc_info=True,"
            },
            "148": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1318,
                "PatchRowcode": "+            )"
            },
            "149": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1319,
                "PatchRowcode": "+            return self.response_422(message=str(ex))"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "import json",
            "import logging",
            "from datetime import datetime",
            "from io import BytesIO",
            "from typing import Any, cast, Dict, List, Optional",
            "from zipfile import is_zipfile, ZipFile",
            "",
            "from flask import request, Response, send_file",
            "from flask_appbuilder.api import expose, protect, rison, safe",
            "from flask_appbuilder.models.sqla.interface import SQLAInterface",
            "from marshmallow import ValidationError",
            "from sqlalchemy.exc import NoSuchTableError, OperationalError, SQLAlchemyError",
            "",
            "from superset import app, event_logger",
            "from superset.commands.importers.exceptions import (",
            "    IncorrectFormatError,",
            "    NoValidFilesFoundError,",
            ")",
            "from superset.commands.importers.v1.utils import get_contents_from_bundle",
            "from superset.constants import MODEL_API_RW_METHOD_PERMISSION_MAP, RouteMethod",
            "from superset.databases.commands.create import CreateDatabaseCommand",
            "from superset.databases.commands.delete import DeleteDatabaseCommand",
            "from superset.databases.commands.exceptions import (",
            "    DatabaseConnectionFailedError,",
            "    DatabaseCreateFailedError,",
            "    DatabaseDeleteDatasetsExistFailedError,",
            "    DatabaseDeleteFailedError,",
            "    DatabaseInvalidError,",
            "    DatabaseNotFoundError,",
            "    DatabaseUpdateFailedError,",
            "    InvalidParametersError,",
            ")",
            "from superset.databases.commands.export import ExportDatabasesCommand",
            "from superset.databases.commands.importers.dispatcher import ImportDatabasesCommand",
            "from superset.databases.commands.test_connection import TestConnectionDatabaseCommand",
            "from superset.databases.commands.update import UpdateDatabaseCommand",
            "from superset.databases.commands.validate import ValidateDatabaseParametersCommand",
            "from superset.databases.commands.validate_sql import ValidateSQLCommand",
            "from superset.databases.dao import DatabaseDAO",
            "from superset.databases.decorators import check_datasource_access",
            "from superset.databases.filters import DatabaseFilter, DatabaseUploadEnabledFilter",
            "from superset.databases.schemas import (",
            "    database_schemas_query_schema,",
            "    DatabaseFunctionNamesResponse,",
            "    DatabasePostSchema,",
            "    DatabasePutSchema,",
            "    DatabaseRelatedObjectsResponse,",
            "    DatabaseTestConnectionSchema,",
            "    DatabaseValidateParametersSchema,",
            "    get_export_ids_schema,",
            "    SchemasResponseSchema,",
            "    SelectStarResponseSchema,",
            "    TableExtraMetadataResponseSchema,",
            "    TableMetadataResponseSchema,",
            "    ValidateSQLRequest,",
            "    ValidateSQLResponse,",
            ")",
            "from superset.databases.utils import get_table_metadata",
            "from superset.db_engine_specs import get_available_engine_specs",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorsException, SupersetException",
            "from superset.extensions import security_manager",
            "from superset.models.core import Database",
            "from superset.superset_typing import FlaskResponse",
            "from superset.utils.core import error_msg_from_exception, parse_js_uri_path_item",
            "from superset.views.base import json_errors_response",
            "from superset.views.base_api import (",
            "    BaseSupersetModelRestApi,",
            "    requires_form_data,",
            "    requires_json,",
            "    statsd_metrics,",
            ")",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DatabaseRestApi(BaseSupersetModelRestApi):",
            "    datamodel = SQLAInterface(Database)",
            "",
            "    include_route_methods = RouteMethod.REST_MODEL_VIEW_CRUD_SET | {",
            "        RouteMethod.EXPORT,",
            "        RouteMethod.IMPORT,",
            "        \"table_metadata\",",
            "        \"table_extra_metadata\",",
            "        \"select_star\",",
            "        \"schemas\",",
            "        \"test_connection\",",
            "        \"related_objects\",",
            "        \"function_names\",",
            "        \"available\",",
            "        \"validate_parameters\",",
            "        \"validate_sql\",",
            "    }",
            "    resource_name = \"database\"",
            "    class_permission_name = \"Database\"",
            "    method_permission_name = MODEL_API_RW_METHOD_PERMISSION_MAP",
            "    allow_browser_login = True",
            "    base_filters = [[\"id\", DatabaseFilter, lambda: []]]",
            "    show_columns = [",
            "        \"id\",",
            "        \"uuid\",",
            "        \"database_name\",",
            "        \"cache_timeout\",",
            "        \"expose_in_sqllab\",",
            "        \"allow_run_async\",",
            "        \"allow_file_upload\",",
            "        \"configuration_method\",",
            "        \"allow_ctas\",",
            "        \"allow_cvas\",",
            "        \"allow_dml\",",
            "        \"backend\",",
            "        \"driver\",",
            "        \"force_ctas_schema\",",
            "        \"impersonate_user\",",
            "        \"masked_encrypted_extra\",",
            "        \"extra\",",
            "        \"parameters\",",
            "        \"parameters_schema\",",
            "        \"server_cert\",",
            "        \"sqlalchemy_uri\",",
            "        \"is_managed_externally\",",
            "        \"engine_information\",",
            "    ]",
            "    list_columns = [",
            "        \"allow_file_upload\",",
            "        \"allow_ctas\",",
            "        \"allow_cvas\",",
            "        \"allow_dml\",",
            "        \"allow_run_async\",",
            "        \"allows_cost_estimate\",",
            "        \"allows_subquery\",",
            "        \"allows_virtual_table_explore\",",
            "        \"backend\",",
            "        \"changed_on\",",
            "        \"changed_on_delta_humanized\",",
            "        \"created_by.first_name\",",
            "        \"created_by.last_name\",",
            "        \"database_name\",",
            "        \"explore_database_id\",",
            "        \"expose_in_sqllab\",",
            "        \"extra\",",
            "        \"force_ctas_schema\",",
            "        \"id\",",
            "        \"uuid\",",
            "        \"disable_data_preview\",",
            "        \"engine_information\",",
            "    ]",
            "    add_columns = [",
            "        \"database_name\",",
            "        \"sqlalchemy_uri\",",
            "        \"cache_timeout\",",
            "        \"expose_in_sqllab\",",
            "        \"allow_run_async\",",
            "        \"allow_file_upload\",",
            "        \"allow_ctas\",",
            "        \"allow_cvas\",",
            "        \"allow_dml\",",
            "        \"configuration_method\",",
            "        \"force_ctas_schema\",",
            "        \"impersonate_user\",",
            "        \"extra\",",
            "        \"encrypted_extra\",",
            "        \"server_cert\",",
            "    ]",
            "",
            "    edit_columns = add_columns",
            "",
            "    search_filters = {\"allow_file_upload\": [DatabaseUploadEnabledFilter]}",
            "",
            "    list_select_columns = list_columns + [\"extra\", \"sqlalchemy_uri\", \"password\"]",
            "    order_columns = [",
            "        \"allow_file_upload\",",
            "        \"allow_dml\",",
            "        \"allow_run_async\",",
            "        \"changed_on\",",
            "        \"changed_on_delta_humanized\",",
            "        \"created_by.first_name\",",
            "        \"database_name\",",
            "        \"expose_in_sqllab\",",
            "    ]",
            "    # Removes the local limit for the page size",
            "    max_page_size = -1",
            "    add_model_schema = DatabasePostSchema()",
            "    edit_model_schema = DatabasePutSchema()",
            "",
            "    apispec_parameter_schemas = {",
            "        \"database_schemas_query_schema\": database_schemas_query_schema,",
            "        \"get_export_ids_schema\": get_export_ids_schema,",
            "    }",
            "",
            "    openapi_spec_tag = \"Database\"",
            "    openapi_spec_component_schemas = (",
            "        DatabaseFunctionNamesResponse,",
            "        DatabaseRelatedObjectsResponse,",
            "        DatabaseTestConnectionSchema,",
            "        DatabaseValidateParametersSchema,",
            "        TableExtraMetadataResponseSchema,",
            "        TableMetadataResponseSchema,",
            "        SelectStarResponseSchema,",
            "        SchemasResponseSchema,",
            "        ValidateSQLRequest,",
            "        ValidateSQLResponse,",
            "    )",
            "",
            "    @expose(\"/\", methods=[\"POST\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.post\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def post(self) -> FlaskResponse:",
            "        \"\"\"Creates a new Database",
            "        ---",
            "        post:",
            "          description: >-",
            "            Create a new Database.",
            "          requestBody:",
            "            description: Database schema",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/{{self.__class__.__name__}}.post'",
            "          responses:",
            "            201:",
            "              description: Database added",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      id:",
            "                        type: number",
            "                      result:",
            "                        $ref: '#/components/schemas/{{self.__class__.__name__}}.post'",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            item = self.add_model_schema.load(request.json)",
            "        # This validates custom Schema with custom validations",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        try:",
            "            new_model = CreateDatabaseCommand(item).run()",
            "            # Return censored version for sqlalchemy URI",
            "            item[\"sqlalchemy_uri\"] = new_model.sqlalchemy_uri",
            "            item[\"expose_in_sqllab\"] = new_model.expose_in_sqllab",
            "",
            "            # If parameters are available return them in the payload",
            "            if new_model.parameters:",
            "                item[\"parameters\"] = new_model.parameters",
            "",
            "            if new_model.driver:",
            "                item[\"driver\"] = new_model.driver",
            "",
            "            return self.response(201, id=new_model.id, result=item)",
            "        except DatabaseInvalidError as ex:",
            "            return self.response_422(message=ex.normalized_messages())",
            "        except DatabaseConnectionFailedError as ex:",
            "            return self.response_422(message=str(ex))",
            "        except SupersetErrorsException as ex:",
            "            return json_errors_response(errors=ex.errors, status=ex.status)",
            "        except DatabaseCreateFailedError as ex:",
            "            logger.error(",
            "                \"Error creating model %s: %s\",",
            "                self.__class__.__name__,",
            "                str(ex),",
            "                exc_info=True,",
            "            )",
            "            return self.response_422(message=str(ex))",
            "        except SupersetException as ex:",
            "            return self.response(ex.status, message=ex.message)",
            "",
            "    @expose(\"/<int:pk>\", methods=[\"PUT\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.put\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def put(self, pk: int) -> Response:",
            "        \"\"\"Changes a Database",
            "        ---",
            "        put:",
            "          description: >-",
            "            Changes a Database.",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "          requestBody:",
            "            description: Database schema",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/{{self.__class__.__name__}}.put'",
            "          responses:",
            "            200:",
            "              description: Database changed",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      id:",
            "                        type: number",
            "                      result:",
            "                        $ref: '#/components/schemas/{{self.__class__.__name__}}.put'",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            403:",
            "              $ref: '#/components/responses/403'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            item = self.edit_model_schema.load(request.json)",
            "        # This validates custom Schema with custom validations",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        try:",
            "            changed_model = UpdateDatabaseCommand(pk, item).run()",
            "            # Return censored version for sqlalchemy URI",
            "            item[\"sqlalchemy_uri\"] = changed_model.sqlalchemy_uri",
            "            if changed_model.parameters:",
            "                item[\"parameters\"] = changed_model.parameters",
            "            return self.response(200, id=changed_model.id, result=item)",
            "        except DatabaseNotFoundError:",
            "            return self.response_404()",
            "        except DatabaseInvalidError as ex:",
            "            return self.response_422(message=ex.normalized_messages())",
            "        except DatabaseConnectionFailedError as ex:",
            "            return self.response_422(message=str(ex))",
            "        except DatabaseUpdateFailedError as ex:",
            "            logger.error(",
            "                \"Error updating model %s: %s\",",
            "                self.__class__.__name__,",
            "                str(ex),",
            "                exc_info=True,",
            "            )",
            "            return self.response_422(message=str(ex))",
            "",
            "    @expose(\"/<int:pk>\", methods=[\"DELETE\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\" f\".delete\",",
            "        log_to_statsd=False,",
            "    )",
            "    def delete(self, pk: int) -> Response:",
            "        \"\"\"Deletes a Database",
            "        ---",
            "        delete:",
            "          description: >-",
            "            Deletes a Database.",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "          responses:",
            "            200:",
            "              description: Database deleted",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            403:",
            "              $ref: '#/components/responses/403'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            DeleteDatabaseCommand(pk).run()",
            "            return self.response(200, message=\"OK\")",
            "        except DatabaseNotFoundError:",
            "            return self.response_404()",
            "        except DatabaseDeleteDatasetsExistFailedError as ex:",
            "            return self.response_422(message=str(ex))",
            "        except DatabaseDeleteFailedError as ex:",
            "            logger.error(",
            "                \"Error deleting model %s: %s\",",
            "                self.__class__.__name__,",
            "                str(ex),",
            "                exc_info=True,",
            "            )",
            "            return self.response_422(message=str(ex))",
            "",
            "    @expose(\"/<int:pk>/schemas/\")",
            "    @protect()",
            "    @safe",
            "    @rison(database_schemas_query_schema)",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\" f\".schemas\",",
            "        log_to_statsd=False,",
            "    )",
            "    def schemas(self, pk: int, **kwargs: Any) -> FlaskResponse:",
            "        \"\"\"Get all schemas from a database",
            "        ---",
            "        get:",
            "          description: Get all schemas from a database",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: query",
            "            name: q",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/database_schemas_query_schema'",
            "          responses:",
            "            200:",
            "              description: A List of all schemas from the database",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/SchemasResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        database = self.datamodel.get(pk, self._base_filters)",
            "        if not database:",
            "            return self.response_404()",
            "        try:",
            "            schemas = database.get_all_schema_names(",
            "                cache=database.schema_cache_enabled,",
            "                cache_timeout=database.schema_cache_timeout,",
            "                force=kwargs[\"rison\"].get(\"force\", False),",
            "            )",
            "            schemas = security_manager.get_schemas_accessible_by_user(database, schemas)",
            "            return self.response(200, result=schemas)",
            "        except OperationalError:",
            "            return self.response(",
            "                500, message=\"There was an error connecting to the database\"",
            "            )",
            "        except SupersetException as ex:",
            "            return self.response(ex.status, message=ex.message)",
            "",
            "    @expose(\"/<int:pk>/table/<table_name>/<schema_name>/\", methods=[\"GET\"])",
            "    @protect()",
            "    @check_datasource_access",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".table_metadata\",",
            "        log_to_statsd=False,",
            "    )",
            "    def table_metadata(",
            "        self, database: Database, table_name: str, schema_name: str",
            "    ) -> FlaskResponse:",
            "        \"\"\"Table schema info",
            "        ---",
            "        get:",
            "          description: Get database table metadata",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: table_name",
            "            description: Table name",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: schema_name",
            "            description: Table schema",
            "          responses:",
            "            200:",
            "              description: Table metadata information",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/TableMetadataResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        self.incr_stats(\"init\", self.table_metadata.__name__)",
            "        try:",
            "            table_info = get_table_metadata(database, table_name, schema_name)",
            "        except SQLAlchemyError as ex:",
            "            self.incr_stats(\"error\", self.table_metadata.__name__)",
            "            return self.response_422(error_msg_from_exception(ex))",
            "        except SupersetException as ex:",
            "            return self.response(ex.status, message=ex.message)",
            "",
            "        self.incr_stats(\"success\", self.table_metadata.__name__)",
            "        return self.response(200, **table_info)",
            "",
            "    @expose(\"/<int:pk>/table_extra/<table_name>/<schema_name>/\", methods=[\"GET\"])",
            "    @protect()",
            "    @check_datasource_access",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".table_extra_metadata\",",
            "        log_to_statsd=False,",
            "    )",
            "    def table_extra_metadata(",
            "        self, database: Database, table_name: str, schema_name: str",
            "    ) -> FlaskResponse:",
            "        \"\"\"Table schema info",
            "        ---",
            "        get:",
            "          summary: >-",
            "            Get table extra metadata",
            "          description: >-",
            "            Response depends on each DB engine spec normally focused on partitions",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: table_name",
            "            description: Table name",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: schema_name",
            "            description: Table schema",
            "          responses:",
            "            200:",
            "              description: Table extra metadata information",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/TableExtraMetadataResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        self.incr_stats(\"init\", self.table_metadata.__name__)",
            "",
            "        parsed_schema = parse_js_uri_path_item(schema_name, eval_undefined=True)",
            "        table_name = cast(str, parse_js_uri_path_item(table_name))",
            "        payload = database.db_engine_spec.extra_table_metadata(",
            "            database, table_name, parsed_schema",
            "        )",
            "        return self.response(200, **payload)",
            "",
            "    @expose(\"/<int:pk>/select_star/<table_name>/\", methods=[\"GET\"])",
            "    @expose(\"/<int:pk>/select_star/<table_name>/<schema_name>/\", methods=[\"GET\"])",
            "    @protect()",
            "    @check_datasource_access",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.select_star\",",
            "        log_to_statsd=False,",
            "    )",
            "    def select_star(",
            "        self, database: Database, table_name: str, schema_name: Optional[str] = None",
            "    ) -> FlaskResponse:",
            "        \"\"\"Table schema info",
            "        ---",
            "        get:",
            "          description: Get database select star for table",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: table_name",
            "            description: Table name",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: schema_name",
            "            description: Table schema",
            "          responses:",
            "            200:",
            "              description: SQL statement for a select star for table",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/SelectStarResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        self.incr_stats(\"init\", self.select_star.__name__)",
            "        try:",
            "            result = database.select_star(",
            "                table_name, schema_name, latest_partition=True, show_cols=True",
            "            )",
            "        except NoSuchTableError:",
            "            self.incr_stats(\"error\", self.select_star.__name__)",
            "            return self.response(404, message=\"Table not found on the database\")",
            "        self.incr_stats(\"success\", self.select_star.__name__)",
            "        return self.response(200, result=result)",
            "",
            "    @expose(\"/test_connection/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".test_connection\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def test_connection(self) -> FlaskResponse:",
            "        \"\"\"Tests a database connection",
            "        ---",
            "        post:",
            "          description: >-",
            "            Tests a database connection",
            "          requestBody:",
            "            description: Database schema",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: \"#/components/schemas/DatabaseTestConnectionSchema\"",
            "          responses:",
            "            200:",
            "              description: Database Test Connection",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            item = DatabaseTestConnectionSchema().load(request.json)",
            "        # This validates custom Schema with custom validations",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        TestConnectionDatabaseCommand(item).run()",
            "        return self.response(200, message=\"OK\")",
            "",
            "    @expose(\"/<int:pk>/related_objects/\", methods=[\"GET\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".related_objects\",",
            "        log_to_statsd=False,",
            "    )",
            "    def related_objects(self, pk: int) -> Response:",
            "        \"\"\"Get charts and dashboards count associated to a database",
            "        ---",
            "        get:",
            "          description:",
            "            Get charts and dashboards count associated to a database",
            "          parameters:",
            "          - in: path",
            "            name: pk",
            "            schema:",
            "              type: integer",
            "          responses:",
            "            200:",
            "              description: Query result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/DatabaseRelatedObjectsResponse\"",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        database = DatabaseDAO.find_by_id(pk)",
            "        if not database:",
            "            return self.response_404()",
            "        data = DatabaseDAO.get_related_objects(pk)",
            "        charts = [",
            "            {",
            "                \"id\": chart.id,",
            "                \"slice_name\": chart.slice_name,",
            "                \"viz_type\": chart.viz_type,",
            "            }",
            "            for chart in data[\"charts\"]",
            "        ]",
            "        dashboards = [",
            "            {",
            "                \"id\": dashboard.id,",
            "                \"json_metadata\": dashboard.json_metadata,",
            "                \"slug\": dashboard.slug,",
            "                \"title\": dashboard.dashboard_title,",
            "            }",
            "            for dashboard in data[\"dashboards\"]",
            "        ]",
            "        sqllab_tab_states = [",
            "            {\"id\": tab_state.id, \"label\": tab_state.label, \"active\": tab_state.active}",
            "            for tab_state in data[\"sqllab_tab_states\"]",
            "        ]",
            "        return self.response(",
            "            200,",
            "            charts={\"count\": len(charts), \"result\": charts},",
            "            dashboards={\"count\": len(dashboards), \"result\": dashboards},",
            "            sqllab_tab_states={",
            "                \"count\": len(sqllab_tab_states),",
            "                \"result\": sqllab_tab_states,",
            "            },",
            "        )",
            "",
            "    @expose(\"/<int:pk>/validate_sql/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.validate_sql\",",
            "        log_to_statsd=False,",
            "    )",
            "    def validate_sql(self, pk: int) -> FlaskResponse:",
            "        \"\"\"",
            "        ---",
            "        post:",
            "          summary: >-",
            "            Validates that arbitrary sql is acceptable for the given database",
            "          description: >-",
            "            Validates arbitrary SQL.",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "          requestBody:",
            "            description: Validate SQL request",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/ValidateSQLRequest'",
            "          responses:",
            "            200:",
            "              description: Validation result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      result:",
            "                        description: >-",
            "                          A List of SQL errors found on the statement",
            "                        type: array",
            "                        items:",
            "                          $ref: '#/components/schemas/ValidateSQLResponse'",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            sql_request = ValidateSQLRequest().load(request.json)",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        try:",
            "            validator_errors = ValidateSQLCommand(pk, sql_request).run()",
            "            return self.response(200, result=validator_errors)",
            "        except DatabaseNotFoundError:",
            "            return self.response_404()",
            "",
            "    @expose(\"/export/\", methods=[\"GET\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @rison(get_export_ids_schema)",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.export\",",
            "        log_to_statsd=False,",
            "    )",
            "    def export(self, **kwargs: Any) -> Response:",
            "        \"\"\"Export database(s) with associated datasets",
            "        ---",
            "        get:",
            "          description: Download database(s) and associated dataset(s) as a zip file",
            "          parameters:",
            "          - in: query",
            "            name: q",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/get_export_ids_schema'",
            "          responses:",
            "            200:",
            "              description: A zip file with database(s) and dataset(s) as YAML",
            "              content:",
            "                application/zip:",
            "                  schema:",
            "                    type: string",
            "                    format: binary",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        token = request.args.get(\"token\")",
            "        requested_ids = kwargs[\"rison\"]",
            "        timestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")",
            "        root = f\"database_export_{timestamp}\"",
            "        filename = f\"{root}.zip\"",
            "",
            "        buf = BytesIO()",
            "        with ZipFile(buf, \"w\") as bundle:",
            "            try:",
            "                for file_name, file_content in ExportDatabasesCommand(",
            "                    requested_ids",
            "                ).run():",
            "                    with bundle.open(f\"{root}/{file_name}\", \"w\") as fp:",
            "                        fp.write(file_content.encode())",
            "            except DatabaseNotFoundError:",
            "                return self.response_404()",
            "        buf.seek(0)",
            "",
            "        response = send_file(",
            "            buf,",
            "            mimetype=\"application/zip\",",
            "            as_attachment=True,",
            "            attachment_filename=filename,",
            "        )",
            "        if token:",
            "            response.set_cookie(token, \"done\", max_age=600)",
            "        return response",
            "",
            "    @expose(\"/import/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.import_\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_form_data",
            "    def import_(self) -> Response:",
            "        \"\"\"Import database(s) with associated datasets",
            "        ---",
            "        post:",
            "          requestBody:",
            "            required: true",
            "            content:",
            "              multipart/form-data:",
            "                schema:",
            "                  type: object",
            "                  properties:",
            "                    formData:",
            "                      description: upload file (ZIP)",
            "                      type: string",
            "                      format: binary",
            "                    passwords:",
            "                      description: >-",
            "                        JSON map of passwords for each featured database in the",
            "                        ZIP file. If the ZIP includes a database config in the path",
            "                        `databases/MyDatabase.yaml`, the password should be provided",
            "                        in the following format:",
            "                        `{\"databases/MyDatabase.yaml\": \"my_password\"}`.",
            "                      type: string",
            "                    overwrite:",
            "                      description: overwrite existing databases?",
            "                      type: boolean",
            "          responses:",
            "            200:",
            "              description: Database import result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        upload = request.files.get(\"formData\")",
            "        if not upload:",
            "            return self.response_400()",
            "        if not is_zipfile(upload):",
            "            raise IncorrectFormatError(\"Not a ZIP file\")",
            "        with ZipFile(upload) as bundle:",
            "            contents = get_contents_from_bundle(bundle)",
            "",
            "        if not contents:",
            "            raise NoValidFilesFoundError()",
            "",
            "        passwords = (",
            "            json.loads(request.form[\"passwords\"])",
            "            if \"passwords\" in request.form",
            "            else None",
            "        )",
            "        overwrite = request.form.get(\"overwrite\") == \"true\"",
            "",
            "        command = ImportDatabasesCommand(",
            "            contents, passwords=passwords, overwrite=overwrite",
            "        )",
            "        command.run()",
            "        return self.response(200, message=\"OK\")",
            "",
            "    @expose(\"/<int:pk>/function_names/\", methods=[\"GET\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".function_names\",",
            "        log_to_statsd=False,",
            "    )",
            "    def function_names(self, pk: int) -> Response:",
            "        \"\"\"Get function names supported by a database",
            "        ---",
            "        get:",
            "          description:",
            "            Get function names supported by a database",
            "          parameters:",
            "          - in: path",
            "            name: pk",
            "            schema:",
            "              type: integer",
            "          responses:",
            "            200:",
            "              description: Query result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/DatabaseFunctionNamesResponse\"",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        database = DatabaseDAO.find_by_id(pk)",
            "        if not database:",
            "            return self.response_404()",
            "        return self.response(",
            "            200,",
            "            function_names=database.function_names,",
            "        )",
            "",
            "    @expose(\"/available/\", methods=[\"GET\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\" f\".available\",",
            "        log_to_statsd=False,",
            "    )",
            "    def available(self) -> Response:",
            "        \"\"\"Return names of databases currently available",
            "        ---",
            "        get:",
            "          description:",
            "            Get names of databases currently available",
            "          responses:",
            "            200:",
            "              description: Database names",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: array",
            "                    items:",
            "                      type: object",
            "                      properties:",
            "                        name:",
            "                          description: Name of the database",
            "                          type: string",
            "                        engine:",
            "                          description: Name of the SQLAlchemy engine",
            "                          type: string",
            "                        available_drivers:",
            "                          description: Installed drivers for the engine",
            "                          type: array",
            "                          items:",
            "                            type: string",
            "                        default_driver:",
            "                          description: Default driver for the engine",
            "                          type: string",
            "                        preferred:",
            "                          description: Is the database preferred?",
            "                          type: boolean",
            "                        sqlalchemy_uri_placeholder:",
            "                          description: Example placeholder for the SQLAlchemy URI",
            "                          type: string",
            "                        parameters:",
            "                          description: JSON schema defining the needed parameters",
            "                          type: object",
            "                        engine_information:",
            "                          description: Dict with public properties form the DB Engine",
            "                          type: object",
            "                          properties:",
            "                            supports_file_upload:",
            "                              description: Whether the engine supports file uploads",
            "                              type: boolean",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        preferred_databases: List[str] = app.config.get(\"PREFERRED_DATABASES\", [])",
            "        available_databases = []",
            "        for engine_spec, drivers in get_available_engine_specs().items():",
            "            if not drivers:",
            "                continue",
            "",
            "            payload: Dict[str, Any] = {",
            "                \"name\": engine_spec.engine_name,",
            "                \"engine\": engine_spec.engine,",
            "                \"available_drivers\": sorted(drivers),",
            "                \"preferred\": engine_spec.engine_name in preferred_databases,",
            "                \"engine_information\": engine_spec.get_public_information(),",
            "            }",
            "",
            "            if engine_spec.default_driver:",
            "                payload[\"default_driver\"] = engine_spec.default_driver",
            "",
            "            # show configuration parameters for DBs that support it",
            "            if (",
            "                hasattr(engine_spec, \"parameters_json_schema\")",
            "                and hasattr(engine_spec, \"sqlalchemy_uri_placeholder\")",
            "                and getattr(engine_spec, \"default_driver\") in drivers",
            "            ):",
            "                payload[",
            "                    \"parameters\"",
            "                ] = engine_spec.parameters_json_schema()  # type: ignore",
            "                payload[",
            "                    \"sqlalchemy_uri_placeholder\"",
            "                ] = engine_spec.sqlalchemy_uri_placeholder  # type: ignore",
            "",
            "            available_databases.append(payload)",
            "",
            "        # sort preferred first",
            "        response = sorted(",
            "            (payload for payload in available_databases if payload[\"preferred\"]),",
            "            key=lambda payload: preferred_databases.index(payload[\"name\"]),",
            "        )",
            "",
            "        # add others",
            "        response.extend(",
            "            sorted(",
            "                (",
            "                    payload",
            "                    for payload in available_databases",
            "                    if not payload[\"preferred\"]",
            "                ),",
            "                key=lambda payload: payload[\"name\"],",
            "            )",
            "        )",
            "",
            "        return self.response(200, databases=response)",
            "",
            "    @expose(\"/validate_parameters/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".validate_parameters\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def validate_parameters(self) -> FlaskResponse:",
            "        \"\"\"validates database connection parameters",
            "        ---",
            "        post:",
            "          description: >-",
            "            Validates parameters used to connect to a database",
            "          requestBody:",
            "            description: DB-specific parameters",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: \"#/components/schemas/DatabaseValidateParametersSchema\"",
            "          responses:",
            "            200:",
            "              description: Database Test Connection",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            payload = DatabaseValidateParametersSchema().load(request.json)",
            "        except ValidationError as ex:",
            "            errors = [",
            "                SupersetError(",
            "                    message=\"\\n\".join(messages),",
            "                    error_type=SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [attribute]},",
            "                )",
            "                for attribute, messages in ex.messages.items()",
            "            ]",
            "            raise InvalidParametersError(errors) from ex",
            "",
            "        command = ValidateDatabaseParametersCommand(payload)",
            "        command.run()",
            "        return self.response(200, message=\"OK\")"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "import json",
            "import logging",
            "from datetime import datetime",
            "from io import BytesIO",
            "from typing import Any, cast, Dict, List, Optional",
            "from zipfile import is_zipfile, ZipFile",
            "",
            "from flask import request, Response, send_file",
            "from flask_appbuilder.api import expose, protect, rison, safe",
            "from flask_appbuilder.models.sqla.interface import SQLAInterface",
            "from marshmallow import ValidationError",
            "from sqlalchemy.exc import NoSuchTableError, OperationalError, SQLAlchemyError",
            "",
            "from superset import app, event_logger",
            "from superset.commands.importers.exceptions import (",
            "    IncorrectFormatError,",
            "    NoValidFilesFoundError,",
            ")",
            "from superset.commands.importers.v1.utils import get_contents_from_bundle",
            "from superset.constants import MODEL_API_RW_METHOD_PERMISSION_MAP, RouteMethod",
            "from superset.databases.commands.create import CreateDatabaseCommand",
            "from superset.databases.commands.delete import DeleteDatabaseCommand",
            "from superset.databases.commands.exceptions import (",
            "    DatabaseConnectionFailedError,",
            "    DatabaseCreateFailedError,",
            "    DatabaseDeleteDatasetsExistFailedError,",
            "    DatabaseDeleteFailedError,",
            "    DatabaseInvalidError,",
            "    DatabaseNotFoundError,",
            "    DatabaseUpdateFailedError,",
            "    InvalidParametersError,",
            ")",
            "from superset.databases.commands.export import ExportDatabasesCommand",
            "from superset.databases.commands.importers.dispatcher import ImportDatabasesCommand",
            "from superset.databases.commands.test_connection import TestConnectionDatabaseCommand",
            "from superset.databases.commands.update import UpdateDatabaseCommand",
            "from superset.databases.commands.validate import ValidateDatabaseParametersCommand",
            "from superset.databases.commands.validate_sql import ValidateSQLCommand",
            "from superset.databases.dao import DatabaseDAO",
            "from superset.databases.decorators import check_datasource_access",
            "from superset.databases.filters import DatabaseFilter, DatabaseUploadEnabledFilter",
            "from superset.databases.schemas import (",
            "    database_schemas_query_schema,",
            "    DatabaseFunctionNamesResponse,",
            "    DatabasePostSchema,",
            "    DatabasePutSchema,",
            "    DatabaseRelatedObjectsResponse,",
            "    DatabaseTestConnectionSchema,",
            "    DatabaseValidateParametersSchema,",
            "    get_export_ids_schema,",
            "    SchemasResponseSchema,",
            "    SelectStarResponseSchema,",
            "    TableExtraMetadataResponseSchema,",
            "    TableMetadataResponseSchema,",
            "    ValidateSQLRequest,",
            "    ValidateSQLResponse,",
            ")",
            "from superset.databases.ssh_tunnel.commands.delete import DeleteSSHTunnelCommand",
            "from superset.databases.ssh_tunnel.commands.exceptions import (",
            "    SSHTunnelDeleteFailedError,",
            "    SSHTunnelNotFoundError,",
            ")",
            "from superset.databases.utils import get_table_metadata",
            "from superset.db_engine_specs import get_available_engine_specs",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorsException, SupersetException",
            "from superset.extensions import security_manager",
            "from superset.models.core import Database",
            "from superset.superset_typing import FlaskResponse",
            "from superset.utils.core import error_msg_from_exception, parse_js_uri_path_item",
            "from superset.utils.ssh_tunnel import mask_password_info",
            "from superset.views.base import json_errors_response",
            "from superset.views.base_api import (",
            "    BaseSupersetModelRestApi,",
            "    requires_form_data,",
            "    requires_json,",
            "    statsd_metrics,",
            ")",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DatabaseRestApi(BaseSupersetModelRestApi):",
            "    datamodel = SQLAInterface(Database)",
            "",
            "    include_route_methods = RouteMethod.REST_MODEL_VIEW_CRUD_SET | {",
            "        RouteMethod.EXPORT,",
            "        RouteMethod.IMPORT,",
            "        \"table_metadata\",",
            "        \"table_extra_metadata\",",
            "        \"select_star\",",
            "        \"schemas\",",
            "        \"test_connection\",",
            "        \"related_objects\",",
            "        \"function_names\",",
            "        \"available\",",
            "        \"validate_parameters\",",
            "        \"validate_sql\",",
            "        \"delete_ssh_tunnel\",",
            "    }",
            "    resource_name = \"database\"",
            "    class_permission_name = \"Database\"",
            "    method_permission_name = MODEL_API_RW_METHOD_PERMISSION_MAP",
            "    allow_browser_login = True",
            "    base_filters = [[\"id\", DatabaseFilter, lambda: []]]",
            "    show_columns = [",
            "        \"id\",",
            "        \"uuid\",",
            "        \"database_name\",",
            "        \"cache_timeout\",",
            "        \"expose_in_sqllab\",",
            "        \"allow_run_async\",",
            "        \"allow_file_upload\",",
            "        \"configuration_method\",",
            "        \"allow_ctas\",",
            "        \"allow_cvas\",",
            "        \"allow_dml\",",
            "        \"backend\",",
            "        \"driver\",",
            "        \"force_ctas_schema\",",
            "        \"impersonate_user\",",
            "        \"masked_encrypted_extra\",",
            "        \"extra\",",
            "        \"parameters\",",
            "        \"parameters_schema\",",
            "        \"server_cert\",",
            "        \"sqlalchemy_uri\",",
            "        \"is_managed_externally\",",
            "        \"engine_information\",",
            "    ]",
            "    list_columns = [",
            "        \"allow_file_upload\",",
            "        \"allow_ctas\",",
            "        \"allow_cvas\",",
            "        \"allow_dml\",",
            "        \"allow_run_async\",",
            "        \"allows_cost_estimate\",",
            "        \"allows_subquery\",",
            "        \"allows_virtual_table_explore\",",
            "        \"backend\",",
            "        \"changed_on\",",
            "        \"changed_on_delta_humanized\",",
            "        \"created_by.first_name\",",
            "        \"created_by.last_name\",",
            "        \"database_name\",",
            "        \"explore_database_id\",",
            "        \"expose_in_sqllab\",",
            "        \"extra\",",
            "        \"force_ctas_schema\",",
            "        \"id\",",
            "        \"uuid\",",
            "        \"disable_data_preview\",",
            "        \"engine_information\",",
            "    ]",
            "    add_columns = [",
            "        \"database_name\",",
            "        \"sqlalchemy_uri\",",
            "        \"cache_timeout\",",
            "        \"expose_in_sqllab\",",
            "        \"allow_run_async\",",
            "        \"allow_file_upload\",",
            "        \"allow_ctas\",",
            "        \"allow_cvas\",",
            "        \"allow_dml\",",
            "        \"configuration_method\",",
            "        \"force_ctas_schema\",",
            "        \"impersonate_user\",",
            "        \"extra\",",
            "        \"encrypted_extra\",",
            "        \"server_cert\",",
            "    ]",
            "",
            "    edit_columns = add_columns",
            "",
            "    search_filters = {\"allow_file_upload\": [DatabaseUploadEnabledFilter]}",
            "",
            "    list_select_columns = list_columns + [\"extra\", \"sqlalchemy_uri\", \"password\"]",
            "    order_columns = [",
            "        \"allow_file_upload\",",
            "        \"allow_dml\",",
            "        \"allow_run_async\",",
            "        \"changed_on\",",
            "        \"changed_on_delta_humanized\",",
            "        \"created_by.first_name\",",
            "        \"database_name\",",
            "        \"expose_in_sqllab\",",
            "    ]",
            "    # Removes the local limit for the page size",
            "    max_page_size = -1",
            "    add_model_schema = DatabasePostSchema()",
            "    edit_model_schema = DatabasePutSchema()",
            "",
            "    apispec_parameter_schemas = {",
            "        \"database_schemas_query_schema\": database_schemas_query_schema,",
            "        \"get_export_ids_schema\": get_export_ids_schema,",
            "    }",
            "",
            "    openapi_spec_tag = \"Database\"",
            "    openapi_spec_component_schemas = (",
            "        DatabaseFunctionNamesResponse,",
            "        DatabaseRelatedObjectsResponse,",
            "        DatabaseTestConnectionSchema,",
            "        DatabaseValidateParametersSchema,",
            "        TableExtraMetadataResponseSchema,",
            "        TableMetadataResponseSchema,",
            "        SelectStarResponseSchema,",
            "        SchemasResponseSchema,",
            "        ValidateSQLRequest,",
            "        ValidateSQLResponse,",
            "    )",
            "",
            "    @expose(\"/<int:pk>\", methods=[\"GET\"])",
            "    @protect()",
            "    @safe",
            "    def get(self, pk: int, **kwargs: Any) -> Response:",
            "        \"\"\"Get a database",
            "        ---",
            "        get:",
            "          description: >-",
            "            Get a database",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            description: The database id",
            "            name: pk",
            "          responses:",
            "            200:",
            "              description: Database",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        data = self.get_headless(pk, **kwargs)",
            "        try:",
            "            if ssh_tunnel := DatabaseDAO.get_ssh_tunnel(pk):",
            "                payload = data.json",
            "                payload[\"result\"][\"ssh_tunnel\"] = ssh_tunnel.data",
            "                return payload",
            "            return data",
            "        except SupersetException as ex:",
            "            return self.response(ex.status, message=ex.message)",
            "",
            "    @expose(\"/\", methods=[\"POST\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.post\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def post(self) -> FlaskResponse:",
            "        \"\"\"Creates a new Database",
            "        ---",
            "        post:",
            "          description: >-",
            "            Create a new Database.",
            "          requestBody:",
            "            description: Database schema",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/{{self.__class__.__name__}}.post'",
            "          responses:",
            "            201:",
            "              description: Database added",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      id:",
            "                        type: number",
            "                      result:",
            "                        $ref: '#/components/schemas/{{self.__class__.__name__}}.post'",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            item = self.add_model_schema.load(request.json)",
            "        # This validates custom Schema with custom validations",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        try:",
            "            new_model = CreateDatabaseCommand(item).run()",
            "            # Return censored version for sqlalchemy URI",
            "            item[\"sqlalchemy_uri\"] = new_model.sqlalchemy_uri",
            "            item[\"expose_in_sqllab\"] = new_model.expose_in_sqllab",
            "",
            "            # If parameters are available return them in the payload",
            "            if new_model.parameters:",
            "                item[\"parameters\"] = new_model.parameters",
            "",
            "            if new_model.driver:",
            "                item[\"driver\"] = new_model.driver",
            "",
            "            # Return SSH Tunnel and hide passwords if any",
            "            if item.get(\"ssh_tunnel\"):",
            "                item[\"ssh_tunnel\"] = mask_password_info(",
            "                    new_model.ssh_tunnel  # pylint: disable=no-member",
            "                )",
            "",
            "            return self.response(201, id=new_model.id, result=item)",
            "        except DatabaseInvalidError as ex:",
            "            return self.response_422(message=ex.normalized_messages())",
            "        except DatabaseConnectionFailedError as ex:",
            "            return self.response_422(message=str(ex))",
            "        except SupersetErrorsException as ex:",
            "            return json_errors_response(errors=ex.errors, status=ex.status)",
            "        except DatabaseCreateFailedError as ex:",
            "            logger.error(",
            "                \"Error creating model %s: %s\",",
            "                self.__class__.__name__,",
            "                str(ex),",
            "                exc_info=True,",
            "            )",
            "            return self.response_422(message=str(ex))",
            "        except SupersetException as ex:",
            "            return self.response(ex.status, message=ex.message)",
            "",
            "    @expose(\"/<int:pk>\", methods=[\"PUT\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.put\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def put(self, pk: int) -> Response:",
            "        \"\"\"Changes a Database",
            "        ---",
            "        put:",
            "          description: >-",
            "            Changes a Database.",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "          requestBody:",
            "            description: Database schema",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/{{self.__class__.__name__}}.put'",
            "          responses:",
            "            200:",
            "              description: Database changed",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      id:",
            "                        type: number",
            "                      result:",
            "                        $ref: '#/components/schemas/{{self.__class__.__name__}}.put'",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            403:",
            "              $ref: '#/components/responses/403'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            item = self.edit_model_schema.load(request.json)",
            "        # This validates custom Schema with custom validations",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        try:",
            "            changed_model = UpdateDatabaseCommand(pk, item).run()",
            "            # Return censored version for sqlalchemy URI",
            "            item[\"sqlalchemy_uri\"] = changed_model.sqlalchemy_uri",
            "            if changed_model.parameters:",
            "                item[\"parameters\"] = changed_model.parameters",
            "            # Return SSH Tunnel and hide passwords if any",
            "            if item.get(\"ssh_tunnel\"):",
            "                item[\"ssh_tunnel\"] = mask_password_info(changed_model.ssh_tunnel)",
            "            return self.response(200, id=changed_model.id, result=item)",
            "        except DatabaseNotFoundError:",
            "            return self.response_404()",
            "        except DatabaseInvalidError as ex:",
            "            return self.response_422(message=ex.normalized_messages())",
            "        except DatabaseConnectionFailedError as ex:",
            "            return self.response_422(message=str(ex))",
            "        except DatabaseUpdateFailedError as ex:",
            "            logger.error(",
            "                \"Error updating model %s: %s\",",
            "                self.__class__.__name__,",
            "                str(ex),",
            "                exc_info=True,",
            "            )",
            "            return self.response_422(message=str(ex))",
            "",
            "    @expose(\"/<int:pk>\", methods=[\"DELETE\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\" f\".delete\",",
            "        log_to_statsd=False,",
            "    )",
            "    def delete(self, pk: int) -> Response:",
            "        \"\"\"Deletes a Database",
            "        ---",
            "        delete:",
            "          description: >-",
            "            Deletes a Database.",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "          responses:",
            "            200:",
            "              description: Database deleted",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            403:",
            "              $ref: '#/components/responses/403'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            DeleteDatabaseCommand(pk).run()",
            "            return self.response(200, message=\"OK\")",
            "        except DatabaseNotFoundError:",
            "            return self.response_404()",
            "        except DatabaseDeleteDatasetsExistFailedError as ex:",
            "            return self.response_422(message=str(ex))",
            "        except DatabaseDeleteFailedError as ex:",
            "            logger.error(",
            "                \"Error deleting model %s: %s\",",
            "                self.__class__.__name__,",
            "                str(ex),",
            "                exc_info=True,",
            "            )",
            "            return self.response_422(message=str(ex))",
            "",
            "    @expose(\"/<int:pk>/schemas/\")",
            "    @protect()",
            "    @safe",
            "    @rison(database_schemas_query_schema)",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\" f\".schemas\",",
            "        log_to_statsd=False,",
            "    )",
            "    def schemas(self, pk: int, **kwargs: Any) -> FlaskResponse:",
            "        \"\"\"Get all schemas from a database",
            "        ---",
            "        get:",
            "          description: Get all schemas from a database",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: query",
            "            name: q",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/database_schemas_query_schema'",
            "          responses:",
            "            200:",
            "              description: A List of all schemas from the database",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/SchemasResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        database = self.datamodel.get(pk, self._base_filters)",
            "        if not database:",
            "            return self.response_404()",
            "        try:",
            "            schemas = database.get_all_schema_names(",
            "                cache=database.schema_cache_enabled,",
            "                cache_timeout=database.schema_cache_timeout,",
            "                force=kwargs[\"rison\"].get(\"force\", False),",
            "            )",
            "            schemas = security_manager.get_schemas_accessible_by_user(database, schemas)",
            "            return self.response(200, result=schemas)",
            "        except OperationalError:",
            "            return self.response(",
            "                500, message=\"There was an error connecting to the database\"",
            "            )",
            "        except SupersetException as ex:",
            "            return self.response(ex.status, message=ex.message)",
            "",
            "    @expose(\"/<int:pk>/table/<table_name>/<schema_name>/\", methods=[\"GET\"])",
            "    @protect()",
            "    @check_datasource_access",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".table_metadata\",",
            "        log_to_statsd=False,",
            "    )",
            "    def table_metadata(",
            "        self, database: Database, table_name: str, schema_name: str",
            "    ) -> FlaskResponse:",
            "        \"\"\"Table schema info",
            "        ---",
            "        get:",
            "          description: Get database table metadata",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: table_name",
            "            description: Table name",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: schema_name",
            "            description: Table schema",
            "          responses:",
            "            200:",
            "              description: Table metadata information",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/TableMetadataResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        self.incr_stats(\"init\", self.table_metadata.__name__)",
            "        try:",
            "            table_info = get_table_metadata(database, table_name, schema_name)",
            "        except SQLAlchemyError as ex:",
            "            self.incr_stats(\"error\", self.table_metadata.__name__)",
            "            return self.response_422(error_msg_from_exception(ex))",
            "        except SupersetException as ex:",
            "            return self.response(ex.status, message=ex.message)",
            "",
            "        self.incr_stats(\"success\", self.table_metadata.__name__)",
            "        return self.response(200, **table_info)",
            "",
            "    @expose(\"/<int:pk>/table_extra/<table_name>/<schema_name>/\", methods=[\"GET\"])",
            "    @protect()",
            "    @check_datasource_access",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".table_extra_metadata\",",
            "        log_to_statsd=False,",
            "    )",
            "    def table_extra_metadata(",
            "        self, database: Database, table_name: str, schema_name: str",
            "    ) -> FlaskResponse:",
            "        \"\"\"Table schema info",
            "        ---",
            "        get:",
            "          summary: >-",
            "            Get table extra metadata",
            "          description: >-",
            "            Response depends on each DB engine spec normally focused on partitions",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: table_name",
            "            description: Table name",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: schema_name",
            "            description: Table schema",
            "          responses:",
            "            200:",
            "              description: Table extra metadata information",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/TableExtraMetadataResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        self.incr_stats(\"init\", self.table_metadata.__name__)",
            "",
            "        parsed_schema = parse_js_uri_path_item(schema_name, eval_undefined=True)",
            "        table_name = cast(str, parse_js_uri_path_item(table_name))",
            "        payload = database.db_engine_spec.extra_table_metadata(",
            "            database, table_name, parsed_schema",
            "        )",
            "        return self.response(200, **payload)",
            "",
            "    @expose(\"/<int:pk>/select_star/<table_name>/\", methods=[\"GET\"])",
            "    @expose(\"/<int:pk>/select_star/<table_name>/<schema_name>/\", methods=[\"GET\"])",
            "    @protect()",
            "    @check_datasource_access",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.select_star\",",
            "        log_to_statsd=False,",
            "    )",
            "    def select_star(",
            "        self, database: Database, table_name: str, schema_name: Optional[str] = None",
            "    ) -> FlaskResponse:",
            "        \"\"\"Table schema info",
            "        ---",
            "        get:",
            "          description: Get database select star for table",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "            description: The database id",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: table_name",
            "            description: Table name",
            "          - in: path",
            "            schema:",
            "              type: string",
            "            name: schema_name",
            "            description: Table schema",
            "          responses:",
            "            200:",
            "              description: SQL statement for a select star for table",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/SelectStarResponseSchema\"",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        self.incr_stats(\"init\", self.select_star.__name__)",
            "        try:",
            "            result = database.select_star(",
            "                table_name, schema_name, latest_partition=True, show_cols=True",
            "            )",
            "        except NoSuchTableError:",
            "            self.incr_stats(\"error\", self.select_star.__name__)",
            "            return self.response(404, message=\"Table not found on the database\")",
            "        self.incr_stats(\"success\", self.select_star.__name__)",
            "        return self.response(200, result=result)",
            "",
            "    @expose(\"/test_connection/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".test_connection\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def test_connection(self) -> FlaskResponse:",
            "        \"\"\"Tests a database connection",
            "        ---",
            "        post:",
            "          description: >-",
            "            Tests a database connection",
            "          requestBody:",
            "            description: Database schema",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: \"#/components/schemas/DatabaseTestConnectionSchema\"",
            "          responses:",
            "            200:",
            "              description: Database Test Connection",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            item = DatabaseTestConnectionSchema().load(request.json)",
            "        # This validates custom Schema with custom validations",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        TestConnectionDatabaseCommand(item).run()",
            "        return self.response(200, message=\"OK\")",
            "",
            "    @expose(\"/<int:pk>/related_objects/\", methods=[\"GET\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".related_objects\",",
            "        log_to_statsd=False,",
            "    )",
            "    def related_objects(self, pk: int) -> Response:",
            "        \"\"\"Get charts and dashboards count associated to a database",
            "        ---",
            "        get:",
            "          description:",
            "            Get charts and dashboards count associated to a database",
            "          parameters:",
            "          - in: path",
            "            name: pk",
            "            schema:",
            "              type: integer",
            "          responses:",
            "            200:",
            "              description: Query result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/DatabaseRelatedObjectsResponse\"",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        database = DatabaseDAO.find_by_id(pk)",
            "        if not database:",
            "            return self.response_404()",
            "        data = DatabaseDAO.get_related_objects(pk)",
            "        charts = [",
            "            {",
            "                \"id\": chart.id,",
            "                \"slice_name\": chart.slice_name,",
            "                \"viz_type\": chart.viz_type,",
            "            }",
            "            for chart in data[\"charts\"]",
            "        ]",
            "        dashboards = [",
            "            {",
            "                \"id\": dashboard.id,",
            "                \"json_metadata\": dashboard.json_metadata,",
            "                \"slug\": dashboard.slug,",
            "                \"title\": dashboard.dashboard_title,",
            "            }",
            "            for dashboard in data[\"dashboards\"]",
            "        ]",
            "        sqllab_tab_states = [",
            "            {\"id\": tab_state.id, \"label\": tab_state.label, \"active\": tab_state.active}",
            "            for tab_state in data[\"sqllab_tab_states\"]",
            "        ]",
            "        return self.response(",
            "            200,",
            "            charts={\"count\": len(charts), \"result\": charts},",
            "            dashboards={\"count\": len(dashboards), \"result\": dashboards},",
            "            sqllab_tab_states={",
            "                \"count\": len(sqllab_tab_states),",
            "                \"result\": sqllab_tab_states,",
            "            },",
            "        )",
            "",
            "    @expose(\"/<int:pk>/validate_sql/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.validate_sql\",",
            "        log_to_statsd=False,",
            "    )",
            "    def validate_sql(self, pk: int) -> FlaskResponse:",
            "        \"\"\"",
            "        ---",
            "        post:",
            "          summary: >-",
            "            Validates that arbitrary sql is acceptable for the given database",
            "          description: >-",
            "            Validates arbitrary SQL.",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "          requestBody:",
            "            description: Validate SQL request",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/ValidateSQLRequest'",
            "          responses:",
            "            200:",
            "              description: Validation result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      result:",
            "                        description: >-",
            "                          A List of SQL errors found on the statement",
            "                        type: array",
            "                        items:",
            "                          $ref: '#/components/schemas/ValidateSQLResponse'",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            sql_request = ValidateSQLRequest().load(request.json)",
            "        except ValidationError as error:",
            "            return self.response_400(message=error.messages)",
            "        try:",
            "            validator_errors = ValidateSQLCommand(pk, sql_request).run()",
            "            return self.response(200, result=validator_errors)",
            "        except DatabaseNotFoundError:",
            "            return self.response_404()",
            "",
            "    @expose(\"/export/\", methods=[\"GET\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @rison(get_export_ids_schema)",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.export\",",
            "        log_to_statsd=False,",
            "    )",
            "    def export(self, **kwargs: Any) -> Response:",
            "        \"\"\"Export database(s) with associated datasets",
            "        ---",
            "        get:",
            "          description: Download database(s) and associated dataset(s) as a zip file",
            "          parameters:",
            "          - in: query",
            "            name: q",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: '#/components/schemas/get_export_ids_schema'",
            "          responses:",
            "            200:",
            "              description: A zip file with database(s) and dataset(s) as YAML",
            "              content:",
            "                application/zip:",
            "                  schema:",
            "                    type: string",
            "                    format: binary",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        token = request.args.get(\"token\")",
            "        requested_ids = kwargs[\"rison\"]",
            "        timestamp = datetime.now().strftime(\"%Y%m%dT%H%M%S\")",
            "        root = f\"database_export_{timestamp}\"",
            "        filename = f\"{root}.zip\"",
            "",
            "        buf = BytesIO()",
            "        with ZipFile(buf, \"w\") as bundle:",
            "            try:",
            "                for file_name, file_content in ExportDatabasesCommand(",
            "                    requested_ids",
            "                ).run():",
            "                    with bundle.open(f\"{root}/{file_name}\", \"w\") as fp:",
            "                        fp.write(file_content.encode())",
            "            except DatabaseNotFoundError:",
            "                return self.response_404()",
            "        buf.seek(0)",
            "",
            "        response = send_file(",
            "            buf,",
            "            mimetype=\"application/zip\",",
            "            as_attachment=True,",
            "            attachment_filename=filename,",
            "        )",
            "        if token:",
            "            response.set_cookie(token, \"done\", max_age=600)",
            "        return response",
            "",
            "    @expose(\"/import/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}.import_\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_form_data",
            "    def import_(self) -> Response:",
            "        \"\"\"Import database(s) with associated datasets",
            "        ---",
            "        post:",
            "          requestBody:",
            "            required: true",
            "            content:",
            "              multipart/form-data:",
            "                schema:",
            "                  type: object",
            "                  properties:",
            "                    formData:",
            "                      description: upload file (ZIP)",
            "                      type: string",
            "                      format: binary",
            "                    passwords:",
            "                      description: >-",
            "                        JSON map of passwords for each featured database in the",
            "                        ZIP file. If the ZIP includes a database config in the path",
            "                        `databases/MyDatabase.yaml`, the password should be provided",
            "                        in the following format:",
            "                        `{\"databases/MyDatabase.yaml\": \"my_password\"}`.",
            "                      type: string",
            "                    overwrite:",
            "                      description: overwrite existing databases?",
            "                      type: boolean",
            "          responses:",
            "            200:",
            "              description: Database import result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        upload = request.files.get(\"formData\")",
            "        if not upload:",
            "            return self.response_400()",
            "        if not is_zipfile(upload):",
            "            raise IncorrectFormatError(\"Not a ZIP file\")",
            "        with ZipFile(upload) as bundle:",
            "            contents = get_contents_from_bundle(bundle)",
            "",
            "        if not contents:",
            "            raise NoValidFilesFoundError()",
            "",
            "        passwords = (",
            "            json.loads(request.form[\"passwords\"])",
            "            if \"passwords\" in request.form",
            "            else None",
            "        )",
            "        overwrite = request.form.get(\"overwrite\") == \"true\"",
            "",
            "        command = ImportDatabasesCommand(",
            "            contents, passwords=passwords, overwrite=overwrite",
            "        )",
            "        command.run()",
            "        return self.response(200, message=\"OK\")",
            "",
            "    @expose(\"/<int:pk>/function_names/\", methods=[\"GET\"])",
            "    @protect()",
            "    @safe",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".function_names\",",
            "        log_to_statsd=False,",
            "    )",
            "    def function_names(self, pk: int) -> Response:",
            "        \"\"\"Get function names supported by a database",
            "        ---",
            "        get:",
            "          description:",
            "            Get function names supported by a database",
            "          parameters:",
            "          - in: path",
            "            name: pk",
            "            schema:",
            "              type: integer",
            "          responses:",
            "            200:",
            "              description: Query result",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    $ref: \"#/components/schemas/DatabaseFunctionNamesResponse\"",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        database = DatabaseDAO.find_by_id(pk)",
            "        if not database:",
            "            return self.response_404()",
            "        return self.response(",
            "            200,",
            "            function_names=database.function_names,",
            "        )",
            "",
            "    @expose(\"/available/\", methods=[\"GET\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\" f\".available\",",
            "        log_to_statsd=False,",
            "    )",
            "    def available(self) -> Response:",
            "        \"\"\"Return names of databases currently available",
            "        ---",
            "        get:",
            "          description:",
            "            Get names of databases currently available",
            "          responses:",
            "            200:",
            "              description: Database names",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: array",
            "                    items:",
            "                      type: object",
            "                      properties:",
            "                        name:",
            "                          description: Name of the database",
            "                          type: string",
            "                        engine:",
            "                          description: Name of the SQLAlchemy engine",
            "                          type: string",
            "                        available_drivers:",
            "                          description: Installed drivers for the engine",
            "                          type: array",
            "                          items:",
            "                            type: string",
            "                        default_driver:",
            "                          description: Default driver for the engine",
            "                          type: string",
            "                        preferred:",
            "                          description: Is the database preferred?",
            "                          type: boolean",
            "                        sqlalchemy_uri_placeholder:",
            "                          description: Example placeholder for the SQLAlchemy URI",
            "                          type: string",
            "                        parameters:",
            "                          description: JSON schema defining the needed parameters",
            "                          type: object",
            "                        engine_information:",
            "                          description: Dict with public properties form the DB Engine",
            "                          type: object",
            "                          properties:",
            "                            supports_file_upload:",
            "                              description: Whether the engine supports file uploads",
            "                              type: boolean",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        preferred_databases: List[str] = app.config.get(\"PREFERRED_DATABASES\", [])",
            "        available_databases = []",
            "        for engine_spec, drivers in get_available_engine_specs().items():",
            "            if not drivers:",
            "                continue",
            "",
            "            payload: Dict[str, Any] = {",
            "                \"name\": engine_spec.engine_name,",
            "                \"engine\": engine_spec.engine,",
            "                \"available_drivers\": sorted(drivers),",
            "                \"preferred\": engine_spec.engine_name in preferred_databases,",
            "                \"engine_information\": engine_spec.get_public_information(),",
            "            }",
            "",
            "            if engine_spec.default_driver:",
            "                payload[\"default_driver\"] = engine_spec.default_driver",
            "",
            "            # show configuration parameters for DBs that support it",
            "            if (",
            "                hasattr(engine_spec, \"parameters_json_schema\")",
            "                and hasattr(engine_spec, \"sqlalchemy_uri_placeholder\")",
            "                and getattr(engine_spec, \"default_driver\") in drivers",
            "            ):",
            "                payload[",
            "                    \"parameters\"",
            "                ] = engine_spec.parameters_json_schema()  # type: ignore",
            "                payload[",
            "                    \"sqlalchemy_uri_placeholder\"",
            "                ] = engine_spec.sqlalchemy_uri_placeholder  # type: ignore",
            "",
            "            available_databases.append(payload)",
            "",
            "        # sort preferred first",
            "        response = sorted(",
            "            (payload for payload in available_databases if payload[\"preferred\"]),",
            "            key=lambda payload: preferred_databases.index(payload[\"name\"]),",
            "        )",
            "",
            "        # add others",
            "        response.extend(",
            "            sorted(",
            "                (",
            "                    payload",
            "                    for payload in available_databases",
            "                    if not payload[\"preferred\"]",
            "                ),",
            "                key=lambda payload: payload[\"name\"],",
            "            )",
            "        )",
            "",
            "        return self.response(200, databases=response)",
            "",
            "    @expose(\"/validate_parameters/\", methods=[\"POST\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".validate_parameters\",",
            "        log_to_statsd=False,",
            "    )",
            "    @requires_json",
            "    def validate_parameters(self) -> FlaskResponse:",
            "        \"\"\"validates database connection parameters",
            "        ---",
            "        post:",
            "          description: >-",
            "            Validates parameters used to connect to a database",
            "          requestBody:",
            "            description: DB-specific parameters",
            "            required: true",
            "            content:",
            "              application/json:",
            "                schema:",
            "                  $ref: \"#/components/schemas/DatabaseValidateParametersSchema\"",
            "          responses:",
            "            200:",
            "              description: Database Test Connection",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            400:",
            "              $ref: '#/components/responses/400'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            payload = DatabaseValidateParametersSchema().load(request.json)",
            "        except ValidationError as ex:",
            "            errors = [",
            "                SupersetError(",
            "                    message=\"\\n\".join(messages),",
            "                    error_type=SupersetErrorType.INVALID_PAYLOAD_SCHEMA_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [attribute]},",
            "                )",
            "                for attribute, messages in ex.messages.items()",
            "            ]",
            "            raise InvalidParametersError(errors) from ex",
            "",
            "        command = ValidateDatabaseParametersCommand(payload)",
            "        command.run()",
            "        return self.response(200, message=\"OK\")",
            "",
            "    @expose(\"/<int:pk>/ssh_tunnel/\", methods=[\"DELETE\"])",
            "    @protect()",
            "    @statsd_metrics",
            "    @event_logger.log_this_with_context(",
            "        action=lambda self, *args, **kwargs: f\"{self.__class__.__name__}\"",
            "        f\".delete_ssh_tunnel\",",
            "        log_to_statsd=False,",
            "    )",
            "    def delete_ssh_tunnel(self, pk: int) -> Response:",
            "        \"\"\"Deletes a SSH Tunnel",
            "        ---",
            "        delete:",
            "          description: >-",
            "            Deletes a SSH Tunnel.",
            "          parameters:",
            "          - in: path",
            "            schema:",
            "              type: integer",
            "            name: pk",
            "          responses:",
            "            200:",
            "              description: SSH Tunnel deleted",
            "              content:",
            "                application/json:",
            "                  schema:",
            "                    type: object",
            "                    properties:",
            "                      message:",
            "                        type: string",
            "            401:",
            "              $ref: '#/components/responses/401'",
            "            403:",
            "              $ref: '#/components/responses/403'",
            "            404:",
            "              $ref: '#/components/responses/404'",
            "            422:",
            "              $ref: '#/components/responses/422'",
            "            500:",
            "              $ref: '#/components/responses/500'",
            "        \"\"\"",
            "        try:",
            "            DeleteSSHTunnelCommand(pk).run()",
            "            return self.response(200, message=\"OK\")",
            "        except SSHTunnelNotFoundError:",
            "            return self.response_404()",
            "        except SSHTunnelDeleteFailedError as ex:",
            "            logger.error(",
            "                \"Error deleting SSH Tunnel %s: %s\",",
            "                self.__class__.__name__,",
            "                str(ex),",
            "                exc_info=True,",
            "            )",
            "            return self.response_422(message=str(ex))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "superset.databases.api.DatabaseRestApi.edit_columns",
            "src.saml2.aes.AESCipher",
            "superset.databases.api.DatabaseRestApi.search_filters",
            "superset.databases.api.DatabaseRestApi.apispec_parameter_schemas",
            "superset.databases.api.DatabaseRestApi.list_select_columns",
            "superset.databases.api.DatabaseRestApi.show_columns",
            "superset.databases.api.DatabaseRestApi.order_columns",
            "superset.databases.api.DatabaseRestApi.self",
            "superset.databases.api.DatabaseRestApi.list_columns",
            "superset.databases.api.DatabaseRestApi.add_columns",
            "superset.databases.api.DatabaseRestApi.base_filters"
        ]
    },
    "superset/databases/commands/create.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " )"
            },
            "1": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " from superset.databases.commands.test_connection import TestConnectionDatabaseCommand"
            },
            "2": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " from superset.databases.dao import DatabaseDAO"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+from superset.databases.ssh_tunnel.commands.create import CreateSSHTunnelCommand"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+from superset.databases.ssh_tunnel.commands.exceptions import ("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+    SSHTunnelCreateFailedError,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+    SSHTunnelInvalidError,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+)"
            },
            "8": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " from superset.exceptions import SupersetErrorsException"
            },
            "9": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " from superset.extensions import db, event_logger, security_manager"
            },
            "10": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 76,
                "PatchRowcode": "             database = DatabaseDAO.create(self._properties, commit=False)"
            },
            "12": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "             database.set_sqlalchemy_uri(database.sqlalchemy_uri)"
            },
            "13": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 78,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+            ssh_tunnel = None"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+            if ssh_tunnel_properties := self._properties.get(\"ssh_tunnel\"):"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+                try:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+                    # So database.id is not None"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+                    db.session.flush()"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+                    ssh_tunnel = CreateSSHTunnelCommand("
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+                        database.id, ssh_tunnel_properties"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+                    ).run()"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+                except (SSHTunnelInvalidError, SSHTunnelCreateFailedError) as ex:"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+                    event_logger.log_with_context("
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+                        action=f\"db_creation_failed.{ex.__class__.__name__}\","
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+                        engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+                    )"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+                    # So we can show the original message"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+                    raise ex"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+                except Exception as ex:"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+                    event_logger.log_with_context("
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+                        action=f\"db_creation_failed.{ex.__class__.__name__}\","
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+                        engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+                    )"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                    raise DatabaseCreateFailedError() from ex"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+"
            },
            "36": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "             # adding a new database we always want to force refresh schema list"
            },
            "37": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            schemas = database.get_all_schema_names(cache=False)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+            schemas = database.get_all_schema_names(cache=False, ssh_tunnel=ssh_tunnel)"
            },
            "39": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "             for schema in schemas:"
            },
            "40": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "                 security_manager.add_permission_view_menu("
            },
            "41": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "                     \"schema_access\", security_manager.get_schema_perm(database, schema)"
            },
            "42": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "                 )"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+"
            },
            "44": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "             db.session.commit()"
            },
            "45": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 109,
                "PatchRowcode": "         except DAOCreateFailedError as ex:"
            },
            "46": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "             db.session.rollback()"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "from typing import Any, Dict, List, Optional",
            "",
            "from flask_appbuilder.models.sqla import Model",
            "from marshmallow import ValidationError",
            "",
            "from superset.commands.base import BaseCommand",
            "from superset.dao.exceptions import DAOCreateFailedError",
            "from superset.databases.commands.exceptions import (",
            "    DatabaseConnectionFailedError,",
            "    DatabaseCreateFailedError,",
            "    DatabaseExistsValidationError,",
            "    DatabaseInvalidError,",
            "    DatabaseRequiredFieldValidationError,",
            ")",
            "from superset.databases.commands.test_connection import TestConnectionDatabaseCommand",
            "from superset.databases.dao import DatabaseDAO",
            "from superset.exceptions import SupersetErrorsException",
            "from superset.extensions import db, event_logger, security_manager",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class CreateDatabaseCommand(BaseCommand):",
            "    def __init__(self, data: Dict[str, Any]):",
            "        self._properties = data.copy()",
            "",
            "    def run(self) -> Model:",
            "        self.validate()",
            "",
            "        try:",
            "            # Test connection before starting create transaction",
            "            TestConnectionDatabaseCommand(self._properties).run()",
            "        except SupersetErrorsException as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],",
            "            )",
            "            # So we can show the original message",
            "            raise ex",
            "        except Exception as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],",
            "            )",
            "            raise DatabaseConnectionFailedError() from ex",
            "",
            "        # when creating a new database we don't need to unmask encrypted extra",
            "        self._properties[\"encrypted_extra\"] = self._properties.pop(",
            "            \"masked_encrypted_extra\",",
            "            \"{}\",",
            "        )",
            "",
            "        try:",
            "            database = DatabaseDAO.create(self._properties, commit=False)",
            "            database.set_sqlalchemy_uri(database.sqlalchemy_uri)",
            "",
            "            # adding a new database we always want to force refresh schema list",
            "            schemas = database.get_all_schema_names(cache=False)",
            "            for schema in schemas:",
            "                security_manager.add_permission_view_menu(",
            "                    \"schema_access\", security_manager.get_schema_perm(database, schema)",
            "                )",
            "            db.session.commit()",
            "        except DAOCreateFailedError as ex:",
            "            db.session.rollback()",
            "            event_logger.log_with_context(",
            "                action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            raise DatabaseCreateFailedError() from ex",
            "        return database",
            "",
            "    def validate(self) -> None:",
            "        exceptions: List[ValidationError] = []",
            "        sqlalchemy_uri: Optional[str] = self._properties.get(\"sqlalchemy_uri\")",
            "        database_name: Optional[str] = self._properties.get(\"database_name\")",
            "        if not sqlalchemy_uri:",
            "            exceptions.append(DatabaseRequiredFieldValidationError(\"sqlalchemy_uri\"))",
            "        if not database_name:",
            "            exceptions.append(DatabaseRequiredFieldValidationError(\"database_name\"))",
            "        else:",
            "            # Check database_name uniqueness",
            "            if not DatabaseDAO.validate_uniqueness(database_name):",
            "                exceptions.append(DatabaseExistsValidationError())",
            "        if exceptions:",
            "            exception = DatabaseInvalidError()",
            "            exception.add_list(exceptions)",
            "            event_logger.log_with_context(",
            "                action=\"db_connection_failed.{}.{}\".format(",
            "                    exception.__class__.__name__,",
            "                    \".\".join(exception.get_list_classnames()),",
            "                )",
            "            )",
            "            raise exception"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "from typing import Any, Dict, List, Optional",
            "",
            "from flask_appbuilder.models.sqla import Model",
            "from marshmallow import ValidationError",
            "",
            "from superset.commands.base import BaseCommand",
            "from superset.dao.exceptions import DAOCreateFailedError",
            "from superset.databases.commands.exceptions import (",
            "    DatabaseConnectionFailedError,",
            "    DatabaseCreateFailedError,",
            "    DatabaseExistsValidationError,",
            "    DatabaseInvalidError,",
            "    DatabaseRequiredFieldValidationError,",
            ")",
            "from superset.databases.commands.test_connection import TestConnectionDatabaseCommand",
            "from superset.databases.dao import DatabaseDAO",
            "from superset.databases.ssh_tunnel.commands.create import CreateSSHTunnelCommand",
            "from superset.databases.ssh_tunnel.commands.exceptions import (",
            "    SSHTunnelCreateFailedError,",
            "    SSHTunnelInvalidError,",
            ")",
            "from superset.exceptions import SupersetErrorsException",
            "from superset.extensions import db, event_logger, security_manager",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class CreateDatabaseCommand(BaseCommand):",
            "    def __init__(self, data: Dict[str, Any]):",
            "        self._properties = data.copy()",
            "",
            "    def run(self) -> Model:",
            "        self.validate()",
            "",
            "        try:",
            "            # Test connection before starting create transaction",
            "            TestConnectionDatabaseCommand(self._properties).run()",
            "        except SupersetErrorsException as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],",
            "            )",
            "            # So we can show the original message",
            "            raise ex",
            "        except Exception as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],",
            "            )",
            "            raise DatabaseConnectionFailedError() from ex",
            "",
            "        # when creating a new database we don't need to unmask encrypted extra",
            "        self._properties[\"encrypted_extra\"] = self._properties.pop(",
            "            \"masked_encrypted_extra\",",
            "            \"{}\",",
            "        )",
            "",
            "        try:",
            "            database = DatabaseDAO.create(self._properties, commit=False)",
            "            database.set_sqlalchemy_uri(database.sqlalchemy_uri)",
            "",
            "            ssh_tunnel = None",
            "            if ssh_tunnel_properties := self._properties.get(\"ssh_tunnel\"):",
            "                try:",
            "                    # So database.id is not None",
            "                    db.session.flush()",
            "                    ssh_tunnel = CreateSSHTunnelCommand(",
            "                        database.id, ssh_tunnel_properties",
            "                    ).run()",
            "                except (SSHTunnelInvalidError, SSHTunnelCreateFailedError) as ex:",
            "                    event_logger.log_with_context(",
            "                        action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                        engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],",
            "                    )",
            "                    # So we can show the original message",
            "                    raise ex",
            "                except Exception as ex:",
            "                    event_logger.log_with_context(",
            "                        action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                        engine=self._properties.get(\"sqlalchemy_uri\", \"\").split(\":\")[0],",
            "                    )",
            "                    raise DatabaseCreateFailedError() from ex",
            "",
            "            # adding a new database we always want to force refresh schema list",
            "            schemas = database.get_all_schema_names(cache=False, ssh_tunnel=ssh_tunnel)",
            "            for schema in schemas:",
            "                security_manager.add_permission_view_menu(",
            "                    \"schema_access\", security_manager.get_schema_perm(database, schema)",
            "                )",
            "",
            "            db.session.commit()",
            "        except DAOCreateFailedError as ex:",
            "            db.session.rollback()",
            "            event_logger.log_with_context(",
            "                action=f\"db_creation_failed.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            raise DatabaseCreateFailedError() from ex",
            "        return database",
            "",
            "    def validate(self) -> None:",
            "        exceptions: List[ValidationError] = []",
            "        sqlalchemy_uri: Optional[str] = self._properties.get(\"sqlalchemy_uri\")",
            "        database_name: Optional[str] = self._properties.get(\"database_name\")",
            "        if not sqlalchemy_uri:",
            "            exceptions.append(DatabaseRequiredFieldValidationError(\"sqlalchemy_uri\"))",
            "        if not database_name:",
            "            exceptions.append(DatabaseRequiredFieldValidationError(\"database_name\"))",
            "        else:",
            "            # Check database_name uniqueness",
            "            if not DatabaseDAO.validate_uniqueness(database_name):",
            "                exceptions.append(DatabaseExistsValidationError())",
            "        if exceptions:",
            "            exception = DatabaseInvalidError()",
            "            exception.add_list(exceptions)",
            "            event_logger.log_with_context(",
            "                action=\"db_connection_failed.{}.{}\".format(",
            "                    exception.__class__.__name__,",
            "                    \".\".join(exception.get_list_classnames()),",
            "                )",
            "            )",
            "            raise exception"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "75": [
                "CreateDatabaseCommand",
                "run"
            ]
        },
        "addLocation": [
            "src.saml2.aes.AESCipher"
        ]
    },
    "superset/databases/commands/test_connection.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "     DatabaseTestConnectionUnexpectedError,"
            },
            "1": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " )"
            },
            "2": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " from superset.databases.dao import DatabaseDAO"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+from superset.databases.ssh_tunnel.models import SSHTunnel"
            },
            "4": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " from superset.databases.utils import make_url_safe"
            },
            "5": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " from superset.errors import ErrorLevel, SupersetErrorType"
            },
            "6": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " from superset.exceptions import ("
            },
            "7": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "             database.set_sqlalchemy_uri(uri)"
            },
            "8": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "             database.db_engine_spec.mutate_db_for_connection_test(database)"
            },
            "9": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 93,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+            # Generate tunnel if present in the properties"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+            if ssh_tunnel := self._properties.get(\"ssh_tunnel\"):"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+                ssh_tunnel = SSHTunnel(**ssh_tunnel)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+"
            },
            "14": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "             event_logger.log_with_context("
            },
            "15": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "                 action=\"test_connection_attempt\","
            },
            "16": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 100,
                "PatchRowcode": "                 engine=database.db_engine_spec.__name__,"
            },
            "17": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "                 with closing(engine.raw_connection()) as conn:"
            },
            "18": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "                     return engine.dialect.do_ping(conn)"
            },
            "19": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 106,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            with database.get_sqla_engine_with_context() as engine:"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+            with database.get_sqla_engine_with_context("
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+                override_ssh_tunnel=ssh_tunnel"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+            ) as engine:"
            },
            "24": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "                 try:"
            },
            "25": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "                     alive = func_timeout("
            },
            "26": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "                         app.config[\"TEST_DATABASE_CONNECTION_TIMEOUT\"].total_seconds(),"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "import sqlite3",
            "from contextlib import closing",
            "from typing import Any, Dict, Optional",
            "",
            "from flask import current_app as app",
            "from flask_babel import gettext as _",
            "from func_timeout import func_timeout, FunctionTimedOut",
            "from sqlalchemy.engine import Engine",
            "from sqlalchemy.exc import DBAPIError, NoSuchModuleError",
            "",
            "from superset.commands.base import BaseCommand",
            "from superset.databases.commands.exceptions import (",
            "    DatabaseSecurityUnsafeError,",
            "    DatabaseTestConnectionDriverError,",
            "    DatabaseTestConnectionUnexpectedError,",
            ")",
            "from superset.databases.dao import DatabaseDAO",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetErrorType",
            "from superset.exceptions import (",
            "    SupersetErrorsException,",
            "    SupersetSecurityException,",
            "    SupersetTimeoutException,",
            ")",
            "from superset.extensions import event_logger",
            "from superset.models.core import Database",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class TestConnectionDatabaseCommand(BaseCommand):",
            "    def __init__(self, data: Dict[str, Any]):",
            "        self._properties = data.copy()",
            "        self._model: Optional[Database] = None",
            "",
            "    def run(self) -> None:  # pylint: disable=too-many-statements",
            "        self.validate()",
            "        ex_str = \"\"",
            "        uri = self._properties.get(\"sqlalchemy_uri\", \"\")",
            "        if self._model and uri == self._model.safe_sqlalchemy_uri():",
            "            uri = self._model.sqlalchemy_uri_decrypted",
            "",
            "        # context for error messages",
            "        url = make_url_safe(uri)",
            "        context = {",
            "            \"hostname\": url.host,",
            "            \"password\": url.password,",
            "            \"port\": url.port,",
            "            \"username\": url.username,",
            "            \"database\": url.database,",
            "        }",
            "",
            "        serialized_encrypted_extra = self._properties.get(",
            "            \"masked_encrypted_extra\",",
            "            \"{}\",",
            "        )",
            "        if self._model:",
            "            serialized_encrypted_extra = (",
            "                self._model.db_engine_spec.unmask_encrypted_extra(",
            "                    self._model.encrypted_extra,",
            "                    serialized_encrypted_extra,",
            "                )",
            "            )",
            "",
            "        try:",
            "            database = DatabaseDAO.build_db_for_connection_test(",
            "                server_cert=self._properties.get(\"server_cert\", \"\"),",
            "                extra=self._properties.get(\"extra\", \"{}\"),",
            "                impersonate_user=self._properties.get(\"impersonate_user\", False),",
            "                encrypted_extra=serialized_encrypted_extra,",
            "            )",
            "",
            "            database.set_sqlalchemy_uri(uri)",
            "            database.db_engine_spec.mutate_db_for_connection_test(database)",
            "",
            "            event_logger.log_with_context(",
            "                action=\"test_connection_attempt\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "",
            "            def ping(engine: Engine) -> bool:",
            "                with closing(engine.raw_connection()) as conn:",
            "                    return engine.dialect.do_ping(conn)",
            "",
            "            with database.get_sqla_engine_with_context() as engine:",
            "                try:",
            "                    alive = func_timeout(",
            "                        app.config[\"TEST_DATABASE_CONNECTION_TIMEOUT\"].total_seconds(),",
            "                        ping,",
            "                        args=(engine,),",
            "                    )",
            "                except (sqlite3.ProgrammingError, RuntimeError):",
            "                    # SQLite can't run on a separate thread, so ``func_timeout`` fails",
            "                    # RuntimeError catches the equivalent error from duckdb.",
            "                    alive = engine.dialect.do_ping(engine)",
            "                except FunctionTimedOut as ex:",
            "                    raise SupersetTimeoutException(",
            "                        error_type=SupersetErrorType.CONNECTION_DATABASE_TIMEOUT,",
            "                        message=(",
            "                            \"Please check your connection details and database settings, \"",
            "                            \"and ensure that your database is accepting connections, \"",
            "                            \"then try connecting again.\"",
            "                        ),",
            "                        level=ErrorLevel.ERROR,",
            "                        extra={\"sqlalchemy_uri\": database.sqlalchemy_uri},",
            "                    ) from ex",
            "                except Exception as ex:  # pylint: disable=broad-except",
            "                    alive = False",
            "                    # So we stop losing the original message if any",
            "                    ex_str = str(ex)",
            "",
            "            if not alive:",
            "                raise DBAPIError(ex_str or None, None, None)",
            "",
            "            # Log succesful connection test with engine",
            "            event_logger.log_with_context(",
            "                action=\"test_connection_success\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "",
            "        except (NoSuchModuleError, ModuleNotFoundError) as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            raise DatabaseTestConnectionDriverError(",
            "                message=_(\"Could not load database driver: {}\").format(",
            "                    database.db_engine_spec.__name__",
            "                ),",
            "            ) from ex",
            "        except DBAPIError as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            # check for custom errors (wrong username, wrong password, etc)",
            "            errors = database.db_engine_spec.extract_errors(ex, context)",
            "            raise SupersetErrorsException(errors) from ex",
            "        except SupersetSecurityException as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            raise DatabaseSecurityUnsafeError(message=str(ex)) from ex",
            "        except SupersetTimeoutException as ex:",
            "",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            # bubble up the exception to return a 408",
            "            raise ex",
            "        except Exception as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            errors = database.db_engine_spec.extract_errors(ex, context)",
            "            raise DatabaseTestConnectionUnexpectedError(errors) from ex",
            "",
            "    def validate(self) -> None:",
            "        database_name = self._properties.get(\"database_name\")",
            "        if database_name is not None:",
            "            self._model = DatabaseDAO.get_database_by_name(database_name)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "import sqlite3",
            "from contextlib import closing",
            "from typing import Any, Dict, Optional",
            "",
            "from flask import current_app as app",
            "from flask_babel import gettext as _",
            "from func_timeout import func_timeout, FunctionTimedOut",
            "from sqlalchemy.engine import Engine",
            "from sqlalchemy.exc import DBAPIError, NoSuchModuleError",
            "",
            "from superset.commands.base import BaseCommand",
            "from superset.databases.commands.exceptions import (",
            "    DatabaseSecurityUnsafeError,",
            "    DatabaseTestConnectionDriverError,",
            "    DatabaseTestConnectionUnexpectedError,",
            ")",
            "from superset.databases.dao import DatabaseDAO",
            "from superset.databases.ssh_tunnel.models import SSHTunnel",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetErrorType",
            "from superset.exceptions import (",
            "    SupersetErrorsException,",
            "    SupersetSecurityException,",
            "    SupersetTimeoutException,",
            ")",
            "from superset.extensions import event_logger",
            "from superset.models.core import Database",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class TestConnectionDatabaseCommand(BaseCommand):",
            "    def __init__(self, data: Dict[str, Any]):",
            "        self._properties = data.copy()",
            "        self._model: Optional[Database] = None",
            "",
            "    def run(self) -> None:  # pylint: disable=too-many-statements",
            "        self.validate()",
            "        ex_str = \"\"",
            "        uri = self._properties.get(\"sqlalchemy_uri\", \"\")",
            "        if self._model and uri == self._model.safe_sqlalchemy_uri():",
            "            uri = self._model.sqlalchemy_uri_decrypted",
            "",
            "        # context for error messages",
            "        url = make_url_safe(uri)",
            "        context = {",
            "            \"hostname\": url.host,",
            "            \"password\": url.password,",
            "            \"port\": url.port,",
            "            \"username\": url.username,",
            "            \"database\": url.database,",
            "        }",
            "",
            "        serialized_encrypted_extra = self._properties.get(",
            "            \"masked_encrypted_extra\",",
            "            \"{}\",",
            "        )",
            "        if self._model:",
            "            serialized_encrypted_extra = (",
            "                self._model.db_engine_spec.unmask_encrypted_extra(",
            "                    self._model.encrypted_extra,",
            "                    serialized_encrypted_extra,",
            "                )",
            "            )",
            "",
            "        try:",
            "            database = DatabaseDAO.build_db_for_connection_test(",
            "                server_cert=self._properties.get(\"server_cert\", \"\"),",
            "                extra=self._properties.get(\"extra\", \"{}\"),",
            "                impersonate_user=self._properties.get(\"impersonate_user\", False),",
            "                encrypted_extra=serialized_encrypted_extra,",
            "            )",
            "",
            "            database.set_sqlalchemy_uri(uri)",
            "            database.db_engine_spec.mutate_db_for_connection_test(database)",
            "",
            "            # Generate tunnel if present in the properties",
            "            if ssh_tunnel := self._properties.get(\"ssh_tunnel\"):",
            "                ssh_tunnel = SSHTunnel(**ssh_tunnel)",
            "",
            "            event_logger.log_with_context(",
            "                action=\"test_connection_attempt\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "",
            "            def ping(engine: Engine) -> bool:",
            "                with closing(engine.raw_connection()) as conn:",
            "                    return engine.dialect.do_ping(conn)",
            "",
            "            with database.get_sqla_engine_with_context(",
            "                override_ssh_tunnel=ssh_tunnel",
            "            ) as engine:",
            "                try:",
            "                    alive = func_timeout(",
            "                        app.config[\"TEST_DATABASE_CONNECTION_TIMEOUT\"].total_seconds(),",
            "                        ping,",
            "                        args=(engine,),",
            "                    )",
            "                except (sqlite3.ProgrammingError, RuntimeError):",
            "                    # SQLite can't run on a separate thread, so ``func_timeout`` fails",
            "                    # RuntimeError catches the equivalent error from duckdb.",
            "                    alive = engine.dialect.do_ping(engine)",
            "                except FunctionTimedOut as ex:",
            "                    raise SupersetTimeoutException(",
            "                        error_type=SupersetErrorType.CONNECTION_DATABASE_TIMEOUT,",
            "                        message=(",
            "                            \"Please check your connection details and database settings, \"",
            "                            \"and ensure that your database is accepting connections, \"",
            "                            \"then try connecting again.\"",
            "                        ),",
            "                        level=ErrorLevel.ERROR,",
            "                        extra={\"sqlalchemy_uri\": database.sqlalchemy_uri},",
            "                    ) from ex",
            "                except Exception as ex:  # pylint: disable=broad-except",
            "                    alive = False",
            "                    # So we stop losing the original message if any",
            "                    ex_str = str(ex)",
            "",
            "            if not alive:",
            "                raise DBAPIError(ex_str or None, None, None)",
            "",
            "            # Log succesful connection test with engine",
            "            event_logger.log_with_context(",
            "                action=\"test_connection_success\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "",
            "        except (NoSuchModuleError, ModuleNotFoundError) as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            raise DatabaseTestConnectionDriverError(",
            "                message=_(\"Could not load database driver: {}\").format(",
            "                    database.db_engine_spec.__name__",
            "                ),",
            "            ) from ex",
            "        except DBAPIError as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            # check for custom errors (wrong username, wrong password, etc)",
            "            errors = database.db_engine_spec.extract_errors(ex, context)",
            "            raise SupersetErrorsException(errors) from ex",
            "        except SupersetSecurityException as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            raise DatabaseSecurityUnsafeError(message=str(ex)) from ex",
            "        except SupersetTimeoutException as ex:",
            "",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            # bubble up the exception to return a 408",
            "            raise ex",
            "        except Exception as ex:",
            "            event_logger.log_with_context(",
            "                action=f\"test_connection_error.{ex.__class__.__name__}\",",
            "                engine=database.db_engine_spec.__name__,",
            "            )",
            "            errors = database.db_engine_spec.extract_errors(ex, context)",
            "            raise DatabaseTestConnectionUnexpectedError(errors) from ex",
            "",
            "    def validate(self) -> None:",
            "        database_name = self._properties.get(\"database_name\")",
            "        if database_name is not None:",
            "            self._model = DatabaseDAO.get_database_by_name(database_name)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "102": [
                "TestConnectionDatabaseCommand",
                "run"
            ]
        },
        "addLocation": [
            "superset.databases.commands.test_connection.TestConnectionDatabaseCommand.run.context",
            "src.saml2.aes.AESCipher"
        ]
    }
}