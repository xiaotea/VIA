{
    "src/borg/archiver.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 3806,
                "afterPatchRowNumber": 3806,
                "PatchRowcode": "                 return forced_result"
            },
            "1": {
                "beforePatchRowNumber": 3807,
                "afterPatchRowNumber": 3807,
                "PatchRowcode": "             # we only take specific options from the forced \"borg serve\" command:"
            },
            "2": {
                "beforePatchRowNumber": 3808,
                "afterPatchRowNumber": 3808,
                "PatchRowcode": "             result.restrict_to_paths = forced_result.restrict_to_paths"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3809,
                "PatchRowcode": "+            result.restrict_to_repositories = forced_result.restrict_to_repositories"
            },
            "4": {
                "beforePatchRowNumber": 3809,
                "afterPatchRowNumber": 3810,
                "PatchRowcode": "             result.append_only = forced_result.append_only"
            },
            "5": {
                "beforePatchRowNumber": 3810,
                "afterPatchRowNumber": 3811,
                "PatchRowcode": "         return result"
            },
            "6": {
                "beforePatchRowNumber": 3811,
                "afterPatchRowNumber": 3812,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import argparse",
            "import collections",
            "import configparser",
            "import faulthandler",
            "import functools",
            "import hashlib",
            "import inspect",
            "import itertools",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import shlex",
            "import shutil",
            "import signal",
            "import stat",
            "import subprocess",
            "import sys",
            "import tarfile",
            "import textwrap",
            "import time",
            "import traceback",
            "from binascii import unhexlify",
            "from contextlib import contextmanager",
            "from datetime import datetime, timedelta",
            "from itertools import zip_longest",
            "",
            "from .logger import create_logger, setup_logging",
            "",
            "logger = create_logger()",
            "",
            "import msgpack",
            "",
            "import borg",
            "from . import __version__",
            "from . import helpers",
            "from .algorithms.checksums import crc32",
            "from .archive import Archive, ArchiveChecker, ArchiveRecreater, Statistics, is_special",
            "from .archive import BackupOSError, backup_io",
            "from .archive import FilesystemObjectProcessors, MetadataCollector, ChunksProcessor",
            "from .cache import Cache, assert_secure",
            "from .constants import *  # NOQA",
            "from .compress import CompressionSpec",
            "from .crypto.key import key_creator, key_argument_names, tam_required_file, tam_required, RepoKey, PassphraseKey",
            "from .crypto.keymanager import KeyManager",
            "from .helpers import EXIT_SUCCESS, EXIT_WARNING, EXIT_ERROR",
            "from .helpers import Error, NoManifestError, set_ec",
            "from .helpers import positive_int_validator, location_validator, archivename_validator, ChunkerParams",
            "from .helpers import PrefixSpec, SortBySpec, FilesCacheMode",
            "from .helpers import BaseFormatter, ItemFormatter, ArchiveFormatter",
            "from .helpers import format_timedelta, format_file_size, parse_file_size, format_archive",
            "from .helpers import safe_encode, remove_surrogates, bin_to_hex, prepare_dump_dict",
            "from .helpers import interval, prune_within, prune_split, PRUNING_PATTERNS",
            "from .helpers import timestamp",
            "from .helpers import get_cache_dir",
            "from .helpers import Manifest, AI_HUMAN_SORT_KEYS",
            "from .helpers import hardlinkable",
            "from .helpers import StableDict",
            "from .helpers import check_python, check_extension_modules",
            "from .helpers import dir_is_tagged, is_slow_msgpack, yes, sysinfo",
            "from .helpers import log_multi",
            "from .helpers import signal_handler, raising_signal_handler, SigHup, SigTerm",
            "from .helpers import ErrorIgnoringTextIOWrapper",
            "from .helpers import ProgressIndicatorPercent",
            "from .helpers import basic_json_data, json_print",
            "from .helpers import replace_placeholders",
            "from .helpers import ChunkIteratorFileWrapper",
            "from .helpers import popen_with_error_handling, prepare_subprocess_env",
            "from .helpers import dash_open",
            "from .helpers import umount",
            "from .nanorst import rst_to_terminal",
            "from .patterns import ArgparsePatternAction, ArgparseExcludeFileAction, ArgparsePatternFileAction, parse_exclude_pattern",
            "from .patterns import PatternMatcher",
            "from .item import Item",
            "from .platform import get_flags, get_process_id, SyncFile",
            "from .remote import RepositoryServer, RemoteRepository, cache_if_remote",
            "from .repository import Repository, LIST_SCAN_LIMIT",
            "from .selftest import selftest",
            "from .upgrader import AtticRepositoryUpgrader, BorgRepositoryUpgrader",
            "",
            "",
            "STATS_HEADER = \"                       Original size      Compressed size    Deduplicated size\"",
            "",
            "",
            "def argument(args, str_or_bool):",
            "    \"\"\"If bool is passed, return it. If str is passed, retrieve named attribute from args.\"\"\"",
            "    if isinstance(str_or_bool, str):",
            "        return getattr(args, str_or_bool)",
            "    if isinstance(str_or_bool, (list, tuple)):",
            "        return any(getattr(args, item) for item in str_or_bool)",
            "    return str_or_bool",
            "",
            "",
            "def with_repository(fake=False, invert_fake=False, create=False, lock=True,",
            "                    exclusive=False, manifest=True, cache=False, secure=True,",
            "                    compatibility=None):",
            "    \"\"\"",
            "    Method decorator for subcommand-handling methods: do_XYZ(self, args, repository, \u2026)",
            "",
            "    If a parameter (where allowed) is a str the attribute named of args is used instead.",
            "    :param fake: (str or bool) use None instead of repository, don't do anything else",
            "    :param create: create repository",
            "    :param lock: lock repository",
            "    :param exclusive: (str or bool) lock repository exclusively (for writing)",
            "    :param manifest: load manifest and key, pass them as keyword arguments",
            "    :param cache: open cache, pass it as keyword argument (implies manifest)",
            "    :param secure: do assert_secure after loading manifest",
            "    :param compatibility: mandatory if not create and (manifest or cache), specifies mandatory feature categories to check",
            "    \"\"\"",
            "",
            "    if not create and (manifest or cache):",
            "        if compatibility is None:",
            "            raise AssertionError(\"with_repository decorator used without compatibility argument\")",
            "        if type(compatibility) is not tuple:",
            "            raise AssertionError(\"with_repository decorator compatibility argument must be of type tuple\")",
            "    else:",
            "        if compatibility is not None:",
            "            raise AssertionError(\"with_repository called with compatibility argument but would not check\" + repr(compatibility))",
            "        if create:",
            "            compatibility = Manifest.NO_OPERATION_CHECK",
            "",
            "    def decorator(method):",
            "        @functools.wraps(method)",
            "        def wrapper(self, args, **kwargs):",
            "            location = args.location  # note: 'location' must be always present in args",
            "            append_only = getattr(args, 'append_only', False)",
            "            if argument(args, fake) ^ invert_fake:",
            "                return method(self, args, repository=None, **kwargs)",
            "            elif location.proto == 'ssh':",
            "                repository = RemoteRepository(location, create=create, exclusive=argument(args, exclusive),",
            "                                              lock_wait=self.lock_wait, lock=lock, append_only=append_only, args=args)",
            "            else:",
            "                repository = Repository(location.path, create=create, exclusive=argument(args, exclusive),",
            "                                        lock_wait=self.lock_wait, lock=lock,",
            "                                        append_only=append_only)",
            "            with repository:",
            "                if manifest or cache:",
            "                    kwargs['manifest'], kwargs['key'] = Manifest.load(repository, compatibility)",
            "                    if 'compression' in args:",
            "                        kwargs['key'].compressor = args.compression.compressor",
            "                    if secure:",
            "                        assert_secure(repository, kwargs['manifest'])",
            "                if cache:",
            "                    with Cache(repository, kwargs['key'], kwargs['manifest'],",
            "                               do_files=getattr(args, 'cache_files', False),",
            "                               progress=getattr(args, 'progress', False), lock_wait=self.lock_wait) as cache_:",
            "                        return method(self, args, repository=repository, cache=cache_, **kwargs)",
            "                else:",
            "                    return method(self, args, repository=repository, **kwargs)",
            "        return wrapper",
            "    return decorator",
            "",
            "",
            "def with_archive(method):",
            "    @functools.wraps(method)",
            "    def wrapper(self, args, repository, key, manifest, **kwargs):",
            "        archive = Archive(repository, key, manifest, args.location.archive,",
            "                          numeric_owner=getattr(args, 'numeric_owner', False),",
            "                          nobsdflags=getattr(args, 'nobsdflags', False),",
            "                          cache=kwargs.get('cache'),",
            "                          consider_part_files=args.consider_part_files, log_json=args.log_json)",
            "        return method(self, args, repository=repository, manifest=manifest, key=key, archive=archive, **kwargs)",
            "    return wrapper",
            "",
            "",
            "def parse_storage_quota(storage_quota):",
            "    parsed = parse_file_size(storage_quota)",
            "    if parsed < parse_file_size('10M'):",
            "        raise argparse.ArgumentTypeError('quota is too small (%s). At least 10M are required.' % storage_quota)",
            "    return parsed",
            "",
            "",
            "def get_func(args):",
            "    # This works around http://bugs.python.org/issue9351",
            "    # func is used at the leaf parsers of the argparse parser tree,",
            "    # fallback_func at next level towards the root,",
            "    # fallback2_func at the 2nd next level (which is root in our case).",
            "    for name in 'func', 'fallback_func', 'fallback2_func':",
            "        func = getattr(args, name, None)",
            "        if func is not None:",
            "            return func",
            "    raise Exception('expected func attributes not found')",
            "",
            "",
            "class Archiver:",
            "",
            "    def __init__(self, lock_wait=None, prog=None):",
            "        self.exit_code = EXIT_SUCCESS",
            "        self.lock_wait = lock_wait",
            "        self.prog = prog",
            "",
            "    def print_error(self, msg, *args):",
            "        msg = args and msg % args or msg",
            "        self.exit_code = EXIT_ERROR",
            "        logger.error(msg)",
            "",
            "    def print_warning(self, msg, *args):",
            "        msg = args and msg % args or msg",
            "        self.exit_code = EXIT_WARNING  # we do not terminate here, so it is a warning",
            "        logger.warning(msg)",
            "",
            "    def print_file_status(self, status, path):",
            "        if self.output_list and (self.output_filter is None or status in self.output_filter):",
            "            if self.log_json:",
            "                print(json.dumps({",
            "                    'type': 'file_status',",
            "                    'status': status,",
            "                    'path': remove_surrogates(path),",
            "                }), file=sys.stderr)",
            "            else:",
            "                logging.getLogger('borg.output.list').info(\"%1s %s\", status, remove_surrogates(path))",
            "",
            "    @staticmethod",
            "    def build_matcher(inclexcl_patterns, include_paths):",
            "        matcher = PatternMatcher()",
            "        matcher.add_inclexcl(inclexcl_patterns)",
            "        matcher.add_includepaths(include_paths)",
            "        return matcher",
            "",
            "    def do_serve(self, args):",
            "        \"\"\"Start in server mode. This command is usually not used manually.\"\"\"",
            "        RepositoryServer(",
            "            restrict_to_paths=args.restrict_to_paths,",
            "            restrict_to_repositories=args.restrict_to_repositories,",
            "            append_only=args.append_only,",
            "            storage_quota=args.storage_quota,",
            "        ).serve()",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(create=True, exclusive=True, manifest=False)",
            "    def do_init(self, args, repository):",
            "        \"\"\"Initialize an empty repository\"\"\"",
            "        path = args.location.canonical_path()",
            "        logger.info('Initializing repository at \"%s\"' % path)",
            "        try:",
            "            key = key_creator(repository, args)",
            "        except (EOFError, KeyboardInterrupt):",
            "            repository.destroy()",
            "            return EXIT_WARNING",
            "        manifest = Manifest(key, repository)",
            "        manifest.key = key",
            "        manifest.write()",
            "        repository.commit()",
            "        with Cache(repository, key, manifest, warn_if_unencrypted=False):",
            "            pass",
            "        if key.tam_required:",
            "            tam_file = tam_required_file(repository)",
            "            open(tam_file, 'w').close()",
            "            logger.warning(",
            "                '\\n'",
            "                'By default repositories initialized with this version will produce security\\n'",
            "                'errors if written to with an older version (up to and including Borg 1.0.8).\\n'",
            "                '\\n'",
            "                'If you want to use these older versions, you can disable the check by running:\\n'",
            "                'borg upgrade --disable-tam \\'%s\\'\\n'",
            "                '\\n'",
            "                'See https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability '",
            "                'for details about the security implications.', path)",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, manifest=False)",
            "    def do_check(self, args, repository):",
            "        \"\"\"Check repository consistency\"\"\"",
            "        if args.repair:",
            "            msg = (\"'check --repair' is an experimental feature that might result in data loss.\" +",
            "                   \"\\n\" +",
            "                   \"Type 'YES' if you understand this and want to continue: \")",
            "            if not yes(msg, false_msg=\"Aborting.\", invalid_msg=\"Invalid answer, aborting.\",",
            "                       truish=('YES', ), retry=False,",
            "                       env_var_override='BORG_CHECK_I_KNOW_WHAT_I_AM_DOING'):",
            "                return EXIT_ERROR",
            "        if args.repo_only and any((args.verify_data, args.first, args.last, args.prefix)):",
            "            self.print_error(\"--repository-only contradicts --first, --last, --prefix and --verify-data arguments.\")",
            "            return EXIT_ERROR",
            "        if not args.archives_only:",
            "            if not repository.check(repair=args.repair, save_space=args.save_space):",
            "                return EXIT_WARNING",
            "        if args.prefix:",
            "            args.glob_archives = args.prefix + '*'",
            "        if not args.repo_only and not ArchiveChecker().check(",
            "                repository, repair=args.repair, archive=args.location.archive,",
            "                first=args.first, last=args.last, sort_by=args.sort_by or 'ts', glob=args.glob_archives,",
            "                verify_data=args.verify_data, save_space=args.save_space):",
            "            return EXIT_WARNING",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.CHECK,))",
            "    def do_change_passphrase(self, args, repository, manifest, key):",
            "        \"\"\"Change repository key file passphrase\"\"\"",
            "        if not hasattr(key, 'change_passphrase'):",
            "            print('This repository is not encrypted, cannot change the passphrase.')",
            "            return EXIT_ERROR",
            "        key.change_passphrase()",
            "        logger.info('Key updated')",
            "        if hasattr(key, 'find_key'):",
            "            # print key location to make backing it up easier",
            "            logger.info('Key location: %s', key.find_key())",
            "        return EXIT_SUCCESS",
            "",
            "    def do_change_passphrase_deprecated(self, args):",
            "        logger.warning('\"borg change-passphrase\" is deprecated and will be removed in Borg 1.2.\\n'",
            "                       'Use \"borg key change-passphrase\" instead.')",
            "        return self.do_change_passphrase(args)",
            "",
            "    @with_repository(lock=False, exclusive=False, manifest=False, cache=False)",
            "    def do_key_export(self, args, repository):",
            "        \"\"\"Export the repository key for backup\"\"\"",
            "        manager = KeyManager(repository)",
            "        manager.load_keyblob()",
            "        if args.paper:",
            "            manager.export_paperkey(args.path)",
            "        else:",
            "            if not args.path:",
            "                self.print_error(\"output file to export key to expected\")",
            "                return EXIT_ERROR",
            "            if args.qr:",
            "                manager.export_qr(args.path)",
            "            else:",
            "                manager.export(args.path)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(lock=False, exclusive=False, manifest=False, cache=False)",
            "    def do_key_import(self, args, repository):",
            "        \"\"\"Import the repository key from backup\"\"\"",
            "        manager = KeyManager(repository)",
            "        if args.paper:",
            "            if args.path:",
            "                self.print_error(\"with --paper import from file is not supported\")",
            "                return EXIT_ERROR",
            "            manager.import_paperkey(args)",
            "        else:",
            "            if not args.path:",
            "                self.print_error(\"input file to import key from expected\")",
            "                return EXIT_ERROR",
            "            if args.path != '-' and not os.path.exists(args.path):",
            "                self.print_error(\"input file does not exist: \" + args.path)",
            "                return EXIT_ERROR",
            "            manager.import_keyfile(args)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False)",
            "    def do_migrate_to_repokey(self, args, repository):",
            "        \"\"\"Migrate passphrase -> repokey\"\"\"",
            "        manifest_data = repository.get(Manifest.MANIFEST_ID)",
            "        key_old = PassphraseKey.detect(repository, manifest_data)",
            "        key_new = RepoKey(repository)",
            "        key_new.target = repository",
            "        key_new.repository_id = repository.id",
            "        key_new.enc_key = key_old.enc_key",
            "        key_new.enc_hmac_key = key_old.enc_hmac_key",
            "        key_new.id_key = key_old.id_key",
            "        key_new.chunk_seed = key_old.chunk_seed",
            "        key_new.change_passphrase()  # option to change key protection passphrase, save",
            "        logger.info('Key updated')",
            "        return EXIT_SUCCESS",
            "",
            "    def do_benchmark_crud(self, args):",
            "        \"\"\"Benchmark Create, Read, Update, Delete for archives.\"\"\"",
            "        def measurement_run(repo, path):",
            "            archive = repo + '::borg-benchmark-crud'",
            "            compression = '--compression=none'",
            "            # measure create perf (without files cache to always have it chunking)",
            "            t_start = time.monotonic()",
            "            rc = self.do_create(self.parse_args(['create', compression, '--files-cache=disabled', archive + '1', path]))",
            "            t_end = time.monotonic()",
            "            dt_create = t_end - t_start",
            "            assert rc == 0",
            "            # now build files cache",
            "            rc1 = self.do_create(self.parse_args(['create', compression, archive + '2', path]))",
            "            rc2 = self.do_delete(self.parse_args(['delete', archive + '2']))",
            "            assert rc1 == rc2 == 0",
            "            # measure a no-change update (archive1 is still present)",
            "            t_start = time.monotonic()",
            "            rc1 = self.do_create(self.parse_args(['create', compression, archive + '3', path]))",
            "            t_end = time.monotonic()",
            "            dt_update = t_end - t_start",
            "            rc2 = self.do_delete(self.parse_args(['delete', archive + '3']))",
            "            assert rc1 == rc2 == 0",
            "            # measure extraction (dry-run: without writing result to disk)",
            "            t_start = time.monotonic()",
            "            rc = self.do_extract(self.parse_args(['extract', '--dry-run', archive + '1']))",
            "            t_end = time.monotonic()",
            "            dt_extract = t_end - t_start",
            "            assert rc == 0",
            "            # measure archive deletion (of LAST present archive with the data)",
            "            t_start = time.monotonic()",
            "            rc = self.do_delete(self.parse_args(['delete', archive + '1']))",
            "            t_end = time.monotonic()",
            "            dt_delete = t_end - t_start",
            "            assert rc == 0",
            "            return dt_create, dt_update, dt_extract, dt_delete",
            "",
            "        @contextmanager",
            "        def test_files(path, count, size, random):",
            "            path = os.path.join(path, 'borg-test-data')",
            "            os.makedirs(path)",
            "            for i in range(count):",
            "                fname = os.path.join(path, 'file_%d' % i)",
            "                data = b'\\0' * size if not random else os.urandom(size)",
            "                with SyncFile(fname, binary=True) as fd:  # used for posix_fadvise's sake",
            "                    fd.write(data)",
            "            yield path",
            "            shutil.rmtree(path)",
            "",
            "        if '_BORG_BENCHMARK_CRUD_TEST' in os.environ:",
            "            tests = [",
            "                ('Z-TEST', 1, 1, False),",
            "                ('R-TEST', 1, 1, True),",
            "            ]",
            "        else:",
            "            tests = [",
            "                ('Z-BIG', 10, 100000000, False),",
            "                ('R-BIG', 10, 100000000, True),",
            "                ('Z-MEDIUM', 1000, 1000000, False),",
            "                ('R-MEDIUM', 1000, 1000000, True),",
            "                ('Z-SMALL', 10000, 10000, False),",
            "                ('R-SMALL', 10000, 10000, True),",
            "            ]",
            "",
            "        for msg, count, size, random in tests:",
            "            with test_files(args.path, count, size, random) as path:",
            "                dt_create, dt_update, dt_extract, dt_delete = measurement_run(args.location.canonical_path(), path)",
            "            total_size_MB = count * size / 1e06",
            "            file_size_formatted = format_file_size(size)",
            "            content = 'random' if random else 'all-zero'",
            "            fmt = '%s-%-10s %9.2f MB/s (%d * %s %s files: %.2fs)'",
            "            print(fmt % ('C', msg, total_size_MB / dt_create, count, file_size_formatted, content, dt_create))",
            "            print(fmt % ('R', msg, total_size_MB / dt_extract, count, file_size_formatted, content, dt_extract))",
            "            print(fmt % ('U', msg, total_size_MB / dt_update, count, file_size_formatted, content, dt_update))",
            "            print(fmt % ('D', msg, total_size_MB / dt_delete, count, file_size_formatted, content, dt_delete))",
            "",
            "        return 0",
            "",
            "    @with_repository(fake='dry_run', exclusive=True, compatibility=(Manifest.Operation.WRITE,))",
            "    def do_create(self, args, repository, manifest=None, key=None):",
            "        \"\"\"Create new archive\"\"\"",
            "        matcher = PatternMatcher(fallback=True)",
            "        matcher.add_inclexcl(args.patterns)",
            "",
            "        def create_inner(archive, cache, fso):",
            "            # Add cache dir to inode_skip list",
            "            skip_inodes = set()",
            "            try:",
            "                st = os.stat(get_cache_dir())",
            "                skip_inodes.add((st.st_ino, st.st_dev))",
            "            except OSError:",
            "                pass",
            "            # Add local repository dir to inode_skip list",
            "            if not args.location.host:",
            "                try:",
            "                    st = os.stat(args.location.path)",
            "                    skip_inodes.add((st.st_ino, st.st_dev))",
            "                except OSError:",
            "                    pass",
            "            for path in args.paths:",
            "                if path == '-':  # stdin",
            "                    path = 'stdin'",
            "                    if not dry_run:",
            "                        try:",
            "                            status = fso.process_stdin(path, cache)",
            "                        except BackupOSError as e:",
            "                            status = 'E'",
            "                            self.print_warning('%s: %s', path, e)",
            "                    else:",
            "                        status = '-'",
            "                    self.print_file_status(status, path)",
            "                    continue",
            "                path = os.path.normpath(path)",
            "                try:",
            "                    st = os.stat(path, follow_symlinks=False)",
            "                except OSError as e:",
            "                    self.print_warning('%s: %s', path, e)",
            "                    continue",
            "                if args.one_file_system:",
            "                    restrict_dev = st.st_dev",
            "                else:",
            "                    restrict_dev = None",
            "                self._process(fso, cache, matcher, args.exclude_caches, args.exclude_if_present,",
            "                              args.keep_exclude_tags, skip_inodes, path, restrict_dev,",
            "                              read_special=args.read_special, dry_run=dry_run, st=st)",
            "            if not dry_run:",
            "                archive.save(comment=args.comment, timestamp=args.timestamp)",
            "                if args.progress:",
            "                    archive.stats.show_progress(final=True)",
            "                args.stats |= args.json",
            "                archive.stats += fso.stats",
            "                if args.stats:",
            "                    if args.json:",
            "                        json_print(basic_json_data(manifest, cache=cache, extra={",
            "                            'archive': archive,",
            "                        }))",
            "                    else:",
            "                        log_multi(DASHES,",
            "                                  str(archive),",
            "                                  DASHES,",
            "                                  STATS_HEADER,",
            "                                  str(archive.stats),",
            "                                  str(cache),",
            "                                  DASHES, logger=logging.getLogger('borg.output.stats'))",
            "",
            "        self.output_filter = args.output_filter",
            "        self.output_list = args.output_list",
            "        self.ignore_inode = args.ignore_inode",
            "        self.nobsdflags = args.nobsdflags",
            "        self.exclude_nodump = args.exclude_nodump",
            "        self.files_cache_mode = args.files_cache_mode",
            "        dry_run = args.dry_run",
            "        t0 = datetime.utcnow()",
            "        t0_monotonic = time.monotonic()",
            "        if not dry_run:",
            "            with Cache(repository, key, manifest, do_files=args.cache_files, progress=args.progress,",
            "                       lock_wait=self.lock_wait, permit_adhoc_cache=args.no_cache_sync) as cache:",
            "                archive = Archive(repository, key, manifest, args.location.archive, cache=cache,",
            "                                  create=True, checkpoint_interval=args.checkpoint_interval,",
            "                                  numeric_owner=args.numeric_owner, noatime=args.noatime, noctime=args.noctime,",
            "                                  progress=args.progress,",
            "                                  chunker_params=args.chunker_params, start=t0, start_monotonic=t0_monotonic,",
            "                                  log_json=args.log_json)",
            "                metadata_collector = MetadataCollector(noatime=args.noatime, noctime=args.noctime,",
            "                    nobsdflags=args.nobsdflags, numeric_owner=args.numeric_owner, nobirthtime=args.nobirthtime)",
            "                cp = ChunksProcessor(cache=cache, key=key,",
            "                    add_item=archive.add_item, write_checkpoint=archive.write_checkpoint,",
            "                    checkpoint_interval=args.checkpoint_interval, rechunkify=False)",
            "                fso = FilesystemObjectProcessors(metadata_collector=metadata_collector, cache=cache, key=key,",
            "                    process_file_chunks=cp.process_file_chunks, add_item=archive.add_item,",
            "                    chunker_params=args.chunker_params)",
            "                create_inner(archive, cache, fso)",
            "        else:",
            "            create_inner(None, None, None)",
            "        return self.exit_code",
            "",
            "    def _process(self, fso, cache, matcher, exclude_caches, exclude_if_present,",
            "                 keep_exclude_tags, skip_inodes, path, restrict_dev,",
            "                 read_special=False, dry_run=False, st=None):",
            "        \"\"\"",
            "        Process *path* recursively according to the various parameters.",
            "",
            "        *st* (if given) is a *os.stat_result* object for *path*.",
            "",
            "        This should only raise on critical errors. Per-item errors must be handled within this method.",
            "        \"\"\"",
            "        try:",
            "            recurse_excluded_dir = False",
            "            if matcher.match(path):",
            "                if st is None:",
            "                    with backup_io('stat'):",
            "                        st = os.stat(path, follow_symlinks=False)",
            "            else:",
            "                self.print_file_status('x', path)",
            "                # get out here as quickly as possible:",
            "                # we only need to continue if we shall recurse into an excluded directory.",
            "                # if we shall not recurse, then do not even touch (stat()) the item, it",
            "                # could trigger an error, e.g. if access is forbidden, see #3209.",
            "                if not matcher.recurse_dir:",
            "                    return",
            "                if st is None:",
            "                    with backup_io('stat'):",
            "                        st = os.stat(path, follow_symlinks=False)",
            "                recurse_excluded_dir = stat.S_ISDIR(st.st_mode)",
            "                if not recurse_excluded_dir:",
            "                    return",
            "",
            "            if (st.st_ino, st.st_dev) in skip_inodes:",
            "                return",
            "            # if restrict_dev is given, we do not want to recurse into a new filesystem,",
            "            # but we WILL save the mountpoint directory (or more precise: the root",
            "            # directory of the mounted filesystem that shadows the mountpoint dir).",
            "            recurse = restrict_dev is None or st.st_dev == restrict_dev",
            "            status = None",
            "            if self.exclude_nodump:",
            "                # Ignore if nodump flag is set",
            "                with backup_io('flags'):",
            "                    if get_flags(path, st) & stat.UF_NODUMP:",
            "                        self.print_file_status('x', path)",
            "                        return",
            "            if stat.S_ISREG(st.st_mode):",
            "                if not dry_run:",
            "                    status = fso.process_file(path, st, cache, self.ignore_inode, self.files_cache_mode)",
            "            elif stat.S_ISDIR(st.st_mode):",
            "                if recurse:",
            "                    tag_paths = dir_is_tagged(path, exclude_caches, exclude_if_present)",
            "                    if tag_paths:",
            "                        if keep_exclude_tags and not dry_run:",
            "                            fso.process_dir(path, st)",
            "                            for tag_path in tag_paths:",
            "                                self._process(fso, cache, matcher, exclude_caches, exclude_if_present,",
            "                                              keep_exclude_tags, skip_inodes, tag_path, restrict_dev,",
            "                                              read_special=read_special, dry_run=dry_run)",
            "                        self.print_file_status('x', path)",
            "                        return",
            "                if not dry_run:",
            "                    if not recurse_excluded_dir:",
            "                        status = fso.process_dir(path, st)",
            "                if recurse:",
            "                    with backup_io('scandir'):",
            "                        entries = helpers.scandir_inorder(path)",
            "                    for dirent in entries:",
            "                        normpath = os.path.normpath(dirent.path)",
            "                        self._process(fso, cache, matcher, exclude_caches, exclude_if_present,",
            "                                      keep_exclude_tags, skip_inodes, normpath, restrict_dev,",
            "                                      read_special=read_special, dry_run=dry_run)",
            "            elif stat.S_ISLNK(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_symlink(path, st)",
            "                    else:",
            "                        try:",
            "                            st_target = os.stat(path)",
            "                        except OSError:",
            "                            special = False",
            "                        else:",
            "                            special = is_special(st_target.st_mode)",
            "                        if special:",
            "                            status = fso.process_file(path, st_target, cache)",
            "                        else:",
            "                            status = fso.process_symlink(path, st)",
            "            elif stat.S_ISFIFO(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_fifo(path, st)",
            "                    else:",
            "                        status = fso.process_file(path, st, cache)",
            "            elif stat.S_ISCHR(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_dev(path, st, 'c')",
            "                    else:",
            "                        status = fso.process_file(path, st, cache)",
            "            elif stat.S_ISBLK(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_dev(path, st, 'b')",
            "                    else:",
            "                        status = fso.process_file(path, st, cache)",
            "            elif stat.S_ISSOCK(st.st_mode):",
            "                # Ignore unix sockets",
            "                return",
            "            elif stat.S_ISDOOR(st.st_mode):",
            "                # Ignore Solaris doors",
            "                return",
            "            elif stat.S_ISPORT(st.st_mode):",
            "                # Ignore Solaris event ports",
            "                return",
            "            else:",
            "                self.print_warning('Unknown file type: %s', path)",
            "                return",
            "        except BackupOSError as e:",
            "            self.print_warning('%s: %s', path, e)",
            "            status = 'E'",
            "        # Status output",
            "        if status is None:",
            "            if not dry_run:",
            "                status = '?'  # need to add a status code somewhere",
            "            else:",
            "                status = '-'  # dry run, item was not backed up",
            "",
            "        if not recurse_excluded_dir:",
            "            self.print_file_status(status, path)",
            "",
            "    @staticmethod",
            "    def build_filter(matcher, peek_and_store_hardlink_masters, strip_components):",
            "        if strip_components:",
            "            def item_filter(item):",
            "                matched = matcher.match(item.path) and os.sep.join(item.path.split(os.sep)[strip_components:])",
            "                peek_and_store_hardlink_masters(item, matched)",
            "                return matched",
            "        else:",
            "            def item_filter(item):",
            "                matched = matcher.match(item.path)",
            "                peek_and_store_hardlink_masters(item, matched)",
            "                return matched",
            "        return item_filter",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    @with_archive",
            "    def do_extract(self, args, repository, manifest, key, archive):",
            "        \"\"\"Extract archive contents\"\"\"",
            "        # be restrictive when restoring files, restore permissions later",
            "        if sys.getfilesystemencoding() == 'ascii':",
            "            logger.warning('Warning: File system encoding is \"ascii\", extracting non-ascii filenames will not be supported.')",
            "            if sys.platform.startswith(('linux', 'freebsd', 'netbsd', 'openbsd', 'darwin', )):",
            "                logger.warning('Hint: You likely need to fix your locale setup. E.g. install locales and use: LANG=en_US.UTF-8')",
            "",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "",
            "        progress = args.progress",
            "        output_list = args.output_list",
            "        dry_run = args.dry_run",
            "        stdout = args.stdout",
            "        sparse = args.sparse",
            "        strip_components = args.strip_components",
            "        dirs = []",
            "        partial_extract = not matcher.empty() or strip_components",
            "        hardlink_masters = {} if partial_extract else None",
            "",
            "        def peek_and_store_hardlink_masters(item, matched):",
            "            if (partial_extract and not matched and hardlinkable(item.mode) and",
            "                    item.get('hardlink_master', True) and 'source' not in item):",
            "                hardlink_masters[item.get('path')] = (item.get('chunks'), None)",
            "",
            "        filter = self.build_filter(matcher, peek_and_store_hardlink_masters, strip_components)",
            "        if progress:",
            "            pi = ProgressIndicatorPercent(msg='%5.1f%% Extracting: %s', step=0.1, msgid='extract')",
            "            pi.output('Calculating size')",
            "            extracted_size = sum(item.get_size(hardlink_masters) for item in archive.iter_items(filter))",
            "            pi.total = extracted_size",
            "        else:",
            "            pi = None",
            "",
            "        for item in archive.iter_items(filter, preload=True):",
            "            orig_path = item.path",
            "            if strip_components:",
            "                item.path = os.sep.join(orig_path.split(os.sep)[strip_components:])",
            "            if not args.dry_run:",
            "                while dirs and not item.path.startswith(dirs[-1].path):",
            "                    dir_item = dirs.pop(-1)",
            "                    try:",
            "                        archive.extract_item(dir_item, stdout=stdout)",
            "                    except BackupOSError as e:",
            "                        self.print_warning('%s: %s', remove_surrogates(dir_item.path), e)",
            "            if output_list:",
            "                logging.getLogger('borg.output.list').info(remove_surrogates(orig_path))",
            "            try:",
            "                if dry_run:",
            "                    archive.extract_item(item, dry_run=True, pi=pi)",
            "                else:",
            "                    if stat.S_ISDIR(item.mode):",
            "                        dirs.append(item)",
            "                        archive.extract_item(item, stdout=stdout, restore_attrs=False)",
            "                    else:",
            "                        archive.extract_item(item, stdout=stdout, sparse=sparse, hardlink_masters=hardlink_masters,",
            "                                             stripped_components=strip_components, original_path=orig_path, pi=pi)",
            "            except BackupOSError as e:",
            "                self.print_warning('%s: %s', remove_surrogates(orig_path), e)",
            "",
            "        if pi:",
            "            pi.finish()",
            "",
            "        if not args.dry_run:",
            "            pi = ProgressIndicatorPercent(total=len(dirs), msg='Setting directory permissions %3.0f%%',",
            "                                          msgid='extract.permissions')",
            "            while dirs:",
            "                pi.show()",
            "                dir_item = dirs.pop(-1)",
            "                try:",
            "                    archive.extract_item(dir_item, stdout=stdout)",
            "                except BackupOSError as e:",
            "                    self.print_warning('%s: %s', remove_surrogates(dir_item.path), e)",
            "        for pattern in matcher.get_unmatched_include_patterns():",
            "            self.print_warning(\"Include pattern '%s' never matched.\", pattern)",
            "        if pi:",
            "            # clear progress output",
            "            pi.finish()",
            "        return self.exit_code",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    @with_archive",
            "    def do_export_tar(self, args, repository, manifest, key, archive):",
            "        \"\"\"Export archive contents as a tarball\"\"\"",
            "        self.output_list = args.output_list",
            "",
            "        # A quick note about the general design of tar_filter and tarfile;",
            "        # The tarfile module of Python can provide some compression mechanisms",
            "        # by itself, using the builtin gzip, bz2 and lzma modules (and \"tarmodes\"",
            "        # such as \"w:xz\").",
            "        #",
            "        # Doing so would have three major drawbacks:",
            "        # For one the compressor runs on the same thread as the program using the",
            "        # tarfile, stealing valuable CPU time from Borg and thus reducing throughput.",
            "        # Then this limits the available options - what about lz4? Brotli? zstd?",
            "        # The third issue is that systems can ship more optimized versions than those",
            "        # built into Python, e.g. pigz or pxz, which can use more than one thread for",
            "        # compression.",
            "        #",
            "        # Therefore we externalize compression by using a filter program, which has",
            "        # none of these drawbacks. The only issue of using an external filter is",
            "        # that it has to be installed -- hardly a problem, considering that",
            "        # the decompressor must be installed as well to make use of the exported tarball!",
            "",
            "        filter = None",
            "        if args.tar_filter == 'auto':",
            "            # Note that filter remains None if tarfile is '-'.",
            "            if args.tarfile.endswith('.tar.gz'):",
            "                filter = 'gzip'",
            "            elif args.tarfile.endswith('.tar.bz2'):",
            "                filter = 'bzip2'",
            "            elif args.tarfile.endswith('.tar.xz'):",
            "                filter = 'xz'",
            "            logger.debug('Automatically determined tar filter: %s', filter)",
            "        else:",
            "            filter = args.tar_filter",
            "",
            "        tarstream = dash_open(args.tarfile, 'wb')",
            "        tarstream_close = args.tarfile != '-'",
            "",
            "        if filter:",
            "            # When we put a filter between us and the final destination,",
            "            # the selected output (tarstream until now) becomes the output of the filter (=filterout).",
            "            # The decision whether to close that or not remains the same.",
            "            filterout = tarstream",
            "            filterout_close = tarstream_close",
            "            env = prepare_subprocess_env(system=True)",
            "            # There is no deadlock potential here (the subprocess docs warn about this), because",
            "            # communication with the process is a one-way road, i.e. the process can never block",
            "            # for us to do something while we block on the process for something different.",
            "            filterproc = popen_with_error_handling(filter, stdin=subprocess.PIPE, stdout=filterout,",
            "                                                   log_prefix='--tar-filter: ', env=env)",
            "            if not filterproc:",
            "                return EXIT_ERROR",
            "            # Always close the pipe, otherwise the filter process would not notice when we are done.",
            "            tarstream = filterproc.stdin",
            "            tarstream_close = True",
            "",
            "        # The | (pipe) symbol instructs tarfile to use a streaming mode of operation",
            "        # where it never seeks on the passed fileobj.",
            "        tar = tarfile.open(fileobj=tarstream, mode='w|')",
            "",
            "        self._export_tar(args, archive, tar)",
            "",
            "        # This does not close the fileobj (tarstream) we passed to it -- a side effect of the | mode.",
            "        tar.close()",
            "",
            "        if tarstream_close:",
            "            tarstream.close()",
            "",
            "        if filter:",
            "            logger.debug('Done creating tar, waiting for filter to die...')",
            "            rc = filterproc.wait()",
            "            if rc:",
            "                logger.error('--tar-filter exited with code %d, output file is likely unusable!', rc)",
            "                self.exit_code = EXIT_ERROR",
            "            else:",
            "                logger.debug('filter exited with code %d', rc)",
            "",
            "            if filterout_close:",
            "                filterout.close()",
            "",
            "        return self.exit_code",
            "",
            "    def _export_tar(self, args, archive, tar):",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "",
            "        progress = args.progress",
            "        output_list = args.output_list",
            "        strip_components = args.strip_components",
            "        partial_extract = not matcher.empty() or strip_components",
            "        hardlink_masters = {} if partial_extract else None",
            "",
            "        def peek_and_store_hardlink_masters(item, matched):",
            "            if (partial_extract and not matched and hardlinkable(item.mode) and",
            "                    item.get('hardlink_master', True) and 'source' not in item):",
            "                hardlink_masters[item.get('path')] = (item.get('chunks'), None)",
            "",
            "        filter = self.build_filter(matcher, peek_and_store_hardlink_masters, strip_components)",
            "",
            "        if progress:",
            "            pi = ProgressIndicatorPercent(msg='%5.1f%% Processing: %s', step=0.1, msgid='extract')",
            "            pi.output('Calculating size')",
            "            extracted_size = sum(item.get_size(hardlink_masters) for item in archive.iter_items(filter))",
            "            pi.total = extracted_size",
            "        else:",
            "            pi = None",
            "",
            "        def item_content_stream(item):",
            "            \"\"\"",
            "            Return a file-like object that reads from the chunks of *item*.",
            "            \"\"\"",
            "            chunk_iterator = archive.pipeline.fetch_many([chunk_id for chunk_id, _, _ in item.chunks])",
            "            if pi:",
            "                info = [remove_surrogates(item.path)]",
            "                return ChunkIteratorFileWrapper(chunk_iterator,",
            "                                                lambda read_bytes: pi.show(increase=len(read_bytes), info=info))",
            "            else:",
            "                return ChunkIteratorFileWrapper(chunk_iterator)",
            "",
            "        def item_to_tarinfo(item, original_path):",
            "            \"\"\"",
            "            Transform a Borg *item* into a tarfile.TarInfo object.",
            "",
            "            Return a tuple (tarinfo, stream), where stream may be a file-like object that represents",
            "            the file contents, if any, and is None otherwise. When *tarinfo* is None, the *item*",
            "            cannot be represented as a TarInfo object and should be skipped.",
            "            \"\"\"",
            "",
            "            # If we would use the PAX (POSIX) format (which we currently don't),",
            "            # we can support most things that aren't possible with classic tar",
            "            # formats, including GNU tar, such as:",
            "            # atime, ctime, possibly Linux capabilities (security.* xattrs)",
            "            # and various additions supported by GNU tar in POSIX mode.",
            "",
            "            stream = None",
            "            tarinfo = tarfile.TarInfo()",
            "            tarinfo.name = item.path",
            "            tarinfo.mtime = item.mtime / 1e9",
            "            tarinfo.mode = stat.S_IMODE(item.mode)",
            "            tarinfo.uid = item.uid",
            "            tarinfo.gid = item.gid",
            "            tarinfo.uname = item.user or ''",
            "            tarinfo.gname = item.group or ''",
            "            # The linkname in tar has the same dual use the 'source' attribute of Borg items,",
            "            # i.e. for symlinks it means the destination, while for hardlinks it refers to the",
            "            # file.",
            "            # Since hardlinks in tar have a different type code (LNKTYPE) the format might",
            "            # support hardlinking arbitrary objects (including symlinks and directories), but",
            "            # whether implementations actually support that is a whole different question...",
            "            tarinfo.linkname = \"\"",
            "",
            "            modebits = stat.S_IFMT(item.mode)",
            "            if modebits == stat.S_IFREG:",
            "                tarinfo.type = tarfile.REGTYPE",
            "                if 'source' in item:",
            "                    source = os.sep.join(item.source.split(os.sep)[strip_components:])",
            "                    if hardlink_masters is None:",
            "                        linkname = source",
            "                    else:",
            "                        chunks, linkname = hardlink_masters.get(item.source, (None, source))",
            "                    if linkname:",
            "                        # Master was already added to the archive, add a hardlink reference to it.",
            "                        tarinfo.type = tarfile.LNKTYPE",
            "                        tarinfo.linkname = linkname",
            "                    elif chunks is not None:",
            "                        # The item which has the chunks was not put into the tar, therefore",
            "                        # we do that now and update hardlink_masters to reflect that.",
            "                        item.chunks = chunks",
            "                        tarinfo.size = item.get_size()",
            "                        stream = item_content_stream(item)",
            "                        hardlink_masters[item.get('source') or original_path] = (None, item.path)",
            "                else:",
            "                    tarinfo.size = item.get_size()",
            "                    stream = item_content_stream(item)",
            "            elif modebits == stat.S_IFDIR:",
            "                tarinfo.type = tarfile.DIRTYPE",
            "            elif modebits == stat.S_IFLNK:",
            "                tarinfo.type = tarfile.SYMTYPE",
            "                tarinfo.linkname = item.source",
            "            elif modebits == stat.S_IFBLK:",
            "                tarinfo.type = tarfile.BLKTYPE",
            "                tarinfo.devmajor = os.major(item.rdev)",
            "                tarinfo.devminor = os.minor(item.rdev)",
            "            elif modebits == stat.S_IFCHR:",
            "                tarinfo.type = tarfile.CHRTYPE",
            "                tarinfo.devmajor = os.major(item.rdev)",
            "                tarinfo.devminor = os.minor(item.rdev)",
            "            elif modebits == stat.S_IFIFO:",
            "                tarinfo.type = tarfile.FIFOTYPE",
            "            else:",
            "                self.print_warning('%s: unsupported file type %o for tar export', remove_surrogates(item.path), modebits)",
            "                set_ec(EXIT_WARNING)",
            "                return None, stream",
            "            return tarinfo, stream",
            "",
            "        for item in archive.iter_items(filter, preload=True):",
            "            orig_path = item.path",
            "            if strip_components:",
            "                item.path = os.sep.join(orig_path.split(os.sep)[strip_components:])",
            "            tarinfo, stream = item_to_tarinfo(item, orig_path)",
            "            if tarinfo:",
            "                if output_list:",
            "                    logging.getLogger('borg.output.list').info(remove_surrogates(orig_path))",
            "                tar.addfile(tarinfo, stream)",
            "",
            "        if pi:",
            "            pi.finish()",
            "",
            "        for pattern in matcher.get_unmatched_include_patterns():",
            "            self.print_warning(\"Include pattern '%s' never matched.\", pattern)",
            "        return self.exit_code",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    @with_archive",
            "    def do_diff(self, args, repository, manifest, key, archive):",
            "        \"\"\"Diff contents of two archives\"\"\"",
            "",
            "        def print_output(diff, path):",
            "            print(\"{:<19} {}\".format(diff, path))",
            "",
            "        archive1 = archive",
            "        archive2 = Archive(repository, key, manifest, args.archive2,",
            "                           consider_part_files=args.consider_part_files)",
            "",
            "        can_compare_chunk_ids = archive1.metadata.get('chunker_params', False) == archive2.metadata.get(",
            "            'chunker_params', True) or args.same_chunker_params",
            "        if not can_compare_chunk_ids:",
            "            self.print_warning('--chunker-params might be different between archives, diff will be slow.\\n'",
            "                               'If you know for certain that they are the same, pass --same-chunker-params '",
            "                               'to override this check.')",
            "",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "",
            "        diffs = Archive.compare_archives_iter(archive1, archive2, matcher, can_compare_chunk_ids=can_compare_chunk_ids)",
            "        # Conversion to string and filtering for diff.equal to save memory if sorting",
            "        diffs = ((path, str(diff)) for path, diff in diffs if not diff.equal)",
            "",
            "        if args.sort:",
            "            diffs = sorted(diffs)",
            "",
            "        for path, diff in diffs:",
            "            print_output(diff, path)",
            "",
            "        for pattern in matcher.get_unmatched_include_patterns():",
            "            self.print_warning(\"Include pattern '%s' never matched.\", pattern)",
            "",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, cache=True, compatibility=(Manifest.Operation.CHECK,))",
            "    @with_archive",
            "    def do_rename(self, args, repository, manifest, key, cache, archive):",
            "        \"\"\"Rename an existing archive\"\"\"",
            "        name = replace_placeholders(args.name)",
            "        archive.rename(name)",
            "        manifest.write()",
            "        repository.commit()",
            "        cache.commit()",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, manifest=False)",
            "    def do_delete(self, args, repository):",
            "        \"\"\"Delete an existing repository or archives\"\"\"",
            "        archive_filter_specified = args.first or args.last or args.prefix or args.glob_archives",
            "        explicit_archives_specified = args.location.archive or args.archives",
            "        if archive_filter_specified and explicit_archives_specified:",
            "            self.print_error('Mixing archive filters and explicitly named archives is not supported.')",
            "            return self.exit_code",
            "        if archive_filter_specified or explicit_archives_specified:",
            "            return self._delete_archives(args, repository)",
            "        else:",
            "            return self._delete_repository(args, repository)",
            "",
            "    def _delete_archives(self, args, repository):",
            "        \"\"\"Delete archives\"\"\"",
            "        manifest, key = Manifest.load(repository, (Manifest.Operation.DELETE,))",
            "",
            "        if args.location.archive or args.archives:",
            "            archives = list(args.archives)",
            "            if args.location.archive:",
            "                archives.insert(0, args.location.archive)",
            "            archive_names = tuple(archives)",
            "        else:",
            "            archive_names = tuple(x.name for x in manifest.archives.list_considering(args))",
            "            if not archive_names:",
            "                return self.exit_code",
            "",
            "        if args.forced == 2:",
            "            deleted = False",
            "            for i, archive_name in enumerate(archive_names, 1):",
            "                try:",
            "                    del manifest.archives[archive_name]",
            "                except KeyError:",
            "                    self.exit_code = EXIT_WARNING",
            "                    logger.warning('Archive {} not found ({}/{}).'.format(archive_name, i, len(archive_names)))",
            "                else:",
            "                    deleted = True",
            "                    logger.info('Deleted {} ({}/{}).'.format(archive_name, i, len(archive_names)))",
            "            if deleted:",
            "                manifest.write()",
            "                # note: might crash in compact() after committing the repo",
            "                repository.commit()",
            "                logger.info('Done. Run \"borg check --repair\" to clean up the mess.')",
            "            else:",
            "                logger.warning('Aborted.')",
            "            return self.exit_code",
            "",
            "        stats_logger = logging.getLogger('borg.output.stats')",
            "        if args.stats:",
            "            log_multi(DASHES, STATS_HEADER, logger=stats_logger)",
            "",
            "        with Cache(repository, key, manifest, progress=args.progress, lock_wait=self.lock_wait) as cache:",
            "            for i, archive_name in enumerate(archive_names, 1):",
            "                logger.info('Deleting {} ({}/{}):'.format(archive_name, i, len(archive_names)))",
            "                archive = Archive(repository, key, manifest, archive_name, cache=cache)",
            "                stats = Statistics()",
            "                archive.delete(stats, progress=args.progress, forced=args.forced)",
            "                manifest.write()",
            "                repository.commit(save_space=args.save_space)",
            "                cache.commit()",
            "                logger.info(\"Archive deleted.\")",
            "                if args.stats:",
            "                    log_multi(stats.summary.format(label='Deleted data:', stats=stats),",
            "                              DASHES, logger=stats_logger)",
            "                if args.forced == 0 and self.exit_code:",
            "                    break",
            "            if args.stats:",
            "                stats_logger.info(str(cache))",
            "",
            "        return self.exit_code",
            "",
            "    def _delete_repository(self, args, repository):",
            "        \"\"\"Delete a repository\"\"\"",
            "        if not args.cache_only:",
            "            msg = []",
            "            try:",
            "                manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            except NoManifestError:",
            "                msg.append(\"You requested to completely DELETE the repository *including* all archives it may \"",
            "                           \"contain.\")",
            "                msg.append(\"This repository seems to have no manifest, so we can't tell anything about its \"",
            "                           \"contents.\")",
            "            else:",
            "                msg.append(\"You requested to completely DELETE the repository *including* all archives it \"",
            "                           \"contains:\")",
            "                for archive_info in manifest.archives.list(sort_by=['ts']):",
            "                    msg.append(format_archive(archive_info))",
            "            msg.append(\"Type 'YES' if you understand this and want to continue: \")",
            "            msg = '\\n'.join(msg)",
            "            if not yes(msg, false_msg=\"Aborting.\", invalid_msg='Invalid answer, aborting.', truish=('YES',),",
            "                       retry=False, env_var_override='BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'):",
            "                self.exit_code = EXIT_ERROR",
            "                return self.exit_code",
            "            repository.destroy()",
            "            logger.info(\"Repository deleted.\")",
            "        Cache.destroy(repository)",
            "        logger.info(\"Cache deleted.\")",
            "        return self.exit_code",
            "",
            "    def do_mount(self, args):",
            "        \"\"\"Mount archive or an entire repository as a FUSE filesystem\"\"\"",
            "        # Perform these checks before opening the repository and asking for a passphrase.",
            "",
            "        try:",
            "            import borg.fuse",
            "        except ImportError as e:",
            "            self.print_error('borg mount not available: loading FUSE support failed [ImportError: %s]' % str(e))",
            "            return self.exit_code",
            "",
            "        if not os.path.isdir(args.mountpoint) or not os.access(args.mountpoint, os.R_OK | os.W_OK | os.X_OK):",
            "            self.print_error('%s: Mountpoint must be a writable directory' % args.mountpoint)",
            "            return self.exit_code",
            "",
            "        return self._do_mount(args)",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    def _do_mount(self, args, repository, manifest, key):",
            "        from .fuse import FuseOperations",
            "",
            "        with cache_if_remote(repository, decrypted_cache=key) as cached_repo:",
            "            operations = FuseOperations(key, repository, manifest, args, cached_repo)",
            "            logger.info(\"Mounting filesystem\")",
            "            try:",
            "                operations.mount(args.mountpoint, args.options, args.foreground)",
            "            except RuntimeError:",
            "                # Relevant error message already printed to stderr by FUSE",
            "                self.exit_code = EXIT_ERROR",
            "        return self.exit_code",
            "",
            "    def do_umount(self, args):",
            "        \"\"\"un-mount the FUSE filesystem\"\"\"",
            "        return umount(args.mountpoint)",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    def do_list(self, args, repository, manifest, key):",
            "        \"\"\"List archive or repository contents\"\"\"",
            "        if args.location.archive:",
            "            if args.json:",
            "                self.print_error('The --json option is only valid for listing archives, not archive contents.')",
            "                return self.exit_code",
            "            return self._list_archive(args, repository, manifest, key)",
            "        else:",
            "            if args.json_lines:",
            "                self.print_error('The --json-lines option is only valid for listing archive contents, not archives.')",
            "                return self.exit_code",
            "            return self._list_repository(args, repository, manifest, key)",
            "",
            "    def _list_archive(self, args, repository, manifest, key):",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "        if args.format is not None:",
            "            format = args.format",
            "        elif args.short:",
            "            format = \"{path}{NL}\"",
            "        else:",
            "            format = \"{mode} {user:6} {group:6} {size:8} {mtime} {path}{extra}{NL}\"",
            "",
            "        def _list_inner(cache):",
            "            archive = Archive(repository, key, manifest, args.location.archive, cache=cache,",
            "                              consider_part_files=args.consider_part_files)",
            "",
            "            formatter = ItemFormatter(archive, format, json_lines=args.json_lines)",
            "            for item in archive.iter_items(lambda item: matcher.match(item.path)):",
            "                sys.stdout.write(formatter.format_item(item))",
            "",
            "        # Only load the cache if it will be used",
            "        if ItemFormatter.format_needs_cache(format):",
            "            with Cache(repository, key, manifest, lock_wait=self.lock_wait) as cache:",
            "                _list_inner(cache)",
            "        else:",
            "            _list_inner(cache=None)",
            "",
            "        return self.exit_code",
            "",
            "    def _list_repository(self, args, repository, manifest, key):",
            "        if args.format is not None:",
            "            format = args.format",
            "        elif args.short:",
            "            format = \"{archive}{NL}\"",
            "        else:",
            "            format = \"{archive:<36} {time} [{id}]{NL}\"",
            "        formatter = ArchiveFormatter(format, repository, manifest, key, json=args.json)",
            "",
            "        output_data = []",
            "",
            "        for archive_info in manifest.archives.list_considering(args):",
            "            if args.json:",
            "                output_data.append(formatter.get_item_data(archive_info))",
            "            else:",
            "                sys.stdout.write(formatter.format_item(archive_info))",
            "",
            "        if args.json:",
            "            json_print(basic_json_data(manifest, extra={",
            "                'archives': output_data",
            "            }))",
            "",
            "        return self.exit_code",
            "",
            "    @with_repository(cache=True, compatibility=(Manifest.Operation.READ,))",
            "    def do_info(self, args, repository, manifest, key, cache):",
            "        \"\"\"Show archive details such as disk space used\"\"\"",
            "        if any((args.location.archive, args.first, args.last, args.prefix, args.glob_archives)):",
            "            return self._info_archives(args, repository, manifest, key, cache)",
            "        else:",
            "            return self._info_repository(args, repository, manifest, key, cache)",
            "",
            "    def _info_archives(self, args, repository, manifest, key, cache):",
            "        def format_cmdline(cmdline):",
            "            return remove_surrogates(' '.join(shlex.quote(x) for x in cmdline))",
            "",
            "        if args.location.archive:",
            "            archive_names = (args.location.archive,)",
            "        else:",
            "            archive_names = tuple(x.name for x in manifest.archives.list_considering(args))",
            "            if not archive_names:",
            "                return self.exit_code",
            "",
            "        output_data = []",
            "",
            "        for i, archive_name in enumerate(archive_names, 1):",
            "            archive = Archive(repository, key, manifest, archive_name, cache=cache,",
            "                              consider_part_files=args.consider_part_files)",
            "            info = archive.info()",
            "            if args.json:",
            "                output_data.append(info)",
            "            else:",
            "                info['duration'] = format_timedelta(timedelta(seconds=info['duration']))",
            "                info['command_line'] = format_cmdline(info['command_line'])",
            "                print(textwrap.dedent(\"\"\"",
            "                Archive name: {name}",
            "                Archive fingerprint: {id}",
            "                Comment: {comment}",
            "                Hostname: {hostname}",
            "                Username: {username}",
            "                Time (start): {start}",
            "                Time (end): {end}",
            "                Duration: {duration}",
            "                Number of files: {stats[nfiles]}",
            "                Command line: {command_line}",
            "                Utilization of maximum supported archive size: {limits[max_archive_size]:.0%}",
            "                ------------------------------------------------------------------------------",
            "                                       Original size      Compressed size    Deduplicated size",
            "                This archive:   {stats[original_size]:>20s} {stats[compressed_size]:>20s} {stats[deduplicated_size]:>20s}",
            "                {cache}",
            "                \"\"\").strip().format(cache=cache, **info))",
            "            if self.exit_code:",
            "                break",
            "            if not args.json and len(archive_names) - i:",
            "                print()",
            "",
            "        if args.json:",
            "            json_print(basic_json_data(manifest, cache=cache, extra={",
            "                'archives': output_data,",
            "            }))",
            "        return self.exit_code",
            "",
            "    def _info_repository(self, args, repository, manifest, key, cache):",
            "        info = basic_json_data(manifest, cache=cache, extra={",
            "            'security_dir': cache.security_manager.dir,",
            "        })",
            "",
            "        if args.json:",
            "            json_print(info)",
            "        else:",
            "            encryption = 'Encrypted: '",
            "            if key.NAME == 'plaintext':",
            "                encryption += 'No'",
            "            else:",
            "                encryption += 'Yes (%s)' % key.NAME",
            "            if key.NAME.startswith('key file'):",
            "                encryption += '\\nKey file: %s' % key.find_key()",
            "            info['encryption'] = encryption",
            "",
            "            print(textwrap.dedent(\"\"\"",
            "            Repository ID: {id}",
            "            Location: {location}",
            "            {encryption}",
            "            Cache: {cache.path}",
            "            Security dir: {security_dir}",
            "            \"\"\").strip().format(",
            "                id=bin_to_hex(repository.id),",
            "                location=repository._location.canonical_path(),",
            "                **info))",
            "            print(DASHES)",
            "            print(STATS_HEADER)",
            "            print(str(cache))",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, compatibility=(Manifest.Operation.DELETE,))",
            "    def do_prune(self, args, repository, manifest, key):",
            "        \"\"\"Prune repository archives according to specified rules\"\"\"",
            "        if not any((args.secondly, args.minutely, args.hourly, args.daily,",
            "                    args.weekly, args.monthly, args.yearly, args.within)):",
            "            self.print_error('At least one of the \"keep-within\", \"keep-last\", '",
            "                             '\"keep-secondly\", \"keep-minutely\", \"keep-hourly\", \"keep-daily\", '",
            "                             '\"keep-weekly\", \"keep-monthly\" or \"keep-yearly\" settings must be specified.')",
            "            return self.exit_code",
            "        if args.prefix:",
            "            args.glob_archives = args.prefix + '*'",
            "        checkpoint_re = r'\\.checkpoint(\\.\\d+)?'",
            "        archives_checkpoints = manifest.archives.list(glob=args.glob_archives,",
            "                                                      match_end=r'(%s)?\\Z' % checkpoint_re,",
            "                                                      sort_by=['ts'], reverse=True)",
            "        is_checkpoint = re.compile(r'(%s)\\Z' % checkpoint_re).search",
            "        checkpoints = [arch for arch in archives_checkpoints if is_checkpoint(arch.name)]",
            "        # keep the latest checkpoint, if there is no later non-checkpoint archive",
            "        if archives_checkpoints and checkpoints and archives_checkpoints[0] is checkpoints[0]:",
            "            keep_checkpoints = checkpoints[:1]",
            "        else:",
            "            keep_checkpoints = []",
            "        checkpoints = set(checkpoints)",
            "        # ignore all checkpoint archives to avoid keeping one (which is an incomplete backup)",
            "        # that is newer than a successfully completed backup - and killing the successful backup.",
            "        archives = [arch for arch in archives_checkpoints if arch not in checkpoints]",
            "        keep = []",
            "        # collect the rule responsible for the keeping of each archive in this dict",
            "        # keys are archive ids, values are a tuple",
            "        #   (<rulename>, <how many archives were kept by this rule so far >)",
            "        kept_because = {}",
            "",
            "        # find archives which need to be kept because of the keep-within rule",
            "        if args.within:",
            "            keep += prune_within(archives, args.within, kept_because)",
            "",
            "        # find archives which need to be kept because of the various time period rules",
            "        for rule in PRUNING_PATTERNS.keys():",
            "            num = getattr(args, rule, None)",
            "            if num is not None:",
            "                keep += prune_split(archives, rule, num, kept_because)",
            "",
            "        to_delete = (set(archives) | checkpoints) - (set(keep) | set(keep_checkpoints))",
            "        stats = Statistics()",
            "        with Cache(repository, key, manifest, do_files=False, lock_wait=self.lock_wait) as cache:",
            "            list_logger = logging.getLogger('borg.output.list')",
            "            # set up counters for the progress display",
            "            to_delete_len = len(to_delete)",
            "            archives_deleted = 0",
            "            for archive in archives_checkpoints:",
            "                if archive in to_delete:",
            "                    if args.dry_run:",
            "                        log_message = 'Would prune:'",
            "                    else:",
            "                        archives_deleted += 1",
            "                        log_message = 'Pruning archive (%d/%d):' % (archives_deleted, to_delete_len)",
            "                        Archive(repository, key, manifest, archive.name, cache,",
            "                                progress=args.progress).delete(stats, forced=args.forced)",
            "                else:",
            "                    if is_checkpoint(archive.name):",
            "                        log_message = 'Keeping checkpoint archive:'",
            "                    else:",
            "                        log_message = 'Keeping archive (rule: {rule} #{num}):'.format(",
            "                            rule=kept_because[archive.id][0], num=kept_because[archive.id][1]",
            "                        )",
            "                if args.output_list:",
            "                    list_logger.info(\"{message:<40} {archive}\".format(",
            "                        message=log_message, archive=format_archive(archive)",
            "                    ))",
            "            if to_delete and not args.dry_run:",
            "                manifest.write()",
            "                repository.commit(save_space=args.save_space)",
            "                cache.commit()",
            "            if args.stats:",
            "                log_multi(DASHES,",
            "                          STATS_HEADER,",
            "                          stats.summary.format(label='Deleted data:', stats=stats),",
            "                          str(cache),",
            "                          DASHES, logger=logging.getLogger('borg.output.stats'))",
            "        return self.exit_code",
            "",
            "    @with_repository(fake=('tam', 'disable_tam'), invert_fake=True, manifest=False, exclusive=True)",
            "    def do_upgrade(self, args, repository, manifest=None, key=None):",
            "        \"\"\"upgrade a repository from a previous version\"\"\"",
            "        if args.tam:",
            "            manifest, key = Manifest.load(repository, (Manifest.Operation.CHECK,), force_tam_not_required=args.force)",
            "",
            "            if not hasattr(key, 'change_passphrase'):",
            "                print('This repository is not encrypted, cannot enable TAM.')",
            "                return EXIT_ERROR",
            "",
            "            if not manifest.tam_verified or not manifest.config.get(b'tam_required', False):",
            "                # The standard archive listing doesn't include the archive ID like in borg 1.1.x",
            "                print('Manifest contents:')",
            "                for archive_info in manifest.archives.list(sort_by=['ts']):",
            "                    print(format_archive(archive_info), '[%s]' % bin_to_hex(archive_info.id))",
            "                manifest.config[b'tam_required'] = True",
            "                manifest.write()",
            "                repository.commit()",
            "            if not key.tam_required:",
            "                key.tam_required = True",
            "                key.change_passphrase(key._passphrase)",
            "                print('Key updated')",
            "                if hasattr(key, 'find_key'):",
            "                    print('Key location:', key.find_key())",
            "            if not tam_required(repository):",
            "                tam_file = tam_required_file(repository)",
            "                open(tam_file, 'w').close()",
            "                print('Updated security database')",
            "        elif args.disable_tam:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK, force_tam_not_required=True)",
            "            if tam_required(repository):",
            "                os.unlink(tam_required_file(repository))",
            "            if key.tam_required:",
            "                key.tam_required = False",
            "                key.change_passphrase(key._passphrase)",
            "                print('Key updated')",
            "                if hasattr(key, 'find_key'):",
            "                    print('Key location:', key.find_key())",
            "            manifest.config[b'tam_required'] = False",
            "            manifest.write()",
            "            repository.commit()",
            "        else:",
            "            # mainly for upgrades from Attic repositories,",
            "            # but also supports borg 0.xx -> 1.0 upgrade.",
            "",
            "            repo = AtticRepositoryUpgrader(args.location.path, create=False)",
            "            try:",
            "                repo.upgrade(args.dry_run, inplace=args.inplace, progress=args.progress)",
            "            except NotImplementedError as e:",
            "                print(\"warning: %s\" % e)",
            "            repo = BorgRepositoryUpgrader(args.location.path, create=False)",
            "            try:",
            "                repo.upgrade(args.dry_run, inplace=args.inplace, progress=args.progress)",
            "            except NotImplementedError as e:",
            "                print(\"warning: %s\" % e)",
            "        return self.exit_code",
            "",
            "    @with_repository(cache=True, exclusive=True, compatibility=(Manifest.Operation.CHECK,))",
            "    def do_recreate(self, args, repository, manifest, key, cache):",
            "        \"\"\"Re-create archives\"\"\"",
            "        msg = (\"recreate is an experimental feature.\\n\"",
            "               \"Type 'YES' if you understand this and want to continue: \")",
            "        if not yes(msg, false_msg=\"Aborting.\", truish=('YES',),",
            "                   env_var_override='BORG_RECREATE_I_KNOW_WHAT_I_AM_DOING'):",
            "            return EXIT_ERROR",
            "",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "        self.output_list = args.output_list",
            "        self.output_filter = args.output_filter",
            "        recompress = args.recompress != 'never'",
            "        always_recompress = args.recompress == 'always'",
            "",
            "        recreater = ArchiveRecreater(repository, manifest, key, cache, matcher,",
            "                                     exclude_caches=args.exclude_caches, exclude_if_present=args.exclude_if_present,",
            "                                     keep_exclude_tags=args.keep_exclude_tags, chunker_params=args.chunker_params,",
            "                                     compression=args.compression, recompress=recompress, always_recompress=always_recompress,",
            "                                     progress=args.progress, stats=args.stats,",
            "                                     file_status_printer=self.print_file_status,",
            "                                     checkpoint_interval=args.checkpoint_interval,",
            "                                     dry_run=args.dry_run)",
            "",
            "        if args.location.archive:",
            "            name = args.location.archive",
            "            target = replace_placeholders(args.target) if args.target else None",
            "            if recreater.is_temporary_archive(name):",
            "                self.print_error('Refusing to work on temporary archive of prior recreate: %s', name)",
            "                return self.exit_code",
            "            if not recreater.recreate(name, args.comment, target):",
            "                self.print_error('Nothing to do. Archive was not processed.\\n'",
            "                                 'Specify at least one pattern, PATH, --comment, re-compression or re-chunking option.')",
            "        else:",
            "            if args.target is not None:",
            "                self.print_error('--target: Need to specify single archive')",
            "                return self.exit_code",
            "            for archive in manifest.archives.list(sort_by=['ts']):",
            "                name = archive.name",
            "                if recreater.is_temporary_archive(name):",
            "                    continue",
            "                print('Processing', name)",
            "                if not recreater.recreate(name, args.comment):",
            "                    logger.info('Skipped archive %s: Nothing to do. Archive was not processed.', name)",
            "        if not args.dry_run:",
            "            manifest.write()",
            "            repository.commit()",
            "            cache.commit()",
            "        return self.exit_code",
            "",
            "    @with_repository(manifest=False, exclusive=True)",
            "    def do_with_lock(self, args, repository):",
            "        \"\"\"run a user specified command with the repository lock held\"\"\"",
            "        # for a new server, this will immediately take an exclusive lock.",
            "        # to support old servers, that do not have \"exclusive\" arg in open()",
            "        # RPC API, we also do it the old way:",
            "        # re-write manifest to start a repository transaction - this causes a",
            "        # lock upgrade to exclusive for remote (and also for local) repositories.",
            "        # by using manifest=False in the decorator, we avoid having to require",
            "        # the encryption key (and can operate just with encrypted data).",
            "        data = repository.get(Manifest.MANIFEST_ID)",
            "        repository.put(Manifest.MANIFEST_ID, data)",
            "        # usually, a 0 byte (open for writing) segment file would be visible in the filesystem here.",
            "        # we write and close this file, to rather have a valid segment file on disk, before invoking the subprocess.",
            "        # we can only do this for local repositories (with .io), though:",
            "        if hasattr(repository, 'io'):",
            "            repository.io.close_segment()",
            "        env = prepare_subprocess_env(system=True)",
            "        try:",
            "            # we exit with the return code we get from the subprocess",
            "            return subprocess.call([args.command] + args.args, env=env)",
            "        finally:",
            "            # we need to commit the \"no change\" operation we did to the manifest",
            "            # because it created a new segment file in the repository. if we would",
            "            # roll back, the same file would be later used otherwise (for other content).",
            "            # that would be bad if somebody uses rsync with ignore-existing (or",
            "            # any other mechanism relying on existing segment data not changing).",
            "            # see issue #1867.",
            "            repository.commit()",
            "",
            "    @with_repository(exclusive=True, cache=True, compatibility=(Manifest.Operation.WRITE,))",
            "    def do_config(self, args, repository, manifest, key, cache):",
            "        \"\"\"get, set, and delete values in a repository or cache config file\"\"\"",
            "        try:",
            "            section, name = args.name.split('.')",
            "        except ValueError:",
            "            section = args.cache and \"cache\" or \"repository\"",
            "            name = args.name",
            "",
            "        if args.cache:",
            "            cache.cache_config.load()",
            "            config = cache.cache_config._config",
            "            save = cache.cache_config.save",
            "        else:",
            "            config = repository.config",
            "            save = lambda: repository.save_config(repository.path, repository.config)",
            "",
            "        if args.delete:",
            "            config.remove_option(section, name)",
            "            if len(config.options(section)) == 0:",
            "                config.remove_section(section)",
            "            save()",
            "        elif args.value:",
            "            if section not in config.sections():",
            "                config.add_section(section)",
            "            config.set(section, name, args.value)",
            "            save()",
            "        else:",
            "            try:",
            "                print(config.get(section, name))",
            "            except (configparser.NoOptionError, configparser.NoSectionError) as e:",
            "                print(e, file=sys.stderr)",
            "                return EXIT_WARNING",
            "        return EXIT_SUCCESS",
            "",
            "    def do_debug_info(self, args):",
            "        \"\"\"display system information for debugging / bug reports\"\"\"",
            "        print(sysinfo())",
            "",
            "        # Additional debug information",
            "        print('CRC implementation:', crc32.__name__)",
            "        print('Process ID:', get_process_id())",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_archive_items(self, args, repository, manifest, key):",
            "        \"\"\"dump (decrypted, decompressed) archive items metadata (not: data)\"\"\"",
            "        archive = Archive(repository, key, manifest, args.location.archive,",
            "                          consider_part_files=args.consider_part_files)",
            "        for i, item_id in enumerate(archive.metadata.items):",
            "            data = key.decrypt(item_id, repository.get(item_id))",
            "            filename = '%06d_%s.items' % (i, bin_to_hex(item_id))",
            "            print('Dumping', filename)",
            "            with open(filename, 'wb') as fd:",
            "                fd.write(data)",
            "        print('Done.')",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_archive(self, args, repository, manifest, key):",
            "        \"\"\"dump decoded archive metadata (not: data)\"\"\"",
            "",
            "        try:",
            "            archive_meta_orig = manifest.archives.get_raw_dict()[safe_encode(args.location.archive)]",
            "        except KeyError:",
            "            raise Archive.DoesNotExist(args.location.archive)",
            "",
            "        indent = 4",
            "",
            "        def do_indent(d):",
            "            return textwrap.indent(json.dumps(d, indent=indent), prefix=' ' * indent)",
            "",
            "        def output(fd):",
            "            # this outputs megabytes of data for a modest sized archive, so some manual streaming json output",
            "            fd.write('{\\n')",
            "            fd.write('    \"_name\": ' + json.dumps(args.location.archive) + \",\\n\")",
            "            fd.write('    \"_manifest_entry\":\\n')",
            "            fd.write(do_indent(prepare_dump_dict(archive_meta_orig)))",
            "            fd.write(',\\n')",
            "",
            "            data = key.decrypt(archive_meta_orig[b'id'], repository.get(archive_meta_orig[b'id']))",
            "            archive_org_dict = msgpack.unpackb(data, object_hook=StableDict, unicode_errors='surrogateescape')",
            "",
            "            fd.write('    \"_meta\":\\n')",
            "            fd.write(do_indent(prepare_dump_dict(archive_org_dict)))",
            "            fd.write(',\\n')",
            "            fd.write('    \"_items\": [\\n')",
            "",
            "            unpacker = msgpack.Unpacker(use_list=False, object_hook=StableDict)",
            "            first = True",
            "            for item_id in archive_org_dict[b'items']:",
            "                data = key.decrypt(item_id, repository.get(item_id))",
            "                unpacker.feed(data)",
            "                for item in unpacker:",
            "                    item = prepare_dump_dict(item)",
            "                    if first:",
            "                        first = False",
            "                    else:",
            "                        fd.write(',\\n')",
            "                    fd.write(do_indent(item))",
            "",
            "            fd.write('\\n')",
            "            fd.write('    ]\\n}\\n')",
            "",
            "        with dash_open(args.path, 'w') as fd:",
            "            output(fd)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_manifest(self, args, repository, manifest, key):",
            "        \"\"\"dump decoded repository manifest\"\"\"",
            "",
            "        data = key.decrypt(None, repository.get(manifest.MANIFEST_ID))",
            "",
            "        meta = prepare_dump_dict(msgpack.fallback.unpackb(data, object_hook=StableDict, unicode_errors='surrogateescape'))",
            "",
            "        with dash_open(args.path, 'w') as fd:",
            "            json.dump(meta, fd, indent=4)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_repo_objs(self, args, repository, manifest, key):",
            "        \"\"\"dump (decrypted, decompressed) repo objects\"\"\"",
            "        marker = None",
            "        i = 0",
            "        while True:",
            "            result = repository.list(limit=LIST_SCAN_LIMIT, marker=marker)",
            "            if not result:",
            "                break",
            "            marker = result[-1]",
            "            for id in result:",
            "                cdata = repository.get(id)",
            "                give_id = id if id != Manifest.MANIFEST_ID else None",
            "                data = key.decrypt(give_id, cdata)",
            "                filename = '%06d_%s.obj' % (i, bin_to_hex(id))",
            "                print('Dumping', filename)",
            "                with open(filename, 'wb') as fd:",
            "                    fd.write(data)",
            "                i += 1",
            "        print('Done.')",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False)",
            "    def do_debug_get_obj(self, args, repository):",
            "        \"\"\"get object contents from the repository and write it into file\"\"\"",
            "        hex_id = args.id",
            "        try:",
            "            id = unhexlify(hex_id)",
            "        except ValueError:",
            "            print(\"object id %s is invalid.\" % hex_id)",
            "        else:",
            "            try:",
            "                data = repository.get(id)",
            "            except Repository.ObjectNotFound:",
            "                print(\"object %s not found.\" % hex_id)",
            "            else:",
            "                with open(args.path, \"wb\") as f:",
            "                    f.write(data)",
            "                print(\"object %s fetched.\" % hex_id)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False, exclusive=True)",
            "    def do_debug_put_obj(self, args, repository):",
            "        \"\"\"put file(s) contents into the repository\"\"\"",
            "        for path in args.paths:",
            "            with open(path, \"rb\") as f:",
            "                data = f.read()",
            "            h = hashlib.sha256(data)  # XXX hardcoded",
            "            repository.put(h.digest(), data)",
            "            print(\"object %s put.\" % h.hexdigest())",
            "        repository.commit()",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False, exclusive=True)",
            "    def do_debug_delete_obj(self, args, repository):",
            "        \"\"\"delete the objects with the given IDs from the repo\"\"\"",
            "        modified = False",
            "        for hex_id in args.ids:",
            "            try:",
            "                id = unhexlify(hex_id)",
            "            except ValueError:",
            "                print(\"object id %s is invalid.\" % hex_id)",
            "            else:",
            "                try:",
            "                    repository.delete(id)",
            "                    modified = True",
            "                    print(\"object %s deleted.\" % hex_id)",
            "                except Repository.ObjectNotFound:",
            "                    print(\"object %s not found.\" % hex_id)",
            "        if modified:",
            "            repository.commit()",
            "        print('Done.')",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False, exclusive=True, cache=True, compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_refcount_obj(self, args, repository, manifest, key, cache):",
            "        \"\"\"display refcounts for the objects with the given IDs\"\"\"",
            "        for hex_id in args.ids:",
            "            try:",
            "                id = unhexlify(hex_id)",
            "            except ValueError:",
            "                print(\"object id %s is invalid.\" % hex_id)",
            "            else:",
            "                try:",
            "                    refcount = cache.chunks[id][0]",
            "                    print(\"object %s has %d referrers [info from chunks cache].\" % (hex_id, refcount))",
            "                except KeyError:",
            "                    print(\"object %s not found [info from chunks cache].\" % hex_id)",
            "        return EXIT_SUCCESS",
            "",
            "    def do_debug_convert_profile(self, args):",
            "        \"\"\"convert Borg profile to Python profile\"\"\"",
            "        import marshal",
            "        with args.output, args.input:",
            "            marshal.dump(msgpack.unpack(args.input, use_list=False, encoding='utf-8'), args.output)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(lock=False, manifest=False)",
            "    def do_break_lock(self, args, repository):",
            "        \"\"\"Break the repository lock (e.g. in case it was left by a dead borg.\"\"\"",
            "        repository.break_lock()",
            "        Cache.break_lock(repository)",
            "        return self.exit_code",
            "",
            "    helptext = collections.OrderedDict()",
            "    helptext['patterns'] = textwrap.dedent('''",
            "        File patterns support these styles: fnmatch, shell, regular expressions,",
            "        path prefixes and path full-matches. By default, fnmatch is used for",
            "        ``--exclude`` patterns and shell-style is used for the experimental ``--pattern``",
            "        option.",
            "",
            "        If followed by a colon (':') the first two characters of a pattern are used as a",
            "        style selector. Explicit style selection is necessary when a",
            "        non-default style is desired or when the desired pattern starts with",
            "        two alphanumeric characters followed by a colon (i.e. `aa:something/*`).",
            "",
            "        `Fnmatch <https://docs.python.org/3/library/fnmatch.html>`_, selector `fm:`",
            "            This is the default style for ``--exclude`` and ``--exclude-from``.",
            "            These patterns use a variant of shell pattern syntax, with '\\*' matching",
            "            any number of characters, '?' matching any single character, '[...]'",
            "            matching any single character specified, including ranges, and '[!...]'",
            "            matching any character not specified. For the purpose of these patterns,",
            "            the path separator ('\\\\' for Windows and '/' on other systems) is not",
            "            treated specially. Wrap meta-characters in brackets for a literal",
            "            match (i.e. `[?]` to match the literal character `?`). For a path",
            "            to match a pattern, it must completely match from start to end, or",
            "            must match from the start to just before a path separator. Except",
            "            for the root path, paths will never end in the path separator when",
            "            matching is attempted.  Thus, if a given pattern ends in a path",
            "            separator, a '\\*' is appended before matching is attempted.",
            "",
            "        Shell-style patterns, selector `sh:`",
            "            This is the default style for ``--pattern`` and ``--patterns-from``.",
            "            Like fnmatch patterns these are similar to shell patterns. The difference",
            "            is that the pattern may include `**/` for matching zero or more directory",
            "            levels, `*` for matching zero or more arbitrary characters with the",
            "            exception of any path separator.",
            "",
            "        Regular expressions, selector `re:`",
            "            Regular expressions similar to those found in Perl are supported. Unlike",
            "            shell patterns regular expressions are not required to match the complete",
            "            path and any substring match is sufficient. It is strongly recommended to",
            "            anchor patterns to the start ('^'), to the end ('$') or both. Path",
            "            separators ('\\\\' for Windows and '/' on other systems) in paths are",
            "            always normalized to a forward slash ('/') before applying a pattern. The",
            "            regular expression syntax is described in the `Python documentation for",
            "            the re module <https://docs.python.org/3/library/re.html>`_.",
            "",
            "        Path prefix, selector `pp:`",
            "            This pattern style is useful to match whole sub-directories. The pattern",
            "            `pp:/data/bar` matches `/data/bar` and everything therein.",
            "",
            "        Path full-match, selector `pf:`",
            "            This pattern style is useful to match whole paths.",
            "            This is kind of a pseudo pattern as it can not have any variable or",
            "            unspecified parts - the full, precise path must be given.",
            "            `pf:/data/foo.txt` matches `/data/foo.txt` only.",
            "",
            "            Implementation note: this is implemented via very time-efficient O(1)",
            "            hashtable lookups (this means you can have huge amounts of such patterns",
            "            without impacting performance much).",
            "            Due to that, this kind of pattern does not respect any context or order.",
            "            If you use such a pattern to include a file, it will always be included",
            "            (if the directory recursion encounters it).",
            "            Other include/exclude patterns that would normally match will be ignored.",
            "            Same logic applies for exclude.",
            "",
            "        .. note::",
            "",
            "            `re:`, `sh:` and `fm:` patterns are all implemented on top of the Python SRE",
            "            engine. It is very easy to formulate patterns for each of these types which",
            "            requires an inordinate amount of time to match paths. If untrusted users",
            "            are able to supply patterns, ensure they cannot supply `re:` patterns.",
            "            Further, ensure that `sh:` and `fm:` patterns only contain a handful of",
            "            wildcards at most.",
            "",
            "        Exclusions can be passed via the command line option ``--exclude``. When used",
            "        from within a shell the patterns should be quoted to protect them from",
            "        expansion.",
            "",
            "        The ``--exclude-from`` option permits loading exclusion patterns from a text",
            "        file with one pattern per line. Lines empty or starting with the number sign",
            "        ('#') after removing whitespace on both ends are ignored. The optional style",
            "        selector prefix is also supported for patterns loaded from a file. Due to",
            "        whitespace removal paths with whitespace at the beginning or end can only be",
            "        excluded using regular expressions.",
            "",
            "        Examples::",
            "",
            "            # Exclude '/home/user/file.o' but not '/home/user/file.odt':",
            "            $ borg create -e '*.o' backup /",
            "",
            "            # Exclude '/home/user/junk' and '/home/user/subdir/junk' but",
            "            # not '/home/user/importantjunk' or '/etc/junk':",
            "            $ borg create -e '/home/*/junk' backup /",
            "",
            "            # Exclude the contents of '/home/user/cache' but not the directory itself:",
            "            $ borg create -e /home/user/cache/ backup /",
            "",
            "            # The file '/home/user/cache/important' is *not* backed up:",
            "            $ borg create -e /home/user/cache/ backup / /home/user/cache/important",
            "",
            "            # The contents of directories in '/home' are not backed up when their name",
            "            # ends in '.tmp'",
            "            $ borg create --exclude 're:^/home/[^/]+\\.tmp/' backup /",
            "",
            "            # Load exclusions from file",
            "            $ cat >exclude.txt <<EOF",
            "            # Comment line",
            "            /home/*/junk",
            "            *.tmp",
            "            fm:aa:something/*",
            "            re:^/home/[^/]\\.tmp/",
            "            sh:/home/*/.thumbnails",
            "            EOF",
            "            $ borg create --exclude-from exclude.txt backup /",
            "",
            "        .. container:: experimental",
            "",
            "            A more general and easier to use way to define filename matching patterns exists",
            "            with the experimental ``--pattern`` and ``--patterns-from`` options. Using these, you",
            "            may specify the backup roots (starting points) and patterns for inclusion/exclusion.",
            "            A root path starts with the prefix `R`, followed by a path (a plain path, not a",
            "            file pattern). An include rule starts with the prefix +, an exclude rule starts",
            "            with the prefix -, an exclude-norecurse rule starts with !, all followed by a pattern.",
            "            Inclusion patterns are useful to include paths that are contained in an excluded",
            "            path. The first matching pattern is used so if an include pattern matches before",
            "            an exclude pattern, the file is backed up. If an exclude-norecurse pattern matches",
            "            a directory, it won't recurse into it and won't discover any potential matches for",
            "            include rules below that directory.",
            "",
            "            Note that the default pattern style for ``--pattern`` and ``--patterns-from`` is",
            "            shell style (`sh:`), so those patterns behave similar to rsync include/exclude",
            "            patterns. The pattern style can be set via the `P` prefix.",
            "",
            "            Patterns (``--pattern``) and excludes (``--exclude``) from the command line are",
            "            considered first (in the order of appearance). Then patterns from ``--patterns-from``",
            "            are added. Exclusion patterns from ``--exclude-from`` files are appended last.",
            "",
            "            An example ``--patterns-from`` file could look like that::",
            "",
            "                # \"sh:\" pattern style is the default, so the following line is not needed:",
            "                P sh",
            "                R /",
            "                # can be rebuild",
            "                - /home/*/.cache",
            "                # they're downloads for a reason",
            "                - /home/*/Downloads",
            "                # susan is a nice person",
            "                # include susans home",
            "                + /home/susan",
            "                # don't backup the other home directories",
            "                - /home/*\\n\\n''')",
            "    helptext['placeholders'] = textwrap.dedent('''",
            "        Repository (or Archive) URLs, ``--prefix`` and ``--remote-path`` values support these",
            "        placeholders:",
            "",
            "        {hostname}",
            "            The (short) hostname of the machine.",
            "",
            "        {fqdn}",
            "            The full name of the machine.",
            "",
            "        {now}",
            "            The current local date and time, by default in ISO-8601 format.",
            "            You can also supply your own `format string <https://docs.python.org/3.5/library/datetime.html#strftime-and-strptime-behavior>`_, e.g. {now:%Y-%m-%d_%H:%M:%S}",
            "",
            "        {utcnow}",
            "            The current UTC date and time, by default in ISO-8601 format.",
            "            You can also supply your own `format string <https://docs.python.org/3.5/library/datetime.html#strftime-and-strptime-behavior>`_, e.g. {utcnow:%Y-%m-%d_%H:%M:%S}",
            "",
            "        {user}",
            "            The user name (or UID, if no name is available) of the user running borg.",
            "",
            "        {pid}",
            "            The current process ID.",
            "",
            "        {borgversion}",
            "            The version of borg, e.g.: 1.0.8rc1",
            "",
            "        {borgmajor}",
            "            The version of borg, only the major version, e.g.: 1",
            "",
            "        {borgminor}",
            "            The version of borg, only major and minor version, e.g.: 1.0",
            "",
            "        {borgpatch}",
            "            The version of borg, only major, minor and patch version, e.g.: 1.0.8",
            "",
            "        If literal curly braces need to be used, double them for escaping::",
            "",
            "            borg create /path/to/repo::{{literal_text}}",
            "",
            "        Examples::",
            "",
            "            borg create /path/to/repo::{hostname}-{user}-{utcnow} ...",
            "            borg create /path/to/repo::{hostname}-{now:%Y-%m-%d_%H:%M:%S} ...",
            "            borg prune --prefix '{hostname}-' ...",
            "",
            "        .. note::",
            "            systemd uses a difficult, non-standard syntax for command lines in unit files (refer to",
            "            the `systemd.unit(5)` manual page).",
            "",
            "            When invoking borg from unit files, pay particular attention to escaping,",
            "            especially when using the now/utcnow placeholders, since systemd performs its own",
            "            %-based variable replacement even in quoted text. To avoid interference from systemd,",
            "            double all percent signs (``{hostname}-{now:%Y-%m-%d_%H:%M:%S}``",
            "            becomes ``{hostname}-{now:%%Y-%%m-%%d_%%H:%%M:%%S}``).\\n\\n''')",
            "    helptext['compression'] = textwrap.dedent('''",
            "        It is no problem to mix different compression methods in one repo,",
            "        deduplication is done on the source data chunks (not on the compressed",
            "        or encrypted data).",
            "",
            "        If some specific chunk was once compressed and stored into the repo, creating",
            "        another backup that also uses this chunk will not change the stored chunk.",
            "        So if you use different compression specs for the backups, whichever stores a",
            "        chunk first determines its compression. See also borg recreate.",
            "",
            "        Compression is lz4 by default. If you want something else, you have to specify what you want.",
            "",
            "        Valid compression specifiers are:",
            "",
            "        none",
            "            Do not compress.",
            "",
            "        lz4",
            "            Use lz4 compression. High speed, low compression. (default)",
            "",
            "        zlib[,L]",
            "            Use zlib (\"gz\") compression. Medium speed, medium compression.",
            "            If you do not explicitely give the compression level L (ranging from 0",
            "            to 9), it will use level 6.",
            "            Giving level 0 (means \"no compression\", but still has zlib protocol",
            "            overhead) is usually pointless, you better use \"none\" compression.",
            "",
            "        lzma[,L]",
            "            Use lzma (\"xz\") compression. Low speed, high compression.",
            "            If you do not explicitely give the compression level L (ranging from 0",
            "            to 9), it will use level 6.",
            "            Giving levels above 6 is pointless and counterproductive because it does",
            "            not compress better due to the buffer size used by borg - but it wastes",
            "            lots of CPU cycles and RAM.",
            "",
            "        auto,C[,L]",
            "            Use a built-in heuristic to decide per chunk whether to compress or not.",
            "            The heuristic tries with lz4 whether the data is compressible.",
            "            For incompressible data, it will not use compression (uses \"none\").",
            "            For compressible data, it uses the given C[,L] compression - with C[,L]",
            "            being any valid compression specifier.",
            "",
            "        Examples::",
            "",
            "            borg create --compression lz4 REPO::ARCHIVE data",
            "            borg create --compression zlib REPO::ARCHIVE data",
            "            borg create --compression zlib,1 REPO::ARCHIVE data",
            "            borg create --compression auto,lzma,6 REPO::ARCHIVE data",
            "            borg create --compression auto,lzma ...\\n\\n''')",
            "",
            "    def do_help(self, parser, commands, args):",
            "        if not args.topic:",
            "            parser.print_help()",
            "        elif args.topic in self.helptext:",
            "            print(rst_to_terminal(self.helptext[args.topic]))",
            "        elif args.topic in commands:",
            "            if args.epilog_only:",
            "                print(commands[args.topic].epilog)",
            "            elif args.usage_only:",
            "                commands[args.topic].epilog = None",
            "                commands[args.topic].print_help()",
            "            else:",
            "                commands[args.topic].print_help()",
            "        else:",
            "            parser.error('No help available on %s' % (args.topic,))",
            "        return self.exit_code",
            "",
            "    def do_subcommand_help(self, parser, args):",
            "        \"\"\"display infos about subcommand\"\"\"",
            "        parser.print_help()",
            "        return EXIT_SUCCESS",
            "",
            "    do_maincommand_help = do_subcommand_help",
            "",
            "    def preprocess_args(self, args):",
            "        deprecations = [",
            "            # ('--old', '--new' or None, 'Warning: \"--old\" has been deprecated. Use \"--new\" instead.'),",
            "            ('--list-format', '--format', 'Warning: \"--list-format\" has been deprecated. Use \"--format\" instead.'),",
            "            ('--keep-tag-files', '--keep-exclude-tags', 'Warning: \"--keep-tag-files\" has been deprecated. Use \"--keep-exclude-tags\" instead.'),",
            "            ('--ignore-inode', None, 'Warning: \"--ignore-inode\" has been deprecated. Use \"--files-cache=ctime,size\" or \"...=mtime,size\" instead.'),",
            "            ('--no-files-cache', None, 'Warning: \"--no-files-cache\" has been deprecated. Use \"--files-cache=disabled\" instead.'),",
            "        ]",
            "        for i, arg in enumerate(args[:]):",
            "            for old_name, new_name, warning in deprecations:",
            "                if arg.startswith(old_name):",
            "                    if new_name is not None:",
            "                        args[i] = arg.replace(old_name, new_name)",
            "                    print(warning, file=sys.stderr)",
            "        return args",
            "",
            "    class CommonOptions:",
            "        \"\"\"",
            "        Support class to allow specifying common options directly after the top-level command.",
            "",
            "        Normally options can only be specified on the parser defining them, which means",
            "        that generally speaking *all* options go after all sub-commands. This is annoying",
            "        for common options in scripts, e.g. --remote-path or logging options.",
            "",
            "        This class allows adding the same set of options to both the top-level parser",
            "        and the final sub-command parsers (but not intermediary sub-commands, at least for now).",
            "",
            "        It does so by giving every option's target name (\"dest\") a suffix indicating its level",
            "        -- no two options in the parser hierarchy can have the same target --",
            "        then, after parsing the command line, multiple definitions are resolved.",
            "",
            "        Defaults are handled by only setting them on the top-level parser and setting",
            "        a sentinel object in all sub-parsers, which then allows to discern which parser",
            "        supplied the option.",
            "        \"\"\"",
            "",
            "        def __init__(self, define_common_options, suffix_precedence):",
            "            \"\"\"",
            "            *define_common_options* should be a callable taking one argument, which",
            "            will be a argparse.Parser.add_argument-like function.",
            "",
            "            *define_common_options* will be called multiple times, and should call",
            "            the passed function to define common options exactly the same way each time.",
            "",
            "            *suffix_precedence* should be a tuple of the suffixes that will be used.",
            "            It is ordered from lowest precedence to highest precedence:",
            "            An option specified on the parser belonging to index 0 is overridden if the",
            "            same option is specified on any parser with a higher index.",
            "            \"\"\"",
            "            self.define_common_options = define_common_options",
            "            self.suffix_precedence = suffix_precedence",
            "",
            "            # Maps suffixes to sets of target names.",
            "            # E.g. common_options[\"_subcommand\"] = {..., \"log_level\", ...}",
            "            self.common_options = dict()",
            "            # Set of options with the 'append' action.",
            "            self.append_options = set()",
            "            # This is the sentinel object that replaces all default values in parsers",
            "            # below the top-level parser.",
            "            self.default_sentinel = object()",
            "",
            "        def add_common_group(self, parser, suffix, provide_defaults=False):",
            "            \"\"\"",
            "            Add common options to *parser*.",
            "",
            "            *provide_defaults* must only be True exactly once in a parser hierarchy,",
            "            at the top level, and False on all lower levels. The default is chosen",
            "            accordingly.",
            "",
            "            *suffix* indicates the suffix to use internally. It also indicates",
            "            which precedence the *parser* has for common options. See *suffix_precedence*",
            "            of __init__.",
            "            \"\"\"",
            "            assert suffix in self.suffix_precedence",
            "",
            "            def add_argument(*args, **kwargs):",
            "                if 'dest' in kwargs:",
            "                    kwargs.setdefault('action', 'store')",
            "                    assert kwargs['action'] in ('help', 'store_const', 'store_true', 'store_false', 'store', 'append')",
            "                    is_append = kwargs['action'] == 'append'",
            "                    if is_append:",
            "                        self.append_options.add(kwargs['dest'])",
            "                        assert kwargs['default'] == [], 'The default is explicitly constructed as an empty list in resolve()'",
            "                    else:",
            "                        self.common_options.setdefault(suffix, set()).add(kwargs['dest'])",
            "                    kwargs['dest'] += suffix",
            "                    if not provide_defaults:",
            "                        # Interpolate help now, in case the %(default)d (or so) is mentioned,",
            "                        # to avoid producing incorrect help output.",
            "                        # Assumption: Interpolated output can safely be interpolated again,",
            "                        # which should always be the case.",
            "                        # Note: We control all inputs.",
            "                        kwargs['help'] = kwargs['help'] % kwargs",
            "                        if not is_append:",
            "                            kwargs['default'] = self.default_sentinel",
            "",
            "                common_group.add_argument(*args, **kwargs)",
            "",
            "            common_group = parser.add_argument_group('Common options')",
            "            self.define_common_options(add_argument)",
            "",
            "        def resolve(self, args: argparse.Namespace):  # Namespace has \"in\" but otherwise is not like a dict.",
            "            \"\"\"",
            "            Resolve the multiple definitions of each common option to the final value.",
            "            \"\"\"",
            "            for suffix in self.suffix_precedence:",
            "                # From highest level to lowest level, so the \"most-specific\" option wins, e.g.",
            "                # \"borg --debug create --info\" shall result in --info being effective.",
            "                for dest in self.common_options.get(suffix, []):",
            "                    # map_from is this suffix' option name, e.g. log_level_subcommand",
            "                    # map_to is the target name, e.g. log_level",
            "                    map_from = dest + suffix",
            "                    map_to = dest",
            "                    # Retrieve value; depending on the action it may not exist, but usually does",
            "                    # (store_const/store_true/store_false), either because the action implied a default",
            "                    # or a default is explicitly supplied.",
            "                    # Note that defaults on lower levels are replaced with default_sentinel.",
            "                    # Only the top level has defaults.",
            "                    value = getattr(args, map_from, self.default_sentinel)",
            "                    if value is not self.default_sentinel:",
            "                        # value was indeed specified on this level. Transfer value to target,",
            "                        # and un-clobber the args (for tidiness - you *cannot* use the suffixed",
            "                        # names for other purposes, obviously).",
            "                        setattr(args, map_to, value)",
            "                    try:",
            "                        delattr(args, map_from)",
            "                    except AttributeError:",
            "                        pass",
            "",
            "            # Options with an \"append\" action need some special treatment. Instead of",
            "            # overriding values, all specified values are merged together.",
            "            for dest in self.append_options:",
            "                option_value = []",
            "                for suffix in self.suffix_precedence:",
            "                    # Find values of this suffix, if any, and add them to the final list",
            "                    extend_from = dest + suffix",
            "                    if extend_from in args:",
            "                        values = getattr(args, extend_from)",
            "                        delattr(args, extend_from)",
            "                        option_value.extend(values)",
            "                setattr(args, dest, option_value)",
            "",
            "    def build_parser(self):",
            "        # You can use :ref:`xyz` in the following usage pages. However, for plain-text view,",
            "        # e.g. through \"borg ... --help\", define a substitution for the reference here.",
            "        # It will replace the entire :ref:`foo` verbatim.",
            "        rst_plain_text_references = {",
            "            'a_status_oddity': '\"I am seeing \u2018A\u2019 (added) status for a unchanged file!?\"',",
            "        }",
            "",
            "        def process_epilog(epilog):",
            "            epilog = textwrap.dedent(epilog).splitlines()",
            "            try:",
            "                mode = borg.doc_mode",
            "            except AttributeError:",
            "                mode = 'command-line'",
            "            if mode in ('command-line', 'build_usage'):",
            "                epilog = [line for line in epilog if not line.startswith('.. man')]",
            "            epilog = '\\n'.join(epilog)",
            "            if mode == 'command-line':",
            "                epilog = rst_to_terminal(epilog, rst_plain_text_references)",
            "            return epilog",
            "",
            "        def define_common_options(add_common_option):",
            "            add_common_option('-h', '--help', action='help', help='show this help message and exit')",
            "            add_common_option('--critical', dest='log_level',",
            "                              action='store_const', const='critical', default='warning',",
            "                              help='work on log level CRITICAL')",
            "            add_common_option('--error', dest='log_level',",
            "                              action='store_const', const='error', default='warning',",
            "                              help='work on log level ERROR')",
            "            add_common_option('--warning', dest='log_level',",
            "                              action='store_const', const='warning', default='warning',",
            "                              help='work on log level WARNING (default)')",
            "            add_common_option('--info', '-v', '--verbose', dest='log_level',",
            "                              action='store_const', const='info', default='warning',",
            "                              help='work on log level INFO')",
            "            add_common_option('--debug', dest='log_level',",
            "                              action='store_const', const='debug', default='warning',",
            "                              help='enable debug output, work on log level DEBUG')",
            "            add_common_option('--debug-topic', metavar='TOPIC', dest='debug_topics', action='append', default=[],",
            "                              help='enable TOPIC debugging (can be specified multiple times). '",
            "                                   'The logger path is borg.debug.<TOPIC> if TOPIC is not fully qualified.')",
            "            add_common_option('-p', '--progress', dest='progress', action='store_true',",
            "                              help='show progress information')",
            "            add_common_option('--log-json', dest='log_json', action='store_true',",
            "                              help='Output one JSON object per log line instead of formatted text.')",
            "            add_common_option('--lock-wait', metavar='SECONDS', dest='lock_wait', type=int, default=1,",
            "                              help='wait at most SECONDS for acquiring a repository/cache lock (default: %(default)d).')",
            "            add_common_option('--show-version', dest='show_version', action='store_true',",
            "                              help='show/log the borg version')",
            "            add_common_option('--show-rc', dest='show_rc', action='store_true',",
            "                              help='show/log the return code (rc)')",
            "            add_common_option('--umask', metavar='M', dest='umask', type=lambda s: int(s, 8), default=UMASK_DEFAULT,",
            "                              help='set umask to M (local and remote, default: %(default)04o)')",
            "            add_common_option('--remote-path', metavar='PATH', dest='remote_path',",
            "                              help='use PATH as borg executable on the remote (default: \"borg\")')",
            "            add_common_option('--remote-ratelimit', metavar='RATE', dest='remote_ratelimit', type=int,",
            "                              help='set remote network upload rate limit in kiByte/s (default: 0=unlimited)')",
            "            add_common_option('--consider-part-files', dest='consider_part_files', action='store_true',",
            "                              help='treat part files like normal files (e.g. to list/extract them)')",
            "            add_common_option('--debug-profile', metavar='FILE', dest='debug_profile', default=None,",
            "                              help='Write execution profile in Borg format into FILE. For local use a Python-'",
            "                                   'compatible file can be generated by suffixing FILE with \".pyprof\".')",
            "",
            "        def define_exclude_and_patterns(add_option, *, tag_files=False, strip_components=False):",
            "            add_option('-e', '--exclude', metavar='PATTERN', dest='patterns',",
            "                       type=parse_exclude_pattern, action='append',",
            "                       help='exclude paths matching PATTERN')",
            "            add_option('--exclude-from', metavar='EXCLUDEFILE', action=ArgparseExcludeFileAction,",
            "                       help='read exclude patterns from EXCLUDEFILE, one per line')",
            "            add_option('--pattern', metavar='PATTERN', action=ArgparsePatternAction,",
            "                       help='experimental: include/exclude paths matching PATTERN')",
            "            add_option('--patterns-from', metavar='PATTERNFILE', action=ArgparsePatternFileAction,",
            "                       help='experimental: read include/exclude patterns from PATTERNFILE, one per line')",
            "",
            "            if tag_files:",
            "                add_option('--exclude-caches', dest='exclude_caches', action='store_true',",
            "                           help='exclude directories that contain a CACHEDIR.TAG file '",
            "                                '(http://www.brynosaurus.com/cachedir/spec.html)')",
            "                add_option('--exclude-if-present', metavar='NAME', dest='exclude_if_present',",
            "                           action='append', type=str,",
            "                           help='exclude directories that are tagged by containing a filesystem object with '",
            "                                'the given NAME')",
            "                add_option('--keep-exclude-tags', '--keep-tag-files', dest='keep_exclude_tags',",
            "                           action='store_true',",
            "                           help='if tag objects are specified with ``--exclude-if-present``, '",
            "                                'don\\'t omit the tag objects themselves from the backup archive')",
            "",
            "            if strip_components:",
            "                add_option('--strip-components', metavar='NUMBER', dest='strip_components', type=int, default=0,",
            "                           help='Remove the specified number of leading path elements. '",
            "                                'Paths with fewer elements will be silently skipped.')",
            "",
            "        def define_exclusion_group(subparser, **kwargs):",
            "            exclude_group = subparser.add_argument_group('Exclusion options')",
            "            define_exclude_and_patterns(exclude_group.add_argument, **kwargs)",
            "            return exclude_group",
            "",
            "        def define_archive_filters_group(subparser, *, sort_by=True, first_last=True):",
            "            filters_group = subparser.add_argument_group('Archive filters',",
            "                                                         'Archive filters can be applied to repository targets.')",
            "            group = filters_group.add_mutually_exclusive_group()",
            "            group.add_argument('-P', '--prefix', metavar='PREFIX', dest='prefix', type=PrefixSpec, default='',",
            "                               help='only consider archive names starting with this prefix.')",
            "            group.add_argument('-a', '--glob-archives', metavar='GLOB', dest='glob_archives', default=None,",
            "                               help='only consider archive names matching the glob. '",
            "                                    'sh: rules apply, see \"borg help patterns\". '",
            "                                    '``--prefix`` and ``--glob-archives`` are mutually exclusive.')",
            "",
            "            if sort_by:",
            "                sort_by_default = 'timestamp'",
            "                filters_group.add_argument('--sort-by', metavar='KEYS', dest='sort_by',",
            "                                           type=SortBySpec, default=sort_by_default,",
            "                                           help='Comma-separated list of sorting keys; valid keys are: {}; default is: {}'",
            "                                           .format(', '.join(AI_HUMAN_SORT_KEYS), sort_by_default))",
            "",
            "            if first_last:",
            "                group = filters_group.add_mutually_exclusive_group()",
            "                group.add_argument('--first', metavar='N', dest='first', default=0, type=positive_int_validator,",
            "                                   help='consider first N archives after other filters were applied')",
            "                group.add_argument('--last', metavar='N', dest='last', default=0, type=positive_int_validator,",
            "                                   help='consider last N archives after other filters were applied')",
            "",
            "        parser = argparse.ArgumentParser(prog=self.prog, description='Borg - Deduplicated Backups',",
            "                                         add_help=False)",
            "        parser.set_defaults(fallback2_func=functools.partial(self.do_maincommand_help, parser))",
            "        parser.common_options = self.CommonOptions(define_common_options,",
            "                                                   suffix_precedence=('_maincommand', '_midcommand', '_subcommand'))",
            "        parser.add_argument('-V', '--version', action='version', version='%(prog)s ' + __version__,",
            "                            help='show version number and exit')",
            "        parser.common_options.add_common_group(parser, '_maincommand', provide_defaults=True)",
            "",
            "        common_parser = argparse.ArgumentParser(add_help=False, prog=self.prog)",
            "        # some empty defaults for all subparsers",
            "        common_parser.set_defaults(paths=[], patterns=[])",
            "        parser.common_options.add_common_group(common_parser, '_subcommand')",
            "",
            "        mid_common_parser = argparse.ArgumentParser(add_help=False, prog=self.prog)",
            "        mid_common_parser.set_defaults(paths=[], patterns=[])",
            "        parser.common_options.add_common_group(mid_common_parser, '_midcommand')",
            "",
            "        mount_epilog = process_epilog(\"\"\"",
            "        This command mounts an archive as a FUSE filesystem. This can be useful for",
            "        browsing an archive or restoring individual files. Unless the ``--foreground``",
            "        option is given the command will run in the background until the filesystem",
            "        is ``umounted``.",
            "",
            "        The command ``borgfs`` provides a wrapper for ``borg mount``. This can also be",
            "        used in fstab entries:",
            "        ``/path/to/repo /mnt/point fuse.borgfs defaults,noauto 0 0``",
            "",
            "        To allow a regular user to use fstab entries, add the ``user`` option:",
            "        ``/path/to/repo /mnt/point fuse.borgfs defaults,noauto,user 0 0``",
            "",
            "        For mount options, see the fuse(8) manual page. Additional mount options",
            "        supported by borg:",
            "",
            "        - versions: when used with a repository mount, this gives a merged, versioned",
            "          view of the files in the archives. EXPERIMENTAL, layout may change in future.",
            "        - allow_damaged_files: by default damaged files (where missing chunks were",
            "          replaced with runs of zeros by borg check ``--repair``) are not readable and",
            "          return EIO (I/O error). Set this option to read such files.",
            "",
            "        The BORG_MOUNT_DATA_CACHE_ENTRIES environment variable is meant for advanced users",
            "        to tweak the performance. It sets the number of cached data chunks; additional",
            "        memory usage can be up to ~8 MiB times this number. The default is the number",
            "        of CPU cores.",
            "",
            "        When the daemonized process receives a signal or crashes, it does not unmount.",
            "        Unmounting in these cases could cause an active rsync or similar process",
            "        to unintentionally delete data.",
            "",
            "        When running in the foreground ^C/SIGINT unmounts cleanly, but other",
            "        signals or crashes do not.",
            "        \"\"\")",
            "",
            "        if parser.prog == 'borgfs':",
            "            parser.description = self.do_mount.__doc__",
            "            parser.epilog = mount_epilog",
            "            parser.formatter_class = argparse.RawDescriptionHelpFormatter",
            "            parser.help = 'mount repository'",
            "            subparser = parser",
            "        else:",
            "            subparsers = parser.add_subparsers(title='required arguments', metavar='<command>')",
            "            subparser = subparsers.add_parser('mount', parents=[common_parser], add_help=False,",
            "                                            description=self.do_mount.__doc__,",
            "                                            epilog=mount_epilog,",
            "                                            formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                            help='mount repository')",
            "        subparser.set_defaults(func=self.do_mount)",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', type=location_validator(),",
            "                            help='repository/archive to mount')",
            "        subparser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,",
            "                            help='where to mount filesystem')",
            "        subparser.add_argument('-f', '--foreground', dest='foreground',",
            "                            action='store_true',",
            "                            help='stay in foreground, do not daemonize')",
            "        subparser.add_argument('-o', dest='options', type=str,",
            "                            help='Extra mount options')",
            "        define_archive_filters_group(subparser)",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract; patterns are supported')",
            "        define_exclusion_group(subparser, strip_components=True)",
            "        if parser.prog == 'borgfs':",
            "            return parser",
            "",
            "        serve_epilog = process_epilog(\"\"\"",
            "        This command starts a repository server process. This command is usually not used manually.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('serve', parents=[common_parser], add_help=False,",
            "                                          description=self.do_serve.__doc__, epilog=serve_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='start repository server process')",
            "        subparser.set_defaults(func=self.do_serve)",
            "        subparser.add_argument('--restrict-to-path', metavar='PATH', dest='restrict_to_paths', action='append',",
            "                               help='restrict repository access to PATH. '",
            "                                    'Can be specified multiple times to allow the client access to several directories. '",
            "                                    'Access to all sub-directories is granted implicitly; PATH doesn\\'t need to directly point to a repository.')",
            "        subparser.add_argument('--restrict-to-repository', metavar='PATH', dest='restrict_to_repositories', action='append',",
            "                                help='restrict repository access. Only the repository located at PATH '",
            "                                     '(no sub-directories are considered) is accessible. '",
            "                                     'Can be specified multiple times to allow the client access to several repositories. '",
            "                                     'Unlike ``--restrict-to-path`` sub-directories are not accessible; '",
            "                                     'PATH needs to directly point at a repository location. '",
            "                                     'PATH may be an empty directory or the last element of PATH may not exist, in which case '",
            "                                     'the client may initialize a repository there.')",
            "        subparser.add_argument('--append-only', dest='append_only', action='store_true',",
            "                               help='only allow appending to repository segment files')",
            "        subparser.add_argument('--storage-quota', metavar='QUOTA', dest='storage_quota',",
            "                               type=parse_storage_quota, default=None,",
            "                               help='Override storage quota of the repository (e.g. 5G, 1.5T). '",
            "                                    'When a new repository is initialized, sets the storage quota on the new '",
            "                                    'repository as well. Default: no quota.')",
            "",
            "        init_epilog = process_epilog(\"\"\"",
            "        This command initializes an empty repository. A repository is a filesystem",
            "        directory containing the deduplicated data from zero or more archives.",
            "",
            "        Encryption can be enabled at repository init time. It cannot be changed later.",
            "",
            "        It is not recommended to work without encryption. Repository encryption protects",
            "        you e.g. against the case that an attacker has access to your backup repository.",
            "",
            "        But be careful with the key / the passphrase:",
            "",
            "        If you want \"passphrase-only\" security, use one of the repokey modes. The",
            "        key will be stored inside the repository (in its \"config\" file). In above",
            "        mentioned attack scenario, the attacker will have the key (but not the",
            "        passphrase).",
            "",
            "        If you want \"passphrase and having-the-key\" security, use one of the keyfile",
            "        modes. The key will be stored in your home directory (in .config/borg/keys).",
            "        In the attack scenario, the attacker who has just access to your repo won't",
            "        have the key (and also not the passphrase).",
            "",
            "        Make a backup copy of the key file (keyfile mode) or repo config file",
            "        (repokey mode) and keep it at a safe place, so you still have the key in",
            "        case it gets corrupted or lost. Also keep the passphrase at a safe place.",
            "        The backup that is encrypted with that key won't help you with that, of course.",
            "",
            "        Make sure you use a good passphrase. Not too short, not too simple. The real",
            "        encryption / decryption key is encrypted with / locked by your passphrase.",
            "        If an attacker gets your key, he can't unlock and use it without knowing the",
            "        passphrase.",
            "",
            "        Be careful with special or non-ascii characters in your passphrase:",
            "",
            "        - Borg processes the passphrase as unicode (and encodes it as utf-8),",
            "          so it does not have problems dealing with even the strangest characters.",
            "        - BUT: that does not necessarily apply to your OS / VM / keyboard configuration.",
            "",
            "        So better use a long passphrase made from simple ascii chars than one that",
            "        includes non-ascii stuff or characters that are hard/impossible to enter on",
            "        a different keyboard layout.",
            "",
            "        You can change your passphrase for existing repos at any time, it won't affect",
            "        the encryption/decryption key or other secrets.",
            "",
            "        Encryption modes",
            "        ++++++++++++++++",
            "",
            "        .. nanorst: inline-fill",
            "",
            "        +----------+---------------+------------------------+--------------------------+",
            "        | Hash/MAC | Not encrypted | Not encrypted,         | Encrypted (AEAD w/ AES)  |",
            "        |          | no auth       | but authenticated      | and authenticated        |",
            "        +----------+---------------+------------------------+--------------------------+",
            "        | SHA-256  | none          | `authenticated`        | repokey                  |",
            "        |          |               |                        | keyfile                  |",
            "        +----------+---------------+------------------------+--------------------------+",
            "        | BLAKE2b  | n/a           | `authenticated-blake2` | `repokey-blake2`         |",
            "        |          |               |                        | `keyfile-blake2`         |",
            "        +----------+---------------+------------------------+--------------------------+",
            "",
            "        .. nanorst: inline-replace",
            "",
            "        `Marked modes` are new in Borg 1.1 and are not backwards-compatible with Borg 1.0.x.",
            "",
            "        On modern Intel/AMD CPUs (except very cheap ones), AES is usually",
            "        hardware-accelerated.",
            "        BLAKE2b is faster than SHA256 on Intel/AMD 64-bit CPUs",
            "        (except AMD Ryzen and future CPUs with SHA extensions),",
            "        which makes `authenticated-blake2` faster than `none` and `authenticated`.",
            "",
            "        On modern ARM CPUs, NEON provides hardware acceleration for SHA256 making it faster",
            "        than BLAKE2b-256 there. NEON accelerates AES as well.",
            "",
            "        Hardware acceleration is always used automatically when available.",
            "",
            "        `repokey` and `keyfile` use AES-CTR-256 for encryption and HMAC-SHA256 for",
            "        authentication in an encrypt-then-MAC (EtM) construction. The chunk ID hash",
            "        is HMAC-SHA256 as well (with a separate key).",
            "        These modes are compatible with Borg 1.0.x.",
            "",
            "        `repokey-blake2` and `keyfile-blake2` are also authenticated encryption modes,",
            "        but use BLAKE2b-256 instead of HMAC-SHA256 for authentication. The chunk ID",
            "        hash is a keyed BLAKE2b-256 hash.",
            "        These modes are new and *not* compatible with Borg 1.0.x.",
            "",
            "        `authenticated` mode uses no encryption, but authenticates repository contents",
            "        through the same HMAC-SHA256 hash as the `repokey` and `keyfile` modes (it uses it",
            "        as the chunk ID hash). The key is stored like `repokey`.",
            "        This mode is new and *not* compatible with Borg 1.0.x.",
            "",
            "        `authenticated-blake2` is like `authenticated`, but uses the keyed BLAKE2b-256 hash",
            "        from the other blake2 modes.",
            "        This mode is new and *not* compatible with Borg 1.0.x.",
            "",
            "        `none` mode uses no encryption and no authentication. It uses SHA256 as chunk",
            "        ID hash. Not recommended, rather consider using an authenticated or",
            "        authenticated/encrypted mode. This mode has possible denial-of-service issues",
            "        when running ``borg create`` on contents controlled by an attacker.",
            "        Use it only for new repositories where no encryption is wanted **and** when compatibility",
            "        with 1.0.x is important. If compatibility with 1.0.x is not important, use",
            "        `authenticated-blake2` or `authenticated` instead.",
            "        This mode is compatible with Borg 1.0.x.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('init', parents=[common_parser], add_help=False,",
            "                                          description=self.do_init.__doc__, epilog=init_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='initialize empty repository')",
            "        subparser.set_defaults(func=self.do_init)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to create')",
            "        subparser.add_argument('-e', '--encryption', metavar='MODE', dest='encryption', required=True,",
            "                               choices=key_argument_names(),",
            "                               help='select encryption key mode **(required)**')",
            "        subparser.add_argument('--append-only', dest='append_only', action='store_true',",
            "                               help='create an append-only mode repository')",
            "        subparser.add_argument('--storage-quota', metavar='QUOTA', dest='storage_quota', default=None,",
            "                               type=parse_storage_quota,",
            "                               help='Set storage quota of the new repository (e.g. 5G, 1.5T). Default: no quota.')",
            "",
            "        check_epilog = process_epilog(\"\"\"",
            "        The check command verifies the consistency of a repository and the corresponding archives.",
            "",
            "        First, the underlying repository data files are checked:",
            "",
            "        - For all segments the segment magic (header) is checked",
            "        - For all objects stored in the segments, all metadata (e.g. crc and size) and",
            "          all data is read. The read data is checked by size and CRC. Bit rot and other",
            "          types of accidental damage can be detected this way.",
            "        - If we are in repair mode and a integrity error is detected for a segment,",
            "          we try to recover as many objects from the segment as possible.",
            "        - In repair mode, it makes sure that the index is consistent with the data",
            "          stored in the segments.",
            "        - If you use a remote repo server via ssh:, the repo check is executed on the",
            "          repo server without causing significant network traffic.",
            "        - The repository check can be skipped using the ``--archives-only`` option.",
            "",
            "        Second, the consistency and correctness of the archive metadata is verified:",
            "",
            "        - Is the repo manifest present? If not, it is rebuilt from archive metadata",
            "          chunks (this requires reading and decrypting of all metadata and data).",
            "        - Check if archive metadata chunk is present. if not, remove archive from",
            "          manifest.",
            "        - For all files (items) in the archive, for all chunks referenced by these",
            "          files, check if chunk is present.",
            "          If a chunk is not present and we are in repair mode, replace it with a same-size",
            "          replacement chunk of zeros.",
            "          If a previously lost chunk reappears (e.g. via a later backup) and we are in",
            "          repair mode, the all-zero replacement chunk will be replaced by the correct chunk.",
            "          This requires reading of archive and file metadata, but not data.",
            "        - If we are in repair mode and we checked all the archives: delete orphaned",
            "          chunks from the repo.",
            "        - if you use a remote repo server via ssh:, the archive check is executed on",
            "          the client machine (because if encryption is enabled, the checks will require",
            "          decryption and this is always done client-side, because key access will be",
            "          required).",
            "        - The archive checks can be time consuming, they can be skipped using the",
            "          ``--repository-only`` option.",
            "",
            "        The ``--verify-data`` option will perform a full integrity verification (as opposed to",
            "        checking the CRC32 of the segment) of data, which means reading the data from the",
            "        repository, decrypting and decompressing it. This is a cryptographic verification,",
            "        which will detect (accidental) corruption. For encrypted repositories it is",
            "        tamper-resistant as well, unless the attacker has access to the keys.",
            "",
            "        It is also very slow.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('check', parents=[common_parser], add_help=False,",
            "                                          description=self.do_check.__doc__,",
            "                                          epilog=check_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='verify repository')",
            "        subparser.set_defaults(func=self.do_check)",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='repository or archive to check consistency of')",
            "        subparser.add_argument('--repository-only', dest='repo_only', action='store_true',",
            "                               help='only perform repository checks')",
            "        subparser.add_argument('--archives-only', dest='archives_only', action='store_true',",
            "                               help='only perform archives checks')",
            "        subparser.add_argument('--verify-data', dest='verify_data', action='store_true',",
            "                               help='perform cryptographic archive data integrity verification '",
            "                                    '(conflicts with ``--repository-only``)')",
            "        subparser.add_argument('--repair', dest='repair', action='store_true',",
            "                               help='attempt to repair any inconsistencies found')",
            "        subparser.add_argument('--save-space', dest='save_space', action='store_true',",
            "                               help='work slower, but using less space')",
            "        define_archive_filters_group(subparser)",
            "",
            "        subparser = subparsers.add_parser('key', parents=[mid_common_parser], add_help=False,",
            "                                          description=\"Manage a keyfile or repokey of a repository\",",
            "                                          epilog=\"\",",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='manage repository key')",
            "",
            "        key_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')",
            "        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))",
            "",
            "        key_export_epilog = process_epilog(\"\"\"",
            "        If repository encryption is used, the repository is inaccessible",
            "        without the key. This command allows to backup this essential key.",
            "        Note that the backup produced does not include the passphrase itself",
            "        (i.e. the exported key stays encrypted). In order to regain access to a",
            "        repository, one needs both the exported key and the original passphrase.",
            "",
            "        There are three backup formats. The normal backup format is suitable for",
            "        digital storage as a file. The ``--paper`` backup format is optimized",
            "        for printing and typing in while importing, with per line checks to",
            "        reduce problems with manual input. The ``--qr-html`` creates a printable",
            "        HTML template with a QR code and a copy of the ``--paper``-formatted key.",
            "",
            "        For repositories using keyfile encryption the key is saved locally",
            "        on the system that is capable of doing backups. To guard against loss",
            "        of this key, the key needs to be backed up independently of the main",
            "        data backup.",
            "",
            "        For repositories using the repokey encryption the key is saved in the",
            "        repository in the config file. A backup is thus not strictly needed,",
            "        but guards against the repository becoming inaccessible if the file",
            "        is damaged for some reason.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('export', parents=[common_parser], add_help=False,",
            "                                          description=self.do_key_export.__doc__,",
            "                                          epilog=key_export_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='export repository key for backup')",
            "        subparser.set_defaults(func=self.do_key_export)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "        subparser.add_argument('path', metavar='PATH', nargs='?', type=str,",
            "                               help='where to store the backup')",
            "        subparser.add_argument('--paper', dest='paper', action='store_true',",
            "                               help='Create an export suitable for printing and later type-in')",
            "        subparser.add_argument('--qr-html', dest='qr', action='store_true',",
            "                               help='Create an html file suitable for printing and later type-in or qr scan')",
            "",
            "        key_import_epilog = process_epilog(\"\"\"",
            "        This command allows to restore a key previously backed up with the",
            "        export command.",
            "",
            "        If the ``--paper`` option is given, the import will be an interactive",
            "        process in which each line is checked for plausibility before",
            "        proceeding to the next line. For this format PATH must not be given.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('import', parents=[common_parser], add_help=False,",
            "                                          description=self.do_key_import.__doc__,",
            "                                          epilog=key_import_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='import repository key from backup')",
            "        subparser.set_defaults(func=self.do_key_import)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "        subparser.add_argument('path', metavar='PATH', nargs='?', type=str,",
            "                               help='path to the backup (\\'-\\' to read from stdin)')",
            "        subparser.add_argument('--paper', dest='paper', action='store_true',",
            "                               help='interactively import from a backup done with ``--paper``')",
            "",
            "        change_passphrase_epilog = process_epilog(\"\"\"",
            "        The key files used for repository encryption are optionally passphrase",
            "        protected. This command can be used to change this passphrase.",
            "",
            "        Please note that this command only changes the passphrase, but not any",
            "        secret protected by it (like e.g. encryption/MAC keys or chunker seed).",
            "        Thus, changing the passphrase after passphrase and borg key got compromised",
            "        does not protect future (nor past) backups to the same repository.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('change-passphrase', parents=[common_parser], add_help=False,",
            "                                          description=self.do_change_passphrase.__doc__,",
            "                                          epilog=change_passphrase_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='change repository passphrase')",
            "        subparser.set_defaults(func=self.do_change_passphrase)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "",
            "        # Borg 1.0 alias for change passphrase (without the \"key\" subcommand)",
            "        subparser = subparsers.add_parser('change-passphrase', parents=[common_parser], add_help=False,",
            "                                          description=self.do_change_passphrase.__doc__,",
            "                                          epilog=change_passphrase_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='change repository passphrase')",
            "        subparser.set_defaults(func=self.do_change_passphrase_deprecated)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "",
            "        migrate_to_repokey_epilog = process_epilog(\"\"\"",
            "        This command migrates a repository from passphrase mode (removed in Borg 1.0)",
            "        to repokey mode.",
            "",
            "        You will be first asked for the repository passphrase (to open it in passphrase",
            "        mode). This is the same passphrase as you used to use for this repo before 1.0.",
            "",
            "        It will then derive the different secrets from this passphrase.",
            "",
            "        Then you will be asked for a new passphrase (twice, for safety). This",
            "        passphrase will be used to protect the repokey (which contains these same",
            "        secrets in encrypted form). You may use the same passphrase as you used to",
            "        use, but you may also use a different one.",
            "",
            "        After migrating to repokey mode, you can change the passphrase at any time.",
            "        But please note: the secrets will always stay the same and they could always",
            "        be derived from your (old) passphrase-mode passphrase.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('migrate-to-repokey', parents=[common_parser], add_help=False,",
            "                                          description=self.do_migrate_to_repokey.__doc__,",
            "                                          epilog=migrate_to_repokey_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='migrate passphrase-mode repository to repokey')",
            "        subparser.set_defaults(func=self.do_migrate_to_repokey)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "",
            "        create_epilog = process_epilog(\"\"\"",
            "        This command creates a backup archive containing all files found while recursively",
            "        traversing all paths specified. Paths are added to the archive as they are given,",
            "        that means if relative paths are desired, the command has to be run from the correct",
            "        directory.",
            "",
            "        When giving '-' as path, borg will read data from standard input and create a",
            "        file 'stdin' in the created archive from that data.",
            "",
            "        The archive will consume almost no disk space for files or parts of files that",
            "        have already been stored in other archives.",
            "",
            "        The archive name needs to be unique. It must not end in '.checkpoint' or",
            "        '.checkpoint.N' (with N being a number), because these names are used for",
            "        checkpoints and treated in special ways.",
            "",
            "        In the archive name, you may use the following placeholders:",
            "        {now}, {utcnow}, {fqdn}, {hostname}, {user} and some others.",
            "",
            "        Backup speed is increased by not reprocessing files that are already part of",
            "        existing archives and weren't modified. The detection of unmodified files is",
            "        done by comparing multiple file metadata values with previous values kept in",
            "        the files cache.",
            "",
            "        This comparison can operate in different modes as given by ``--files-cache``:",
            "",
            "        - ctime,size,inode (default)",
            "        - mtime,size,inode (default behaviour of borg versions older than 1.1.0rc4)",
            "        - ctime,size (ignore the inode number)",
            "        - mtime,size (ignore the inode number)",
            "        - rechunk,ctime (all files are considered modified - rechunk, cache ctime)",
            "        - rechunk,mtime (all files are considered modified - rechunk, cache mtime)",
            "        - disabled (disable the files cache, all files considered modified - rechunk)",
            "",
            "        inode number: better safety, but often unstable on network filesystems",
            "",
            "        Normally, detecting file modifications will take inode information into",
            "        consideration to improve the reliability of file change detection.",
            "        This is problematic for files located on sshfs and similar network file",
            "        systems which do not provide stable inode numbers, such files will always",
            "        be considered modified. You can use modes without `inode` in this case to",
            "        improve performance, but reliability of change detection might be reduced.",
            "",
            "        ctime vs. mtime: safety vs. speed",
            "",
            "        - ctime is a rather safe way to detect changes to a file (metadata and contents)",
            "          as it can not be set from userspace. But, a metadata-only change will already",
            "          update the ctime, so there might be some unnecessary chunking/hashing even",
            "          without content changes. Some filesystems do not support ctime (change time).",
            "        - mtime usually works and only updates if file contents were changed. But mtime",
            "          can be arbitrarily set from userspace, e.g. to set mtime back to the same value",
            "          it had before a content change happened. This can be used maliciously as well as",
            "          well-meant, but in both cases mtime based cache modes can be problematic.",
            "",
            "        The mount points of filesystems or filesystem snapshots should be the same for every",
            "        creation of a new archive to ensure fast operation. This is because the file cache that",
            "        is used to determine changed files quickly uses absolute filenames.",
            "        If this is not possible, consider creating a bind mount to a stable location.",
            "",
            "        The ``--progress`` option shows (from left to right) Original, Compressed and Deduplicated",
            "        (O, C and D, respectively), then the Number of files (N) processed so far, followed by",
            "        the currently processed path.",
            "",
            "        When using ``--stats``, you will get some statistics about how much data was",
            "        added - the \"This Archive\" deduplicated size there is most interesting as that is",
            "        how much your repository will grow. Please note that the \"All archives\" stats refer to",
            "        the state after creation. Also, the ``--stats`` and ``--dry-run`` options are mutually",
            "        exclusive because the data is not actually compressed and deduplicated during a dry run.",
            "",
            "        See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "        See the output of the \"borg help placeholders\" command for more help on placeholders.",
            "",
            "        .. man NOTES",
            "",
            "        The ``--exclude`` patterns are not like tar. In tar ``--exclude`` .bundler/gems will",
            "        exclude foo/.bundler/gems. In borg it will not, you need to use ``--exclude``",
            "        '\\*/.bundler/gems' to get the same effect. See ``borg help patterns`` for",
            "        more information.",
            "",
            "        In addition to using ``--exclude`` patterns, it is possible to use",
            "        ``--exclude-if-present`` to specify the name of a filesystem object (e.g. a file",
            "        or folder name) which, when contained within another folder, will prevent the",
            "        containing folder from being backed up.  By default, the containing folder and",
            "        all of its contents will be omitted from the backup.  If, however, you wish to",
            "        only include the objects specified by ``--exclude-if-present`` in your backup,",
            "        and not include any other contents of the containing folder, this can be enabled",
            "        through using the ``--keep-exclude-tags`` option.",
            "",
            "        Item flags",
            "        ++++++++++",
            "",
            "        ``--list`` outputs a list of all files, directories and other",
            "        file system items it considered (no matter whether they had content changes",
            "        or not). For each item, it prefixes a single-letter flag that indicates type",
            "        and/or status of the item.",
            "",
            "        If you are interested only in a subset of that output, you can give e.g.",
            "        ``--filter=AME`` and it will only show regular files with A, M or E status (see",
            "        below).",
            "",
            "        A uppercase character represents the status of a regular file relative to the",
            "        \"files\" cache (not relative to the repo -- this is an issue if the files cache",
            "        is not used). Metadata is stored in any case and for 'A' and 'M' also new data",
            "        chunks are stored. For 'U' all data chunks refer to already existing chunks.",
            "",
            "        - 'A' = regular file, added (see also :ref:`a_status_oddity` in the FAQ)",
            "        - 'M' = regular file, modified",
            "        - 'U' = regular file, unchanged",
            "        - 'E' = regular file, an error happened while accessing/reading *this* file",
            "",
            "        A lowercase character means a file type other than a regular file,",
            "        borg usually just stores their metadata:",
            "",
            "        - 'd' = directory",
            "        - 'b' = block device",
            "        - 'c' = char device",
            "        - 'h' = regular file, hardlink (to already seen inodes)",
            "        - 's' = symlink",
            "        - 'f' = fifo",
            "",
            "        Other flags used include:",
            "",
            "        - 'i' = backup data was read from standard input (stdin)",
            "        - '-' = dry run, item was *not* backed up",
            "        - 'x' = excluded, item was *not* backed up",
            "        - '?' = missing status code (if you see this, please file a bug report!)",
            "        \"\"\")",
            "",
            "        subparser = subparsers.add_parser('create', parents=[common_parser], add_help=False,",
            "                                          description=self.do_create.__doc__,",
            "                                          epilog=create_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='create backup')",
            "        subparser.set_defaults(func=self.do_create)",
            "",
            "        dryrun_group = subparser.add_mutually_exclusive_group()",
            "        dryrun_group.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not create a backup archive')",
            "        dryrun_group.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics for the created archive')",
            "",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('--filter', metavar='STATUSCHARS', dest='output_filter',",
            "                               help='only display items with the given status characters (see description)')",
            "        subparser.add_argument('--json', action='store_true',",
            "                               help='output stats as JSON. Implies ``--stats``.')",
            "        subparser.add_argument('--no-cache-sync', dest='no_cache_sync', action='store_true',",
            "                               help='experimental: do not synchronize the cache. Implies not using the files cache.')",
            "        subparser.add_argument('--no-files-cache', dest='cache_files', action='store_false',",
            "                               help='do not load/update the file metadata cache used to detect unchanged files')",
            "",
            "        exclude_group = define_exclusion_group(subparser, tag_files=True)",
            "        exclude_group.add_argument('--exclude-nodump', dest='exclude_nodump', action='store_true',",
            "                                   help='exclude files flagged NODUMP')",
            "",
            "        fs_group = subparser.add_argument_group('Filesystem options')",
            "        fs_group.add_argument('-x', '--one-file-system', dest='one_file_system', action='store_true',",
            "                              help='stay in the same file system and do not store mount points of other file systems')",
            "        fs_group.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',",
            "                              help='only store numeric user and group identifiers')",
            "        fs_group.add_argument('--noatime', dest='noatime', action='store_true',",
            "                              help='do not store atime into archive')",
            "        fs_group.add_argument('--noctime', dest='noctime', action='store_true',",
            "                              help='do not store ctime into archive')",
            "        fs_group.add_argument('--nobirthtime', dest='nobirthtime', action='store_true',",
            "                              help='do not store birthtime (creation date) into archive')",
            "        fs_group.add_argument('--nobsdflags', dest='nobsdflags', action='store_true',",
            "                              help='do not read and store bsdflags (e.g. NODUMP, IMMUTABLE) into archive')",
            "        fs_group.add_argument('--ignore-inode', dest='ignore_inode', action='store_true',",
            "                              help='ignore inode data in the file metadata cache used to detect unchanged files.')",
            "        fs_group.add_argument('--files-cache', metavar='MODE', dest='files_cache_mode',",
            "                              type=FilesCacheMode, default=DEFAULT_FILES_CACHE_MODE_UI,",
            "                              help='operate files cache in MODE. default: %s' % DEFAULT_FILES_CACHE_MODE_UI)",
            "        fs_group.add_argument('--read-special', dest='read_special', action='store_true',",
            "                              help='open and read block and char device files as well as FIFOs as if they were '",
            "                                   'regular files. Also follows symlinks pointing to these kinds of files.')",
            "",
            "        archive_group = subparser.add_argument_group('Archive options')",
            "        archive_group.add_argument('--comment', dest='comment', metavar='COMMENT', default='',",
            "                                   help='add a comment text to the archive')",
            "        archive_group.add_argument('--timestamp', metavar='TIMESTAMP', dest='timestamp',",
            "                                   type=timestamp, default=None,",
            "                                   help='manually specify the archive creation date/time (UTC, yyyy-mm-ddThh:mm:ss format). '",
            "                                        'Alternatively, give a reference file/directory.')",
            "        archive_group.add_argument('-c', '--checkpoint-interval', metavar='SECONDS', dest='checkpoint_interval',",
            "                                   type=int, default=1800,",
            "                                   help='write checkpoint every SECONDS seconds (Default: 1800)')",
            "        archive_group.add_argument('--chunker-params', metavar='PARAMS', dest='chunker_params',",
            "                                   type=ChunkerParams, default=CHUNKER_PARAMS,",
            "                                   help='specify the chunker parameters (CHUNK_MIN_EXP, CHUNK_MAX_EXP, '",
            "                                        'HASH_MASK_BITS, HASH_WINDOW_SIZE). default: %d,%d,%d,%d' % CHUNKER_PARAMS)",
            "        archive_group.add_argument('-C', '--compression', metavar='COMPRESSION', dest='compression',",
            "                                   type=CompressionSpec, default=CompressionSpec('lz4'),",
            "                                   help='select compression algorithm, see the output of the '",
            "                                        '\"borg help compression\" command for details.')",
            "",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='name of archive to create (must be also a valid directory name)')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to archive')",
            "",
            "        extract_epilog = process_epilog(\"\"\"",
            "        This command extracts the contents of an archive. By default the entire",
            "        archive is extracted but a subset of files and directories can be selected",
            "        by passing a list of ``PATHs`` as arguments. The file selection can further",
            "        be restricted by using the ``--exclude`` option.",
            "",
            "        See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "",
            "        By using ``--dry-run``, you can do all extraction steps except actually writing the",
            "        output data: reading metadata and data chunks from the repo, checking the hash/hmac,",
            "        decrypting, decompressing.",
            "",
            "        ``--progress`` can be slower than no progress display, since it makes one additional",
            "        pass over the archive metadata.",
            "",
            "        .. note::",
            "",
            "            Currently, extract always writes into the current working directory (\".\"),",
            "            so make sure you ``cd`` to the right place before calling ``borg extract``.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('extract', parents=[common_parser], add_help=False,",
            "                                          description=self.do_extract.__doc__,",
            "                                          epilog=extract_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='extract archive contents')",
            "        subparser.set_defaults(func=self.do_extract)",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not actually change any files')",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',",
            "                               help='only obey numeric user and group identifiers')",
            "        subparser.add_argument('--nobsdflags', dest='nobsdflags', action='store_true',",
            "                               help='do not extract/set bsdflags (e.g. NODUMP, IMMUTABLE)')",
            "        subparser.add_argument('--stdout', dest='stdout', action='store_true',",
            "                               help='write all extracted data to stdout')",
            "        subparser.add_argument('--sparse', dest='sparse', action='store_true',",
            "                               help='create holes in output sparse file from all-zero chunks')",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to extract')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract; patterns are supported')",
            "        define_exclusion_group(subparser, strip_components=True)",
            "",
            "        export_tar_epilog = process_epilog(\"\"\"",
            "        This command creates a tarball from an archive.",
            "",
            "        When giving '-' as the output FILE, Borg will write a tar stream to standard output.",
            "",
            "        By default (``--tar-filter=auto``) Borg will detect whether the FILE should be compressed",
            "        based on its file extension and pipe the tarball through an appropriate filter",
            "        before writing it to FILE:",
            "",
            "        - .tar.gz: gzip",
            "        - .tar.bz2: bzip2",
            "        - .tar.xz: xz",
            "",
            "        Alternatively a ``--tar-filter`` program may be explicitly specified. It should",
            "        read the uncompressed tar stream from stdin and write a compressed/filtered",
            "        tar stream to stdout.",
            "",
            "        The generated tarball uses the GNU tar format.",
            "",
            "        export-tar is a lossy conversion:",
            "        BSD flags, ACLs, extended attributes (xattrs), atime and ctime are not exported.",
            "        Timestamp resolution is limited to whole seconds, not the nanosecond resolution",
            "        otherwise supported by Borg.",
            "",
            "        A ``--sparse`` option (as found in borg extract) is not supported.",
            "",
            "        By default the entire archive is extracted but a subset of files and directories",
            "        can be selected by passing a list of ``PATHs`` as arguments.",
            "        The file selection can further be restricted by using the ``--exclude`` option.",
            "",
            "        See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "",
            "        ``--progress`` can be slower than no progress display, since it makes one additional",
            "        pass over the archive metadata.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('export-tar', parents=[common_parser], add_help=False,",
            "                                          description=self.do_export_tar.__doc__,",
            "                                          epilog=export_tar_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='create tarball from archive')",
            "        subparser.set_defaults(func=self.do_export_tar)",
            "        subparser.add_argument('--tar-filter', dest='tar_filter', default='auto',",
            "                               help='filter program to pipe data through')",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to export')",
            "        subparser.add_argument('tarfile', metavar='FILE',",
            "                               help='output tar file. \"-\" to write to stdout instead.')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract; patterns are supported')",
            "        define_exclusion_group(subparser, strip_components=True)",
            "",
            "        diff_epilog = process_epilog(\"\"\"",
            "            This command finds differences (file contents, user/group/mode) between archives.",
            "",
            "            A repository location and an archive name must be specified for REPO_ARCHIVE1.",
            "            ARCHIVE2 is just another archive name in same repository (no repository location",
            "            allowed).",
            "",
            "            For archives created with Borg 1.1 or newer diff automatically detects whether",
            "            the archives are created with the same chunker params. If so, only chunk IDs",
            "            are compared, which is very fast.",
            "",
            "            For archives prior to Borg 1.1 chunk contents are compared by default.",
            "            If you did not create the archives with different chunker params,",
            "            pass ``--same-chunker-params``.",
            "            Note that the chunker params changed from Borg 0.xx to 1.0.",
            "",
            "            See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "            \"\"\")",
            "        subparser = subparsers.add_parser('diff', parents=[common_parser], add_help=False,",
            "                                          description=self.do_diff.__doc__,",
            "                                          epilog=diff_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='find differences in archive contents')",
            "        subparser.set_defaults(func=self.do_diff)",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',",
            "                               help='only consider numeric user and group identifiers')",
            "        subparser.add_argument('--same-chunker-params', dest='same_chunker_params', action='store_true',",
            "                               help='Override check of chunker parameters.')",
            "        subparser.add_argument('--sort', dest='sort', action='store_true',",
            "                               help='Sort the output lines by file path.')",
            "        subparser.add_argument('location', metavar='REPO_ARCHIVE1',",
            "                               type=location_validator(archive=True),",
            "                               help='repository location and ARCHIVE1 name')",
            "        subparser.add_argument('archive2', metavar='ARCHIVE2',",
            "                               type=archivename_validator(),",
            "                               help='ARCHIVE2 name (no repository location allowed)')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths of items inside the archives to compare; patterns are supported')",
            "        define_exclusion_group(subparser)",
            "",
            "        rename_epilog = process_epilog(\"\"\"",
            "        This command renames an archive in the repository.",
            "",
            "        This results in a different archive ID.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('rename', parents=[common_parser], add_help=False,",
            "                                          description=self.do_rename.__doc__,",
            "                                          epilog=rename_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='rename archive')",
            "        subparser.set_defaults(func=self.do_rename)",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to rename')",
            "        subparser.add_argument('name', metavar='NEWNAME',",
            "                               type=archivename_validator(),",
            "                               help='the new archive name to use')",
            "",
            "        delete_epilog = process_epilog(\"\"\"",
            "        This command deletes an archive from the repository or the complete repository.",
            "        Disk space is reclaimed accordingly. If you delete the complete repository, the",
            "        local cache for it (if any) is also deleted.",
            "",
            "        When using ``--stats``, you will get some statistics about how much data was",
            "        deleted - the \"Deleted data\" deduplicated size there is most interesting as",
            "        that is how much your repository will shrink.",
            "        Please note that the \"All archives\" stats refer to the state after deletion.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('delete', parents=[common_parser], add_help=False,",
            "                                          description=self.do_delete.__doc__,",
            "                                          epilog=delete_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='delete archive')",
            "        subparser.set_defaults(func=self.do_delete)",
            "        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('--cache-only', dest='cache_only', action='store_true',",
            "                               help='delete only the local cache for the given repository')",
            "        subparser.add_argument('--force', dest='forced',",
            "                               action='count', default=0,",
            "                               help='force deletion of corrupted archives, '",
            "                                    'use ``--force --force`` in case ``--force`` does not work.')",
            "        subparser.add_argument('--save-space', dest='save_space', action='store_true',",
            "                               help='work slower, but using less space')",
            "        subparser.add_argument('location', metavar='TARGET', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='archive or repository to delete')",
            "        subparser.add_argument('archives', metavar='ARCHIVE', nargs='*',",
            "                               help='archives to delete')",
            "        define_archive_filters_group(subparser)",
            "",
            "        list_epilog = process_epilog(\"\"\"",
            "        This command lists the contents of a repository or an archive.",
            "",
            "        See the \"borg help patterns\" command for more help on exclude patterns.",
            "",
            "        .. man NOTES",
            "",
            "        The following keys are available for ``--format``:",
            "",
            "",
            "        \"\"\") + BaseFormatter.keys_help() + textwrap.dedent(\"\"\"",
            "",
            "        Keys for listing repository archives:",
            "",
            "        \"\"\") + ArchiveFormatter.keys_help() + textwrap.dedent(\"\"\"",
            "",
            "        Keys for listing archive files:",
            "",
            "        \"\"\") + ItemFormatter.keys_help()",
            "        subparser = subparsers.add_parser('list', parents=[common_parser], add_help=False,",
            "                                          description=self.do_list.__doc__,",
            "                                          epilog=list_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='list archive or repository contents')",
            "        subparser.set_defaults(func=self.do_list)",
            "        subparser.add_argument('--short', dest='short', action='store_true',",
            "                               help='only print file/directory names, nothing else')",
            "        subparser.add_argument('--format', '--list-format', metavar='FORMAT', dest='format',",
            "                               help='specify format for file listing '",
            "                                    '(default: \"{mode} {user:6} {group:6} {size:8d} {mtime} {path}{extra}{NL}\")')",
            "        subparser.add_argument('--json', action='store_true',",
            "                               help='Only valid for listing repository contents. Format output as JSON. '",
            "                                    'The form of ``--format`` is ignored, '",
            "                                    'but keys used in it are added to the JSON output. '",
            "                                    'Some keys are always present. Note: JSON can only represent text. '",
            "                                    'A \"barchive\" key is therefore not available.')",
            "        subparser.add_argument('--json-lines', action='store_true',",
            "                               help='Only valid for listing archive contents. Format output as JSON Lines. '",
            "                                    'The form of ``--format`` is ignored, '",
            "                                    'but keys used in it are added to the JSON output. '",
            "                                    'Some keys are always present. Note: JSON can only represent text. '",
            "                                    'A \"bpath\" key is therefore not available.')",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='repository/archive to list contents of')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to list; patterns are supported')",
            "        define_archive_filters_group(subparser)",
            "        define_exclusion_group(subparser)",
            "",
            "        umount_epilog = process_epilog(\"\"\"",
            "        This command un-mounts a FUSE filesystem that was mounted with ``borg mount``.",
            "",
            "        This is a convenience wrapper that just calls the platform-specific shell",
            "        command - usually this is either umount or fusermount -u.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('umount', parents=[common_parser], add_help=False,",
            "                                          description=self.do_umount.__doc__,",
            "                                          epilog=umount_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='umount repository')",
            "        subparser.set_defaults(func=self.do_umount)",
            "        subparser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,",
            "                               help='mountpoint of the filesystem to umount')",
            "",
            "        info_epilog = process_epilog(\"\"\"",
            "        This command displays detailed information about the specified archive or repository.",
            "",
            "        Please note that the deduplicated sizes of the individual archives do not add",
            "        up to the deduplicated size of the repository (\"all archives\"), because the two",
            "        are meaning different things:",
            "",
            "        This archive / deduplicated size = amount of data stored ONLY for this archive",
            "        = unique chunks of this archive.",
            "        All archives / deduplicated size = amount of data stored in the repo",
            "        = all chunks in the repository.",
            "",
            "        Borg archives can only contain a limited amount of file metadata.",
            "        The size of an archive relative to this limit depends on a number of factors,",
            "        mainly the number of files, the lengths of paths and other metadata stored for files.",
            "        This is shown as *utilization of maximum supported archive size*.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('info', parents=[common_parser], add_help=False,",
            "                                          description=self.do_info.__doc__,",
            "                                          epilog=info_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='show repository or archive information')",
            "        subparser.set_defaults(func=self.do_info)",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='archive or repository to display information about')",
            "        subparser.add_argument('--json', action='store_true',",
            "                               help='format output as JSON')",
            "        define_archive_filters_group(subparser)",
            "",
            "        break_lock_epilog = process_epilog(\"\"\"",
            "        This command breaks the repository and cache locks.",
            "        Please use carefully and only while no borg process (on any machine) is",
            "        trying to access the Cache or the Repository.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('break-lock', parents=[common_parser], add_help=False,",
            "                                          description=self.do_break_lock.__doc__,",
            "                                          epilog=break_lock_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='break repository and cache locks')",
            "        subparser.set_defaults(func=self.do_break_lock)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository for which to break the locks')",
            "",
            "        prune_epilog = process_epilog(\"\"\"",
            "        The prune command prunes a repository by deleting all archives not matching",
            "        any of the specified retention options. This command is normally used by",
            "        automated backup scripts wanting to keep a certain number of historic backups.",
            "",
            "        Also, prune automatically removes checkpoint archives (incomplete archives left",
            "        behind by interrupted backup runs) except if the checkpoint is the latest",
            "        archive (and thus still needed). Checkpoint archives are not considered when",
            "        comparing archive counts against the retention limits (``--keep-X``).",
            "",
            "        If a prefix is set with -P, then only archives that start with the prefix are",
            "        considered for deletion and only those archives count towards the totals",
            "        specified by the rules.",
            "        Otherwise, *all* archives in the repository are candidates for deletion!",
            "        There is no automatic distinction between archives representing different",
            "        contents. These need to be distinguished by specifying matching prefixes.",
            "",
            "        If you have multiple sequences of archives with different data sets (e.g.",
            "        from different machines) in one shared repository, use one prune call per",
            "        data set that matches only the respective archives using the -P option.",
            "",
            "        The ``--keep-within`` option takes an argument of the form \"<int><char>\",",
            "        where char is \"H\", \"d\", \"w\", \"m\", \"y\". For example, ``--keep-within 2d`` means",
            "        to keep all archives that were created within the past 48 hours.",
            "        \"1m\" is taken to mean \"31d\". The archives kept with this option do not",
            "        count towards the totals specified by any other options.",
            "",
            "        A good procedure is to thin out more and more the older your backups get.",
            "        As an example, ``--keep-daily 7`` means to keep the latest backup on each day,",
            "        up to 7 most recent days with backups (days without backups do not count).",
            "        The rules are applied from secondly to yearly, and backups selected by previous",
            "        rules do not count towards those of later rules. The time that each backup",
            "        starts is used for pruning purposes. Dates and times are interpreted in",
            "        the local timezone, and weeks go from Monday to Sunday. Specifying a",
            "        negative number of archives to keep means that there is no limit.",
            "",
            "        The ``--keep-last N`` option is doing the same as ``--keep-secondly N`` (and it will",
            "        keep the last N archives under the assumption that you do not create more than one",
            "        backup archive in the same second).",
            "",
            "        When using ``--stats``, you will get some statistics about how much data was",
            "        deleted - the \"Deleted data\" deduplicated size there is most interesting as",
            "        that is how much your repository will shrink.",
            "        Please note that the \"All archives\" stats refer to the state after pruning.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('prune', parents=[common_parser], add_help=False,",
            "                                          description=self.do_prune.__doc__,",
            "                                          epilog=prune_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='prune archives')",
            "        subparser.set_defaults(func=self.do_prune)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not change repository')",
            "        subparser.add_argument('--force', dest='forced', action='store_true',",
            "                               help='force pruning of corrupted archives')",
            "        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of archives it keeps/prunes')",
            "        subparser.add_argument('--keep-within', metavar='INTERVAL', dest='within', type=interval,",
            "                               help='keep all archives within this time interval')",
            "        subparser.add_argument('--keep-last', '--keep-secondly', dest='secondly', type=int, default=0,",
            "                               help='number of secondly archives to keep')",
            "        subparser.add_argument('--keep-minutely', dest='minutely', type=int, default=0,",
            "                               help='number of minutely archives to keep')",
            "        subparser.add_argument('-H', '--keep-hourly', dest='hourly', type=int, default=0,",
            "                               help='number of hourly archives to keep')",
            "        subparser.add_argument('-d', '--keep-daily', dest='daily', type=int, default=0,",
            "                               help='number of daily archives to keep')",
            "        subparser.add_argument('-w', '--keep-weekly', dest='weekly', type=int, default=0,",
            "                               help='number of weekly archives to keep')",
            "        subparser.add_argument('-m', '--keep-monthly', dest='monthly', type=int, default=0,",
            "                               help='number of monthly archives to keep')",
            "        subparser.add_argument('-y', '--keep-yearly', dest='yearly', type=int, default=0,",
            "                               help='number of yearly archives to keep')",
            "        define_archive_filters_group(subparser, sort_by=False, first_last=False)",
            "        subparser.add_argument('--save-space', dest='save_space', action='store_true',",
            "                               help='work slower, but using less space')",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to prune')",
            "",
            "        upgrade_epilog = process_epilog(\"\"\"",
            "        Upgrade an existing, local Borg repository.",
            "",
            "        When you do not need borg upgrade",
            "        +++++++++++++++++++++++++++++++++",
            "",
            "        Not every change requires that you run ``borg upgrade``.",
            "",
            "        You do **not** need to run it when:",
            "",
            "        - moving your repository to a different place",
            "        - upgrading to another point release (like 1.0.x to 1.0.y),",
            "          except when noted otherwise in the changelog",
            "        - upgrading from 1.0.x to 1.1.x,",
            "          except when noted otherwise in the changelog",
            "",
            "        Borg 1.x.y upgrades",
            "        +++++++++++++++++++",
            "",
            "        Use ``borg upgrade --tam REPO`` to require manifest authentication",
            "        introduced with Borg 1.0.9 to address security issues. This means",
            "        that modifying the repository after doing this with a version prior",
            "        to 1.0.9 will raise a validation error, so only perform this upgrade",
            "        after updating all clients using the repository to 1.0.9 or newer.",
            "",
            "        This upgrade should be done on each client for safety reasons.",
            "",
            "        If a repository is accidentally modified with a pre-1.0.9 client after",
            "        this upgrade, use ``borg upgrade --tam --force REPO`` to remedy it.",
            "",
            "        If you routinely do this you might not want to enable this upgrade",
            "        (which will leave you exposed to the security issue). You can",
            "        reverse the upgrade by issuing ``borg upgrade --disable-tam REPO``.",
            "",
            "        See",
            "        https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability",
            "        for details.",
            "",
            "        Attic and Borg 0.xx to Borg 1.x",
            "        +++++++++++++++++++++++++++++++",
            "",
            "        This currently supports converting an Attic repository to Borg and also",
            "        helps with converting Borg 0.xx to 1.0.",
            "",
            "        Currently, only LOCAL repositories can be upgraded (issue #465).",
            "",
            "        Please note that ``borg create`` (since 1.0.0) uses bigger chunks by",
            "        default than old borg or attic did, so the new chunks won't deduplicate",
            "        with the old chunks in the upgraded repository.",
            "        See ``--chunker-params`` option of ``borg create`` and ``borg recreate``.",
            "",
            "        ``borg upgrade`` will change the magic strings in the repository's",
            "        segments to match the new Borg magic strings. The keyfiles found in",
            "        $ATTIC_KEYS_DIR or ~/.attic/keys/ will also be converted and",
            "        copied to $BORG_KEYS_DIR or ~/.config/borg/keys.",
            "",
            "        The cache files are converted, from $ATTIC_CACHE_DIR or",
            "        ~/.cache/attic to $BORG_CACHE_DIR or ~/.cache/borg, but the",
            "        cache layout between Borg and Attic changed, so it is possible",
            "        the first backup after the conversion takes longer than expected",
            "        due to the cache resync.",
            "",
            "        Upgrade should be able to resume if interrupted, although it",
            "        will still iterate over all segments. If you want to start",
            "        from scratch, use `borg delete` over the copied repository to",
            "        make sure the cache files are also removed:",
            "",
            "            borg delete borg",
            "",
            "        Unless ``--inplace`` is specified, the upgrade process first creates a backup",
            "        copy of the repository, in REPOSITORY.before-upgrade-DATETIME, using hardlinks.",
            "        This requires that the repository and its parent directory reside on same",
            "        filesystem so the hardlink copy can work.",
            "        This takes longer than in place upgrades, but is much safer and gives",
            "        progress information (as opposed to ``cp -al``). Once you are satisfied",
            "        with the conversion, you can safely destroy the backup copy.",
            "",
            "        WARNING: Running the upgrade in place will make the current",
            "        copy unusable with older version, with no way of going back",
            "        to previous versions. This can PERMANENTLY DAMAGE YOUR",
            "        REPOSITORY!  Attic CAN NOT READ BORG REPOSITORIES, as the",
            "        magic strings have changed. You have been warned.\"\"\")",
            "        subparser = subparsers.add_parser('upgrade', parents=[common_parser], add_help=False,",
            "                                          description=self.do_upgrade.__doc__,",
            "                                          epilog=upgrade_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='upgrade repository format')",
            "        subparser.set_defaults(func=self.do_upgrade)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not change repository')",
            "        subparser.add_argument('--inplace', dest='inplace', action='store_true',",
            "                               help='rewrite repository in place, with no chance of going back '",
            "                                    'to older versions of the repository.')",
            "        subparser.add_argument('--force', dest='force', action='store_true',",
            "                               help='Force upgrade')",
            "        subparser.add_argument('--tam', dest='tam', action='store_true',",
            "                               help='Enable manifest authentication (in key and cache) (Borg 1.0.9 and later).')",
            "        subparser.add_argument('--disable-tam', dest='disable_tam', action='store_true',",
            "                               help='Disable manifest authentication (in key and cache).')",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='path to the repository to be upgraded')",
            "",
            "        recreate_epilog = process_epilog(\"\"\"",
            "        Recreate the contents of existing archives.",
            "",
            "        This is an *experimental* feature. Do *not* use this on your only backup.",
            "",
            "        ``--exclude``, ``--exclude-from``, ``--exclude-if-present``, ``--keep-exclude-tags``, and PATH",
            "        have the exact same semantics as in \"borg create\". If PATHs are specified the",
            "        resulting archive will only contain files from these PATHs.",
            "",
            "        Note that all paths in an archive are relative, therefore absolute patterns/paths",
            "        will *not* match (``--exclude``, ``--exclude-from``, PATHs).",
            "",
            "        ``--recompress`` allows to change the compression of existing data in archives.",
            "        Due to how Borg stores compressed size information this might display",
            "        incorrect information for archives that were not recreated at the same time.",
            "        There is no risk of data loss by this.",
            "",
            "        ``--chunker-params`` will re-chunk all files in the archive, this can be",
            "        used to have upgraded Borg 0.xx or Attic archives deduplicate with",
            "        Borg 1.x archives.",
            "",
            "        **USE WITH CAUTION.**",
            "        Depending on the PATHs and patterns given, recreate can be used to permanently",
            "        delete files from archives.",
            "        When in doubt, use ``--dry-run --verbose --list`` to see how patterns/PATHS are",
            "        interpreted.",
            "",
            "        The archive being recreated is only removed after the operation completes. The",
            "        archive that is built during the operation exists at the same time at",
            "        \"<ARCHIVE>.recreate\". The new archive will have a different archive ID.",
            "",
            "        With ``--target`` the original archive is not replaced, instead a new archive is created.",
            "",
            "        When rechunking space usage can be substantial, expect at least the entire",
            "        deduplicated size of the archives using the previous chunker params.",
            "        When recompressing expect approx. (throughput / checkpoint-interval) in space usage,",
            "        assuming all chunks are recompressed.",
            "",
            "        If you recently ran borg check --repair and it had to fix lost chunks with all-zero",
            "        replacement chunks, please first run another backup for the same data and re-run",
            "        borg check --repair afterwards to heal any archives that had lost chunks which are",
            "        still generated from the input data.",
            "",
            "        Important: running borg recreate to re-chunk will remove the chunks_healthy",
            "        metadata of all items with replacement chunks, so healing will not be possible",
            "        any more after re-chunking (it is also unlikely it would ever work: due to the",
            "        change of chunking parameters, the missing chunk likely will never be seen again",
            "        even if you still have the data that produced it).",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('recreate', parents=[common_parser], add_help=False,",
            "                                          description=self.do_recreate.__doc__,",
            "                                          epilog=recreate_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help=self.do_recreate.__doc__)",
            "        subparser.set_defaults(func=self.do_recreate)",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('--filter', metavar='STATUSCHARS', dest='output_filter',",
            "                               help='only display items with the given status characters (listed in borg create --help)')",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not change anything')",
            "        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics at end')",
            "",
            "        define_exclusion_group(subparser, tag_files=True)",
            "",
            "        archive_group = subparser.add_argument_group('Archive options')",
            "        archive_group.add_argument('--target', dest='target', metavar='TARGET', default=None,",
            "                                   type=archivename_validator(),",
            "                                   help='create a new archive with the name ARCHIVE, do not replace existing archive '",
            "                                        '(only applies for a single archive)')",
            "        archive_group.add_argument('-c', '--checkpoint-interval', dest='checkpoint_interval',",
            "                                   type=int, default=1800, metavar='SECONDS',",
            "                                   help='write checkpoint every SECONDS seconds (Default: 1800)')",
            "        archive_group.add_argument('--comment', dest='comment', metavar='COMMENT', default=None,",
            "                                   help='add a comment text to the archive')",
            "        archive_group.add_argument('--timestamp', metavar='TIMESTAMP', dest='timestamp',",
            "                                   type=timestamp, default=None,",
            "                                   help='manually specify the archive creation date/time (UTC, yyyy-mm-ddThh:mm:ss format). '",
            "                                        'alternatively, give a reference file/directory.')",
            "        archive_group.add_argument('-C', '--compression', metavar='COMPRESSION', dest='compression',",
            "                                   type=CompressionSpec, default=CompressionSpec('lz4'),",
            "                                   help='select compression algorithm, see the output of the '",
            "                                        '\"borg help compression\" command for details.')",
            "        archive_group.add_argument('--recompress', dest='recompress', nargs='?', default='never', const='if-different',",
            "                                   choices=('never', 'if-different', 'always'),",
            "                                   help='recompress data chunks according to ``--compression`` if `if-different`. '",
            "                                        'When `always`, chunks that are already compressed that way are not skipped, '",
            "                                        'but compressed again. Only the algorithm is considered for `if-different`, '",
            "                                        'not the compression level (if any).')",
            "        archive_group.add_argument('--chunker-params', metavar='PARAMS', dest='chunker_params',",
            "                                   type=ChunkerParams, default=CHUNKER_PARAMS,",
            "                                   help='specify the chunker parameters (CHUNK_MIN_EXP, CHUNK_MAX_EXP, '",
            "                                        'HASH_MASK_BITS, HASH_WINDOW_SIZE) or `default` to use the current defaults. '",
            "                                        'default: %d,%d,%d,%d' % CHUNKER_PARAMS)",
            "",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='repository/archive to recreate')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to recreate; patterns are supported')",
            "",
            "        with_lock_epilog = process_epilog(\"\"\"",
            "        This command runs a user-specified command while the repository lock is held.",
            "",
            "        It will first try to acquire the lock (make sure that no other operation is",
            "        running in the repo), then execute the given command as a subprocess and wait",
            "        for its termination, release the lock and return the user command's return",
            "        code as borg's return code.",
            "",
            "        .. note::",
            "",
            "            If you copy a repository with the lock held, the lock will be present in",
            "            the copy. Thus, before using borg on the copy from a different host,",
            "            you need to use \"borg break-lock\" on the copied repository, because",
            "            Borg is cautious and does not automatically remove stale locks made by a different host.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('with-lock', parents=[common_parser], add_help=False,",
            "                                          description=self.do_with_lock.__doc__,",
            "                                          epilog=with_lock_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='run user command with lock held')",
            "        subparser.set_defaults(func=self.do_with_lock)",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to lock')",
            "        subparser.add_argument('command', metavar='COMMAND',",
            "                               help='command to run')",
            "        subparser.add_argument('args', metavar='ARGS', nargs=argparse.REMAINDER,",
            "                               help='command arguments')",
            "",
            "        config_epilog = process_epilog(\"\"\"",
            "        This command gets and sets options in a local repository or cache config file.",
            "        For security reasons, this command only works on local repositories.",
            "",
            "        To delete a config value entirely, use ``--delete``. To get an existing key, pass",
            "        only the key name. To set a key, pass both the key name and the new value. Keys",
            "        can be specified in the format \"section.name\" or simply \"name\"; the section will",
            "        default to \"repository\" and \"cache\" for the repo and cache configs, respectively.",
            "",
            "        By default, borg config manipulates the repository config file. Using ``--cache``",
            "        edits the repository cache's config file instead.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('config', parents=[common_parser], add_help=False,",
            "                                          description=self.do_config.__doc__,",
            "                                          epilog=config_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='get and set configuration values')",
            "        subparser.set_defaults(func=self.do_config)",
            "        subparser.add_argument('-c', '--cache', dest='cache', action='store_true',",
            "                               help='get and set values from the repo cache')",
            "        subparser.add_argument('-d', '--delete', dest='delete', action='store_true',",
            "                               help='delete the key from the config file')",
            "",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False, proto='file'),",
            "                               help='repository to configure')",
            "        subparser.add_argument('name', metavar='NAME',",
            "                               help='name of config key')",
            "        subparser.add_argument('value', metavar='VALUE', nargs='?',",
            "                               help='new value for key')",
            "",
            "        subparser = subparsers.add_parser('help', parents=[common_parser], add_help=False,",
            "                                          description='Extra help')",
            "        subparser.add_argument('--epilog-only', dest='epilog_only', action='store_true')",
            "        subparser.add_argument('--usage-only', dest='usage_only', action='store_true')",
            "        subparser.set_defaults(func=functools.partial(self.do_help, parser, subparsers.choices))",
            "        subparser.add_argument('topic', metavar='TOPIC', type=str, nargs='?',",
            "                               help='additional help on TOPIC')",
            "",
            "        debug_epilog = process_epilog(\"\"\"",
            "        These commands are not intended for normal use and potentially very",
            "        dangerous if used incorrectly.",
            "",
            "        They exist to improve debugging capabilities without direct system access, e.g.",
            "        in case you ever run into some severe malfunction. Use them only if you know",
            "        what you are doing or if a trusted developer tells you what to do.\"\"\")",
            "",
            "        subparser = subparsers.add_parser('debug', parents=[mid_common_parser], add_help=False,",
            "                                          description='debugging command (not intended for normal use)',",
            "                                          epilog=debug_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='debugging command (not intended for normal use)')",
            "",
            "        debug_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')",
            "        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))",
            "",
            "        debug_info_epilog = process_epilog(\"\"\"",
            "        This command displays some system information that might be useful for bug",
            "        reports and debugging problems. If a traceback happens, this information is",
            "        already appended at the end of the traceback.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('info', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_info.__doc__,",
            "                                          epilog=debug_info_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='show system infos for debugging / bug reports (debug)')",
            "        subparser.set_defaults(func=self.do_debug_info)",
            "",
            "        debug_dump_archive_items_epilog = process_epilog(\"\"\"",
            "        This command dumps raw (but decrypted and decompressed) archive items (only metadata) to files.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-archive-items', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_archive_items.__doc__,",
            "                                          epilog=debug_dump_archive_items_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump archive items (metadata) (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_archive_items)",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to dump')",
            "",
            "        debug_dump_archive_epilog = process_epilog(\"\"\"",
            "        This command dumps all metadata of an archive in a decoded form to a file.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-archive', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_archive.__doc__,",
            "                                          epilog=debug_dump_archive_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump decoded archive metadata (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_archive)",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to dump')",
            "        subparser.add_argument('path', metavar='PATH', type=str,",
            "                               help='file to dump data into')",
            "",
            "        debug_dump_manifest_epilog = process_epilog(\"\"\"",
            "        This command dumps manifest metadata of a repository in a decoded form to a file.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-manifest', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_manifest.__doc__,",
            "                                          epilog=debug_dump_manifest_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump decoded repository metadata (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_manifest)",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to dump')",
            "        subparser.add_argument('path', metavar='PATH', type=str,",
            "                               help='file to dump data into')",
            "",
            "        debug_dump_repo_objs_epilog = process_epilog(\"\"\"",
            "        This command dumps raw (but decrypted and decompressed) repo objects to files.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-repo-objs', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_repo_objs.__doc__,",
            "                                          epilog=debug_dump_repo_objs_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump repo objects (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_repo_objs)",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repo to dump')",
            "",
            "        debug_get_obj_epilog = process_epilog(\"\"\"",
            "        This command gets an object from the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('get-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_get_obj.__doc__,",
            "                                          epilog=debug_get_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='get object from repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_get_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('id', metavar='ID', type=str,",
            "                               help='hex object ID to get from the repo')",
            "        subparser.add_argument('path', metavar='PATH', type=str,",
            "                               help='file to write object data into')",
            "",
            "        debug_put_obj_epilog = process_epilog(\"\"\"",
            "        This command puts objects into the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('put-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_put_obj.__doc__,",
            "                                          epilog=debug_put_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='put object to repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_put_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='+', type=str,",
            "                               help='file(s) to read and create object(s) from')",
            "",
            "        debug_delete_obj_epilog = process_epilog(\"\"\"",
            "        This command deletes objects from the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('delete-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_delete_obj.__doc__,",
            "                                          epilog=debug_delete_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='delete object from repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_delete_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('ids', metavar='IDs', nargs='+', type=str,",
            "                               help='hex object ID(s) to delete from the repo')",
            "",
            "        debug_refcount_obj_epilog = process_epilog(\"\"\"",
            "        This command displays the reference count for objects from the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('refcount-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_refcount_obj.__doc__,",
            "                                          epilog=debug_refcount_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='show refcount for object from repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_refcount_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('ids', metavar='IDs', nargs='+', type=str,",
            "                               help='hex object ID(s) to show refcounts for')",
            "",
            "        debug_convert_profile_epilog = process_epilog(\"\"\"",
            "        Convert a Borg profile to a Python cProfile compatible profile.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('convert-profile', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_convert_profile.__doc__,",
            "                                          epilog=debug_convert_profile_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='convert Borg profile to Python profile (debug)')",
            "        subparser.set_defaults(func=self.do_debug_convert_profile)",
            "        subparser.add_argument('input', metavar='INPUT', type=argparse.FileType('rb'),",
            "                               help='Borg profile')",
            "        subparser.add_argument('output', metavar='OUTPUT', type=argparse.FileType('wb'),",
            "                               help='Output file')",
            "",
            "        benchmark_epilog = process_epilog(\"These commands do various benchmarks.\")",
            "",
            "        subparser = subparsers.add_parser('benchmark', parents=[mid_common_parser], add_help=False,",
            "                                          description='benchmark command',",
            "                                          epilog=benchmark_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='benchmark command')",
            "",
            "        benchmark_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')",
            "        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))",
            "",
            "        bench_crud_epilog = process_epilog(\"\"\"",
            "        This command benchmarks borg CRUD (create, read, update, delete) operations.",
            "",
            "        It creates input data below the given PATH and backups this data into the given REPO.",
            "        The REPO must already exist (it could be a fresh empty repo or an existing repo, the",
            "        command will create / read / update / delete some archives named borg-test-data\\* there.",
            "",
            "        Make sure you have free space there, you'll need about 1GB each (+ overhead).",
            "",
            "        If your repository is encrypted and borg needs a passphrase to unlock the key, use:",
            "",
            "        BORG_PASSPHRASE=mysecret borg benchmark crud REPO PATH",
            "",
            "        Measurements are done with different input file sizes and counts.",
            "        The file contents are very artificial (either all zero or all random),",
            "        thus the measurement results do not necessarily reflect performance with real data.",
            "        Also, due to the kind of content used, no compression is used in these benchmarks.",
            "",
            "        C- == borg create (1st archive creation, no compression, do not use files cache)",
            "              C-Z- == all-zero files. full dedup, this is primarily measuring reader/chunker/hasher.",
            "              C-R- == random files. no dedup, measuring throughput through all processing stages.",
            "",
            "        R- == borg extract (extract archive, dry-run, do everything, but do not write files to disk)",
            "              R-Z- == all zero files. Measuring heavily duplicated files.",
            "              R-R- == random files. No duplication here, measuring throughput through all processing",
            "              stages, except writing to disk.",
            "",
            "        U- == borg create (2nd archive creation of unchanged input files, measure files cache speed)",
            "              The throughput value is kind of virtual here, it does not actually read the file.",
            "              U-Z- == needs to check the 2 all-zero chunks' existence in the repo.",
            "              U-R- == needs to check existence of a lot of different chunks in the repo.",
            "",
            "        D- == borg delete archive (delete last remaining archive, measure deletion + compaction)",
            "              D-Z- == few chunks to delete / few segments to compact/remove.",
            "              D-R- == many chunks to delete / many segments to compact/remove.",
            "",
            "        Please note that there might be quite some variance in these measurements.",
            "        Try multiple measurements and having a otherwise idle machine (and network, if you use it).",
            "        \"\"\")",
            "        subparser = benchmark_parsers.add_parser('crud', parents=[common_parser], add_help=False,",
            "                                                 description=self.do_benchmark_crud.__doc__,",
            "                                                 epilog=bench_crud_epilog,",
            "                                                 formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                                 help='benchmarks borg CRUD (create, extract, update, delete).')",
            "        subparser.set_defaults(func=self.do_benchmark_crud)",
            "",
            "        subparser.add_argument('location', metavar='REPO',",
            "                               type=location_validator(archive=False),",
            "                               help='repo to use for benchmark (must exist)')",
            "",
            "        subparser.add_argument('path', metavar='PATH', help='path were to create benchmark input data')",
            "",
            "        return parser",
            "",
            "    def get_args(self, argv, cmd):",
            "        \"\"\"usually, just returns argv, except if we deal with a ssh forced command for borg serve.\"\"\"",
            "        result = self.parse_args(argv[1:])",
            "        if cmd is not None and result.func == self.do_serve:",
            "            forced_result = result",
            "            argv = shlex.split(cmd)",
            "            # Drop environment variables (do *not* interpret them) before trying to parse",
            "            # the borg command line.",
            "            argv = list(itertools.dropwhile(lambda arg: '=' in arg, argv))",
            "            result = self.parse_args(argv[1:])",
            "            if result.func != forced_result.func:",
            "                # someone is trying to execute a different borg subcommand, don't do that!",
            "                return forced_result",
            "            # we only take specific options from the forced \"borg serve\" command:",
            "            result.restrict_to_paths = forced_result.restrict_to_paths",
            "            result.append_only = forced_result.append_only",
            "        return result",
            "",
            "    def parse_args(self, args=None):",
            "        # We can't use argparse for \"serve\" since we don't want it to show up in \"Available commands\"",
            "        if args:",
            "            args = self.preprocess_args(args)",
            "        parser = self.build_parser()",
            "        args = parser.parse_args(args or ['-h'])",
            "        parser.common_options.resolve(args)",
            "        func = get_func(args)",
            "        if func == self.do_create and not args.paths:",
            "            # need at least 1 path but args.paths may also be populated from patterns",
            "            parser.error('Need at least one PATH argument.')",
            "        return args",
            "",
            "    def prerun_checks(self, logger):",
            "        check_python()",
            "        check_extension_modules()",
            "        selftest(logger)",
            "",
            "    def _setup_implied_logging(self, args):",
            "        \"\"\" turn on INFO level logging for args that imply that they will produce output \"\"\"",
            "        # map of option name to name of logger for that option",
            "        option_logger = {",
            "            'output_list': 'borg.output.list',",
            "            'show_version': 'borg.output.show-version',",
            "            'show_rc': 'borg.output.show-rc',",
            "            'stats': 'borg.output.stats',",
            "            'progress': 'borg.output.progress',",
            "        }",
            "        for option, logger_name in option_logger.items():",
            "            option_set = args.get(option, False)",
            "            logging.getLogger(logger_name).setLevel('INFO' if option_set else 'WARN')",
            "",
            "    def _setup_topic_debugging(self, args):",
            "        \"\"\"Turn on DEBUG level logging for specified --debug-topics.\"\"\"",
            "        for topic in args.debug_topics:",
            "            if '.' not in topic:",
            "                topic = 'borg.debug.' + topic",
            "            logger.debug('Enabling debug topic %s', topic)",
            "            logging.getLogger(topic).setLevel('DEBUG')",
            "",
            "    def run(self, args):",
            "        os.umask(args.umask)  # early, before opening files",
            "        self.lock_wait = args.lock_wait",
            "        func = get_func(args)",
            "        # do not use loggers before this!",
            "        is_serve = func == self.do_serve",
            "        setup_logging(level=args.log_level, is_serve=is_serve, json=args.log_json)",
            "        self.log_json = args.log_json",
            "        args.progress |= is_serve",
            "        self._setup_implied_logging(vars(args))",
            "        self._setup_topic_debugging(args)",
            "        if args.show_version:",
            "            logging.getLogger('borg.output.show-version').info('borgbackup version %s' % __version__)",
            "        self.prerun_checks(logger)",
            "        if is_slow_msgpack():",
            "            logger.warning(\"Using a pure-python msgpack! This will result in lower performance.\")",
            "        if args.debug_profile:",
            "            # Import only when needed - avoids a further increase in startup time",
            "            import cProfile",
            "            import marshal",
            "            logger.debug('Writing execution profile to %s', args.debug_profile)",
            "            # Open the file early, before running the main program, to avoid",
            "            # a very late crash in case the specified path is invalid.",
            "            with open(args.debug_profile, 'wb') as fd:",
            "                profiler = cProfile.Profile()",
            "                variables = dict(locals())",
            "                profiler.enable()",
            "                try:",
            "                    return set_ec(func(args))",
            "                finally:",
            "                    profiler.disable()",
            "                    profiler.snapshot_stats()",
            "                    if args.debug_profile.endswith('.pyprof'):",
            "                        marshal.dump(profiler.stats, fd)",
            "                    else:",
            "                        # We use msgpack here instead of the marshal module used by cProfile itself,",
            "                        # because the latter is insecure. Since these files may be shared over the",
            "                        # internet we don't want a format that is impossible to interpret outside",
            "                        # an insecure implementation.",
            "                        # See scripts/msgpack2marshal.py for a small script that turns a msgpack file",
            "                        # into a marshal file that can be read by e.g. pyprof2calltree.",
            "                        # For local use it's unnecessary hassle, though, that's why .pyprof makes",
            "                        # it compatible (see above).",
            "                        msgpack.pack(profiler.stats, fd, use_bin_type=True)",
            "        else:",
            "            return set_ec(func(args))",
            "",
            "",
            "def sig_info_handler(sig_no, stack):  # pragma: no cover",
            "    \"\"\"search the stack for infos about the currently processed file and print them\"\"\"",
            "    with signal_handler(sig_no, signal.SIG_IGN):",
            "        for frame in inspect.getouterframes(stack):",
            "            func, loc = frame[3], frame[0].f_locals",
            "            if func in ('process_file', '_process', ):  # create op",
            "                path = loc['path']",
            "                try:",
            "                    pos = loc['fd'].tell()",
            "                    total = loc['st'].st_size",
            "                except Exception:",
            "                    pos, total = 0, 0",
            "                logger.info(\"{0} {1}/{2}\".format(path, format_file_size(pos), format_file_size(total)))",
            "                break",
            "            if func in ('extract_item', ):  # extract op",
            "                path = loc['item'].path",
            "                try:",
            "                    pos = loc['fd'].tell()",
            "                except Exception:",
            "                    pos = 0",
            "                logger.info(\"{0} {1}/???\".format(path, format_file_size(pos)))",
            "                break",
            "",
            "",
            "def sig_trace_handler(sig_no, stack):  # pragma: no cover",
            "    print('\\nReceived SIGUSR2 at %s, dumping trace...' % datetime.now().replace(microsecond=0), file=sys.stderr)",
            "    faulthandler.dump_traceback()",
            "",
            "",
            "def main():  # pragma: no cover",
            "    # Make sure stdout and stderr have errors='replace' to avoid unicode",
            "    # issues when print()-ing unicode file names",
            "    sys.stdout = ErrorIgnoringTextIOWrapper(sys.stdout.buffer, sys.stdout.encoding, 'replace', line_buffering=True)",
            "    sys.stderr = ErrorIgnoringTextIOWrapper(sys.stderr.buffer, sys.stderr.encoding, 'replace', line_buffering=True)",
            "",
            "    # If we receive SIGINT (ctrl-c), SIGTERM (kill) or SIGHUP (kill -HUP),",
            "    # catch them and raise a proper exception that can be handled for an",
            "    # orderly exit.",
            "    # SIGHUP is important especially for systemd systems, where logind",
            "    # sends it when a session exits, in addition to any traditional use.",
            "    # Output some info if we receive SIGUSR1 or SIGINFO (ctrl-t).",
            "",
            "    # Register fault handler for SIGSEGV, SIGFPE, SIGABRT, SIGBUS and SIGILL.",
            "    faulthandler.enable()",
            "    with signal_handler('SIGINT', raising_signal_handler(KeyboardInterrupt)), \\",
            "         signal_handler('SIGHUP', raising_signal_handler(SigHup)), \\",
            "         signal_handler('SIGTERM', raising_signal_handler(SigTerm)), \\",
            "         signal_handler('SIGUSR1', sig_info_handler), \\",
            "         signal_handler('SIGUSR2', sig_trace_handler), \\",
            "         signal_handler('SIGINFO', sig_info_handler):",
            "        archiver = Archiver()",
            "        msg = msgid = tb = None",
            "        tb_log_level = logging.ERROR",
            "        try:",
            "            args = archiver.get_args(sys.argv, os.environ.get('SSH_ORIGINAL_COMMAND'))",
            "        except Error as e:",
            "            msg = e.get_message()",
            "            tb_log_level = logging.ERROR if e.traceback else logging.DEBUG",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            # we might not have logging setup yet, so get out quickly",
            "            print(msg, file=sys.stderr)",
            "            if tb_log_level == logging.ERROR:",
            "                print(tb, file=sys.stderr)",
            "            sys.exit(e.exit_code)",
            "        try:",
            "            exit_code = archiver.run(args)",
            "        except Error as e:",
            "            msg = e.get_message()",
            "            msgid = type(e).__qualname__",
            "            tb_log_level = logging.ERROR if e.traceback else logging.DEBUG",
            "            tb = \"%s\\n%s\" % (traceback.format_exc(), sysinfo())",
            "            exit_code = e.exit_code",
            "        except RemoteRepository.RPCError as e:",
            "            important = e.exception_class not in ('LockTimeout', ) and e.traceback",
            "            msgid = e.exception_class",
            "            tb_log_level = logging.ERROR if important else logging.DEBUG",
            "            if important:",
            "                msg = e.exception_full",
            "            else:",
            "                msg = e.get_message()",
            "            tb = '\\n'.join('Borg server: ' + l for l in e.sysinfo.splitlines())",
            "            tb += \"\\n\" + sysinfo()",
            "            exit_code = EXIT_ERROR",
            "        except Exception:",
            "            msg = 'Local Exception'",
            "            msgid = 'Exception'",
            "            tb_log_level = logging.ERROR",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            exit_code = EXIT_ERROR",
            "        except KeyboardInterrupt:",
            "            msg = 'Keyboard interrupt'",
            "            tb_log_level = logging.DEBUG",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            exit_code = EXIT_ERROR",
            "        except SigTerm:",
            "            msg = 'Received SIGTERM'",
            "            msgid = 'Signal.SIGTERM'",
            "            tb_log_level = logging.DEBUG",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            exit_code = EXIT_ERROR",
            "        except SigHup:",
            "            msg = 'Received SIGHUP.'",
            "            msgid = 'Signal.SIGHUP'",
            "            exit_code = EXIT_ERROR",
            "        if msg:",
            "            logger.error(msg, msgid=msgid)",
            "        if tb:",
            "            logger.log(tb_log_level, tb)",
            "        if args.show_rc:",
            "            rc_logger = logging.getLogger('borg.output.show-rc')",
            "            exit_msg = 'terminating with %s status, rc %d'",
            "            if exit_code == EXIT_SUCCESS:",
            "                rc_logger.info(exit_msg % ('success', exit_code))",
            "            elif exit_code == EXIT_WARNING:",
            "                rc_logger.warning(exit_msg % ('warning', exit_code))",
            "            elif exit_code == EXIT_ERROR:",
            "                rc_logger.error(exit_msg % ('error', exit_code))",
            "            else:",
            "                rc_logger.error(exit_msg % ('abnormal', exit_code or 666))",
            "        sys.exit(exit_code)",
            "",
            "",
            "if __name__ == '__main__':",
            "    main()"
        ],
        "afterPatchFile": [
            "import argparse",
            "import collections",
            "import configparser",
            "import faulthandler",
            "import functools",
            "import hashlib",
            "import inspect",
            "import itertools",
            "import json",
            "import logging",
            "import os",
            "import re",
            "import shlex",
            "import shutil",
            "import signal",
            "import stat",
            "import subprocess",
            "import sys",
            "import tarfile",
            "import textwrap",
            "import time",
            "import traceback",
            "from binascii import unhexlify",
            "from contextlib import contextmanager",
            "from datetime import datetime, timedelta",
            "from itertools import zip_longest",
            "",
            "from .logger import create_logger, setup_logging",
            "",
            "logger = create_logger()",
            "",
            "import msgpack",
            "",
            "import borg",
            "from . import __version__",
            "from . import helpers",
            "from .algorithms.checksums import crc32",
            "from .archive import Archive, ArchiveChecker, ArchiveRecreater, Statistics, is_special",
            "from .archive import BackupOSError, backup_io",
            "from .archive import FilesystemObjectProcessors, MetadataCollector, ChunksProcessor",
            "from .cache import Cache, assert_secure",
            "from .constants import *  # NOQA",
            "from .compress import CompressionSpec",
            "from .crypto.key import key_creator, key_argument_names, tam_required_file, tam_required, RepoKey, PassphraseKey",
            "from .crypto.keymanager import KeyManager",
            "from .helpers import EXIT_SUCCESS, EXIT_WARNING, EXIT_ERROR",
            "from .helpers import Error, NoManifestError, set_ec",
            "from .helpers import positive_int_validator, location_validator, archivename_validator, ChunkerParams",
            "from .helpers import PrefixSpec, SortBySpec, FilesCacheMode",
            "from .helpers import BaseFormatter, ItemFormatter, ArchiveFormatter",
            "from .helpers import format_timedelta, format_file_size, parse_file_size, format_archive",
            "from .helpers import safe_encode, remove_surrogates, bin_to_hex, prepare_dump_dict",
            "from .helpers import interval, prune_within, prune_split, PRUNING_PATTERNS",
            "from .helpers import timestamp",
            "from .helpers import get_cache_dir",
            "from .helpers import Manifest, AI_HUMAN_SORT_KEYS",
            "from .helpers import hardlinkable",
            "from .helpers import StableDict",
            "from .helpers import check_python, check_extension_modules",
            "from .helpers import dir_is_tagged, is_slow_msgpack, yes, sysinfo",
            "from .helpers import log_multi",
            "from .helpers import signal_handler, raising_signal_handler, SigHup, SigTerm",
            "from .helpers import ErrorIgnoringTextIOWrapper",
            "from .helpers import ProgressIndicatorPercent",
            "from .helpers import basic_json_data, json_print",
            "from .helpers import replace_placeholders",
            "from .helpers import ChunkIteratorFileWrapper",
            "from .helpers import popen_with_error_handling, prepare_subprocess_env",
            "from .helpers import dash_open",
            "from .helpers import umount",
            "from .nanorst import rst_to_terminal",
            "from .patterns import ArgparsePatternAction, ArgparseExcludeFileAction, ArgparsePatternFileAction, parse_exclude_pattern",
            "from .patterns import PatternMatcher",
            "from .item import Item",
            "from .platform import get_flags, get_process_id, SyncFile",
            "from .remote import RepositoryServer, RemoteRepository, cache_if_remote",
            "from .repository import Repository, LIST_SCAN_LIMIT",
            "from .selftest import selftest",
            "from .upgrader import AtticRepositoryUpgrader, BorgRepositoryUpgrader",
            "",
            "",
            "STATS_HEADER = \"                       Original size      Compressed size    Deduplicated size\"",
            "",
            "",
            "def argument(args, str_or_bool):",
            "    \"\"\"If bool is passed, return it. If str is passed, retrieve named attribute from args.\"\"\"",
            "    if isinstance(str_or_bool, str):",
            "        return getattr(args, str_or_bool)",
            "    if isinstance(str_or_bool, (list, tuple)):",
            "        return any(getattr(args, item) for item in str_or_bool)",
            "    return str_or_bool",
            "",
            "",
            "def with_repository(fake=False, invert_fake=False, create=False, lock=True,",
            "                    exclusive=False, manifest=True, cache=False, secure=True,",
            "                    compatibility=None):",
            "    \"\"\"",
            "    Method decorator for subcommand-handling methods: do_XYZ(self, args, repository, \u2026)",
            "",
            "    If a parameter (where allowed) is a str the attribute named of args is used instead.",
            "    :param fake: (str or bool) use None instead of repository, don't do anything else",
            "    :param create: create repository",
            "    :param lock: lock repository",
            "    :param exclusive: (str or bool) lock repository exclusively (for writing)",
            "    :param manifest: load manifest and key, pass them as keyword arguments",
            "    :param cache: open cache, pass it as keyword argument (implies manifest)",
            "    :param secure: do assert_secure after loading manifest",
            "    :param compatibility: mandatory if not create and (manifest or cache), specifies mandatory feature categories to check",
            "    \"\"\"",
            "",
            "    if not create and (manifest or cache):",
            "        if compatibility is None:",
            "            raise AssertionError(\"with_repository decorator used without compatibility argument\")",
            "        if type(compatibility) is not tuple:",
            "            raise AssertionError(\"with_repository decorator compatibility argument must be of type tuple\")",
            "    else:",
            "        if compatibility is not None:",
            "            raise AssertionError(\"with_repository called with compatibility argument but would not check\" + repr(compatibility))",
            "        if create:",
            "            compatibility = Manifest.NO_OPERATION_CHECK",
            "",
            "    def decorator(method):",
            "        @functools.wraps(method)",
            "        def wrapper(self, args, **kwargs):",
            "            location = args.location  # note: 'location' must be always present in args",
            "            append_only = getattr(args, 'append_only', False)",
            "            if argument(args, fake) ^ invert_fake:",
            "                return method(self, args, repository=None, **kwargs)",
            "            elif location.proto == 'ssh':",
            "                repository = RemoteRepository(location, create=create, exclusive=argument(args, exclusive),",
            "                                              lock_wait=self.lock_wait, lock=lock, append_only=append_only, args=args)",
            "            else:",
            "                repository = Repository(location.path, create=create, exclusive=argument(args, exclusive),",
            "                                        lock_wait=self.lock_wait, lock=lock,",
            "                                        append_only=append_only)",
            "            with repository:",
            "                if manifest or cache:",
            "                    kwargs['manifest'], kwargs['key'] = Manifest.load(repository, compatibility)",
            "                    if 'compression' in args:",
            "                        kwargs['key'].compressor = args.compression.compressor",
            "                    if secure:",
            "                        assert_secure(repository, kwargs['manifest'])",
            "                if cache:",
            "                    with Cache(repository, kwargs['key'], kwargs['manifest'],",
            "                               do_files=getattr(args, 'cache_files', False),",
            "                               progress=getattr(args, 'progress', False), lock_wait=self.lock_wait) as cache_:",
            "                        return method(self, args, repository=repository, cache=cache_, **kwargs)",
            "                else:",
            "                    return method(self, args, repository=repository, **kwargs)",
            "        return wrapper",
            "    return decorator",
            "",
            "",
            "def with_archive(method):",
            "    @functools.wraps(method)",
            "    def wrapper(self, args, repository, key, manifest, **kwargs):",
            "        archive = Archive(repository, key, manifest, args.location.archive,",
            "                          numeric_owner=getattr(args, 'numeric_owner', False),",
            "                          nobsdflags=getattr(args, 'nobsdflags', False),",
            "                          cache=kwargs.get('cache'),",
            "                          consider_part_files=args.consider_part_files, log_json=args.log_json)",
            "        return method(self, args, repository=repository, manifest=manifest, key=key, archive=archive, **kwargs)",
            "    return wrapper",
            "",
            "",
            "def parse_storage_quota(storage_quota):",
            "    parsed = parse_file_size(storage_quota)",
            "    if parsed < parse_file_size('10M'):",
            "        raise argparse.ArgumentTypeError('quota is too small (%s). At least 10M are required.' % storage_quota)",
            "    return parsed",
            "",
            "",
            "def get_func(args):",
            "    # This works around http://bugs.python.org/issue9351",
            "    # func is used at the leaf parsers of the argparse parser tree,",
            "    # fallback_func at next level towards the root,",
            "    # fallback2_func at the 2nd next level (which is root in our case).",
            "    for name in 'func', 'fallback_func', 'fallback2_func':",
            "        func = getattr(args, name, None)",
            "        if func is not None:",
            "            return func",
            "    raise Exception('expected func attributes not found')",
            "",
            "",
            "class Archiver:",
            "",
            "    def __init__(self, lock_wait=None, prog=None):",
            "        self.exit_code = EXIT_SUCCESS",
            "        self.lock_wait = lock_wait",
            "        self.prog = prog",
            "",
            "    def print_error(self, msg, *args):",
            "        msg = args and msg % args or msg",
            "        self.exit_code = EXIT_ERROR",
            "        logger.error(msg)",
            "",
            "    def print_warning(self, msg, *args):",
            "        msg = args and msg % args or msg",
            "        self.exit_code = EXIT_WARNING  # we do not terminate here, so it is a warning",
            "        logger.warning(msg)",
            "",
            "    def print_file_status(self, status, path):",
            "        if self.output_list and (self.output_filter is None or status in self.output_filter):",
            "            if self.log_json:",
            "                print(json.dumps({",
            "                    'type': 'file_status',",
            "                    'status': status,",
            "                    'path': remove_surrogates(path),",
            "                }), file=sys.stderr)",
            "            else:",
            "                logging.getLogger('borg.output.list').info(\"%1s %s\", status, remove_surrogates(path))",
            "",
            "    @staticmethod",
            "    def build_matcher(inclexcl_patterns, include_paths):",
            "        matcher = PatternMatcher()",
            "        matcher.add_inclexcl(inclexcl_patterns)",
            "        matcher.add_includepaths(include_paths)",
            "        return matcher",
            "",
            "    def do_serve(self, args):",
            "        \"\"\"Start in server mode. This command is usually not used manually.\"\"\"",
            "        RepositoryServer(",
            "            restrict_to_paths=args.restrict_to_paths,",
            "            restrict_to_repositories=args.restrict_to_repositories,",
            "            append_only=args.append_only,",
            "            storage_quota=args.storage_quota,",
            "        ).serve()",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(create=True, exclusive=True, manifest=False)",
            "    def do_init(self, args, repository):",
            "        \"\"\"Initialize an empty repository\"\"\"",
            "        path = args.location.canonical_path()",
            "        logger.info('Initializing repository at \"%s\"' % path)",
            "        try:",
            "            key = key_creator(repository, args)",
            "        except (EOFError, KeyboardInterrupt):",
            "            repository.destroy()",
            "            return EXIT_WARNING",
            "        manifest = Manifest(key, repository)",
            "        manifest.key = key",
            "        manifest.write()",
            "        repository.commit()",
            "        with Cache(repository, key, manifest, warn_if_unencrypted=False):",
            "            pass",
            "        if key.tam_required:",
            "            tam_file = tam_required_file(repository)",
            "            open(tam_file, 'w').close()",
            "            logger.warning(",
            "                '\\n'",
            "                'By default repositories initialized with this version will produce security\\n'",
            "                'errors if written to with an older version (up to and including Borg 1.0.8).\\n'",
            "                '\\n'",
            "                'If you want to use these older versions, you can disable the check by running:\\n'",
            "                'borg upgrade --disable-tam \\'%s\\'\\n'",
            "                '\\n'",
            "                'See https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability '",
            "                'for details about the security implications.', path)",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, manifest=False)",
            "    def do_check(self, args, repository):",
            "        \"\"\"Check repository consistency\"\"\"",
            "        if args.repair:",
            "            msg = (\"'check --repair' is an experimental feature that might result in data loss.\" +",
            "                   \"\\n\" +",
            "                   \"Type 'YES' if you understand this and want to continue: \")",
            "            if not yes(msg, false_msg=\"Aborting.\", invalid_msg=\"Invalid answer, aborting.\",",
            "                       truish=('YES', ), retry=False,",
            "                       env_var_override='BORG_CHECK_I_KNOW_WHAT_I_AM_DOING'):",
            "                return EXIT_ERROR",
            "        if args.repo_only and any((args.verify_data, args.first, args.last, args.prefix)):",
            "            self.print_error(\"--repository-only contradicts --first, --last, --prefix and --verify-data arguments.\")",
            "            return EXIT_ERROR",
            "        if not args.archives_only:",
            "            if not repository.check(repair=args.repair, save_space=args.save_space):",
            "                return EXIT_WARNING",
            "        if args.prefix:",
            "            args.glob_archives = args.prefix + '*'",
            "        if not args.repo_only and not ArchiveChecker().check(",
            "                repository, repair=args.repair, archive=args.location.archive,",
            "                first=args.first, last=args.last, sort_by=args.sort_by or 'ts', glob=args.glob_archives,",
            "                verify_data=args.verify_data, save_space=args.save_space):",
            "            return EXIT_WARNING",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.CHECK,))",
            "    def do_change_passphrase(self, args, repository, manifest, key):",
            "        \"\"\"Change repository key file passphrase\"\"\"",
            "        if not hasattr(key, 'change_passphrase'):",
            "            print('This repository is not encrypted, cannot change the passphrase.')",
            "            return EXIT_ERROR",
            "        key.change_passphrase()",
            "        logger.info('Key updated')",
            "        if hasattr(key, 'find_key'):",
            "            # print key location to make backing it up easier",
            "            logger.info('Key location: %s', key.find_key())",
            "        return EXIT_SUCCESS",
            "",
            "    def do_change_passphrase_deprecated(self, args):",
            "        logger.warning('\"borg change-passphrase\" is deprecated and will be removed in Borg 1.2.\\n'",
            "                       'Use \"borg key change-passphrase\" instead.')",
            "        return self.do_change_passphrase(args)",
            "",
            "    @with_repository(lock=False, exclusive=False, manifest=False, cache=False)",
            "    def do_key_export(self, args, repository):",
            "        \"\"\"Export the repository key for backup\"\"\"",
            "        manager = KeyManager(repository)",
            "        manager.load_keyblob()",
            "        if args.paper:",
            "            manager.export_paperkey(args.path)",
            "        else:",
            "            if not args.path:",
            "                self.print_error(\"output file to export key to expected\")",
            "                return EXIT_ERROR",
            "            if args.qr:",
            "                manager.export_qr(args.path)",
            "            else:",
            "                manager.export(args.path)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(lock=False, exclusive=False, manifest=False, cache=False)",
            "    def do_key_import(self, args, repository):",
            "        \"\"\"Import the repository key from backup\"\"\"",
            "        manager = KeyManager(repository)",
            "        if args.paper:",
            "            if args.path:",
            "                self.print_error(\"with --paper import from file is not supported\")",
            "                return EXIT_ERROR",
            "            manager.import_paperkey(args)",
            "        else:",
            "            if not args.path:",
            "                self.print_error(\"input file to import key from expected\")",
            "                return EXIT_ERROR",
            "            if args.path != '-' and not os.path.exists(args.path):",
            "                self.print_error(\"input file does not exist: \" + args.path)",
            "                return EXIT_ERROR",
            "            manager.import_keyfile(args)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False)",
            "    def do_migrate_to_repokey(self, args, repository):",
            "        \"\"\"Migrate passphrase -> repokey\"\"\"",
            "        manifest_data = repository.get(Manifest.MANIFEST_ID)",
            "        key_old = PassphraseKey.detect(repository, manifest_data)",
            "        key_new = RepoKey(repository)",
            "        key_new.target = repository",
            "        key_new.repository_id = repository.id",
            "        key_new.enc_key = key_old.enc_key",
            "        key_new.enc_hmac_key = key_old.enc_hmac_key",
            "        key_new.id_key = key_old.id_key",
            "        key_new.chunk_seed = key_old.chunk_seed",
            "        key_new.change_passphrase()  # option to change key protection passphrase, save",
            "        logger.info('Key updated')",
            "        return EXIT_SUCCESS",
            "",
            "    def do_benchmark_crud(self, args):",
            "        \"\"\"Benchmark Create, Read, Update, Delete for archives.\"\"\"",
            "        def measurement_run(repo, path):",
            "            archive = repo + '::borg-benchmark-crud'",
            "            compression = '--compression=none'",
            "            # measure create perf (without files cache to always have it chunking)",
            "            t_start = time.monotonic()",
            "            rc = self.do_create(self.parse_args(['create', compression, '--files-cache=disabled', archive + '1', path]))",
            "            t_end = time.monotonic()",
            "            dt_create = t_end - t_start",
            "            assert rc == 0",
            "            # now build files cache",
            "            rc1 = self.do_create(self.parse_args(['create', compression, archive + '2', path]))",
            "            rc2 = self.do_delete(self.parse_args(['delete', archive + '2']))",
            "            assert rc1 == rc2 == 0",
            "            # measure a no-change update (archive1 is still present)",
            "            t_start = time.monotonic()",
            "            rc1 = self.do_create(self.parse_args(['create', compression, archive + '3', path]))",
            "            t_end = time.monotonic()",
            "            dt_update = t_end - t_start",
            "            rc2 = self.do_delete(self.parse_args(['delete', archive + '3']))",
            "            assert rc1 == rc2 == 0",
            "            # measure extraction (dry-run: without writing result to disk)",
            "            t_start = time.monotonic()",
            "            rc = self.do_extract(self.parse_args(['extract', '--dry-run', archive + '1']))",
            "            t_end = time.monotonic()",
            "            dt_extract = t_end - t_start",
            "            assert rc == 0",
            "            # measure archive deletion (of LAST present archive with the data)",
            "            t_start = time.monotonic()",
            "            rc = self.do_delete(self.parse_args(['delete', archive + '1']))",
            "            t_end = time.monotonic()",
            "            dt_delete = t_end - t_start",
            "            assert rc == 0",
            "            return dt_create, dt_update, dt_extract, dt_delete",
            "",
            "        @contextmanager",
            "        def test_files(path, count, size, random):",
            "            path = os.path.join(path, 'borg-test-data')",
            "            os.makedirs(path)",
            "            for i in range(count):",
            "                fname = os.path.join(path, 'file_%d' % i)",
            "                data = b'\\0' * size if not random else os.urandom(size)",
            "                with SyncFile(fname, binary=True) as fd:  # used for posix_fadvise's sake",
            "                    fd.write(data)",
            "            yield path",
            "            shutil.rmtree(path)",
            "",
            "        if '_BORG_BENCHMARK_CRUD_TEST' in os.environ:",
            "            tests = [",
            "                ('Z-TEST', 1, 1, False),",
            "                ('R-TEST', 1, 1, True),",
            "            ]",
            "        else:",
            "            tests = [",
            "                ('Z-BIG', 10, 100000000, False),",
            "                ('R-BIG', 10, 100000000, True),",
            "                ('Z-MEDIUM', 1000, 1000000, False),",
            "                ('R-MEDIUM', 1000, 1000000, True),",
            "                ('Z-SMALL', 10000, 10000, False),",
            "                ('R-SMALL', 10000, 10000, True),",
            "            ]",
            "",
            "        for msg, count, size, random in tests:",
            "            with test_files(args.path, count, size, random) as path:",
            "                dt_create, dt_update, dt_extract, dt_delete = measurement_run(args.location.canonical_path(), path)",
            "            total_size_MB = count * size / 1e06",
            "            file_size_formatted = format_file_size(size)",
            "            content = 'random' if random else 'all-zero'",
            "            fmt = '%s-%-10s %9.2f MB/s (%d * %s %s files: %.2fs)'",
            "            print(fmt % ('C', msg, total_size_MB / dt_create, count, file_size_formatted, content, dt_create))",
            "            print(fmt % ('R', msg, total_size_MB / dt_extract, count, file_size_formatted, content, dt_extract))",
            "            print(fmt % ('U', msg, total_size_MB / dt_update, count, file_size_formatted, content, dt_update))",
            "            print(fmt % ('D', msg, total_size_MB / dt_delete, count, file_size_formatted, content, dt_delete))",
            "",
            "        return 0",
            "",
            "    @with_repository(fake='dry_run', exclusive=True, compatibility=(Manifest.Operation.WRITE,))",
            "    def do_create(self, args, repository, manifest=None, key=None):",
            "        \"\"\"Create new archive\"\"\"",
            "        matcher = PatternMatcher(fallback=True)",
            "        matcher.add_inclexcl(args.patterns)",
            "",
            "        def create_inner(archive, cache, fso):",
            "            # Add cache dir to inode_skip list",
            "            skip_inodes = set()",
            "            try:",
            "                st = os.stat(get_cache_dir())",
            "                skip_inodes.add((st.st_ino, st.st_dev))",
            "            except OSError:",
            "                pass",
            "            # Add local repository dir to inode_skip list",
            "            if not args.location.host:",
            "                try:",
            "                    st = os.stat(args.location.path)",
            "                    skip_inodes.add((st.st_ino, st.st_dev))",
            "                except OSError:",
            "                    pass",
            "            for path in args.paths:",
            "                if path == '-':  # stdin",
            "                    path = 'stdin'",
            "                    if not dry_run:",
            "                        try:",
            "                            status = fso.process_stdin(path, cache)",
            "                        except BackupOSError as e:",
            "                            status = 'E'",
            "                            self.print_warning('%s: %s', path, e)",
            "                    else:",
            "                        status = '-'",
            "                    self.print_file_status(status, path)",
            "                    continue",
            "                path = os.path.normpath(path)",
            "                try:",
            "                    st = os.stat(path, follow_symlinks=False)",
            "                except OSError as e:",
            "                    self.print_warning('%s: %s', path, e)",
            "                    continue",
            "                if args.one_file_system:",
            "                    restrict_dev = st.st_dev",
            "                else:",
            "                    restrict_dev = None",
            "                self._process(fso, cache, matcher, args.exclude_caches, args.exclude_if_present,",
            "                              args.keep_exclude_tags, skip_inodes, path, restrict_dev,",
            "                              read_special=args.read_special, dry_run=dry_run, st=st)",
            "            if not dry_run:",
            "                archive.save(comment=args.comment, timestamp=args.timestamp)",
            "                if args.progress:",
            "                    archive.stats.show_progress(final=True)",
            "                args.stats |= args.json",
            "                archive.stats += fso.stats",
            "                if args.stats:",
            "                    if args.json:",
            "                        json_print(basic_json_data(manifest, cache=cache, extra={",
            "                            'archive': archive,",
            "                        }))",
            "                    else:",
            "                        log_multi(DASHES,",
            "                                  str(archive),",
            "                                  DASHES,",
            "                                  STATS_HEADER,",
            "                                  str(archive.stats),",
            "                                  str(cache),",
            "                                  DASHES, logger=logging.getLogger('borg.output.stats'))",
            "",
            "        self.output_filter = args.output_filter",
            "        self.output_list = args.output_list",
            "        self.ignore_inode = args.ignore_inode",
            "        self.nobsdflags = args.nobsdflags",
            "        self.exclude_nodump = args.exclude_nodump",
            "        self.files_cache_mode = args.files_cache_mode",
            "        dry_run = args.dry_run",
            "        t0 = datetime.utcnow()",
            "        t0_monotonic = time.monotonic()",
            "        if not dry_run:",
            "            with Cache(repository, key, manifest, do_files=args.cache_files, progress=args.progress,",
            "                       lock_wait=self.lock_wait, permit_adhoc_cache=args.no_cache_sync) as cache:",
            "                archive = Archive(repository, key, manifest, args.location.archive, cache=cache,",
            "                                  create=True, checkpoint_interval=args.checkpoint_interval,",
            "                                  numeric_owner=args.numeric_owner, noatime=args.noatime, noctime=args.noctime,",
            "                                  progress=args.progress,",
            "                                  chunker_params=args.chunker_params, start=t0, start_monotonic=t0_monotonic,",
            "                                  log_json=args.log_json)",
            "                metadata_collector = MetadataCollector(noatime=args.noatime, noctime=args.noctime,",
            "                    nobsdflags=args.nobsdflags, numeric_owner=args.numeric_owner, nobirthtime=args.nobirthtime)",
            "                cp = ChunksProcessor(cache=cache, key=key,",
            "                    add_item=archive.add_item, write_checkpoint=archive.write_checkpoint,",
            "                    checkpoint_interval=args.checkpoint_interval, rechunkify=False)",
            "                fso = FilesystemObjectProcessors(metadata_collector=metadata_collector, cache=cache, key=key,",
            "                    process_file_chunks=cp.process_file_chunks, add_item=archive.add_item,",
            "                    chunker_params=args.chunker_params)",
            "                create_inner(archive, cache, fso)",
            "        else:",
            "            create_inner(None, None, None)",
            "        return self.exit_code",
            "",
            "    def _process(self, fso, cache, matcher, exclude_caches, exclude_if_present,",
            "                 keep_exclude_tags, skip_inodes, path, restrict_dev,",
            "                 read_special=False, dry_run=False, st=None):",
            "        \"\"\"",
            "        Process *path* recursively according to the various parameters.",
            "",
            "        *st* (if given) is a *os.stat_result* object for *path*.",
            "",
            "        This should only raise on critical errors. Per-item errors must be handled within this method.",
            "        \"\"\"",
            "        try:",
            "            recurse_excluded_dir = False",
            "            if matcher.match(path):",
            "                if st is None:",
            "                    with backup_io('stat'):",
            "                        st = os.stat(path, follow_symlinks=False)",
            "            else:",
            "                self.print_file_status('x', path)",
            "                # get out here as quickly as possible:",
            "                # we only need to continue if we shall recurse into an excluded directory.",
            "                # if we shall not recurse, then do not even touch (stat()) the item, it",
            "                # could trigger an error, e.g. if access is forbidden, see #3209.",
            "                if not matcher.recurse_dir:",
            "                    return",
            "                if st is None:",
            "                    with backup_io('stat'):",
            "                        st = os.stat(path, follow_symlinks=False)",
            "                recurse_excluded_dir = stat.S_ISDIR(st.st_mode)",
            "                if not recurse_excluded_dir:",
            "                    return",
            "",
            "            if (st.st_ino, st.st_dev) in skip_inodes:",
            "                return",
            "            # if restrict_dev is given, we do not want to recurse into a new filesystem,",
            "            # but we WILL save the mountpoint directory (or more precise: the root",
            "            # directory of the mounted filesystem that shadows the mountpoint dir).",
            "            recurse = restrict_dev is None or st.st_dev == restrict_dev",
            "            status = None",
            "            if self.exclude_nodump:",
            "                # Ignore if nodump flag is set",
            "                with backup_io('flags'):",
            "                    if get_flags(path, st) & stat.UF_NODUMP:",
            "                        self.print_file_status('x', path)",
            "                        return",
            "            if stat.S_ISREG(st.st_mode):",
            "                if not dry_run:",
            "                    status = fso.process_file(path, st, cache, self.ignore_inode, self.files_cache_mode)",
            "            elif stat.S_ISDIR(st.st_mode):",
            "                if recurse:",
            "                    tag_paths = dir_is_tagged(path, exclude_caches, exclude_if_present)",
            "                    if tag_paths:",
            "                        if keep_exclude_tags and not dry_run:",
            "                            fso.process_dir(path, st)",
            "                            for tag_path in tag_paths:",
            "                                self._process(fso, cache, matcher, exclude_caches, exclude_if_present,",
            "                                              keep_exclude_tags, skip_inodes, tag_path, restrict_dev,",
            "                                              read_special=read_special, dry_run=dry_run)",
            "                        self.print_file_status('x', path)",
            "                        return",
            "                if not dry_run:",
            "                    if not recurse_excluded_dir:",
            "                        status = fso.process_dir(path, st)",
            "                if recurse:",
            "                    with backup_io('scandir'):",
            "                        entries = helpers.scandir_inorder(path)",
            "                    for dirent in entries:",
            "                        normpath = os.path.normpath(dirent.path)",
            "                        self._process(fso, cache, matcher, exclude_caches, exclude_if_present,",
            "                                      keep_exclude_tags, skip_inodes, normpath, restrict_dev,",
            "                                      read_special=read_special, dry_run=dry_run)",
            "            elif stat.S_ISLNK(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_symlink(path, st)",
            "                    else:",
            "                        try:",
            "                            st_target = os.stat(path)",
            "                        except OSError:",
            "                            special = False",
            "                        else:",
            "                            special = is_special(st_target.st_mode)",
            "                        if special:",
            "                            status = fso.process_file(path, st_target, cache)",
            "                        else:",
            "                            status = fso.process_symlink(path, st)",
            "            elif stat.S_ISFIFO(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_fifo(path, st)",
            "                    else:",
            "                        status = fso.process_file(path, st, cache)",
            "            elif stat.S_ISCHR(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_dev(path, st, 'c')",
            "                    else:",
            "                        status = fso.process_file(path, st, cache)",
            "            elif stat.S_ISBLK(st.st_mode):",
            "                if not dry_run:",
            "                    if not read_special:",
            "                        status = fso.process_dev(path, st, 'b')",
            "                    else:",
            "                        status = fso.process_file(path, st, cache)",
            "            elif stat.S_ISSOCK(st.st_mode):",
            "                # Ignore unix sockets",
            "                return",
            "            elif stat.S_ISDOOR(st.st_mode):",
            "                # Ignore Solaris doors",
            "                return",
            "            elif stat.S_ISPORT(st.st_mode):",
            "                # Ignore Solaris event ports",
            "                return",
            "            else:",
            "                self.print_warning('Unknown file type: %s', path)",
            "                return",
            "        except BackupOSError as e:",
            "            self.print_warning('%s: %s', path, e)",
            "            status = 'E'",
            "        # Status output",
            "        if status is None:",
            "            if not dry_run:",
            "                status = '?'  # need to add a status code somewhere",
            "            else:",
            "                status = '-'  # dry run, item was not backed up",
            "",
            "        if not recurse_excluded_dir:",
            "            self.print_file_status(status, path)",
            "",
            "    @staticmethod",
            "    def build_filter(matcher, peek_and_store_hardlink_masters, strip_components):",
            "        if strip_components:",
            "            def item_filter(item):",
            "                matched = matcher.match(item.path) and os.sep.join(item.path.split(os.sep)[strip_components:])",
            "                peek_and_store_hardlink_masters(item, matched)",
            "                return matched",
            "        else:",
            "            def item_filter(item):",
            "                matched = matcher.match(item.path)",
            "                peek_and_store_hardlink_masters(item, matched)",
            "                return matched",
            "        return item_filter",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    @with_archive",
            "    def do_extract(self, args, repository, manifest, key, archive):",
            "        \"\"\"Extract archive contents\"\"\"",
            "        # be restrictive when restoring files, restore permissions later",
            "        if sys.getfilesystemencoding() == 'ascii':",
            "            logger.warning('Warning: File system encoding is \"ascii\", extracting non-ascii filenames will not be supported.')",
            "            if sys.platform.startswith(('linux', 'freebsd', 'netbsd', 'openbsd', 'darwin', )):",
            "                logger.warning('Hint: You likely need to fix your locale setup. E.g. install locales and use: LANG=en_US.UTF-8')",
            "",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "",
            "        progress = args.progress",
            "        output_list = args.output_list",
            "        dry_run = args.dry_run",
            "        stdout = args.stdout",
            "        sparse = args.sparse",
            "        strip_components = args.strip_components",
            "        dirs = []",
            "        partial_extract = not matcher.empty() or strip_components",
            "        hardlink_masters = {} if partial_extract else None",
            "",
            "        def peek_and_store_hardlink_masters(item, matched):",
            "            if (partial_extract and not matched and hardlinkable(item.mode) and",
            "                    item.get('hardlink_master', True) and 'source' not in item):",
            "                hardlink_masters[item.get('path')] = (item.get('chunks'), None)",
            "",
            "        filter = self.build_filter(matcher, peek_and_store_hardlink_masters, strip_components)",
            "        if progress:",
            "            pi = ProgressIndicatorPercent(msg='%5.1f%% Extracting: %s', step=0.1, msgid='extract')",
            "            pi.output('Calculating size')",
            "            extracted_size = sum(item.get_size(hardlink_masters) for item in archive.iter_items(filter))",
            "            pi.total = extracted_size",
            "        else:",
            "            pi = None",
            "",
            "        for item in archive.iter_items(filter, preload=True):",
            "            orig_path = item.path",
            "            if strip_components:",
            "                item.path = os.sep.join(orig_path.split(os.sep)[strip_components:])",
            "            if not args.dry_run:",
            "                while dirs and not item.path.startswith(dirs[-1].path):",
            "                    dir_item = dirs.pop(-1)",
            "                    try:",
            "                        archive.extract_item(dir_item, stdout=stdout)",
            "                    except BackupOSError as e:",
            "                        self.print_warning('%s: %s', remove_surrogates(dir_item.path), e)",
            "            if output_list:",
            "                logging.getLogger('borg.output.list').info(remove_surrogates(orig_path))",
            "            try:",
            "                if dry_run:",
            "                    archive.extract_item(item, dry_run=True, pi=pi)",
            "                else:",
            "                    if stat.S_ISDIR(item.mode):",
            "                        dirs.append(item)",
            "                        archive.extract_item(item, stdout=stdout, restore_attrs=False)",
            "                    else:",
            "                        archive.extract_item(item, stdout=stdout, sparse=sparse, hardlink_masters=hardlink_masters,",
            "                                             stripped_components=strip_components, original_path=orig_path, pi=pi)",
            "            except BackupOSError as e:",
            "                self.print_warning('%s: %s', remove_surrogates(orig_path), e)",
            "",
            "        if pi:",
            "            pi.finish()",
            "",
            "        if not args.dry_run:",
            "            pi = ProgressIndicatorPercent(total=len(dirs), msg='Setting directory permissions %3.0f%%',",
            "                                          msgid='extract.permissions')",
            "            while dirs:",
            "                pi.show()",
            "                dir_item = dirs.pop(-1)",
            "                try:",
            "                    archive.extract_item(dir_item, stdout=stdout)",
            "                except BackupOSError as e:",
            "                    self.print_warning('%s: %s', remove_surrogates(dir_item.path), e)",
            "        for pattern in matcher.get_unmatched_include_patterns():",
            "            self.print_warning(\"Include pattern '%s' never matched.\", pattern)",
            "        if pi:",
            "            # clear progress output",
            "            pi.finish()",
            "        return self.exit_code",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    @with_archive",
            "    def do_export_tar(self, args, repository, manifest, key, archive):",
            "        \"\"\"Export archive contents as a tarball\"\"\"",
            "        self.output_list = args.output_list",
            "",
            "        # A quick note about the general design of tar_filter and tarfile;",
            "        # The tarfile module of Python can provide some compression mechanisms",
            "        # by itself, using the builtin gzip, bz2 and lzma modules (and \"tarmodes\"",
            "        # such as \"w:xz\").",
            "        #",
            "        # Doing so would have three major drawbacks:",
            "        # For one the compressor runs on the same thread as the program using the",
            "        # tarfile, stealing valuable CPU time from Borg and thus reducing throughput.",
            "        # Then this limits the available options - what about lz4? Brotli? zstd?",
            "        # The third issue is that systems can ship more optimized versions than those",
            "        # built into Python, e.g. pigz or pxz, which can use more than one thread for",
            "        # compression.",
            "        #",
            "        # Therefore we externalize compression by using a filter program, which has",
            "        # none of these drawbacks. The only issue of using an external filter is",
            "        # that it has to be installed -- hardly a problem, considering that",
            "        # the decompressor must be installed as well to make use of the exported tarball!",
            "",
            "        filter = None",
            "        if args.tar_filter == 'auto':",
            "            # Note that filter remains None if tarfile is '-'.",
            "            if args.tarfile.endswith('.tar.gz'):",
            "                filter = 'gzip'",
            "            elif args.tarfile.endswith('.tar.bz2'):",
            "                filter = 'bzip2'",
            "            elif args.tarfile.endswith('.tar.xz'):",
            "                filter = 'xz'",
            "            logger.debug('Automatically determined tar filter: %s', filter)",
            "        else:",
            "            filter = args.tar_filter",
            "",
            "        tarstream = dash_open(args.tarfile, 'wb')",
            "        tarstream_close = args.tarfile != '-'",
            "",
            "        if filter:",
            "            # When we put a filter between us and the final destination,",
            "            # the selected output (tarstream until now) becomes the output of the filter (=filterout).",
            "            # The decision whether to close that or not remains the same.",
            "            filterout = tarstream",
            "            filterout_close = tarstream_close",
            "            env = prepare_subprocess_env(system=True)",
            "            # There is no deadlock potential here (the subprocess docs warn about this), because",
            "            # communication with the process is a one-way road, i.e. the process can never block",
            "            # for us to do something while we block on the process for something different.",
            "            filterproc = popen_with_error_handling(filter, stdin=subprocess.PIPE, stdout=filterout,",
            "                                                   log_prefix='--tar-filter: ', env=env)",
            "            if not filterproc:",
            "                return EXIT_ERROR",
            "            # Always close the pipe, otherwise the filter process would not notice when we are done.",
            "            tarstream = filterproc.stdin",
            "            tarstream_close = True",
            "",
            "        # The | (pipe) symbol instructs tarfile to use a streaming mode of operation",
            "        # where it never seeks on the passed fileobj.",
            "        tar = tarfile.open(fileobj=tarstream, mode='w|')",
            "",
            "        self._export_tar(args, archive, tar)",
            "",
            "        # This does not close the fileobj (tarstream) we passed to it -- a side effect of the | mode.",
            "        tar.close()",
            "",
            "        if tarstream_close:",
            "            tarstream.close()",
            "",
            "        if filter:",
            "            logger.debug('Done creating tar, waiting for filter to die...')",
            "            rc = filterproc.wait()",
            "            if rc:",
            "                logger.error('--tar-filter exited with code %d, output file is likely unusable!', rc)",
            "                self.exit_code = EXIT_ERROR",
            "            else:",
            "                logger.debug('filter exited with code %d', rc)",
            "",
            "            if filterout_close:",
            "                filterout.close()",
            "",
            "        return self.exit_code",
            "",
            "    def _export_tar(self, args, archive, tar):",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "",
            "        progress = args.progress",
            "        output_list = args.output_list",
            "        strip_components = args.strip_components",
            "        partial_extract = not matcher.empty() or strip_components",
            "        hardlink_masters = {} if partial_extract else None",
            "",
            "        def peek_and_store_hardlink_masters(item, matched):",
            "            if (partial_extract and not matched and hardlinkable(item.mode) and",
            "                    item.get('hardlink_master', True) and 'source' not in item):",
            "                hardlink_masters[item.get('path')] = (item.get('chunks'), None)",
            "",
            "        filter = self.build_filter(matcher, peek_and_store_hardlink_masters, strip_components)",
            "",
            "        if progress:",
            "            pi = ProgressIndicatorPercent(msg='%5.1f%% Processing: %s', step=0.1, msgid='extract')",
            "            pi.output('Calculating size')",
            "            extracted_size = sum(item.get_size(hardlink_masters) for item in archive.iter_items(filter))",
            "            pi.total = extracted_size",
            "        else:",
            "            pi = None",
            "",
            "        def item_content_stream(item):",
            "            \"\"\"",
            "            Return a file-like object that reads from the chunks of *item*.",
            "            \"\"\"",
            "            chunk_iterator = archive.pipeline.fetch_many([chunk_id for chunk_id, _, _ in item.chunks])",
            "            if pi:",
            "                info = [remove_surrogates(item.path)]",
            "                return ChunkIteratorFileWrapper(chunk_iterator,",
            "                                                lambda read_bytes: pi.show(increase=len(read_bytes), info=info))",
            "            else:",
            "                return ChunkIteratorFileWrapper(chunk_iterator)",
            "",
            "        def item_to_tarinfo(item, original_path):",
            "            \"\"\"",
            "            Transform a Borg *item* into a tarfile.TarInfo object.",
            "",
            "            Return a tuple (tarinfo, stream), where stream may be a file-like object that represents",
            "            the file contents, if any, and is None otherwise. When *tarinfo* is None, the *item*",
            "            cannot be represented as a TarInfo object and should be skipped.",
            "            \"\"\"",
            "",
            "            # If we would use the PAX (POSIX) format (which we currently don't),",
            "            # we can support most things that aren't possible with classic tar",
            "            # formats, including GNU tar, such as:",
            "            # atime, ctime, possibly Linux capabilities (security.* xattrs)",
            "            # and various additions supported by GNU tar in POSIX mode.",
            "",
            "            stream = None",
            "            tarinfo = tarfile.TarInfo()",
            "            tarinfo.name = item.path",
            "            tarinfo.mtime = item.mtime / 1e9",
            "            tarinfo.mode = stat.S_IMODE(item.mode)",
            "            tarinfo.uid = item.uid",
            "            tarinfo.gid = item.gid",
            "            tarinfo.uname = item.user or ''",
            "            tarinfo.gname = item.group or ''",
            "            # The linkname in tar has the same dual use the 'source' attribute of Borg items,",
            "            # i.e. for symlinks it means the destination, while for hardlinks it refers to the",
            "            # file.",
            "            # Since hardlinks in tar have a different type code (LNKTYPE) the format might",
            "            # support hardlinking arbitrary objects (including symlinks and directories), but",
            "            # whether implementations actually support that is a whole different question...",
            "            tarinfo.linkname = \"\"",
            "",
            "            modebits = stat.S_IFMT(item.mode)",
            "            if modebits == stat.S_IFREG:",
            "                tarinfo.type = tarfile.REGTYPE",
            "                if 'source' in item:",
            "                    source = os.sep.join(item.source.split(os.sep)[strip_components:])",
            "                    if hardlink_masters is None:",
            "                        linkname = source",
            "                    else:",
            "                        chunks, linkname = hardlink_masters.get(item.source, (None, source))",
            "                    if linkname:",
            "                        # Master was already added to the archive, add a hardlink reference to it.",
            "                        tarinfo.type = tarfile.LNKTYPE",
            "                        tarinfo.linkname = linkname",
            "                    elif chunks is not None:",
            "                        # The item which has the chunks was not put into the tar, therefore",
            "                        # we do that now and update hardlink_masters to reflect that.",
            "                        item.chunks = chunks",
            "                        tarinfo.size = item.get_size()",
            "                        stream = item_content_stream(item)",
            "                        hardlink_masters[item.get('source') or original_path] = (None, item.path)",
            "                else:",
            "                    tarinfo.size = item.get_size()",
            "                    stream = item_content_stream(item)",
            "            elif modebits == stat.S_IFDIR:",
            "                tarinfo.type = tarfile.DIRTYPE",
            "            elif modebits == stat.S_IFLNK:",
            "                tarinfo.type = tarfile.SYMTYPE",
            "                tarinfo.linkname = item.source",
            "            elif modebits == stat.S_IFBLK:",
            "                tarinfo.type = tarfile.BLKTYPE",
            "                tarinfo.devmajor = os.major(item.rdev)",
            "                tarinfo.devminor = os.minor(item.rdev)",
            "            elif modebits == stat.S_IFCHR:",
            "                tarinfo.type = tarfile.CHRTYPE",
            "                tarinfo.devmajor = os.major(item.rdev)",
            "                tarinfo.devminor = os.minor(item.rdev)",
            "            elif modebits == stat.S_IFIFO:",
            "                tarinfo.type = tarfile.FIFOTYPE",
            "            else:",
            "                self.print_warning('%s: unsupported file type %o for tar export', remove_surrogates(item.path), modebits)",
            "                set_ec(EXIT_WARNING)",
            "                return None, stream",
            "            return tarinfo, stream",
            "",
            "        for item in archive.iter_items(filter, preload=True):",
            "            orig_path = item.path",
            "            if strip_components:",
            "                item.path = os.sep.join(orig_path.split(os.sep)[strip_components:])",
            "            tarinfo, stream = item_to_tarinfo(item, orig_path)",
            "            if tarinfo:",
            "                if output_list:",
            "                    logging.getLogger('borg.output.list').info(remove_surrogates(orig_path))",
            "                tar.addfile(tarinfo, stream)",
            "",
            "        if pi:",
            "            pi.finish()",
            "",
            "        for pattern in matcher.get_unmatched_include_patterns():",
            "            self.print_warning(\"Include pattern '%s' never matched.\", pattern)",
            "        return self.exit_code",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    @with_archive",
            "    def do_diff(self, args, repository, manifest, key, archive):",
            "        \"\"\"Diff contents of two archives\"\"\"",
            "",
            "        def print_output(diff, path):",
            "            print(\"{:<19} {}\".format(diff, path))",
            "",
            "        archive1 = archive",
            "        archive2 = Archive(repository, key, manifest, args.archive2,",
            "                           consider_part_files=args.consider_part_files)",
            "",
            "        can_compare_chunk_ids = archive1.metadata.get('chunker_params', False) == archive2.metadata.get(",
            "            'chunker_params', True) or args.same_chunker_params",
            "        if not can_compare_chunk_ids:",
            "            self.print_warning('--chunker-params might be different between archives, diff will be slow.\\n'",
            "                               'If you know for certain that they are the same, pass --same-chunker-params '",
            "                               'to override this check.')",
            "",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "",
            "        diffs = Archive.compare_archives_iter(archive1, archive2, matcher, can_compare_chunk_ids=can_compare_chunk_ids)",
            "        # Conversion to string and filtering for diff.equal to save memory if sorting",
            "        diffs = ((path, str(diff)) for path, diff in diffs if not diff.equal)",
            "",
            "        if args.sort:",
            "            diffs = sorted(diffs)",
            "",
            "        for path, diff in diffs:",
            "            print_output(diff, path)",
            "",
            "        for pattern in matcher.get_unmatched_include_patterns():",
            "            self.print_warning(\"Include pattern '%s' never matched.\", pattern)",
            "",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, cache=True, compatibility=(Manifest.Operation.CHECK,))",
            "    @with_archive",
            "    def do_rename(self, args, repository, manifest, key, cache, archive):",
            "        \"\"\"Rename an existing archive\"\"\"",
            "        name = replace_placeholders(args.name)",
            "        archive.rename(name)",
            "        manifest.write()",
            "        repository.commit()",
            "        cache.commit()",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, manifest=False)",
            "    def do_delete(self, args, repository):",
            "        \"\"\"Delete an existing repository or archives\"\"\"",
            "        archive_filter_specified = args.first or args.last or args.prefix or args.glob_archives",
            "        explicit_archives_specified = args.location.archive or args.archives",
            "        if archive_filter_specified and explicit_archives_specified:",
            "            self.print_error('Mixing archive filters and explicitly named archives is not supported.')",
            "            return self.exit_code",
            "        if archive_filter_specified or explicit_archives_specified:",
            "            return self._delete_archives(args, repository)",
            "        else:",
            "            return self._delete_repository(args, repository)",
            "",
            "    def _delete_archives(self, args, repository):",
            "        \"\"\"Delete archives\"\"\"",
            "        manifest, key = Manifest.load(repository, (Manifest.Operation.DELETE,))",
            "",
            "        if args.location.archive or args.archives:",
            "            archives = list(args.archives)",
            "            if args.location.archive:",
            "                archives.insert(0, args.location.archive)",
            "            archive_names = tuple(archives)",
            "        else:",
            "            archive_names = tuple(x.name for x in manifest.archives.list_considering(args))",
            "            if not archive_names:",
            "                return self.exit_code",
            "",
            "        if args.forced == 2:",
            "            deleted = False",
            "            for i, archive_name in enumerate(archive_names, 1):",
            "                try:",
            "                    del manifest.archives[archive_name]",
            "                except KeyError:",
            "                    self.exit_code = EXIT_WARNING",
            "                    logger.warning('Archive {} not found ({}/{}).'.format(archive_name, i, len(archive_names)))",
            "                else:",
            "                    deleted = True",
            "                    logger.info('Deleted {} ({}/{}).'.format(archive_name, i, len(archive_names)))",
            "            if deleted:",
            "                manifest.write()",
            "                # note: might crash in compact() after committing the repo",
            "                repository.commit()",
            "                logger.info('Done. Run \"borg check --repair\" to clean up the mess.')",
            "            else:",
            "                logger.warning('Aborted.')",
            "            return self.exit_code",
            "",
            "        stats_logger = logging.getLogger('borg.output.stats')",
            "        if args.stats:",
            "            log_multi(DASHES, STATS_HEADER, logger=stats_logger)",
            "",
            "        with Cache(repository, key, manifest, progress=args.progress, lock_wait=self.lock_wait) as cache:",
            "            for i, archive_name in enumerate(archive_names, 1):",
            "                logger.info('Deleting {} ({}/{}):'.format(archive_name, i, len(archive_names)))",
            "                archive = Archive(repository, key, manifest, archive_name, cache=cache)",
            "                stats = Statistics()",
            "                archive.delete(stats, progress=args.progress, forced=args.forced)",
            "                manifest.write()",
            "                repository.commit(save_space=args.save_space)",
            "                cache.commit()",
            "                logger.info(\"Archive deleted.\")",
            "                if args.stats:",
            "                    log_multi(stats.summary.format(label='Deleted data:', stats=stats),",
            "                              DASHES, logger=stats_logger)",
            "                if args.forced == 0 and self.exit_code:",
            "                    break",
            "            if args.stats:",
            "                stats_logger.info(str(cache))",
            "",
            "        return self.exit_code",
            "",
            "    def _delete_repository(self, args, repository):",
            "        \"\"\"Delete a repository\"\"\"",
            "        if not args.cache_only:",
            "            msg = []",
            "            try:",
            "                manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            except NoManifestError:",
            "                msg.append(\"You requested to completely DELETE the repository *including* all archives it may \"",
            "                           \"contain.\")",
            "                msg.append(\"This repository seems to have no manifest, so we can't tell anything about its \"",
            "                           \"contents.\")",
            "            else:",
            "                msg.append(\"You requested to completely DELETE the repository *including* all archives it \"",
            "                           \"contains:\")",
            "                for archive_info in manifest.archives.list(sort_by=['ts']):",
            "                    msg.append(format_archive(archive_info))",
            "            msg.append(\"Type 'YES' if you understand this and want to continue: \")",
            "            msg = '\\n'.join(msg)",
            "            if not yes(msg, false_msg=\"Aborting.\", invalid_msg='Invalid answer, aborting.', truish=('YES',),",
            "                       retry=False, env_var_override='BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'):",
            "                self.exit_code = EXIT_ERROR",
            "                return self.exit_code",
            "            repository.destroy()",
            "            logger.info(\"Repository deleted.\")",
            "        Cache.destroy(repository)",
            "        logger.info(\"Cache deleted.\")",
            "        return self.exit_code",
            "",
            "    def do_mount(self, args):",
            "        \"\"\"Mount archive or an entire repository as a FUSE filesystem\"\"\"",
            "        # Perform these checks before opening the repository and asking for a passphrase.",
            "",
            "        try:",
            "            import borg.fuse",
            "        except ImportError as e:",
            "            self.print_error('borg mount not available: loading FUSE support failed [ImportError: %s]' % str(e))",
            "            return self.exit_code",
            "",
            "        if not os.path.isdir(args.mountpoint) or not os.access(args.mountpoint, os.R_OK | os.W_OK | os.X_OK):",
            "            self.print_error('%s: Mountpoint must be a writable directory' % args.mountpoint)",
            "            return self.exit_code",
            "",
            "        return self._do_mount(args)",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    def _do_mount(self, args, repository, manifest, key):",
            "        from .fuse import FuseOperations",
            "",
            "        with cache_if_remote(repository, decrypted_cache=key) as cached_repo:",
            "            operations = FuseOperations(key, repository, manifest, args, cached_repo)",
            "            logger.info(\"Mounting filesystem\")",
            "            try:",
            "                operations.mount(args.mountpoint, args.options, args.foreground)",
            "            except RuntimeError:",
            "                # Relevant error message already printed to stderr by FUSE",
            "                self.exit_code = EXIT_ERROR",
            "        return self.exit_code",
            "",
            "    def do_umount(self, args):",
            "        \"\"\"un-mount the FUSE filesystem\"\"\"",
            "        return umount(args.mountpoint)",
            "",
            "    @with_repository(compatibility=(Manifest.Operation.READ,))",
            "    def do_list(self, args, repository, manifest, key):",
            "        \"\"\"List archive or repository contents\"\"\"",
            "        if args.location.archive:",
            "            if args.json:",
            "                self.print_error('The --json option is only valid for listing archives, not archive contents.')",
            "                return self.exit_code",
            "            return self._list_archive(args, repository, manifest, key)",
            "        else:",
            "            if args.json_lines:",
            "                self.print_error('The --json-lines option is only valid for listing archive contents, not archives.')",
            "                return self.exit_code",
            "            return self._list_repository(args, repository, manifest, key)",
            "",
            "    def _list_archive(self, args, repository, manifest, key):",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "        if args.format is not None:",
            "            format = args.format",
            "        elif args.short:",
            "            format = \"{path}{NL}\"",
            "        else:",
            "            format = \"{mode} {user:6} {group:6} {size:8} {mtime} {path}{extra}{NL}\"",
            "",
            "        def _list_inner(cache):",
            "            archive = Archive(repository, key, manifest, args.location.archive, cache=cache,",
            "                              consider_part_files=args.consider_part_files)",
            "",
            "            formatter = ItemFormatter(archive, format, json_lines=args.json_lines)",
            "            for item in archive.iter_items(lambda item: matcher.match(item.path)):",
            "                sys.stdout.write(formatter.format_item(item))",
            "",
            "        # Only load the cache if it will be used",
            "        if ItemFormatter.format_needs_cache(format):",
            "            with Cache(repository, key, manifest, lock_wait=self.lock_wait) as cache:",
            "                _list_inner(cache)",
            "        else:",
            "            _list_inner(cache=None)",
            "",
            "        return self.exit_code",
            "",
            "    def _list_repository(self, args, repository, manifest, key):",
            "        if args.format is not None:",
            "            format = args.format",
            "        elif args.short:",
            "            format = \"{archive}{NL}\"",
            "        else:",
            "            format = \"{archive:<36} {time} [{id}]{NL}\"",
            "        formatter = ArchiveFormatter(format, repository, manifest, key, json=args.json)",
            "",
            "        output_data = []",
            "",
            "        for archive_info in manifest.archives.list_considering(args):",
            "            if args.json:",
            "                output_data.append(formatter.get_item_data(archive_info))",
            "            else:",
            "                sys.stdout.write(formatter.format_item(archive_info))",
            "",
            "        if args.json:",
            "            json_print(basic_json_data(manifest, extra={",
            "                'archives': output_data",
            "            }))",
            "",
            "        return self.exit_code",
            "",
            "    @with_repository(cache=True, compatibility=(Manifest.Operation.READ,))",
            "    def do_info(self, args, repository, manifest, key, cache):",
            "        \"\"\"Show archive details such as disk space used\"\"\"",
            "        if any((args.location.archive, args.first, args.last, args.prefix, args.glob_archives)):",
            "            return self._info_archives(args, repository, manifest, key, cache)",
            "        else:",
            "            return self._info_repository(args, repository, manifest, key, cache)",
            "",
            "    def _info_archives(self, args, repository, manifest, key, cache):",
            "        def format_cmdline(cmdline):",
            "            return remove_surrogates(' '.join(shlex.quote(x) for x in cmdline))",
            "",
            "        if args.location.archive:",
            "            archive_names = (args.location.archive,)",
            "        else:",
            "            archive_names = tuple(x.name for x in manifest.archives.list_considering(args))",
            "            if not archive_names:",
            "                return self.exit_code",
            "",
            "        output_data = []",
            "",
            "        for i, archive_name in enumerate(archive_names, 1):",
            "            archive = Archive(repository, key, manifest, archive_name, cache=cache,",
            "                              consider_part_files=args.consider_part_files)",
            "            info = archive.info()",
            "            if args.json:",
            "                output_data.append(info)",
            "            else:",
            "                info['duration'] = format_timedelta(timedelta(seconds=info['duration']))",
            "                info['command_line'] = format_cmdline(info['command_line'])",
            "                print(textwrap.dedent(\"\"\"",
            "                Archive name: {name}",
            "                Archive fingerprint: {id}",
            "                Comment: {comment}",
            "                Hostname: {hostname}",
            "                Username: {username}",
            "                Time (start): {start}",
            "                Time (end): {end}",
            "                Duration: {duration}",
            "                Number of files: {stats[nfiles]}",
            "                Command line: {command_line}",
            "                Utilization of maximum supported archive size: {limits[max_archive_size]:.0%}",
            "                ------------------------------------------------------------------------------",
            "                                       Original size      Compressed size    Deduplicated size",
            "                This archive:   {stats[original_size]:>20s} {stats[compressed_size]:>20s} {stats[deduplicated_size]:>20s}",
            "                {cache}",
            "                \"\"\").strip().format(cache=cache, **info))",
            "            if self.exit_code:",
            "                break",
            "            if not args.json and len(archive_names) - i:",
            "                print()",
            "",
            "        if args.json:",
            "            json_print(basic_json_data(manifest, cache=cache, extra={",
            "                'archives': output_data,",
            "            }))",
            "        return self.exit_code",
            "",
            "    def _info_repository(self, args, repository, manifest, key, cache):",
            "        info = basic_json_data(manifest, cache=cache, extra={",
            "            'security_dir': cache.security_manager.dir,",
            "        })",
            "",
            "        if args.json:",
            "            json_print(info)",
            "        else:",
            "            encryption = 'Encrypted: '",
            "            if key.NAME == 'plaintext':",
            "                encryption += 'No'",
            "            else:",
            "                encryption += 'Yes (%s)' % key.NAME",
            "            if key.NAME.startswith('key file'):",
            "                encryption += '\\nKey file: %s' % key.find_key()",
            "            info['encryption'] = encryption",
            "",
            "            print(textwrap.dedent(\"\"\"",
            "            Repository ID: {id}",
            "            Location: {location}",
            "            {encryption}",
            "            Cache: {cache.path}",
            "            Security dir: {security_dir}",
            "            \"\"\").strip().format(",
            "                id=bin_to_hex(repository.id),",
            "                location=repository._location.canonical_path(),",
            "                **info))",
            "            print(DASHES)",
            "            print(STATS_HEADER)",
            "            print(str(cache))",
            "        return self.exit_code",
            "",
            "    @with_repository(exclusive=True, compatibility=(Manifest.Operation.DELETE,))",
            "    def do_prune(self, args, repository, manifest, key):",
            "        \"\"\"Prune repository archives according to specified rules\"\"\"",
            "        if not any((args.secondly, args.minutely, args.hourly, args.daily,",
            "                    args.weekly, args.monthly, args.yearly, args.within)):",
            "            self.print_error('At least one of the \"keep-within\", \"keep-last\", '",
            "                             '\"keep-secondly\", \"keep-minutely\", \"keep-hourly\", \"keep-daily\", '",
            "                             '\"keep-weekly\", \"keep-monthly\" or \"keep-yearly\" settings must be specified.')",
            "            return self.exit_code",
            "        if args.prefix:",
            "            args.glob_archives = args.prefix + '*'",
            "        checkpoint_re = r'\\.checkpoint(\\.\\d+)?'",
            "        archives_checkpoints = manifest.archives.list(glob=args.glob_archives,",
            "                                                      match_end=r'(%s)?\\Z' % checkpoint_re,",
            "                                                      sort_by=['ts'], reverse=True)",
            "        is_checkpoint = re.compile(r'(%s)\\Z' % checkpoint_re).search",
            "        checkpoints = [arch for arch in archives_checkpoints if is_checkpoint(arch.name)]",
            "        # keep the latest checkpoint, if there is no later non-checkpoint archive",
            "        if archives_checkpoints and checkpoints and archives_checkpoints[0] is checkpoints[0]:",
            "            keep_checkpoints = checkpoints[:1]",
            "        else:",
            "            keep_checkpoints = []",
            "        checkpoints = set(checkpoints)",
            "        # ignore all checkpoint archives to avoid keeping one (which is an incomplete backup)",
            "        # that is newer than a successfully completed backup - and killing the successful backup.",
            "        archives = [arch for arch in archives_checkpoints if arch not in checkpoints]",
            "        keep = []",
            "        # collect the rule responsible for the keeping of each archive in this dict",
            "        # keys are archive ids, values are a tuple",
            "        #   (<rulename>, <how many archives were kept by this rule so far >)",
            "        kept_because = {}",
            "",
            "        # find archives which need to be kept because of the keep-within rule",
            "        if args.within:",
            "            keep += prune_within(archives, args.within, kept_because)",
            "",
            "        # find archives which need to be kept because of the various time period rules",
            "        for rule in PRUNING_PATTERNS.keys():",
            "            num = getattr(args, rule, None)",
            "            if num is not None:",
            "                keep += prune_split(archives, rule, num, kept_because)",
            "",
            "        to_delete = (set(archives) | checkpoints) - (set(keep) | set(keep_checkpoints))",
            "        stats = Statistics()",
            "        with Cache(repository, key, manifest, do_files=False, lock_wait=self.lock_wait) as cache:",
            "            list_logger = logging.getLogger('borg.output.list')",
            "            # set up counters for the progress display",
            "            to_delete_len = len(to_delete)",
            "            archives_deleted = 0",
            "            for archive in archives_checkpoints:",
            "                if archive in to_delete:",
            "                    if args.dry_run:",
            "                        log_message = 'Would prune:'",
            "                    else:",
            "                        archives_deleted += 1",
            "                        log_message = 'Pruning archive (%d/%d):' % (archives_deleted, to_delete_len)",
            "                        Archive(repository, key, manifest, archive.name, cache,",
            "                                progress=args.progress).delete(stats, forced=args.forced)",
            "                else:",
            "                    if is_checkpoint(archive.name):",
            "                        log_message = 'Keeping checkpoint archive:'",
            "                    else:",
            "                        log_message = 'Keeping archive (rule: {rule} #{num}):'.format(",
            "                            rule=kept_because[archive.id][0], num=kept_because[archive.id][1]",
            "                        )",
            "                if args.output_list:",
            "                    list_logger.info(\"{message:<40} {archive}\".format(",
            "                        message=log_message, archive=format_archive(archive)",
            "                    ))",
            "            if to_delete and not args.dry_run:",
            "                manifest.write()",
            "                repository.commit(save_space=args.save_space)",
            "                cache.commit()",
            "            if args.stats:",
            "                log_multi(DASHES,",
            "                          STATS_HEADER,",
            "                          stats.summary.format(label='Deleted data:', stats=stats),",
            "                          str(cache),",
            "                          DASHES, logger=logging.getLogger('borg.output.stats'))",
            "        return self.exit_code",
            "",
            "    @with_repository(fake=('tam', 'disable_tam'), invert_fake=True, manifest=False, exclusive=True)",
            "    def do_upgrade(self, args, repository, manifest=None, key=None):",
            "        \"\"\"upgrade a repository from a previous version\"\"\"",
            "        if args.tam:",
            "            manifest, key = Manifest.load(repository, (Manifest.Operation.CHECK,), force_tam_not_required=args.force)",
            "",
            "            if not hasattr(key, 'change_passphrase'):",
            "                print('This repository is not encrypted, cannot enable TAM.')",
            "                return EXIT_ERROR",
            "",
            "            if not manifest.tam_verified or not manifest.config.get(b'tam_required', False):",
            "                # The standard archive listing doesn't include the archive ID like in borg 1.1.x",
            "                print('Manifest contents:')",
            "                for archive_info in manifest.archives.list(sort_by=['ts']):",
            "                    print(format_archive(archive_info), '[%s]' % bin_to_hex(archive_info.id))",
            "                manifest.config[b'tam_required'] = True",
            "                manifest.write()",
            "                repository.commit()",
            "            if not key.tam_required:",
            "                key.tam_required = True",
            "                key.change_passphrase(key._passphrase)",
            "                print('Key updated')",
            "                if hasattr(key, 'find_key'):",
            "                    print('Key location:', key.find_key())",
            "            if not tam_required(repository):",
            "                tam_file = tam_required_file(repository)",
            "                open(tam_file, 'w').close()",
            "                print('Updated security database')",
            "        elif args.disable_tam:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK, force_tam_not_required=True)",
            "            if tam_required(repository):",
            "                os.unlink(tam_required_file(repository))",
            "            if key.tam_required:",
            "                key.tam_required = False",
            "                key.change_passphrase(key._passphrase)",
            "                print('Key updated')",
            "                if hasattr(key, 'find_key'):",
            "                    print('Key location:', key.find_key())",
            "            manifest.config[b'tam_required'] = False",
            "            manifest.write()",
            "            repository.commit()",
            "        else:",
            "            # mainly for upgrades from Attic repositories,",
            "            # but also supports borg 0.xx -> 1.0 upgrade.",
            "",
            "            repo = AtticRepositoryUpgrader(args.location.path, create=False)",
            "            try:",
            "                repo.upgrade(args.dry_run, inplace=args.inplace, progress=args.progress)",
            "            except NotImplementedError as e:",
            "                print(\"warning: %s\" % e)",
            "            repo = BorgRepositoryUpgrader(args.location.path, create=False)",
            "            try:",
            "                repo.upgrade(args.dry_run, inplace=args.inplace, progress=args.progress)",
            "            except NotImplementedError as e:",
            "                print(\"warning: %s\" % e)",
            "        return self.exit_code",
            "",
            "    @with_repository(cache=True, exclusive=True, compatibility=(Manifest.Operation.CHECK,))",
            "    def do_recreate(self, args, repository, manifest, key, cache):",
            "        \"\"\"Re-create archives\"\"\"",
            "        msg = (\"recreate is an experimental feature.\\n\"",
            "               \"Type 'YES' if you understand this and want to continue: \")",
            "        if not yes(msg, false_msg=\"Aborting.\", truish=('YES',),",
            "                   env_var_override='BORG_RECREATE_I_KNOW_WHAT_I_AM_DOING'):",
            "            return EXIT_ERROR",
            "",
            "        matcher = self.build_matcher(args.patterns, args.paths)",
            "        self.output_list = args.output_list",
            "        self.output_filter = args.output_filter",
            "        recompress = args.recompress != 'never'",
            "        always_recompress = args.recompress == 'always'",
            "",
            "        recreater = ArchiveRecreater(repository, manifest, key, cache, matcher,",
            "                                     exclude_caches=args.exclude_caches, exclude_if_present=args.exclude_if_present,",
            "                                     keep_exclude_tags=args.keep_exclude_tags, chunker_params=args.chunker_params,",
            "                                     compression=args.compression, recompress=recompress, always_recompress=always_recompress,",
            "                                     progress=args.progress, stats=args.stats,",
            "                                     file_status_printer=self.print_file_status,",
            "                                     checkpoint_interval=args.checkpoint_interval,",
            "                                     dry_run=args.dry_run)",
            "",
            "        if args.location.archive:",
            "            name = args.location.archive",
            "            target = replace_placeholders(args.target) if args.target else None",
            "            if recreater.is_temporary_archive(name):",
            "                self.print_error('Refusing to work on temporary archive of prior recreate: %s', name)",
            "                return self.exit_code",
            "            if not recreater.recreate(name, args.comment, target):",
            "                self.print_error('Nothing to do. Archive was not processed.\\n'",
            "                                 'Specify at least one pattern, PATH, --comment, re-compression or re-chunking option.')",
            "        else:",
            "            if args.target is not None:",
            "                self.print_error('--target: Need to specify single archive')",
            "                return self.exit_code",
            "            for archive in manifest.archives.list(sort_by=['ts']):",
            "                name = archive.name",
            "                if recreater.is_temporary_archive(name):",
            "                    continue",
            "                print('Processing', name)",
            "                if not recreater.recreate(name, args.comment):",
            "                    logger.info('Skipped archive %s: Nothing to do. Archive was not processed.', name)",
            "        if not args.dry_run:",
            "            manifest.write()",
            "            repository.commit()",
            "            cache.commit()",
            "        return self.exit_code",
            "",
            "    @with_repository(manifest=False, exclusive=True)",
            "    def do_with_lock(self, args, repository):",
            "        \"\"\"run a user specified command with the repository lock held\"\"\"",
            "        # for a new server, this will immediately take an exclusive lock.",
            "        # to support old servers, that do not have \"exclusive\" arg in open()",
            "        # RPC API, we also do it the old way:",
            "        # re-write manifest to start a repository transaction - this causes a",
            "        # lock upgrade to exclusive for remote (and also for local) repositories.",
            "        # by using manifest=False in the decorator, we avoid having to require",
            "        # the encryption key (and can operate just with encrypted data).",
            "        data = repository.get(Manifest.MANIFEST_ID)",
            "        repository.put(Manifest.MANIFEST_ID, data)",
            "        # usually, a 0 byte (open for writing) segment file would be visible in the filesystem here.",
            "        # we write and close this file, to rather have a valid segment file on disk, before invoking the subprocess.",
            "        # we can only do this for local repositories (with .io), though:",
            "        if hasattr(repository, 'io'):",
            "            repository.io.close_segment()",
            "        env = prepare_subprocess_env(system=True)",
            "        try:",
            "            # we exit with the return code we get from the subprocess",
            "            return subprocess.call([args.command] + args.args, env=env)",
            "        finally:",
            "            # we need to commit the \"no change\" operation we did to the manifest",
            "            # because it created a new segment file in the repository. if we would",
            "            # roll back, the same file would be later used otherwise (for other content).",
            "            # that would be bad if somebody uses rsync with ignore-existing (or",
            "            # any other mechanism relying on existing segment data not changing).",
            "            # see issue #1867.",
            "            repository.commit()",
            "",
            "    @with_repository(exclusive=True, cache=True, compatibility=(Manifest.Operation.WRITE,))",
            "    def do_config(self, args, repository, manifest, key, cache):",
            "        \"\"\"get, set, and delete values in a repository or cache config file\"\"\"",
            "        try:",
            "            section, name = args.name.split('.')",
            "        except ValueError:",
            "            section = args.cache and \"cache\" or \"repository\"",
            "            name = args.name",
            "",
            "        if args.cache:",
            "            cache.cache_config.load()",
            "            config = cache.cache_config._config",
            "            save = cache.cache_config.save",
            "        else:",
            "            config = repository.config",
            "            save = lambda: repository.save_config(repository.path, repository.config)",
            "",
            "        if args.delete:",
            "            config.remove_option(section, name)",
            "            if len(config.options(section)) == 0:",
            "                config.remove_section(section)",
            "            save()",
            "        elif args.value:",
            "            if section not in config.sections():",
            "                config.add_section(section)",
            "            config.set(section, name, args.value)",
            "            save()",
            "        else:",
            "            try:",
            "                print(config.get(section, name))",
            "            except (configparser.NoOptionError, configparser.NoSectionError) as e:",
            "                print(e, file=sys.stderr)",
            "                return EXIT_WARNING",
            "        return EXIT_SUCCESS",
            "",
            "    def do_debug_info(self, args):",
            "        \"\"\"display system information for debugging / bug reports\"\"\"",
            "        print(sysinfo())",
            "",
            "        # Additional debug information",
            "        print('CRC implementation:', crc32.__name__)",
            "        print('Process ID:', get_process_id())",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_archive_items(self, args, repository, manifest, key):",
            "        \"\"\"dump (decrypted, decompressed) archive items metadata (not: data)\"\"\"",
            "        archive = Archive(repository, key, manifest, args.location.archive,",
            "                          consider_part_files=args.consider_part_files)",
            "        for i, item_id in enumerate(archive.metadata.items):",
            "            data = key.decrypt(item_id, repository.get(item_id))",
            "            filename = '%06d_%s.items' % (i, bin_to_hex(item_id))",
            "            print('Dumping', filename)",
            "            with open(filename, 'wb') as fd:",
            "                fd.write(data)",
            "        print('Done.')",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_archive(self, args, repository, manifest, key):",
            "        \"\"\"dump decoded archive metadata (not: data)\"\"\"",
            "",
            "        try:",
            "            archive_meta_orig = manifest.archives.get_raw_dict()[safe_encode(args.location.archive)]",
            "        except KeyError:",
            "            raise Archive.DoesNotExist(args.location.archive)",
            "",
            "        indent = 4",
            "",
            "        def do_indent(d):",
            "            return textwrap.indent(json.dumps(d, indent=indent), prefix=' ' * indent)",
            "",
            "        def output(fd):",
            "            # this outputs megabytes of data for a modest sized archive, so some manual streaming json output",
            "            fd.write('{\\n')",
            "            fd.write('    \"_name\": ' + json.dumps(args.location.archive) + \",\\n\")",
            "            fd.write('    \"_manifest_entry\":\\n')",
            "            fd.write(do_indent(prepare_dump_dict(archive_meta_orig)))",
            "            fd.write(',\\n')",
            "",
            "            data = key.decrypt(archive_meta_orig[b'id'], repository.get(archive_meta_orig[b'id']))",
            "            archive_org_dict = msgpack.unpackb(data, object_hook=StableDict, unicode_errors='surrogateescape')",
            "",
            "            fd.write('    \"_meta\":\\n')",
            "            fd.write(do_indent(prepare_dump_dict(archive_org_dict)))",
            "            fd.write(',\\n')",
            "            fd.write('    \"_items\": [\\n')",
            "",
            "            unpacker = msgpack.Unpacker(use_list=False, object_hook=StableDict)",
            "            first = True",
            "            for item_id in archive_org_dict[b'items']:",
            "                data = key.decrypt(item_id, repository.get(item_id))",
            "                unpacker.feed(data)",
            "                for item in unpacker:",
            "                    item = prepare_dump_dict(item)",
            "                    if first:",
            "                        first = False",
            "                    else:",
            "                        fd.write(',\\n')",
            "                    fd.write(do_indent(item))",
            "",
            "            fd.write('\\n')",
            "            fd.write('    ]\\n}\\n')",
            "",
            "        with dash_open(args.path, 'w') as fd:",
            "            output(fd)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_manifest(self, args, repository, manifest, key):",
            "        \"\"\"dump decoded repository manifest\"\"\"",
            "",
            "        data = key.decrypt(None, repository.get(manifest.MANIFEST_ID))",
            "",
            "        meta = prepare_dump_dict(msgpack.fallback.unpackb(data, object_hook=StableDict, unicode_errors='surrogateescape'))",
            "",
            "        with dash_open(args.path, 'w') as fd:",
            "            json.dump(meta, fd, indent=4)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_dump_repo_objs(self, args, repository, manifest, key):",
            "        \"\"\"dump (decrypted, decompressed) repo objects\"\"\"",
            "        marker = None",
            "        i = 0",
            "        while True:",
            "            result = repository.list(limit=LIST_SCAN_LIMIT, marker=marker)",
            "            if not result:",
            "                break",
            "            marker = result[-1]",
            "            for id in result:",
            "                cdata = repository.get(id)",
            "                give_id = id if id != Manifest.MANIFEST_ID else None",
            "                data = key.decrypt(give_id, cdata)",
            "                filename = '%06d_%s.obj' % (i, bin_to_hex(id))",
            "                print('Dumping', filename)",
            "                with open(filename, 'wb') as fd:",
            "                    fd.write(data)",
            "                i += 1",
            "        print('Done.')",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False)",
            "    def do_debug_get_obj(self, args, repository):",
            "        \"\"\"get object contents from the repository and write it into file\"\"\"",
            "        hex_id = args.id",
            "        try:",
            "            id = unhexlify(hex_id)",
            "        except ValueError:",
            "            print(\"object id %s is invalid.\" % hex_id)",
            "        else:",
            "            try:",
            "                data = repository.get(id)",
            "            except Repository.ObjectNotFound:",
            "                print(\"object %s not found.\" % hex_id)",
            "            else:",
            "                with open(args.path, \"wb\") as f:",
            "                    f.write(data)",
            "                print(\"object %s fetched.\" % hex_id)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False, exclusive=True)",
            "    def do_debug_put_obj(self, args, repository):",
            "        \"\"\"put file(s) contents into the repository\"\"\"",
            "        for path in args.paths:",
            "            with open(path, \"rb\") as f:",
            "                data = f.read()",
            "            h = hashlib.sha256(data)  # XXX hardcoded",
            "            repository.put(h.digest(), data)",
            "            print(\"object %s put.\" % h.hexdigest())",
            "        repository.commit()",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False, exclusive=True)",
            "    def do_debug_delete_obj(self, args, repository):",
            "        \"\"\"delete the objects with the given IDs from the repo\"\"\"",
            "        modified = False",
            "        for hex_id in args.ids:",
            "            try:",
            "                id = unhexlify(hex_id)",
            "            except ValueError:",
            "                print(\"object id %s is invalid.\" % hex_id)",
            "            else:",
            "                try:",
            "                    repository.delete(id)",
            "                    modified = True",
            "                    print(\"object %s deleted.\" % hex_id)",
            "                except Repository.ObjectNotFound:",
            "                    print(\"object %s not found.\" % hex_id)",
            "        if modified:",
            "            repository.commit()",
            "        print('Done.')",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(manifest=False, exclusive=True, cache=True, compatibility=Manifest.NO_OPERATION_CHECK)",
            "    def do_debug_refcount_obj(self, args, repository, manifest, key, cache):",
            "        \"\"\"display refcounts for the objects with the given IDs\"\"\"",
            "        for hex_id in args.ids:",
            "            try:",
            "                id = unhexlify(hex_id)",
            "            except ValueError:",
            "                print(\"object id %s is invalid.\" % hex_id)",
            "            else:",
            "                try:",
            "                    refcount = cache.chunks[id][0]",
            "                    print(\"object %s has %d referrers [info from chunks cache].\" % (hex_id, refcount))",
            "                except KeyError:",
            "                    print(\"object %s not found [info from chunks cache].\" % hex_id)",
            "        return EXIT_SUCCESS",
            "",
            "    def do_debug_convert_profile(self, args):",
            "        \"\"\"convert Borg profile to Python profile\"\"\"",
            "        import marshal",
            "        with args.output, args.input:",
            "            marshal.dump(msgpack.unpack(args.input, use_list=False, encoding='utf-8'), args.output)",
            "        return EXIT_SUCCESS",
            "",
            "    @with_repository(lock=False, manifest=False)",
            "    def do_break_lock(self, args, repository):",
            "        \"\"\"Break the repository lock (e.g. in case it was left by a dead borg.\"\"\"",
            "        repository.break_lock()",
            "        Cache.break_lock(repository)",
            "        return self.exit_code",
            "",
            "    helptext = collections.OrderedDict()",
            "    helptext['patterns'] = textwrap.dedent('''",
            "        File patterns support these styles: fnmatch, shell, regular expressions,",
            "        path prefixes and path full-matches. By default, fnmatch is used for",
            "        ``--exclude`` patterns and shell-style is used for the experimental ``--pattern``",
            "        option.",
            "",
            "        If followed by a colon (':') the first two characters of a pattern are used as a",
            "        style selector. Explicit style selection is necessary when a",
            "        non-default style is desired or when the desired pattern starts with",
            "        two alphanumeric characters followed by a colon (i.e. `aa:something/*`).",
            "",
            "        `Fnmatch <https://docs.python.org/3/library/fnmatch.html>`_, selector `fm:`",
            "            This is the default style for ``--exclude`` and ``--exclude-from``.",
            "            These patterns use a variant of shell pattern syntax, with '\\*' matching",
            "            any number of characters, '?' matching any single character, '[...]'",
            "            matching any single character specified, including ranges, and '[!...]'",
            "            matching any character not specified. For the purpose of these patterns,",
            "            the path separator ('\\\\' for Windows and '/' on other systems) is not",
            "            treated specially. Wrap meta-characters in brackets for a literal",
            "            match (i.e. `[?]` to match the literal character `?`). For a path",
            "            to match a pattern, it must completely match from start to end, or",
            "            must match from the start to just before a path separator. Except",
            "            for the root path, paths will never end in the path separator when",
            "            matching is attempted.  Thus, if a given pattern ends in a path",
            "            separator, a '\\*' is appended before matching is attempted.",
            "",
            "        Shell-style patterns, selector `sh:`",
            "            This is the default style for ``--pattern`` and ``--patterns-from``.",
            "            Like fnmatch patterns these are similar to shell patterns. The difference",
            "            is that the pattern may include `**/` for matching zero or more directory",
            "            levels, `*` for matching zero or more arbitrary characters with the",
            "            exception of any path separator.",
            "",
            "        Regular expressions, selector `re:`",
            "            Regular expressions similar to those found in Perl are supported. Unlike",
            "            shell patterns regular expressions are not required to match the complete",
            "            path and any substring match is sufficient. It is strongly recommended to",
            "            anchor patterns to the start ('^'), to the end ('$') or both. Path",
            "            separators ('\\\\' for Windows and '/' on other systems) in paths are",
            "            always normalized to a forward slash ('/') before applying a pattern. The",
            "            regular expression syntax is described in the `Python documentation for",
            "            the re module <https://docs.python.org/3/library/re.html>`_.",
            "",
            "        Path prefix, selector `pp:`",
            "            This pattern style is useful to match whole sub-directories. The pattern",
            "            `pp:/data/bar` matches `/data/bar` and everything therein.",
            "",
            "        Path full-match, selector `pf:`",
            "            This pattern style is useful to match whole paths.",
            "            This is kind of a pseudo pattern as it can not have any variable or",
            "            unspecified parts - the full, precise path must be given.",
            "            `pf:/data/foo.txt` matches `/data/foo.txt` only.",
            "",
            "            Implementation note: this is implemented via very time-efficient O(1)",
            "            hashtable lookups (this means you can have huge amounts of such patterns",
            "            without impacting performance much).",
            "            Due to that, this kind of pattern does not respect any context or order.",
            "            If you use such a pattern to include a file, it will always be included",
            "            (if the directory recursion encounters it).",
            "            Other include/exclude patterns that would normally match will be ignored.",
            "            Same logic applies for exclude.",
            "",
            "        .. note::",
            "",
            "            `re:`, `sh:` and `fm:` patterns are all implemented on top of the Python SRE",
            "            engine. It is very easy to formulate patterns for each of these types which",
            "            requires an inordinate amount of time to match paths. If untrusted users",
            "            are able to supply patterns, ensure they cannot supply `re:` patterns.",
            "            Further, ensure that `sh:` and `fm:` patterns only contain a handful of",
            "            wildcards at most.",
            "",
            "        Exclusions can be passed via the command line option ``--exclude``. When used",
            "        from within a shell the patterns should be quoted to protect them from",
            "        expansion.",
            "",
            "        The ``--exclude-from`` option permits loading exclusion patterns from a text",
            "        file with one pattern per line. Lines empty or starting with the number sign",
            "        ('#') after removing whitespace on both ends are ignored. The optional style",
            "        selector prefix is also supported for patterns loaded from a file. Due to",
            "        whitespace removal paths with whitespace at the beginning or end can only be",
            "        excluded using regular expressions.",
            "",
            "        Examples::",
            "",
            "            # Exclude '/home/user/file.o' but not '/home/user/file.odt':",
            "            $ borg create -e '*.o' backup /",
            "",
            "            # Exclude '/home/user/junk' and '/home/user/subdir/junk' but",
            "            # not '/home/user/importantjunk' or '/etc/junk':",
            "            $ borg create -e '/home/*/junk' backup /",
            "",
            "            # Exclude the contents of '/home/user/cache' but not the directory itself:",
            "            $ borg create -e /home/user/cache/ backup /",
            "",
            "            # The file '/home/user/cache/important' is *not* backed up:",
            "            $ borg create -e /home/user/cache/ backup / /home/user/cache/important",
            "",
            "            # The contents of directories in '/home' are not backed up when their name",
            "            # ends in '.tmp'",
            "            $ borg create --exclude 're:^/home/[^/]+\\.tmp/' backup /",
            "",
            "            # Load exclusions from file",
            "            $ cat >exclude.txt <<EOF",
            "            # Comment line",
            "            /home/*/junk",
            "            *.tmp",
            "            fm:aa:something/*",
            "            re:^/home/[^/]\\.tmp/",
            "            sh:/home/*/.thumbnails",
            "            EOF",
            "            $ borg create --exclude-from exclude.txt backup /",
            "",
            "        .. container:: experimental",
            "",
            "            A more general and easier to use way to define filename matching patterns exists",
            "            with the experimental ``--pattern`` and ``--patterns-from`` options. Using these, you",
            "            may specify the backup roots (starting points) and patterns for inclusion/exclusion.",
            "            A root path starts with the prefix `R`, followed by a path (a plain path, not a",
            "            file pattern). An include rule starts with the prefix +, an exclude rule starts",
            "            with the prefix -, an exclude-norecurse rule starts with !, all followed by a pattern.",
            "            Inclusion patterns are useful to include paths that are contained in an excluded",
            "            path. The first matching pattern is used so if an include pattern matches before",
            "            an exclude pattern, the file is backed up. If an exclude-norecurse pattern matches",
            "            a directory, it won't recurse into it and won't discover any potential matches for",
            "            include rules below that directory.",
            "",
            "            Note that the default pattern style for ``--pattern`` and ``--patterns-from`` is",
            "            shell style (`sh:`), so those patterns behave similar to rsync include/exclude",
            "            patterns. The pattern style can be set via the `P` prefix.",
            "",
            "            Patterns (``--pattern``) and excludes (``--exclude``) from the command line are",
            "            considered first (in the order of appearance). Then patterns from ``--patterns-from``",
            "            are added. Exclusion patterns from ``--exclude-from`` files are appended last.",
            "",
            "            An example ``--patterns-from`` file could look like that::",
            "",
            "                # \"sh:\" pattern style is the default, so the following line is not needed:",
            "                P sh",
            "                R /",
            "                # can be rebuild",
            "                - /home/*/.cache",
            "                # they're downloads for a reason",
            "                - /home/*/Downloads",
            "                # susan is a nice person",
            "                # include susans home",
            "                + /home/susan",
            "                # don't backup the other home directories",
            "                - /home/*\\n\\n''')",
            "    helptext['placeholders'] = textwrap.dedent('''",
            "        Repository (or Archive) URLs, ``--prefix`` and ``--remote-path`` values support these",
            "        placeholders:",
            "",
            "        {hostname}",
            "            The (short) hostname of the machine.",
            "",
            "        {fqdn}",
            "            The full name of the machine.",
            "",
            "        {now}",
            "            The current local date and time, by default in ISO-8601 format.",
            "            You can also supply your own `format string <https://docs.python.org/3.5/library/datetime.html#strftime-and-strptime-behavior>`_, e.g. {now:%Y-%m-%d_%H:%M:%S}",
            "",
            "        {utcnow}",
            "            The current UTC date and time, by default in ISO-8601 format.",
            "            You can also supply your own `format string <https://docs.python.org/3.5/library/datetime.html#strftime-and-strptime-behavior>`_, e.g. {utcnow:%Y-%m-%d_%H:%M:%S}",
            "",
            "        {user}",
            "            The user name (or UID, if no name is available) of the user running borg.",
            "",
            "        {pid}",
            "            The current process ID.",
            "",
            "        {borgversion}",
            "            The version of borg, e.g.: 1.0.8rc1",
            "",
            "        {borgmajor}",
            "            The version of borg, only the major version, e.g.: 1",
            "",
            "        {borgminor}",
            "            The version of borg, only major and minor version, e.g.: 1.0",
            "",
            "        {borgpatch}",
            "            The version of borg, only major, minor and patch version, e.g.: 1.0.8",
            "",
            "        If literal curly braces need to be used, double them for escaping::",
            "",
            "            borg create /path/to/repo::{{literal_text}}",
            "",
            "        Examples::",
            "",
            "            borg create /path/to/repo::{hostname}-{user}-{utcnow} ...",
            "            borg create /path/to/repo::{hostname}-{now:%Y-%m-%d_%H:%M:%S} ...",
            "            borg prune --prefix '{hostname}-' ...",
            "",
            "        .. note::",
            "            systemd uses a difficult, non-standard syntax for command lines in unit files (refer to",
            "            the `systemd.unit(5)` manual page).",
            "",
            "            When invoking borg from unit files, pay particular attention to escaping,",
            "            especially when using the now/utcnow placeholders, since systemd performs its own",
            "            %-based variable replacement even in quoted text. To avoid interference from systemd,",
            "            double all percent signs (``{hostname}-{now:%Y-%m-%d_%H:%M:%S}``",
            "            becomes ``{hostname}-{now:%%Y-%%m-%%d_%%H:%%M:%%S}``).\\n\\n''')",
            "    helptext['compression'] = textwrap.dedent('''",
            "        It is no problem to mix different compression methods in one repo,",
            "        deduplication is done on the source data chunks (not on the compressed",
            "        or encrypted data).",
            "",
            "        If some specific chunk was once compressed and stored into the repo, creating",
            "        another backup that also uses this chunk will not change the stored chunk.",
            "        So if you use different compression specs for the backups, whichever stores a",
            "        chunk first determines its compression. See also borg recreate.",
            "",
            "        Compression is lz4 by default. If you want something else, you have to specify what you want.",
            "",
            "        Valid compression specifiers are:",
            "",
            "        none",
            "            Do not compress.",
            "",
            "        lz4",
            "            Use lz4 compression. High speed, low compression. (default)",
            "",
            "        zlib[,L]",
            "            Use zlib (\"gz\") compression. Medium speed, medium compression.",
            "            If you do not explicitely give the compression level L (ranging from 0",
            "            to 9), it will use level 6.",
            "            Giving level 0 (means \"no compression\", but still has zlib protocol",
            "            overhead) is usually pointless, you better use \"none\" compression.",
            "",
            "        lzma[,L]",
            "            Use lzma (\"xz\") compression. Low speed, high compression.",
            "            If you do not explicitely give the compression level L (ranging from 0",
            "            to 9), it will use level 6.",
            "            Giving levels above 6 is pointless and counterproductive because it does",
            "            not compress better due to the buffer size used by borg - but it wastes",
            "            lots of CPU cycles and RAM.",
            "",
            "        auto,C[,L]",
            "            Use a built-in heuristic to decide per chunk whether to compress or not.",
            "            The heuristic tries with lz4 whether the data is compressible.",
            "            For incompressible data, it will not use compression (uses \"none\").",
            "            For compressible data, it uses the given C[,L] compression - with C[,L]",
            "            being any valid compression specifier.",
            "",
            "        Examples::",
            "",
            "            borg create --compression lz4 REPO::ARCHIVE data",
            "            borg create --compression zlib REPO::ARCHIVE data",
            "            borg create --compression zlib,1 REPO::ARCHIVE data",
            "            borg create --compression auto,lzma,6 REPO::ARCHIVE data",
            "            borg create --compression auto,lzma ...\\n\\n''')",
            "",
            "    def do_help(self, parser, commands, args):",
            "        if not args.topic:",
            "            parser.print_help()",
            "        elif args.topic in self.helptext:",
            "            print(rst_to_terminal(self.helptext[args.topic]))",
            "        elif args.topic in commands:",
            "            if args.epilog_only:",
            "                print(commands[args.topic].epilog)",
            "            elif args.usage_only:",
            "                commands[args.topic].epilog = None",
            "                commands[args.topic].print_help()",
            "            else:",
            "                commands[args.topic].print_help()",
            "        else:",
            "            parser.error('No help available on %s' % (args.topic,))",
            "        return self.exit_code",
            "",
            "    def do_subcommand_help(self, parser, args):",
            "        \"\"\"display infos about subcommand\"\"\"",
            "        parser.print_help()",
            "        return EXIT_SUCCESS",
            "",
            "    do_maincommand_help = do_subcommand_help",
            "",
            "    def preprocess_args(self, args):",
            "        deprecations = [",
            "            # ('--old', '--new' or None, 'Warning: \"--old\" has been deprecated. Use \"--new\" instead.'),",
            "            ('--list-format', '--format', 'Warning: \"--list-format\" has been deprecated. Use \"--format\" instead.'),",
            "            ('--keep-tag-files', '--keep-exclude-tags', 'Warning: \"--keep-tag-files\" has been deprecated. Use \"--keep-exclude-tags\" instead.'),",
            "            ('--ignore-inode', None, 'Warning: \"--ignore-inode\" has been deprecated. Use \"--files-cache=ctime,size\" or \"...=mtime,size\" instead.'),",
            "            ('--no-files-cache', None, 'Warning: \"--no-files-cache\" has been deprecated. Use \"--files-cache=disabled\" instead.'),",
            "        ]",
            "        for i, arg in enumerate(args[:]):",
            "            for old_name, new_name, warning in deprecations:",
            "                if arg.startswith(old_name):",
            "                    if new_name is not None:",
            "                        args[i] = arg.replace(old_name, new_name)",
            "                    print(warning, file=sys.stderr)",
            "        return args",
            "",
            "    class CommonOptions:",
            "        \"\"\"",
            "        Support class to allow specifying common options directly after the top-level command.",
            "",
            "        Normally options can only be specified on the parser defining them, which means",
            "        that generally speaking *all* options go after all sub-commands. This is annoying",
            "        for common options in scripts, e.g. --remote-path or logging options.",
            "",
            "        This class allows adding the same set of options to both the top-level parser",
            "        and the final sub-command parsers (but not intermediary sub-commands, at least for now).",
            "",
            "        It does so by giving every option's target name (\"dest\") a suffix indicating its level",
            "        -- no two options in the parser hierarchy can have the same target --",
            "        then, after parsing the command line, multiple definitions are resolved.",
            "",
            "        Defaults are handled by only setting them on the top-level parser and setting",
            "        a sentinel object in all sub-parsers, which then allows to discern which parser",
            "        supplied the option.",
            "        \"\"\"",
            "",
            "        def __init__(self, define_common_options, suffix_precedence):",
            "            \"\"\"",
            "            *define_common_options* should be a callable taking one argument, which",
            "            will be a argparse.Parser.add_argument-like function.",
            "",
            "            *define_common_options* will be called multiple times, and should call",
            "            the passed function to define common options exactly the same way each time.",
            "",
            "            *suffix_precedence* should be a tuple of the suffixes that will be used.",
            "            It is ordered from lowest precedence to highest precedence:",
            "            An option specified on the parser belonging to index 0 is overridden if the",
            "            same option is specified on any parser with a higher index.",
            "            \"\"\"",
            "            self.define_common_options = define_common_options",
            "            self.suffix_precedence = suffix_precedence",
            "",
            "            # Maps suffixes to sets of target names.",
            "            # E.g. common_options[\"_subcommand\"] = {..., \"log_level\", ...}",
            "            self.common_options = dict()",
            "            # Set of options with the 'append' action.",
            "            self.append_options = set()",
            "            # This is the sentinel object that replaces all default values in parsers",
            "            # below the top-level parser.",
            "            self.default_sentinel = object()",
            "",
            "        def add_common_group(self, parser, suffix, provide_defaults=False):",
            "            \"\"\"",
            "            Add common options to *parser*.",
            "",
            "            *provide_defaults* must only be True exactly once in a parser hierarchy,",
            "            at the top level, and False on all lower levels. The default is chosen",
            "            accordingly.",
            "",
            "            *suffix* indicates the suffix to use internally. It also indicates",
            "            which precedence the *parser* has for common options. See *suffix_precedence*",
            "            of __init__.",
            "            \"\"\"",
            "            assert suffix in self.suffix_precedence",
            "",
            "            def add_argument(*args, **kwargs):",
            "                if 'dest' in kwargs:",
            "                    kwargs.setdefault('action', 'store')",
            "                    assert kwargs['action'] in ('help', 'store_const', 'store_true', 'store_false', 'store', 'append')",
            "                    is_append = kwargs['action'] == 'append'",
            "                    if is_append:",
            "                        self.append_options.add(kwargs['dest'])",
            "                        assert kwargs['default'] == [], 'The default is explicitly constructed as an empty list in resolve()'",
            "                    else:",
            "                        self.common_options.setdefault(suffix, set()).add(kwargs['dest'])",
            "                    kwargs['dest'] += suffix",
            "                    if not provide_defaults:",
            "                        # Interpolate help now, in case the %(default)d (or so) is mentioned,",
            "                        # to avoid producing incorrect help output.",
            "                        # Assumption: Interpolated output can safely be interpolated again,",
            "                        # which should always be the case.",
            "                        # Note: We control all inputs.",
            "                        kwargs['help'] = kwargs['help'] % kwargs",
            "                        if not is_append:",
            "                            kwargs['default'] = self.default_sentinel",
            "",
            "                common_group.add_argument(*args, **kwargs)",
            "",
            "            common_group = parser.add_argument_group('Common options')",
            "            self.define_common_options(add_argument)",
            "",
            "        def resolve(self, args: argparse.Namespace):  # Namespace has \"in\" but otherwise is not like a dict.",
            "            \"\"\"",
            "            Resolve the multiple definitions of each common option to the final value.",
            "            \"\"\"",
            "            for suffix in self.suffix_precedence:",
            "                # From highest level to lowest level, so the \"most-specific\" option wins, e.g.",
            "                # \"borg --debug create --info\" shall result in --info being effective.",
            "                for dest in self.common_options.get(suffix, []):",
            "                    # map_from is this suffix' option name, e.g. log_level_subcommand",
            "                    # map_to is the target name, e.g. log_level",
            "                    map_from = dest + suffix",
            "                    map_to = dest",
            "                    # Retrieve value; depending on the action it may not exist, but usually does",
            "                    # (store_const/store_true/store_false), either because the action implied a default",
            "                    # or a default is explicitly supplied.",
            "                    # Note that defaults on lower levels are replaced with default_sentinel.",
            "                    # Only the top level has defaults.",
            "                    value = getattr(args, map_from, self.default_sentinel)",
            "                    if value is not self.default_sentinel:",
            "                        # value was indeed specified on this level. Transfer value to target,",
            "                        # and un-clobber the args (for tidiness - you *cannot* use the suffixed",
            "                        # names for other purposes, obviously).",
            "                        setattr(args, map_to, value)",
            "                    try:",
            "                        delattr(args, map_from)",
            "                    except AttributeError:",
            "                        pass",
            "",
            "            # Options with an \"append\" action need some special treatment. Instead of",
            "            # overriding values, all specified values are merged together.",
            "            for dest in self.append_options:",
            "                option_value = []",
            "                for suffix in self.suffix_precedence:",
            "                    # Find values of this suffix, if any, and add them to the final list",
            "                    extend_from = dest + suffix",
            "                    if extend_from in args:",
            "                        values = getattr(args, extend_from)",
            "                        delattr(args, extend_from)",
            "                        option_value.extend(values)",
            "                setattr(args, dest, option_value)",
            "",
            "    def build_parser(self):",
            "        # You can use :ref:`xyz` in the following usage pages. However, for plain-text view,",
            "        # e.g. through \"borg ... --help\", define a substitution for the reference here.",
            "        # It will replace the entire :ref:`foo` verbatim.",
            "        rst_plain_text_references = {",
            "            'a_status_oddity': '\"I am seeing \u2018A\u2019 (added) status for a unchanged file!?\"',",
            "        }",
            "",
            "        def process_epilog(epilog):",
            "            epilog = textwrap.dedent(epilog).splitlines()",
            "            try:",
            "                mode = borg.doc_mode",
            "            except AttributeError:",
            "                mode = 'command-line'",
            "            if mode in ('command-line', 'build_usage'):",
            "                epilog = [line for line in epilog if not line.startswith('.. man')]",
            "            epilog = '\\n'.join(epilog)",
            "            if mode == 'command-line':",
            "                epilog = rst_to_terminal(epilog, rst_plain_text_references)",
            "            return epilog",
            "",
            "        def define_common_options(add_common_option):",
            "            add_common_option('-h', '--help', action='help', help='show this help message and exit')",
            "            add_common_option('--critical', dest='log_level',",
            "                              action='store_const', const='critical', default='warning',",
            "                              help='work on log level CRITICAL')",
            "            add_common_option('--error', dest='log_level',",
            "                              action='store_const', const='error', default='warning',",
            "                              help='work on log level ERROR')",
            "            add_common_option('--warning', dest='log_level',",
            "                              action='store_const', const='warning', default='warning',",
            "                              help='work on log level WARNING (default)')",
            "            add_common_option('--info', '-v', '--verbose', dest='log_level',",
            "                              action='store_const', const='info', default='warning',",
            "                              help='work on log level INFO')",
            "            add_common_option('--debug', dest='log_level',",
            "                              action='store_const', const='debug', default='warning',",
            "                              help='enable debug output, work on log level DEBUG')",
            "            add_common_option('--debug-topic', metavar='TOPIC', dest='debug_topics', action='append', default=[],",
            "                              help='enable TOPIC debugging (can be specified multiple times). '",
            "                                   'The logger path is borg.debug.<TOPIC> if TOPIC is not fully qualified.')",
            "            add_common_option('-p', '--progress', dest='progress', action='store_true',",
            "                              help='show progress information')",
            "            add_common_option('--log-json', dest='log_json', action='store_true',",
            "                              help='Output one JSON object per log line instead of formatted text.')",
            "            add_common_option('--lock-wait', metavar='SECONDS', dest='lock_wait', type=int, default=1,",
            "                              help='wait at most SECONDS for acquiring a repository/cache lock (default: %(default)d).')",
            "            add_common_option('--show-version', dest='show_version', action='store_true',",
            "                              help='show/log the borg version')",
            "            add_common_option('--show-rc', dest='show_rc', action='store_true',",
            "                              help='show/log the return code (rc)')",
            "            add_common_option('--umask', metavar='M', dest='umask', type=lambda s: int(s, 8), default=UMASK_DEFAULT,",
            "                              help='set umask to M (local and remote, default: %(default)04o)')",
            "            add_common_option('--remote-path', metavar='PATH', dest='remote_path',",
            "                              help='use PATH as borg executable on the remote (default: \"borg\")')",
            "            add_common_option('--remote-ratelimit', metavar='RATE', dest='remote_ratelimit', type=int,",
            "                              help='set remote network upload rate limit in kiByte/s (default: 0=unlimited)')",
            "            add_common_option('--consider-part-files', dest='consider_part_files', action='store_true',",
            "                              help='treat part files like normal files (e.g. to list/extract them)')",
            "            add_common_option('--debug-profile', metavar='FILE', dest='debug_profile', default=None,",
            "                              help='Write execution profile in Borg format into FILE. For local use a Python-'",
            "                                   'compatible file can be generated by suffixing FILE with \".pyprof\".')",
            "",
            "        def define_exclude_and_patterns(add_option, *, tag_files=False, strip_components=False):",
            "            add_option('-e', '--exclude', metavar='PATTERN', dest='patterns',",
            "                       type=parse_exclude_pattern, action='append',",
            "                       help='exclude paths matching PATTERN')",
            "            add_option('--exclude-from', metavar='EXCLUDEFILE', action=ArgparseExcludeFileAction,",
            "                       help='read exclude patterns from EXCLUDEFILE, one per line')",
            "            add_option('--pattern', metavar='PATTERN', action=ArgparsePatternAction,",
            "                       help='experimental: include/exclude paths matching PATTERN')",
            "            add_option('--patterns-from', metavar='PATTERNFILE', action=ArgparsePatternFileAction,",
            "                       help='experimental: read include/exclude patterns from PATTERNFILE, one per line')",
            "",
            "            if tag_files:",
            "                add_option('--exclude-caches', dest='exclude_caches', action='store_true',",
            "                           help='exclude directories that contain a CACHEDIR.TAG file '",
            "                                '(http://www.brynosaurus.com/cachedir/spec.html)')",
            "                add_option('--exclude-if-present', metavar='NAME', dest='exclude_if_present',",
            "                           action='append', type=str,",
            "                           help='exclude directories that are tagged by containing a filesystem object with '",
            "                                'the given NAME')",
            "                add_option('--keep-exclude-tags', '--keep-tag-files', dest='keep_exclude_tags',",
            "                           action='store_true',",
            "                           help='if tag objects are specified with ``--exclude-if-present``, '",
            "                                'don\\'t omit the tag objects themselves from the backup archive')",
            "",
            "            if strip_components:",
            "                add_option('--strip-components', metavar='NUMBER', dest='strip_components', type=int, default=0,",
            "                           help='Remove the specified number of leading path elements. '",
            "                                'Paths with fewer elements will be silently skipped.')",
            "",
            "        def define_exclusion_group(subparser, **kwargs):",
            "            exclude_group = subparser.add_argument_group('Exclusion options')",
            "            define_exclude_and_patterns(exclude_group.add_argument, **kwargs)",
            "            return exclude_group",
            "",
            "        def define_archive_filters_group(subparser, *, sort_by=True, first_last=True):",
            "            filters_group = subparser.add_argument_group('Archive filters',",
            "                                                         'Archive filters can be applied to repository targets.')",
            "            group = filters_group.add_mutually_exclusive_group()",
            "            group.add_argument('-P', '--prefix', metavar='PREFIX', dest='prefix', type=PrefixSpec, default='',",
            "                               help='only consider archive names starting with this prefix.')",
            "            group.add_argument('-a', '--glob-archives', metavar='GLOB', dest='glob_archives', default=None,",
            "                               help='only consider archive names matching the glob. '",
            "                                    'sh: rules apply, see \"borg help patterns\". '",
            "                                    '``--prefix`` and ``--glob-archives`` are mutually exclusive.')",
            "",
            "            if sort_by:",
            "                sort_by_default = 'timestamp'",
            "                filters_group.add_argument('--sort-by', metavar='KEYS', dest='sort_by',",
            "                                           type=SortBySpec, default=sort_by_default,",
            "                                           help='Comma-separated list of sorting keys; valid keys are: {}; default is: {}'",
            "                                           .format(', '.join(AI_HUMAN_SORT_KEYS), sort_by_default))",
            "",
            "            if first_last:",
            "                group = filters_group.add_mutually_exclusive_group()",
            "                group.add_argument('--first', metavar='N', dest='first', default=0, type=positive_int_validator,",
            "                                   help='consider first N archives after other filters were applied')",
            "                group.add_argument('--last', metavar='N', dest='last', default=0, type=positive_int_validator,",
            "                                   help='consider last N archives after other filters were applied')",
            "",
            "        parser = argparse.ArgumentParser(prog=self.prog, description='Borg - Deduplicated Backups',",
            "                                         add_help=False)",
            "        parser.set_defaults(fallback2_func=functools.partial(self.do_maincommand_help, parser))",
            "        parser.common_options = self.CommonOptions(define_common_options,",
            "                                                   suffix_precedence=('_maincommand', '_midcommand', '_subcommand'))",
            "        parser.add_argument('-V', '--version', action='version', version='%(prog)s ' + __version__,",
            "                            help='show version number and exit')",
            "        parser.common_options.add_common_group(parser, '_maincommand', provide_defaults=True)",
            "",
            "        common_parser = argparse.ArgumentParser(add_help=False, prog=self.prog)",
            "        # some empty defaults for all subparsers",
            "        common_parser.set_defaults(paths=[], patterns=[])",
            "        parser.common_options.add_common_group(common_parser, '_subcommand')",
            "",
            "        mid_common_parser = argparse.ArgumentParser(add_help=False, prog=self.prog)",
            "        mid_common_parser.set_defaults(paths=[], patterns=[])",
            "        parser.common_options.add_common_group(mid_common_parser, '_midcommand')",
            "",
            "        mount_epilog = process_epilog(\"\"\"",
            "        This command mounts an archive as a FUSE filesystem. This can be useful for",
            "        browsing an archive or restoring individual files. Unless the ``--foreground``",
            "        option is given the command will run in the background until the filesystem",
            "        is ``umounted``.",
            "",
            "        The command ``borgfs`` provides a wrapper for ``borg mount``. This can also be",
            "        used in fstab entries:",
            "        ``/path/to/repo /mnt/point fuse.borgfs defaults,noauto 0 0``",
            "",
            "        To allow a regular user to use fstab entries, add the ``user`` option:",
            "        ``/path/to/repo /mnt/point fuse.borgfs defaults,noauto,user 0 0``",
            "",
            "        For mount options, see the fuse(8) manual page. Additional mount options",
            "        supported by borg:",
            "",
            "        - versions: when used with a repository mount, this gives a merged, versioned",
            "          view of the files in the archives. EXPERIMENTAL, layout may change in future.",
            "        - allow_damaged_files: by default damaged files (where missing chunks were",
            "          replaced with runs of zeros by borg check ``--repair``) are not readable and",
            "          return EIO (I/O error). Set this option to read such files.",
            "",
            "        The BORG_MOUNT_DATA_CACHE_ENTRIES environment variable is meant for advanced users",
            "        to tweak the performance. It sets the number of cached data chunks; additional",
            "        memory usage can be up to ~8 MiB times this number. The default is the number",
            "        of CPU cores.",
            "",
            "        When the daemonized process receives a signal or crashes, it does not unmount.",
            "        Unmounting in these cases could cause an active rsync or similar process",
            "        to unintentionally delete data.",
            "",
            "        When running in the foreground ^C/SIGINT unmounts cleanly, but other",
            "        signals or crashes do not.",
            "        \"\"\")",
            "",
            "        if parser.prog == 'borgfs':",
            "            parser.description = self.do_mount.__doc__",
            "            parser.epilog = mount_epilog",
            "            parser.formatter_class = argparse.RawDescriptionHelpFormatter",
            "            parser.help = 'mount repository'",
            "            subparser = parser",
            "        else:",
            "            subparsers = parser.add_subparsers(title='required arguments', metavar='<command>')",
            "            subparser = subparsers.add_parser('mount', parents=[common_parser], add_help=False,",
            "                                            description=self.do_mount.__doc__,",
            "                                            epilog=mount_epilog,",
            "                                            formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                            help='mount repository')",
            "        subparser.set_defaults(func=self.do_mount)",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', type=location_validator(),",
            "                            help='repository/archive to mount')",
            "        subparser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,",
            "                            help='where to mount filesystem')",
            "        subparser.add_argument('-f', '--foreground', dest='foreground',",
            "                            action='store_true',",
            "                            help='stay in foreground, do not daemonize')",
            "        subparser.add_argument('-o', dest='options', type=str,",
            "                            help='Extra mount options')",
            "        define_archive_filters_group(subparser)",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract; patterns are supported')",
            "        define_exclusion_group(subparser, strip_components=True)",
            "        if parser.prog == 'borgfs':",
            "            return parser",
            "",
            "        serve_epilog = process_epilog(\"\"\"",
            "        This command starts a repository server process. This command is usually not used manually.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('serve', parents=[common_parser], add_help=False,",
            "                                          description=self.do_serve.__doc__, epilog=serve_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='start repository server process')",
            "        subparser.set_defaults(func=self.do_serve)",
            "        subparser.add_argument('--restrict-to-path', metavar='PATH', dest='restrict_to_paths', action='append',",
            "                               help='restrict repository access to PATH. '",
            "                                    'Can be specified multiple times to allow the client access to several directories. '",
            "                                    'Access to all sub-directories is granted implicitly; PATH doesn\\'t need to directly point to a repository.')",
            "        subparser.add_argument('--restrict-to-repository', metavar='PATH', dest='restrict_to_repositories', action='append',",
            "                                help='restrict repository access. Only the repository located at PATH '",
            "                                     '(no sub-directories are considered) is accessible. '",
            "                                     'Can be specified multiple times to allow the client access to several repositories. '",
            "                                     'Unlike ``--restrict-to-path`` sub-directories are not accessible; '",
            "                                     'PATH needs to directly point at a repository location. '",
            "                                     'PATH may be an empty directory or the last element of PATH may not exist, in which case '",
            "                                     'the client may initialize a repository there.')",
            "        subparser.add_argument('--append-only', dest='append_only', action='store_true',",
            "                               help='only allow appending to repository segment files')",
            "        subparser.add_argument('--storage-quota', metavar='QUOTA', dest='storage_quota',",
            "                               type=parse_storage_quota, default=None,",
            "                               help='Override storage quota of the repository (e.g. 5G, 1.5T). '",
            "                                    'When a new repository is initialized, sets the storage quota on the new '",
            "                                    'repository as well. Default: no quota.')",
            "",
            "        init_epilog = process_epilog(\"\"\"",
            "        This command initializes an empty repository. A repository is a filesystem",
            "        directory containing the deduplicated data from zero or more archives.",
            "",
            "        Encryption can be enabled at repository init time. It cannot be changed later.",
            "",
            "        It is not recommended to work without encryption. Repository encryption protects",
            "        you e.g. against the case that an attacker has access to your backup repository.",
            "",
            "        But be careful with the key / the passphrase:",
            "",
            "        If you want \"passphrase-only\" security, use one of the repokey modes. The",
            "        key will be stored inside the repository (in its \"config\" file). In above",
            "        mentioned attack scenario, the attacker will have the key (but not the",
            "        passphrase).",
            "",
            "        If you want \"passphrase and having-the-key\" security, use one of the keyfile",
            "        modes. The key will be stored in your home directory (in .config/borg/keys).",
            "        In the attack scenario, the attacker who has just access to your repo won't",
            "        have the key (and also not the passphrase).",
            "",
            "        Make a backup copy of the key file (keyfile mode) or repo config file",
            "        (repokey mode) and keep it at a safe place, so you still have the key in",
            "        case it gets corrupted or lost. Also keep the passphrase at a safe place.",
            "        The backup that is encrypted with that key won't help you with that, of course.",
            "",
            "        Make sure you use a good passphrase. Not too short, not too simple. The real",
            "        encryption / decryption key is encrypted with / locked by your passphrase.",
            "        If an attacker gets your key, he can't unlock and use it without knowing the",
            "        passphrase.",
            "",
            "        Be careful with special or non-ascii characters in your passphrase:",
            "",
            "        - Borg processes the passphrase as unicode (and encodes it as utf-8),",
            "          so it does not have problems dealing with even the strangest characters.",
            "        - BUT: that does not necessarily apply to your OS / VM / keyboard configuration.",
            "",
            "        So better use a long passphrase made from simple ascii chars than one that",
            "        includes non-ascii stuff or characters that are hard/impossible to enter on",
            "        a different keyboard layout.",
            "",
            "        You can change your passphrase for existing repos at any time, it won't affect",
            "        the encryption/decryption key or other secrets.",
            "",
            "        Encryption modes",
            "        ++++++++++++++++",
            "",
            "        .. nanorst: inline-fill",
            "",
            "        +----------+---------------+------------------------+--------------------------+",
            "        | Hash/MAC | Not encrypted | Not encrypted,         | Encrypted (AEAD w/ AES)  |",
            "        |          | no auth       | but authenticated      | and authenticated        |",
            "        +----------+---------------+------------------------+--------------------------+",
            "        | SHA-256  | none          | `authenticated`        | repokey                  |",
            "        |          |               |                        | keyfile                  |",
            "        +----------+---------------+------------------------+--------------------------+",
            "        | BLAKE2b  | n/a           | `authenticated-blake2` | `repokey-blake2`         |",
            "        |          |               |                        | `keyfile-blake2`         |",
            "        +----------+---------------+------------------------+--------------------------+",
            "",
            "        .. nanorst: inline-replace",
            "",
            "        `Marked modes` are new in Borg 1.1 and are not backwards-compatible with Borg 1.0.x.",
            "",
            "        On modern Intel/AMD CPUs (except very cheap ones), AES is usually",
            "        hardware-accelerated.",
            "        BLAKE2b is faster than SHA256 on Intel/AMD 64-bit CPUs",
            "        (except AMD Ryzen and future CPUs with SHA extensions),",
            "        which makes `authenticated-blake2` faster than `none` and `authenticated`.",
            "",
            "        On modern ARM CPUs, NEON provides hardware acceleration for SHA256 making it faster",
            "        than BLAKE2b-256 there. NEON accelerates AES as well.",
            "",
            "        Hardware acceleration is always used automatically when available.",
            "",
            "        `repokey` and `keyfile` use AES-CTR-256 for encryption and HMAC-SHA256 for",
            "        authentication in an encrypt-then-MAC (EtM) construction. The chunk ID hash",
            "        is HMAC-SHA256 as well (with a separate key).",
            "        These modes are compatible with Borg 1.0.x.",
            "",
            "        `repokey-blake2` and `keyfile-blake2` are also authenticated encryption modes,",
            "        but use BLAKE2b-256 instead of HMAC-SHA256 for authentication. The chunk ID",
            "        hash is a keyed BLAKE2b-256 hash.",
            "        These modes are new and *not* compatible with Borg 1.0.x.",
            "",
            "        `authenticated` mode uses no encryption, but authenticates repository contents",
            "        through the same HMAC-SHA256 hash as the `repokey` and `keyfile` modes (it uses it",
            "        as the chunk ID hash). The key is stored like `repokey`.",
            "        This mode is new and *not* compatible with Borg 1.0.x.",
            "",
            "        `authenticated-blake2` is like `authenticated`, but uses the keyed BLAKE2b-256 hash",
            "        from the other blake2 modes.",
            "        This mode is new and *not* compatible with Borg 1.0.x.",
            "",
            "        `none` mode uses no encryption and no authentication. It uses SHA256 as chunk",
            "        ID hash. Not recommended, rather consider using an authenticated or",
            "        authenticated/encrypted mode. This mode has possible denial-of-service issues",
            "        when running ``borg create`` on contents controlled by an attacker.",
            "        Use it only for new repositories where no encryption is wanted **and** when compatibility",
            "        with 1.0.x is important. If compatibility with 1.0.x is not important, use",
            "        `authenticated-blake2` or `authenticated` instead.",
            "        This mode is compatible with Borg 1.0.x.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('init', parents=[common_parser], add_help=False,",
            "                                          description=self.do_init.__doc__, epilog=init_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='initialize empty repository')",
            "        subparser.set_defaults(func=self.do_init)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to create')",
            "        subparser.add_argument('-e', '--encryption', metavar='MODE', dest='encryption', required=True,",
            "                               choices=key_argument_names(),",
            "                               help='select encryption key mode **(required)**')",
            "        subparser.add_argument('--append-only', dest='append_only', action='store_true',",
            "                               help='create an append-only mode repository')",
            "        subparser.add_argument('--storage-quota', metavar='QUOTA', dest='storage_quota', default=None,",
            "                               type=parse_storage_quota,",
            "                               help='Set storage quota of the new repository (e.g. 5G, 1.5T). Default: no quota.')",
            "",
            "        check_epilog = process_epilog(\"\"\"",
            "        The check command verifies the consistency of a repository and the corresponding archives.",
            "",
            "        First, the underlying repository data files are checked:",
            "",
            "        - For all segments the segment magic (header) is checked",
            "        - For all objects stored in the segments, all metadata (e.g. crc and size) and",
            "          all data is read. The read data is checked by size and CRC. Bit rot and other",
            "          types of accidental damage can be detected this way.",
            "        - If we are in repair mode and a integrity error is detected for a segment,",
            "          we try to recover as many objects from the segment as possible.",
            "        - In repair mode, it makes sure that the index is consistent with the data",
            "          stored in the segments.",
            "        - If you use a remote repo server via ssh:, the repo check is executed on the",
            "          repo server without causing significant network traffic.",
            "        - The repository check can be skipped using the ``--archives-only`` option.",
            "",
            "        Second, the consistency and correctness of the archive metadata is verified:",
            "",
            "        - Is the repo manifest present? If not, it is rebuilt from archive metadata",
            "          chunks (this requires reading and decrypting of all metadata and data).",
            "        - Check if archive metadata chunk is present. if not, remove archive from",
            "          manifest.",
            "        - For all files (items) in the archive, for all chunks referenced by these",
            "          files, check if chunk is present.",
            "          If a chunk is not present and we are in repair mode, replace it with a same-size",
            "          replacement chunk of zeros.",
            "          If a previously lost chunk reappears (e.g. via a later backup) and we are in",
            "          repair mode, the all-zero replacement chunk will be replaced by the correct chunk.",
            "          This requires reading of archive and file metadata, but not data.",
            "        - If we are in repair mode and we checked all the archives: delete orphaned",
            "          chunks from the repo.",
            "        - if you use a remote repo server via ssh:, the archive check is executed on",
            "          the client machine (because if encryption is enabled, the checks will require",
            "          decryption and this is always done client-side, because key access will be",
            "          required).",
            "        - The archive checks can be time consuming, they can be skipped using the",
            "          ``--repository-only`` option.",
            "",
            "        The ``--verify-data`` option will perform a full integrity verification (as opposed to",
            "        checking the CRC32 of the segment) of data, which means reading the data from the",
            "        repository, decrypting and decompressing it. This is a cryptographic verification,",
            "        which will detect (accidental) corruption. For encrypted repositories it is",
            "        tamper-resistant as well, unless the attacker has access to the keys.",
            "",
            "        It is also very slow.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('check', parents=[common_parser], add_help=False,",
            "                                          description=self.do_check.__doc__,",
            "                                          epilog=check_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='verify repository')",
            "        subparser.set_defaults(func=self.do_check)",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='repository or archive to check consistency of')",
            "        subparser.add_argument('--repository-only', dest='repo_only', action='store_true',",
            "                               help='only perform repository checks')",
            "        subparser.add_argument('--archives-only', dest='archives_only', action='store_true',",
            "                               help='only perform archives checks')",
            "        subparser.add_argument('--verify-data', dest='verify_data', action='store_true',",
            "                               help='perform cryptographic archive data integrity verification '",
            "                                    '(conflicts with ``--repository-only``)')",
            "        subparser.add_argument('--repair', dest='repair', action='store_true',",
            "                               help='attempt to repair any inconsistencies found')",
            "        subparser.add_argument('--save-space', dest='save_space', action='store_true',",
            "                               help='work slower, but using less space')",
            "        define_archive_filters_group(subparser)",
            "",
            "        subparser = subparsers.add_parser('key', parents=[mid_common_parser], add_help=False,",
            "                                          description=\"Manage a keyfile or repokey of a repository\",",
            "                                          epilog=\"\",",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='manage repository key')",
            "",
            "        key_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')",
            "        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))",
            "",
            "        key_export_epilog = process_epilog(\"\"\"",
            "        If repository encryption is used, the repository is inaccessible",
            "        without the key. This command allows to backup this essential key.",
            "        Note that the backup produced does not include the passphrase itself",
            "        (i.e. the exported key stays encrypted). In order to regain access to a",
            "        repository, one needs both the exported key and the original passphrase.",
            "",
            "        There are three backup formats. The normal backup format is suitable for",
            "        digital storage as a file. The ``--paper`` backup format is optimized",
            "        for printing and typing in while importing, with per line checks to",
            "        reduce problems with manual input. The ``--qr-html`` creates a printable",
            "        HTML template with a QR code and a copy of the ``--paper``-formatted key.",
            "",
            "        For repositories using keyfile encryption the key is saved locally",
            "        on the system that is capable of doing backups. To guard against loss",
            "        of this key, the key needs to be backed up independently of the main",
            "        data backup.",
            "",
            "        For repositories using the repokey encryption the key is saved in the",
            "        repository in the config file. A backup is thus not strictly needed,",
            "        but guards against the repository becoming inaccessible if the file",
            "        is damaged for some reason.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('export', parents=[common_parser], add_help=False,",
            "                                          description=self.do_key_export.__doc__,",
            "                                          epilog=key_export_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='export repository key for backup')",
            "        subparser.set_defaults(func=self.do_key_export)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "        subparser.add_argument('path', metavar='PATH', nargs='?', type=str,",
            "                               help='where to store the backup')",
            "        subparser.add_argument('--paper', dest='paper', action='store_true',",
            "                               help='Create an export suitable for printing and later type-in')",
            "        subparser.add_argument('--qr-html', dest='qr', action='store_true',",
            "                               help='Create an html file suitable for printing and later type-in or qr scan')",
            "",
            "        key_import_epilog = process_epilog(\"\"\"",
            "        This command allows to restore a key previously backed up with the",
            "        export command.",
            "",
            "        If the ``--paper`` option is given, the import will be an interactive",
            "        process in which each line is checked for plausibility before",
            "        proceeding to the next line. For this format PATH must not be given.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('import', parents=[common_parser], add_help=False,",
            "                                          description=self.do_key_import.__doc__,",
            "                                          epilog=key_import_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='import repository key from backup')",
            "        subparser.set_defaults(func=self.do_key_import)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "        subparser.add_argument('path', metavar='PATH', nargs='?', type=str,",
            "                               help='path to the backup (\\'-\\' to read from stdin)')",
            "        subparser.add_argument('--paper', dest='paper', action='store_true',",
            "                               help='interactively import from a backup done with ``--paper``')",
            "",
            "        change_passphrase_epilog = process_epilog(\"\"\"",
            "        The key files used for repository encryption are optionally passphrase",
            "        protected. This command can be used to change this passphrase.",
            "",
            "        Please note that this command only changes the passphrase, but not any",
            "        secret protected by it (like e.g. encryption/MAC keys or chunker seed).",
            "        Thus, changing the passphrase after passphrase and borg key got compromised",
            "        does not protect future (nor past) backups to the same repository.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('change-passphrase', parents=[common_parser], add_help=False,",
            "                                          description=self.do_change_passphrase.__doc__,",
            "                                          epilog=change_passphrase_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='change repository passphrase')",
            "        subparser.set_defaults(func=self.do_change_passphrase)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "",
            "        # Borg 1.0 alias for change passphrase (without the \"key\" subcommand)",
            "        subparser = subparsers.add_parser('change-passphrase', parents=[common_parser], add_help=False,",
            "                                          description=self.do_change_passphrase.__doc__,",
            "                                          epilog=change_passphrase_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='change repository passphrase')",
            "        subparser.set_defaults(func=self.do_change_passphrase_deprecated)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "",
            "        migrate_to_repokey_epilog = process_epilog(\"\"\"",
            "        This command migrates a repository from passphrase mode (removed in Borg 1.0)",
            "        to repokey mode.",
            "",
            "        You will be first asked for the repository passphrase (to open it in passphrase",
            "        mode). This is the same passphrase as you used to use for this repo before 1.0.",
            "",
            "        It will then derive the different secrets from this passphrase.",
            "",
            "        Then you will be asked for a new passphrase (twice, for safety). This",
            "        passphrase will be used to protect the repokey (which contains these same",
            "        secrets in encrypted form). You may use the same passphrase as you used to",
            "        use, but you may also use a different one.",
            "",
            "        After migrating to repokey mode, you can change the passphrase at any time.",
            "        But please note: the secrets will always stay the same and they could always",
            "        be derived from your (old) passphrase-mode passphrase.",
            "        \"\"\")",
            "        subparser = key_parsers.add_parser('migrate-to-repokey', parents=[common_parser], add_help=False,",
            "                                          description=self.do_migrate_to_repokey.__doc__,",
            "                                          epilog=migrate_to_repokey_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='migrate passphrase-mode repository to repokey')",
            "        subparser.set_defaults(func=self.do_migrate_to_repokey)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False))",
            "",
            "        create_epilog = process_epilog(\"\"\"",
            "        This command creates a backup archive containing all files found while recursively",
            "        traversing all paths specified. Paths are added to the archive as they are given,",
            "        that means if relative paths are desired, the command has to be run from the correct",
            "        directory.",
            "",
            "        When giving '-' as path, borg will read data from standard input and create a",
            "        file 'stdin' in the created archive from that data.",
            "",
            "        The archive will consume almost no disk space for files or parts of files that",
            "        have already been stored in other archives.",
            "",
            "        The archive name needs to be unique. It must not end in '.checkpoint' or",
            "        '.checkpoint.N' (with N being a number), because these names are used for",
            "        checkpoints and treated in special ways.",
            "",
            "        In the archive name, you may use the following placeholders:",
            "        {now}, {utcnow}, {fqdn}, {hostname}, {user} and some others.",
            "",
            "        Backup speed is increased by not reprocessing files that are already part of",
            "        existing archives and weren't modified. The detection of unmodified files is",
            "        done by comparing multiple file metadata values with previous values kept in",
            "        the files cache.",
            "",
            "        This comparison can operate in different modes as given by ``--files-cache``:",
            "",
            "        - ctime,size,inode (default)",
            "        - mtime,size,inode (default behaviour of borg versions older than 1.1.0rc4)",
            "        - ctime,size (ignore the inode number)",
            "        - mtime,size (ignore the inode number)",
            "        - rechunk,ctime (all files are considered modified - rechunk, cache ctime)",
            "        - rechunk,mtime (all files are considered modified - rechunk, cache mtime)",
            "        - disabled (disable the files cache, all files considered modified - rechunk)",
            "",
            "        inode number: better safety, but often unstable on network filesystems",
            "",
            "        Normally, detecting file modifications will take inode information into",
            "        consideration to improve the reliability of file change detection.",
            "        This is problematic for files located on sshfs and similar network file",
            "        systems which do not provide stable inode numbers, such files will always",
            "        be considered modified. You can use modes without `inode` in this case to",
            "        improve performance, but reliability of change detection might be reduced.",
            "",
            "        ctime vs. mtime: safety vs. speed",
            "",
            "        - ctime is a rather safe way to detect changes to a file (metadata and contents)",
            "          as it can not be set from userspace. But, a metadata-only change will already",
            "          update the ctime, so there might be some unnecessary chunking/hashing even",
            "          without content changes. Some filesystems do not support ctime (change time).",
            "        - mtime usually works and only updates if file contents were changed. But mtime",
            "          can be arbitrarily set from userspace, e.g. to set mtime back to the same value",
            "          it had before a content change happened. This can be used maliciously as well as",
            "          well-meant, but in both cases mtime based cache modes can be problematic.",
            "",
            "        The mount points of filesystems or filesystem snapshots should be the same for every",
            "        creation of a new archive to ensure fast operation. This is because the file cache that",
            "        is used to determine changed files quickly uses absolute filenames.",
            "        If this is not possible, consider creating a bind mount to a stable location.",
            "",
            "        The ``--progress`` option shows (from left to right) Original, Compressed and Deduplicated",
            "        (O, C and D, respectively), then the Number of files (N) processed so far, followed by",
            "        the currently processed path.",
            "",
            "        When using ``--stats``, you will get some statistics about how much data was",
            "        added - the \"This Archive\" deduplicated size there is most interesting as that is",
            "        how much your repository will grow. Please note that the \"All archives\" stats refer to",
            "        the state after creation. Also, the ``--stats`` and ``--dry-run`` options are mutually",
            "        exclusive because the data is not actually compressed and deduplicated during a dry run.",
            "",
            "        See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "        See the output of the \"borg help placeholders\" command for more help on placeholders.",
            "",
            "        .. man NOTES",
            "",
            "        The ``--exclude`` patterns are not like tar. In tar ``--exclude`` .bundler/gems will",
            "        exclude foo/.bundler/gems. In borg it will not, you need to use ``--exclude``",
            "        '\\*/.bundler/gems' to get the same effect. See ``borg help patterns`` for",
            "        more information.",
            "",
            "        In addition to using ``--exclude`` patterns, it is possible to use",
            "        ``--exclude-if-present`` to specify the name of a filesystem object (e.g. a file",
            "        or folder name) which, when contained within another folder, will prevent the",
            "        containing folder from being backed up.  By default, the containing folder and",
            "        all of its contents will be omitted from the backup.  If, however, you wish to",
            "        only include the objects specified by ``--exclude-if-present`` in your backup,",
            "        and not include any other contents of the containing folder, this can be enabled",
            "        through using the ``--keep-exclude-tags`` option.",
            "",
            "        Item flags",
            "        ++++++++++",
            "",
            "        ``--list`` outputs a list of all files, directories and other",
            "        file system items it considered (no matter whether they had content changes",
            "        or not). For each item, it prefixes a single-letter flag that indicates type",
            "        and/or status of the item.",
            "",
            "        If you are interested only in a subset of that output, you can give e.g.",
            "        ``--filter=AME`` and it will only show regular files with A, M or E status (see",
            "        below).",
            "",
            "        A uppercase character represents the status of a regular file relative to the",
            "        \"files\" cache (not relative to the repo -- this is an issue if the files cache",
            "        is not used). Metadata is stored in any case and for 'A' and 'M' also new data",
            "        chunks are stored. For 'U' all data chunks refer to already existing chunks.",
            "",
            "        - 'A' = regular file, added (see also :ref:`a_status_oddity` in the FAQ)",
            "        - 'M' = regular file, modified",
            "        - 'U' = regular file, unchanged",
            "        - 'E' = regular file, an error happened while accessing/reading *this* file",
            "",
            "        A lowercase character means a file type other than a regular file,",
            "        borg usually just stores their metadata:",
            "",
            "        - 'd' = directory",
            "        - 'b' = block device",
            "        - 'c' = char device",
            "        - 'h' = regular file, hardlink (to already seen inodes)",
            "        - 's' = symlink",
            "        - 'f' = fifo",
            "",
            "        Other flags used include:",
            "",
            "        - 'i' = backup data was read from standard input (stdin)",
            "        - '-' = dry run, item was *not* backed up",
            "        - 'x' = excluded, item was *not* backed up",
            "        - '?' = missing status code (if you see this, please file a bug report!)",
            "        \"\"\")",
            "",
            "        subparser = subparsers.add_parser('create', parents=[common_parser], add_help=False,",
            "                                          description=self.do_create.__doc__,",
            "                                          epilog=create_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='create backup')",
            "        subparser.set_defaults(func=self.do_create)",
            "",
            "        dryrun_group = subparser.add_mutually_exclusive_group()",
            "        dryrun_group.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not create a backup archive')",
            "        dryrun_group.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics for the created archive')",
            "",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('--filter', metavar='STATUSCHARS', dest='output_filter',",
            "                               help='only display items with the given status characters (see description)')",
            "        subparser.add_argument('--json', action='store_true',",
            "                               help='output stats as JSON. Implies ``--stats``.')",
            "        subparser.add_argument('--no-cache-sync', dest='no_cache_sync', action='store_true',",
            "                               help='experimental: do not synchronize the cache. Implies not using the files cache.')",
            "        subparser.add_argument('--no-files-cache', dest='cache_files', action='store_false',",
            "                               help='do not load/update the file metadata cache used to detect unchanged files')",
            "",
            "        exclude_group = define_exclusion_group(subparser, tag_files=True)",
            "        exclude_group.add_argument('--exclude-nodump', dest='exclude_nodump', action='store_true',",
            "                                   help='exclude files flagged NODUMP')",
            "",
            "        fs_group = subparser.add_argument_group('Filesystem options')",
            "        fs_group.add_argument('-x', '--one-file-system', dest='one_file_system', action='store_true',",
            "                              help='stay in the same file system and do not store mount points of other file systems')",
            "        fs_group.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',",
            "                              help='only store numeric user and group identifiers')",
            "        fs_group.add_argument('--noatime', dest='noatime', action='store_true',",
            "                              help='do not store atime into archive')",
            "        fs_group.add_argument('--noctime', dest='noctime', action='store_true',",
            "                              help='do not store ctime into archive')",
            "        fs_group.add_argument('--nobirthtime', dest='nobirthtime', action='store_true',",
            "                              help='do not store birthtime (creation date) into archive')",
            "        fs_group.add_argument('--nobsdflags', dest='nobsdflags', action='store_true',",
            "                              help='do not read and store bsdflags (e.g. NODUMP, IMMUTABLE) into archive')",
            "        fs_group.add_argument('--ignore-inode', dest='ignore_inode', action='store_true',",
            "                              help='ignore inode data in the file metadata cache used to detect unchanged files.')",
            "        fs_group.add_argument('--files-cache', metavar='MODE', dest='files_cache_mode',",
            "                              type=FilesCacheMode, default=DEFAULT_FILES_CACHE_MODE_UI,",
            "                              help='operate files cache in MODE. default: %s' % DEFAULT_FILES_CACHE_MODE_UI)",
            "        fs_group.add_argument('--read-special', dest='read_special', action='store_true',",
            "                              help='open and read block and char device files as well as FIFOs as if they were '",
            "                                   'regular files. Also follows symlinks pointing to these kinds of files.')",
            "",
            "        archive_group = subparser.add_argument_group('Archive options')",
            "        archive_group.add_argument('--comment', dest='comment', metavar='COMMENT', default='',",
            "                                   help='add a comment text to the archive')",
            "        archive_group.add_argument('--timestamp', metavar='TIMESTAMP', dest='timestamp',",
            "                                   type=timestamp, default=None,",
            "                                   help='manually specify the archive creation date/time (UTC, yyyy-mm-ddThh:mm:ss format). '",
            "                                        'Alternatively, give a reference file/directory.')",
            "        archive_group.add_argument('-c', '--checkpoint-interval', metavar='SECONDS', dest='checkpoint_interval',",
            "                                   type=int, default=1800,",
            "                                   help='write checkpoint every SECONDS seconds (Default: 1800)')",
            "        archive_group.add_argument('--chunker-params', metavar='PARAMS', dest='chunker_params',",
            "                                   type=ChunkerParams, default=CHUNKER_PARAMS,",
            "                                   help='specify the chunker parameters (CHUNK_MIN_EXP, CHUNK_MAX_EXP, '",
            "                                        'HASH_MASK_BITS, HASH_WINDOW_SIZE). default: %d,%d,%d,%d' % CHUNKER_PARAMS)",
            "        archive_group.add_argument('-C', '--compression', metavar='COMPRESSION', dest='compression',",
            "                                   type=CompressionSpec, default=CompressionSpec('lz4'),",
            "                                   help='select compression algorithm, see the output of the '",
            "                                        '\"borg help compression\" command for details.')",
            "",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='name of archive to create (must be also a valid directory name)')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to archive')",
            "",
            "        extract_epilog = process_epilog(\"\"\"",
            "        This command extracts the contents of an archive. By default the entire",
            "        archive is extracted but a subset of files and directories can be selected",
            "        by passing a list of ``PATHs`` as arguments. The file selection can further",
            "        be restricted by using the ``--exclude`` option.",
            "",
            "        See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "",
            "        By using ``--dry-run``, you can do all extraction steps except actually writing the",
            "        output data: reading metadata and data chunks from the repo, checking the hash/hmac,",
            "        decrypting, decompressing.",
            "",
            "        ``--progress`` can be slower than no progress display, since it makes one additional",
            "        pass over the archive metadata.",
            "",
            "        .. note::",
            "",
            "            Currently, extract always writes into the current working directory (\".\"),",
            "            so make sure you ``cd`` to the right place before calling ``borg extract``.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('extract', parents=[common_parser], add_help=False,",
            "                                          description=self.do_extract.__doc__,",
            "                                          epilog=extract_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='extract archive contents')",
            "        subparser.set_defaults(func=self.do_extract)",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not actually change any files')",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',",
            "                               help='only obey numeric user and group identifiers')",
            "        subparser.add_argument('--nobsdflags', dest='nobsdflags', action='store_true',",
            "                               help='do not extract/set bsdflags (e.g. NODUMP, IMMUTABLE)')",
            "        subparser.add_argument('--stdout', dest='stdout', action='store_true',",
            "                               help='write all extracted data to stdout')",
            "        subparser.add_argument('--sparse', dest='sparse', action='store_true',",
            "                               help='create holes in output sparse file from all-zero chunks')",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to extract')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract; patterns are supported')",
            "        define_exclusion_group(subparser, strip_components=True)",
            "",
            "        export_tar_epilog = process_epilog(\"\"\"",
            "        This command creates a tarball from an archive.",
            "",
            "        When giving '-' as the output FILE, Borg will write a tar stream to standard output.",
            "",
            "        By default (``--tar-filter=auto``) Borg will detect whether the FILE should be compressed",
            "        based on its file extension and pipe the tarball through an appropriate filter",
            "        before writing it to FILE:",
            "",
            "        - .tar.gz: gzip",
            "        - .tar.bz2: bzip2",
            "        - .tar.xz: xz",
            "",
            "        Alternatively a ``--tar-filter`` program may be explicitly specified. It should",
            "        read the uncompressed tar stream from stdin and write a compressed/filtered",
            "        tar stream to stdout.",
            "",
            "        The generated tarball uses the GNU tar format.",
            "",
            "        export-tar is a lossy conversion:",
            "        BSD flags, ACLs, extended attributes (xattrs), atime and ctime are not exported.",
            "        Timestamp resolution is limited to whole seconds, not the nanosecond resolution",
            "        otherwise supported by Borg.",
            "",
            "        A ``--sparse`` option (as found in borg extract) is not supported.",
            "",
            "        By default the entire archive is extracted but a subset of files and directories",
            "        can be selected by passing a list of ``PATHs`` as arguments.",
            "        The file selection can further be restricted by using the ``--exclude`` option.",
            "",
            "        See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "",
            "        ``--progress`` can be slower than no progress display, since it makes one additional",
            "        pass over the archive metadata.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('export-tar', parents=[common_parser], add_help=False,",
            "                                          description=self.do_export_tar.__doc__,",
            "                                          epilog=export_tar_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='create tarball from archive')",
            "        subparser.set_defaults(func=self.do_export_tar)",
            "        subparser.add_argument('--tar-filter', dest='tar_filter', default='auto',",
            "                               help='filter program to pipe data through')",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to export')",
            "        subparser.add_argument('tarfile', metavar='FILE',",
            "                               help='output tar file. \"-\" to write to stdout instead.')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to extract; patterns are supported')",
            "        define_exclusion_group(subparser, strip_components=True)",
            "",
            "        diff_epilog = process_epilog(\"\"\"",
            "            This command finds differences (file contents, user/group/mode) between archives.",
            "",
            "            A repository location and an archive name must be specified for REPO_ARCHIVE1.",
            "            ARCHIVE2 is just another archive name in same repository (no repository location",
            "            allowed).",
            "",
            "            For archives created with Borg 1.1 or newer diff automatically detects whether",
            "            the archives are created with the same chunker params. If so, only chunk IDs",
            "            are compared, which is very fast.",
            "",
            "            For archives prior to Borg 1.1 chunk contents are compared by default.",
            "            If you did not create the archives with different chunker params,",
            "            pass ``--same-chunker-params``.",
            "            Note that the chunker params changed from Borg 0.xx to 1.0.",
            "",
            "            See the output of the \"borg help patterns\" command for more help on exclude patterns.",
            "            \"\"\")",
            "        subparser = subparsers.add_parser('diff', parents=[common_parser], add_help=False,",
            "                                          description=self.do_diff.__doc__,",
            "                                          epilog=diff_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='find differences in archive contents')",
            "        subparser.set_defaults(func=self.do_diff)",
            "        subparser.add_argument('--numeric-owner', dest='numeric_owner', action='store_true',",
            "                               help='only consider numeric user and group identifiers')",
            "        subparser.add_argument('--same-chunker-params', dest='same_chunker_params', action='store_true',",
            "                               help='Override check of chunker parameters.')",
            "        subparser.add_argument('--sort', dest='sort', action='store_true',",
            "                               help='Sort the output lines by file path.')",
            "        subparser.add_argument('location', metavar='REPO_ARCHIVE1',",
            "                               type=location_validator(archive=True),",
            "                               help='repository location and ARCHIVE1 name')",
            "        subparser.add_argument('archive2', metavar='ARCHIVE2',",
            "                               type=archivename_validator(),",
            "                               help='ARCHIVE2 name (no repository location allowed)')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths of items inside the archives to compare; patterns are supported')",
            "        define_exclusion_group(subparser)",
            "",
            "        rename_epilog = process_epilog(\"\"\"",
            "        This command renames an archive in the repository.",
            "",
            "        This results in a different archive ID.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('rename', parents=[common_parser], add_help=False,",
            "                                          description=self.do_rename.__doc__,",
            "                                          epilog=rename_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='rename archive')",
            "        subparser.set_defaults(func=self.do_rename)",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to rename')",
            "        subparser.add_argument('name', metavar='NEWNAME',",
            "                               type=archivename_validator(),",
            "                               help='the new archive name to use')",
            "",
            "        delete_epilog = process_epilog(\"\"\"",
            "        This command deletes an archive from the repository or the complete repository.",
            "        Disk space is reclaimed accordingly. If you delete the complete repository, the",
            "        local cache for it (if any) is also deleted.",
            "",
            "        When using ``--stats``, you will get some statistics about how much data was",
            "        deleted - the \"Deleted data\" deduplicated size there is most interesting as",
            "        that is how much your repository will shrink.",
            "        Please note that the \"All archives\" stats refer to the state after deletion.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('delete', parents=[common_parser], add_help=False,",
            "                                          description=self.do_delete.__doc__,",
            "                                          epilog=delete_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='delete archive')",
            "        subparser.set_defaults(func=self.do_delete)",
            "        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('--cache-only', dest='cache_only', action='store_true',",
            "                               help='delete only the local cache for the given repository')",
            "        subparser.add_argument('--force', dest='forced',",
            "                               action='count', default=0,",
            "                               help='force deletion of corrupted archives, '",
            "                                    'use ``--force --force`` in case ``--force`` does not work.')",
            "        subparser.add_argument('--save-space', dest='save_space', action='store_true',",
            "                               help='work slower, but using less space')",
            "        subparser.add_argument('location', metavar='TARGET', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='archive or repository to delete')",
            "        subparser.add_argument('archives', metavar='ARCHIVE', nargs='*',",
            "                               help='archives to delete')",
            "        define_archive_filters_group(subparser)",
            "",
            "        list_epilog = process_epilog(\"\"\"",
            "        This command lists the contents of a repository or an archive.",
            "",
            "        See the \"borg help patterns\" command for more help on exclude patterns.",
            "",
            "        .. man NOTES",
            "",
            "        The following keys are available for ``--format``:",
            "",
            "",
            "        \"\"\") + BaseFormatter.keys_help() + textwrap.dedent(\"\"\"",
            "",
            "        Keys for listing repository archives:",
            "",
            "        \"\"\") + ArchiveFormatter.keys_help() + textwrap.dedent(\"\"\"",
            "",
            "        Keys for listing archive files:",
            "",
            "        \"\"\") + ItemFormatter.keys_help()",
            "        subparser = subparsers.add_parser('list', parents=[common_parser], add_help=False,",
            "                                          description=self.do_list.__doc__,",
            "                                          epilog=list_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='list archive or repository contents')",
            "        subparser.set_defaults(func=self.do_list)",
            "        subparser.add_argument('--short', dest='short', action='store_true',",
            "                               help='only print file/directory names, nothing else')",
            "        subparser.add_argument('--format', '--list-format', metavar='FORMAT', dest='format',",
            "                               help='specify format for file listing '",
            "                                    '(default: \"{mode} {user:6} {group:6} {size:8d} {mtime} {path}{extra}{NL}\")')",
            "        subparser.add_argument('--json', action='store_true',",
            "                               help='Only valid for listing repository contents. Format output as JSON. '",
            "                                    'The form of ``--format`` is ignored, '",
            "                                    'but keys used in it are added to the JSON output. '",
            "                                    'Some keys are always present. Note: JSON can only represent text. '",
            "                                    'A \"barchive\" key is therefore not available.')",
            "        subparser.add_argument('--json-lines', action='store_true',",
            "                               help='Only valid for listing archive contents. Format output as JSON Lines. '",
            "                                    'The form of ``--format`` is ignored, '",
            "                                    'but keys used in it are added to the JSON output. '",
            "                                    'Some keys are always present. Note: JSON can only represent text. '",
            "                                    'A \"bpath\" key is therefore not available.')",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='repository/archive to list contents of')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to list; patterns are supported')",
            "        define_archive_filters_group(subparser)",
            "        define_exclusion_group(subparser)",
            "",
            "        umount_epilog = process_epilog(\"\"\"",
            "        This command un-mounts a FUSE filesystem that was mounted with ``borg mount``.",
            "",
            "        This is a convenience wrapper that just calls the platform-specific shell",
            "        command - usually this is either umount or fusermount -u.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('umount', parents=[common_parser], add_help=False,",
            "                                          description=self.do_umount.__doc__,",
            "                                          epilog=umount_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='umount repository')",
            "        subparser.set_defaults(func=self.do_umount)",
            "        subparser.add_argument('mountpoint', metavar='MOUNTPOINT', type=str,",
            "                               help='mountpoint of the filesystem to umount')",
            "",
            "        info_epilog = process_epilog(\"\"\"",
            "        This command displays detailed information about the specified archive or repository.",
            "",
            "        Please note that the deduplicated sizes of the individual archives do not add",
            "        up to the deduplicated size of the repository (\"all archives\"), because the two",
            "        are meaning different things:",
            "",
            "        This archive / deduplicated size = amount of data stored ONLY for this archive",
            "        = unique chunks of this archive.",
            "        All archives / deduplicated size = amount of data stored in the repo",
            "        = all chunks in the repository.",
            "",
            "        Borg archives can only contain a limited amount of file metadata.",
            "        The size of an archive relative to this limit depends on a number of factors,",
            "        mainly the number of files, the lengths of paths and other metadata stored for files.",
            "        This is shown as *utilization of maximum supported archive size*.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('info', parents=[common_parser], add_help=False,",
            "                                          description=self.do_info.__doc__,",
            "                                          epilog=info_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='show repository or archive information')",
            "        subparser.set_defaults(func=self.do_info)",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='archive or repository to display information about')",
            "        subparser.add_argument('--json', action='store_true',",
            "                               help='format output as JSON')",
            "        define_archive_filters_group(subparser)",
            "",
            "        break_lock_epilog = process_epilog(\"\"\"",
            "        This command breaks the repository and cache locks.",
            "        Please use carefully and only while no borg process (on any machine) is",
            "        trying to access the Cache or the Repository.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('break-lock', parents=[common_parser], add_help=False,",
            "                                          description=self.do_break_lock.__doc__,",
            "                                          epilog=break_lock_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='break repository and cache locks')",
            "        subparser.set_defaults(func=self.do_break_lock)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository for which to break the locks')",
            "",
            "        prune_epilog = process_epilog(\"\"\"",
            "        The prune command prunes a repository by deleting all archives not matching",
            "        any of the specified retention options. This command is normally used by",
            "        automated backup scripts wanting to keep a certain number of historic backups.",
            "",
            "        Also, prune automatically removes checkpoint archives (incomplete archives left",
            "        behind by interrupted backup runs) except if the checkpoint is the latest",
            "        archive (and thus still needed). Checkpoint archives are not considered when",
            "        comparing archive counts against the retention limits (``--keep-X``).",
            "",
            "        If a prefix is set with -P, then only archives that start with the prefix are",
            "        considered for deletion and only those archives count towards the totals",
            "        specified by the rules.",
            "        Otherwise, *all* archives in the repository are candidates for deletion!",
            "        There is no automatic distinction between archives representing different",
            "        contents. These need to be distinguished by specifying matching prefixes.",
            "",
            "        If you have multiple sequences of archives with different data sets (e.g.",
            "        from different machines) in one shared repository, use one prune call per",
            "        data set that matches only the respective archives using the -P option.",
            "",
            "        The ``--keep-within`` option takes an argument of the form \"<int><char>\",",
            "        where char is \"H\", \"d\", \"w\", \"m\", \"y\". For example, ``--keep-within 2d`` means",
            "        to keep all archives that were created within the past 48 hours.",
            "        \"1m\" is taken to mean \"31d\". The archives kept with this option do not",
            "        count towards the totals specified by any other options.",
            "",
            "        A good procedure is to thin out more and more the older your backups get.",
            "        As an example, ``--keep-daily 7`` means to keep the latest backup on each day,",
            "        up to 7 most recent days with backups (days without backups do not count).",
            "        The rules are applied from secondly to yearly, and backups selected by previous",
            "        rules do not count towards those of later rules. The time that each backup",
            "        starts is used for pruning purposes. Dates and times are interpreted in",
            "        the local timezone, and weeks go from Monday to Sunday. Specifying a",
            "        negative number of archives to keep means that there is no limit.",
            "",
            "        The ``--keep-last N`` option is doing the same as ``--keep-secondly N`` (and it will",
            "        keep the last N archives under the assumption that you do not create more than one",
            "        backup archive in the same second).",
            "",
            "        When using ``--stats``, you will get some statistics about how much data was",
            "        deleted - the \"Deleted data\" deduplicated size there is most interesting as",
            "        that is how much your repository will shrink.",
            "        Please note that the \"All archives\" stats refer to the state after pruning.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('prune', parents=[common_parser], add_help=False,",
            "                                          description=self.do_prune.__doc__,",
            "                                          epilog=prune_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='prune archives')",
            "        subparser.set_defaults(func=self.do_prune)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not change repository')",
            "        subparser.add_argument('--force', dest='forced', action='store_true',",
            "                               help='force pruning of corrupted archives')",
            "        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics for the deleted archive')",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of archives it keeps/prunes')",
            "        subparser.add_argument('--keep-within', metavar='INTERVAL', dest='within', type=interval,",
            "                               help='keep all archives within this time interval')",
            "        subparser.add_argument('--keep-last', '--keep-secondly', dest='secondly', type=int, default=0,",
            "                               help='number of secondly archives to keep')",
            "        subparser.add_argument('--keep-minutely', dest='minutely', type=int, default=0,",
            "                               help='number of minutely archives to keep')",
            "        subparser.add_argument('-H', '--keep-hourly', dest='hourly', type=int, default=0,",
            "                               help='number of hourly archives to keep')",
            "        subparser.add_argument('-d', '--keep-daily', dest='daily', type=int, default=0,",
            "                               help='number of daily archives to keep')",
            "        subparser.add_argument('-w', '--keep-weekly', dest='weekly', type=int, default=0,",
            "                               help='number of weekly archives to keep')",
            "        subparser.add_argument('-m', '--keep-monthly', dest='monthly', type=int, default=0,",
            "                               help='number of monthly archives to keep')",
            "        subparser.add_argument('-y', '--keep-yearly', dest='yearly', type=int, default=0,",
            "                               help='number of yearly archives to keep')",
            "        define_archive_filters_group(subparser, sort_by=False, first_last=False)",
            "        subparser.add_argument('--save-space', dest='save_space', action='store_true',",
            "                               help='work slower, but using less space')",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to prune')",
            "",
            "        upgrade_epilog = process_epilog(\"\"\"",
            "        Upgrade an existing, local Borg repository.",
            "",
            "        When you do not need borg upgrade",
            "        +++++++++++++++++++++++++++++++++",
            "",
            "        Not every change requires that you run ``borg upgrade``.",
            "",
            "        You do **not** need to run it when:",
            "",
            "        - moving your repository to a different place",
            "        - upgrading to another point release (like 1.0.x to 1.0.y),",
            "          except when noted otherwise in the changelog",
            "        - upgrading from 1.0.x to 1.1.x,",
            "          except when noted otherwise in the changelog",
            "",
            "        Borg 1.x.y upgrades",
            "        +++++++++++++++++++",
            "",
            "        Use ``borg upgrade --tam REPO`` to require manifest authentication",
            "        introduced with Borg 1.0.9 to address security issues. This means",
            "        that modifying the repository after doing this with a version prior",
            "        to 1.0.9 will raise a validation error, so only perform this upgrade",
            "        after updating all clients using the repository to 1.0.9 or newer.",
            "",
            "        This upgrade should be done on each client for safety reasons.",
            "",
            "        If a repository is accidentally modified with a pre-1.0.9 client after",
            "        this upgrade, use ``borg upgrade --tam --force REPO`` to remedy it.",
            "",
            "        If you routinely do this you might not want to enable this upgrade",
            "        (which will leave you exposed to the security issue). You can",
            "        reverse the upgrade by issuing ``borg upgrade --disable-tam REPO``.",
            "",
            "        See",
            "        https://borgbackup.readthedocs.io/en/stable/changes.html#pre-1-0-9-manifest-spoofing-vulnerability",
            "        for details.",
            "",
            "        Attic and Borg 0.xx to Borg 1.x",
            "        +++++++++++++++++++++++++++++++",
            "",
            "        This currently supports converting an Attic repository to Borg and also",
            "        helps with converting Borg 0.xx to 1.0.",
            "",
            "        Currently, only LOCAL repositories can be upgraded (issue #465).",
            "",
            "        Please note that ``borg create`` (since 1.0.0) uses bigger chunks by",
            "        default than old borg or attic did, so the new chunks won't deduplicate",
            "        with the old chunks in the upgraded repository.",
            "        See ``--chunker-params`` option of ``borg create`` and ``borg recreate``.",
            "",
            "        ``borg upgrade`` will change the magic strings in the repository's",
            "        segments to match the new Borg magic strings. The keyfiles found in",
            "        $ATTIC_KEYS_DIR or ~/.attic/keys/ will also be converted and",
            "        copied to $BORG_KEYS_DIR or ~/.config/borg/keys.",
            "",
            "        The cache files are converted, from $ATTIC_CACHE_DIR or",
            "        ~/.cache/attic to $BORG_CACHE_DIR or ~/.cache/borg, but the",
            "        cache layout between Borg and Attic changed, so it is possible",
            "        the first backup after the conversion takes longer than expected",
            "        due to the cache resync.",
            "",
            "        Upgrade should be able to resume if interrupted, although it",
            "        will still iterate over all segments. If you want to start",
            "        from scratch, use `borg delete` over the copied repository to",
            "        make sure the cache files are also removed:",
            "",
            "            borg delete borg",
            "",
            "        Unless ``--inplace`` is specified, the upgrade process first creates a backup",
            "        copy of the repository, in REPOSITORY.before-upgrade-DATETIME, using hardlinks.",
            "        This requires that the repository and its parent directory reside on same",
            "        filesystem so the hardlink copy can work.",
            "        This takes longer than in place upgrades, but is much safer and gives",
            "        progress information (as opposed to ``cp -al``). Once you are satisfied",
            "        with the conversion, you can safely destroy the backup copy.",
            "",
            "        WARNING: Running the upgrade in place will make the current",
            "        copy unusable with older version, with no way of going back",
            "        to previous versions. This can PERMANENTLY DAMAGE YOUR",
            "        REPOSITORY!  Attic CAN NOT READ BORG REPOSITORIES, as the",
            "        magic strings have changed. You have been warned.\"\"\")",
            "        subparser = subparsers.add_parser('upgrade', parents=[common_parser], add_help=False,",
            "                                          description=self.do_upgrade.__doc__,",
            "                                          epilog=upgrade_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='upgrade repository format')",
            "        subparser.set_defaults(func=self.do_upgrade)",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not change repository')",
            "        subparser.add_argument('--inplace', dest='inplace', action='store_true',",
            "                               help='rewrite repository in place, with no chance of going back '",
            "                                    'to older versions of the repository.')",
            "        subparser.add_argument('--force', dest='force', action='store_true',",
            "                               help='Force upgrade')",
            "        subparser.add_argument('--tam', dest='tam', action='store_true',",
            "                               help='Enable manifest authentication (in key and cache) (Borg 1.0.9 and later).')",
            "        subparser.add_argument('--disable-tam', dest='disable_tam', action='store_true',",
            "                               help='Disable manifest authentication (in key and cache).')",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='path to the repository to be upgraded')",
            "",
            "        recreate_epilog = process_epilog(\"\"\"",
            "        Recreate the contents of existing archives.",
            "",
            "        This is an *experimental* feature. Do *not* use this on your only backup.",
            "",
            "        ``--exclude``, ``--exclude-from``, ``--exclude-if-present``, ``--keep-exclude-tags``, and PATH",
            "        have the exact same semantics as in \"borg create\". If PATHs are specified the",
            "        resulting archive will only contain files from these PATHs.",
            "",
            "        Note that all paths in an archive are relative, therefore absolute patterns/paths",
            "        will *not* match (``--exclude``, ``--exclude-from``, PATHs).",
            "",
            "        ``--recompress`` allows to change the compression of existing data in archives.",
            "        Due to how Borg stores compressed size information this might display",
            "        incorrect information for archives that were not recreated at the same time.",
            "        There is no risk of data loss by this.",
            "",
            "        ``--chunker-params`` will re-chunk all files in the archive, this can be",
            "        used to have upgraded Borg 0.xx or Attic archives deduplicate with",
            "        Borg 1.x archives.",
            "",
            "        **USE WITH CAUTION.**",
            "        Depending on the PATHs and patterns given, recreate can be used to permanently",
            "        delete files from archives.",
            "        When in doubt, use ``--dry-run --verbose --list`` to see how patterns/PATHS are",
            "        interpreted.",
            "",
            "        The archive being recreated is only removed after the operation completes. The",
            "        archive that is built during the operation exists at the same time at",
            "        \"<ARCHIVE>.recreate\". The new archive will have a different archive ID.",
            "",
            "        With ``--target`` the original archive is not replaced, instead a new archive is created.",
            "",
            "        When rechunking space usage can be substantial, expect at least the entire",
            "        deduplicated size of the archives using the previous chunker params.",
            "        When recompressing expect approx. (throughput / checkpoint-interval) in space usage,",
            "        assuming all chunks are recompressed.",
            "",
            "        If you recently ran borg check --repair and it had to fix lost chunks with all-zero",
            "        replacement chunks, please first run another backup for the same data and re-run",
            "        borg check --repair afterwards to heal any archives that had lost chunks which are",
            "        still generated from the input data.",
            "",
            "        Important: running borg recreate to re-chunk will remove the chunks_healthy",
            "        metadata of all items with replacement chunks, so healing will not be possible",
            "        any more after re-chunking (it is also unlikely it would ever work: due to the",
            "        change of chunking parameters, the missing chunk likely will never be seen again",
            "        even if you still have the data that produced it).",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('recreate', parents=[common_parser], add_help=False,",
            "                                          description=self.do_recreate.__doc__,",
            "                                          epilog=recreate_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help=self.do_recreate.__doc__)",
            "        subparser.set_defaults(func=self.do_recreate)",
            "        subparser.add_argument('--list', dest='output_list', action='store_true',",
            "                               help='output verbose list of items (files, dirs, ...)')",
            "        subparser.add_argument('--filter', metavar='STATUSCHARS', dest='output_filter',",
            "                               help='only display items with the given status characters (listed in borg create --help)')",
            "        subparser.add_argument('-n', '--dry-run', dest='dry_run', action='store_true',",
            "                               help='do not change anything')",
            "        subparser.add_argument('-s', '--stats', dest='stats', action='store_true',",
            "                               help='print statistics at end')",
            "",
            "        define_exclusion_group(subparser, tag_files=True)",
            "",
            "        archive_group = subparser.add_argument_group('Archive options')",
            "        archive_group.add_argument('--target', dest='target', metavar='TARGET', default=None,",
            "                                   type=archivename_validator(),",
            "                                   help='create a new archive with the name ARCHIVE, do not replace existing archive '",
            "                                        '(only applies for a single archive)')",
            "        archive_group.add_argument('-c', '--checkpoint-interval', dest='checkpoint_interval',",
            "                                   type=int, default=1800, metavar='SECONDS',",
            "                                   help='write checkpoint every SECONDS seconds (Default: 1800)')",
            "        archive_group.add_argument('--comment', dest='comment', metavar='COMMENT', default=None,",
            "                                   help='add a comment text to the archive')",
            "        archive_group.add_argument('--timestamp', metavar='TIMESTAMP', dest='timestamp',",
            "                                   type=timestamp, default=None,",
            "                                   help='manually specify the archive creation date/time (UTC, yyyy-mm-ddThh:mm:ss format). '",
            "                                        'alternatively, give a reference file/directory.')",
            "        archive_group.add_argument('-C', '--compression', metavar='COMPRESSION', dest='compression',",
            "                                   type=CompressionSpec, default=CompressionSpec('lz4'),",
            "                                   help='select compression algorithm, see the output of the '",
            "                                        '\"borg help compression\" command for details.')",
            "        archive_group.add_argument('--recompress', dest='recompress', nargs='?', default='never', const='if-different',",
            "                                   choices=('never', 'if-different', 'always'),",
            "                                   help='recompress data chunks according to ``--compression`` if `if-different`. '",
            "                                        'When `always`, chunks that are already compressed that way are not skipped, '",
            "                                        'but compressed again. Only the algorithm is considered for `if-different`, '",
            "                                        'not the compression level (if any).')",
            "        archive_group.add_argument('--chunker-params', metavar='PARAMS', dest='chunker_params',",
            "                                   type=ChunkerParams, default=CHUNKER_PARAMS,",
            "                                   help='specify the chunker parameters (CHUNK_MIN_EXP, CHUNK_MAX_EXP, '",
            "                                        'HASH_MASK_BITS, HASH_WINDOW_SIZE) or `default` to use the current defaults. '",
            "                                        'default: %d,%d,%d,%d' % CHUNKER_PARAMS)",
            "",
            "        subparser.add_argument('location', metavar='REPOSITORY_OR_ARCHIVE', nargs='?', default='',",
            "                               type=location_validator(),",
            "                               help='repository/archive to recreate')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='*', type=str,",
            "                               help='paths to recreate; patterns are supported')",
            "",
            "        with_lock_epilog = process_epilog(\"\"\"",
            "        This command runs a user-specified command while the repository lock is held.",
            "",
            "        It will first try to acquire the lock (make sure that no other operation is",
            "        running in the repo), then execute the given command as a subprocess and wait",
            "        for its termination, release the lock and return the user command's return",
            "        code as borg's return code.",
            "",
            "        .. note::",
            "",
            "            If you copy a repository with the lock held, the lock will be present in",
            "            the copy. Thus, before using borg on the copy from a different host,",
            "            you need to use \"borg break-lock\" on the copied repository, because",
            "            Borg is cautious and does not automatically remove stale locks made by a different host.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('with-lock', parents=[common_parser], add_help=False,",
            "                                          description=self.do_with_lock.__doc__,",
            "                                          epilog=with_lock_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='run user command with lock held')",
            "        subparser.set_defaults(func=self.do_with_lock)",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to lock')",
            "        subparser.add_argument('command', metavar='COMMAND',",
            "                               help='command to run')",
            "        subparser.add_argument('args', metavar='ARGS', nargs=argparse.REMAINDER,",
            "                               help='command arguments')",
            "",
            "        config_epilog = process_epilog(\"\"\"",
            "        This command gets and sets options in a local repository or cache config file.",
            "        For security reasons, this command only works on local repositories.",
            "",
            "        To delete a config value entirely, use ``--delete``. To get an existing key, pass",
            "        only the key name. To set a key, pass both the key name and the new value. Keys",
            "        can be specified in the format \"section.name\" or simply \"name\"; the section will",
            "        default to \"repository\" and \"cache\" for the repo and cache configs, respectively.",
            "",
            "        By default, borg config manipulates the repository config file. Using ``--cache``",
            "        edits the repository cache's config file instead.",
            "        \"\"\")",
            "        subparser = subparsers.add_parser('config', parents=[common_parser], add_help=False,",
            "                                          description=self.do_config.__doc__,",
            "                                          epilog=config_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='get and set configuration values')",
            "        subparser.set_defaults(func=self.do_config)",
            "        subparser.add_argument('-c', '--cache', dest='cache', action='store_true',",
            "                               help='get and set values from the repo cache')",
            "        subparser.add_argument('-d', '--delete', dest='delete', action='store_true',",
            "                               help='delete the key from the config file')",
            "",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False, proto='file'),",
            "                               help='repository to configure')",
            "        subparser.add_argument('name', metavar='NAME',",
            "                               help='name of config key')",
            "        subparser.add_argument('value', metavar='VALUE', nargs='?',",
            "                               help='new value for key')",
            "",
            "        subparser = subparsers.add_parser('help', parents=[common_parser], add_help=False,",
            "                                          description='Extra help')",
            "        subparser.add_argument('--epilog-only', dest='epilog_only', action='store_true')",
            "        subparser.add_argument('--usage-only', dest='usage_only', action='store_true')",
            "        subparser.set_defaults(func=functools.partial(self.do_help, parser, subparsers.choices))",
            "        subparser.add_argument('topic', metavar='TOPIC', type=str, nargs='?',",
            "                               help='additional help on TOPIC')",
            "",
            "        debug_epilog = process_epilog(\"\"\"",
            "        These commands are not intended for normal use and potentially very",
            "        dangerous if used incorrectly.",
            "",
            "        They exist to improve debugging capabilities without direct system access, e.g.",
            "        in case you ever run into some severe malfunction. Use them only if you know",
            "        what you are doing or if a trusted developer tells you what to do.\"\"\")",
            "",
            "        subparser = subparsers.add_parser('debug', parents=[mid_common_parser], add_help=False,",
            "                                          description='debugging command (not intended for normal use)',",
            "                                          epilog=debug_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='debugging command (not intended for normal use)')",
            "",
            "        debug_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')",
            "        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))",
            "",
            "        debug_info_epilog = process_epilog(\"\"\"",
            "        This command displays some system information that might be useful for bug",
            "        reports and debugging problems. If a traceback happens, this information is",
            "        already appended at the end of the traceback.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('info', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_info.__doc__,",
            "                                          epilog=debug_info_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='show system infos for debugging / bug reports (debug)')",
            "        subparser.set_defaults(func=self.do_debug_info)",
            "",
            "        debug_dump_archive_items_epilog = process_epilog(\"\"\"",
            "        This command dumps raw (but decrypted and decompressed) archive items (only metadata) to files.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-archive-items', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_archive_items.__doc__,",
            "                                          epilog=debug_dump_archive_items_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump archive items (metadata) (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_archive_items)",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to dump')",
            "",
            "        debug_dump_archive_epilog = process_epilog(\"\"\"",
            "        This command dumps all metadata of an archive in a decoded form to a file.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-archive', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_archive.__doc__,",
            "                                          epilog=debug_dump_archive_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump decoded archive metadata (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_archive)",
            "        subparser.add_argument('location', metavar='ARCHIVE',",
            "                               type=location_validator(archive=True),",
            "                               help='archive to dump')",
            "        subparser.add_argument('path', metavar='PATH', type=str,",
            "                               help='file to dump data into')",
            "",
            "        debug_dump_manifest_epilog = process_epilog(\"\"\"",
            "        This command dumps manifest metadata of a repository in a decoded form to a file.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-manifest', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_manifest.__doc__,",
            "                                          epilog=debug_dump_manifest_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump decoded repository metadata (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_manifest)",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to dump')",
            "        subparser.add_argument('path', metavar='PATH', type=str,",
            "                               help='file to dump data into')",
            "",
            "        debug_dump_repo_objs_epilog = process_epilog(\"\"\"",
            "        This command dumps raw (but decrypted and decompressed) repo objects to files.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('dump-repo-objs', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_dump_repo_objs.__doc__,",
            "                                          epilog=debug_dump_repo_objs_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='dump repo objects (debug)')",
            "        subparser.set_defaults(func=self.do_debug_dump_repo_objs)",
            "        subparser.add_argument('location', metavar='REPOSITORY',",
            "                               type=location_validator(archive=False),",
            "                               help='repo to dump')",
            "",
            "        debug_get_obj_epilog = process_epilog(\"\"\"",
            "        This command gets an object from the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('get-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_get_obj.__doc__,",
            "                                          epilog=debug_get_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='get object from repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_get_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('id', metavar='ID', type=str,",
            "                               help='hex object ID to get from the repo')",
            "        subparser.add_argument('path', metavar='PATH', type=str,",
            "                               help='file to write object data into')",
            "",
            "        debug_put_obj_epilog = process_epilog(\"\"\"",
            "        This command puts objects into the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('put-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_put_obj.__doc__,",
            "                                          epilog=debug_put_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='put object to repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_put_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('paths', metavar='PATH', nargs='+', type=str,",
            "                               help='file(s) to read and create object(s) from')",
            "",
            "        debug_delete_obj_epilog = process_epilog(\"\"\"",
            "        This command deletes objects from the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('delete-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_delete_obj.__doc__,",
            "                                          epilog=debug_delete_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='delete object from repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_delete_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('ids', metavar='IDs', nargs='+', type=str,",
            "                               help='hex object ID(s) to delete from the repo')",
            "",
            "        debug_refcount_obj_epilog = process_epilog(\"\"\"",
            "        This command displays the reference count for objects from the repository.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('refcount-obj', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_refcount_obj.__doc__,",
            "                                          epilog=debug_refcount_obj_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='show refcount for object from repository (debug)')",
            "        subparser.set_defaults(func=self.do_debug_refcount_obj)",
            "        subparser.add_argument('location', metavar='REPOSITORY', nargs='?', default='',",
            "                               type=location_validator(archive=False),",
            "                               help='repository to use')",
            "        subparser.add_argument('ids', metavar='IDs', nargs='+', type=str,",
            "                               help='hex object ID(s) to show refcounts for')",
            "",
            "        debug_convert_profile_epilog = process_epilog(\"\"\"",
            "        Convert a Borg profile to a Python cProfile compatible profile.",
            "        \"\"\")",
            "        subparser = debug_parsers.add_parser('convert-profile', parents=[common_parser], add_help=False,",
            "                                          description=self.do_debug_convert_profile.__doc__,",
            "                                          epilog=debug_convert_profile_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='convert Borg profile to Python profile (debug)')",
            "        subparser.set_defaults(func=self.do_debug_convert_profile)",
            "        subparser.add_argument('input', metavar='INPUT', type=argparse.FileType('rb'),",
            "                               help='Borg profile')",
            "        subparser.add_argument('output', metavar='OUTPUT', type=argparse.FileType('wb'),",
            "                               help='Output file')",
            "",
            "        benchmark_epilog = process_epilog(\"These commands do various benchmarks.\")",
            "",
            "        subparser = subparsers.add_parser('benchmark', parents=[mid_common_parser], add_help=False,",
            "                                          description='benchmark command',",
            "                                          epilog=benchmark_epilog,",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                          help='benchmark command')",
            "",
            "        benchmark_parsers = subparser.add_subparsers(title='required arguments', metavar='<command>')",
            "        subparser.set_defaults(fallback_func=functools.partial(self.do_subcommand_help, subparser))",
            "",
            "        bench_crud_epilog = process_epilog(\"\"\"",
            "        This command benchmarks borg CRUD (create, read, update, delete) operations.",
            "",
            "        It creates input data below the given PATH and backups this data into the given REPO.",
            "        The REPO must already exist (it could be a fresh empty repo or an existing repo, the",
            "        command will create / read / update / delete some archives named borg-test-data\\* there.",
            "",
            "        Make sure you have free space there, you'll need about 1GB each (+ overhead).",
            "",
            "        If your repository is encrypted and borg needs a passphrase to unlock the key, use:",
            "",
            "        BORG_PASSPHRASE=mysecret borg benchmark crud REPO PATH",
            "",
            "        Measurements are done with different input file sizes and counts.",
            "        The file contents are very artificial (either all zero or all random),",
            "        thus the measurement results do not necessarily reflect performance with real data.",
            "        Also, due to the kind of content used, no compression is used in these benchmarks.",
            "",
            "        C- == borg create (1st archive creation, no compression, do not use files cache)",
            "              C-Z- == all-zero files. full dedup, this is primarily measuring reader/chunker/hasher.",
            "              C-R- == random files. no dedup, measuring throughput through all processing stages.",
            "",
            "        R- == borg extract (extract archive, dry-run, do everything, but do not write files to disk)",
            "              R-Z- == all zero files. Measuring heavily duplicated files.",
            "              R-R- == random files. No duplication here, measuring throughput through all processing",
            "              stages, except writing to disk.",
            "",
            "        U- == borg create (2nd archive creation of unchanged input files, measure files cache speed)",
            "              The throughput value is kind of virtual here, it does not actually read the file.",
            "              U-Z- == needs to check the 2 all-zero chunks' existence in the repo.",
            "              U-R- == needs to check existence of a lot of different chunks in the repo.",
            "",
            "        D- == borg delete archive (delete last remaining archive, measure deletion + compaction)",
            "              D-Z- == few chunks to delete / few segments to compact/remove.",
            "              D-R- == many chunks to delete / many segments to compact/remove.",
            "",
            "        Please note that there might be quite some variance in these measurements.",
            "        Try multiple measurements and having a otherwise idle machine (and network, if you use it).",
            "        \"\"\")",
            "        subparser = benchmark_parsers.add_parser('crud', parents=[common_parser], add_help=False,",
            "                                                 description=self.do_benchmark_crud.__doc__,",
            "                                                 epilog=bench_crud_epilog,",
            "                                                 formatter_class=argparse.RawDescriptionHelpFormatter,",
            "                                                 help='benchmarks borg CRUD (create, extract, update, delete).')",
            "        subparser.set_defaults(func=self.do_benchmark_crud)",
            "",
            "        subparser.add_argument('location', metavar='REPO',",
            "                               type=location_validator(archive=False),",
            "                               help='repo to use for benchmark (must exist)')",
            "",
            "        subparser.add_argument('path', metavar='PATH', help='path were to create benchmark input data')",
            "",
            "        return parser",
            "",
            "    def get_args(self, argv, cmd):",
            "        \"\"\"usually, just returns argv, except if we deal with a ssh forced command for borg serve.\"\"\"",
            "        result = self.parse_args(argv[1:])",
            "        if cmd is not None and result.func == self.do_serve:",
            "            forced_result = result",
            "            argv = shlex.split(cmd)",
            "            # Drop environment variables (do *not* interpret them) before trying to parse",
            "            # the borg command line.",
            "            argv = list(itertools.dropwhile(lambda arg: '=' in arg, argv))",
            "            result = self.parse_args(argv[1:])",
            "            if result.func != forced_result.func:",
            "                # someone is trying to execute a different borg subcommand, don't do that!",
            "                return forced_result",
            "            # we only take specific options from the forced \"borg serve\" command:",
            "            result.restrict_to_paths = forced_result.restrict_to_paths",
            "            result.restrict_to_repositories = forced_result.restrict_to_repositories",
            "            result.append_only = forced_result.append_only",
            "        return result",
            "",
            "    def parse_args(self, args=None):",
            "        # We can't use argparse for \"serve\" since we don't want it to show up in \"Available commands\"",
            "        if args:",
            "            args = self.preprocess_args(args)",
            "        parser = self.build_parser()",
            "        args = parser.parse_args(args or ['-h'])",
            "        parser.common_options.resolve(args)",
            "        func = get_func(args)",
            "        if func == self.do_create and not args.paths:",
            "            # need at least 1 path but args.paths may also be populated from patterns",
            "            parser.error('Need at least one PATH argument.')",
            "        return args",
            "",
            "    def prerun_checks(self, logger):",
            "        check_python()",
            "        check_extension_modules()",
            "        selftest(logger)",
            "",
            "    def _setup_implied_logging(self, args):",
            "        \"\"\" turn on INFO level logging for args that imply that they will produce output \"\"\"",
            "        # map of option name to name of logger for that option",
            "        option_logger = {",
            "            'output_list': 'borg.output.list',",
            "            'show_version': 'borg.output.show-version',",
            "            'show_rc': 'borg.output.show-rc',",
            "            'stats': 'borg.output.stats',",
            "            'progress': 'borg.output.progress',",
            "        }",
            "        for option, logger_name in option_logger.items():",
            "            option_set = args.get(option, False)",
            "            logging.getLogger(logger_name).setLevel('INFO' if option_set else 'WARN')",
            "",
            "    def _setup_topic_debugging(self, args):",
            "        \"\"\"Turn on DEBUG level logging for specified --debug-topics.\"\"\"",
            "        for topic in args.debug_topics:",
            "            if '.' not in topic:",
            "                topic = 'borg.debug.' + topic",
            "            logger.debug('Enabling debug topic %s', topic)",
            "            logging.getLogger(topic).setLevel('DEBUG')",
            "",
            "    def run(self, args):",
            "        os.umask(args.umask)  # early, before opening files",
            "        self.lock_wait = args.lock_wait",
            "        func = get_func(args)",
            "        # do not use loggers before this!",
            "        is_serve = func == self.do_serve",
            "        setup_logging(level=args.log_level, is_serve=is_serve, json=args.log_json)",
            "        self.log_json = args.log_json",
            "        args.progress |= is_serve",
            "        self._setup_implied_logging(vars(args))",
            "        self._setup_topic_debugging(args)",
            "        if args.show_version:",
            "            logging.getLogger('borg.output.show-version').info('borgbackup version %s' % __version__)",
            "        self.prerun_checks(logger)",
            "        if is_slow_msgpack():",
            "            logger.warning(\"Using a pure-python msgpack! This will result in lower performance.\")",
            "        if args.debug_profile:",
            "            # Import only when needed - avoids a further increase in startup time",
            "            import cProfile",
            "            import marshal",
            "            logger.debug('Writing execution profile to %s', args.debug_profile)",
            "            # Open the file early, before running the main program, to avoid",
            "            # a very late crash in case the specified path is invalid.",
            "            with open(args.debug_profile, 'wb') as fd:",
            "                profiler = cProfile.Profile()",
            "                variables = dict(locals())",
            "                profiler.enable()",
            "                try:",
            "                    return set_ec(func(args))",
            "                finally:",
            "                    profiler.disable()",
            "                    profiler.snapshot_stats()",
            "                    if args.debug_profile.endswith('.pyprof'):",
            "                        marshal.dump(profiler.stats, fd)",
            "                    else:",
            "                        # We use msgpack here instead of the marshal module used by cProfile itself,",
            "                        # because the latter is insecure. Since these files may be shared over the",
            "                        # internet we don't want a format that is impossible to interpret outside",
            "                        # an insecure implementation.",
            "                        # See scripts/msgpack2marshal.py for a small script that turns a msgpack file",
            "                        # into a marshal file that can be read by e.g. pyprof2calltree.",
            "                        # For local use it's unnecessary hassle, though, that's why .pyprof makes",
            "                        # it compatible (see above).",
            "                        msgpack.pack(profiler.stats, fd, use_bin_type=True)",
            "        else:",
            "            return set_ec(func(args))",
            "",
            "",
            "def sig_info_handler(sig_no, stack):  # pragma: no cover",
            "    \"\"\"search the stack for infos about the currently processed file and print them\"\"\"",
            "    with signal_handler(sig_no, signal.SIG_IGN):",
            "        for frame in inspect.getouterframes(stack):",
            "            func, loc = frame[3], frame[0].f_locals",
            "            if func in ('process_file', '_process', ):  # create op",
            "                path = loc['path']",
            "                try:",
            "                    pos = loc['fd'].tell()",
            "                    total = loc['st'].st_size",
            "                except Exception:",
            "                    pos, total = 0, 0",
            "                logger.info(\"{0} {1}/{2}\".format(path, format_file_size(pos), format_file_size(total)))",
            "                break",
            "            if func in ('extract_item', ):  # extract op",
            "                path = loc['item'].path",
            "                try:",
            "                    pos = loc['fd'].tell()",
            "                except Exception:",
            "                    pos = 0",
            "                logger.info(\"{0} {1}/???\".format(path, format_file_size(pos)))",
            "                break",
            "",
            "",
            "def sig_trace_handler(sig_no, stack):  # pragma: no cover",
            "    print('\\nReceived SIGUSR2 at %s, dumping trace...' % datetime.now().replace(microsecond=0), file=sys.stderr)",
            "    faulthandler.dump_traceback()",
            "",
            "",
            "def main():  # pragma: no cover",
            "    # Make sure stdout and stderr have errors='replace' to avoid unicode",
            "    # issues when print()-ing unicode file names",
            "    sys.stdout = ErrorIgnoringTextIOWrapper(sys.stdout.buffer, sys.stdout.encoding, 'replace', line_buffering=True)",
            "    sys.stderr = ErrorIgnoringTextIOWrapper(sys.stderr.buffer, sys.stderr.encoding, 'replace', line_buffering=True)",
            "",
            "    # If we receive SIGINT (ctrl-c), SIGTERM (kill) or SIGHUP (kill -HUP),",
            "    # catch them and raise a proper exception that can be handled for an",
            "    # orderly exit.",
            "    # SIGHUP is important especially for systemd systems, where logind",
            "    # sends it when a session exits, in addition to any traditional use.",
            "    # Output some info if we receive SIGUSR1 or SIGINFO (ctrl-t).",
            "",
            "    # Register fault handler for SIGSEGV, SIGFPE, SIGABRT, SIGBUS and SIGILL.",
            "    faulthandler.enable()",
            "    with signal_handler('SIGINT', raising_signal_handler(KeyboardInterrupt)), \\",
            "         signal_handler('SIGHUP', raising_signal_handler(SigHup)), \\",
            "         signal_handler('SIGTERM', raising_signal_handler(SigTerm)), \\",
            "         signal_handler('SIGUSR1', sig_info_handler), \\",
            "         signal_handler('SIGUSR2', sig_trace_handler), \\",
            "         signal_handler('SIGINFO', sig_info_handler):",
            "        archiver = Archiver()",
            "        msg = msgid = tb = None",
            "        tb_log_level = logging.ERROR",
            "        try:",
            "            args = archiver.get_args(sys.argv, os.environ.get('SSH_ORIGINAL_COMMAND'))",
            "        except Error as e:",
            "            msg = e.get_message()",
            "            tb_log_level = logging.ERROR if e.traceback else logging.DEBUG",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            # we might not have logging setup yet, so get out quickly",
            "            print(msg, file=sys.stderr)",
            "            if tb_log_level == logging.ERROR:",
            "                print(tb, file=sys.stderr)",
            "            sys.exit(e.exit_code)",
            "        try:",
            "            exit_code = archiver.run(args)",
            "        except Error as e:",
            "            msg = e.get_message()",
            "            msgid = type(e).__qualname__",
            "            tb_log_level = logging.ERROR if e.traceback else logging.DEBUG",
            "            tb = \"%s\\n%s\" % (traceback.format_exc(), sysinfo())",
            "            exit_code = e.exit_code",
            "        except RemoteRepository.RPCError as e:",
            "            important = e.exception_class not in ('LockTimeout', ) and e.traceback",
            "            msgid = e.exception_class",
            "            tb_log_level = logging.ERROR if important else logging.DEBUG",
            "            if important:",
            "                msg = e.exception_full",
            "            else:",
            "                msg = e.get_message()",
            "            tb = '\\n'.join('Borg server: ' + l for l in e.sysinfo.splitlines())",
            "            tb += \"\\n\" + sysinfo()",
            "            exit_code = EXIT_ERROR",
            "        except Exception:",
            "            msg = 'Local Exception'",
            "            msgid = 'Exception'",
            "            tb_log_level = logging.ERROR",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            exit_code = EXIT_ERROR",
            "        except KeyboardInterrupt:",
            "            msg = 'Keyboard interrupt'",
            "            tb_log_level = logging.DEBUG",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            exit_code = EXIT_ERROR",
            "        except SigTerm:",
            "            msg = 'Received SIGTERM'",
            "            msgid = 'Signal.SIGTERM'",
            "            tb_log_level = logging.DEBUG",
            "            tb = '%s\\n%s' % (traceback.format_exc(), sysinfo())",
            "            exit_code = EXIT_ERROR",
            "        except SigHup:",
            "            msg = 'Received SIGHUP.'",
            "            msgid = 'Signal.SIGHUP'",
            "            exit_code = EXIT_ERROR",
            "        if msg:",
            "            logger.error(msg, msgid=msgid)",
            "        if tb:",
            "            logger.log(tb_log_level, tb)",
            "        if args.show_rc:",
            "            rc_logger = logging.getLogger('borg.output.show-rc')",
            "            exit_msg = 'terminating with %s status, rc %d'",
            "            if exit_code == EXIT_SUCCESS:",
            "                rc_logger.info(exit_msg % ('success', exit_code))",
            "            elif exit_code == EXIT_WARNING:",
            "                rc_logger.warning(exit_msg % ('warning', exit_code))",
            "            elif exit_code == EXIT_ERROR:",
            "                rc_logger.error(exit_msg % ('error', exit_code))",
            "            else:",
            "                rc_logger.error(exit_msg % ('abnormal', exit_code or 666))",
            "        sys.exit(exit_code)",
            "",
            "",
            "if __name__ == '__main__':",
            "    main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "ecdsa.der"
        ]
    },
    "src/borg/testsuite/archiver.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 3546,
                "afterPatchRowNumber": 3546,
                "PatchRowcode": "     assert args.restrict_to_paths == ['/p1', '/p2']"
            },
            "1": {
                "beforePatchRowNumber": 3547,
                "afterPatchRowNumber": 3547,
                "PatchRowcode": "     assert args.umask == 0o027"
            },
            "2": {
                "beforePatchRowNumber": 3548,
                "afterPatchRowNumber": 3548,
                "PatchRowcode": "     assert args.log_level == 'info'"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3549,
                "PatchRowcode": "+    # similar, but with --restrict-to-repository"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3550,
                "PatchRowcode": "+    args = archiver.get_args(['borg', 'serve', '--restrict-to-repository=/r1', '--restrict-to-repository=/r2', ],"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3551,
                "PatchRowcode": "+                             'borg serve --info --umask=0027')"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3552,
                "PatchRowcode": "+    assert args.restrict_to_repositories == ['/r1', '/r2']"
            },
            "7": {
                "beforePatchRowNumber": 3549,
                "afterPatchRowNumber": 3553,
                "PatchRowcode": "     # trying to cheat - break out of path restriction"
            },
            "8": {
                "beforePatchRowNumber": 3550,
                "afterPatchRowNumber": 3554,
                "PatchRowcode": "     args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],"
            },
            "9": {
                "beforePatchRowNumber": 3551,
                "afterPatchRowNumber": 3555,
                "PatchRowcode": "                              'borg serve --restrict-to-path=/')"
            },
            "10": {
                "beforePatchRowNumber": 3552,
                "afterPatchRowNumber": 3556,
                "PatchRowcode": "     assert args.restrict_to_paths == ['/p1', '/p2']"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3557,
                "PatchRowcode": "+    # trying to cheat - break out of repository restriction"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3558,
                "PatchRowcode": "+    args = archiver.get_args(['borg', 'serve', '--restrict-to-repository=/r1', '--restrict-to-repository=/r2', ],"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3559,
                "PatchRowcode": "+                             'borg serve --restrict-to-repository=/')"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3560,
                "PatchRowcode": "+    assert args.restrict_to_repositories == ['/r1', '/r2']"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3561,
                "PatchRowcode": "+    # trying to cheat - break below repository restriction"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3562,
                "PatchRowcode": "+    args = archiver.get_args(['borg', 'serve', '--restrict-to-repository=/r1', '--restrict-to-repository=/r2', ],"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3563,
                "PatchRowcode": "+                             'borg serve --restrict-to-repository=/r1/below')"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3564,
                "PatchRowcode": "+    assert args.restrict_to_repositories == ['/r1', '/r2']"
            },
            "19": {
                "beforePatchRowNumber": 3553,
                "afterPatchRowNumber": 3565,
                "PatchRowcode": "     # trying to cheat - try to execute different subcommand"
            },
            "20": {
                "beforePatchRowNumber": 3554,
                "afterPatchRowNumber": 3566,
                "PatchRowcode": "     args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],"
            },
            "21": {
                "beforePatchRowNumber": 3555,
                "afterPatchRowNumber": 3567,
                "PatchRowcode": "                              'borg init --encryption=repokey /')"
            }
        },
        "frontPatchFile": [
            "import argparse",
            "import errno",
            "import io",
            "import json",
            "import logging",
            "import os",
            "import pstats",
            "import random",
            "import re",
            "import shutil",
            "import socket",
            "import stat",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import unittest",
            "from binascii import unhexlify, b2a_base64",
            "from configparser import ConfigParser",
            "from datetime import datetime",
            "from datetime import timedelta",
            "from hashlib import sha256",
            "from io import BytesIO, StringIO",
            "from unittest.mock import patch",
            "",
            "import msgpack",
            "import pytest",
            "",
            "try:",
            "    import llfuse",
            "except ImportError:",
            "    pass",
            "",
            "import borg",
            "from .. import xattr, helpers, platform",
            "from ..archive import Archive, ChunkBuffer, flags_noatime, flags_normal",
            "from ..archiver import Archiver, parse_storage_quota",
            "from ..cache import Cache, LocalCache",
            "from ..constants import *  # NOQA",
            "from ..crypto.low_level import bytes_to_long, num_cipher_blocks",
            "from ..crypto.key import KeyfileKeyBase, RepoKey, KeyfileKey, Passphrase, TAMRequiredError",
            "from ..crypto.keymanager import RepoIdMismatch, NotABorgKeyFile",
            "from ..crypto.file_integrity import FileIntegrityError",
            "from ..helpers import Location, get_security_dir",
            "from ..helpers import Manifest, MandatoryFeatureUnsupported",
            "from ..helpers import EXIT_SUCCESS, EXIT_WARNING, EXIT_ERROR",
            "from ..helpers import bin_to_hex",
            "from ..helpers import MAX_S",
            "from ..nanorst import RstToTextLazy, rst_to_terminal",
            "from ..patterns import IECommand, PatternMatcher, parse_pattern",
            "from ..item import Item, ItemDiff",
            "from ..logger import setup_logging",
            "from ..remote import RemoteRepository, PathNotAllowed",
            "from ..repository import Repository",
            "from . import has_lchflags, has_llfuse",
            "from . import BaseTestCase, changedir, environment_variable, no_selinux",
            "from . import are_symlinks_supported, are_hardlinks_supported, are_fifos_supported, is_utime_fully_supported, is_birthtime_fully_supported",
            "from .platform import fakeroot_detected",
            "from .upgrader import attic_repo",
            "from . import key",
            "",
            "",
            "src_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))",
            "",
            "",
            "def exec_cmd(*args, archiver=None, fork=False, exe=None, input=b'', binary_output=False, **kw):",
            "    if fork:",
            "        try:",
            "            if exe is None:",
            "                borg = (sys.executable, '-m', 'borg.archiver')",
            "            elif isinstance(exe, str):",
            "                borg = (exe, )",
            "            elif not isinstance(exe, tuple):",
            "                raise ValueError('exe must be None, a tuple or a str')",
            "            output = subprocess.check_output(borg + args, stderr=subprocess.STDOUT, input=input)",
            "            ret = 0",
            "        except subprocess.CalledProcessError as e:",
            "            output = e.output",
            "            ret = e.returncode",
            "        except SystemExit as e:  # possibly raised by argparse",
            "            output = ''",
            "            ret = e.code",
            "        if binary_output:",
            "            return ret, output",
            "        else:",
            "            return ret, os.fsdecode(output)",
            "    else:",
            "        stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr",
            "        try:",
            "            sys.stdin = StringIO(input.decode())",
            "            sys.stdin.buffer = BytesIO(input)",
            "            output = BytesIO()",
            "            # Always use utf-8 here, to simply .decode() below",
            "            output_text = sys.stdout = sys.stderr = io.TextIOWrapper(output, encoding='utf-8')",
            "            if archiver is None:",
            "                archiver = Archiver()",
            "            archiver.prerun_checks = lambda *args: None",
            "            archiver.exit_code = EXIT_SUCCESS",
            "            helpers.exit_code = EXIT_SUCCESS",
            "            try:",
            "                args = archiver.parse_args(list(args))",
            "                # argparse parsing may raise SystemExit when the command line is bad or",
            "                # actions that abort early (eg. --help) where given. Catch this and return",
            "                # the error code as-if we invoked a Borg binary.",
            "            except SystemExit as e:",
            "                output_text.flush()",
            "                return e.code, output.getvalue() if binary_output else output.getvalue().decode()",
            "            ret = archiver.run(args)",
            "            output_text.flush()",
            "            return ret, output.getvalue() if binary_output else output.getvalue().decode()",
            "        finally:",
            "            sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr",
            "",
            "",
            "def have_gnutar():",
            "    if not shutil.which('tar'):",
            "        return False",
            "    popen = subprocess.Popen(['tar', '--version'], stdout=subprocess.PIPE)",
            "    stdout, stderr = popen.communicate()",
            "    return b'GNU tar' in stdout",
            "",
            "",
            "# check if the binary \"borg.exe\" is available (for local testing a symlink to virtualenv/bin/borg should do)",
            "try:",
            "    exec_cmd('help', exe='borg.exe', fork=True)",
            "    BORG_EXES = ['python', 'binary', ]",
            "except FileNotFoundError:",
            "    BORG_EXES = ['python', ]",
            "",
            "",
            "@pytest.fixture(params=BORG_EXES)",
            "def cmd(request):",
            "    if request.param == 'python':",
            "        exe = None",
            "    elif request.param == 'binary':",
            "        exe = 'borg.exe'",
            "    else:",
            "        raise ValueError(\"param must be 'python' or 'binary'\")",
            "",
            "    def exec_fn(*args, **kw):",
            "        return exec_cmd(*args, exe=exe, fork=True, **kw)",
            "    return exec_fn",
            "",
            "",
            "def test_return_codes(cmd, tmpdir):",
            "    repo = tmpdir.mkdir('repo')",
            "    input = tmpdir.mkdir('input')",
            "    output = tmpdir.mkdir('output')",
            "    input.join('test_file').write('content')",
            "    rc, out = cmd('init', '--encryption=none', '%s' % str(repo))",
            "    assert rc == EXIT_SUCCESS",
            "    rc, out = cmd('create', '%s::archive' % repo, str(input))",
            "    assert rc == EXIT_SUCCESS",
            "    with changedir(str(output)):",
            "        rc, out = cmd('extract', '%s::archive' % repo)",
            "        assert rc == EXIT_SUCCESS",
            "    rc, out = cmd('extract', '%s::archive' % repo, 'does/not/match')",
            "    assert rc == EXIT_WARNING  # pattern did not match",
            "    rc, out = cmd('create', '%s::archive' % repo, str(input))",
            "    assert rc == EXIT_ERROR  # duplicate archive name",
            "",
            "",
            "\"\"\"",
            "test_disk_full is very slow and not recommended to be included in daily testing.",
            "for this test, an empty, writable 16MB filesystem mounted on DF_MOUNT is required.",
            "for speed and other reasons, it is recommended that the underlying block device is",
            "in RAM, not a magnetic or flash disk.",
            "",
            "assuming /tmp is a tmpfs (in memory filesystem), one can use this:",
            "dd if=/dev/zero of=/tmp/borg-disk bs=16M count=1",
            "mkfs.ext4 /tmp/borg-disk",
            "mkdir /tmp/borg-mount",
            "sudo mount /tmp/borg-disk /tmp/borg-mount",
            "",
            "if the directory does not exist, the test will be skipped.",
            "\"\"\"",
            "DF_MOUNT = '/tmp/borg-mount'",
            "",
            "",
            "@pytest.mark.skipif(not os.path.exists(DF_MOUNT), reason=\"needs a 16MB fs mounted on %s\" % DF_MOUNT)",
            "def test_disk_full(cmd):",
            "    def make_files(dir, count, size, rnd=True):",
            "        shutil.rmtree(dir, ignore_errors=True)",
            "        os.mkdir(dir)",
            "        if rnd:",
            "            count = random.randint(1, count)",
            "            if size > 1:",
            "                size = random.randint(1, size)",
            "        for i in range(count):",
            "            fn = os.path.join(dir, \"file%03d\" % i)",
            "            with open(fn, 'wb') as f:",
            "                data = os.urandom(size)",
            "                f.write(data)",
            "",
            "    with environment_variable(BORG_CHECK_I_KNOW_WHAT_I_AM_DOING='YES'):",
            "        mount = DF_MOUNT",
            "        assert os.path.exists(mount)",
            "        repo = os.path.join(mount, 'repo')",
            "        input = os.path.join(mount, 'input')",
            "        reserve = os.path.join(mount, 'reserve')",
            "        for j in range(100):",
            "            shutil.rmtree(repo, ignore_errors=True)",
            "            shutil.rmtree(input, ignore_errors=True)",
            "            # keep some space and some inodes in reserve that we can free up later:",
            "            make_files(reserve, 80, 100000, rnd=False)",
            "            rc, out = cmd('init', repo)",
            "            if rc != EXIT_SUCCESS:",
            "                print('init', rc, out)",
            "            assert rc == EXIT_SUCCESS",
            "            try:",
            "                success, i = True, 0",
            "                while success:",
            "                    i += 1",
            "                    try:",
            "                        make_files(input, 20, 200000)",
            "                    except OSError as err:",
            "                        if err.errno == errno.ENOSPC:",
            "                            # already out of space",
            "                            break",
            "                        raise",
            "                    try:",
            "                        rc, out = cmd('create', '%s::test%03d' % (repo, i), input)",
            "                        success = rc == EXIT_SUCCESS",
            "                        if not success:",
            "                            print('create', rc, out)",
            "                    finally:",
            "                        # make sure repo is not locked",
            "                        shutil.rmtree(os.path.join(repo, 'lock.exclusive'), ignore_errors=True)",
            "                        os.remove(os.path.join(repo, 'lock.roster'))",
            "            finally:",
            "                # now some error happened, likely we are out of disk space.",
            "                # free some space so we can expect borg to be able to work normally:",
            "                shutil.rmtree(reserve, ignore_errors=True)",
            "            rc, out = cmd('list', repo)",
            "            if rc != EXIT_SUCCESS:",
            "                print('list', rc, out)",
            "            rc, out = cmd('check', '--repair', repo)",
            "            if rc != EXIT_SUCCESS:",
            "                print('check', rc, out)",
            "            assert rc == EXIT_SUCCESS",
            "",
            "",
            "class ArchiverTestCaseBase(BaseTestCase):",
            "    EXE = None  # python source based",
            "    FORK_DEFAULT = False",
            "    prefix = ''",
            "",
            "    def setUp(self):",
            "        os.environ['BORG_CHECK_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        os.environ['BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        os.environ['BORG_RECREATE_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        os.environ['BORG_PASSPHRASE'] = 'waytooeasyonlyfortests'",
            "        self.archiver = not self.FORK_DEFAULT and Archiver() or None",
            "        self.tmpdir = tempfile.mkdtemp()",
            "        self.repository_path = os.path.join(self.tmpdir, 'repository')",
            "        self.repository_location = self.prefix + self.repository_path",
            "        self.input_path = os.path.join(self.tmpdir, 'input')",
            "        self.output_path = os.path.join(self.tmpdir, 'output')",
            "        self.keys_path = os.path.join(self.tmpdir, 'keys')",
            "        self.cache_path = os.path.join(self.tmpdir, 'cache')",
            "        self.exclude_file_path = os.path.join(self.tmpdir, 'excludes')",
            "        self.patterns_file_path = os.path.join(self.tmpdir, 'patterns')",
            "        os.environ['BORG_KEYS_DIR'] = self.keys_path",
            "        os.environ['BORG_CACHE_DIR'] = self.cache_path",
            "        os.mkdir(self.input_path)",
            "        os.chmod(self.input_path, 0o777)  # avoid troubles with fakeroot / FUSE",
            "        os.mkdir(self.output_path)",
            "        os.mkdir(self.keys_path)",
            "        os.mkdir(self.cache_path)",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b'input/file2\\n# A comment line, then a blank line\\n\\n')",
            "        with open(self.patterns_file_path, 'wb') as fd:",
            "            fd.write(b'+input/file_important\\n- input/file*\\n# A comment line, then a blank line\\n\\n')",
            "        self._old_wd = os.getcwd()",
            "        os.chdir(self.tmpdir)",
            "",
            "    def tearDown(self):",
            "        os.chdir(self._old_wd)",
            "        # note: ignore_errors=True as workaround for issue #862",
            "        shutil.rmtree(self.tmpdir, ignore_errors=True)",
            "        # destroy logging configuration",
            "        logging.Logger.manager.loggerDict.clear()",
            "        setup_logging()",
            "",
            "    def cmd(self, *args, **kw):",
            "        exit_code = kw.pop('exit_code', 0)",
            "        fork = kw.pop('fork', None)",
            "        if fork is None:",
            "            fork = self.FORK_DEFAULT",
            "        ret, output = exec_cmd(*args, fork=fork, exe=self.EXE, archiver=self.archiver, **kw)",
            "        if ret != exit_code:",
            "            print(output)",
            "        self.assert_equal(ret, exit_code)",
            "        return output",
            "",
            "    def create_src_archive(self, name):",
            "        self.cmd('create', '--compression=lz4', self.repository_location + '::' + name, src_dir)",
            "",
            "    def open_archive(self, name):",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        with repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            archive = Archive(repository, key, manifest, name)",
            "        return archive, repository",
            "",
            "    def open_repository(self):",
            "        return Repository(self.repository_path, exclusive=True)",
            "",
            "    def create_regular_file(self, name, size=0, contents=None):",
            "        filename = os.path.join(self.input_path, name)",
            "        if not os.path.exists(os.path.dirname(filename)):",
            "            os.makedirs(os.path.dirname(filename))",
            "        with open(filename, 'wb') as fd:",
            "            if contents is None:",
            "                contents = b'X' * size",
            "            fd.write(contents)",
            "",
            "    def create_test_files(self):",
            "        \"\"\"Create a minimal test case including all supported file types",
            "        \"\"\"",
            "        # File",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('flagfile', size=1024)",
            "        # Directory",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        # File mode",
            "        os.chmod('input/file1', 0o4755)",
            "        # Hard link",
            "        if are_hardlinks_supported():",
            "            os.link(os.path.join(self.input_path, 'file1'),",
            "                    os.path.join(self.input_path, 'hardlink'))",
            "        # Symlink",
            "        if are_symlinks_supported():",
            "            os.symlink('somewhere', os.path.join(self.input_path, 'link1'))",
            "        self.create_regular_file('fusexattr', size=1)",
            "        if not xattr.XATTR_FAKEROOT and xattr.is_enabled(self.input_path):",
            "            # ironically, due to the way how fakeroot works, comparing FUSE file xattrs to orig file xattrs",
            "            # will FAIL if fakeroot supports xattrs, thus we only set the xattr if XATTR_FAKEROOT is False.",
            "            # This is because fakeroot with xattr-support does not propagate xattrs of the underlying file",
            "            # into \"fakeroot space\". Because the xattrs exposed by borgfs are these of an underlying file",
            "            # (from fakeroots point of view) they are invisible to the test process inside the fakeroot.",
            "            xattr.setxattr(os.path.join(self.input_path, 'fusexattr'), 'user.foo', b'bar')",
            "            xattr.setxattr(os.path.join(self.input_path, 'fusexattr'), 'user.empty', b'')",
            "            # XXX this always fails for me",
            "            # ubuntu 14.04, on a TMP dir filesystem with user_xattr, using fakeroot",
            "            # same for newer ubuntu and centos.",
            "            # if this is supported just on specific platform, platform should be checked first,",
            "            # so that the test setup for all tests using it does not fail here always for others.",
            "            # xattr.setxattr(os.path.join(self.input_path, 'link1'), 'user.foo_symlink', b'bar_symlink', follow_symlinks=False)",
            "        # FIFO node",
            "        if are_fifos_supported():",
            "            os.mkfifo(os.path.join(self.input_path, 'fifo1'))",
            "        if has_lchflags:",
            "            platform.set_flags(os.path.join(self.input_path, 'flagfile'), stat.UF_NODUMP)",
            "        try:",
            "            # Block device",
            "            os.mknod('input/bdev', 0o600 | stat.S_IFBLK, os.makedev(10, 20))",
            "            # Char device",
            "            os.mknod('input/cdev', 0o600 | stat.S_IFCHR, os.makedev(30, 40))",
            "            # File mode",
            "            os.chmod('input/dir2', 0o555)  # if we take away write perms, we need root to remove contents",
            "            # File owner",
            "            os.chown('input/file1', 100, 200)  # raises OSError invalid argument on cygwin",
            "            have_root = True  # we have (fake)root",
            "        except PermissionError:",
            "            have_root = False",
            "        except OSError as e:",
            "            # Note: ENOSYS \"Function not implemented\" happens as non-root on Win 10 Linux Subsystem.",
            "            if e.errno not in (errno.EINVAL, errno.ENOSYS):",
            "                raise",
            "            have_root = False",
            "        time.sleep(1)  # \"empty\" must have newer timestamp than other files",
            "        self.create_regular_file('empty', size=0)",
            "        return have_root",
            "",
            "",
            "class ArchiverTestCase(ArchiverTestCaseBase):",
            "    def test_basic_functionality(self):",
            "        have_root = self.create_test_files()",
            "        # fork required to test show-rc output",
            "        output = self.cmd('init', '--encryption=repokey', '--show-version', '--show-rc', self.repository_location, fork=True)",
            "        self.assert_in('borgbackup version', output)",
            "        self.assert_in('terminating with success status, rc 0', output)",
            "        self.cmd('create', '--exclude-nodump', self.repository_location + '::test', 'input')",
            "        output = self.cmd('create', '--exclude-nodump', '--stats', self.repository_location + '::test.2', 'input')",
            "        self.assert_in('Archive name: test.2', output)",
            "        self.assert_in('This archive: ', output)",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        list_output = self.cmd('list', '--short', self.repository_location)",
            "        self.assert_in('test', list_output)",
            "        self.assert_in('test.2', list_output)",
            "        expected = [",
            "            'input',",
            "            'input/bdev',",
            "            'input/cdev',",
            "            'input/dir2',",
            "            'input/dir2/file2',",
            "            'input/empty',",
            "            'input/file1',",
            "            'input/flagfile',",
            "        ]",
            "        if are_fifos_supported():",
            "            expected.append('input/fifo1')",
            "        if are_symlinks_supported():",
            "            expected.append('input/link1')",
            "        if are_hardlinks_supported():",
            "            expected.append('input/hardlink')",
            "        if not have_root:",
            "            # we could not create these device files without (fake)root",
            "            expected.remove('input/bdev')",
            "            expected.remove('input/cdev')",
            "        if has_lchflags:",
            "            # remove the file we did not backup, so input and output become equal",
            "            expected.remove('input/flagfile')  # this file is UF_NODUMP",
            "            os.remove(os.path.join('input', 'flagfile'))",
            "        list_output = self.cmd('list', '--short', self.repository_location + '::test')",
            "        for name in expected:",
            "            self.assert_in(name, list_output)",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        info_output = self.cmd('info', self.repository_location + '::test')",
            "        item_count = 4 if has_lchflags else 5  # one file is UF_NODUMP",
            "        self.assert_in('Number of files: %d' % item_count, info_output)",
            "        shutil.rmtree(self.cache_path)",
            "        info_output2 = self.cmd('info', self.repository_location + '::test')",
            "",
            "        def filter(output):",
            "            # filter for interesting \"info\" output, ignore cache rebuilding related stuff",
            "            prefixes = ['Name:', 'Fingerprint:', 'Number of files:', 'This archive:',",
            "                        'All archives:', 'Chunk index:', ]",
            "            result = []",
            "            for line in output.splitlines():",
            "                for prefix in prefixes:",
            "                    if line.startswith(prefix):",
            "                        result.append(line)",
            "            return '\\n'.join(result)",
            "",
            "        # the interesting parts of info_output2 and info_output should be same",
            "        self.assert_equal(filter(info_output), filter(info_output2))",
            "",
            "    def test_unix_socket(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        try:",
            "            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)",
            "            sock.bind(os.path.join(self.input_path, 'unix-socket'))",
            "        except PermissionError as err:",
            "            if err.errno == errno.EPERM:",
            "                pytest.skip('unix sockets disabled or not supported')",
            "            elif err.errno == errno.EACCES:",
            "                pytest.skip('permission denied to create unix sockets')",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        sock.close()",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert not os.path.exists('input/unix-socket')",
            "",
            "    @pytest.mark.skipif(not are_symlinks_supported(), reason='symlinks not supported')",
            "    def test_symlink_extract(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert os.readlink('input/link1') == 'somewhere'",
            "",
            "    @pytest.mark.skipif(not is_utime_fully_supported(), reason='cannot properly setup and execute test without utime')",
            "    def test_atime(self):",
            "        def has_noatime(some_file):",
            "            atime_before = os.stat(some_file).st_atime_ns",
            "            try:",
            "                with open(os.open(some_file, flags_noatime)) as file:",
            "                    file.read()",
            "            except PermissionError:",
            "                return False",
            "            else:",
            "                atime_after = os.stat(some_file).st_atime_ns",
            "                noatime_used = flags_noatime != flags_normal",
            "                return noatime_used and atime_before == atime_after",
            "",
            "        self.create_test_files()",
            "        atime, mtime = 123456780, 234567890",
            "        have_noatime = has_noatime('input/file1')",
            "        os.utime('input/file1', (atime, mtime))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        sti = os.stat('input/file1')",
            "        sto = os.stat('output/input/file1')",
            "        assert sti.st_mtime_ns == sto.st_mtime_ns == mtime * 1e9",
            "        if have_noatime:",
            "            assert sti.st_atime_ns == sto.st_atime_ns == atime * 1e9",
            "        else:",
            "            # it touched the input file's atime while backing it up",
            "            assert sto.st_atime_ns == atime * 1e9",
            "",
            "    @pytest.mark.skipif(not is_utime_fully_supported(), reason='cannot properly setup and execute test without utime')",
            "    @pytest.mark.skipif(not is_birthtime_fully_supported(), reason='cannot properly setup and execute test without birthtime')",
            "    def test_birthtime(self):",
            "        self.create_test_files()",
            "        birthtime, mtime, atime = 946598400, 946684800, 946771200",
            "        os.utime('input/file1', (atime, birthtime))",
            "        os.utime('input/file1', (atime, mtime))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        sti = os.stat('input/file1')",
            "        sto = os.stat('output/input/file1')",
            "        assert int(sti.st_birthtime * 1e9) == int(sto.st_birthtime * 1e9) == birthtime * 1e9",
            "        assert sti.st_mtime_ns == sto.st_mtime_ns == mtime * 1e9",
            "",
            "    @pytest.mark.skipif(not is_utime_fully_supported(), reason='cannot properly setup and execute test without utime')",
            "    @pytest.mark.skipif(not is_birthtime_fully_supported(), reason='cannot properly setup and execute test without birthtime')",
            "    def test_nobirthtime(self):",
            "        self.create_test_files()",
            "        birthtime, mtime, atime = 946598400, 946684800, 946771200",
            "        os.utime('input/file1', (atime, birthtime))",
            "        os.utime('input/file1', (atime, mtime))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--nobirthtime', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        sti = os.stat('input/file1')",
            "        sto = os.stat('output/input/file1')",
            "        assert int(sti.st_birthtime * 1e9) == birthtime * 1e9",
            "        assert int(sto.st_birthtime * 1e9) == mtime * 1e9",
            "        assert sti.st_mtime_ns == sto.st_mtime_ns == mtime * 1e9",
            "",
            "    def _extract_repository_id(self, path):",
            "        with Repository(self.repository_path) as repository:",
            "            return repository.id",
            "",
            "    def _set_repository_id(self, path, id):",
            "        config = ConfigParser(interpolation=None)",
            "        config.read(os.path.join(path, 'config'))",
            "        config.set('repository', 'id', bin_to_hex(id))",
            "        with open(os.path.join(path, 'config'), 'w') as fd:",
            "            config.write(fd)",
            "        with Repository(self.repository_path) as repository:",
            "            return repository.id",
            "",
            "    def test_sparse_file(self):",
            "        def is_sparse(fn, total_size, hole_size):",
            "            st = os.stat(fn)",
            "            assert st.st_size == total_size",
            "            sparse = True",
            "            if sparse and hasattr(st, 'st_blocks') and st.st_blocks * 512 >= st.st_size:",
            "                sparse = False",
            "            if sparse and hasattr(os, 'SEEK_HOLE') and hasattr(os, 'SEEK_DATA'):",
            "                with open(fn, 'rb') as fd:",
            "                    # only check if the first hole is as expected, because the 2nd hole check",
            "                    # is problematic on xfs due to its \"dynamic speculative EOF preallocation",
            "                    try:",
            "                        if fd.seek(0, os.SEEK_HOLE) != 0:",
            "                            sparse = False",
            "                        if fd.seek(0, os.SEEK_DATA) != hole_size:",
            "                            sparse = False",
            "                    except OSError:",
            "                        # OS/FS does not really support SEEK_HOLE/SEEK_DATA",
            "                        sparse = False",
            "            return sparse",
            "",
            "        filename = os.path.join(self.input_path, 'sparse')",
            "        content = b'foobar'",
            "        hole_size = 5 * (1 << CHUNK_MAX_EXP)  # 5 full chunker buffers",
            "        total_size = hole_size + len(content) + hole_size",
            "        with open(filename, 'wb') as fd:",
            "            # create a file that has a hole at the beginning and end (if the",
            "            # OS and filesystem supports sparse files)",
            "            fd.seek(hole_size, 1)",
            "            fd.write(content)",
            "            fd.seek(hole_size, 1)",
            "            pos = fd.tell()",
            "            fd.truncate(pos)",
            "        # we first check if we could create a sparse input file:",
            "        sparse_support = is_sparse(filename, total_size, hole_size)",
            "        if sparse_support:",
            "            # we could create a sparse input file, so creating a backup of it and",
            "            # extracting it again (as sparse) should also work:",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "            self.cmd('create', self.repository_location + '::test', 'input')",
            "            with changedir(self.output_path):",
            "                self.cmd('extract', '--sparse', self.repository_location + '::test')",
            "            self.assert_dirs_equal('input', 'output/input')",
            "            filename = os.path.join(self.output_path, 'input', 'sparse')",
            "            with open(filename, 'rb') as fd:",
            "                # check if file contents are as expected",
            "                self.assert_equal(fd.read(hole_size), b'\\0' * hole_size)",
            "                self.assert_equal(fd.read(len(content)), content)",
            "                self.assert_equal(fd.read(hole_size), b'\\0' * hole_size)",
            "            self.assert_true(is_sparse(filename, total_size, hole_size))",
            "",
            "    def test_unusual_filenames(self):",
            "        filenames = ['normal', 'with some blanks', '(with_parens)', ]",
            "        for filename in filenames:",
            "            filename = os.path.join(self.input_path, filename)",
            "            with open(filename, 'wb'):",
            "                pass",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        for filename in filenames:",
            "            with changedir('output'):",
            "                self.cmd('extract', self.repository_location + '::test', os.path.join('input', filename))",
            "            assert os.path.exists(os.path.join('output', 'input', filename))",
            "",
            "    def test_repository_swap_detection(self):",
            "        self.create_test_files()",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        shutil.rmtree(self.repository_path)",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self._set_repository_id(self.repository_path, repository_id)",
            "        self.assert_equal(repository_id, self._extract_repository_id(self.repository_path))",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.EncryptionMethodMismatch):",
            "                self.cmd('create', self.repository_location + '::test.2', 'input')",
            "",
            "    def test_repository_swap_detection2(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=none', self.repository_location + '_unencrypted')",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location + '_encrypted')",
            "        self.cmd('create', self.repository_location + '_encrypted::test', 'input')",
            "        shutil.rmtree(self.repository_path + '_encrypted')",
            "        os.rename(self.repository_path + '_unencrypted', self.repository_path + '_encrypted')",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '_encrypted::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.RepositoryAccessAborted):",
            "                self.cmd('create', self.repository_location + '_encrypted::test.2', 'input')",
            "",
            "    def test_repository_swap_detection_no_cache(self):",
            "        self.create_test_files()",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        shutil.rmtree(self.repository_path)",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self._set_repository_id(self.repository_path, repository_id)",
            "        self.assert_equal(repository_id, self._extract_repository_id(self.repository_path))",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.EncryptionMethodMismatch):",
            "                self.cmd('create', self.repository_location + '::test.2', 'input')",
            "",
            "    def test_repository_swap_detection2_no_cache(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=none', self.repository_location + '_unencrypted')",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location + '_encrypted')",
            "        self.cmd('create', self.repository_location + '_encrypted::test', 'input')",
            "        self.cmd('delete', '--cache-only', self.repository_location + '_unencrypted')",
            "        self.cmd('delete', '--cache-only', self.repository_location + '_encrypted')",
            "        shutil.rmtree(self.repository_path + '_encrypted')",
            "        os.rename(self.repository_path + '_unencrypted', self.repository_path + '_encrypted')",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '_encrypted::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.RepositoryAccessAborted):",
            "                self.cmd('create', self.repository_location + '_encrypted::test.2', 'input')",
            "",
            "    def test_repository_swap_detection_repokey_blank_passphrase(self):",
            "        # Check that a repokey repo with a blank passphrase is considered like a plaintext repo.",
            "        self.create_test_files()",
            "        # User initializes her repository with her passphrase",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        # Attacker replaces it with her own repository, which is encrypted but has no passphrase set",
            "        shutil.rmtree(self.repository_path)",
            "        with environment_variable(BORG_PASSPHRASE=''):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "            # Delete cache & security database, AKA switch to user perspective",
            "            self.cmd('delete', '--cache-only', self.repository_location)",
            "            repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "            shutil.rmtree(get_security_dir(repository_id))",
            "        with environment_variable(BORG_PASSPHRASE=None):",
            "            # This is the part were the user would be tricked, e.g. she assumes that BORG_PASSPHRASE",
            "            # is set, while it isn't. Previously this raised no warning,",
            "            # since the repository is, technically, encrypted.",
            "            if self.FORK_DEFAULT:",
            "                self.cmd('create', self.repository_location + '::test.2', 'input', exit_code=EXIT_ERROR)",
            "            else:",
            "                with pytest.raises(Cache.CacheInitAbortedError):",
            "                    self.cmd('create', self.repository_location + '::test.2', 'input')",
            "",
            "    def test_repository_move(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "        os.rename(self.repository_path, self.repository_path + '_new')",
            "        with environment_variable(BORG_RELOCATED_REPO_ACCESS_IS_OK='yes'):",
            "            self.cmd('info', self.repository_location + '_new')",
            "        security_dir = get_security_dir(repository_id)",
            "        with open(os.path.join(security_dir, 'location')) as fd:",
            "            location = fd.read()",
            "            assert location == Location(self.repository_location + '_new').canonical_path()",
            "        # Needs no confirmation anymore",
            "        self.cmd('info', self.repository_location + '_new')",
            "        shutil.rmtree(self.cache_path)",
            "        self.cmd('info', self.repository_location + '_new')",
            "        shutil.rmtree(security_dir)",
            "        self.cmd('info', self.repository_location + '_new')",
            "        for file in ('location', 'key-type', 'manifest-timestamp'):",
            "            assert os.path.exists(os.path.join(security_dir, file))",
            "",
            "    def test_security_dir_compat(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "        security_dir = get_security_dir(repository_id)",
            "        with open(os.path.join(security_dir, 'location'), 'w') as fd:",
            "            fd.write('something outdated')",
            "        # This is fine, because the cache still has the correct information. security_dir and cache can disagree",
            "        # if older versions are used to confirm a renamed repository.",
            "        self.cmd('info', self.repository_location)",
            "",
            "    def test_unknown_unencrypted(self):",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "        security_dir = get_security_dir(repository_id)",
            "        # Ok: repository is known",
            "        self.cmd('info', self.repository_location)",
            "",
            "        # Ok: repository is still known (through security_dir)",
            "        shutil.rmtree(self.cache_path)",
            "        self.cmd('info', self.repository_location)",
            "",
            "        # Needs confirmation: cache and security dir both gone (eg. another host or rm -rf ~)",
            "        shutil.rmtree(self.cache_path)",
            "        shutil.rmtree(security_dir)",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('info', self.repository_location, exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.CacheInitAbortedError):",
            "                self.cmd('info', self.repository_location)",
            "        with environment_variable(BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK='yes'):",
            "            self.cmd('info', self.repository_location)",
            "",
            "    def test_strip_components(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('dir/file')",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', '--strip-components', '3')",
            "            self.assert_true(not os.path.exists('file'))",
            "            with self.assert_creates_file('file'):",
            "                self.cmd('extract', self.repository_location + '::test', '--strip-components', '2')",
            "            with self.assert_creates_file('dir/file'):",
            "                self.cmd('extract', self.repository_location + '::test', '--strip-components', '1')",
            "            with self.assert_creates_file('input/dir/file'):",
            "                self.cmd('extract', self.repository_location + '::test', '--strip-components', '0')",
            "",
            "    def _extract_hardlinks_setup(self):",
            "        os.mkdir(os.path.join(self.input_path, 'dir1'))",
            "        os.mkdir(os.path.join(self.input_path, 'dir1/subdir'))",
            "",
            "        self.create_regular_file('source', contents=b'123456')",
            "        os.link(os.path.join(self.input_path, 'source'),",
            "                os.path.join(self.input_path, 'abba'))",
            "        os.link(os.path.join(self.input_path, 'source'),",
            "                os.path.join(self.input_path, 'dir1/hardlink'))",
            "        os.link(os.path.join(self.input_path, 'source'),",
            "                os.path.join(self.input_path, 'dir1/subdir/hardlink'))",
            "",
            "        self.create_regular_file('dir1/source2')",
            "        os.link(os.path.join(self.input_path, 'dir1/source2'),",
            "                os.path.join(self.input_path, 'dir1/aaaa'))",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "    requires_hardlinks = pytest.mark.skipif(not are_hardlinks_supported(), reason='hardlinks not supported')",
            "",
            "    @requires_hardlinks",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_mount_hardlinks(self):",
            "        self._extract_hardlinks_setup()",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        with self.fuse_mount(self.repository_location + '::test', mountpoint, '--strip-components=2'), \\",
            "             changedir(mountpoint):",
            "            assert os.stat('hardlink').st_nlink == 2",
            "            assert os.stat('subdir/hardlink').st_nlink == 2",
            "            assert open('subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('aaaa').st_nlink == 2",
            "            assert os.stat('source2').st_nlink == 2",
            "        with self.fuse_mount(self.repository_location + '::test', mountpoint, 'input/dir1'), \\",
            "             changedir(mountpoint):",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "        with self.fuse_mount(self.repository_location + '::test', mountpoint), \\",
            "             changedir(mountpoint):",
            "            assert os.stat('input/source').st_nlink == 4",
            "            assert os.stat('input/abba').st_nlink == 4",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 4",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 4",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "",
            "    @requires_hardlinks",
            "    def test_extract_hardlinks(self):",
            "        self._extract_hardlinks_setup()",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', '--strip-components', '2')",
            "            assert os.stat('hardlink').st_nlink == 2",
            "            assert os.stat('subdir/hardlink').st_nlink == 2",
            "            assert open('subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('aaaa').st_nlink == 2",
            "            assert os.stat('source2').st_nlink == 2",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', 'input/dir1')",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert os.stat('input/source').st_nlink == 4",
            "            assert os.stat('input/abba').st_nlink == 4",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 4",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 4",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "",
            "    def test_extract_include_exclude(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.cmd('create', '--exclude=input/file4', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', 'input/file1', )",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1'])",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude=input/file2', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "",
            "    def test_extract_include_exclude_regex(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.create_regular_file('file333', size=1024 * 80)",
            "",
            "        # Create with regular expression exclusion for file4",
            "        self.cmd('create', '--exclude=re:input/file4$', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2', 'file3', 'file333'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Extract with regular expression exclusion",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude=re:file3+', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Combine --exclude with fnmatch and regular expression",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude=input/file2', '--exclude=re:file[01]', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file3', 'file333'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Combine --exclude-from and regular expression exclusion",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, '--exclude=re:file1',",
            "                     '--exclude=re:file(\\\\d)\\\\1\\\\1$', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file3'])",
            "",
            "    def test_extract_include_exclude_regex_from_file(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.create_regular_file('file333', size=1024 * 80)",
            "        self.create_regular_file('aa:something', size=1024 * 80)",
            "",
            "        # Create while excluding using mixed pattern styles",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b're:input/file4$\\n')",
            "            fd.write(b'fm:*aa:*thing\\n')",
            "",
            "        self.cmd('create', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2', 'file3', 'file333'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Exclude using regular expression",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b're:file3+\\n')",
            "",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Mixed exclude pattern styles",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b're:file(\\\\d)\\\\1\\\\1$\\n')",
            "            fd.write(b'fm:nothingwillmatchthis\\n')",
            "            fd.write(b'*/file1\\n')",
            "            fd.write(b're:file2$\\n')",
            "",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file3'])",
            "",
            "    def test_extract_with_pattern(self):",
            "        self.cmd(\"init\", '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file(\"file1\", size=1024 * 80)",
            "        self.create_regular_file(\"file2\", size=1024 * 80)",
            "        self.create_regular_file(\"file3\", size=1024 * 80)",
            "        self.create_regular_file(\"file4\", size=1024 * 80)",
            "        self.create_regular_file(\"file333\", size=1024 * 80)",
            "",
            "        self.cmd(\"create\", self.repository_location + \"::test\", \"input\")",
            "",
            "        # Extract everything with regular expression",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", self.repository_location + \"::test\", \"re:.*\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file1\", \"file2\", \"file3\", \"file333\", \"file4\"])",
            "        shutil.rmtree(\"output/input\")",
            "",
            "        # Extract with pattern while also excluding files",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", \"--exclude=re:file[34]$\", self.repository_location + \"::test\", r\"re:file\\d$\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file1\", \"file2\"])",
            "        shutil.rmtree(\"output/input\")",
            "",
            "        # Combine --exclude with pattern for extraction",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", \"--exclude=input/file1\", self.repository_location + \"::test\", \"re:file[12]$\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file2\"])",
            "        shutil.rmtree(\"output/input\")",
            "",
            "        # Multiple pattern",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", self.repository_location + \"::test\", \"fm:input/file1\", \"fm:*file33*\", \"input/file2\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file1\", \"file2\", \"file333\"])",
            "",
            "    def test_extract_list_output(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file', size=1024 * 80)",
            "",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_not_in(\"input/file\", output)",
            "        shutil.rmtree('output/input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', '--info', self.repository_location + '::test')",
            "        self.assert_not_in(\"input/file\", output)",
            "        shutil.rmtree('output/input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', '--list', self.repository_location + '::test')",
            "        self.assert_in(\"input/file\", output)",
            "        shutil.rmtree('output/input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', '--list', '--info', self.repository_location + '::test')",
            "        self.assert_in(\"input/file\", output)",
            "",
            "    def test_extract_progress(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file', size=1024 * 80)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', self.repository_location + '::test', '--progress')",
            "            assert 'Extracting:' in output",
            "",
            "    def _create_test_caches(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('cache1/%s' % CACHE_TAG_NAME,",
            "                                 contents=CACHE_TAG_CONTENTS + b' extra stuff')",
            "        self.create_regular_file('cache2/%s' % CACHE_TAG_NAME,",
            "                                 contents=b'invalid signature')",
            "        os.mkdir('input/cache3')",
            "        os.link('input/cache1/%s' % CACHE_TAG_NAME, 'input/cache3/%s' % CACHE_TAG_NAME)",
            "",
            "    def test_create_stdin(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        input_data = b'\\x00foo\\n\\nbar\\n   \\n'",
            "        self.cmd('create', self.repository_location + '::test', '-', input=input_data)",
            "        item = json.loads(self.cmd('list', '--json-lines', self.repository_location + '::test'))",
            "        assert item['uid'] == 0",
            "        assert item['gid'] == 0",
            "        assert item['size'] == len(input_data)",
            "        assert item['path'] == 'stdin'",
            "        extracted_data = self.cmd('extract', '--stdout', self.repository_location + '::test', binary_output=True)",
            "        assert extracted_data == input_data",
            "",
            "    def test_create_without_root(self):",
            "        \"\"\"test create without a root\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', exit_code=2)",
            "",
            "    def test_create_pattern_root(self):",
            "        \"\"\"test create with only a root pattern\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list', '--pattern=R input', self.repository_location + '::test')",
            "        self.assert_in(\"A input/file1\", output)",
            "        self.assert_in(\"A input/file2\", output)",
            "",
            "    def test_create_pattern(self):",
            "        \"\"\"test file patterns during create\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file_important', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--pattern=+input/file_important', '--pattern=-input/file*',",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file_important\", output)",
            "        self.assert_in('x input/file1', output)",
            "        self.assert_in('x input/file2', output)",
            "",
            "    def test_create_pattern_file(self):",
            "        \"\"\"test file patterns during create\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('otherfile', size=1024 * 80)",
            "        self.create_regular_file('file_important', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--pattern=-input/otherfile', '--patterns-from=' + self.patterns_file_path,",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file_important\", output)",
            "        self.assert_in('x input/file1', output)",
            "        self.assert_in('x input/file2', output)",
            "        self.assert_in('x input/otherfile', output)",
            "",
            "    def test_create_pattern_exclude_folder_but_recurse(self):",
            "        \"\"\"test when patterns exclude a parent folder, but include a child\"\"\"",
            "        self.patterns_file_path2 = os.path.join(self.tmpdir, 'patterns2')",
            "        with open(self.patterns_file_path2, 'wb') as fd:",
            "            fd.write(b'+ input/x/b\\n- input/x*\\n')",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('x/a/foo_a', size=1024 * 80)",
            "        self.create_regular_file('x/b/foo_b', size=1024 * 80)",
            "        self.create_regular_file('y/foo_y', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--patterns-from=' + self.patterns_file_path2,",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_in('x input/x/a/foo_a', output)",
            "        self.assert_in(\"A input/x/b/foo_b\", output)",
            "        self.assert_in('A input/y/foo_y', output)",
            "",
            "    def test_create_pattern_exclude_folder_no_recurse(self):",
            "        \"\"\"test when patterns exclude a parent folder and, but include a child\"\"\"",
            "        self.patterns_file_path2 = os.path.join(self.tmpdir, 'patterns2')",
            "        with open(self.patterns_file_path2, 'wb') as fd:",
            "            fd.write(b'+ input/x/b\\n! input/x*\\n')",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('x/a/foo_a', size=1024 * 80)",
            "        self.create_regular_file('x/b/foo_b', size=1024 * 80)",
            "        self.create_regular_file('y/foo_y', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--patterns-from=' + self.patterns_file_path2,",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_not_in('input/x/a/foo_a', output)",
            "        self.assert_not_in('input/x/a', output)",
            "        self.assert_in('A input/y/foo_y', output)",
            "",
            "    def test_create_pattern_intermediate_folders_first(self):",
            "        \"\"\"test that intermediate folders appear first when patterns exclude a parent folder but include a child\"\"\"",
            "        self.patterns_file_path2 = os.path.join(self.tmpdir, 'patterns2')",
            "        with open(self.patterns_file_path2, 'wb') as fd:",
            "            fd.write(b'+ input/x/a\\n+ input/x/b\\n- input/x*\\n')",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "        self.create_regular_file('x/a/foo_a', size=1024 * 80)",
            "        self.create_regular_file('x/b/foo_b', size=1024 * 80)",
            "        with changedir('input'):",
            "            self.cmd('create', '--patterns-from=' + self.patterns_file_path2,",
            "                     self.repository_location + '::test', '.')",
            "",
            "        # list the archive and verify that the \"intermediate\" folders appear before",
            "        # their contents",
            "        out = self.cmd('list', '--format', '{type} {path}{NL}', self.repository_location + '::test')",
            "        out_list = out.splitlines()",
            "",
            "        self.assert_in('d x/a', out_list)",
            "        self.assert_in('d x/b', out_list)",
            "",
            "        assert out_list.index('d x/a') < out_list.index('- x/a/foo_a')",
            "        assert out_list.index('d x/b') < out_list.index('- x/b/foo_b')",
            "",
            "    def test_create_no_cache_sync(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        create_json = json.loads(self.cmd('create', '--no-cache-sync', self.repository_location + '::test', 'input',",
            "                                          '--json', '--error'))  # ignore experimental warning",
            "        info_json = json.loads(self.cmd('info', self.repository_location + '::test', '--json'))",
            "        create_stats = create_json['cache']['stats']",
            "        info_stats = info_json['cache']['stats']",
            "        assert create_stats == info_stats",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        self.cmd('create', '--no-cache-sync', self.repository_location + '::test2', 'input')",
            "        self.cmd('info', self.repository_location)",
            "        self.cmd('check', self.repository_location)",
            "",
            "    def test_extract_pattern_opt(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file_important', size=1024 * 80)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract',",
            "                     '--pattern=+input/file_important', '--pattern=-input/file*',",
            "                     self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file_important'])",
            "",
            "    def _assert_test_caches(self):",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['cache2', 'file1'])",
            "        self.assert_equal(sorted(os.listdir('output/input/cache2')), [CACHE_TAG_NAME])",
            "",
            "    def test_exclude_caches(self):",
            "        self._create_test_caches()",
            "        self.cmd('create', '--exclude-caches', self.repository_location + '::test', 'input')",
            "        self._assert_test_caches()",
            "",
            "    def test_recreate_exclude_caches(self):",
            "        self._create_test_caches()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('recreate', '--exclude-caches', self.repository_location + '::test')",
            "        self._assert_test_caches()",
            "",
            "    def _create_test_tagged(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('tagged1/.NOBACKUP')",
            "        self.create_regular_file('tagged2/00-NOBACKUP')",
            "        self.create_regular_file('tagged3/.NOBACKUP/file2', size=1024)",
            "",
            "    def _assert_test_tagged(self):",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1'])",
            "",
            "    def test_exclude_tagged(self):",
            "        self._create_test_tagged()",
            "        self.cmd('create', '--exclude-if-present', '.NOBACKUP', '--exclude-if-present', '00-NOBACKUP', self.repository_location + '::test', 'input')",
            "        self._assert_test_tagged()",
            "",
            "    def test_recreate_exclude_tagged(self):",
            "        self._create_test_tagged()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('recreate', '--exclude-if-present', '.NOBACKUP', '--exclude-if-present', '00-NOBACKUP',",
            "                 self.repository_location + '::test')",
            "        self._assert_test_tagged()",
            "",
            "    def _create_test_keep_tagged(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file0', size=1024)",
            "        self.create_regular_file('tagged1/.NOBACKUP1')",
            "        self.create_regular_file('tagged1/file1', size=1024)",
            "        self.create_regular_file('tagged2/.NOBACKUP2/subfile1', size=1024)",
            "        self.create_regular_file('tagged2/file2', size=1024)",
            "        self.create_regular_file('tagged3/%s' % CACHE_TAG_NAME,",
            "                                 contents=CACHE_TAG_CONTENTS + b' extra stuff')",
            "        self.create_regular_file('tagged3/file3', size=1024)",
            "        self.create_regular_file('taggedall/.NOBACKUP1')",
            "        self.create_regular_file('taggedall/.NOBACKUP2/subfile1', size=1024)",
            "        self.create_regular_file('taggedall/%s' % CACHE_TAG_NAME,",
            "                                 contents=CACHE_TAG_CONTENTS + b' extra stuff')",
            "        self.create_regular_file('taggedall/file4', size=1024)",
            "",
            "    def _assert_test_keep_tagged(self):",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file0', 'tagged1', 'tagged2', 'tagged3', 'taggedall'])",
            "        self.assert_equal(os.listdir('output/input/tagged1'), ['.NOBACKUP1'])",
            "        self.assert_equal(os.listdir('output/input/tagged2'), ['.NOBACKUP2'])",
            "        self.assert_equal(os.listdir('output/input/tagged3'), [CACHE_TAG_NAME])",
            "        self.assert_equal(sorted(os.listdir('output/input/taggedall')),",
            "                          ['.NOBACKUP1', '.NOBACKUP2', CACHE_TAG_NAME, ])",
            "",
            "    def test_exclude_keep_tagged_deprecation(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output_warn = self.cmd('create', '--exclude-caches', '--keep-tag-files', self.repository_location + '::test', src_dir)",
            "        self.assert_in('--keep-tag-files\" has been deprecated.', output_warn)",
            "",
            "    def test_exclude_keep_tagged(self):",
            "        self._create_test_keep_tagged()",
            "        self.cmd('create', '--exclude-if-present', '.NOBACKUP1', '--exclude-if-present', '.NOBACKUP2',",
            "                 '--exclude-caches', '--keep-exclude-tags', self.repository_location + '::test', 'input')",
            "        self._assert_test_keep_tagged()",
            "",
            "    def test_recreate_exclude_keep_tagged(self):",
            "        self._create_test_keep_tagged()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('recreate', '--exclude-if-present', '.NOBACKUP1', '--exclude-if-present', '.NOBACKUP2',",
            "                 '--exclude-caches', '--keep-exclude-tags', self.repository_location + '::test')",
            "        self._assert_test_keep_tagged()",
            "",
            "    @pytest.mark.skipif(not xattr.XATTR_FAKEROOT, reason='Linux capabilities test, requires fakeroot >= 1.20.2')",
            "    def test_extract_capabilities(self):",
            "        fchown = os.fchown",
            "",
            "        # We need to manually patch chown to get the behaviour Linux has, since fakeroot does not",
            "        # accurately model the interaction of chown(2) and Linux capabilities, i.e. it does not remove them.",
            "        def patched_fchown(fd, uid, gid):",
            "            xattr.setxattr(fd, 'security.capability', None, follow_symlinks=False)",
            "            fchown(fd, uid, gid)",
            "",
            "        # The capability descriptor used here is valid and taken from a /usr/bin/ping",
            "        capabilities = b'\\x01\\x00\\x00\\x02\\x00 \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'",
            "        self.create_regular_file('file')",
            "        xattr.setxattr('input/file', 'security.capability', capabilities)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            with patch.object(os, 'fchown', patched_fchown):",
            "                self.cmd('extract', self.repository_location + '::test')",
            "            assert xattr.getxattr('input/file', 'security.capability') == capabilities",
            "",
            "    @pytest.mark.skipif(not xattr.XATTR_FAKEROOT, reason='xattr not supported on this system or on this version of'",
            "                                                         'fakeroot')",
            "    def test_extract_xattrs_errors(self):",
            "        def patched_setxattr_E2BIG(*args, **kwargs):",
            "            raise OSError(errno.E2BIG, 'E2BIG')",
            "",
            "        def patched_setxattr_ENOTSUP(*args, **kwargs):",
            "            raise OSError(errno.ENOTSUP, 'ENOTSUP')",
            "",
            "        def patched_setxattr_EACCES(*args, **kwargs):",
            "            raise OSError(errno.EACCES, 'EACCES')",
            "",
            "        self.create_regular_file('file')",
            "        xattr.setxattr('input/file', 'attribute', 'value')",
            "        self.cmd('init', self.repository_location, '-e' 'none')",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            input_abspath = os.path.abspath('input/file')",
            "            with patch.object(xattr, 'setxattr', patched_setxattr_E2BIG):",
            "                out = self.cmd('extract', self.repository_location + '::test', exit_code=EXIT_WARNING)",
            "                assert out == (input_abspath + ': Value or key of extended attribute attribute is too big for this '",
            "                                               'filesystem\\n')",
            "            os.remove(input_abspath)",
            "            with patch.object(xattr, 'setxattr', patched_setxattr_ENOTSUP):",
            "                out = self.cmd('extract', self.repository_location + '::test', exit_code=EXIT_WARNING)",
            "                assert out == (input_abspath + ': Extended attributes are not supported on this filesystem\\n')",
            "            os.remove(input_abspath)",
            "            with patch.object(xattr, 'setxattr', patched_setxattr_EACCES):",
            "                out = self.cmd('extract', self.repository_location + '::test', exit_code=EXIT_WARNING)",
            "                assert out == (input_abspath + ': Permission denied when setting extended attribute attribute\\n')",
            "            assert os.path.isfile(input_abspath)",
            "",
            "    def test_path_normalization(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('dir1/dir2/file', size=1024 * 80)",
            "        with changedir('input/dir1/dir2'):",
            "            self.cmd('create', self.repository_location + '::test', '../../../input/dir1/../dir1/dir2/..')",
            "        output = self.cmd('list', self.repository_location + '::test')",
            "        self.assert_not_in('..', output)",
            "        self.assert_in(' input/dir1/dir2/file', output)",
            "",
            "    def test_exclude_normalization(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        with changedir('input'):",
            "            self.cmd('create', '--exclude=file1', self.repository_location + '::test1', '.')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test1')",
            "        self.assert_equal(sorted(os.listdir('output')), ['file2'])",
            "        with changedir('input'):",
            "            self.cmd('create', '--exclude=./file1', self.repository_location + '::test2', '.')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test2')",
            "        self.assert_equal(sorted(os.listdir('output')), ['file2'])",
            "        self.cmd('create', '--exclude=input/./file1', self.repository_location + '::test3', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test3')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file2'])",
            "",
            "    def test_repeated_files(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input', 'input')",
            "",
            "    def test_overwrite(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        # Overwriting regular files and directories should be supported",
            "        os.mkdir('output/input')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/dir2')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        # But non-empty dirs should fail",
            "        os.unlink('output/input/file1')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/file1/dir')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', exit_code=1)",
            "",
            "    def test_rename(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.cmd('rename', self.repository_location + '::test', 'test.3')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.cmd('rename', self.repository_location + '::test.2', 'test.4')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.3')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.4')",
            "        # Make sure both archives have been renamed",
            "        with Repository(self.repository_path) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "        self.assert_equal(len(manifest.archives), 2)",
            "        self.assert_in('test.3', manifest.archives)",
            "        self.assert_in('test.4', manifest.archives)",
            "",
            "    def test_info(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        info_repo = self.cmd('info', self.repository_location)",
            "        assert 'All archives:' in info_repo",
            "        info_archive = self.cmd('info', self.repository_location + '::test')",
            "        assert 'Archive name: test\\n' in info_archive",
            "        info_archive = self.cmd('info', '--first', '1', self.repository_location)",
            "        assert 'Archive name: test\\n' in info_archive",
            "",
            "    def test_info_json(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        info_repo = json.loads(self.cmd('info', '--json', self.repository_location))",
            "        repository = info_repo['repository']",
            "        assert len(repository['id']) == 64",
            "        assert 'last_modified' in repository",
            "        assert datetime.strptime(repository['last_modified'], ISO_FORMAT)  # must not raise",
            "        assert info_repo['encryption']['mode'] == 'repokey'",
            "        assert 'keyfile' not in info_repo['encryption']",
            "        cache = info_repo['cache']",
            "        stats = cache['stats']",
            "        assert all(isinstance(o, int) for o in stats.values())",
            "        assert all(key in stats for key in ('total_chunks', 'total_csize', 'total_size', 'total_unique_chunks', 'unique_csize', 'unique_size'))",
            "",
            "        info_archive = json.loads(self.cmd('info', '--json', self.repository_location + '::test'))",
            "        assert info_repo['repository'] == info_archive['repository']",
            "        assert info_repo['cache'] == info_archive['cache']",
            "        archives = info_archive['archives']",
            "        assert len(archives) == 1",
            "        archive = archives[0]",
            "        assert archive['name'] == 'test'",
            "        assert isinstance(archive['command_line'], list)",
            "        assert isinstance(archive['duration'], float)",
            "        assert len(archive['id']) == 64",
            "        assert 'stats' in archive",
            "        assert datetime.strptime(archive['start'], ISO_FORMAT)",
            "        assert datetime.strptime(archive['end'], ISO_FORMAT)",
            "",
            "    def test_comment(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "        self.cmd('create', '--comment', 'this is the comment', self.repository_location + '::test2', 'input')",
            "        self.cmd('create', '--comment', '\"deleted\" comment', self.repository_location + '::test3', 'input')",
            "        self.cmd('create', '--comment', 'preserved comment', self.repository_location + '::test4', 'input')",
            "        assert 'Comment: \\n' in self.cmd('info', self.repository_location + '::test1')",
            "        assert 'Comment: this is the comment' in self.cmd('info', self.repository_location + '::test2')",
            "",
            "        self.cmd('recreate', self.repository_location + '::test1', '--comment', 'added comment')",
            "        self.cmd('recreate', self.repository_location + '::test2', '--comment', 'modified comment')",
            "        self.cmd('recreate', self.repository_location + '::test3', '--comment', '')",
            "        self.cmd('recreate', self.repository_location + '::test4', '12345')",
            "        assert 'Comment: added comment' in self.cmd('info', self.repository_location + '::test1')",
            "        assert 'Comment: modified comment' in self.cmd('info', self.repository_location + '::test2')",
            "        assert 'Comment: \\n' in self.cmd('info', self.repository_location + '::test3')",
            "        assert 'Comment: preserved comment' in self.cmd('info', self.repository_location + '::test4')",
            "",
            "    def test_delete(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        self.cmd('create', self.repository_location + '::test.3', 'input')",
            "        self.cmd('create', self.repository_location + '::another_test.1', 'input')",
            "        self.cmd('create', self.repository_location + '::another_test.2', 'input')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.cmd('delete', '--prefix', 'another_', self.repository_location)",
            "        self.cmd('delete', '--last', '1', self.repository_location)",
            "        self.cmd('delete', self.repository_location + '::test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        output = self.cmd('delete', '--stats', self.repository_location + '::test.2')",
            "        self.assert_in('Deleted data:', output)",
            "        # Make sure all data except the manifest has been deleted",
            "        with Repository(self.repository_path) as repository:",
            "            self.assert_equal(len(repository), 1)",
            "",
            "    def test_delete_multiple(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "        self.cmd('create', self.repository_location + '::test2', 'input')",
            "        self.cmd('create', self.repository_location + '::test3', 'input')",
            "        self.cmd('delete', self.repository_location + '::test1', 'test2')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test3')",
            "        self.cmd('delete', self.repository_location, 'test3')",
            "        assert not self.cmd('list', self.repository_location)",
            "",
            "    def test_delete_repo(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        os.environ['BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'] = 'no'",
            "        self.cmd('delete', self.repository_location, exit_code=2)",
            "        assert os.path.exists(self.repository_path)",
            "        os.environ['BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        self.cmd('delete', self.repository_location)",
            "        # Make sure the repo is gone",
            "        self.assertFalse(os.path.exists(self.repository_path))",
            "",
            "    def test_delete_force(self):",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self.create_src_archive('test')",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            archive = Archive(repository, key, manifest, 'test')",
            "            for item in archive.iter_items():",
            "                if 'chunks' in item:",
            "                    first_chunk_id = item.chunks[0].id",
            "                    repository.delete(first_chunk_id)",
            "                    repository.commit()",
            "                    break",
            "        output = self.cmd('delete', '--force', self.repository_location + '::test')",
            "        self.assert_in('deleted archive was corrupted', output)",
            "        self.cmd('check', '--repair', self.repository_location)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test', output)",
            "",
            "    def test_delete_double_force(self):",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self.create_src_archive('test')",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            archive = Archive(repository, key, manifest, 'test')",
            "            id = archive.metadata.items[0]",
            "            repository.put(id, b'corrupted items metadata stream chunk')",
            "            repository.commit()",
            "        self.cmd('delete', '--force', '--force', self.repository_location + '::test')",
            "        self.cmd('check', '--repair', self.repository_location)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test', output)",
            "",
            "    def test_corrupted_repository(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        output = self.cmd('check', '--show-version', self.repository_location)",
            "        self.assert_in('borgbackup version', output)  # implied output even without --info given",
            "        self.assert_not_in('Starting repository check', output)  # --info not given for root logger",
            "",
            "        name = sorted(os.listdir(os.path.join(self.tmpdir, 'repository', 'data', '0')), reverse=True)[1]",
            "        with open(os.path.join(self.tmpdir, 'repository', 'data', '0', name), 'r+b') as fd:",
            "            fd.seek(100)",
            "            fd.write(b'XXXX')",
            "        output = self.cmd('check', '--info', self.repository_location, exit_code=1)",
            "        self.assert_in('Starting repository check', output)  # --info given for root logger",
            "",
            "    # we currently need to be able to create a lock directory inside the repo:",
            "    @pytest.mark.xfail(reason=\"we need to be able to create the lock directory inside the repo\")",
            "    def test_readonly_repository(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('test')",
            "        os.system('chmod -R ugo-w ' + self.repository_path)",
            "        try:",
            "            self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        finally:",
            "            # Restore permissions so shutil.rmtree is able to delete it",
            "            os.system('chmod -R u+w ' + self.repository_path)",
            "",
            "    @pytest.mark.skipif('BORG_TESTS_IGNORE_MODES' in os.environ, reason='modes unreliable')",
            "    def test_umask(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        mode = os.stat(self.repository_path).st_mode",
            "        self.assertEqual(stat.S_IMODE(mode), 0o700)",
            "",
            "    def test_create_dry_run(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--dry-run', self.repository_location + '::test', 'input')",
            "        # Make sure no archive has been created",
            "        with Repository(self.repository_path) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "        self.assert_equal(len(manifest.archives), 0)",
            "",
            "    def add_unknown_feature(self, operation):",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            manifest.config[b'feature_flags'] = {operation.value.encode(): {b'mandatory': [b'unknown-feature']}}",
            "            manifest.write()",
            "            repository.commit()",
            "",
            "    def cmd_raises_unknown_feature(self, args):",
            "        if self.FORK_DEFAULT:",
            "            self.cmd(*args, exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(MandatoryFeatureUnsupported) as excinfo:",
            "                self.cmd(*args)",
            "            assert excinfo.value.args == (['unknown-feature'],)",
            "",
            "    def test_unknown_feature_on_create(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.add_unknown_feature(Manifest.Operation.WRITE)",
            "        self.cmd_raises_unknown_feature(['create', self.repository_location + '::test', 'input'])",
            "",
            "    def test_unknown_feature_on_cache_sync(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        self.add_unknown_feature(Manifest.Operation.READ)",
            "        self.cmd_raises_unknown_feature(['create', self.repository_location + '::test', 'input'])",
            "",
            "    def test_unknown_feature_on_change_passphrase(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.add_unknown_feature(Manifest.Operation.CHECK)",
            "        self.cmd_raises_unknown_feature(['change-passphrase', self.repository_location])",
            "",
            "    def test_unknown_feature_on_read(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.READ)",
            "        with changedir('output'):",
            "            self.cmd_raises_unknown_feature(['extract', self.repository_location + '::test'])",
            "",
            "        self.cmd_raises_unknown_feature(['list', self.repository_location])",
            "        self.cmd_raises_unknown_feature(['info', self.repository_location + '::test'])",
            "",
            "    def test_unknown_feature_on_rename(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.CHECK)",
            "        self.cmd_raises_unknown_feature(['rename', self.repository_location + '::test', 'other'])",
            "",
            "    def test_unknown_feature_on_delete(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.DELETE)",
            "        # delete of an archive raises",
            "        self.cmd_raises_unknown_feature(['delete', self.repository_location + '::test'])",
            "        self.cmd_raises_unknown_feature(['prune', '--keep-daily=3', self.repository_location])",
            "        # delete of the whole repository ignores features",
            "        self.cmd('delete', self.repository_location)",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_unknown_feature_on_mount(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.READ)",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        os.mkdir(mountpoint)",
            "        # XXX this might hang if it doesn't raise an error",
            "        self.cmd_raises_unknown_feature(['mount', self.repository_location + '::test', mountpoint])",
            "",
            "    @pytest.mark.allow_cache_wipe",
            "    def test_unknown_mandatory_feature_in_cache(self):",
            "        if self.prefix:",
            "            path_prefix = 'ssh://__testsuite__'",
            "        else:",
            "            path_prefix = ''",
            "",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            if path_prefix:",
            "                repository._location = Location(self.repository_location)",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest) as cache:",
            "                cache.begin_txn()",
            "                cache.cache_config.mandatory_features = set(['unknown-feature'])",
            "                cache.commit()",
            "",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '::test', 'input')",
            "        else:",
            "            called = False",
            "            wipe_cache_safe = LocalCache.wipe_cache",
            "",
            "            def wipe_wrapper(*args):",
            "                nonlocal called",
            "                called = True",
            "                wipe_cache_safe(*args)",
            "",
            "            with patch.object(LocalCache, 'wipe_cache', wipe_wrapper):",
            "                self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "            assert called",
            "",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            if path_prefix:",
            "                repository._location = Location(self.repository_location)",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest) as cache:",
            "                assert cache.cache_config.mandatory_features == set([])",
            "",
            "    def test_progress_on(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--progress', self.repository_location + '::test4', 'input')",
            "        self.assert_in(\"\\r\", output)",
            "",
            "    def test_progress_off(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', self.repository_location + '::test5', 'input')",
            "        self.assert_not_in(\"\\r\", output)",
            "",
            "    def test_file_status(self):",
            "        \"\"\"test that various file status show expected results",
            "",
            "        clearly incomplete: only tests for the weird \"unchanged\" status for now\"\"\"",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "        self.assert_in(\"A input/file2\", output)",
            "        # should find first file as unmodified",
            "        output = self.cmd('create', '--list', self.repository_location + '::test1', 'input')",
            "        self.assert_in(\"U input/file1\", output)",
            "        # this is expected, although surprising, for why, see:",
            "        # https://borgbackup.readthedocs.org/en/latest/faq.html#i-am-seeing-a-added-status-for-a-unchanged-file",
            "        self.assert_in(\"A input/file2\", output)",
            "",
            "    def test_file_status_cs_cache_mode(self):",
            "        \"\"\"test that a changed file with faked \"previous\" mtime still gets backed up in ctime,size cache_mode\"\"\"",
            "        self.create_regular_file('file1', contents=b'123')",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=10)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--files-cache=ctime,size', self.repository_location + '::test1', 'input')",
            "        # modify file1, but cheat with the mtime (and atime) and also keep same size:",
            "        st = os.stat('input/file1')",
            "        self.create_regular_file('file1', contents=b'321')",
            "        os.utime('input/file1', ns=(st.st_atime_ns, st.st_mtime_ns))",
            "        # this mode uses ctime for change detection, so it should find file1 as modified",
            "        output = self.cmd('create', '--list', '--files-cache=ctime,size', self.repository_location + '::test2', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "",
            "    def test_file_status_ms_cache_mode(self):",
            "        \"\"\"test that a chmod'ed file with no content changes does not get chunked again in mtime,size cache_mode\"\"\"",
            "        self.create_regular_file('file1', size=10)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=10)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--files-cache=mtime,size', self.repository_location + '::test1', 'input')",
            "        # change mode of file1, no content change:",
            "        st = os.stat('input/file1')",
            "        os.chmod('input/file1', st.st_mode ^ stat.S_IRWXO)  # this triggers a ctime change, but mtime is unchanged",
            "        # this mode uses mtime for change detection, so it should find file1 as unmodified",
            "        output = self.cmd('create', '--list', '--files-cache=mtime,size', self.repository_location + '::test2', 'input')",
            "        self.assert_in(\"U input/file1\", output)",
            "",
            "    def test_file_status_rc_cache_mode(self):",
            "        \"\"\"test that files get rechunked unconditionally in rechunk,ctime cache mode\"\"\"",
            "        self.create_regular_file('file1', size=10)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=10)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--files-cache=rechunk,ctime', self.repository_location + '::test1', 'input')",
            "        # no changes here, but this mode rechunks unconditionally",
            "        output = self.cmd('create', '--list', '--files-cache=rechunk,ctime', self.repository_location + '::test2', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "",
            "    def test_file_status_excluded(self):",
            "        \"\"\"test that excluded paths are listed\"\"\"",
            "",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        if has_lchflags:",
            "            self.create_regular_file('file3', size=1024 * 80)",
            "            platform.set_flags(os.path.join(self.input_path, 'file3'), stat.UF_NODUMP)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--exclude-nodump', self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "        self.assert_in(\"A input/file2\", output)",
            "        if has_lchflags:",
            "            self.assert_in(\"x input/file3\", output)",
            "        # should find second file as excluded",
            "        output = self.cmd('create', '--list', '--exclude-nodump', self.repository_location + '::test1', 'input', '--exclude', '*/file2')",
            "        self.assert_in(\"U input/file1\", output)",
            "        self.assert_in(\"x input/file2\", output)",
            "        if has_lchflags:",
            "            self.assert_in(\"x input/file3\", output)",
            "",
            "    def test_create_json(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        create_info = json.loads(self.cmd('create', '--json', self.repository_location + '::test', 'input'))",
            "        # The usual keys",
            "        assert 'encryption' in create_info",
            "        assert 'repository' in create_info",
            "        assert 'cache' in create_info",
            "        assert 'last_modified' in create_info['repository']",
            "",
            "        archive = create_info['archive']",
            "        assert archive['name'] == 'test'",
            "        assert isinstance(archive['command_line'], list)",
            "        assert isinstance(archive['duration'], float)",
            "        assert len(archive['id']) == 64",
            "        assert 'stats' in archive",
            "",
            "    def test_create_topical(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        # no listing by default",
            "        output = self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.assert_not_in('file1', output)",
            "        # shouldn't be listed even if unchanged",
            "        output = self.cmd('create', self.repository_location + '::test0', 'input')",
            "        self.assert_not_in('file1', output)",
            "        # should list the file as unchanged",
            "        output = self.cmd('create', '--list', '--filter=U', self.repository_location + '::test1', 'input')",
            "        self.assert_in('file1', output)",
            "        # should *not* list the file as changed",
            "        output = self.cmd('create', '--list', '--filter=AM', self.repository_location + '::test2', 'input')",
            "        self.assert_not_in('file1', output)",
            "        # change the file",
            "        self.create_regular_file('file1', size=1024 * 100)",
            "        # should list the file as changed",
            "        output = self.cmd('create', '--list', '--filter=AM', self.repository_location + '::test3', 'input')",
            "        self.assert_in('file1', output)",
            "",
            "    def test_create_read_special_broken_symlink(self):",
            "        os.symlink('somewhere doesnt exist', os.path.join(self.input_path, 'link'))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        archive = self.repository_location + '::test'",
            "        self.cmd('create', '--read-special', archive, 'input')",
            "        output = self.cmd('list', archive)",
            "        assert 'input/link -> somewhere doesnt exist' in output",
            "",
            "    # def test_cmdline_compatibility(self):",
            "    #    self.create_regular_file('file1', size=1024 * 80)",
            "    #    self.cmd('init', '--encryption=repokey', self.repository_location)",
            "    #    self.cmd('create', self.repository_location + '::test', 'input')",
            "    #    output = self.cmd('foo', self.repository_location, '--old')",
            "    #    self.assert_in('\"--old\" has been deprecated. Use \"--new\" instead', output)",
            "",
            "    def test_prune_repository(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test2', src_dir)",
            "        # these are not really a checkpoints, but they look like some:",
            "        self.cmd('create', self.repository_location + '::test3.checkpoint', src_dir)",
            "        self.cmd('create', self.repository_location + '::test3.checkpoint.1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test4.checkpoint', src_dir)",
            "        output = self.cmd('prune', '--list', '--dry-run', self.repository_location, '--keep-daily=2')",
            "        assert re.search(r'Would prune:\\s+test1', output)",
            "        # must keep the latest non-checkpoint archive:",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+test2', output)",
            "        # must keep the latest checkpoint archive:",
            "        assert re.search(r'Keeping checkpoint archive:\\s+test4.checkpoint', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('test1', output)",
            "        self.assert_in('test2', output)",
            "        self.assert_in('test3.checkpoint', output)",
            "        self.assert_in('test3.checkpoint.1', output)",
            "        self.assert_in('test4.checkpoint', output)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test1', output)",
            "        # the latest non-checkpoint archive must be still there:",
            "        self.assert_in('test2', output)",
            "        # only the latest checkpoint archive must still be there:",
            "        self.assert_not_in('test3.checkpoint', output)",
            "        self.assert_not_in('test3.checkpoint.1', output)",
            "        self.assert_in('test4.checkpoint', output)",
            "        # now we supercede the latest checkpoint by a successful backup:",
            "        self.cmd('create', self.repository_location + '::test5', src_dir)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2')",
            "        output = self.cmd('list', self.repository_location)",
            "        # all checkpoints should be gone now:",
            "        self.assert_not_in('checkpoint', output)",
            "        # the latest archive must be still there",
            "        self.assert_in('test5', output)",
            "",
            "    def test_prune_repository_save_space(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test2', src_dir)",
            "        output = self.cmd('prune', '--list', '--stats', '--dry-run', self.repository_location, '--keep-daily=2')",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+test2', output)",
            "        assert re.search(r'Would prune:\\s+test1', output)",
            "        self.assert_in('Deleted data:', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('test1', output)",
            "        self.assert_in('test2', output)",
            "        self.cmd('prune', '--save-space', self.repository_location, '--keep-daily=2')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test1', output)",
            "        self.assert_in('test2', output)",
            "",
            "    def test_prune_repository_prefix(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::foo-2015-08-12-10:00', src_dir)",
            "        self.cmd('create', self.repository_location + '::foo-2015-08-12-20:00', src_dir)",
            "        self.cmd('create', self.repository_location + '::bar-2015-08-12-10:00', src_dir)",
            "        self.cmd('create', self.repository_location + '::bar-2015-08-12-20:00', src_dir)",
            "        output = self.cmd('prune', '--list', '--dry-run', self.repository_location, '--keep-daily=2', '--prefix=foo-')",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+foo-2015-08-12-20:00', output)",
            "        assert re.search(r'Would prune:\\s+foo-2015-08-12-10:00', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('foo-2015-08-12-10:00', output)",
            "        self.assert_in('foo-2015-08-12-20:00', output)",
            "        self.assert_in('bar-2015-08-12-10:00', output)",
            "        self.assert_in('bar-2015-08-12-20:00', output)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2', '--prefix=foo-')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('foo-2015-08-12-10:00', output)",
            "        self.assert_in('foo-2015-08-12-20:00', output)",
            "        self.assert_in('bar-2015-08-12-10:00', output)",
            "        self.assert_in('bar-2015-08-12-20:00', output)",
            "",
            "    def test_prune_repository_glob(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-10:00-foo', src_dir)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-20:00-foo', src_dir)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-10:00-bar', src_dir)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-20:00-bar', src_dir)",
            "        output = self.cmd('prune', '--list', '--dry-run', self.repository_location, '--keep-daily=2', '--glob-archives=2015-*-foo')",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+2015-08-12-20:00-foo', output)",
            "        assert re.search(r'Would prune:\\s+2015-08-12-10:00-foo', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('2015-08-12-10:00-foo', output)",
            "        self.assert_in('2015-08-12-20:00-foo', output)",
            "        self.assert_in('2015-08-12-10:00-bar', output)",
            "        self.assert_in('2015-08-12-20:00-bar', output)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2', '--glob-archives=2015-*-foo')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('2015-08-12-10:00-foo', output)",
            "        self.assert_in('2015-08-12-20:00-foo', output)",
            "        self.assert_in('2015-08-12-10:00-bar', output)",
            "        self.assert_in('2015-08-12-20:00-bar', output)",
            "",
            "    def test_list_prefix(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test-1', src_dir)",
            "        self.cmd('create', self.repository_location + '::something-else-than-test-1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test-2', src_dir)",
            "        output = self.cmd('list', '--prefix=test-', self.repository_location)",
            "        self.assert_in('test-1', output)",
            "        self.assert_in('test-2', output)",
            "        self.assert_not_in('something-else', output)",
            "",
            "    def test_list_format(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', test_archive, src_dir)",
            "        output_warn = self.cmd('list', '--list-format', '-', test_archive)",
            "        self.assert_in('--list-format\" has been deprecated.', output_warn)",
            "        output_1 = self.cmd('list', test_archive)",
            "        output_2 = self.cmd('list', '--format', '{mode} {user:6} {group:6} {size:8d} {mtime} {path}{extra}{NEWLINE}', test_archive)",
            "        output_3 = self.cmd('list', '--format', '{mtime:%s} {path}{NL}', test_archive)",
            "        self.assertEqual(output_1, output_2)",
            "        self.assertNotEqual(output_1, output_3)",
            "",
            "    def test_list_repository_format(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--comment', 'comment 1', self.repository_location + '::test-1', src_dir)",
            "        self.cmd('create', '--comment', 'comment 2', self.repository_location + '::test-2', src_dir)",
            "        output_1 = self.cmd('list', self.repository_location)",
            "        output_2 = self.cmd('list', '--format', '{archive:<36} {time} [{id}]{NL}', self.repository_location)",
            "        self.assertEqual(output_1, output_2)",
            "        output_1 = self.cmd('list', '--short', self.repository_location)",
            "        self.assertEqual(output_1, 'test-1\\ntest-2\\n')",
            "        output_1 = self.cmd('list', '--format', '{barchive}/', self.repository_location)",
            "        self.assertEqual(output_1, 'test-1/test-2/')",
            "        output_3 = self.cmd('list', '--format', '{name} {comment}{NL}', self.repository_location)",
            "        self.assert_in('test-1 comment 1\\n', output_3)",
            "        self.assert_in('test-2 comment 2\\n', output_3)",
            "",
            "    def test_list_hash(self):",
            "        self.create_regular_file('empty_file', size=0)",
            "        self.create_regular_file('amb', contents=b'a' * 1000000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', test_archive, 'input')",
            "        output = self.cmd('list', '--format', '{sha256} {path}{NL}', test_archive)",
            "        assert \"cdc76e5c9914fb9281a1c7e284d73e67f1809a48a497200e046d39ccc7112cd0 input/amb\" in output",
            "        assert \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 input/empty_file\" in output",
            "",
            "    def test_list_chunk_counts(self):",
            "        self.create_regular_file('empty_file', size=0)",
            "        self.create_regular_file('two_chunks')",
            "        with open(os.path.join(self.input_path, 'two_chunks'), 'wb') as fd:",
            "            fd.write(b'abba' * 2000000)",
            "            fd.write(b'baab' * 2000000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', test_archive, 'input')",
            "        output = self.cmd('list', '--format', '{num_chunks} {unique_chunks} {path}{NL}', test_archive)",
            "        assert \"0 0 input/empty_file\" in output",
            "        assert \"2 2 input/two_chunks\" in output",
            "",
            "    def test_list_size(self):",
            "        self.create_regular_file('compressible_file', size=10000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', '-C', 'lz4', test_archive, 'input')",
            "        output = self.cmd('list', '--format', '{size} {csize} {dsize} {dcsize} {path}{NL}', test_archive)",
            "        size, csize, dsize, dcsize, path = output.split(\"\\n\")[1].split(\" \")",
            "        assert int(csize) < int(size)",
            "        assert int(dcsize) < int(dsize)",
            "        assert int(dsize) <= int(size)",
            "        assert int(dcsize) <= int(csize)",
            "",
            "    def test_list_json(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        list_repo = json.loads(self.cmd('list', '--json', self.repository_location))",
            "        repository = list_repo['repository']",
            "        assert len(repository['id']) == 64",
            "        assert datetime.strptime(repository['last_modified'], ISO_FORMAT)  # must not raise",
            "        assert list_repo['encryption']['mode'] == 'repokey'",
            "        assert 'keyfile' not in list_repo['encryption']",
            "        archive0 = list_repo['archives'][0]",
            "        assert datetime.strptime(archive0['time'], ISO_FORMAT)  # must not raise",
            "",
            "        list_archive = self.cmd('list', '--json-lines', self.repository_location + '::test')",
            "        items = [json.loads(s) for s in list_archive.splitlines()]",
            "        assert len(items) == 2",
            "        file1 = items[1]",
            "        assert file1['path'] == 'input/file1'",
            "        assert file1['size'] == 81920",
            "        assert datetime.strptime(file1['mtime'], ISO_FORMAT)  # must not raise",
            "",
            "        list_archive = self.cmd('list', '--json-lines', '--format={sha256}', self.repository_location + '::test')",
            "        items = [json.loads(s) for s in list_archive.splitlines()]",
            "        assert len(items) == 2",
            "        file1 = items[1]",
            "        assert file1['path'] == 'input/file1'",
            "        assert file1['sha256'] == 'b2915eb69f260d8d3c25249195f2c8f4f716ea82ec760ae929732c0262442b2b'",
            "",
            "    def test_list_json_args(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('list', '--json-lines', self.repository_location, exit_code=2)",
            "        self.cmd('list', '--json', self.repository_location + '::archive', exit_code=2)",
            "",
            "    def test_log_json(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        log = self.cmd('create', '--log-json', self.repository_location + '::test', 'input', '--list', '--debug')",
            "        messages = {}  # type -> message, one of each kind",
            "        for line in log.splitlines():",
            "            msg = json.loads(line)",
            "            messages[msg['type']] = msg",
            "",
            "        file_status = messages['file_status']",
            "        assert 'status' in file_status",
            "        assert file_status['path'].startswith('input')",
            "",
            "        log_message = messages['log_message']",
            "        assert isinstance(log_message['time'], float)",
            "        assert log_message['levelname'] == 'DEBUG'  # there should only be DEBUG messages",
            "        assert isinstance(log_message['message'], str)",
            "",
            "    def test_debug_profile(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input', '--debug-profile=create.prof')",
            "        self.cmd('debug', 'convert-profile', 'create.prof', 'create.pyprof')",
            "        stats = pstats.Stats('create.pyprof')",
            "        stats.strip_dirs()",
            "        stats.sort_stats('cumtime')",
            "",
            "        self.cmd('create', self.repository_location + '::test2', 'input', '--debug-profile=create.pyprof')",
            "        stats = pstats.Stats('create.pyprof')  # Only do this on trusted data!",
            "        stats.strip_dirs()",
            "        stats.sort_stats('cumtime')",
            "",
            "    def test_common_options(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        log = self.cmd('--debug', 'create', self.repository_location + '::test', 'input')",
            "        assert 'security: read previous location' in log",
            "",
            "    def _get_sizes(self, compression, compressible, size=10000):",
            "        if compressible:",
            "            contents = b'X' * size",
            "        else:",
            "            contents = os.urandom(size)",
            "        self.create_regular_file('file', contents=contents)",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        archive = self.repository_location + '::test'",
            "        self.cmd('create', '-C', compression, archive, 'input')",
            "        output = self.cmd('list', '--format', '{size} {csize} {path}{NL}', archive)",
            "        size, csize, path = output.split(\"\\n\")[1].split(\" \")",
            "        return int(size), int(csize)",
            "",
            "    def test_compression_none_compressible(self):",
            "        size, csize = self._get_sizes('none', compressible=True)",
            "        assert csize >= size",
            "        assert csize == size + 3",
            "",
            "    def test_compression_none_uncompressible(self):",
            "        size, csize = self._get_sizes('none', compressible=False)",
            "        assert csize >= size",
            "        assert csize == size + 3",
            "",
            "    def test_compression_zlib_compressible(self):",
            "        size, csize = self._get_sizes('zlib', compressible=True)",
            "        assert csize < size * 0.1",
            "        assert csize == 35",
            "",
            "    def test_compression_zlib_uncompressible(self):",
            "        size, csize = self._get_sizes('zlib', compressible=False)",
            "        assert csize >= size",
            "",
            "    def test_compression_auto_compressible(self):",
            "        size, csize = self._get_sizes('auto,zlib', compressible=True)",
            "        assert csize < size * 0.1",
            "        assert csize == 35  # same as compression 'zlib'",
            "",
            "    def test_compression_auto_uncompressible(self):",
            "        size, csize = self._get_sizes('auto,zlib', compressible=False)",
            "        assert csize >= size",
            "        assert csize == size + 3  # same as compression 'none'",
            "",
            "    def test_compression_lz4_compressible(self):",
            "        size, csize = self._get_sizes('lz4', compressible=True)",
            "        assert csize < size * 0.1",
            "",
            "    def test_compression_lz4_uncompressible(self):",
            "        size, csize = self._get_sizes('lz4', compressible=False)",
            "        assert csize >= size",
            "",
            "    def test_compression_lzma_compressible(self):",
            "        size, csize = self._get_sizes('lzma', compressible=True)",
            "        assert csize < size * 0.1",
            "",
            "    def test_compression_lzma_uncompressible(self):",
            "        size, csize = self._get_sizes('lzma', compressible=False)",
            "        assert csize >= size",
            "",
            "    def test_change_passphrase(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        os.environ['BORG_NEW_PASSPHRASE'] = 'newpassphrase'",
            "        # here we have both BORG_PASSPHRASE and BORG_NEW_PASSPHRASE set:",
            "        self.cmd('change-passphrase', self.repository_location)",
            "        os.environ['BORG_PASSPHRASE'] = 'newpassphrase'",
            "        self.cmd('list', self.repository_location)",
            "",
            "    def test_break_lock(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('break-lock', self.repository_location)",
            "",
            "    def test_usage(self):",
            "        self.cmd()",
            "        self.cmd('-h')",
            "",
            "    def test_help(self):",
            "        assert 'Borg' in self.cmd('help')",
            "        assert 'patterns' in self.cmd('help', 'patterns')",
            "        assert 'Initialize' in self.cmd('help', 'init')",
            "        assert 'positional arguments' not in self.cmd('help', 'init', '--epilog-only')",
            "        assert 'This command initializes' not in self.cmd('help', 'init', '--usage-only')",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse(self):",
            "        def has_noatime(some_file):",
            "            atime_before = os.stat(some_file).st_atime_ns",
            "            try:",
            "                os.close(os.open(some_file, flags_noatime))",
            "            except PermissionError:",
            "                return False",
            "            else:",
            "                atime_after = os.stat(some_file).st_atime_ns",
            "                noatime_used = flags_noatime != flags_normal",
            "                return noatime_used and atime_before == atime_after",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_test_files()",
            "        have_noatime = has_noatime('input/file1')",
            "        self.cmd('create', '--exclude-nodump', self.repository_location + '::archive', 'input')",
            "        self.cmd('create', '--exclude-nodump', self.repository_location + '::archive2', 'input')",
            "        if has_lchflags:",
            "            # remove the file we did not backup, so input and output become equal",
            "            os.remove(os.path.join('input', 'flagfile'))",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        # mount the whole repository, archive contents shall show up in archivename subdirs of mountpoint:",
            "        with self.fuse_mount(self.repository_location, mountpoint):",
            "            # bsdflags are not supported by the FUSE mount",
            "            # we also ignore xattrs here, they are tested separately",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive', 'input'),",
            "                                   ignore_bsdflags=True, ignore_xattrs=True)",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive2', 'input'),",
            "                                   ignore_bsdflags=True, ignore_xattrs=True)",
            "        # mount only 1 archive, its contents shall show up directly in mountpoint:",
            "        with self.fuse_mount(self.repository_location + '::archive', mountpoint):",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'input'),",
            "                                   ignore_bsdflags=True, ignore_xattrs=True)",
            "            # regular file",
            "            in_fn = 'input/file1'",
            "            out_fn = os.path.join(mountpoint, 'input', 'file1')",
            "            # stat",
            "            sti1 = os.stat(in_fn)",
            "            sto1 = os.stat(out_fn)",
            "            assert sti1.st_mode == sto1.st_mode",
            "            assert sti1.st_uid == sto1.st_uid",
            "            assert sti1.st_gid == sto1.st_gid",
            "            assert sti1.st_size == sto1.st_size",
            "            if have_noatime:",
            "                assert sti1.st_atime == sto1.st_atime",
            "            assert sti1.st_ctime == sto1.st_ctime",
            "            assert sti1.st_mtime == sto1.st_mtime",
            "            # note: there is another hardlink to this, see below",
            "            assert sti1.st_nlink == sto1.st_nlink == 2",
            "            # read",
            "            with open(in_fn, 'rb') as in_f, open(out_fn, 'rb') as out_f:",
            "                assert in_f.read() == out_f.read()",
            "            # hardlink (to 'input/file1')",
            "            if are_hardlinks_supported():",
            "                in_fn = 'input/hardlink'",
            "                out_fn = os.path.join(mountpoint, 'input', 'hardlink')",
            "                sti2 = os.stat(in_fn)",
            "                sto2 = os.stat(out_fn)",
            "                assert sti2.st_nlink == sto2.st_nlink == 2",
            "                assert sto1.st_ino == sto2.st_ino",
            "            # symlink",
            "            if are_symlinks_supported():",
            "                in_fn = 'input/link1'",
            "                out_fn = os.path.join(mountpoint, 'input', 'link1')",
            "                sti = os.stat(in_fn, follow_symlinks=False)",
            "                sto = os.stat(out_fn, follow_symlinks=False)",
            "                assert sti.st_size == len('somewhere')",
            "                assert sto.st_size == len('somewhere')",
            "                assert stat.S_ISLNK(sti.st_mode)",
            "                assert stat.S_ISLNK(sto.st_mode)",
            "                assert os.readlink(in_fn) == os.readlink(out_fn)",
            "            # FIFO",
            "            if are_fifos_supported():",
            "                out_fn = os.path.join(mountpoint, 'input', 'fifo1')",
            "                sto = os.stat(out_fn)",
            "                assert stat.S_ISFIFO(sto.st_mode)",
            "            # list/read xattrs",
            "            try:",
            "                in_fn = 'input/fusexattr'",
            "                out_fn = os.path.join(mountpoint, 'input', 'fusexattr')",
            "                if not xattr.XATTR_FAKEROOT and xattr.is_enabled(self.input_path):",
            "                    assert sorted(no_selinux(xattr.listxattr(out_fn))) == ['user.empty', 'user.foo', ]",
            "                    assert xattr.getxattr(out_fn, 'user.foo') == b'bar'",
            "                    # Special case: getxattr returns None (not b'') when reading an empty xattr.",
            "                    assert xattr.getxattr(out_fn, 'user.empty') is None",
            "                else:",
            "                    assert xattr.listxattr(out_fn) == []",
            "                    try:",
            "                        xattr.getxattr(out_fn, 'user.foo')",
            "                    except OSError as e:",
            "                        assert e.errno == llfuse.ENOATTR",
            "                    else:",
            "                        assert False, \"expected OSError(ENOATTR), but no error was raised\"",
            "            except OSError as err:",
            "                if sys.platform.startswith(('freebsd', )) and err.errno == errno.ENOTSUP:",
            "                    # some systems have no xattr support on FUSE",
            "                    pass",
            "                else:",
            "                    raise",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_versions_view(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('test', contents=b'first')",
            "        if are_hardlinks_supported():",
            "            self.create_regular_file('hardlink1', contents=b'123456')",
            "            os.link('input/hardlink1', 'input/hardlink2')",
            "            os.link('input/hardlink1', 'input/hardlink3')",
            "        self.cmd('create', self.repository_location + '::archive1', 'input')",
            "        self.create_regular_file('test', contents=b'second')",
            "        self.cmd('create', self.repository_location + '::archive2', 'input')",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        # mount the whole repository, archive contents shall show up in versioned view:",
            "        with self.fuse_mount(self.repository_location, mountpoint, '-o', 'versions'):",
            "            path = os.path.join(mountpoint, 'input', 'test')  # filename shows up as directory ...",
            "            files = os.listdir(path)",
            "            assert all(f.startswith('test.') for f in files)  # ... with files test.xxxxx in there",
            "            assert {b'first', b'second'} == {open(os.path.join(path, f), 'rb').read() for f in files}",
            "            if are_hardlinks_supported():",
            "                hl1 = os.path.join(mountpoint, 'input', 'hardlink1', 'hardlink1.00001')",
            "                hl2 = os.path.join(mountpoint, 'input', 'hardlink2', 'hardlink2.00001')",
            "                hl3 = os.path.join(mountpoint, 'input', 'hardlink3', 'hardlink3.00001')",
            "                assert os.stat(hl1).st_ino == os.stat(hl2).st_ino == os.stat(hl3).st_ino",
            "                assert open(hl3, 'rb').read() == b'123456'",
            "        # similar again, but exclude the hardlink master:",
            "        with self.fuse_mount(self.repository_location, mountpoint, '-o', 'versions', '-e', 'input/hardlink1'):",
            "            if are_hardlinks_supported():",
            "                hl2 = os.path.join(mountpoint, 'input', 'hardlink2', 'hardlink2.00001')",
            "                hl3 = os.path.join(mountpoint, 'input', 'hardlink3', 'hardlink3.00001')",
            "                assert os.stat(hl2).st_ino == os.stat(hl3).st_ino",
            "                assert open(hl3, 'rb').read() == b'123456'",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_allow_damaged_files(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive')",
            "        # Get rid of a chunk and repair it",
            "        archive, repository = self.open_archive('archive')",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith('testsuite/archiver.py'):",
            "                    repository.delete(item.chunks[-1].id)",
            "                    path = item.path  # store full path for later",
            "                    break",
            "            else:",
            "                assert False  # missed the file",
            "            repository.commit()",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        with self.fuse_mount(self.repository_location + '::archive', mountpoint):",
            "            with pytest.raises(OSError) as excinfo:",
            "                open(os.path.join(mountpoint, path))",
            "            assert excinfo.value.errno == errno.EIO",
            "        with self.fuse_mount(self.repository_location + '::archive', mountpoint, '-o', 'allow_damaged_files'):",
            "            open(os.path.join(mountpoint, path)).close()",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_mount_options(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('arch11')",
            "        self.create_src_archive('arch12')",
            "        self.create_src_archive('arch21')",
            "        self.create_src_archive('arch22')",
            "",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--first=2', '--sort=name'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch11', 'arch12']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--last=2', '--sort=name'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch21', 'arch22']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=arch1'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch11', 'arch12']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=arch2'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch21', 'arch22']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=arch'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch11', 'arch12', 'arch21', 'arch22']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=nope'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == []",
            "",
            "    def verify_aes_counter_uniqueness(self, method):",
            "        seen = set()  # Chunks already seen",
            "        used = set()  # counter values already used",
            "",
            "        def verify_uniqueness():",
            "            with Repository(self.repository_path) as repository:",
            "                for id, _ in repository.open_index(repository.get_transaction_id()).iteritems():",
            "                    data = repository.get(id)",
            "                    hash = sha256(data).digest()",
            "                    if hash not in seen:",
            "                        seen.add(hash)",
            "                        num_blocks = num_cipher_blocks(len(data) - 41)",
            "                        nonce = bytes_to_long(data[33:41])",
            "                        for counter in range(nonce, nonce + num_blocks):",
            "                            self.assert_not_in(counter, used)",
            "                            used.add(counter)",
            "",
            "        self.create_test_files()",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=' + method, self.repository_location)",
            "        verify_uniqueness()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        verify_uniqueness()",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        verify_uniqueness()",
            "        self.cmd('delete', self.repository_location + '::test.2')",
            "        verify_uniqueness()",
            "",
            "    def test_aes_counter_uniqueness_keyfile(self):",
            "        self.verify_aes_counter_uniqueness('keyfile')",
            "",
            "    def test_aes_counter_uniqueness_passphrase(self):",
            "        self.verify_aes_counter_uniqueness('repokey')",
            "",
            "    def test_debug_dump_archive_items(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            output = self.cmd('debug', 'dump-archive-items', self.repository_location + '::test')",
            "        output_dir = sorted(os.listdir('output'))",
            "        assert len(output_dir) > 0 and output_dir[0].startswith('000000_')",
            "        assert 'Done.' in output",
            "",
            "    def test_debug_dump_repo_objs(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            output = self.cmd('debug', 'dump-repo-objs', self.repository_location)",
            "        output_dir = sorted(os.listdir('output'))",
            "        assert len(output_dir) > 0 and output_dir[0].startswith('000000_')",
            "        assert 'Done.' in output",
            "",
            "    def test_debug_put_get_delete_obj(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        data = b'some data'",
            "        hexkey = sha256(data).hexdigest()",
            "        self.create_regular_file('file', contents=data)",
            "        output = self.cmd('debug', 'put-obj', self.repository_location, 'input/file')",
            "        assert hexkey in output",
            "        output = self.cmd('debug', 'get-obj', self.repository_location, hexkey, 'output/file')",
            "        assert hexkey in output",
            "        with open('output/file', 'rb') as f:",
            "            data_read = f.read()",
            "        assert data == data_read",
            "        output = self.cmd('debug', 'delete-obj', self.repository_location, hexkey)",
            "        assert \"deleted\" in output",
            "        output = self.cmd('debug', 'delete-obj', self.repository_location, hexkey)",
            "        assert \"not found\" in output",
            "        output = self.cmd('debug', 'delete-obj', self.repository_location, 'invalid')",
            "        assert \"is invalid\" in output",
            "",
            "    def test_init_interrupt(self):",
            "        def raise_eof(*args):",
            "            raise EOFError",
            "",
            "        with patch.object(KeyfileKeyBase, 'create', raise_eof):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location, exit_code=1)",
            "        assert not os.path.exists(self.repository_location)",
            "",
            "    def test_init_requires_encryption_option(self):",
            "        self.cmd('init', self.repository_location, exit_code=2)",
            "",
            "    def test_init_nested_repositories(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('init', '--encryption=repokey', self.repository_location + '/nested', exit_code=2)",
            "        else:",
            "            with pytest.raises(Repository.AlreadyExists):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location + '/nested')",
            "",
            "    def check_cache(self):",
            "        # First run a regular borg check",
            "        self.cmd('check', self.repository_location)",
            "        # Then check that the cache on disk matches exactly what's in the repo.",
            "        with self.open_repository() as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest, sync=False) as cache:",
            "                original_chunks = cache.chunks",
            "            Cache.destroy(repository)",
            "            with Cache(repository, key, manifest) as cache:",
            "                correct_chunks = cache.chunks",
            "        assert original_chunks is not correct_chunks",
            "        seen = set()",
            "        for id, (refcount, size, csize) in correct_chunks.iteritems():",
            "            o_refcount, o_size, o_csize = original_chunks[id]",
            "            assert refcount == o_refcount",
            "            assert size == o_size",
            "            assert csize == o_csize",
            "            seen.add(id)",
            "        for id, (refcount, size, csize) in original_chunks.iteritems():",
            "            assert id in seen",
            "",
            "    def test_check_cache(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with self.open_repository() as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest, sync=False) as cache:",
            "                cache.begin_txn()",
            "                cache.chunks.incref(list(cache.chunks.iteritems())[0][0])",
            "                cache.commit()",
            "        with pytest.raises(AssertionError):",
            "            self.check_cache()",
            "",
            "    def test_recreate_target_rc(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('recreate', self.repository_location, '--target=asdf', exit_code=2)",
            "        assert 'Need to specify single archive' in output",
            "",
            "    def test_recreate_target(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.check_cache()",
            "        archive = self.repository_location + '::test0'",
            "        self.cmd('create', archive, 'input')",
            "        self.check_cache()",
            "        original_archive = self.cmd('list', self.repository_location)",
            "        self.cmd('recreate', archive, 'input/dir2', '-e', 'input/dir2/file3', '--target=new-archive')",
            "        self.check_cache()",
            "        archives = self.cmd('list', self.repository_location)",
            "        assert original_archive in archives",
            "        assert 'new-archive' in archives",
            "",
            "        archive = self.repository_location + '::new-archive'",
            "        listing = self.cmd('list', '--short', archive)",
            "        assert 'file1' not in listing",
            "        assert 'dir2/file2' in listing",
            "        assert 'dir2/file3' not in listing",
            "",
            "    def test_recreate_basic(self):",
            "        self.create_test_files()",
            "        self.create_regular_file('dir2/file3', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        archive = self.repository_location + '::test0'",
            "        self.cmd('create', archive, 'input')",
            "        self.cmd('recreate', archive, 'input/dir2', '-e', 'input/dir2/file3')",
            "        self.check_cache()",
            "        listing = self.cmd('list', '--short', archive)",
            "        assert 'file1' not in listing",
            "        assert 'dir2/file2' in listing",
            "        assert 'dir2/file3' not in listing",
            "",
            "    @pytest.mark.skipif(not are_hardlinks_supported(), reason='hardlinks not supported')",
            "    def test_recreate_subtree_hardlinks(self):",
            "        # This is essentially the same problem set as in test_extract_hardlinks",
            "        self._extract_hardlinks_setup()",
            "        self.cmd('create', self.repository_location + '::test2', 'input')",
            "        self.cmd('recreate', self.repository_location + '::test', 'input/dir1')",
            "        self.check_cache()",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test2')",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 4",
            "",
            "    def test_recreate_rechunkify(self):",
            "        with open(os.path.join(self.input_path, 'large_file'), 'wb') as fd:",
            "            fd.write(b'a' * 280)",
            "            fd.write(b'b' * 280)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--chunker-params', '7,9,8,128', self.repository_location + '::test1', 'input')",
            "        self.cmd('create', self.repository_location + '::test2', 'input', '--files-cache=disabled')",
            "        list = self.cmd('list', self.repository_location + '::test1', 'input/large_file',",
            "                        '--format', '{num_chunks} {unique_chunks}')",
            "        num_chunks, unique_chunks = map(int, list.split(' '))",
            "        # test1 and test2 do not deduplicate",
            "        assert num_chunks == unique_chunks",
            "        self.cmd('recreate', self.repository_location, '--chunker-params', 'default')",
            "        self.check_cache()",
            "        # test1 and test2 do deduplicate after recreate",
            "        assert int(self.cmd('list', self.repository_location + '::test1', 'input/large_file', '--format={size}'))",
            "        assert not int(self.cmd('list', self.repository_location + '::test1', 'input/large_file',",
            "                                '--format', '{unique_chunks}'))",
            "",
            "    def test_recreate_recompress(self):",
            "        self.create_regular_file('compressible', size=10000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input', '-C', 'none')",
            "        file_list = self.cmd('list', self.repository_location + '::test', 'input/compressible',",
            "                             '--format', '{size} {csize} {sha256}')",
            "        size, csize, sha256_before = file_list.split(' ')",
            "        assert int(csize) >= int(size)  # >= due to metadata overhead",
            "        self.cmd('recreate', self.repository_location, '-C', 'lz4', '--recompress')",
            "        self.check_cache()",
            "        file_list = self.cmd('list', self.repository_location + '::test', 'input/compressible',",
            "                             '--format', '{size} {csize} {sha256}')",
            "        size, csize, sha256_after = file_list.split(' ')",
            "        assert int(csize) < int(size)",
            "        assert sha256_before == sha256_after",
            "",
            "    def test_recreate_dry_run(self):",
            "        self.create_regular_file('compressible', size=10000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        archives_before = self.cmd('list', self.repository_location + '::test')",
            "        self.cmd('recreate', self.repository_location, '-n', '-e', 'input/compressible')",
            "        self.check_cache()",
            "        archives_after = self.cmd('list', self.repository_location + '::test')",
            "        assert archives_after == archives_before",
            "",
            "    def test_recreate_skips_nothing_to_do(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        info_before = self.cmd('info', self.repository_location + '::test')",
            "        self.cmd('recreate', self.repository_location, '--chunker-params', 'default')",
            "        self.check_cache()",
            "        info_after = self.cmd('info', self.repository_location + '::test')",
            "        assert info_before == info_after  # includes archive ID",
            "",
            "    def test_with_lock(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        lock_path = os.path.join(self.repository_path, 'lock.exclusive')",
            "        cmd = 'python3', '-c', 'import os, sys; sys.exit(42 if os.path.exists(\"%s\") else 23)' % lock_path",
            "        self.cmd('with-lock', self.repository_location, *cmd, fork=True, exit_code=42)",
            "",
            "    def test_recreate_list_output(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=0)",
            "        self.create_regular_file('file2', size=0)",
            "        self.create_regular_file('file3', size=0)",
            "        self.create_regular_file('file4', size=0)",
            "        self.create_regular_file('file5', size=0)",
            "",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "        output = self.cmd('recreate', '--list', '--info', self.repository_location + '::test', '-e', 'input/file2')",
            "        self.check_cache()",
            "        self.assert_in(\"input/file1\", output)",
            "        self.assert_in(\"x input/file2\", output)",
            "",
            "        output = self.cmd('recreate', '--list', self.repository_location + '::test', '-e', 'input/file3')",
            "        self.check_cache()",
            "        self.assert_in(\"input/file1\", output)",
            "        self.assert_in(\"x input/file3\", output)",
            "",
            "        output = self.cmd('recreate', self.repository_location + '::test', '-e', 'input/file4')",
            "        self.check_cache()",
            "        self.assert_not_in(\"input/file1\", output)",
            "        self.assert_not_in(\"x input/file4\", output)",
            "",
            "        output = self.cmd('recreate', '--info', self.repository_location + '::test', '-e', 'input/file5')",
            "        self.check_cache()",
            "        self.assert_not_in(\"input/file1\", output)",
            "        self.assert_not_in(\"x input/file5\", output)",
            "",
            "    def test_bad_filters(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('delete', '--first', '1', '--last', '1', self.repository_location, fork=True, exit_code=2)",
            "",
            "    def test_key_export_keyfile(self):",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "        repo_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('key', 'export', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert export_contents.startswith('BORG_KEY ' + bin_to_hex(repo_id) + '\\n')",
            "",
            "        key_file = self.keys_path + '/' + os.listdir(self.keys_path)[0]",
            "",
            "        with open(key_file, 'r') as fd:",
            "            key_contents = fd.read()",
            "",
            "        assert key_contents == export_contents",
            "",
            "        os.unlink(key_file)",
            "",
            "        self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "        with open(key_file, 'r') as fd:",
            "            key_contents2 = fd.read()",
            "",
            "        assert key_contents2 == key_contents",
            "",
            "    def test_key_export_repokey(self):",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'repokey')",
            "        repo_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('key', 'export', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert export_contents.startswith('BORG_KEY ' + bin_to_hex(repo_id) + '\\n')",
            "",
            "        with Repository(self.repository_path) as repository:",
            "            repo_key = RepoKey(repository)",
            "            repo_key.load(None, Passphrase.env_passphrase())",
            "",
            "        backup_key = KeyfileKey(key.TestKey.MockRepository())",
            "        backup_key.load(export_file, Passphrase.env_passphrase())",
            "",
            "        assert repo_key.enc_key == backup_key.enc_key",
            "",
            "        with Repository(self.repository_path) as repository:",
            "            repository.save_key(b'')",
            "",
            "        self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "        with Repository(self.repository_path) as repository:",
            "            repo_key2 = RepoKey(repository)",
            "            repo_key2.load(None, Passphrase.env_passphrase())",
            "",
            "        assert repo_key2.enc_key == repo_key2.enc_key",
            "",
            "    def test_key_export_qr(self):",
            "        export_file = self.output_path + '/exported.html'",
            "        self.cmd('init', self.repository_location, '--encryption', 'repokey')",
            "        repo_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('key', 'export', '--qr-html', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r', encoding='utf-8') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert bin_to_hex(repo_id) in export_contents",
            "        assert export_contents.startswith('<!doctype html>')",
            "        assert export_contents.endswith('</html>')",
            "",
            "    def test_key_import_errors(self):",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "",
            "        self.cmd('key', 'import', self.repository_location, export_file, exit_code=EXIT_ERROR)",
            "",
            "        with open(export_file, 'w') as fd:",
            "            fd.write('something not a key\\n')",
            "",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('key', 'import', self.repository_location, export_file, exit_code=2)",
            "        else:",
            "            with pytest.raises(NotABorgKeyFile):",
            "                self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'w') as fd:",
            "            fd.write('BORG_KEY a0a0a0\\n')",
            "",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('key', 'import', self.repository_location, export_file, exit_code=2)",
            "        else:",
            "            with pytest.raises(RepoIdMismatch):",
            "                self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "    def test_key_export_paperkey(self):",
            "        repo_id = 'e294423506da4e1ea76e8dcdf1a3919624ae3ae496fddf905610c351d3f09239'",
            "",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "        self._set_repository_id(self.repository_path, unhexlify(repo_id))",
            "",
            "        key_file = self.keys_path + '/' + os.listdir(self.keys_path)[0]",
            "",
            "        with open(key_file, 'w') as fd:",
            "            fd.write(KeyfileKey.FILE_ID + ' ' + repo_id + '\\n')",
            "            fd.write(b2a_base64(b'abcdefghijklmnopqrstu').decode())",
            "",
            "        self.cmd('key', 'export', '--paper', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert export_contents == \"\"\"To restore key use borg key import --paper /path/to/repo",
            "",
            "BORG PAPER KEY v1",
            "id: 2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02",
            " 1: 616263 646566 676869 6a6b6c 6d6e6f 707172 - 6d",
            " 2: 737475 - 88",
            "\"\"\"",
            "",
            "    def test_key_import_paperkey(self):",
            "        repo_id = 'e294423506da4e1ea76e8dcdf1a3919624ae3ae496fddf905610c351d3f09239'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "        self._set_repository_id(self.repository_path, unhexlify(repo_id))",
            "",
            "        key_file = self.keys_path + '/' + os.listdir(self.keys_path)[0]",
            "        with open(key_file, 'w') as fd:",
            "            fd.write(KeyfileKey.FILE_ID + ' ' + repo_id + '\\n')",
            "            fd.write(b2a_base64(b'abcdefghijklmnopqrstu').decode())",
            "",
            "        typed_input = (",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41  02\\n'   # Forgot to type \"-\"",
            "            b'2 / e29442 3506da 4e1ea7  25f62a 5a3d41 - 02\\n'   # Forgot to type second \"/\"",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d42 - 02\\n'  # Typo (..42 not ..41)",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02\\n'  # Correct! Congratulations",
            "            b'616263 646566 676869 6a6b6c 6d6e6f 707172 - 6d\\n'",
            "            b'\\n\\n'  # Abort [yN] => N",
            "            b'737475 88\\n'  # missing \"-\"",
            "            b'73747i - 88\\n'  # typo",
            "            b'73747 - 88\\n'  # missing nibble",
            "            b'73 74 75  -  89\\n'  # line checksum mismatch",
            "            b'00a1 - 88\\n'  # line hash collision - overall hash mismatch, have to start over",
            "",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02\\n'",
            "            b'616263 646566 676869 6a6b6c 6d6e6f 707172 - 6d\\n'",
            "            b'73 74 75  -  88\\n'",
            "        )",
            "",
            "        # In case that this has to change, here is a quick way to find a colliding line hash:",
            "        #",
            "        # from hashlib import sha256",
            "        # hash_fn = lambda x: sha256(b'\\x00\\x02' + x).hexdigest()[:2]",
            "        # for i in range(1000):",
            "        #     if hash_fn(i.to_bytes(2, byteorder='big')) == '88':  # 88 = line hash",
            "        #         print(i.to_bytes(2, 'big'))",
            "        #         break",
            "",
            "        self.cmd('key', 'import', '--paper', self.repository_location, input=typed_input)",
            "",
            "        # Test abort paths",
            "        typed_input = b'\\ny\\n'",
            "        self.cmd('key', 'import', '--paper', self.repository_location, input=typed_input)",
            "        typed_input = b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02\\n\\ny\\n'",
            "        self.cmd('key', 'import', '--paper', self.repository_location, input=typed_input)",
            "",
            "    def test_debug_dump_manifest(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        dump_file = self.output_path + '/dump'",
            "        output = self.cmd('debug', 'dump-manifest', self.repository_location, dump_file)",
            "        assert output == \"\"",
            "        with open(dump_file, \"r\") as f:",
            "            result = json.load(f)",
            "        assert 'archives' in result",
            "        assert 'config' in result",
            "        assert 'item_keys' in result",
            "        assert 'timestamp' in result",
            "        assert 'version' in result",
            "",
            "    def test_debug_dump_archive(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        dump_file = self.output_path + '/dump'",
            "        output = self.cmd('debug', 'dump-archive', self.repository_location + \"::test\", dump_file)",
            "        assert output == \"\"",
            "        with open(dump_file, \"r\") as f:",
            "            result = json.load(f)",
            "        assert '_name' in result",
            "        assert '_manifest_entry' in result",
            "        assert '_meta' in result",
            "        assert '_items' in result",
            "",
            "    def test_debug_refcount_obj(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('debug', 'refcount-obj', self.repository_location, '0' * 64).strip()",
            "        assert output == 'object 0000000000000000000000000000000000000000000000000000000000000000 not found [info from chunks cache].'",
            "",
            "        create_json = json.loads(self.cmd('create', '--json', self.repository_location + '::test', 'input'))",
            "        archive_id = create_json['archive']['id']",
            "        output = self.cmd('debug', 'refcount-obj', self.repository_location, archive_id).strip()",
            "        assert output == 'object ' + archive_id + ' has 1 referrers [info from chunks cache].'",
            "",
            "        # Invalid IDs do not abort or return an error",
            "        output = self.cmd('debug', 'refcount-obj', self.repository_location, '124', 'xyza').strip()",
            "        assert output == 'object id 124 is invalid.\\nobject id xyza is invalid.'",
            "",
            "    def test_debug_info(self):",
            "        output = self.cmd('debug', 'info')",
            "        assert 'CRC implementation' in output",
            "        assert 'Python' in output",
            "",
            "    def test_benchmark_crud(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        with environment_variable(_BORG_BENCHMARK_CRUD_TEST='YES'):",
            "            self.cmd('benchmark', 'crud', self.repository_location, self.input_path)",
            "",
            "    def test_config(self):",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        for flags in [[], ['--cache']]:",
            "            for cfg_key in {'testkey', 'testsection.testkey'}:",
            "                self.cmd('config', self.repository_location, *flags, cfg_key, exit_code=1)",
            "                self.cmd('config', self.repository_location, *flags, cfg_key, 'testcontents')",
            "                output = self.cmd('config', self.repository_location, *flags, cfg_key)",
            "                assert output == 'testcontents\\n'",
            "                self.cmd('config', self.repository_location, *flags, '--delete', cfg_key)",
            "                self.cmd('config', self.repository_location, *flags, cfg_key, exit_code=1)",
            "",
            "    requires_gnutar = pytest.mark.skipif(not have_gnutar(), reason='GNU tar must be installed for this test.')",
            "    requires_gzip = pytest.mark.skipif(not shutil.which('gzip'), reason='gzip must be installed for this test.')",
            "",
            "    @requires_gnutar",
            "    def test_export_tar(self):",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('export-tar', self.repository_location + '::test', 'simple.tar', '--progress')",
            "        with changedir('output'):",
            "            # This probably assumes GNU tar. Note -p switch to extract permissions regardless of umask.",
            "            subprocess.check_call(['tar', 'xpf', '../simple.tar', '--warning=no-timestamp'])",
            "        self.assert_dirs_equal('input', 'output/input', ignore_bsdflags=True, ignore_xattrs=True, ignore_ns=True)",
            "",
            "    @requires_gnutar",
            "    @requires_gzip",
            "    def test_export_tar_gz(self):",
            "        if not shutil.which('gzip'):",
            "            pytest.skip('gzip is not installed')",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        list = self.cmd('export-tar', self.repository_location + '::test', 'simple.tar.gz', '--list')",
            "        assert 'input/file1\\n' in list",
            "        assert 'input/dir2\\n' in list",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../simple.tar.gz', '--warning=no-timestamp'])",
            "        self.assert_dirs_equal('input', 'output/input', ignore_bsdflags=True, ignore_xattrs=True, ignore_ns=True)",
            "",
            "    @requires_gnutar",
            "    def test_export_tar_strip_components(self):",
            "        if not shutil.which('gzip'):",
            "            pytest.skip('gzip is not installed')",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        list = self.cmd('export-tar', self.repository_location + '::test', 'simple.tar', '--strip-components=1', '--list')",
            "        # --list's path are those before processing with --strip-components",
            "        assert 'input/file1\\n' in list",
            "        assert 'input/dir2\\n' in list",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../simple.tar', '--warning=no-timestamp'])",
            "        self.assert_dirs_equal('input', 'output/', ignore_bsdflags=True, ignore_xattrs=True, ignore_ns=True)",
            "",
            "    @requires_hardlinks",
            "    @requires_gnutar",
            "    def test_export_tar_strip_components_links(self):",
            "        self._extract_hardlinks_setup()",
            "        self.cmd('export-tar', self.repository_location + '::test', 'output.tar', '--strip-components=2')",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../output.tar', '--warning=no-timestamp'])",
            "            assert os.stat('hardlink').st_nlink == 2",
            "            assert os.stat('subdir/hardlink').st_nlink == 2",
            "            assert os.stat('aaaa').st_nlink == 2",
            "            assert os.stat('source2').st_nlink == 2",
            "",
            "    @requires_hardlinks",
            "    @requires_gnutar",
            "    def test_extract_hardlinks(self):",
            "        self._extract_hardlinks_setup()",
            "        self.cmd('export-tar', self.repository_location + '::test', 'output.tar', 'input/dir1')",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../output.tar', '--warning=no-timestamp'])",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "",
            "    def test_detect_attic_repo(self):",
            "        path = attic_repo(self.repository_path)",
            "        cmds = [",
            "            ['create', path + '::test', self.tmpdir],",
            "            ['extract', path + '::test'],",
            "            ['check', path],",
            "            ['rename', path + '::test', 'newname'],",
            "            ['list', path],",
            "            ['delete', path],",
            "            ['prune', path],",
            "            ['info', path + '::test'],",
            "            ['key', 'export', path, 'exported'],",
            "            ['key', 'import', path, 'import'],",
            "            ['change-passphrase', path],",
            "            ['break-lock', path],",
            "        ]",
            "        for args in cmds:",
            "            output = self.cmd(*args, fork=True, exit_code=2)",
            "            assert 'Attic repository detected.' in output",
            "",
            "",
            "@unittest.skipUnless('binary' in BORG_EXES, 'no borg.exe available')",
            "class ArchiverTestCaseBinary(ArchiverTestCase):",
            "    EXE = 'borg.exe'",
            "    FORK_DEFAULT = True",
            "",
            "    @unittest.skip('patches objects')",
            "    def test_init_interrupt(self):",
            "        pass",
            "",
            "    @unittest.skip('patches objects')",
            "    def test_extract_capabilities(self):",
            "        pass",
            "",
            "    @unittest.skip('patches objects')",
            "    def test_extract_xattrs_errors(self):",
            "        pass",
            "",
            "    @unittest.skip('test_basic_functionality seems incompatible with fakeroot and/or the binary.')",
            "    def test_basic_functionality(self):",
            "        pass",
            "",
            "    @unittest.skip('test_overwrite seems incompatible with fakeroot and/or the binary.')",
            "    def test_overwrite(self):",
            "        pass",
            "",
            "    def test_fuse(self):",
            "        if fakeroot_detected():",
            "            unittest.skip('test_fuse with the binary is not compatible with fakeroot')",
            "        else:",
            "            super().test_fuse()",
            "",
            "",
            "class ArchiverCheckTestCase(ArchiverTestCaseBase):",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        with patch.object(ChunkBuffer, 'BUFFER_SIZE', 10):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "            self.create_src_archive('archive1')",
            "            self.create_src_archive('archive2')",
            "",
            "    def test_check_usage(self):",
            "        output = self.cmd('check', '-v', '--progress', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "        self.assert_in('Checking segments', output)",
            "        # reset logging to new process default to avoid need for fork=True on next check",
            "        logging.getLogger('borg.output.progress').setLevel(logging.NOTSET)",
            "        output = self.cmd('check', '-v', '--repository-only', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_not_in('Starting archive consistency check', output)",
            "        self.assert_not_in('Checking segments', output)",
            "        output = self.cmd('check', '-v', '--archives-only', self.repository_location, exit_code=0)",
            "        self.assert_not_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "        output = self.cmd('check', '-v', '--archives-only', '--prefix=archive2', self.repository_location, exit_code=0)",
            "        self.assert_not_in('archive1', output)",
            "        output = self.cmd('check', '-v', '--archives-only', '--first=1', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_not_in('archive2', output)",
            "        output = self.cmd('check', '-v', '--archives-only', '--last=1', self.repository_location, exit_code=0)",
            "        self.assert_not_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "",
            "    def test_missing_file_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith('testsuite/archiver.py'):",
            "                    valid_chunks = item.chunks",
            "                    killed_chunk = valid_chunks[-1]",
            "                    repository.delete(killed_chunk.id)",
            "                    break",
            "            else:",
            "                self.fail('should not happen')",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('New missing file chunk detected', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        output = self.cmd('list', '--format={health}#{path}{LF}', self.repository_location + '::archive1', exit_code=0)",
            "        self.assert_in('broken#', output)",
            "        # check that the file in the old archives has now a different chunk list without the killed chunk",
            "        for archive_name in ('archive1', 'archive2'):",
            "            archive, repository = self.open_archive(archive_name)",
            "            with repository:",
            "                for item in archive.iter_items():",
            "                    if item.path.endswith('testsuite/archiver.py'):",
            "                        self.assert_not_equal(valid_chunks, item.chunks)",
            "                        self.assert_not_in(killed_chunk, item.chunks)",
            "                        break",
            "                else:",
            "                    self.fail('should not happen')",
            "        # do a fresh backup (that will include the killed chunk)",
            "        with patch.object(ChunkBuffer, 'BUFFER_SIZE', 10):",
            "            self.create_src_archive('archive3')",
            "        # check should be able to heal the file now:",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('Healed previously missing file chunk', output)",
            "        self.assert_in('testsuite/archiver.py: Completely healed previously damaged file!', output)",
            "        # check that the file in the old archives has the correct chunks again",
            "        for archive_name in ('archive1', 'archive2'):",
            "            archive, repository = self.open_archive(archive_name)",
            "            with repository:",
            "                for item in archive.iter_items():",
            "                    if item.path.endswith('testsuite/archiver.py'):",
            "                        self.assert_equal(valid_chunks, item.chunks)",
            "                        break",
            "                else:",
            "                    self.fail('should not happen')",
            "        # list is also all-healthy again",
            "        output = self.cmd('list', '--format={health}#{path}{LF}', self.repository_location + '::archive1', exit_code=0)",
            "        self.assert_not_in('broken#', output)",
            "",
            "    def test_missing_archive_item_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            repository.delete(archive.metadata.items[-5])",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_archive_metadata(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            repository.delete(archive.id)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_manifest(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            repository.delete(Manifest.MANIFEST_ID)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_corrupted_manifest(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            manifest = repository.get(Manifest.MANIFEST_ID)",
            "            corrupted_manifest = manifest + b'corrupted!'",
            "            repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_manifest_rebuild_corrupted_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            manifest = repository.get(Manifest.MANIFEST_ID)",
            "            corrupted_manifest = manifest + b'corrupted!'",
            "            repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "",
            "            chunk = repository.get(archive.id)",
            "            corrupted_chunk = chunk + b'corrupted!'",
            "            repository.put(archive.id, corrupted_chunk)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive2', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_manifest_rebuild_duplicate_archive(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        key = archive.key",
            "        with repository:",
            "            manifest = repository.get(Manifest.MANIFEST_ID)",
            "            corrupted_manifest = manifest + b'corrupted!'",
            "            repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "",
            "            archive = msgpack.packb({",
            "                'cmdline': [],",
            "                'items': [],",
            "                'hostname': 'foo',",
            "                'username': 'bar',",
            "                'name': 'archive1',",
            "                'time': '2016-12-15T18:49:51.849711',",
            "                'version': 1,",
            "            })",
            "            archive_id = key.id_hash(archive)",
            "            repository.put(archive_id, key.encrypt(archive))",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive1.1', output)",
            "        self.assert_in('archive2', output)",
            "",
            "    def test_extra_chunks(self):",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        with Repository(self.repository_location, exclusive=True) as repository:",
            "            repository.put(b'01234567890123456789012345678901', b'xxxx')",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::archive1', exit_code=0)",
            "",
            "    def _test_verify_data(self, *init_args):",
            "        shutil.rmtree(self.repository_path)",
            "        self.cmd('init', self.repository_location, *init_args)",
            "        self.create_src_archive('archive1')",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith('testsuite/archiver.py'):",
            "                    chunk = item.chunks[-1]",
            "                    data = repository.get(chunk.id) + b'1234'",
            "                    repository.put(chunk.id, data)",
            "                    break",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        output = self.cmd('check', '--verify-data', self.repository_location, exit_code=1)",
            "        assert bin_to_hex(chunk.id) + ', integrity error' in output",
            "        # repair (heal is tested in another test)",
            "        output = self.cmd('check', '--repair', '--verify-data', self.repository_location, exit_code=0)",
            "        assert bin_to_hex(chunk.id) + ', integrity error' in output",
            "        assert 'testsuite/archiver.py: New missing file chunk detected' in output",
            "",
            "    def test_verify_data(self):",
            "        self._test_verify_data('--encryption', 'repokey')",
            "",
            "    def test_verify_data_unencrypted(self):",
            "        self._test_verify_data('--encryption', 'none')",
            "",
            "    def test_empty_repository(self):",
            "        with Repository(self.repository_location, exclusive=True) as repository:",
            "            for id_ in repository.list():",
            "                repository.delete(id_)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "",
            "    def test_attic013_acl_bug(self):",
            "        # Attic up to release 0.13 contained a bug where every item unintentionally received",
            "        # a b'acl'=None key-value pair.",
            "        # This bug can still live on in Borg repositories (through borg upgrade).",
            "        class Attic013Item:",
            "            def as_dict(self):",
            "                return {",
            "                    # These are required",
            "                    b'path': '1234',",
            "                    b'mtime': 0,",
            "                    b'mode': 0,",
            "                    b'user': b'0',",
            "                    b'group': b'0',",
            "                    b'uid': 0,",
            "                    b'gid': 0,",
            "                    # acl is the offending key.",
            "                    b'acl': None,",
            "                }",
            "",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest) as cache:",
            "                archive = Archive(repository, key, manifest, '0.13', cache=cache, create=True)",
            "                archive.items_buffer.add(Attic013Item())",
            "                archive.save()",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        self.cmd('list', self.repository_location + '::0.13', exit_code=0)",
            "",
            "",
            "class ManifestAuthenticationTest(ArchiverTestCaseBase):",
            "    def spoof_manifest(self, repository):",
            "        with repository:",
            "            _, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            repository.put(Manifest.MANIFEST_ID, key.encrypt(msgpack.packb({",
            "                'version': 1,",
            "                'archives': {},",
            "                'config': {},",
            "                'timestamp': (datetime.utcnow() + timedelta(days=1)).strftime(ISO_FORMAT),",
            "            })))",
            "            repository.commit()",
            "",
            "    def test_fresh_init_tam_required(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        with repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            repository.put(Manifest.MANIFEST_ID, key.encrypt(msgpack.packb({",
            "                'version': 1,",
            "                'archives': {},",
            "                'timestamp': (datetime.utcnow() + timedelta(days=1)).strftime(ISO_FORMAT),",
            "            })))",
            "            repository.commit()",
            "",
            "        with pytest.raises(TAMRequiredError):",
            "            self.cmd('list', self.repository_location)",
            "",
            "    def test_not_required(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive1234')",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        with repository:",
            "            shutil.rmtree(get_security_dir(bin_to_hex(repository.id)))",
            "            _, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            key.tam_required = False",
            "            key.change_passphrase(key._passphrase)",
            "",
            "            manifest = msgpack.unpackb(key.decrypt(None, repository.get(Manifest.MANIFEST_ID)))",
            "            del manifest[b'tam']",
            "            repository.put(Manifest.MANIFEST_ID, key.encrypt(msgpack.packb(manifest)))",
            "            repository.commit()",
            "        output = self.cmd('list', '--debug', self.repository_location)",
            "        assert 'archive1234' in output",
            "        assert 'TAM not found and not required' in output",
            "        # Run upgrade",
            "        self.cmd('upgrade', '--tam', self.repository_location)",
            "        # Manifest must be authenticated now",
            "        output = self.cmd('list', '--debug', self.repository_location)",
            "        assert 'archive1234' in output",
            "        assert 'TAM-verified manifest' in output",
            "        # Try to spoof / modify pre-1.0.9",
            "        self.spoof_manifest(repository)",
            "        # Fails",
            "        with pytest.raises(TAMRequiredError):",
            "            self.cmd('list', self.repository_location)",
            "        # Force upgrade",
            "        self.cmd('upgrade', '--tam', '--force', self.repository_location)",
            "        self.cmd('list', self.repository_location)",
            "",
            "    def test_disable(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive1234')",
            "        self.cmd('upgrade', '--disable-tam', self.repository_location)",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        self.spoof_manifest(repository)",
            "        assert not self.cmd('list', self.repository_location)",
            "",
            "    def test_disable2(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive1234')",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        self.spoof_manifest(repository)",
            "        self.cmd('upgrade', '--disable-tam', self.repository_location)",
            "        assert not self.cmd('list', self.repository_location)",
            "",
            "",
            "class RemoteArchiverTestCase(ArchiverTestCase):",
            "    prefix = '__testsuite__:'",
            "",
            "    def open_repository(self):",
            "        return RemoteRepository(Location(self.repository_location))",
            "",
            "    def test_remote_repo_restrict_to_path(self):",
            "        # restricted to repo directory itself:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', self.repository_path]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        # restricted to repo directory itself, fail for other directories with same prefix:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', self.repository_path]):",
            "            with pytest.raises(PathNotAllowed):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location + '_0')",
            "",
            "        # restricted to a completely different path:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo']):",
            "            with pytest.raises(PathNotAllowed):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location + '_1')",
            "        path_prefix = os.path.dirname(self.repository_path)",
            "        # restrict to repo directory's parent directory:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', path_prefix]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location + '_2')",
            "        # restrict to repo directory's parent directory and another directory:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo', '--restrict-to-path', path_prefix]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location + '_3')",
            "",
            "    def test_remote_repo_restrict_to_repository(self):",
            "        # restricted to repo directory itself:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-repository', self.repository_path]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        parent_path = os.path.join(self.repository_path, '..')",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-repository', parent_path]):",
            "            with pytest.raises(PathNotAllowed):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "    @unittest.skip('only works locally')",
            "    def test_debug_put_get_delete_obj(self):",
            "        pass",
            "",
            "    @unittest.skip('only works locally')",
            "    def test_config(self):",
            "        pass",
            "",
            "    def test_strip_components_doesnt_leak(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('dir/file', contents=b\"test file contents 1\")",
            "        self.create_regular_file('dir/file2', contents=b\"test file contents 2\")",
            "        self.create_regular_file('skipped-file1', contents=b\"test file contents 3\")",
            "        self.create_regular_file('skipped-file2', contents=b\"test file contents 4\")",
            "        self.create_regular_file('skipped-file3', contents=b\"test file contents 5\")",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        marker = 'cached responses left in RemoteRepository'",
            "        with changedir('output'):",
            "            res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '3')",
            "            self.assert_true(marker not in res)",
            "            with self.assert_creates_file('file'):",
            "                res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '2')",
            "                self.assert_true(marker not in res)",
            "            with self.assert_creates_file('dir/file'):",
            "                res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '1')",
            "                self.assert_true(marker not in res)",
            "            with self.assert_creates_file('input/dir/file'):",
            "                res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '0')",
            "                self.assert_true(marker not in res)",
            "",
            "",
            "class ArchiverCorruptionTestCase(ArchiverTestCaseBase):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cache_path = json.loads(self.cmd('info', self.repository_location, '--json'))['cache']['path']",
            "",
            "    def corrupt(self, file):",
            "        with open(file, 'r+b') as fd:",
            "            fd.seek(-1, io.SEEK_END)",
            "            fd.write(b'1')",
            "",
            "    def test_cache_chunks(self):",
            "        self.corrupt(os.path.join(self.cache_path, 'chunks'))",
            "",
            "        if self.FORK_DEFAULT:",
            "            out = self.cmd('info', self.repository_location, exit_code=2)",
            "            assert 'failed integrity check' in out",
            "        else:",
            "            with pytest.raises(FileIntegrityError):",
            "                self.cmd('info', self.repository_location)",
            "",
            "    def test_cache_files(self):",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.corrupt(os.path.join(self.cache_path, 'files'))",
            "",
            "        if self.FORK_DEFAULT:",
            "            out = self.cmd('create', self.repository_location + '::test1', 'input', exit_code=2)",
            "            assert 'failed integrity check' in out",
            "        else:",
            "            with pytest.raises(FileIntegrityError):",
            "                self.cmd('create', self.repository_location + '::test1', 'input')",
            "",
            "    def test_chunks_archive(self):",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "        # Find ID of test1 so we can corrupt it later :)",
            "        target_id = self.cmd('list', self.repository_location, '--format={id}{LF}').strip()",
            "        self.cmd('create', self.repository_location + '::test2', 'input')",
            "",
            "        # Force cache sync, creating archive chunks of test1 and test2 in chunks.archive.d",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        self.cmd('info', self.repository_location, '--json')",
            "",
            "        chunks_archive = os.path.join(self.cache_path, 'chunks.archive.d')",
            "        assert len(os.listdir(chunks_archive)) == 4  # two archives, one chunks cache and one .integrity file each",
            "",
            "        self.corrupt(os.path.join(chunks_archive, target_id + '.compact'))",
            "",
            "        # Trigger cache sync by changing the manifest ID in the cache config",
            "        config_path = os.path.join(self.cache_path, 'config')",
            "        config = ConfigParser(interpolation=None)",
            "        config.read(config_path)",
            "        config.set('cache', 'manifest', bin_to_hex(bytes(32)))",
            "        with open(config_path, 'w') as fd:",
            "            config.write(fd)",
            "",
            "        # Cache sync notices corrupted archive chunks, but automatically recovers.",
            "        out = self.cmd('create', '-v', self.repository_location + '::test3', 'input', exit_code=1)",
            "        assert 'Reading cached archive chunk index for test1' in out",
            "        assert 'Cached archive chunk index of test1 is corrupted' in out",
            "        assert 'Fetching and building archive index for test1' in out",
            "",
            "    def test_old_version_interfered(self):",
            "        # Modify the main manifest ID without touching the manifest ID in the integrity section.",
            "        # This happens if a version without integrity checking modifies the cache.",
            "        config_path = os.path.join(self.cache_path, 'config')",
            "        config = ConfigParser(interpolation=None)",
            "        config.read(config_path)",
            "        config.set('cache', 'manifest', bin_to_hex(bytes(32)))",
            "        with open(config_path, 'w') as fd:",
            "            config.write(fd)",
            "",
            "        out = self.cmd('info', self.repository_location)",
            "        assert 'Cache integrity data not available: old Borg version modified the cache.' in out",
            "",
            "",
            "class DiffArchiverTestCase(ArchiverTestCaseBase):",
            "    def test_basic_functionality(self):",
            "        # Initialize test folder",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "        # Setup files for the first snapshot",
            "        self.create_regular_file('file_unchanged', size=128)",
            "        self.create_regular_file('file_removed', size=256)",
            "        self.create_regular_file('file_removed2', size=512)",
            "        self.create_regular_file('file_replaced', size=1024)",
            "        os.mkdir('input/dir_replaced_with_file')",
            "        os.chmod('input/dir_replaced_with_file', stat.S_IFDIR | 0o755)",
            "        os.mkdir('input/dir_removed')",
            "        if are_symlinks_supported():",
            "            os.mkdir('input/dir_replaced_with_link')",
            "            os.symlink('input/dir_replaced_with_file', 'input/link_changed')",
            "            os.symlink('input/file_unchanged', 'input/link_removed')",
            "            os.symlink('input/file_removed2', 'input/link_target_removed')",
            "            os.symlink('input/empty', 'input/link_target_contents_changed')",
            "            os.symlink('input/empty', 'input/link_replaced_by_file')",
            "        if are_hardlinks_supported():",
            "            os.link('input/empty', 'input/hardlink_contents_changed')",
            "            os.link('input/file_removed', 'input/hardlink_removed')",
            "            os.link('input/file_removed2', 'input/hardlink_target_removed')",
            "            os.link('input/file_replaced', 'input/hardlink_target_replaced')",
            "",
            "        # Create the first snapshot",
            "        self.cmd('create', self.repository_location + '::test0', 'input')",
            "",
            "        # Setup files for the second snapshot",
            "        self.create_regular_file('file_added', size=2048)",
            "        os.unlink('input/file_removed')",
            "        os.unlink('input/file_removed2')",
            "        os.unlink('input/file_replaced')",
            "        self.create_regular_file('file_replaced', size=4096, contents=b'0')",
            "        os.rmdir('input/dir_replaced_with_file')",
            "        self.create_regular_file('dir_replaced_with_file', size=8192)",
            "        os.chmod('input/dir_replaced_with_file', stat.S_IFREG | 0o755)",
            "        os.mkdir('input/dir_added')",
            "        os.rmdir('input/dir_removed')",
            "        if are_symlinks_supported():",
            "            os.rmdir('input/dir_replaced_with_link')",
            "            os.symlink('input/dir_added', 'input/dir_replaced_with_link')",
            "            os.unlink('input/link_changed')",
            "            os.symlink('input/dir_added', 'input/link_changed')",
            "            os.symlink('input/dir_added', 'input/link_added')",
            "            os.unlink('input/link_replaced_by_file')",
            "            self.create_regular_file('link_replaced_by_file', size=16384)",
            "            os.unlink('input/link_removed')",
            "        if are_hardlinks_supported():",
            "            os.unlink('input/hardlink_removed')",
            "            os.link('input/file_added', 'input/hardlink_added')",
            "",
            "        with open('input/empty', 'ab') as fd:",
            "            fd.write(b'appended_data')",
            "",
            "        # Create the second snapshot",
            "        self.cmd('create', self.repository_location + '::test1a', 'input')",
            "        self.cmd('create', '--chunker-params', '16,18,17,4095', self.repository_location + '::test1b', 'input')",
            "",
            "        def do_asserts(output, can_compare_ids):",
            "            # File contents changed (deleted and replaced with a new file)",
            "            change = 'B' if can_compare_ids else '{:<19}'.format('modified')",
            "            assert '{} input/file_replaced'.format(change) in output",
            "",
            "            # File unchanged",
            "            assert 'input/file_unchanged' not in output",
            "",
            "            # Directory replaced with a regular file",
            "            if 'BORG_TESTS_IGNORE_MODES' not in os.environ:",
            "                assert '[drwxr-xr-x -> -rwxr-xr-x] input/dir_replaced_with_file' in output",
            "",
            "            # Basic directory cases",
            "            assert 'added directory     input/dir_added' in output",
            "            assert 'removed directory   input/dir_removed' in output",
            "",
            "            if are_symlinks_supported():",
            "                # Basic symlink cases",
            "                assert 'changed link        input/link_changed' in output",
            "                assert 'added link          input/link_added' in output",
            "                assert 'removed link        input/link_removed' in output",
            "",
            "                # Symlink replacing or being replaced",
            "                assert '] input/dir_replaced_with_link' in output",
            "                assert '] input/link_replaced_by_file' in output",
            "",
            "                # Symlink target removed. Should not affect the symlink at all.",
            "                assert 'input/link_target_removed' not in output",
            "",
            "            # The inode has two links and the file contents changed. Borg",
            "            # should notice the changes in both links. However, the symlink",
            "            # pointing to the file is not changed.",
            "            change = '0 B' if can_compare_ids else '{:<19}'.format('modified')",
            "            assert '{} input/empty'.format(change) in output",
            "            if are_hardlinks_supported():",
            "                assert '{} input/hardlink_contents_changed'.format(change) in output",
            "            if are_symlinks_supported():",
            "                assert 'input/link_target_contents_changed' not in output",
            "",
            "            # Added a new file and a hard link to it. Both links to the same",
            "            # inode should appear as separate files.",
            "            assert 'added       2.05 kB input/file_added' in output",
            "            if are_hardlinks_supported():",
            "                assert 'added       2.05 kB input/hardlink_added' in output",
            "",
            "            # The inode has two links and both of them are deleted. They should",
            "            # appear as two deleted files.",
            "            assert 'removed       256 B input/file_removed' in output",
            "            if are_hardlinks_supported():",
            "                assert 'removed       256 B input/hardlink_removed' in output",
            "",
            "            # Another link (marked previously as the source in borg) to the",
            "            # same inode was removed. This should not change this link at all.",
            "            if are_hardlinks_supported():",
            "                assert 'input/hardlink_target_removed' not in output",
            "",
            "            # Another link (marked previously as the source in borg) to the",
            "            # same inode was replaced with a new regular file. This should not",
            "            # change this link at all.",
            "            if are_hardlinks_supported():",
            "                assert 'input/hardlink_target_replaced' not in output",
            "",
            "        do_asserts(self.cmd('diff', self.repository_location + '::test0', 'test1a'), True)",
            "        # We expect exit_code=1 due to the chunker params warning",
            "        do_asserts(self.cmd('diff', self.repository_location + '::test0', 'test1b', exit_code=1), False)",
            "",
            "    def test_sort_option(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "        self.create_regular_file('a_file_removed', size=8)",
            "        self.create_regular_file('f_file_removed', size=16)",
            "        self.create_regular_file('c_file_changed', size=32)",
            "        self.create_regular_file('e_file_changed', size=64)",
            "        self.cmd('create', self.repository_location + '::test0', 'input')",
            "",
            "        os.unlink('input/a_file_removed')",
            "        os.unlink('input/f_file_removed')",
            "        os.unlink('input/c_file_changed')",
            "        os.unlink('input/e_file_changed')",
            "        self.create_regular_file('c_file_changed', size=512)",
            "        self.create_regular_file('e_file_changed', size=1024)",
            "        self.create_regular_file('b_file_added', size=128)",
            "        self.create_regular_file('d_file_added', size=256)",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "",
            "        output = self.cmd('diff', '--sort', self.repository_location + '::test0', 'test1')",
            "        expected = [",
            "            'a_file_removed',",
            "            'b_file_added',",
            "            'c_file_changed',",
            "            'd_file_added',",
            "            'e_file_changed',",
            "            'f_file_removed',",
            "        ]",
            "",
            "        assert all(x in line for x, line in zip(expected, output.splitlines()))",
            "",
            "",
            "def test_get_args():",
            "    archiver = Archiver()",
            "    # everything normal:",
            "    # first param is argv as produced by ssh forced command,",
            "    # second param is like from SSH_ORIGINAL_COMMAND env variable",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],",
            "                             'borg serve --info --umask=0027')",
            "    assert args.func == archiver.do_serve",
            "    assert args.restrict_to_paths == ['/p1', '/p2']",
            "    assert args.umask == 0o027",
            "    assert args.log_level == 'info'",
            "    # trying to cheat - break out of path restriction",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],",
            "                             'borg serve --restrict-to-path=/')",
            "    assert args.restrict_to_paths == ['/p1', '/p2']",
            "    # trying to cheat - try to execute different subcommand",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],",
            "                             'borg init --encryption=repokey /')",
            "    assert args.func == archiver.do_serve",
            "",
            "    # Check that environment variables in the forced command don't cause issues. If the command",
            "    # were not forced, environment variables would be interpreted by the shell, but this does not",
            "    # happen for forced commands - we get the verbatim command line and need to deal with env vars.",
            "    args = archiver.get_args(['borg', 'serve', ],",
            "                             'BORG_HOSTNAME_IS_UNIQUE=yes borg serve --info')",
            "    assert args.func == archiver.do_serve",
            "",
            "",
            "def test_chunk_content_equal():",
            "    def ccc(a, b):",
            "        chunks_a = [data for data in a]",
            "        chunks_b = [data for data in b]",
            "        compare1 = ItemDiff._chunk_content_equal(iter(chunks_a), iter(chunks_b))",
            "        compare2 = ItemDiff._chunk_content_equal(iter(chunks_b), iter(chunks_a))",
            "        assert compare1 == compare2",
            "        return compare1",
            "    assert ccc([",
            "        b'1234', b'567A', b'bC'",
            "    ], [",
            "        b'1', b'23', b'4567A', b'b', b'C'",
            "    ])",
            "    # one iterator exhausted before the other",
            "    assert not ccc([",
            "        b'12345',",
            "    ], [",
            "        b'1234', b'56'",
            "    ])",
            "    # content mismatch",
            "    assert not ccc([",
            "        b'1234', b'65'",
            "    ], [",
            "        b'1234', b'56'",
            "    ])",
            "    # first is the prefix of second",
            "    assert not ccc([",
            "        b'1234', b'56'",
            "    ], [",
            "        b'1234', b'565'",
            "    ])",
            "",
            "",
            "class TestBuildFilter:",
            "    @staticmethod",
            "    def peek_and_store_hardlink_masters(item, matched):",
            "        pass",
            "",
            "    def test_basic(self):",
            "        matcher = PatternMatcher()",
            "        matcher.add([parse_pattern('included')], IECommand.Include)",
            "        filter = Archiver.build_filter(matcher, self.peek_and_store_hardlink_masters, 0)",
            "        assert filter(Item(path='included'))",
            "        assert filter(Item(path='included/file'))",
            "        assert not filter(Item(path='something else'))",
            "",
            "    def test_empty(self):",
            "        matcher = PatternMatcher(fallback=True)",
            "        filter = Archiver.build_filter(matcher, self.peek_and_store_hardlink_masters, 0)",
            "        assert filter(Item(path='anything'))",
            "",
            "    def test_strip_components(self):",
            "        matcher = PatternMatcher(fallback=True)",
            "        filter = Archiver.build_filter(matcher, self.peek_and_store_hardlink_masters, strip_components=1)",
            "        assert not filter(Item(path='shallow'))",
            "        assert not filter(Item(path='shallow/'))  # can this even happen? paths are normalized...",
            "        assert filter(Item(path='deep enough/file'))",
            "        assert filter(Item(path='something/dir/file'))",
            "",
            "",
            "class TestCommonOptions:",
            "    @staticmethod",
            "    def define_common_options(add_common_option):",
            "        add_common_option('-h', '--help', action='help', help='show this help message and exit')",
            "        add_common_option('--critical', dest='log_level', help='foo',",
            "                          action='store_const', const='critical', default='warning')",
            "        add_common_option('--error', dest='log_level', help='foo',",
            "                          action='store_const', const='error', default='warning')",
            "        add_common_option('--append', dest='append', help='foo',",
            "                          action='append', metavar='TOPIC', default=[])",
            "        add_common_option('-p', '--progress', dest='progress', action='store_true', help='foo')",
            "        add_common_option('--lock-wait', dest='lock_wait', type=int, metavar='N', default=1,",
            "                          help='(default: %(default)d).')",
            "",
            "    @pytest.fixture",
            "    def basic_parser(self):",
            "        parser = argparse.ArgumentParser(prog='test', description='test parser', add_help=False)",
            "        parser.common_options = Archiver.CommonOptions(self.define_common_options,",
            "                                                       suffix_precedence=('_level0', '_level1'))",
            "        return parser",
            "",
            "    @pytest.fixture",
            "    def subparsers(self, basic_parser):",
            "        return basic_parser.add_subparsers(title='required arguments', metavar='<command>')",
            "",
            "    @pytest.fixture",
            "    def parser(self, basic_parser):",
            "        basic_parser.common_options.add_common_group(basic_parser, '_level0', provide_defaults=True)",
            "        return basic_parser",
            "",
            "    @pytest.fixture",
            "    def common_parser(self, parser):",
            "        common_parser = argparse.ArgumentParser(add_help=False, prog='test')",
            "        parser.common_options.add_common_group(common_parser, '_level1')",
            "        return common_parser",
            "",
            "    @pytest.fixture",
            "    def parse_vars_from_line(self, parser, subparsers, common_parser):",
            "        subparser = subparsers.add_parser('subcommand', parents=[common_parser], add_help=False,",
            "                                          description='foo', epilog='bar', help='baz',",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=1234)",
            "        subparser.add_argument('--append-only', dest='append_only', action='store_true')",
            "",
            "        def parse_vars_from_line(*line):",
            "            print(line)",
            "            args = parser.parse_args(line)",
            "            parser.common_options.resolve(args)",
            "            return vars(args)",
            "",
            "        return parse_vars_from_line",
            "",
            "    def test_simple(self, parse_vars_from_line):",
            "        assert parse_vars_from_line('--error') == {",
            "            'append': [],",
            "            'lock_wait': 1,",
            "            'log_level': 'error',",
            "            'progress': False",
            "        }",
            "",
            "        assert parse_vars_from_line('--error', 'subcommand', '--critical') == {",
            "            'append': [],",
            "            'lock_wait': 1,",
            "            'log_level': 'critical',",
            "            'progress': False,",
            "            'append_only': False,",
            "            'func': 1234,",
            "        }",
            "",
            "        with pytest.raises(SystemExit):",
            "            parse_vars_from_line('--append-only', 'subcommand')",
            "",
            "        assert parse_vars_from_line('--append=foo', '--append', 'bar', 'subcommand', '--append', 'baz') == {",
            "            'append': ['foo', 'bar', 'baz'],",
            "            'lock_wait': 1,",
            "            'log_level': 'warning',",
            "            'progress': False,",
            "            'append_only': False,",
            "            'func': 1234,",
            "        }",
            "",
            "    @pytest.mark.parametrize('position', ('before', 'after', 'both'))",
            "    @pytest.mark.parametrize('flag,args_key,args_value', (",
            "        ('-p', 'progress', True),",
            "        ('--lock-wait=3', 'lock_wait', 3),",
            "    ))",
            "    def test_flag_position_independence(self, parse_vars_from_line, position, flag, args_key, args_value):",
            "        line = []",
            "        if position in ('before', 'both'):",
            "            line.append(flag)",
            "        line.append('subcommand')",
            "        if position in ('after', 'both'):",
            "            line.append(flag)",
            "",
            "        result = {",
            "            'append': [],",
            "            'lock_wait': 1,",
            "            'log_level': 'warning',",
            "            'progress': False,",
            "            'append_only': False,",
            "            'func': 1234,",
            "        }",
            "        result[args_key] = args_value",
            "",
            "        assert parse_vars_from_line(*line) == result",
            "",
            "",
            "def test_parse_storage_quota():",
            "    assert parse_storage_quota('50M') == 50 * 1000**2",
            "    with pytest.raises(argparse.ArgumentTypeError):",
            "        parse_storage_quota('5M')",
            "",
            "",
            "def get_all_parsers():",
            "    \"\"\"",
            "    Return dict mapping command to parser.",
            "    \"\"\"",
            "    parser = Archiver(prog='borg').build_parser()",
            "    borgfs_parser = Archiver(prog='borgfs').build_parser()",
            "    parsers = {}",
            "",
            "    def discover_level(prefix, parser, Archiver, extra_choices=None):",
            "        choices = {}",
            "        for action in parser._actions:",
            "            if action.choices is not None and 'SubParsersAction' in str(action.__class__):",
            "                for cmd, parser in action.choices.items():",
            "                    choices[prefix + cmd] = parser",
            "        if extra_choices is not None:",
            "            choices.update(extra_choices)",
            "        if prefix and not choices:",
            "            return",
            "",
            "        for command, parser in sorted(choices.items()):",
            "            discover_level(command + \" \", parser, Archiver)",
            "            parsers[command] = parser",
            "",
            "    discover_level(\"\", parser, Archiver, {'borgfs': borgfs_parser})",
            "    return parsers",
            "",
            "",
            "@pytest.mark.parametrize('command, parser', list(get_all_parsers().items()))",
            "def test_help_formatting(command, parser):",
            "    if isinstance(parser.epilog, RstToTextLazy):",
            "        assert parser.epilog.rst",
            "",
            "",
            "@pytest.mark.parametrize('topic, helptext', list(Archiver.helptext.items()))",
            "def test_help_formatting_helptexts(topic, helptext):",
            "    assert str(rst_to_terminal(helptext))"
        ],
        "afterPatchFile": [
            "import argparse",
            "import errno",
            "import io",
            "import json",
            "import logging",
            "import os",
            "import pstats",
            "import random",
            "import re",
            "import shutil",
            "import socket",
            "import stat",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "import unittest",
            "from binascii import unhexlify, b2a_base64",
            "from configparser import ConfigParser",
            "from datetime import datetime",
            "from datetime import timedelta",
            "from hashlib import sha256",
            "from io import BytesIO, StringIO",
            "from unittest.mock import patch",
            "",
            "import msgpack",
            "import pytest",
            "",
            "try:",
            "    import llfuse",
            "except ImportError:",
            "    pass",
            "",
            "import borg",
            "from .. import xattr, helpers, platform",
            "from ..archive import Archive, ChunkBuffer, flags_noatime, flags_normal",
            "from ..archiver import Archiver, parse_storage_quota",
            "from ..cache import Cache, LocalCache",
            "from ..constants import *  # NOQA",
            "from ..crypto.low_level import bytes_to_long, num_cipher_blocks",
            "from ..crypto.key import KeyfileKeyBase, RepoKey, KeyfileKey, Passphrase, TAMRequiredError",
            "from ..crypto.keymanager import RepoIdMismatch, NotABorgKeyFile",
            "from ..crypto.file_integrity import FileIntegrityError",
            "from ..helpers import Location, get_security_dir",
            "from ..helpers import Manifest, MandatoryFeatureUnsupported",
            "from ..helpers import EXIT_SUCCESS, EXIT_WARNING, EXIT_ERROR",
            "from ..helpers import bin_to_hex",
            "from ..helpers import MAX_S",
            "from ..nanorst import RstToTextLazy, rst_to_terminal",
            "from ..patterns import IECommand, PatternMatcher, parse_pattern",
            "from ..item import Item, ItemDiff",
            "from ..logger import setup_logging",
            "from ..remote import RemoteRepository, PathNotAllowed",
            "from ..repository import Repository",
            "from . import has_lchflags, has_llfuse",
            "from . import BaseTestCase, changedir, environment_variable, no_selinux",
            "from . import are_symlinks_supported, are_hardlinks_supported, are_fifos_supported, is_utime_fully_supported, is_birthtime_fully_supported",
            "from .platform import fakeroot_detected",
            "from .upgrader import attic_repo",
            "from . import key",
            "",
            "",
            "src_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))",
            "",
            "",
            "def exec_cmd(*args, archiver=None, fork=False, exe=None, input=b'', binary_output=False, **kw):",
            "    if fork:",
            "        try:",
            "            if exe is None:",
            "                borg = (sys.executable, '-m', 'borg.archiver')",
            "            elif isinstance(exe, str):",
            "                borg = (exe, )",
            "            elif not isinstance(exe, tuple):",
            "                raise ValueError('exe must be None, a tuple or a str')",
            "            output = subprocess.check_output(borg + args, stderr=subprocess.STDOUT, input=input)",
            "            ret = 0",
            "        except subprocess.CalledProcessError as e:",
            "            output = e.output",
            "            ret = e.returncode",
            "        except SystemExit as e:  # possibly raised by argparse",
            "            output = ''",
            "            ret = e.code",
            "        if binary_output:",
            "            return ret, output",
            "        else:",
            "            return ret, os.fsdecode(output)",
            "    else:",
            "        stdin, stdout, stderr = sys.stdin, sys.stdout, sys.stderr",
            "        try:",
            "            sys.stdin = StringIO(input.decode())",
            "            sys.stdin.buffer = BytesIO(input)",
            "            output = BytesIO()",
            "            # Always use utf-8 here, to simply .decode() below",
            "            output_text = sys.stdout = sys.stderr = io.TextIOWrapper(output, encoding='utf-8')",
            "            if archiver is None:",
            "                archiver = Archiver()",
            "            archiver.prerun_checks = lambda *args: None",
            "            archiver.exit_code = EXIT_SUCCESS",
            "            helpers.exit_code = EXIT_SUCCESS",
            "            try:",
            "                args = archiver.parse_args(list(args))",
            "                # argparse parsing may raise SystemExit when the command line is bad or",
            "                # actions that abort early (eg. --help) where given. Catch this and return",
            "                # the error code as-if we invoked a Borg binary.",
            "            except SystemExit as e:",
            "                output_text.flush()",
            "                return e.code, output.getvalue() if binary_output else output.getvalue().decode()",
            "            ret = archiver.run(args)",
            "            output_text.flush()",
            "            return ret, output.getvalue() if binary_output else output.getvalue().decode()",
            "        finally:",
            "            sys.stdin, sys.stdout, sys.stderr = stdin, stdout, stderr",
            "",
            "",
            "def have_gnutar():",
            "    if not shutil.which('tar'):",
            "        return False",
            "    popen = subprocess.Popen(['tar', '--version'], stdout=subprocess.PIPE)",
            "    stdout, stderr = popen.communicate()",
            "    return b'GNU tar' in stdout",
            "",
            "",
            "# check if the binary \"borg.exe\" is available (for local testing a symlink to virtualenv/bin/borg should do)",
            "try:",
            "    exec_cmd('help', exe='borg.exe', fork=True)",
            "    BORG_EXES = ['python', 'binary', ]",
            "except FileNotFoundError:",
            "    BORG_EXES = ['python', ]",
            "",
            "",
            "@pytest.fixture(params=BORG_EXES)",
            "def cmd(request):",
            "    if request.param == 'python':",
            "        exe = None",
            "    elif request.param == 'binary':",
            "        exe = 'borg.exe'",
            "    else:",
            "        raise ValueError(\"param must be 'python' or 'binary'\")",
            "",
            "    def exec_fn(*args, **kw):",
            "        return exec_cmd(*args, exe=exe, fork=True, **kw)",
            "    return exec_fn",
            "",
            "",
            "def test_return_codes(cmd, tmpdir):",
            "    repo = tmpdir.mkdir('repo')",
            "    input = tmpdir.mkdir('input')",
            "    output = tmpdir.mkdir('output')",
            "    input.join('test_file').write('content')",
            "    rc, out = cmd('init', '--encryption=none', '%s' % str(repo))",
            "    assert rc == EXIT_SUCCESS",
            "    rc, out = cmd('create', '%s::archive' % repo, str(input))",
            "    assert rc == EXIT_SUCCESS",
            "    with changedir(str(output)):",
            "        rc, out = cmd('extract', '%s::archive' % repo)",
            "        assert rc == EXIT_SUCCESS",
            "    rc, out = cmd('extract', '%s::archive' % repo, 'does/not/match')",
            "    assert rc == EXIT_WARNING  # pattern did not match",
            "    rc, out = cmd('create', '%s::archive' % repo, str(input))",
            "    assert rc == EXIT_ERROR  # duplicate archive name",
            "",
            "",
            "\"\"\"",
            "test_disk_full is very slow and not recommended to be included in daily testing.",
            "for this test, an empty, writable 16MB filesystem mounted on DF_MOUNT is required.",
            "for speed and other reasons, it is recommended that the underlying block device is",
            "in RAM, not a magnetic or flash disk.",
            "",
            "assuming /tmp is a tmpfs (in memory filesystem), one can use this:",
            "dd if=/dev/zero of=/tmp/borg-disk bs=16M count=1",
            "mkfs.ext4 /tmp/borg-disk",
            "mkdir /tmp/borg-mount",
            "sudo mount /tmp/borg-disk /tmp/borg-mount",
            "",
            "if the directory does not exist, the test will be skipped.",
            "\"\"\"",
            "DF_MOUNT = '/tmp/borg-mount'",
            "",
            "",
            "@pytest.mark.skipif(not os.path.exists(DF_MOUNT), reason=\"needs a 16MB fs mounted on %s\" % DF_MOUNT)",
            "def test_disk_full(cmd):",
            "    def make_files(dir, count, size, rnd=True):",
            "        shutil.rmtree(dir, ignore_errors=True)",
            "        os.mkdir(dir)",
            "        if rnd:",
            "            count = random.randint(1, count)",
            "            if size > 1:",
            "                size = random.randint(1, size)",
            "        for i in range(count):",
            "            fn = os.path.join(dir, \"file%03d\" % i)",
            "            with open(fn, 'wb') as f:",
            "                data = os.urandom(size)",
            "                f.write(data)",
            "",
            "    with environment_variable(BORG_CHECK_I_KNOW_WHAT_I_AM_DOING='YES'):",
            "        mount = DF_MOUNT",
            "        assert os.path.exists(mount)",
            "        repo = os.path.join(mount, 'repo')",
            "        input = os.path.join(mount, 'input')",
            "        reserve = os.path.join(mount, 'reserve')",
            "        for j in range(100):",
            "            shutil.rmtree(repo, ignore_errors=True)",
            "            shutil.rmtree(input, ignore_errors=True)",
            "            # keep some space and some inodes in reserve that we can free up later:",
            "            make_files(reserve, 80, 100000, rnd=False)",
            "            rc, out = cmd('init', repo)",
            "            if rc != EXIT_SUCCESS:",
            "                print('init', rc, out)",
            "            assert rc == EXIT_SUCCESS",
            "            try:",
            "                success, i = True, 0",
            "                while success:",
            "                    i += 1",
            "                    try:",
            "                        make_files(input, 20, 200000)",
            "                    except OSError as err:",
            "                        if err.errno == errno.ENOSPC:",
            "                            # already out of space",
            "                            break",
            "                        raise",
            "                    try:",
            "                        rc, out = cmd('create', '%s::test%03d' % (repo, i), input)",
            "                        success = rc == EXIT_SUCCESS",
            "                        if not success:",
            "                            print('create', rc, out)",
            "                    finally:",
            "                        # make sure repo is not locked",
            "                        shutil.rmtree(os.path.join(repo, 'lock.exclusive'), ignore_errors=True)",
            "                        os.remove(os.path.join(repo, 'lock.roster'))",
            "            finally:",
            "                # now some error happened, likely we are out of disk space.",
            "                # free some space so we can expect borg to be able to work normally:",
            "                shutil.rmtree(reserve, ignore_errors=True)",
            "            rc, out = cmd('list', repo)",
            "            if rc != EXIT_SUCCESS:",
            "                print('list', rc, out)",
            "            rc, out = cmd('check', '--repair', repo)",
            "            if rc != EXIT_SUCCESS:",
            "                print('check', rc, out)",
            "            assert rc == EXIT_SUCCESS",
            "",
            "",
            "class ArchiverTestCaseBase(BaseTestCase):",
            "    EXE = None  # python source based",
            "    FORK_DEFAULT = False",
            "    prefix = ''",
            "",
            "    def setUp(self):",
            "        os.environ['BORG_CHECK_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        os.environ['BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        os.environ['BORG_RECREATE_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        os.environ['BORG_PASSPHRASE'] = 'waytooeasyonlyfortests'",
            "        self.archiver = not self.FORK_DEFAULT and Archiver() or None",
            "        self.tmpdir = tempfile.mkdtemp()",
            "        self.repository_path = os.path.join(self.tmpdir, 'repository')",
            "        self.repository_location = self.prefix + self.repository_path",
            "        self.input_path = os.path.join(self.tmpdir, 'input')",
            "        self.output_path = os.path.join(self.tmpdir, 'output')",
            "        self.keys_path = os.path.join(self.tmpdir, 'keys')",
            "        self.cache_path = os.path.join(self.tmpdir, 'cache')",
            "        self.exclude_file_path = os.path.join(self.tmpdir, 'excludes')",
            "        self.patterns_file_path = os.path.join(self.tmpdir, 'patterns')",
            "        os.environ['BORG_KEYS_DIR'] = self.keys_path",
            "        os.environ['BORG_CACHE_DIR'] = self.cache_path",
            "        os.mkdir(self.input_path)",
            "        os.chmod(self.input_path, 0o777)  # avoid troubles with fakeroot / FUSE",
            "        os.mkdir(self.output_path)",
            "        os.mkdir(self.keys_path)",
            "        os.mkdir(self.cache_path)",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b'input/file2\\n# A comment line, then a blank line\\n\\n')",
            "        with open(self.patterns_file_path, 'wb') as fd:",
            "            fd.write(b'+input/file_important\\n- input/file*\\n# A comment line, then a blank line\\n\\n')",
            "        self._old_wd = os.getcwd()",
            "        os.chdir(self.tmpdir)",
            "",
            "    def tearDown(self):",
            "        os.chdir(self._old_wd)",
            "        # note: ignore_errors=True as workaround for issue #862",
            "        shutil.rmtree(self.tmpdir, ignore_errors=True)",
            "        # destroy logging configuration",
            "        logging.Logger.manager.loggerDict.clear()",
            "        setup_logging()",
            "",
            "    def cmd(self, *args, **kw):",
            "        exit_code = kw.pop('exit_code', 0)",
            "        fork = kw.pop('fork', None)",
            "        if fork is None:",
            "            fork = self.FORK_DEFAULT",
            "        ret, output = exec_cmd(*args, fork=fork, exe=self.EXE, archiver=self.archiver, **kw)",
            "        if ret != exit_code:",
            "            print(output)",
            "        self.assert_equal(ret, exit_code)",
            "        return output",
            "",
            "    def create_src_archive(self, name):",
            "        self.cmd('create', '--compression=lz4', self.repository_location + '::' + name, src_dir)",
            "",
            "    def open_archive(self, name):",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        with repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            archive = Archive(repository, key, manifest, name)",
            "        return archive, repository",
            "",
            "    def open_repository(self):",
            "        return Repository(self.repository_path, exclusive=True)",
            "",
            "    def create_regular_file(self, name, size=0, contents=None):",
            "        filename = os.path.join(self.input_path, name)",
            "        if not os.path.exists(os.path.dirname(filename)):",
            "            os.makedirs(os.path.dirname(filename))",
            "        with open(filename, 'wb') as fd:",
            "            if contents is None:",
            "                contents = b'X' * size",
            "            fd.write(contents)",
            "",
            "    def create_test_files(self):",
            "        \"\"\"Create a minimal test case including all supported file types",
            "        \"\"\"",
            "        # File",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('flagfile', size=1024)",
            "        # Directory",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        # File mode",
            "        os.chmod('input/file1', 0o4755)",
            "        # Hard link",
            "        if are_hardlinks_supported():",
            "            os.link(os.path.join(self.input_path, 'file1'),",
            "                    os.path.join(self.input_path, 'hardlink'))",
            "        # Symlink",
            "        if are_symlinks_supported():",
            "            os.symlink('somewhere', os.path.join(self.input_path, 'link1'))",
            "        self.create_regular_file('fusexattr', size=1)",
            "        if not xattr.XATTR_FAKEROOT and xattr.is_enabled(self.input_path):",
            "            # ironically, due to the way how fakeroot works, comparing FUSE file xattrs to orig file xattrs",
            "            # will FAIL if fakeroot supports xattrs, thus we only set the xattr if XATTR_FAKEROOT is False.",
            "            # This is because fakeroot with xattr-support does not propagate xattrs of the underlying file",
            "            # into \"fakeroot space\". Because the xattrs exposed by borgfs are these of an underlying file",
            "            # (from fakeroots point of view) they are invisible to the test process inside the fakeroot.",
            "            xattr.setxattr(os.path.join(self.input_path, 'fusexattr'), 'user.foo', b'bar')",
            "            xattr.setxattr(os.path.join(self.input_path, 'fusexattr'), 'user.empty', b'')",
            "            # XXX this always fails for me",
            "            # ubuntu 14.04, on a TMP dir filesystem with user_xattr, using fakeroot",
            "            # same for newer ubuntu and centos.",
            "            # if this is supported just on specific platform, platform should be checked first,",
            "            # so that the test setup for all tests using it does not fail here always for others.",
            "            # xattr.setxattr(os.path.join(self.input_path, 'link1'), 'user.foo_symlink', b'bar_symlink', follow_symlinks=False)",
            "        # FIFO node",
            "        if are_fifos_supported():",
            "            os.mkfifo(os.path.join(self.input_path, 'fifo1'))",
            "        if has_lchflags:",
            "            platform.set_flags(os.path.join(self.input_path, 'flagfile'), stat.UF_NODUMP)",
            "        try:",
            "            # Block device",
            "            os.mknod('input/bdev', 0o600 | stat.S_IFBLK, os.makedev(10, 20))",
            "            # Char device",
            "            os.mknod('input/cdev', 0o600 | stat.S_IFCHR, os.makedev(30, 40))",
            "            # File mode",
            "            os.chmod('input/dir2', 0o555)  # if we take away write perms, we need root to remove contents",
            "            # File owner",
            "            os.chown('input/file1', 100, 200)  # raises OSError invalid argument on cygwin",
            "            have_root = True  # we have (fake)root",
            "        except PermissionError:",
            "            have_root = False",
            "        except OSError as e:",
            "            # Note: ENOSYS \"Function not implemented\" happens as non-root on Win 10 Linux Subsystem.",
            "            if e.errno not in (errno.EINVAL, errno.ENOSYS):",
            "                raise",
            "            have_root = False",
            "        time.sleep(1)  # \"empty\" must have newer timestamp than other files",
            "        self.create_regular_file('empty', size=0)",
            "        return have_root",
            "",
            "",
            "class ArchiverTestCase(ArchiverTestCaseBase):",
            "    def test_basic_functionality(self):",
            "        have_root = self.create_test_files()",
            "        # fork required to test show-rc output",
            "        output = self.cmd('init', '--encryption=repokey', '--show-version', '--show-rc', self.repository_location, fork=True)",
            "        self.assert_in('borgbackup version', output)",
            "        self.assert_in('terminating with success status, rc 0', output)",
            "        self.cmd('create', '--exclude-nodump', self.repository_location + '::test', 'input')",
            "        output = self.cmd('create', '--exclude-nodump', '--stats', self.repository_location + '::test.2', 'input')",
            "        self.assert_in('Archive name: test.2', output)",
            "        self.assert_in('This archive: ', output)",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        list_output = self.cmd('list', '--short', self.repository_location)",
            "        self.assert_in('test', list_output)",
            "        self.assert_in('test.2', list_output)",
            "        expected = [",
            "            'input',",
            "            'input/bdev',",
            "            'input/cdev',",
            "            'input/dir2',",
            "            'input/dir2/file2',",
            "            'input/empty',",
            "            'input/file1',",
            "            'input/flagfile',",
            "        ]",
            "        if are_fifos_supported():",
            "            expected.append('input/fifo1')",
            "        if are_symlinks_supported():",
            "            expected.append('input/link1')",
            "        if are_hardlinks_supported():",
            "            expected.append('input/hardlink')",
            "        if not have_root:",
            "            # we could not create these device files without (fake)root",
            "            expected.remove('input/bdev')",
            "            expected.remove('input/cdev')",
            "        if has_lchflags:",
            "            # remove the file we did not backup, so input and output become equal",
            "            expected.remove('input/flagfile')  # this file is UF_NODUMP",
            "            os.remove(os.path.join('input', 'flagfile'))",
            "        list_output = self.cmd('list', '--short', self.repository_location + '::test')",
            "        for name in expected:",
            "            self.assert_in(name, list_output)",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        info_output = self.cmd('info', self.repository_location + '::test')",
            "        item_count = 4 if has_lchflags else 5  # one file is UF_NODUMP",
            "        self.assert_in('Number of files: %d' % item_count, info_output)",
            "        shutil.rmtree(self.cache_path)",
            "        info_output2 = self.cmd('info', self.repository_location + '::test')",
            "",
            "        def filter(output):",
            "            # filter for interesting \"info\" output, ignore cache rebuilding related stuff",
            "            prefixes = ['Name:', 'Fingerprint:', 'Number of files:', 'This archive:',",
            "                        'All archives:', 'Chunk index:', ]",
            "            result = []",
            "            for line in output.splitlines():",
            "                for prefix in prefixes:",
            "                    if line.startswith(prefix):",
            "                        result.append(line)",
            "            return '\\n'.join(result)",
            "",
            "        # the interesting parts of info_output2 and info_output should be same",
            "        self.assert_equal(filter(info_output), filter(info_output2))",
            "",
            "    def test_unix_socket(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        try:",
            "            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)",
            "            sock.bind(os.path.join(self.input_path, 'unix-socket'))",
            "        except PermissionError as err:",
            "            if err.errno == errno.EPERM:",
            "                pytest.skip('unix sockets disabled or not supported')",
            "            elif err.errno == errno.EACCES:",
            "                pytest.skip('permission denied to create unix sockets')",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        sock.close()",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert not os.path.exists('input/unix-socket')",
            "",
            "    @pytest.mark.skipif(not are_symlinks_supported(), reason='symlinks not supported')",
            "    def test_symlink_extract(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert os.readlink('input/link1') == 'somewhere'",
            "",
            "    @pytest.mark.skipif(not is_utime_fully_supported(), reason='cannot properly setup and execute test without utime')",
            "    def test_atime(self):",
            "        def has_noatime(some_file):",
            "            atime_before = os.stat(some_file).st_atime_ns",
            "            try:",
            "                with open(os.open(some_file, flags_noatime)) as file:",
            "                    file.read()",
            "            except PermissionError:",
            "                return False",
            "            else:",
            "                atime_after = os.stat(some_file).st_atime_ns",
            "                noatime_used = flags_noatime != flags_normal",
            "                return noatime_used and atime_before == atime_after",
            "",
            "        self.create_test_files()",
            "        atime, mtime = 123456780, 234567890",
            "        have_noatime = has_noatime('input/file1')",
            "        os.utime('input/file1', (atime, mtime))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        sti = os.stat('input/file1')",
            "        sto = os.stat('output/input/file1')",
            "        assert sti.st_mtime_ns == sto.st_mtime_ns == mtime * 1e9",
            "        if have_noatime:",
            "            assert sti.st_atime_ns == sto.st_atime_ns == atime * 1e9",
            "        else:",
            "            # it touched the input file's atime while backing it up",
            "            assert sto.st_atime_ns == atime * 1e9",
            "",
            "    @pytest.mark.skipif(not is_utime_fully_supported(), reason='cannot properly setup and execute test without utime')",
            "    @pytest.mark.skipif(not is_birthtime_fully_supported(), reason='cannot properly setup and execute test without birthtime')",
            "    def test_birthtime(self):",
            "        self.create_test_files()",
            "        birthtime, mtime, atime = 946598400, 946684800, 946771200",
            "        os.utime('input/file1', (atime, birthtime))",
            "        os.utime('input/file1', (atime, mtime))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        sti = os.stat('input/file1')",
            "        sto = os.stat('output/input/file1')",
            "        assert int(sti.st_birthtime * 1e9) == int(sto.st_birthtime * 1e9) == birthtime * 1e9",
            "        assert sti.st_mtime_ns == sto.st_mtime_ns == mtime * 1e9",
            "",
            "    @pytest.mark.skipif(not is_utime_fully_supported(), reason='cannot properly setup and execute test without utime')",
            "    @pytest.mark.skipif(not is_birthtime_fully_supported(), reason='cannot properly setup and execute test without birthtime')",
            "    def test_nobirthtime(self):",
            "        self.create_test_files()",
            "        birthtime, mtime, atime = 946598400, 946684800, 946771200",
            "        os.utime('input/file1', (atime, birthtime))",
            "        os.utime('input/file1', (atime, mtime))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--nobirthtime', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        sti = os.stat('input/file1')",
            "        sto = os.stat('output/input/file1')",
            "        assert int(sti.st_birthtime * 1e9) == birthtime * 1e9",
            "        assert int(sto.st_birthtime * 1e9) == mtime * 1e9",
            "        assert sti.st_mtime_ns == sto.st_mtime_ns == mtime * 1e9",
            "",
            "    def _extract_repository_id(self, path):",
            "        with Repository(self.repository_path) as repository:",
            "            return repository.id",
            "",
            "    def _set_repository_id(self, path, id):",
            "        config = ConfigParser(interpolation=None)",
            "        config.read(os.path.join(path, 'config'))",
            "        config.set('repository', 'id', bin_to_hex(id))",
            "        with open(os.path.join(path, 'config'), 'w') as fd:",
            "            config.write(fd)",
            "        with Repository(self.repository_path) as repository:",
            "            return repository.id",
            "",
            "    def test_sparse_file(self):",
            "        def is_sparse(fn, total_size, hole_size):",
            "            st = os.stat(fn)",
            "            assert st.st_size == total_size",
            "            sparse = True",
            "            if sparse and hasattr(st, 'st_blocks') and st.st_blocks * 512 >= st.st_size:",
            "                sparse = False",
            "            if sparse and hasattr(os, 'SEEK_HOLE') and hasattr(os, 'SEEK_DATA'):",
            "                with open(fn, 'rb') as fd:",
            "                    # only check if the first hole is as expected, because the 2nd hole check",
            "                    # is problematic on xfs due to its \"dynamic speculative EOF preallocation",
            "                    try:",
            "                        if fd.seek(0, os.SEEK_HOLE) != 0:",
            "                            sparse = False",
            "                        if fd.seek(0, os.SEEK_DATA) != hole_size:",
            "                            sparse = False",
            "                    except OSError:",
            "                        # OS/FS does not really support SEEK_HOLE/SEEK_DATA",
            "                        sparse = False",
            "            return sparse",
            "",
            "        filename = os.path.join(self.input_path, 'sparse')",
            "        content = b'foobar'",
            "        hole_size = 5 * (1 << CHUNK_MAX_EXP)  # 5 full chunker buffers",
            "        total_size = hole_size + len(content) + hole_size",
            "        with open(filename, 'wb') as fd:",
            "            # create a file that has a hole at the beginning and end (if the",
            "            # OS and filesystem supports sparse files)",
            "            fd.seek(hole_size, 1)",
            "            fd.write(content)",
            "            fd.seek(hole_size, 1)",
            "            pos = fd.tell()",
            "            fd.truncate(pos)",
            "        # we first check if we could create a sparse input file:",
            "        sparse_support = is_sparse(filename, total_size, hole_size)",
            "        if sparse_support:",
            "            # we could create a sparse input file, so creating a backup of it and",
            "            # extracting it again (as sparse) should also work:",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "            self.cmd('create', self.repository_location + '::test', 'input')",
            "            with changedir(self.output_path):",
            "                self.cmd('extract', '--sparse', self.repository_location + '::test')",
            "            self.assert_dirs_equal('input', 'output/input')",
            "            filename = os.path.join(self.output_path, 'input', 'sparse')",
            "            with open(filename, 'rb') as fd:",
            "                # check if file contents are as expected",
            "                self.assert_equal(fd.read(hole_size), b'\\0' * hole_size)",
            "                self.assert_equal(fd.read(len(content)), content)",
            "                self.assert_equal(fd.read(hole_size), b'\\0' * hole_size)",
            "            self.assert_true(is_sparse(filename, total_size, hole_size))",
            "",
            "    def test_unusual_filenames(self):",
            "        filenames = ['normal', 'with some blanks', '(with_parens)', ]",
            "        for filename in filenames:",
            "            filename = os.path.join(self.input_path, filename)",
            "            with open(filename, 'wb'):",
            "                pass",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        for filename in filenames:",
            "            with changedir('output'):",
            "                self.cmd('extract', self.repository_location + '::test', os.path.join('input', filename))",
            "            assert os.path.exists(os.path.join('output', 'input', filename))",
            "",
            "    def test_repository_swap_detection(self):",
            "        self.create_test_files()",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        shutil.rmtree(self.repository_path)",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self._set_repository_id(self.repository_path, repository_id)",
            "        self.assert_equal(repository_id, self._extract_repository_id(self.repository_path))",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.EncryptionMethodMismatch):",
            "                self.cmd('create', self.repository_location + '::test.2', 'input')",
            "",
            "    def test_repository_swap_detection2(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=none', self.repository_location + '_unencrypted')",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location + '_encrypted')",
            "        self.cmd('create', self.repository_location + '_encrypted::test', 'input')",
            "        shutil.rmtree(self.repository_path + '_encrypted')",
            "        os.rename(self.repository_path + '_unencrypted', self.repository_path + '_encrypted')",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '_encrypted::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.RepositoryAccessAborted):",
            "                self.cmd('create', self.repository_location + '_encrypted::test.2', 'input')",
            "",
            "    def test_repository_swap_detection_no_cache(self):",
            "        self.create_test_files()",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        shutil.rmtree(self.repository_path)",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self._set_repository_id(self.repository_path, repository_id)",
            "        self.assert_equal(repository_id, self._extract_repository_id(self.repository_path))",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.EncryptionMethodMismatch):",
            "                self.cmd('create', self.repository_location + '::test.2', 'input')",
            "",
            "    def test_repository_swap_detection2_no_cache(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=none', self.repository_location + '_unencrypted')",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=repokey', self.repository_location + '_encrypted')",
            "        self.cmd('create', self.repository_location + '_encrypted::test', 'input')",
            "        self.cmd('delete', '--cache-only', self.repository_location + '_unencrypted')",
            "        self.cmd('delete', '--cache-only', self.repository_location + '_encrypted')",
            "        shutil.rmtree(self.repository_path + '_encrypted')",
            "        os.rename(self.repository_path + '_unencrypted', self.repository_path + '_encrypted')",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '_encrypted::test.2', 'input', exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.RepositoryAccessAborted):",
            "                self.cmd('create', self.repository_location + '_encrypted::test.2', 'input')",
            "",
            "    def test_repository_swap_detection_repokey_blank_passphrase(self):",
            "        # Check that a repokey repo with a blank passphrase is considered like a plaintext repo.",
            "        self.create_test_files()",
            "        # User initializes her repository with her passphrase",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        # Attacker replaces it with her own repository, which is encrypted but has no passphrase set",
            "        shutil.rmtree(self.repository_path)",
            "        with environment_variable(BORG_PASSPHRASE=''):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "            # Delete cache & security database, AKA switch to user perspective",
            "            self.cmd('delete', '--cache-only', self.repository_location)",
            "            repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "            shutil.rmtree(get_security_dir(repository_id))",
            "        with environment_variable(BORG_PASSPHRASE=None):",
            "            # This is the part were the user would be tricked, e.g. she assumes that BORG_PASSPHRASE",
            "            # is set, while it isn't. Previously this raised no warning,",
            "            # since the repository is, technically, encrypted.",
            "            if self.FORK_DEFAULT:",
            "                self.cmd('create', self.repository_location + '::test.2', 'input', exit_code=EXIT_ERROR)",
            "            else:",
            "                with pytest.raises(Cache.CacheInitAbortedError):",
            "                    self.cmd('create', self.repository_location + '::test.2', 'input')",
            "",
            "    def test_repository_move(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "        os.rename(self.repository_path, self.repository_path + '_new')",
            "        with environment_variable(BORG_RELOCATED_REPO_ACCESS_IS_OK='yes'):",
            "            self.cmd('info', self.repository_location + '_new')",
            "        security_dir = get_security_dir(repository_id)",
            "        with open(os.path.join(security_dir, 'location')) as fd:",
            "            location = fd.read()",
            "            assert location == Location(self.repository_location + '_new').canonical_path()",
            "        # Needs no confirmation anymore",
            "        self.cmd('info', self.repository_location + '_new')",
            "        shutil.rmtree(self.cache_path)",
            "        self.cmd('info', self.repository_location + '_new')",
            "        shutil.rmtree(security_dir)",
            "        self.cmd('info', self.repository_location + '_new')",
            "        for file in ('location', 'key-type', 'manifest-timestamp'):",
            "            assert os.path.exists(os.path.join(security_dir, file))",
            "",
            "    def test_security_dir_compat(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "        security_dir = get_security_dir(repository_id)",
            "        with open(os.path.join(security_dir, 'location'), 'w') as fd:",
            "            fd.write('something outdated')",
            "        # This is fine, because the cache still has the correct information. security_dir and cache can disagree",
            "        # if older versions are used to confirm a renamed repository.",
            "        self.cmd('info', self.repository_location)",
            "",
            "    def test_unknown_unencrypted(self):",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        repository_id = bin_to_hex(self._extract_repository_id(self.repository_path))",
            "        security_dir = get_security_dir(repository_id)",
            "        # Ok: repository is known",
            "        self.cmd('info', self.repository_location)",
            "",
            "        # Ok: repository is still known (through security_dir)",
            "        shutil.rmtree(self.cache_path)",
            "        self.cmd('info', self.repository_location)",
            "",
            "        # Needs confirmation: cache and security dir both gone (eg. another host or rm -rf ~)",
            "        shutil.rmtree(self.cache_path)",
            "        shutil.rmtree(security_dir)",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('info', self.repository_location, exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(Cache.CacheInitAbortedError):",
            "                self.cmd('info', self.repository_location)",
            "        with environment_variable(BORG_UNKNOWN_UNENCRYPTED_REPO_ACCESS_IS_OK='yes'):",
            "            self.cmd('info', self.repository_location)",
            "",
            "    def test_strip_components(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('dir/file')",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', '--strip-components', '3')",
            "            self.assert_true(not os.path.exists('file'))",
            "            with self.assert_creates_file('file'):",
            "                self.cmd('extract', self.repository_location + '::test', '--strip-components', '2')",
            "            with self.assert_creates_file('dir/file'):",
            "                self.cmd('extract', self.repository_location + '::test', '--strip-components', '1')",
            "            with self.assert_creates_file('input/dir/file'):",
            "                self.cmd('extract', self.repository_location + '::test', '--strip-components', '0')",
            "",
            "    def _extract_hardlinks_setup(self):",
            "        os.mkdir(os.path.join(self.input_path, 'dir1'))",
            "        os.mkdir(os.path.join(self.input_path, 'dir1/subdir'))",
            "",
            "        self.create_regular_file('source', contents=b'123456')",
            "        os.link(os.path.join(self.input_path, 'source'),",
            "                os.path.join(self.input_path, 'abba'))",
            "        os.link(os.path.join(self.input_path, 'source'),",
            "                os.path.join(self.input_path, 'dir1/hardlink'))",
            "        os.link(os.path.join(self.input_path, 'source'),",
            "                os.path.join(self.input_path, 'dir1/subdir/hardlink'))",
            "",
            "        self.create_regular_file('dir1/source2')",
            "        os.link(os.path.join(self.input_path, 'dir1/source2'),",
            "                os.path.join(self.input_path, 'dir1/aaaa'))",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "    requires_hardlinks = pytest.mark.skipif(not are_hardlinks_supported(), reason='hardlinks not supported')",
            "",
            "    @requires_hardlinks",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_mount_hardlinks(self):",
            "        self._extract_hardlinks_setup()",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        with self.fuse_mount(self.repository_location + '::test', mountpoint, '--strip-components=2'), \\",
            "             changedir(mountpoint):",
            "            assert os.stat('hardlink').st_nlink == 2",
            "            assert os.stat('subdir/hardlink').st_nlink == 2",
            "            assert open('subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('aaaa').st_nlink == 2",
            "            assert os.stat('source2').st_nlink == 2",
            "        with self.fuse_mount(self.repository_location + '::test', mountpoint, 'input/dir1'), \\",
            "             changedir(mountpoint):",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "        with self.fuse_mount(self.repository_location + '::test', mountpoint), \\",
            "             changedir(mountpoint):",
            "            assert os.stat('input/source').st_nlink == 4",
            "            assert os.stat('input/abba').st_nlink == 4",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 4",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 4",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "",
            "    @requires_hardlinks",
            "    def test_extract_hardlinks(self):",
            "        self._extract_hardlinks_setup()",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', '--strip-components', '2')",
            "            assert os.stat('hardlink').st_nlink == 2",
            "            assert os.stat('subdir/hardlink').st_nlink == 2",
            "            assert open('subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('aaaa').st_nlink == 2",
            "            assert os.stat('source2').st_nlink == 2",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', 'input/dir1')",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert os.stat('input/source').st_nlink == 4",
            "            assert os.stat('input/abba').st_nlink == 4",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 4",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 4",
            "            assert open('input/dir1/subdir/hardlink', 'rb').read() == b'123456'",
            "",
            "    def test_extract_include_exclude(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.cmd('create', '--exclude=input/file4', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', 'input/file1', )",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1'])",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude=input/file2', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file3'])",
            "",
            "    def test_extract_include_exclude_regex(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.create_regular_file('file333', size=1024 * 80)",
            "",
            "        # Create with regular expression exclusion for file4",
            "        self.cmd('create', '--exclude=re:input/file4$', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2', 'file3', 'file333'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Extract with regular expression exclusion",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude=re:file3+', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Combine --exclude with fnmatch and regular expression",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude=input/file2', '--exclude=re:file[01]', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file3', 'file333'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Combine --exclude-from and regular expression exclusion",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, '--exclude=re:file1',",
            "                     '--exclude=re:file(\\\\d)\\\\1\\\\1$', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file3'])",
            "",
            "    def test_extract_include_exclude_regex_from_file(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file3', size=1024 * 80)",
            "        self.create_regular_file('file4', size=1024 * 80)",
            "        self.create_regular_file('file333', size=1024 * 80)",
            "        self.create_regular_file('aa:something', size=1024 * 80)",
            "",
            "        # Create while excluding using mixed pattern styles",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b're:input/file4$\\n')",
            "            fd.write(b'fm:*aa:*thing\\n')",
            "",
            "        self.cmd('create', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2', 'file3', 'file333'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Exclude using regular expression",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b're:file3+\\n')",
            "",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1', 'file2'])",
            "        shutil.rmtree('output/input')",
            "",
            "        # Mixed exclude pattern styles",
            "        with open(self.exclude_file_path, 'wb') as fd:",
            "            fd.write(b're:file(\\\\d)\\\\1\\\\1$\\n')",
            "            fd.write(b'fm:nothingwillmatchthis\\n')",
            "            fd.write(b'*/file1\\n')",
            "            fd.write(b're:file2$\\n')",
            "",
            "        with changedir('output'):",
            "            self.cmd('extract', '--exclude-from=' + self.exclude_file_path, self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file3'])",
            "",
            "    def test_extract_with_pattern(self):",
            "        self.cmd(\"init\", '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file(\"file1\", size=1024 * 80)",
            "        self.create_regular_file(\"file2\", size=1024 * 80)",
            "        self.create_regular_file(\"file3\", size=1024 * 80)",
            "        self.create_regular_file(\"file4\", size=1024 * 80)",
            "        self.create_regular_file(\"file333\", size=1024 * 80)",
            "",
            "        self.cmd(\"create\", self.repository_location + \"::test\", \"input\")",
            "",
            "        # Extract everything with regular expression",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", self.repository_location + \"::test\", \"re:.*\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file1\", \"file2\", \"file3\", \"file333\", \"file4\"])",
            "        shutil.rmtree(\"output/input\")",
            "",
            "        # Extract with pattern while also excluding files",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", \"--exclude=re:file[34]$\", self.repository_location + \"::test\", r\"re:file\\d$\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file1\", \"file2\"])",
            "        shutil.rmtree(\"output/input\")",
            "",
            "        # Combine --exclude with pattern for extraction",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", \"--exclude=input/file1\", self.repository_location + \"::test\", \"re:file[12]$\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file2\"])",
            "        shutil.rmtree(\"output/input\")",
            "",
            "        # Multiple pattern",
            "        with changedir(\"output\"):",
            "            self.cmd(\"extract\", self.repository_location + \"::test\", \"fm:input/file1\", \"fm:*file33*\", \"input/file2\")",
            "        self.assert_equal(sorted(os.listdir(\"output/input\")), [\"file1\", \"file2\", \"file333\"])",
            "",
            "    def test_extract_list_output(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file', size=1024 * 80)",
            "",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_not_in(\"input/file\", output)",
            "        shutil.rmtree('output/input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', '--info', self.repository_location + '::test')",
            "        self.assert_not_in(\"input/file\", output)",
            "        shutil.rmtree('output/input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', '--list', self.repository_location + '::test')",
            "        self.assert_in(\"input/file\", output)",
            "        shutil.rmtree('output/input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', '--list', '--info', self.repository_location + '::test')",
            "        self.assert_in(\"input/file\", output)",
            "",
            "    def test_extract_progress(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file', size=1024 * 80)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "        with changedir('output'):",
            "            output = self.cmd('extract', self.repository_location + '::test', '--progress')",
            "            assert 'Extracting:' in output",
            "",
            "    def _create_test_caches(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('cache1/%s' % CACHE_TAG_NAME,",
            "                                 contents=CACHE_TAG_CONTENTS + b' extra stuff')",
            "        self.create_regular_file('cache2/%s' % CACHE_TAG_NAME,",
            "                                 contents=b'invalid signature')",
            "        os.mkdir('input/cache3')",
            "        os.link('input/cache1/%s' % CACHE_TAG_NAME, 'input/cache3/%s' % CACHE_TAG_NAME)",
            "",
            "    def test_create_stdin(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        input_data = b'\\x00foo\\n\\nbar\\n   \\n'",
            "        self.cmd('create', self.repository_location + '::test', '-', input=input_data)",
            "        item = json.loads(self.cmd('list', '--json-lines', self.repository_location + '::test'))",
            "        assert item['uid'] == 0",
            "        assert item['gid'] == 0",
            "        assert item['size'] == len(input_data)",
            "        assert item['path'] == 'stdin'",
            "        extracted_data = self.cmd('extract', '--stdout', self.repository_location + '::test', binary_output=True)",
            "        assert extracted_data == input_data",
            "",
            "    def test_create_without_root(self):",
            "        \"\"\"test create without a root\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', exit_code=2)",
            "",
            "    def test_create_pattern_root(self):",
            "        \"\"\"test create with only a root pattern\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list', '--pattern=R input', self.repository_location + '::test')",
            "        self.assert_in(\"A input/file1\", output)",
            "        self.assert_in(\"A input/file2\", output)",
            "",
            "    def test_create_pattern(self):",
            "        \"\"\"test file patterns during create\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file_important', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--pattern=+input/file_important', '--pattern=-input/file*',",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file_important\", output)",
            "        self.assert_in('x input/file1', output)",
            "        self.assert_in('x input/file2', output)",
            "",
            "    def test_create_pattern_file(self):",
            "        \"\"\"test file patterns during create\"\"\"",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('otherfile', size=1024 * 80)",
            "        self.create_regular_file('file_important', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--pattern=-input/otherfile', '--patterns-from=' + self.patterns_file_path,",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file_important\", output)",
            "        self.assert_in('x input/file1', output)",
            "        self.assert_in('x input/file2', output)",
            "        self.assert_in('x input/otherfile', output)",
            "",
            "    def test_create_pattern_exclude_folder_but_recurse(self):",
            "        \"\"\"test when patterns exclude a parent folder, but include a child\"\"\"",
            "        self.patterns_file_path2 = os.path.join(self.tmpdir, 'patterns2')",
            "        with open(self.patterns_file_path2, 'wb') as fd:",
            "            fd.write(b'+ input/x/b\\n- input/x*\\n')",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('x/a/foo_a', size=1024 * 80)",
            "        self.create_regular_file('x/b/foo_b', size=1024 * 80)",
            "        self.create_regular_file('y/foo_y', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--patterns-from=' + self.patterns_file_path2,",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_in('x input/x/a/foo_a', output)",
            "        self.assert_in(\"A input/x/b/foo_b\", output)",
            "        self.assert_in('A input/y/foo_y', output)",
            "",
            "    def test_create_pattern_exclude_folder_no_recurse(self):",
            "        \"\"\"test when patterns exclude a parent folder and, but include a child\"\"\"",
            "        self.patterns_file_path2 = os.path.join(self.tmpdir, 'patterns2')",
            "        with open(self.patterns_file_path2, 'wb') as fd:",
            "            fd.write(b'+ input/x/b\\n! input/x*\\n')",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('x/a/foo_a', size=1024 * 80)",
            "        self.create_regular_file('x/b/foo_b', size=1024 * 80)",
            "        self.create_regular_file('y/foo_y', size=1024 * 80)",
            "        output = self.cmd('create', '-v', '--list',",
            "                          '--patterns-from=' + self.patterns_file_path2,",
            "                          self.repository_location + '::test', 'input')",
            "        self.assert_not_in('input/x/a/foo_a', output)",
            "        self.assert_not_in('input/x/a', output)",
            "        self.assert_in('A input/y/foo_y', output)",
            "",
            "    def test_create_pattern_intermediate_folders_first(self):",
            "        \"\"\"test that intermediate folders appear first when patterns exclude a parent folder but include a child\"\"\"",
            "        self.patterns_file_path2 = os.path.join(self.tmpdir, 'patterns2')",
            "        with open(self.patterns_file_path2, 'wb') as fd:",
            "            fd.write(b'+ input/x/a\\n+ input/x/b\\n- input/x*\\n')",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "        self.create_regular_file('x/a/foo_a', size=1024 * 80)",
            "        self.create_regular_file('x/b/foo_b', size=1024 * 80)",
            "        with changedir('input'):",
            "            self.cmd('create', '--patterns-from=' + self.patterns_file_path2,",
            "                     self.repository_location + '::test', '.')",
            "",
            "        # list the archive and verify that the \"intermediate\" folders appear before",
            "        # their contents",
            "        out = self.cmd('list', '--format', '{type} {path}{NL}', self.repository_location + '::test')",
            "        out_list = out.splitlines()",
            "",
            "        self.assert_in('d x/a', out_list)",
            "        self.assert_in('d x/b', out_list)",
            "",
            "        assert out_list.index('d x/a') < out_list.index('- x/a/foo_a')",
            "        assert out_list.index('d x/b') < out_list.index('- x/b/foo_b')",
            "",
            "    def test_create_no_cache_sync(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        create_json = json.loads(self.cmd('create', '--no-cache-sync', self.repository_location + '::test', 'input',",
            "                                          '--json', '--error'))  # ignore experimental warning",
            "        info_json = json.loads(self.cmd('info', self.repository_location + '::test', '--json'))",
            "        create_stats = create_json['cache']['stats']",
            "        info_stats = info_json['cache']['stats']",
            "        assert create_stats == info_stats",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        self.cmd('create', '--no-cache-sync', self.repository_location + '::test2', 'input')",
            "        self.cmd('info', self.repository_location)",
            "        self.cmd('check', self.repository_location)",
            "",
            "    def test_extract_pattern_opt(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.create_regular_file('file_important', size=1024 * 80)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract',",
            "                     '--pattern=+input/file_important', '--pattern=-input/file*',",
            "                     self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file_important'])",
            "",
            "    def _assert_test_caches(self):",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['cache2', 'file1'])",
            "        self.assert_equal(sorted(os.listdir('output/input/cache2')), [CACHE_TAG_NAME])",
            "",
            "    def test_exclude_caches(self):",
            "        self._create_test_caches()",
            "        self.cmd('create', '--exclude-caches', self.repository_location + '::test', 'input')",
            "        self._assert_test_caches()",
            "",
            "    def test_recreate_exclude_caches(self):",
            "        self._create_test_caches()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('recreate', '--exclude-caches', self.repository_location + '::test')",
            "        self._assert_test_caches()",
            "",
            "    def _create_test_tagged(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('tagged1/.NOBACKUP')",
            "        self.create_regular_file('tagged2/00-NOBACKUP')",
            "        self.create_regular_file('tagged3/.NOBACKUP/file2', size=1024)",
            "",
            "    def _assert_test_tagged(self):",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file1'])",
            "",
            "    def test_exclude_tagged(self):",
            "        self._create_test_tagged()",
            "        self.cmd('create', '--exclude-if-present', '.NOBACKUP', '--exclude-if-present', '00-NOBACKUP', self.repository_location + '::test', 'input')",
            "        self._assert_test_tagged()",
            "",
            "    def test_recreate_exclude_tagged(self):",
            "        self._create_test_tagged()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('recreate', '--exclude-if-present', '.NOBACKUP', '--exclude-if-present', '00-NOBACKUP',",
            "                 self.repository_location + '::test')",
            "        self._assert_test_tagged()",
            "",
            "    def _create_test_keep_tagged(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file0', size=1024)",
            "        self.create_regular_file('tagged1/.NOBACKUP1')",
            "        self.create_regular_file('tagged1/file1', size=1024)",
            "        self.create_regular_file('tagged2/.NOBACKUP2/subfile1', size=1024)",
            "        self.create_regular_file('tagged2/file2', size=1024)",
            "        self.create_regular_file('tagged3/%s' % CACHE_TAG_NAME,",
            "                                 contents=CACHE_TAG_CONTENTS + b' extra stuff')",
            "        self.create_regular_file('tagged3/file3', size=1024)",
            "        self.create_regular_file('taggedall/.NOBACKUP1')",
            "        self.create_regular_file('taggedall/.NOBACKUP2/subfile1', size=1024)",
            "        self.create_regular_file('taggedall/%s' % CACHE_TAG_NAME,",
            "                                 contents=CACHE_TAG_CONTENTS + b' extra stuff')",
            "        self.create_regular_file('taggedall/file4', size=1024)",
            "",
            "    def _assert_test_keep_tagged(self):",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file0', 'tagged1', 'tagged2', 'tagged3', 'taggedall'])",
            "        self.assert_equal(os.listdir('output/input/tagged1'), ['.NOBACKUP1'])",
            "        self.assert_equal(os.listdir('output/input/tagged2'), ['.NOBACKUP2'])",
            "        self.assert_equal(os.listdir('output/input/tagged3'), [CACHE_TAG_NAME])",
            "        self.assert_equal(sorted(os.listdir('output/input/taggedall')),",
            "                          ['.NOBACKUP1', '.NOBACKUP2', CACHE_TAG_NAME, ])",
            "",
            "    def test_exclude_keep_tagged_deprecation(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output_warn = self.cmd('create', '--exclude-caches', '--keep-tag-files', self.repository_location + '::test', src_dir)",
            "        self.assert_in('--keep-tag-files\" has been deprecated.', output_warn)",
            "",
            "    def test_exclude_keep_tagged(self):",
            "        self._create_test_keep_tagged()",
            "        self.cmd('create', '--exclude-if-present', '.NOBACKUP1', '--exclude-if-present', '.NOBACKUP2',",
            "                 '--exclude-caches', '--keep-exclude-tags', self.repository_location + '::test', 'input')",
            "        self._assert_test_keep_tagged()",
            "",
            "    def test_recreate_exclude_keep_tagged(self):",
            "        self._create_test_keep_tagged()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('recreate', '--exclude-if-present', '.NOBACKUP1', '--exclude-if-present', '.NOBACKUP2',",
            "                 '--exclude-caches', '--keep-exclude-tags', self.repository_location + '::test')",
            "        self._assert_test_keep_tagged()",
            "",
            "    @pytest.mark.skipif(not xattr.XATTR_FAKEROOT, reason='Linux capabilities test, requires fakeroot >= 1.20.2')",
            "    def test_extract_capabilities(self):",
            "        fchown = os.fchown",
            "",
            "        # We need to manually patch chown to get the behaviour Linux has, since fakeroot does not",
            "        # accurately model the interaction of chown(2) and Linux capabilities, i.e. it does not remove them.",
            "        def patched_fchown(fd, uid, gid):",
            "            xattr.setxattr(fd, 'security.capability', None, follow_symlinks=False)",
            "            fchown(fd, uid, gid)",
            "",
            "        # The capability descriptor used here is valid and taken from a /usr/bin/ping",
            "        capabilities = b'\\x01\\x00\\x00\\x02\\x00 \\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'",
            "        self.create_regular_file('file')",
            "        xattr.setxattr('input/file', 'security.capability', capabilities)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            with patch.object(os, 'fchown', patched_fchown):",
            "                self.cmd('extract', self.repository_location + '::test')",
            "            assert xattr.getxattr('input/file', 'security.capability') == capabilities",
            "",
            "    @pytest.mark.skipif(not xattr.XATTR_FAKEROOT, reason='xattr not supported on this system or on this version of'",
            "                                                         'fakeroot')",
            "    def test_extract_xattrs_errors(self):",
            "        def patched_setxattr_E2BIG(*args, **kwargs):",
            "            raise OSError(errno.E2BIG, 'E2BIG')",
            "",
            "        def patched_setxattr_ENOTSUP(*args, **kwargs):",
            "            raise OSError(errno.ENOTSUP, 'ENOTSUP')",
            "",
            "        def patched_setxattr_EACCES(*args, **kwargs):",
            "            raise OSError(errno.EACCES, 'EACCES')",
            "",
            "        self.create_regular_file('file')",
            "        xattr.setxattr('input/file', 'attribute', 'value')",
            "        self.cmd('init', self.repository_location, '-e' 'none')",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            input_abspath = os.path.abspath('input/file')",
            "            with patch.object(xattr, 'setxattr', patched_setxattr_E2BIG):",
            "                out = self.cmd('extract', self.repository_location + '::test', exit_code=EXIT_WARNING)",
            "                assert out == (input_abspath + ': Value or key of extended attribute attribute is too big for this '",
            "                                               'filesystem\\n')",
            "            os.remove(input_abspath)",
            "            with patch.object(xattr, 'setxattr', patched_setxattr_ENOTSUP):",
            "                out = self.cmd('extract', self.repository_location + '::test', exit_code=EXIT_WARNING)",
            "                assert out == (input_abspath + ': Extended attributes are not supported on this filesystem\\n')",
            "            os.remove(input_abspath)",
            "            with patch.object(xattr, 'setxattr', patched_setxattr_EACCES):",
            "                out = self.cmd('extract', self.repository_location + '::test', exit_code=EXIT_WARNING)",
            "                assert out == (input_abspath + ': Permission denied when setting extended attribute attribute\\n')",
            "            assert os.path.isfile(input_abspath)",
            "",
            "    def test_path_normalization(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('dir1/dir2/file', size=1024 * 80)",
            "        with changedir('input/dir1/dir2'):",
            "            self.cmd('create', self.repository_location + '::test', '../../../input/dir1/../dir1/dir2/..')",
            "        output = self.cmd('list', self.repository_location + '::test')",
            "        self.assert_not_in('..', output)",
            "        self.assert_in(' input/dir1/dir2/file', output)",
            "",
            "    def test_exclude_normalization(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        with changedir('input'):",
            "            self.cmd('create', '--exclude=file1', self.repository_location + '::test1', '.')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test1')",
            "        self.assert_equal(sorted(os.listdir('output')), ['file2'])",
            "        with changedir('input'):",
            "            self.cmd('create', '--exclude=./file1', self.repository_location + '::test2', '.')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test2')",
            "        self.assert_equal(sorted(os.listdir('output')), ['file2'])",
            "        self.cmd('create', '--exclude=input/./file1', self.repository_location + '::test3', 'input')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test3')",
            "        self.assert_equal(sorted(os.listdir('output/input')), ['file2'])",
            "",
            "    def test_repeated_files(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input', 'input')",
            "",
            "    def test_overwrite(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        # Overwriting regular files and directories should be supported",
            "        os.mkdir('output/input')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/dir2')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "        self.assert_dirs_equal('input', 'output/input')",
            "        # But non-empty dirs should fail",
            "        os.unlink('output/input/file1')",
            "        os.mkdir('output/input/file1')",
            "        os.mkdir('output/input/file1/dir')",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test', exit_code=1)",
            "",
            "    def test_rename(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.cmd('rename', self.repository_location + '::test', 'test.3')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.cmd('rename', self.repository_location + '::test.2', 'test.4')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.3')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.4')",
            "        # Make sure both archives have been renamed",
            "        with Repository(self.repository_path) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "        self.assert_equal(len(manifest.archives), 2)",
            "        self.assert_in('test.3', manifest.archives)",
            "        self.assert_in('test.4', manifest.archives)",
            "",
            "    def test_info(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        info_repo = self.cmd('info', self.repository_location)",
            "        assert 'All archives:' in info_repo",
            "        info_archive = self.cmd('info', self.repository_location + '::test')",
            "        assert 'Archive name: test\\n' in info_archive",
            "        info_archive = self.cmd('info', '--first', '1', self.repository_location)",
            "        assert 'Archive name: test\\n' in info_archive",
            "",
            "    def test_info_json(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        info_repo = json.loads(self.cmd('info', '--json', self.repository_location))",
            "        repository = info_repo['repository']",
            "        assert len(repository['id']) == 64",
            "        assert 'last_modified' in repository",
            "        assert datetime.strptime(repository['last_modified'], ISO_FORMAT)  # must not raise",
            "        assert info_repo['encryption']['mode'] == 'repokey'",
            "        assert 'keyfile' not in info_repo['encryption']",
            "        cache = info_repo['cache']",
            "        stats = cache['stats']",
            "        assert all(isinstance(o, int) for o in stats.values())",
            "        assert all(key in stats for key in ('total_chunks', 'total_csize', 'total_size', 'total_unique_chunks', 'unique_csize', 'unique_size'))",
            "",
            "        info_archive = json.loads(self.cmd('info', '--json', self.repository_location + '::test'))",
            "        assert info_repo['repository'] == info_archive['repository']",
            "        assert info_repo['cache'] == info_archive['cache']",
            "        archives = info_archive['archives']",
            "        assert len(archives) == 1",
            "        archive = archives[0]",
            "        assert archive['name'] == 'test'",
            "        assert isinstance(archive['command_line'], list)",
            "        assert isinstance(archive['duration'], float)",
            "        assert len(archive['id']) == 64",
            "        assert 'stats' in archive",
            "        assert datetime.strptime(archive['start'], ISO_FORMAT)",
            "        assert datetime.strptime(archive['end'], ISO_FORMAT)",
            "",
            "    def test_comment(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "        self.cmd('create', '--comment', 'this is the comment', self.repository_location + '::test2', 'input')",
            "        self.cmd('create', '--comment', '\"deleted\" comment', self.repository_location + '::test3', 'input')",
            "        self.cmd('create', '--comment', 'preserved comment', self.repository_location + '::test4', 'input')",
            "        assert 'Comment: \\n' in self.cmd('info', self.repository_location + '::test1')",
            "        assert 'Comment: this is the comment' in self.cmd('info', self.repository_location + '::test2')",
            "",
            "        self.cmd('recreate', self.repository_location + '::test1', '--comment', 'added comment')",
            "        self.cmd('recreate', self.repository_location + '::test2', '--comment', 'modified comment')",
            "        self.cmd('recreate', self.repository_location + '::test3', '--comment', '')",
            "        self.cmd('recreate', self.repository_location + '::test4', '12345')",
            "        assert 'Comment: added comment' in self.cmd('info', self.repository_location + '::test1')",
            "        assert 'Comment: modified comment' in self.cmd('info', self.repository_location + '::test2')",
            "        assert 'Comment: \\n' in self.cmd('info', self.repository_location + '::test3')",
            "        assert 'Comment: preserved comment' in self.cmd('info', self.repository_location + '::test4')",
            "",
            "    def test_delete(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        self.cmd('create', self.repository_location + '::test.3', 'input')",
            "        self.cmd('create', self.repository_location + '::another_test.1', 'input')",
            "        self.cmd('create', self.repository_location + '::another_test.2', 'input')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        self.cmd('delete', '--prefix', 'another_', self.repository_location)",
            "        self.cmd('delete', '--last', '1', self.repository_location)",
            "        self.cmd('delete', self.repository_location + '::test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test.2')",
            "        output = self.cmd('delete', '--stats', self.repository_location + '::test.2')",
            "        self.assert_in('Deleted data:', output)",
            "        # Make sure all data except the manifest has been deleted",
            "        with Repository(self.repository_path) as repository:",
            "            self.assert_equal(len(repository), 1)",
            "",
            "    def test_delete_multiple(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "        self.cmd('create', self.repository_location + '::test2', 'input')",
            "        self.cmd('create', self.repository_location + '::test3', 'input')",
            "        self.cmd('delete', self.repository_location + '::test1', 'test2')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test3')",
            "        self.cmd('delete', self.repository_location, 'test3')",
            "        assert not self.cmd('list', self.repository_location)",
            "",
            "    def test_delete_repo(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.create_regular_file('dir2/file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        os.environ['BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'] = 'no'",
            "        self.cmd('delete', self.repository_location, exit_code=2)",
            "        assert os.path.exists(self.repository_path)",
            "        os.environ['BORG_DELETE_I_KNOW_WHAT_I_AM_DOING'] = 'YES'",
            "        self.cmd('delete', self.repository_location)",
            "        # Make sure the repo is gone",
            "        self.assertFalse(os.path.exists(self.repository_path))",
            "",
            "    def test_delete_force(self):",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self.create_src_archive('test')",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            archive = Archive(repository, key, manifest, 'test')",
            "            for item in archive.iter_items():",
            "                if 'chunks' in item:",
            "                    first_chunk_id = item.chunks[0].id",
            "                    repository.delete(first_chunk_id)",
            "                    repository.commit()",
            "                    break",
            "        output = self.cmd('delete', '--force', self.repository_location + '::test')",
            "        self.assert_in('deleted archive was corrupted', output)",
            "        self.cmd('check', '--repair', self.repository_location)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test', output)",
            "",
            "    def test_delete_double_force(self):",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        self.create_src_archive('test')",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            archive = Archive(repository, key, manifest, 'test')",
            "            id = archive.metadata.items[0]",
            "            repository.put(id, b'corrupted items metadata stream chunk')",
            "            repository.commit()",
            "        self.cmd('delete', '--force', '--force', self.repository_location + '::test')",
            "        self.cmd('check', '--repair', self.repository_location)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test', output)",
            "",
            "    def test_corrupted_repository(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('test')",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        output = self.cmd('check', '--show-version', self.repository_location)",
            "        self.assert_in('borgbackup version', output)  # implied output even without --info given",
            "        self.assert_not_in('Starting repository check', output)  # --info not given for root logger",
            "",
            "        name = sorted(os.listdir(os.path.join(self.tmpdir, 'repository', 'data', '0')), reverse=True)[1]",
            "        with open(os.path.join(self.tmpdir, 'repository', 'data', '0', name), 'r+b') as fd:",
            "            fd.seek(100)",
            "            fd.write(b'XXXX')",
            "        output = self.cmd('check', '--info', self.repository_location, exit_code=1)",
            "        self.assert_in('Starting repository check', output)  # --info given for root logger",
            "",
            "    # we currently need to be able to create a lock directory inside the repo:",
            "    @pytest.mark.xfail(reason=\"we need to be able to create the lock directory inside the repo\")",
            "    def test_readonly_repository(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('test')",
            "        os.system('chmod -R ugo-w ' + self.repository_path)",
            "        try:",
            "            self.cmd('extract', '--dry-run', self.repository_location + '::test')",
            "        finally:",
            "            # Restore permissions so shutil.rmtree is able to delete it",
            "            os.system('chmod -R u+w ' + self.repository_path)",
            "",
            "    @pytest.mark.skipif('BORG_TESTS_IGNORE_MODES' in os.environ, reason='modes unreliable')",
            "    def test_umask(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        mode = os.stat(self.repository_path).st_mode",
            "        self.assertEqual(stat.S_IMODE(mode), 0o700)",
            "",
            "    def test_create_dry_run(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--dry-run', self.repository_location + '::test', 'input')",
            "        # Make sure no archive has been created",
            "        with Repository(self.repository_path) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "        self.assert_equal(len(manifest.archives), 0)",
            "",
            "    def add_unknown_feature(self, operation):",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            manifest.config[b'feature_flags'] = {operation.value.encode(): {b'mandatory': [b'unknown-feature']}}",
            "            manifest.write()",
            "            repository.commit()",
            "",
            "    def cmd_raises_unknown_feature(self, args):",
            "        if self.FORK_DEFAULT:",
            "            self.cmd(*args, exit_code=EXIT_ERROR)",
            "        else:",
            "            with pytest.raises(MandatoryFeatureUnsupported) as excinfo:",
            "                self.cmd(*args)",
            "            assert excinfo.value.args == (['unknown-feature'],)",
            "",
            "    def test_unknown_feature_on_create(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.add_unknown_feature(Manifest.Operation.WRITE)",
            "        self.cmd_raises_unknown_feature(['create', self.repository_location + '::test', 'input'])",
            "",
            "    def test_unknown_feature_on_cache_sync(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        self.add_unknown_feature(Manifest.Operation.READ)",
            "        self.cmd_raises_unknown_feature(['create', self.repository_location + '::test', 'input'])",
            "",
            "    def test_unknown_feature_on_change_passphrase(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.add_unknown_feature(Manifest.Operation.CHECK)",
            "        self.cmd_raises_unknown_feature(['change-passphrase', self.repository_location])",
            "",
            "    def test_unknown_feature_on_read(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.READ)",
            "        with changedir('output'):",
            "            self.cmd_raises_unknown_feature(['extract', self.repository_location + '::test'])",
            "",
            "        self.cmd_raises_unknown_feature(['list', self.repository_location])",
            "        self.cmd_raises_unknown_feature(['info', self.repository_location + '::test'])",
            "",
            "    def test_unknown_feature_on_rename(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.CHECK)",
            "        self.cmd_raises_unknown_feature(['rename', self.repository_location + '::test', 'other'])",
            "",
            "    def test_unknown_feature_on_delete(self):",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.DELETE)",
            "        # delete of an archive raises",
            "        self.cmd_raises_unknown_feature(['delete', self.repository_location + '::test'])",
            "        self.cmd_raises_unknown_feature(['prune', '--keep-daily=3', self.repository_location])",
            "        # delete of the whole repository ignores features",
            "        self.cmd('delete', self.repository_location)",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_unknown_feature_on_mount(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.add_unknown_feature(Manifest.Operation.READ)",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        os.mkdir(mountpoint)",
            "        # XXX this might hang if it doesn't raise an error",
            "        self.cmd_raises_unknown_feature(['mount', self.repository_location + '::test', mountpoint])",
            "",
            "    @pytest.mark.allow_cache_wipe",
            "    def test_unknown_mandatory_feature_in_cache(self):",
            "        if self.prefix:",
            "            path_prefix = 'ssh://__testsuite__'",
            "        else:",
            "            path_prefix = ''",
            "",
            "        print(self.cmd('init', '--encryption=repokey', self.repository_location))",
            "",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            if path_prefix:",
            "                repository._location = Location(self.repository_location)",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest) as cache:",
            "                cache.begin_txn()",
            "                cache.cache_config.mandatory_features = set(['unknown-feature'])",
            "                cache.commit()",
            "",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('create', self.repository_location + '::test', 'input')",
            "        else:",
            "            called = False",
            "            wipe_cache_safe = LocalCache.wipe_cache",
            "",
            "            def wipe_wrapper(*args):",
            "                nonlocal called",
            "                called = True",
            "                wipe_cache_safe(*args)",
            "",
            "            with patch.object(LocalCache, 'wipe_cache', wipe_wrapper):",
            "                self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "            assert called",
            "",
            "        with Repository(self.repository_path, exclusive=True) as repository:",
            "            if path_prefix:",
            "                repository._location = Location(self.repository_location)",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest) as cache:",
            "                assert cache.cache_config.mandatory_features == set([])",
            "",
            "    def test_progress_on(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--progress', self.repository_location + '::test4', 'input')",
            "        self.assert_in(\"\\r\", output)",
            "",
            "    def test_progress_off(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', self.repository_location + '::test5', 'input')",
            "        self.assert_not_in(\"\\r\", output)",
            "",
            "    def test_file_status(self):",
            "        \"\"\"test that various file status show expected results",
            "",
            "        clearly incomplete: only tests for the weird \"unchanged\" status for now\"\"\"",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "        self.assert_in(\"A input/file2\", output)",
            "        # should find first file as unmodified",
            "        output = self.cmd('create', '--list', self.repository_location + '::test1', 'input')",
            "        self.assert_in(\"U input/file1\", output)",
            "        # this is expected, although surprising, for why, see:",
            "        # https://borgbackup.readthedocs.org/en/latest/faq.html#i-am-seeing-a-added-status-for-a-unchanged-file",
            "        self.assert_in(\"A input/file2\", output)",
            "",
            "    def test_file_status_cs_cache_mode(self):",
            "        \"\"\"test that a changed file with faked \"previous\" mtime still gets backed up in ctime,size cache_mode\"\"\"",
            "        self.create_regular_file('file1', contents=b'123')",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=10)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--files-cache=ctime,size', self.repository_location + '::test1', 'input')",
            "        # modify file1, but cheat with the mtime (and atime) and also keep same size:",
            "        st = os.stat('input/file1')",
            "        self.create_regular_file('file1', contents=b'321')",
            "        os.utime('input/file1', ns=(st.st_atime_ns, st.st_mtime_ns))",
            "        # this mode uses ctime for change detection, so it should find file1 as modified",
            "        output = self.cmd('create', '--list', '--files-cache=ctime,size', self.repository_location + '::test2', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "",
            "    def test_file_status_ms_cache_mode(self):",
            "        \"\"\"test that a chmod'ed file with no content changes does not get chunked again in mtime,size cache_mode\"\"\"",
            "        self.create_regular_file('file1', size=10)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=10)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--files-cache=mtime,size', self.repository_location + '::test1', 'input')",
            "        # change mode of file1, no content change:",
            "        st = os.stat('input/file1')",
            "        os.chmod('input/file1', st.st_mode ^ stat.S_IRWXO)  # this triggers a ctime change, but mtime is unchanged",
            "        # this mode uses mtime for change detection, so it should find file1 as unmodified",
            "        output = self.cmd('create', '--list', '--files-cache=mtime,size', self.repository_location + '::test2', 'input')",
            "        self.assert_in(\"U input/file1\", output)",
            "",
            "    def test_file_status_rc_cache_mode(self):",
            "        \"\"\"test that files get rechunked unconditionally in rechunk,ctime cache mode\"\"\"",
            "        self.create_regular_file('file1', size=10)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=10)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--files-cache=rechunk,ctime', self.repository_location + '::test1', 'input')",
            "        # no changes here, but this mode rechunks unconditionally",
            "        output = self.cmd('create', '--list', '--files-cache=rechunk,ctime', self.repository_location + '::test2', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "",
            "    def test_file_status_excluded(self):",
            "        \"\"\"test that excluded paths are listed\"\"\"",
            "",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        if has_lchflags:",
            "            self.create_regular_file('file3', size=1024 * 80)",
            "            platform.set_flags(os.path.join(self.input_path, 'file3'), stat.UF_NODUMP)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('create', '--list', '--exclude-nodump', self.repository_location + '::test', 'input')",
            "        self.assert_in(\"A input/file1\", output)",
            "        self.assert_in(\"A input/file2\", output)",
            "        if has_lchflags:",
            "            self.assert_in(\"x input/file3\", output)",
            "        # should find second file as excluded",
            "        output = self.cmd('create', '--list', '--exclude-nodump', self.repository_location + '::test1', 'input', '--exclude', '*/file2')",
            "        self.assert_in(\"U input/file1\", output)",
            "        self.assert_in(\"x input/file2\", output)",
            "        if has_lchflags:",
            "            self.assert_in(\"x input/file3\", output)",
            "",
            "    def test_create_json(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        create_info = json.loads(self.cmd('create', '--json', self.repository_location + '::test', 'input'))",
            "        # The usual keys",
            "        assert 'encryption' in create_info",
            "        assert 'repository' in create_info",
            "        assert 'cache' in create_info",
            "        assert 'last_modified' in create_info['repository']",
            "",
            "        archive = create_info['archive']",
            "        assert archive['name'] == 'test'",
            "        assert isinstance(archive['command_line'], list)",
            "        assert isinstance(archive['duration'], float)",
            "        assert len(archive['id']) == 64",
            "        assert 'stats' in archive",
            "",
            "    def test_create_topical(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        time.sleep(1)  # file2 must have newer timestamps than file1",
            "        self.create_regular_file('file2', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        # no listing by default",
            "        output = self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.assert_not_in('file1', output)",
            "        # shouldn't be listed even if unchanged",
            "        output = self.cmd('create', self.repository_location + '::test0', 'input')",
            "        self.assert_not_in('file1', output)",
            "        # should list the file as unchanged",
            "        output = self.cmd('create', '--list', '--filter=U', self.repository_location + '::test1', 'input')",
            "        self.assert_in('file1', output)",
            "        # should *not* list the file as changed",
            "        output = self.cmd('create', '--list', '--filter=AM', self.repository_location + '::test2', 'input')",
            "        self.assert_not_in('file1', output)",
            "        # change the file",
            "        self.create_regular_file('file1', size=1024 * 100)",
            "        # should list the file as changed",
            "        output = self.cmd('create', '--list', '--filter=AM', self.repository_location + '::test3', 'input')",
            "        self.assert_in('file1', output)",
            "",
            "    def test_create_read_special_broken_symlink(self):",
            "        os.symlink('somewhere doesnt exist', os.path.join(self.input_path, 'link'))",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        archive = self.repository_location + '::test'",
            "        self.cmd('create', '--read-special', archive, 'input')",
            "        output = self.cmd('list', archive)",
            "        assert 'input/link -> somewhere doesnt exist' in output",
            "",
            "    # def test_cmdline_compatibility(self):",
            "    #    self.create_regular_file('file1', size=1024 * 80)",
            "    #    self.cmd('init', '--encryption=repokey', self.repository_location)",
            "    #    self.cmd('create', self.repository_location + '::test', 'input')",
            "    #    output = self.cmd('foo', self.repository_location, '--old')",
            "    #    self.assert_in('\"--old\" has been deprecated. Use \"--new\" instead', output)",
            "",
            "    def test_prune_repository(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test2', src_dir)",
            "        # these are not really a checkpoints, but they look like some:",
            "        self.cmd('create', self.repository_location + '::test3.checkpoint', src_dir)",
            "        self.cmd('create', self.repository_location + '::test3.checkpoint.1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test4.checkpoint', src_dir)",
            "        output = self.cmd('prune', '--list', '--dry-run', self.repository_location, '--keep-daily=2')",
            "        assert re.search(r'Would prune:\\s+test1', output)",
            "        # must keep the latest non-checkpoint archive:",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+test2', output)",
            "        # must keep the latest checkpoint archive:",
            "        assert re.search(r'Keeping checkpoint archive:\\s+test4.checkpoint', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('test1', output)",
            "        self.assert_in('test2', output)",
            "        self.assert_in('test3.checkpoint', output)",
            "        self.assert_in('test3.checkpoint.1', output)",
            "        self.assert_in('test4.checkpoint', output)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test1', output)",
            "        # the latest non-checkpoint archive must be still there:",
            "        self.assert_in('test2', output)",
            "        # only the latest checkpoint archive must still be there:",
            "        self.assert_not_in('test3.checkpoint', output)",
            "        self.assert_not_in('test3.checkpoint.1', output)",
            "        self.assert_in('test4.checkpoint', output)",
            "        # now we supercede the latest checkpoint by a successful backup:",
            "        self.cmd('create', self.repository_location + '::test5', src_dir)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2')",
            "        output = self.cmd('list', self.repository_location)",
            "        # all checkpoints should be gone now:",
            "        self.assert_not_in('checkpoint', output)",
            "        # the latest archive must be still there",
            "        self.assert_in('test5', output)",
            "",
            "    def test_prune_repository_save_space(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test2', src_dir)",
            "        output = self.cmd('prune', '--list', '--stats', '--dry-run', self.repository_location, '--keep-daily=2')",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+test2', output)",
            "        assert re.search(r'Would prune:\\s+test1', output)",
            "        self.assert_in('Deleted data:', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('test1', output)",
            "        self.assert_in('test2', output)",
            "        self.cmd('prune', '--save-space', self.repository_location, '--keep-daily=2')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('test1', output)",
            "        self.assert_in('test2', output)",
            "",
            "    def test_prune_repository_prefix(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::foo-2015-08-12-10:00', src_dir)",
            "        self.cmd('create', self.repository_location + '::foo-2015-08-12-20:00', src_dir)",
            "        self.cmd('create', self.repository_location + '::bar-2015-08-12-10:00', src_dir)",
            "        self.cmd('create', self.repository_location + '::bar-2015-08-12-20:00', src_dir)",
            "        output = self.cmd('prune', '--list', '--dry-run', self.repository_location, '--keep-daily=2', '--prefix=foo-')",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+foo-2015-08-12-20:00', output)",
            "        assert re.search(r'Would prune:\\s+foo-2015-08-12-10:00', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('foo-2015-08-12-10:00', output)",
            "        self.assert_in('foo-2015-08-12-20:00', output)",
            "        self.assert_in('bar-2015-08-12-10:00', output)",
            "        self.assert_in('bar-2015-08-12-20:00', output)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2', '--prefix=foo-')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('foo-2015-08-12-10:00', output)",
            "        self.assert_in('foo-2015-08-12-20:00', output)",
            "        self.assert_in('bar-2015-08-12-10:00', output)",
            "        self.assert_in('bar-2015-08-12-20:00', output)",
            "",
            "    def test_prune_repository_glob(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-10:00-foo', src_dir)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-20:00-foo', src_dir)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-10:00-bar', src_dir)",
            "        self.cmd('create', self.repository_location + '::2015-08-12-20:00-bar', src_dir)",
            "        output = self.cmd('prune', '--list', '--dry-run', self.repository_location, '--keep-daily=2', '--glob-archives=2015-*-foo')",
            "        assert re.search(r'Keeping archive \\(rule: daily #1\\):\\s+2015-08-12-20:00-foo', output)",
            "        assert re.search(r'Would prune:\\s+2015-08-12-10:00-foo', output)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('2015-08-12-10:00-foo', output)",
            "        self.assert_in('2015-08-12-20:00-foo', output)",
            "        self.assert_in('2015-08-12-10:00-bar', output)",
            "        self.assert_in('2015-08-12-20:00-bar', output)",
            "        self.cmd('prune', self.repository_location, '--keep-daily=2', '--glob-archives=2015-*-foo')",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_not_in('2015-08-12-10:00-foo', output)",
            "        self.assert_in('2015-08-12-20:00-foo', output)",
            "        self.assert_in('2015-08-12-10:00-bar', output)",
            "        self.assert_in('2015-08-12-20:00-bar', output)",
            "",
            "    def test_list_prefix(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test-1', src_dir)",
            "        self.cmd('create', self.repository_location + '::something-else-than-test-1', src_dir)",
            "        self.cmd('create', self.repository_location + '::test-2', src_dir)",
            "        output = self.cmd('list', '--prefix=test-', self.repository_location)",
            "        self.assert_in('test-1', output)",
            "        self.assert_in('test-2', output)",
            "        self.assert_not_in('something-else', output)",
            "",
            "    def test_list_format(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', test_archive, src_dir)",
            "        output_warn = self.cmd('list', '--list-format', '-', test_archive)",
            "        self.assert_in('--list-format\" has been deprecated.', output_warn)",
            "        output_1 = self.cmd('list', test_archive)",
            "        output_2 = self.cmd('list', '--format', '{mode} {user:6} {group:6} {size:8d} {mtime} {path}{extra}{NEWLINE}', test_archive)",
            "        output_3 = self.cmd('list', '--format', '{mtime:%s} {path}{NL}', test_archive)",
            "        self.assertEqual(output_1, output_2)",
            "        self.assertNotEqual(output_1, output_3)",
            "",
            "    def test_list_repository_format(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--comment', 'comment 1', self.repository_location + '::test-1', src_dir)",
            "        self.cmd('create', '--comment', 'comment 2', self.repository_location + '::test-2', src_dir)",
            "        output_1 = self.cmd('list', self.repository_location)",
            "        output_2 = self.cmd('list', '--format', '{archive:<36} {time} [{id}]{NL}', self.repository_location)",
            "        self.assertEqual(output_1, output_2)",
            "        output_1 = self.cmd('list', '--short', self.repository_location)",
            "        self.assertEqual(output_1, 'test-1\\ntest-2\\n')",
            "        output_1 = self.cmd('list', '--format', '{barchive}/', self.repository_location)",
            "        self.assertEqual(output_1, 'test-1/test-2/')",
            "        output_3 = self.cmd('list', '--format', '{name} {comment}{NL}', self.repository_location)",
            "        self.assert_in('test-1 comment 1\\n', output_3)",
            "        self.assert_in('test-2 comment 2\\n', output_3)",
            "",
            "    def test_list_hash(self):",
            "        self.create_regular_file('empty_file', size=0)",
            "        self.create_regular_file('amb', contents=b'a' * 1000000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', test_archive, 'input')",
            "        output = self.cmd('list', '--format', '{sha256} {path}{NL}', test_archive)",
            "        assert \"cdc76e5c9914fb9281a1c7e284d73e67f1809a48a497200e046d39ccc7112cd0 input/amb\" in output",
            "        assert \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 input/empty_file\" in output",
            "",
            "    def test_list_chunk_counts(self):",
            "        self.create_regular_file('empty_file', size=0)",
            "        self.create_regular_file('two_chunks')",
            "        with open(os.path.join(self.input_path, 'two_chunks'), 'wb') as fd:",
            "            fd.write(b'abba' * 2000000)",
            "            fd.write(b'baab' * 2000000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', test_archive, 'input')",
            "        output = self.cmd('list', '--format', '{num_chunks} {unique_chunks} {path}{NL}', test_archive)",
            "        assert \"0 0 input/empty_file\" in output",
            "        assert \"2 2 input/two_chunks\" in output",
            "",
            "    def test_list_size(self):",
            "        self.create_regular_file('compressible_file', size=10000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        test_archive = self.repository_location + '::test'",
            "        self.cmd('create', '-C', 'lz4', test_archive, 'input')",
            "        output = self.cmd('list', '--format', '{size} {csize} {dsize} {dcsize} {path}{NL}', test_archive)",
            "        size, csize, dsize, dcsize, path = output.split(\"\\n\")[1].split(\" \")",
            "        assert int(csize) < int(size)",
            "        assert int(dcsize) < int(dsize)",
            "        assert int(dsize) <= int(size)",
            "        assert int(dcsize) <= int(csize)",
            "",
            "    def test_list_json(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        list_repo = json.loads(self.cmd('list', '--json', self.repository_location))",
            "        repository = list_repo['repository']",
            "        assert len(repository['id']) == 64",
            "        assert datetime.strptime(repository['last_modified'], ISO_FORMAT)  # must not raise",
            "        assert list_repo['encryption']['mode'] == 'repokey'",
            "        assert 'keyfile' not in list_repo['encryption']",
            "        archive0 = list_repo['archives'][0]",
            "        assert datetime.strptime(archive0['time'], ISO_FORMAT)  # must not raise",
            "",
            "        list_archive = self.cmd('list', '--json-lines', self.repository_location + '::test')",
            "        items = [json.loads(s) for s in list_archive.splitlines()]",
            "        assert len(items) == 2",
            "        file1 = items[1]",
            "        assert file1['path'] == 'input/file1'",
            "        assert file1['size'] == 81920",
            "        assert datetime.strptime(file1['mtime'], ISO_FORMAT)  # must not raise",
            "",
            "        list_archive = self.cmd('list', '--json-lines', '--format={sha256}', self.repository_location + '::test')",
            "        items = [json.loads(s) for s in list_archive.splitlines()]",
            "        assert len(items) == 2",
            "        file1 = items[1]",
            "        assert file1['path'] == 'input/file1'",
            "        assert file1['sha256'] == 'b2915eb69f260d8d3c25249195f2c8f4f716ea82ec760ae929732c0262442b2b'",
            "",
            "    def test_list_json_args(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('list', '--json-lines', self.repository_location, exit_code=2)",
            "        self.cmd('list', '--json', self.repository_location + '::archive', exit_code=2)",
            "",
            "    def test_log_json(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        log = self.cmd('create', '--log-json', self.repository_location + '::test', 'input', '--list', '--debug')",
            "        messages = {}  # type -> message, one of each kind",
            "        for line in log.splitlines():",
            "            msg = json.loads(line)",
            "            messages[msg['type']] = msg",
            "",
            "        file_status = messages['file_status']",
            "        assert 'status' in file_status",
            "        assert file_status['path'].startswith('input')",
            "",
            "        log_message = messages['log_message']",
            "        assert isinstance(log_message['time'], float)",
            "        assert log_message['levelname'] == 'DEBUG'  # there should only be DEBUG messages",
            "        assert isinstance(log_message['message'], str)",
            "",
            "    def test_debug_profile(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input', '--debug-profile=create.prof')",
            "        self.cmd('debug', 'convert-profile', 'create.prof', 'create.pyprof')",
            "        stats = pstats.Stats('create.pyprof')",
            "        stats.strip_dirs()",
            "        stats.sort_stats('cumtime')",
            "",
            "        self.cmd('create', self.repository_location + '::test2', 'input', '--debug-profile=create.pyprof')",
            "        stats = pstats.Stats('create.pyprof')  # Only do this on trusted data!",
            "        stats.strip_dirs()",
            "        stats.sort_stats('cumtime')",
            "",
            "    def test_common_options(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        log = self.cmd('--debug', 'create', self.repository_location + '::test', 'input')",
            "        assert 'security: read previous location' in log",
            "",
            "    def _get_sizes(self, compression, compressible, size=10000):",
            "        if compressible:",
            "            contents = b'X' * size",
            "        else:",
            "            contents = os.urandom(size)",
            "        self.create_regular_file('file', contents=contents)",
            "        self.cmd('init', '--encryption=none', self.repository_location)",
            "        archive = self.repository_location + '::test'",
            "        self.cmd('create', '-C', compression, archive, 'input')",
            "        output = self.cmd('list', '--format', '{size} {csize} {path}{NL}', archive)",
            "        size, csize, path = output.split(\"\\n\")[1].split(\" \")",
            "        return int(size), int(csize)",
            "",
            "    def test_compression_none_compressible(self):",
            "        size, csize = self._get_sizes('none', compressible=True)",
            "        assert csize >= size",
            "        assert csize == size + 3",
            "",
            "    def test_compression_none_uncompressible(self):",
            "        size, csize = self._get_sizes('none', compressible=False)",
            "        assert csize >= size",
            "        assert csize == size + 3",
            "",
            "    def test_compression_zlib_compressible(self):",
            "        size, csize = self._get_sizes('zlib', compressible=True)",
            "        assert csize < size * 0.1",
            "        assert csize == 35",
            "",
            "    def test_compression_zlib_uncompressible(self):",
            "        size, csize = self._get_sizes('zlib', compressible=False)",
            "        assert csize >= size",
            "",
            "    def test_compression_auto_compressible(self):",
            "        size, csize = self._get_sizes('auto,zlib', compressible=True)",
            "        assert csize < size * 0.1",
            "        assert csize == 35  # same as compression 'zlib'",
            "",
            "    def test_compression_auto_uncompressible(self):",
            "        size, csize = self._get_sizes('auto,zlib', compressible=False)",
            "        assert csize >= size",
            "        assert csize == size + 3  # same as compression 'none'",
            "",
            "    def test_compression_lz4_compressible(self):",
            "        size, csize = self._get_sizes('lz4', compressible=True)",
            "        assert csize < size * 0.1",
            "",
            "    def test_compression_lz4_uncompressible(self):",
            "        size, csize = self._get_sizes('lz4', compressible=False)",
            "        assert csize >= size",
            "",
            "    def test_compression_lzma_compressible(self):",
            "        size, csize = self._get_sizes('lzma', compressible=True)",
            "        assert csize < size * 0.1",
            "",
            "    def test_compression_lzma_uncompressible(self):",
            "        size, csize = self._get_sizes('lzma', compressible=False)",
            "        assert csize >= size",
            "",
            "    def test_change_passphrase(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        os.environ['BORG_NEW_PASSPHRASE'] = 'newpassphrase'",
            "        # here we have both BORG_PASSPHRASE and BORG_NEW_PASSPHRASE set:",
            "        self.cmd('change-passphrase', self.repository_location)",
            "        os.environ['BORG_PASSPHRASE'] = 'newpassphrase'",
            "        self.cmd('list', self.repository_location)",
            "",
            "    def test_break_lock(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('break-lock', self.repository_location)",
            "",
            "    def test_usage(self):",
            "        self.cmd()",
            "        self.cmd('-h')",
            "",
            "    def test_help(self):",
            "        assert 'Borg' in self.cmd('help')",
            "        assert 'patterns' in self.cmd('help', 'patterns')",
            "        assert 'Initialize' in self.cmd('help', 'init')",
            "        assert 'positional arguments' not in self.cmd('help', 'init', '--epilog-only')",
            "        assert 'This command initializes' not in self.cmd('help', 'init', '--usage-only')",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse(self):",
            "        def has_noatime(some_file):",
            "            atime_before = os.stat(some_file).st_atime_ns",
            "            try:",
            "                os.close(os.open(some_file, flags_noatime))",
            "            except PermissionError:",
            "                return False",
            "            else:",
            "                atime_after = os.stat(some_file).st_atime_ns",
            "                noatime_used = flags_noatime != flags_normal",
            "                return noatime_used and atime_before == atime_after",
            "",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_test_files()",
            "        have_noatime = has_noatime('input/file1')",
            "        self.cmd('create', '--exclude-nodump', self.repository_location + '::archive', 'input')",
            "        self.cmd('create', '--exclude-nodump', self.repository_location + '::archive2', 'input')",
            "        if has_lchflags:",
            "            # remove the file we did not backup, so input and output become equal",
            "            os.remove(os.path.join('input', 'flagfile'))",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        # mount the whole repository, archive contents shall show up in archivename subdirs of mountpoint:",
            "        with self.fuse_mount(self.repository_location, mountpoint):",
            "            # bsdflags are not supported by the FUSE mount",
            "            # we also ignore xattrs here, they are tested separately",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive', 'input'),",
            "                                   ignore_bsdflags=True, ignore_xattrs=True)",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'archive2', 'input'),",
            "                                   ignore_bsdflags=True, ignore_xattrs=True)",
            "        # mount only 1 archive, its contents shall show up directly in mountpoint:",
            "        with self.fuse_mount(self.repository_location + '::archive', mountpoint):",
            "            self.assert_dirs_equal(self.input_path, os.path.join(mountpoint, 'input'),",
            "                                   ignore_bsdflags=True, ignore_xattrs=True)",
            "            # regular file",
            "            in_fn = 'input/file1'",
            "            out_fn = os.path.join(mountpoint, 'input', 'file1')",
            "            # stat",
            "            sti1 = os.stat(in_fn)",
            "            sto1 = os.stat(out_fn)",
            "            assert sti1.st_mode == sto1.st_mode",
            "            assert sti1.st_uid == sto1.st_uid",
            "            assert sti1.st_gid == sto1.st_gid",
            "            assert sti1.st_size == sto1.st_size",
            "            if have_noatime:",
            "                assert sti1.st_atime == sto1.st_atime",
            "            assert sti1.st_ctime == sto1.st_ctime",
            "            assert sti1.st_mtime == sto1.st_mtime",
            "            # note: there is another hardlink to this, see below",
            "            assert sti1.st_nlink == sto1.st_nlink == 2",
            "            # read",
            "            with open(in_fn, 'rb') as in_f, open(out_fn, 'rb') as out_f:",
            "                assert in_f.read() == out_f.read()",
            "            # hardlink (to 'input/file1')",
            "            if are_hardlinks_supported():",
            "                in_fn = 'input/hardlink'",
            "                out_fn = os.path.join(mountpoint, 'input', 'hardlink')",
            "                sti2 = os.stat(in_fn)",
            "                sto2 = os.stat(out_fn)",
            "                assert sti2.st_nlink == sto2.st_nlink == 2",
            "                assert sto1.st_ino == sto2.st_ino",
            "            # symlink",
            "            if are_symlinks_supported():",
            "                in_fn = 'input/link1'",
            "                out_fn = os.path.join(mountpoint, 'input', 'link1')",
            "                sti = os.stat(in_fn, follow_symlinks=False)",
            "                sto = os.stat(out_fn, follow_symlinks=False)",
            "                assert sti.st_size == len('somewhere')",
            "                assert sto.st_size == len('somewhere')",
            "                assert stat.S_ISLNK(sti.st_mode)",
            "                assert stat.S_ISLNK(sto.st_mode)",
            "                assert os.readlink(in_fn) == os.readlink(out_fn)",
            "            # FIFO",
            "            if are_fifos_supported():",
            "                out_fn = os.path.join(mountpoint, 'input', 'fifo1')",
            "                sto = os.stat(out_fn)",
            "                assert stat.S_ISFIFO(sto.st_mode)",
            "            # list/read xattrs",
            "            try:",
            "                in_fn = 'input/fusexattr'",
            "                out_fn = os.path.join(mountpoint, 'input', 'fusexattr')",
            "                if not xattr.XATTR_FAKEROOT and xattr.is_enabled(self.input_path):",
            "                    assert sorted(no_selinux(xattr.listxattr(out_fn))) == ['user.empty', 'user.foo', ]",
            "                    assert xattr.getxattr(out_fn, 'user.foo') == b'bar'",
            "                    # Special case: getxattr returns None (not b'') when reading an empty xattr.",
            "                    assert xattr.getxattr(out_fn, 'user.empty') is None",
            "                else:",
            "                    assert xattr.listxattr(out_fn) == []",
            "                    try:",
            "                        xattr.getxattr(out_fn, 'user.foo')",
            "                    except OSError as e:",
            "                        assert e.errno == llfuse.ENOATTR",
            "                    else:",
            "                        assert False, \"expected OSError(ENOATTR), but no error was raised\"",
            "            except OSError as err:",
            "                if sys.platform.startswith(('freebsd', )) and err.errno == errno.ENOTSUP:",
            "                    # some systems have no xattr support on FUSE",
            "                    pass",
            "                else:",
            "                    raise",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_versions_view(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('test', contents=b'first')",
            "        if are_hardlinks_supported():",
            "            self.create_regular_file('hardlink1', contents=b'123456')",
            "            os.link('input/hardlink1', 'input/hardlink2')",
            "            os.link('input/hardlink1', 'input/hardlink3')",
            "        self.cmd('create', self.repository_location + '::archive1', 'input')",
            "        self.create_regular_file('test', contents=b'second')",
            "        self.cmd('create', self.repository_location + '::archive2', 'input')",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        # mount the whole repository, archive contents shall show up in versioned view:",
            "        with self.fuse_mount(self.repository_location, mountpoint, '-o', 'versions'):",
            "            path = os.path.join(mountpoint, 'input', 'test')  # filename shows up as directory ...",
            "            files = os.listdir(path)",
            "            assert all(f.startswith('test.') for f in files)  # ... with files test.xxxxx in there",
            "            assert {b'first', b'second'} == {open(os.path.join(path, f), 'rb').read() for f in files}",
            "            if are_hardlinks_supported():",
            "                hl1 = os.path.join(mountpoint, 'input', 'hardlink1', 'hardlink1.00001')",
            "                hl2 = os.path.join(mountpoint, 'input', 'hardlink2', 'hardlink2.00001')",
            "                hl3 = os.path.join(mountpoint, 'input', 'hardlink3', 'hardlink3.00001')",
            "                assert os.stat(hl1).st_ino == os.stat(hl2).st_ino == os.stat(hl3).st_ino",
            "                assert open(hl3, 'rb').read() == b'123456'",
            "        # similar again, but exclude the hardlink master:",
            "        with self.fuse_mount(self.repository_location, mountpoint, '-o', 'versions', '-e', 'input/hardlink1'):",
            "            if are_hardlinks_supported():",
            "                hl2 = os.path.join(mountpoint, 'input', 'hardlink2', 'hardlink2.00001')",
            "                hl3 = os.path.join(mountpoint, 'input', 'hardlink3', 'hardlink3.00001')",
            "                assert os.stat(hl2).st_ino == os.stat(hl3).st_ino",
            "                assert open(hl3, 'rb').read() == b'123456'",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_allow_damaged_files(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive')",
            "        # Get rid of a chunk and repair it",
            "        archive, repository = self.open_archive('archive')",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith('testsuite/archiver.py'):",
            "                    repository.delete(item.chunks[-1].id)",
            "                    path = item.path  # store full path for later",
            "                    break",
            "            else:",
            "                assert False  # missed the file",
            "            repository.commit()",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        with self.fuse_mount(self.repository_location + '::archive', mountpoint):",
            "            with pytest.raises(OSError) as excinfo:",
            "                open(os.path.join(mountpoint, path))",
            "            assert excinfo.value.errno == errno.EIO",
            "        with self.fuse_mount(self.repository_location + '::archive', mountpoint, '-o', 'allow_damaged_files'):",
            "            open(os.path.join(mountpoint, path)).close()",
            "",
            "    @unittest.skipUnless(has_llfuse, 'llfuse not installed')",
            "    def test_fuse_mount_options(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('arch11')",
            "        self.create_src_archive('arch12')",
            "        self.create_src_archive('arch21')",
            "        self.create_src_archive('arch22')",
            "",
            "        mountpoint = os.path.join(self.tmpdir, 'mountpoint')",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--first=2', '--sort=name'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch11', 'arch12']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--last=2', '--sort=name'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch21', 'arch22']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=arch1'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch11', 'arch12']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=arch2'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch21', 'arch22']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=arch'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == ['arch11', 'arch12', 'arch21', 'arch22']",
            "        with self.fuse_mount(self.repository_location, mountpoint, '--prefix=nope'):",
            "            assert sorted(os.listdir(os.path.join(mountpoint))) == []",
            "",
            "    def verify_aes_counter_uniqueness(self, method):",
            "        seen = set()  # Chunks already seen",
            "        used = set()  # counter values already used",
            "",
            "        def verify_uniqueness():",
            "            with Repository(self.repository_path) as repository:",
            "                for id, _ in repository.open_index(repository.get_transaction_id()).iteritems():",
            "                    data = repository.get(id)",
            "                    hash = sha256(data).digest()",
            "                    if hash not in seen:",
            "                        seen.add(hash)",
            "                        num_blocks = num_cipher_blocks(len(data) - 41)",
            "                        nonce = bytes_to_long(data[33:41])",
            "                        for counter in range(nonce, nonce + num_blocks):",
            "                            self.assert_not_in(counter, used)",
            "                            used.add(counter)",
            "",
            "        self.create_test_files()",
            "        os.environ['BORG_PASSPHRASE'] = 'passphrase'",
            "        self.cmd('init', '--encryption=' + method, self.repository_location)",
            "        verify_uniqueness()",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        verify_uniqueness()",
            "        self.cmd('create', self.repository_location + '::test.2', 'input')",
            "        verify_uniqueness()",
            "        self.cmd('delete', self.repository_location + '::test.2')",
            "        verify_uniqueness()",
            "",
            "    def test_aes_counter_uniqueness_keyfile(self):",
            "        self.verify_aes_counter_uniqueness('keyfile')",
            "",
            "    def test_aes_counter_uniqueness_passphrase(self):",
            "        self.verify_aes_counter_uniqueness('repokey')",
            "",
            "    def test_debug_dump_archive_items(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            output = self.cmd('debug', 'dump-archive-items', self.repository_location + '::test')",
            "        output_dir = sorted(os.listdir('output'))",
            "        assert len(output_dir) > 0 and output_dir[0].startswith('000000_')",
            "        assert 'Done.' in output",
            "",
            "    def test_debug_dump_repo_objs(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with changedir('output'):",
            "            output = self.cmd('debug', 'dump-repo-objs', self.repository_location)",
            "        output_dir = sorted(os.listdir('output'))",
            "        assert len(output_dir) > 0 and output_dir[0].startswith('000000_')",
            "        assert 'Done.' in output",
            "",
            "    def test_debug_put_get_delete_obj(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        data = b'some data'",
            "        hexkey = sha256(data).hexdigest()",
            "        self.create_regular_file('file', contents=data)",
            "        output = self.cmd('debug', 'put-obj', self.repository_location, 'input/file')",
            "        assert hexkey in output",
            "        output = self.cmd('debug', 'get-obj', self.repository_location, hexkey, 'output/file')",
            "        assert hexkey in output",
            "        with open('output/file', 'rb') as f:",
            "            data_read = f.read()",
            "        assert data == data_read",
            "        output = self.cmd('debug', 'delete-obj', self.repository_location, hexkey)",
            "        assert \"deleted\" in output",
            "        output = self.cmd('debug', 'delete-obj', self.repository_location, hexkey)",
            "        assert \"not found\" in output",
            "        output = self.cmd('debug', 'delete-obj', self.repository_location, 'invalid')",
            "        assert \"is invalid\" in output",
            "",
            "    def test_init_interrupt(self):",
            "        def raise_eof(*args):",
            "            raise EOFError",
            "",
            "        with patch.object(KeyfileKeyBase, 'create', raise_eof):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location, exit_code=1)",
            "        assert not os.path.exists(self.repository_location)",
            "",
            "    def test_init_requires_encryption_option(self):",
            "        self.cmd('init', self.repository_location, exit_code=2)",
            "",
            "    def test_init_nested_repositories(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('init', '--encryption=repokey', self.repository_location + '/nested', exit_code=2)",
            "        else:",
            "            with pytest.raises(Repository.AlreadyExists):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location + '/nested')",
            "",
            "    def check_cache(self):",
            "        # First run a regular borg check",
            "        self.cmd('check', self.repository_location)",
            "        # Then check that the cache on disk matches exactly what's in the repo.",
            "        with self.open_repository() as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest, sync=False) as cache:",
            "                original_chunks = cache.chunks",
            "            Cache.destroy(repository)",
            "            with Cache(repository, key, manifest) as cache:",
            "                correct_chunks = cache.chunks",
            "        assert original_chunks is not correct_chunks",
            "        seen = set()",
            "        for id, (refcount, size, csize) in correct_chunks.iteritems():",
            "            o_refcount, o_size, o_csize = original_chunks[id]",
            "            assert refcount == o_refcount",
            "            assert size == o_size",
            "            assert csize == o_csize",
            "            seen.add(id)",
            "        for id, (refcount, size, csize) in original_chunks.iteritems():",
            "            assert id in seen",
            "",
            "    def test_check_cache(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        with self.open_repository() as repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest, sync=False) as cache:",
            "                cache.begin_txn()",
            "                cache.chunks.incref(list(cache.chunks.iteritems())[0][0])",
            "                cache.commit()",
            "        with pytest.raises(AssertionError):",
            "            self.check_cache()",
            "",
            "    def test_recreate_target_rc(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('recreate', self.repository_location, '--target=asdf', exit_code=2)",
            "        assert 'Need to specify single archive' in output",
            "",
            "    def test_recreate_target(self):",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.check_cache()",
            "        archive = self.repository_location + '::test0'",
            "        self.cmd('create', archive, 'input')",
            "        self.check_cache()",
            "        original_archive = self.cmd('list', self.repository_location)",
            "        self.cmd('recreate', archive, 'input/dir2', '-e', 'input/dir2/file3', '--target=new-archive')",
            "        self.check_cache()",
            "        archives = self.cmd('list', self.repository_location)",
            "        assert original_archive in archives",
            "        assert 'new-archive' in archives",
            "",
            "        archive = self.repository_location + '::new-archive'",
            "        listing = self.cmd('list', '--short', archive)",
            "        assert 'file1' not in listing",
            "        assert 'dir2/file2' in listing",
            "        assert 'dir2/file3' not in listing",
            "",
            "    def test_recreate_basic(self):",
            "        self.create_test_files()",
            "        self.create_regular_file('dir2/file3', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        archive = self.repository_location + '::test0'",
            "        self.cmd('create', archive, 'input')",
            "        self.cmd('recreate', archive, 'input/dir2', '-e', 'input/dir2/file3')",
            "        self.check_cache()",
            "        listing = self.cmd('list', '--short', archive)",
            "        assert 'file1' not in listing",
            "        assert 'dir2/file2' in listing",
            "        assert 'dir2/file3' not in listing",
            "",
            "    @pytest.mark.skipif(not are_hardlinks_supported(), reason='hardlinks not supported')",
            "    def test_recreate_subtree_hardlinks(self):",
            "        # This is essentially the same problem set as in test_extract_hardlinks",
            "        self._extract_hardlinks_setup()",
            "        self.cmd('create', self.repository_location + '::test2', 'input')",
            "        self.cmd('recreate', self.repository_location + '::test', 'input/dir1')",
            "        self.check_cache()",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test')",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "        with changedir('output'):",
            "            self.cmd('extract', self.repository_location + '::test2')",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 4",
            "",
            "    def test_recreate_rechunkify(self):",
            "        with open(os.path.join(self.input_path, 'large_file'), 'wb') as fd:",
            "            fd.write(b'a' * 280)",
            "            fd.write(b'b' * 280)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', '--chunker-params', '7,9,8,128', self.repository_location + '::test1', 'input')",
            "        self.cmd('create', self.repository_location + '::test2', 'input', '--files-cache=disabled')",
            "        list = self.cmd('list', self.repository_location + '::test1', 'input/large_file',",
            "                        '--format', '{num_chunks} {unique_chunks}')",
            "        num_chunks, unique_chunks = map(int, list.split(' '))",
            "        # test1 and test2 do not deduplicate",
            "        assert num_chunks == unique_chunks",
            "        self.cmd('recreate', self.repository_location, '--chunker-params', 'default')",
            "        self.check_cache()",
            "        # test1 and test2 do deduplicate after recreate",
            "        assert int(self.cmd('list', self.repository_location + '::test1', 'input/large_file', '--format={size}'))",
            "        assert not int(self.cmd('list', self.repository_location + '::test1', 'input/large_file',",
            "                                '--format', '{unique_chunks}'))",
            "",
            "    def test_recreate_recompress(self):",
            "        self.create_regular_file('compressible', size=10000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input', '-C', 'none')",
            "        file_list = self.cmd('list', self.repository_location + '::test', 'input/compressible',",
            "                             '--format', '{size} {csize} {sha256}')",
            "        size, csize, sha256_before = file_list.split(' ')",
            "        assert int(csize) >= int(size)  # >= due to metadata overhead",
            "        self.cmd('recreate', self.repository_location, '-C', 'lz4', '--recompress')",
            "        self.check_cache()",
            "        file_list = self.cmd('list', self.repository_location + '::test', 'input/compressible',",
            "                             '--format', '{size} {csize} {sha256}')",
            "        size, csize, sha256_after = file_list.split(' ')",
            "        assert int(csize) < int(size)",
            "        assert sha256_before == sha256_after",
            "",
            "    def test_recreate_dry_run(self):",
            "        self.create_regular_file('compressible', size=10000)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        archives_before = self.cmd('list', self.repository_location + '::test')",
            "        self.cmd('recreate', self.repository_location, '-n', '-e', 'input/compressible')",
            "        self.check_cache()",
            "        archives_after = self.cmd('list', self.repository_location + '::test')",
            "        assert archives_after == archives_before",
            "",
            "    def test_recreate_skips_nothing_to_do(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        info_before = self.cmd('info', self.repository_location + '::test')",
            "        self.cmd('recreate', self.repository_location, '--chunker-params', 'default')",
            "        self.check_cache()",
            "        info_after = self.cmd('info', self.repository_location + '::test')",
            "        assert info_before == info_after  # includes archive ID",
            "",
            "    def test_with_lock(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        lock_path = os.path.join(self.repository_path, 'lock.exclusive')",
            "        cmd = 'python3', '-c', 'import os, sys; sys.exit(42 if os.path.exists(\"%s\") else 23)' % lock_path",
            "        self.cmd('with-lock', self.repository_location, *cmd, fork=True, exit_code=42)",
            "",
            "    def test_recreate_list_output(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('file1', size=0)",
            "        self.create_regular_file('file2', size=0)",
            "        self.create_regular_file('file3', size=0)",
            "        self.create_regular_file('file4', size=0)",
            "        self.create_regular_file('file5', size=0)",
            "",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "",
            "        output = self.cmd('recreate', '--list', '--info', self.repository_location + '::test', '-e', 'input/file2')",
            "        self.check_cache()",
            "        self.assert_in(\"input/file1\", output)",
            "        self.assert_in(\"x input/file2\", output)",
            "",
            "        output = self.cmd('recreate', '--list', self.repository_location + '::test', '-e', 'input/file3')",
            "        self.check_cache()",
            "        self.assert_in(\"input/file1\", output)",
            "        self.assert_in(\"x input/file3\", output)",
            "",
            "        output = self.cmd('recreate', self.repository_location + '::test', '-e', 'input/file4')",
            "        self.check_cache()",
            "        self.assert_not_in(\"input/file1\", output)",
            "        self.assert_not_in(\"x input/file4\", output)",
            "",
            "        output = self.cmd('recreate', '--info', self.repository_location + '::test', '-e', 'input/file5')",
            "        self.check_cache()",
            "        self.assert_not_in(\"input/file1\", output)",
            "        self.assert_not_in(\"x input/file5\", output)",
            "",
            "    def test_bad_filters(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('delete', '--first', '1', '--last', '1', self.repository_location, fork=True, exit_code=2)",
            "",
            "    def test_key_export_keyfile(self):",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "        repo_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('key', 'export', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert export_contents.startswith('BORG_KEY ' + bin_to_hex(repo_id) + '\\n')",
            "",
            "        key_file = self.keys_path + '/' + os.listdir(self.keys_path)[0]",
            "",
            "        with open(key_file, 'r') as fd:",
            "            key_contents = fd.read()",
            "",
            "        assert key_contents == export_contents",
            "",
            "        os.unlink(key_file)",
            "",
            "        self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "        with open(key_file, 'r') as fd:",
            "            key_contents2 = fd.read()",
            "",
            "        assert key_contents2 == key_contents",
            "",
            "    def test_key_export_repokey(self):",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'repokey')",
            "        repo_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('key', 'export', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert export_contents.startswith('BORG_KEY ' + bin_to_hex(repo_id) + '\\n')",
            "",
            "        with Repository(self.repository_path) as repository:",
            "            repo_key = RepoKey(repository)",
            "            repo_key.load(None, Passphrase.env_passphrase())",
            "",
            "        backup_key = KeyfileKey(key.TestKey.MockRepository())",
            "        backup_key.load(export_file, Passphrase.env_passphrase())",
            "",
            "        assert repo_key.enc_key == backup_key.enc_key",
            "",
            "        with Repository(self.repository_path) as repository:",
            "            repository.save_key(b'')",
            "",
            "        self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "        with Repository(self.repository_path) as repository:",
            "            repo_key2 = RepoKey(repository)",
            "            repo_key2.load(None, Passphrase.env_passphrase())",
            "",
            "        assert repo_key2.enc_key == repo_key2.enc_key",
            "",
            "    def test_key_export_qr(self):",
            "        export_file = self.output_path + '/exported.html'",
            "        self.cmd('init', self.repository_location, '--encryption', 'repokey')",
            "        repo_id = self._extract_repository_id(self.repository_path)",
            "        self.cmd('key', 'export', '--qr-html', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r', encoding='utf-8') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert bin_to_hex(repo_id) in export_contents",
            "        assert export_contents.startswith('<!doctype html>')",
            "        assert export_contents.endswith('</html>')",
            "",
            "    def test_key_import_errors(self):",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "",
            "        self.cmd('key', 'import', self.repository_location, export_file, exit_code=EXIT_ERROR)",
            "",
            "        with open(export_file, 'w') as fd:",
            "            fd.write('something not a key\\n')",
            "",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('key', 'import', self.repository_location, export_file, exit_code=2)",
            "        else:",
            "            with pytest.raises(NotABorgKeyFile):",
            "                self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'w') as fd:",
            "            fd.write('BORG_KEY a0a0a0\\n')",
            "",
            "        if self.FORK_DEFAULT:",
            "            self.cmd('key', 'import', self.repository_location, export_file, exit_code=2)",
            "        else:",
            "            with pytest.raises(RepoIdMismatch):",
            "                self.cmd('key', 'import', self.repository_location, export_file)",
            "",
            "    def test_key_export_paperkey(self):",
            "        repo_id = 'e294423506da4e1ea76e8dcdf1a3919624ae3ae496fddf905610c351d3f09239'",
            "",
            "        export_file = self.output_path + '/exported'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "        self._set_repository_id(self.repository_path, unhexlify(repo_id))",
            "",
            "        key_file = self.keys_path + '/' + os.listdir(self.keys_path)[0]",
            "",
            "        with open(key_file, 'w') as fd:",
            "            fd.write(KeyfileKey.FILE_ID + ' ' + repo_id + '\\n')",
            "            fd.write(b2a_base64(b'abcdefghijklmnopqrstu').decode())",
            "",
            "        self.cmd('key', 'export', '--paper', self.repository_location, export_file)",
            "",
            "        with open(export_file, 'r') as fd:",
            "            export_contents = fd.read()",
            "",
            "        assert export_contents == \"\"\"To restore key use borg key import --paper /path/to/repo",
            "",
            "BORG PAPER KEY v1",
            "id: 2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02",
            " 1: 616263 646566 676869 6a6b6c 6d6e6f 707172 - 6d",
            " 2: 737475 - 88",
            "\"\"\"",
            "",
            "    def test_key_import_paperkey(self):",
            "        repo_id = 'e294423506da4e1ea76e8dcdf1a3919624ae3ae496fddf905610c351d3f09239'",
            "        self.cmd('init', self.repository_location, '--encryption', 'keyfile')",
            "        self._set_repository_id(self.repository_path, unhexlify(repo_id))",
            "",
            "        key_file = self.keys_path + '/' + os.listdir(self.keys_path)[0]",
            "        with open(key_file, 'w') as fd:",
            "            fd.write(KeyfileKey.FILE_ID + ' ' + repo_id + '\\n')",
            "            fd.write(b2a_base64(b'abcdefghijklmnopqrstu').decode())",
            "",
            "        typed_input = (",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41  02\\n'   # Forgot to type \"-\"",
            "            b'2 / e29442 3506da 4e1ea7  25f62a 5a3d41 - 02\\n'   # Forgot to type second \"/\"",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d42 - 02\\n'  # Typo (..42 not ..41)",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02\\n'  # Correct! Congratulations",
            "            b'616263 646566 676869 6a6b6c 6d6e6f 707172 - 6d\\n'",
            "            b'\\n\\n'  # Abort [yN] => N",
            "            b'737475 88\\n'  # missing \"-\"",
            "            b'73747i - 88\\n'  # typo",
            "            b'73747 - 88\\n'  # missing nibble",
            "            b'73 74 75  -  89\\n'  # line checksum mismatch",
            "            b'00a1 - 88\\n'  # line hash collision - overall hash mismatch, have to start over",
            "",
            "            b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02\\n'",
            "            b'616263 646566 676869 6a6b6c 6d6e6f 707172 - 6d\\n'",
            "            b'73 74 75  -  88\\n'",
            "        )",
            "",
            "        # In case that this has to change, here is a quick way to find a colliding line hash:",
            "        #",
            "        # from hashlib import sha256",
            "        # hash_fn = lambda x: sha256(b'\\x00\\x02' + x).hexdigest()[:2]",
            "        # for i in range(1000):",
            "        #     if hash_fn(i.to_bytes(2, byteorder='big')) == '88':  # 88 = line hash",
            "        #         print(i.to_bytes(2, 'big'))",
            "        #         break",
            "",
            "        self.cmd('key', 'import', '--paper', self.repository_location, input=typed_input)",
            "",
            "        # Test abort paths",
            "        typed_input = b'\\ny\\n'",
            "        self.cmd('key', 'import', '--paper', self.repository_location, input=typed_input)",
            "        typed_input = b'2 / e29442 3506da 4e1ea7 / 25f62a 5a3d41 - 02\\n\\ny\\n'",
            "        self.cmd('key', 'import', '--paper', self.repository_location, input=typed_input)",
            "",
            "    def test_debug_dump_manifest(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        dump_file = self.output_path + '/dump'",
            "        output = self.cmd('debug', 'dump-manifest', self.repository_location, dump_file)",
            "        assert output == \"\"",
            "        with open(dump_file, \"r\") as f:",
            "            result = json.load(f)",
            "        assert 'archives' in result",
            "        assert 'config' in result",
            "        assert 'item_keys' in result",
            "        assert 'timestamp' in result",
            "        assert 'version' in result",
            "",
            "    def test_debug_dump_archive(self):",
            "        self.create_regular_file('file1', size=1024 * 80)",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        dump_file = self.output_path + '/dump'",
            "        output = self.cmd('debug', 'dump-archive', self.repository_location + \"::test\", dump_file)",
            "        assert output == \"\"",
            "        with open(dump_file, \"r\") as f:",
            "            result = json.load(f)",
            "        assert '_name' in result",
            "        assert '_manifest_entry' in result",
            "        assert '_meta' in result",
            "        assert '_items' in result",
            "",
            "    def test_debug_refcount_obj(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        output = self.cmd('debug', 'refcount-obj', self.repository_location, '0' * 64).strip()",
            "        assert output == 'object 0000000000000000000000000000000000000000000000000000000000000000 not found [info from chunks cache].'",
            "",
            "        create_json = json.loads(self.cmd('create', '--json', self.repository_location + '::test', 'input'))",
            "        archive_id = create_json['archive']['id']",
            "        output = self.cmd('debug', 'refcount-obj', self.repository_location, archive_id).strip()",
            "        assert output == 'object ' + archive_id + ' has 1 referrers [info from chunks cache].'",
            "",
            "        # Invalid IDs do not abort or return an error",
            "        output = self.cmd('debug', 'refcount-obj', self.repository_location, '124', 'xyza').strip()",
            "        assert output == 'object id 124 is invalid.\\nobject id xyza is invalid.'",
            "",
            "    def test_debug_info(self):",
            "        output = self.cmd('debug', 'info')",
            "        assert 'CRC implementation' in output",
            "        assert 'Python' in output",
            "",
            "    def test_benchmark_crud(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        with environment_variable(_BORG_BENCHMARK_CRUD_TEST='YES'):",
            "            self.cmd('benchmark', 'crud', self.repository_location, self.input_path)",
            "",
            "    def test_config(self):",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        for flags in [[], ['--cache']]:",
            "            for cfg_key in {'testkey', 'testsection.testkey'}:",
            "                self.cmd('config', self.repository_location, *flags, cfg_key, exit_code=1)",
            "                self.cmd('config', self.repository_location, *flags, cfg_key, 'testcontents')",
            "                output = self.cmd('config', self.repository_location, *flags, cfg_key)",
            "                assert output == 'testcontents\\n'",
            "                self.cmd('config', self.repository_location, *flags, '--delete', cfg_key)",
            "                self.cmd('config', self.repository_location, *flags, cfg_key, exit_code=1)",
            "",
            "    requires_gnutar = pytest.mark.skipif(not have_gnutar(), reason='GNU tar must be installed for this test.')",
            "    requires_gzip = pytest.mark.skipif(not shutil.which('gzip'), reason='gzip must be installed for this test.')",
            "",
            "    @requires_gnutar",
            "    def test_export_tar(self):",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.cmd('export-tar', self.repository_location + '::test', 'simple.tar', '--progress')",
            "        with changedir('output'):",
            "            # This probably assumes GNU tar. Note -p switch to extract permissions regardless of umask.",
            "            subprocess.check_call(['tar', 'xpf', '../simple.tar', '--warning=no-timestamp'])",
            "        self.assert_dirs_equal('input', 'output/input', ignore_bsdflags=True, ignore_xattrs=True, ignore_ns=True)",
            "",
            "    @requires_gnutar",
            "    @requires_gzip",
            "    def test_export_tar_gz(self):",
            "        if not shutil.which('gzip'):",
            "            pytest.skip('gzip is not installed')",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        list = self.cmd('export-tar', self.repository_location + '::test', 'simple.tar.gz', '--list')",
            "        assert 'input/file1\\n' in list",
            "        assert 'input/dir2\\n' in list",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../simple.tar.gz', '--warning=no-timestamp'])",
            "        self.assert_dirs_equal('input', 'output/input', ignore_bsdflags=True, ignore_xattrs=True, ignore_ns=True)",
            "",
            "    @requires_gnutar",
            "    def test_export_tar_strip_components(self):",
            "        if not shutil.which('gzip'):",
            "            pytest.skip('gzip is not installed')",
            "        self.create_test_files()",
            "        os.unlink('input/flagfile')",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        list = self.cmd('export-tar', self.repository_location + '::test', 'simple.tar', '--strip-components=1', '--list')",
            "        # --list's path are those before processing with --strip-components",
            "        assert 'input/file1\\n' in list",
            "        assert 'input/dir2\\n' in list",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../simple.tar', '--warning=no-timestamp'])",
            "        self.assert_dirs_equal('input', 'output/', ignore_bsdflags=True, ignore_xattrs=True, ignore_ns=True)",
            "",
            "    @requires_hardlinks",
            "    @requires_gnutar",
            "    def test_export_tar_strip_components_links(self):",
            "        self._extract_hardlinks_setup()",
            "        self.cmd('export-tar', self.repository_location + '::test', 'output.tar', '--strip-components=2')",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../output.tar', '--warning=no-timestamp'])",
            "            assert os.stat('hardlink').st_nlink == 2",
            "            assert os.stat('subdir/hardlink').st_nlink == 2",
            "            assert os.stat('aaaa').st_nlink == 2",
            "            assert os.stat('source2').st_nlink == 2",
            "",
            "    @requires_hardlinks",
            "    @requires_gnutar",
            "    def test_extract_hardlinks(self):",
            "        self._extract_hardlinks_setup()",
            "        self.cmd('export-tar', self.repository_location + '::test', 'output.tar', 'input/dir1')",
            "        with changedir('output'):",
            "            subprocess.check_call(['tar', 'xpf', '../output.tar', '--warning=no-timestamp'])",
            "            assert os.stat('input/dir1/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/subdir/hardlink').st_nlink == 2",
            "            assert os.stat('input/dir1/aaaa').st_nlink == 2",
            "            assert os.stat('input/dir1/source2').st_nlink == 2",
            "",
            "    def test_detect_attic_repo(self):",
            "        path = attic_repo(self.repository_path)",
            "        cmds = [",
            "            ['create', path + '::test', self.tmpdir],",
            "            ['extract', path + '::test'],",
            "            ['check', path],",
            "            ['rename', path + '::test', 'newname'],",
            "            ['list', path],",
            "            ['delete', path],",
            "            ['prune', path],",
            "            ['info', path + '::test'],",
            "            ['key', 'export', path, 'exported'],",
            "            ['key', 'import', path, 'import'],",
            "            ['change-passphrase', path],",
            "            ['break-lock', path],",
            "        ]",
            "        for args in cmds:",
            "            output = self.cmd(*args, fork=True, exit_code=2)",
            "            assert 'Attic repository detected.' in output",
            "",
            "",
            "@unittest.skipUnless('binary' in BORG_EXES, 'no borg.exe available')",
            "class ArchiverTestCaseBinary(ArchiverTestCase):",
            "    EXE = 'borg.exe'",
            "    FORK_DEFAULT = True",
            "",
            "    @unittest.skip('patches objects')",
            "    def test_init_interrupt(self):",
            "        pass",
            "",
            "    @unittest.skip('patches objects')",
            "    def test_extract_capabilities(self):",
            "        pass",
            "",
            "    @unittest.skip('patches objects')",
            "    def test_extract_xattrs_errors(self):",
            "        pass",
            "",
            "    @unittest.skip('test_basic_functionality seems incompatible with fakeroot and/or the binary.')",
            "    def test_basic_functionality(self):",
            "        pass",
            "",
            "    @unittest.skip('test_overwrite seems incompatible with fakeroot and/or the binary.')",
            "    def test_overwrite(self):",
            "        pass",
            "",
            "    def test_fuse(self):",
            "        if fakeroot_detected():",
            "            unittest.skip('test_fuse with the binary is not compatible with fakeroot')",
            "        else:",
            "            super().test_fuse()",
            "",
            "",
            "class ArchiverCheckTestCase(ArchiverTestCaseBase):",
            "",
            "    def setUp(self):",
            "        super().setUp()",
            "        with patch.object(ChunkBuffer, 'BUFFER_SIZE', 10):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "            self.create_src_archive('archive1')",
            "            self.create_src_archive('archive2')",
            "",
            "    def test_check_usage(self):",
            "        output = self.cmd('check', '-v', '--progress', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "        self.assert_in('Checking segments', output)",
            "        # reset logging to new process default to avoid need for fork=True on next check",
            "        logging.getLogger('borg.output.progress').setLevel(logging.NOTSET)",
            "        output = self.cmd('check', '-v', '--repository-only', self.repository_location, exit_code=0)",
            "        self.assert_in('Starting repository check', output)",
            "        self.assert_not_in('Starting archive consistency check', output)",
            "        self.assert_not_in('Checking segments', output)",
            "        output = self.cmd('check', '-v', '--archives-only', self.repository_location, exit_code=0)",
            "        self.assert_not_in('Starting repository check', output)",
            "        self.assert_in('Starting archive consistency check', output)",
            "        output = self.cmd('check', '-v', '--archives-only', '--prefix=archive2', self.repository_location, exit_code=0)",
            "        self.assert_not_in('archive1', output)",
            "        output = self.cmd('check', '-v', '--archives-only', '--first=1', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_not_in('archive2', output)",
            "        output = self.cmd('check', '-v', '--archives-only', '--last=1', self.repository_location, exit_code=0)",
            "        self.assert_not_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "",
            "    def test_missing_file_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith('testsuite/archiver.py'):",
            "                    valid_chunks = item.chunks",
            "                    killed_chunk = valid_chunks[-1]",
            "                    repository.delete(killed_chunk.id)",
            "                    break",
            "            else:",
            "                self.fail('should not happen')",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('New missing file chunk detected', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        output = self.cmd('list', '--format={health}#{path}{LF}', self.repository_location + '::archive1', exit_code=0)",
            "        self.assert_in('broken#', output)",
            "        # check that the file in the old archives has now a different chunk list without the killed chunk",
            "        for archive_name in ('archive1', 'archive2'):",
            "            archive, repository = self.open_archive(archive_name)",
            "            with repository:",
            "                for item in archive.iter_items():",
            "                    if item.path.endswith('testsuite/archiver.py'):",
            "                        self.assert_not_equal(valid_chunks, item.chunks)",
            "                        self.assert_not_in(killed_chunk, item.chunks)",
            "                        break",
            "                else:",
            "                    self.fail('should not happen')",
            "        # do a fresh backup (that will include the killed chunk)",
            "        with patch.object(ChunkBuffer, 'BUFFER_SIZE', 10):",
            "            self.create_src_archive('archive3')",
            "        # check should be able to heal the file now:",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('Healed previously missing file chunk', output)",
            "        self.assert_in('testsuite/archiver.py: Completely healed previously damaged file!', output)",
            "        # check that the file in the old archives has the correct chunks again",
            "        for archive_name in ('archive1', 'archive2'):",
            "            archive, repository = self.open_archive(archive_name)",
            "            with repository:",
            "                for item in archive.iter_items():",
            "                    if item.path.endswith('testsuite/archiver.py'):",
            "                        self.assert_equal(valid_chunks, item.chunks)",
            "                        break",
            "                else:",
            "                    self.fail('should not happen')",
            "        # list is also all-healthy again",
            "        output = self.cmd('list', '--format={health}#{path}{LF}', self.repository_location + '::archive1', exit_code=0)",
            "        self.assert_not_in('broken#', output)",
            "",
            "    def test_missing_archive_item_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            repository.delete(archive.metadata.items[-5])",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_archive_metadata(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            repository.delete(archive.id)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_missing_manifest(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            repository.delete(Manifest.MANIFEST_ID)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_corrupted_manifest(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            manifest = repository.get(Manifest.MANIFEST_ID)",
            "            corrupted_manifest = manifest + b'corrupted!'",
            "            repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive2', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_manifest_rebuild_corrupted_chunk(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            manifest = repository.get(Manifest.MANIFEST_ID)",
            "            corrupted_manifest = manifest + b'corrupted!'",
            "            repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "",
            "            chunk = repository.get(archive.id)",
            "            corrupted_chunk = chunk + b'corrupted!'",
            "            repository.put(archive.id, corrupted_chunk)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        output = self.cmd('check', '-v', '--repair', self.repository_location, exit_code=0)",
            "        self.assert_in('archive2', output)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "",
            "    def test_manifest_rebuild_duplicate_archive(self):",
            "        archive, repository = self.open_archive('archive1')",
            "        key = archive.key",
            "        with repository:",
            "            manifest = repository.get(Manifest.MANIFEST_ID)",
            "            corrupted_manifest = manifest + b'corrupted!'",
            "            repository.put(Manifest.MANIFEST_ID, corrupted_manifest)",
            "",
            "            archive = msgpack.packb({",
            "                'cmdline': [],",
            "                'items': [],",
            "                'hostname': 'foo',",
            "                'username': 'bar',",
            "                'name': 'archive1',",
            "                'time': '2016-12-15T18:49:51.849711',",
            "                'version': 1,",
            "            })",
            "            archive_id = key.id_hash(archive)",
            "            repository.put(archive_id, key.encrypt(archive))",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        output = self.cmd('list', self.repository_location)",
            "        self.assert_in('archive1', output)",
            "        self.assert_in('archive1.1', output)",
            "        self.assert_in('archive2', output)",
            "",
            "    def test_extra_chunks(self):",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        with Repository(self.repository_location, exclusive=True) as repository:",
            "            repository.put(b'01234567890123456789012345678901', b'xxxx')",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "        self.cmd('check', '--repair', self.repository_location, exit_code=0)",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        self.cmd('extract', '--dry-run', self.repository_location + '::archive1', exit_code=0)",
            "",
            "    def _test_verify_data(self, *init_args):",
            "        shutil.rmtree(self.repository_path)",
            "        self.cmd('init', self.repository_location, *init_args)",
            "        self.create_src_archive('archive1')",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            for item in archive.iter_items():",
            "                if item.path.endswith('testsuite/archiver.py'):",
            "                    chunk = item.chunks[-1]",
            "                    data = repository.get(chunk.id) + b'1234'",
            "                    repository.put(chunk.id, data)",
            "                    break",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        output = self.cmd('check', '--verify-data', self.repository_location, exit_code=1)",
            "        assert bin_to_hex(chunk.id) + ', integrity error' in output",
            "        # repair (heal is tested in another test)",
            "        output = self.cmd('check', '--repair', '--verify-data', self.repository_location, exit_code=0)",
            "        assert bin_to_hex(chunk.id) + ', integrity error' in output",
            "        assert 'testsuite/archiver.py: New missing file chunk detected' in output",
            "",
            "    def test_verify_data(self):",
            "        self._test_verify_data('--encryption', 'repokey')",
            "",
            "    def test_verify_data_unencrypted(self):",
            "        self._test_verify_data('--encryption', 'none')",
            "",
            "    def test_empty_repository(self):",
            "        with Repository(self.repository_location, exclusive=True) as repository:",
            "            for id_ in repository.list():",
            "                repository.delete(id_)",
            "            repository.commit()",
            "        self.cmd('check', self.repository_location, exit_code=1)",
            "",
            "    def test_attic013_acl_bug(self):",
            "        # Attic up to release 0.13 contained a bug where every item unintentionally received",
            "        # a b'acl'=None key-value pair.",
            "        # This bug can still live on in Borg repositories (through borg upgrade).",
            "        class Attic013Item:",
            "            def as_dict(self):",
            "                return {",
            "                    # These are required",
            "                    b'path': '1234',",
            "                    b'mtime': 0,",
            "                    b'mode': 0,",
            "                    b'user': b'0',",
            "                    b'group': b'0',",
            "                    b'uid': 0,",
            "                    b'gid': 0,",
            "                    # acl is the offending key.",
            "                    b'acl': None,",
            "                }",
            "",
            "        archive, repository = self.open_archive('archive1')",
            "        with repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            with Cache(repository, key, manifest) as cache:",
            "                archive = Archive(repository, key, manifest, '0.13', cache=cache, create=True)",
            "                archive.items_buffer.add(Attic013Item())",
            "                archive.save()",
            "        self.cmd('check', self.repository_location, exit_code=0)",
            "        self.cmd('list', self.repository_location + '::0.13', exit_code=0)",
            "",
            "",
            "class ManifestAuthenticationTest(ArchiverTestCaseBase):",
            "    def spoof_manifest(self, repository):",
            "        with repository:",
            "            _, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            repository.put(Manifest.MANIFEST_ID, key.encrypt(msgpack.packb({",
            "                'version': 1,",
            "                'archives': {},",
            "                'config': {},",
            "                'timestamp': (datetime.utcnow() + timedelta(days=1)).strftime(ISO_FORMAT),",
            "            })))",
            "            repository.commit()",
            "",
            "    def test_fresh_init_tam_required(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        with repository:",
            "            manifest, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            repository.put(Manifest.MANIFEST_ID, key.encrypt(msgpack.packb({",
            "                'version': 1,",
            "                'archives': {},",
            "                'timestamp': (datetime.utcnow() + timedelta(days=1)).strftime(ISO_FORMAT),",
            "            })))",
            "            repository.commit()",
            "",
            "        with pytest.raises(TAMRequiredError):",
            "            self.cmd('list', self.repository_location)",
            "",
            "    def test_not_required(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive1234')",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        with repository:",
            "            shutil.rmtree(get_security_dir(bin_to_hex(repository.id)))",
            "            _, key = Manifest.load(repository, Manifest.NO_OPERATION_CHECK)",
            "            key.tam_required = False",
            "            key.change_passphrase(key._passphrase)",
            "",
            "            manifest = msgpack.unpackb(key.decrypt(None, repository.get(Manifest.MANIFEST_ID)))",
            "            del manifest[b'tam']",
            "            repository.put(Manifest.MANIFEST_ID, key.encrypt(msgpack.packb(manifest)))",
            "            repository.commit()",
            "        output = self.cmd('list', '--debug', self.repository_location)",
            "        assert 'archive1234' in output",
            "        assert 'TAM not found and not required' in output",
            "        # Run upgrade",
            "        self.cmd('upgrade', '--tam', self.repository_location)",
            "        # Manifest must be authenticated now",
            "        output = self.cmd('list', '--debug', self.repository_location)",
            "        assert 'archive1234' in output",
            "        assert 'TAM-verified manifest' in output",
            "        # Try to spoof / modify pre-1.0.9",
            "        self.spoof_manifest(repository)",
            "        # Fails",
            "        with pytest.raises(TAMRequiredError):",
            "            self.cmd('list', self.repository_location)",
            "        # Force upgrade",
            "        self.cmd('upgrade', '--tam', '--force', self.repository_location)",
            "        self.cmd('list', self.repository_location)",
            "",
            "    def test_disable(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive1234')",
            "        self.cmd('upgrade', '--disable-tam', self.repository_location)",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        self.spoof_manifest(repository)",
            "        assert not self.cmd('list', self.repository_location)",
            "",
            "    def test_disable2(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_src_archive('archive1234')",
            "        repository = Repository(self.repository_path, exclusive=True)",
            "        self.spoof_manifest(repository)",
            "        self.cmd('upgrade', '--disable-tam', self.repository_location)",
            "        assert not self.cmd('list', self.repository_location)",
            "",
            "",
            "class RemoteArchiverTestCase(ArchiverTestCase):",
            "    prefix = '__testsuite__:'",
            "",
            "    def open_repository(self):",
            "        return RemoteRepository(Location(self.repository_location))",
            "",
            "    def test_remote_repo_restrict_to_path(self):",
            "        # restricted to repo directory itself:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', self.repository_path]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        # restricted to repo directory itself, fail for other directories with same prefix:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', self.repository_path]):",
            "            with pytest.raises(PathNotAllowed):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location + '_0')",
            "",
            "        # restricted to a completely different path:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo']):",
            "            with pytest.raises(PathNotAllowed):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location + '_1')",
            "        path_prefix = os.path.dirname(self.repository_path)",
            "        # restrict to repo directory's parent directory:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', path_prefix]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location + '_2')",
            "        # restrict to repo directory's parent directory and another directory:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-path', '/foo', '--restrict-to-path', path_prefix]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location + '_3')",
            "",
            "    def test_remote_repo_restrict_to_repository(self):",
            "        # restricted to repo directory itself:",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-repository', self.repository_path]):",
            "            self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        parent_path = os.path.join(self.repository_path, '..')",
            "        with patch.object(RemoteRepository, 'extra_test_args', ['--restrict-to-repository', parent_path]):",
            "            with pytest.raises(PathNotAllowed):",
            "                self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "    @unittest.skip('only works locally')",
            "    def test_debug_put_get_delete_obj(self):",
            "        pass",
            "",
            "    @unittest.skip('only works locally')",
            "    def test_config(self):",
            "        pass",
            "",
            "    def test_strip_components_doesnt_leak(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.create_regular_file('dir/file', contents=b\"test file contents 1\")",
            "        self.create_regular_file('dir/file2', contents=b\"test file contents 2\")",
            "        self.create_regular_file('skipped-file1', contents=b\"test file contents 3\")",
            "        self.create_regular_file('skipped-file2', contents=b\"test file contents 4\")",
            "        self.create_regular_file('skipped-file3', contents=b\"test file contents 5\")",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        marker = 'cached responses left in RemoteRepository'",
            "        with changedir('output'):",
            "            res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '3')",
            "            self.assert_true(marker not in res)",
            "            with self.assert_creates_file('file'):",
            "                res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '2')",
            "                self.assert_true(marker not in res)",
            "            with self.assert_creates_file('dir/file'):",
            "                res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '1')",
            "                self.assert_true(marker not in res)",
            "            with self.assert_creates_file('input/dir/file'):",
            "                res = self.cmd('extract', \"--debug\", self.repository_location + '::test', '--strip-components', '0')",
            "                self.assert_true(marker not in res)",
            "",
            "",
            "class ArchiverCorruptionTestCase(ArchiverTestCaseBase):",
            "    def setUp(self):",
            "        super().setUp()",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "        self.cache_path = json.loads(self.cmd('info', self.repository_location, '--json'))['cache']['path']",
            "",
            "    def corrupt(self, file):",
            "        with open(file, 'r+b') as fd:",
            "            fd.seek(-1, io.SEEK_END)",
            "            fd.write(b'1')",
            "",
            "    def test_cache_chunks(self):",
            "        self.corrupt(os.path.join(self.cache_path, 'chunks'))",
            "",
            "        if self.FORK_DEFAULT:",
            "            out = self.cmd('info', self.repository_location, exit_code=2)",
            "            assert 'failed integrity check' in out",
            "        else:",
            "            with pytest.raises(FileIntegrityError):",
            "                self.cmd('info', self.repository_location)",
            "",
            "    def test_cache_files(self):",
            "        self.cmd('create', self.repository_location + '::test', 'input')",
            "        self.corrupt(os.path.join(self.cache_path, 'files'))",
            "",
            "        if self.FORK_DEFAULT:",
            "            out = self.cmd('create', self.repository_location + '::test1', 'input', exit_code=2)",
            "            assert 'failed integrity check' in out",
            "        else:",
            "            with pytest.raises(FileIntegrityError):",
            "                self.cmd('create', self.repository_location + '::test1', 'input')",
            "",
            "    def test_chunks_archive(self):",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "        # Find ID of test1 so we can corrupt it later :)",
            "        target_id = self.cmd('list', self.repository_location, '--format={id}{LF}').strip()",
            "        self.cmd('create', self.repository_location + '::test2', 'input')",
            "",
            "        # Force cache sync, creating archive chunks of test1 and test2 in chunks.archive.d",
            "        self.cmd('delete', '--cache-only', self.repository_location)",
            "        self.cmd('info', self.repository_location, '--json')",
            "",
            "        chunks_archive = os.path.join(self.cache_path, 'chunks.archive.d')",
            "        assert len(os.listdir(chunks_archive)) == 4  # two archives, one chunks cache and one .integrity file each",
            "",
            "        self.corrupt(os.path.join(chunks_archive, target_id + '.compact'))",
            "",
            "        # Trigger cache sync by changing the manifest ID in the cache config",
            "        config_path = os.path.join(self.cache_path, 'config')",
            "        config = ConfigParser(interpolation=None)",
            "        config.read(config_path)",
            "        config.set('cache', 'manifest', bin_to_hex(bytes(32)))",
            "        with open(config_path, 'w') as fd:",
            "            config.write(fd)",
            "",
            "        # Cache sync notices corrupted archive chunks, but automatically recovers.",
            "        out = self.cmd('create', '-v', self.repository_location + '::test3', 'input', exit_code=1)",
            "        assert 'Reading cached archive chunk index for test1' in out",
            "        assert 'Cached archive chunk index of test1 is corrupted' in out",
            "        assert 'Fetching and building archive index for test1' in out",
            "",
            "    def test_old_version_interfered(self):",
            "        # Modify the main manifest ID without touching the manifest ID in the integrity section.",
            "        # This happens if a version without integrity checking modifies the cache.",
            "        config_path = os.path.join(self.cache_path, 'config')",
            "        config = ConfigParser(interpolation=None)",
            "        config.read(config_path)",
            "        config.set('cache', 'manifest', bin_to_hex(bytes(32)))",
            "        with open(config_path, 'w') as fd:",
            "            config.write(fd)",
            "",
            "        out = self.cmd('info', self.repository_location)",
            "        assert 'Cache integrity data not available: old Borg version modified the cache.' in out",
            "",
            "",
            "class DiffArchiverTestCase(ArchiverTestCaseBase):",
            "    def test_basic_functionality(self):",
            "        # Initialize test folder",
            "        self.create_test_files()",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "        # Setup files for the first snapshot",
            "        self.create_regular_file('file_unchanged', size=128)",
            "        self.create_regular_file('file_removed', size=256)",
            "        self.create_regular_file('file_removed2', size=512)",
            "        self.create_regular_file('file_replaced', size=1024)",
            "        os.mkdir('input/dir_replaced_with_file')",
            "        os.chmod('input/dir_replaced_with_file', stat.S_IFDIR | 0o755)",
            "        os.mkdir('input/dir_removed')",
            "        if are_symlinks_supported():",
            "            os.mkdir('input/dir_replaced_with_link')",
            "            os.symlink('input/dir_replaced_with_file', 'input/link_changed')",
            "            os.symlink('input/file_unchanged', 'input/link_removed')",
            "            os.symlink('input/file_removed2', 'input/link_target_removed')",
            "            os.symlink('input/empty', 'input/link_target_contents_changed')",
            "            os.symlink('input/empty', 'input/link_replaced_by_file')",
            "        if are_hardlinks_supported():",
            "            os.link('input/empty', 'input/hardlink_contents_changed')",
            "            os.link('input/file_removed', 'input/hardlink_removed')",
            "            os.link('input/file_removed2', 'input/hardlink_target_removed')",
            "            os.link('input/file_replaced', 'input/hardlink_target_replaced')",
            "",
            "        # Create the first snapshot",
            "        self.cmd('create', self.repository_location + '::test0', 'input')",
            "",
            "        # Setup files for the second snapshot",
            "        self.create_regular_file('file_added', size=2048)",
            "        os.unlink('input/file_removed')",
            "        os.unlink('input/file_removed2')",
            "        os.unlink('input/file_replaced')",
            "        self.create_regular_file('file_replaced', size=4096, contents=b'0')",
            "        os.rmdir('input/dir_replaced_with_file')",
            "        self.create_regular_file('dir_replaced_with_file', size=8192)",
            "        os.chmod('input/dir_replaced_with_file', stat.S_IFREG | 0o755)",
            "        os.mkdir('input/dir_added')",
            "        os.rmdir('input/dir_removed')",
            "        if are_symlinks_supported():",
            "            os.rmdir('input/dir_replaced_with_link')",
            "            os.symlink('input/dir_added', 'input/dir_replaced_with_link')",
            "            os.unlink('input/link_changed')",
            "            os.symlink('input/dir_added', 'input/link_changed')",
            "            os.symlink('input/dir_added', 'input/link_added')",
            "            os.unlink('input/link_replaced_by_file')",
            "            self.create_regular_file('link_replaced_by_file', size=16384)",
            "            os.unlink('input/link_removed')",
            "        if are_hardlinks_supported():",
            "            os.unlink('input/hardlink_removed')",
            "            os.link('input/file_added', 'input/hardlink_added')",
            "",
            "        with open('input/empty', 'ab') as fd:",
            "            fd.write(b'appended_data')",
            "",
            "        # Create the second snapshot",
            "        self.cmd('create', self.repository_location + '::test1a', 'input')",
            "        self.cmd('create', '--chunker-params', '16,18,17,4095', self.repository_location + '::test1b', 'input')",
            "",
            "        def do_asserts(output, can_compare_ids):",
            "            # File contents changed (deleted and replaced with a new file)",
            "            change = 'B' if can_compare_ids else '{:<19}'.format('modified')",
            "            assert '{} input/file_replaced'.format(change) in output",
            "",
            "            # File unchanged",
            "            assert 'input/file_unchanged' not in output",
            "",
            "            # Directory replaced with a regular file",
            "            if 'BORG_TESTS_IGNORE_MODES' not in os.environ:",
            "                assert '[drwxr-xr-x -> -rwxr-xr-x] input/dir_replaced_with_file' in output",
            "",
            "            # Basic directory cases",
            "            assert 'added directory     input/dir_added' in output",
            "            assert 'removed directory   input/dir_removed' in output",
            "",
            "            if are_symlinks_supported():",
            "                # Basic symlink cases",
            "                assert 'changed link        input/link_changed' in output",
            "                assert 'added link          input/link_added' in output",
            "                assert 'removed link        input/link_removed' in output",
            "",
            "                # Symlink replacing or being replaced",
            "                assert '] input/dir_replaced_with_link' in output",
            "                assert '] input/link_replaced_by_file' in output",
            "",
            "                # Symlink target removed. Should not affect the symlink at all.",
            "                assert 'input/link_target_removed' not in output",
            "",
            "            # The inode has two links and the file contents changed. Borg",
            "            # should notice the changes in both links. However, the symlink",
            "            # pointing to the file is not changed.",
            "            change = '0 B' if can_compare_ids else '{:<19}'.format('modified')",
            "            assert '{} input/empty'.format(change) in output",
            "            if are_hardlinks_supported():",
            "                assert '{} input/hardlink_contents_changed'.format(change) in output",
            "            if are_symlinks_supported():",
            "                assert 'input/link_target_contents_changed' not in output",
            "",
            "            # Added a new file and a hard link to it. Both links to the same",
            "            # inode should appear as separate files.",
            "            assert 'added       2.05 kB input/file_added' in output",
            "            if are_hardlinks_supported():",
            "                assert 'added       2.05 kB input/hardlink_added' in output",
            "",
            "            # The inode has two links and both of them are deleted. They should",
            "            # appear as two deleted files.",
            "            assert 'removed       256 B input/file_removed' in output",
            "            if are_hardlinks_supported():",
            "                assert 'removed       256 B input/hardlink_removed' in output",
            "",
            "            # Another link (marked previously as the source in borg) to the",
            "            # same inode was removed. This should not change this link at all.",
            "            if are_hardlinks_supported():",
            "                assert 'input/hardlink_target_removed' not in output",
            "",
            "            # Another link (marked previously as the source in borg) to the",
            "            # same inode was replaced with a new regular file. This should not",
            "            # change this link at all.",
            "            if are_hardlinks_supported():",
            "                assert 'input/hardlink_target_replaced' not in output",
            "",
            "        do_asserts(self.cmd('diff', self.repository_location + '::test0', 'test1a'), True)",
            "        # We expect exit_code=1 due to the chunker params warning",
            "        do_asserts(self.cmd('diff', self.repository_location + '::test0', 'test1b', exit_code=1), False)",
            "",
            "    def test_sort_option(self):",
            "        self.cmd('init', '--encryption=repokey', self.repository_location)",
            "",
            "        self.create_regular_file('a_file_removed', size=8)",
            "        self.create_regular_file('f_file_removed', size=16)",
            "        self.create_regular_file('c_file_changed', size=32)",
            "        self.create_regular_file('e_file_changed', size=64)",
            "        self.cmd('create', self.repository_location + '::test0', 'input')",
            "",
            "        os.unlink('input/a_file_removed')",
            "        os.unlink('input/f_file_removed')",
            "        os.unlink('input/c_file_changed')",
            "        os.unlink('input/e_file_changed')",
            "        self.create_regular_file('c_file_changed', size=512)",
            "        self.create_regular_file('e_file_changed', size=1024)",
            "        self.create_regular_file('b_file_added', size=128)",
            "        self.create_regular_file('d_file_added', size=256)",
            "        self.cmd('create', self.repository_location + '::test1', 'input')",
            "",
            "        output = self.cmd('diff', '--sort', self.repository_location + '::test0', 'test1')",
            "        expected = [",
            "            'a_file_removed',",
            "            'b_file_added',",
            "            'c_file_changed',",
            "            'd_file_added',",
            "            'e_file_changed',",
            "            'f_file_removed',",
            "        ]",
            "",
            "        assert all(x in line for x, line in zip(expected, output.splitlines()))",
            "",
            "",
            "def test_get_args():",
            "    archiver = Archiver()",
            "    # everything normal:",
            "    # first param is argv as produced by ssh forced command,",
            "    # second param is like from SSH_ORIGINAL_COMMAND env variable",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],",
            "                             'borg serve --info --umask=0027')",
            "    assert args.func == archiver.do_serve",
            "    assert args.restrict_to_paths == ['/p1', '/p2']",
            "    assert args.umask == 0o027",
            "    assert args.log_level == 'info'",
            "    # similar, but with --restrict-to-repository",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-repository=/r1', '--restrict-to-repository=/r2', ],",
            "                             'borg serve --info --umask=0027')",
            "    assert args.restrict_to_repositories == ['/r1', '/r2']",
            "    # trying to cheat - break out of path restriction",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],",
            "                             'borg serve --restrict-to-path=/')",
            "    assert args.restrict_to_paths == ['/p1', '/p2']",
            "    # trying to cheat - break out of repository restriction",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-repository=/r1', '--restrict-to-repository=/r2', ],",
            "                             'borg serve --restrict-to-repository=/')",
            "    assert args.restrict_to_repositories == ['/r1', '/r2']",
            "    # trying to cheat - break below repository restriction",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-repository=/r1', '--restrict-to-repository=/r2', ],",
            "                             'borg serve --restrict-to-repository=/r1/below')",
            "    assert args.restrict_to_repositories == ['/r1', '/r2']",
            "    # trying to cheat - try to execute different subcommand",
            "    args = archiver.get_args(['borg', 'serve', '--restrict-to-path=/p1', '--restrict-to-path=/p2', ],",
            "                             'borg init --encryption=repokey /')",
            "    assert args.func == archiver.do_serve",
            "",
            "    # Check that environment variables in the forced command don't cause issues. If the command",
            "    # were not forced, environment variables would be interpreted by the shell, but this does not",
            "    # happen for forced commands - we get the verbatim command line and need to deal with env vars.",
            "    args = archiver.get_args(['borg', 'serve', ],",
            "                             'BORG_HOSTNAME_IS_UNIQUE=yes borg serve --info')",
            "    assert args.func == archiver.do_serve",
            "",
            "",
            "def test_chunk_content_equal():",
            "    def ccc(a, b):",
            "        chunks_a = [data for data in a]",
            "        chunks_b = [data for data in b]",
            "        compare1 = ItemDiff._chunk_content_equal(iter(chunks_a), iter(chunks_b))",
            "        compare2 = ItemDiff._chunk_content_equal(iter(chunks_b), iter(chunks_a))",
            "        assert compare1 == compare2",
            "        return compare1",
            "    assert ccc([",
            "        b'1234', b'567A', b'bC'",
            "    ], [",
            "        b'1', b'23', b'4567A', b'b', b'C'",
            "    ])",
            "    # one iterator exhausted before the other",
            "    assert not ccc([",
            "        b'12345',",
            "    ], [",
            "        b'1234', b'56'",
            "    ])",
            "    # content mismatch",
            "    assert not ccc([",
            "        b'1234', b'65'",
            "    ], [",
            "        b'1234', b'56'",
            "    ])",
            "    # first is the prefix of second",
            "    assert not ccc([",
            "        b'1234', b'56'",
            "    ], [",
            "        b'1234', b'565'",
            "    ])",
            "",
            "",
            "class TestBuildFilter:",
            "    @staticmethod",
            "    def peek_and_store_hardlink_masters(item, matched):",
            "        pass",
            "",
            "    def test_basic(self):",
            "        matcher = PatternMatcher()",
            "        matcher.add([parse_pattern('included')], IECommand.Include)",
            "        filter = Archiver.build_filter(matcher, self.peek_and_store_hardlink_masters, 0)",
            "        assert filter(Item(path='included'))",
            "        assert filter(Item(path='included/file'))",
            "        assert not filter(Item(path='something else'))",
            "",
            "    def test_empty(self):",
            "        matcher = PatternMatcher(fallback=True)",
            "        filter = Archiver.build_filter(matcher, self.peek_and_store_hardlink_masters, 0)",
            "        assert filter(Item(path='anything'))",
            "",
            "    def test_strip_components(self):",
            "        matcher = PatternMatcher(fallback=True)",
            "        filter = Archiver.build_filter(matcher, self.peek_and_store_hardlink_masters, strip_components=1)",
            "        assert not filter(Item(path='shallow'))",
            "        assert not filter(Item(path='shallow/'))  # can this even happen? paths are normalized...",
            "        assert filter(Item(path='deep enough/file'))",
            "        assert filter(Item(path='something/dir/file'))",
            "",
            "",
            "class TestCommonOptions:",
            "    @staticmethod",
            "    def define_common_options(add_common_option):",
            "        add_common_option('-h', '--help', action='help', help='show this help message and exit')",
            "        add_common_option('--critical', dest='log_level', help='foo',",
            "                          action='store_const', const='critical', default='warning')",
            "        add_common_option('--error', dest='log_level', help='foo',",
            "                          action='store_const', const='error', default='warning')",
            "        add_common_option('--append', dest='append', help='foo',",
            "                          action='append', metavar='TOPIC', default=[])",
            "        add_common_option('-p', '--progress', dest='progress', action='store_true', help='foo')",
            "        add_common_option('--lock-wait', dest='lock_wait', type=int, metavar='N', default=1,",
            "                          help='(default: %(default)d).')",
            "",
            "    @pytest.fixture",
            "    def basic_parser(self):",
            "        parser = argparse.ArgumentParser(prog='test', description='test parser', add_help=False)",
            "        parser.common_options = Archiver.CommonOptions(self.define_common_options,",
            "                                                       suffix_precedence=('_level0', '_level1'))",
            "        return parser",
            "",
            "    @pytest.fixture",
            "    def subparsers(self, basic_parser):",
            "        return basic_parser.add_subparsers(title='required arguments', metavar='<command>')",
            "",
            "    @pytest.fixture",
            "    def parser(self, basic_parser):",
            "        basic_parser.common_options.add_common_group(basic_parser, '_level0', provide_defaults=True)",
            "        return basic_parser",
            "",
            "    @pytest.fixture",
            "    def common_parser(self, parser):",
            "        common_parser = argparse.ArgumentParser(add_help=False, prog='test')",
            "        parser.common_options.add_common_group(common_parser, '_level1')",
            "        return common_parser",
            "",
            "    @pytest.fixture",
            "    def parse_vars_from_line(self, parser, subparsers, common_parser):",
            "        subparser = subparsers.add_parser('subcommand', parents=[common_parser], add_help=False,",
            "                                          description='foo', epilog='bar', help='baz',",
            "                                          formatter_class=argparse.RawDescriptionHelpFormatter)",
            "        subparser.set_defaults(func=1234)",
            "        subparser.add_argument('--append-only', dest='append_only', action='store_true')",
            "",
            "        def parse_vars_from_line(*line):",
            "            print(line)",
            "            args = parser.parse_args(line)",
            "            parser.common_options.resolve(args)",
            "            return vars(args)",
            "",
            "        return parse_vars_from_line",
            "",
            "    def test_simple(self, parse_vars_from_line):",
            "        assert parse_vars_from_line('--error') == {",
            "            'append': [],",
            "            'lock_wait': 1,",
            "            'log_level': 'error',",
            "            'progress': False",
            "        }",
            "",
            "        assert parse_vars_from_line('--error', 'subcommand', '--critical') == {",
            "            'append': [],",
            "            'lock_wait': 1,",
            "            'log_level': 'critical',",
            "            'progress': False,",
            "            'append_only': False,",
            "            'func': 1234,",
            "        }",
            "",
            "        with pytest.raises(SystemExit):",
            "            parse_vars_from_line('--append-only', 'subcommand')",
            "",
            "        assert parse_vars_from_line('--append=foo', '--append', 'bar', 'subcommand', '--append', 'baz') == {",
            "            'append': ['foo', 'bar', 'baz'],",
            "            'lock_wait': 1,",
            "            'log_level': 'warning',",
            "            'progress': False,",
            "            'append_only': False,",
            "            'func': 1234,",
            "        }",
            "",
            "    @pytest.mark.parametrize('position', ('before', 'after', 'both'))",
            "    @pytest.mark.parametrize('flag,args_key,args_value', (",
            "        ('-p', 'progress', True),",
            "        ('--lock-wait=3', 'lock_wait', 3),",
            "    ))",
            "    def test_flag_position_independence(self, parse_vars_from_line, position, flag, args_key, args_value):",
            "        line = []",
            "        if position in ('before', 'both'):",
            "            line.append(flag)",
            "        line.append('subcommand')",
            "        if position in ('after', 'both'):",
            "            line.append(flag)",
            "",
            "        result = {",
            "            'append': [],",
            "            'lock_wait': 1,",
            "            'log_level': 'warning',",
            "            'progress': False,",
            "            'append_only': False,",
            "            'func': 1234,",
            "        }",
            "        result[args_key] = args_value",
            "",
            "        assert parse_vars_from_line(*line) == result",
            "",
            "",
            "def test_parse_storage_quota():",
            "    assert parse_storage_quota('50M') == 50 * 1000**2",
            "    with pytest.raises(argparse.ArgumentTypeError):",
            "        parse_storage_quota('5M')",
            "",
            "",
            "def get_all_parsers():",
            "    \"\"\"",
            "    Return dict mapping command to parser.",
            "    \"\"\"",
            "    parser = Archiver(prog='borg').build_parser()",
            "    borgfs_parser = Archiver(prog='borgfs').build_parser()",
            "    parsers = {}",
            "",
            "    def discover_level(prefix, parser, Archiver, extra_choices=None):",
            "        choices = {}",
            "        for action in parser._actions:",
            "            if action.choices is not None and 'SubParsersAction' in str(action.__class__):",
            "                for cmd, parser in action.choices.items():",
            "                    choices[prefix + cmd] = parser",
            "        if extra_choices is not None:",
            "            choices.update(extra_choices)",
            "        if prefix and not choices:",
            "            return",
            "",
            "        for command, parser in sorted(choices.items()):",
            "            discover_level(command + \" \", parser, Archiver)",
            "            parsers[command] = parser",
            "",
            "    discover_level(\"\", parser, Archiver, {'borgfs': borgfs_parser})",
            "    return parsers",
            "",
            "",
            "@pytest.mark.parametrize('command, parser', list(get_all_parsers().items()))",
            "def test_help_formatting(command, parser):",
            "    if isinstance(parser.epilog, RstToTextLazy):",
            "        assert parser.epilog.rst",
            "",
            "",
            "@pytest.mark.parametrize('topic, helptext', list(Archiver.helptext.items()))",
            "def test_help_formatting_helptexts(topic, helptext):",
            "    assert str(rst_to_terminal(helptext))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "ecdsa.der"
        ]
    }
}