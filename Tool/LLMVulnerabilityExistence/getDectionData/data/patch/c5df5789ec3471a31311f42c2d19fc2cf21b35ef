{
    "bleach/sanitizer.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import string"
            },
            "1": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " import six"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+from six.moves.urllib.parse import urlparse"
            },
            "4": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from xml.sax.saxutils import unescape"
            },
            "5": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " import html5lib"
            },
            "7": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " from bleach.utils import alphabetize_attributes, force_unicode"
            },
            "8": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+#: Map of entity name to expanded entity"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+ENTITIES = entities"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " #: Trie of html entity string -> character representation"
            },
            "14": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-ENTITIES_TRIE = Trie(entities)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+ENTITIES_TRIE = Trie(ENTITIES)"
            },
            "16": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " #: List of allowed tags"
            },
            "18": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " ALLOWED_TAGS = ["
            },
            "19": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 83,
                "PatchRowcode": " INVISIBLE_REPLACEMENT_CHAR = '?'"
            },
            "20": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 85,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+def convert_entity(value):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+    \"\"\"Convert an entity (minus the & and ; part) into what it represents"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+    This handles numeric, hex, and text entities."
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+    :arg value: the string (minus the ``&`` and ``;`` part) to convert"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+    :returns: unicode character"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+    \"\"\""
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+    if value[0] == '#':"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+        if value[1] in ('x', 'X'):"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+            return six.unichr(int(value[2:], 16))"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+        return six.unichr(int(value[1:], 10))"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+    return ENTITIES[value]"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+def convert_entities(text):"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+    \"\"\"Converts all found entities in the text"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+    :arg text: the text to convert entities in"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+    :returns: unicode text with converted entities"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+    \"\"\""
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+    if '&' not in text:"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+        return text"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+    new_text = []"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+    for part in next_possible_entity(text):"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+        if not part:"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+            continue"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+        if part.startswith('&'):"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+            entity = match_entity(part)"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+            if entity is not None:"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+                new_text.append(convert_entity(entity))"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+                remainder = part[len(entity) + 2:]"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+                if part:"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+                    new_text.append(remainder)"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+                continue"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+        new_text.append(part)"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+    return u''.join(new_text)"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+"
            },
            "70": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 134,
                "PatchRowcode": " class BleachHTMLTokenizer(HTMLTokenizer):"
            },
            "71": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "     def consumeEntity(self, allowedChar=None, fromAttribute=False):"
            },
            "72": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "         # We don't want to consume and convert entities, so this overrides the"
            },
            "73": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "         # html5lib tokenizer's consumeEntity so that it's now a no-op."
            },
            "74": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "         #"
            },
            "75": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 139,
                "PatchRowcode": "         # However, when that gets called, it's consumed an &, so we put that in"
            },
            "76": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # the steam."
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+        # the stream."
            },
            "78": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "         if fromAttribute:"
            },
            "79": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 142,
                "PatchRowcode": "             self.currentToken['data'][-1][1] += '&'"
            },
            "80": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 143,
                "PatchRowcode": " "
            },
            "81": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": 531,
                "PatchRowcode": "                     new_tokens.append({'type': 'Entity', 'name': entity})"
            },
            "82": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": 532,
                "PatchRowcode": "                     # Length of the entity plus 2--one for & at the beginning"
            },
            "83": {
                "beforePatchRowNumber": 481,
                "afterPatchRowNumber": 533,
                "PatchRowcode": "                     # and and one for ; at the end"
            },
            "84": {
                "beforePatchRowNumber": 482,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    part = part[len(entity) + 2:]"
            },
            "85": {
                "beforePatchRowNumber": 483,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    if part:"
            },
            "86": {
                "beforePatchRowNumber": 484,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        new_tokens.append({'type': 'Characters', 'data': part})"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 534,
                "PatchRowcode": "+                    remainder = part[len(entity) + 2:]"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 535,
                "PatchRowcode": "+                    if remainder:"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 536,
                "PatchRowcode": "+                        new_tokens.append({'type': 'Characters', 'data': remainder})"
            },
            "90": {
                "beforePatchRowNumber": 485,
                "afterPatchRowNumber": 537,
                "PatchRowcode": "                     continue"
            },
            "91": {
                "beforePatchRowNumber": 486,
                "afterPatchRowNumber": 538,
                "PatchRowcode": " "
            },
            "92": {
                "beforePatchRowNumber": 487,
                "afterPatchRowNumber": 539,
                "PatchRowcode": "             new_tokens.append({'type': 'Characters', 'data': part})"
            },
            "93": {
                "beforePatchRowNumber": 488,
                "afterPatchRowNumber": 540,
                "PatchRowcode": " "
            },
            "94": {
                "beforePatchRowNumber": 489,
                "afterPatchRowNumber": 541,
                "PatchRowcode": "         return new_tokens"
            },
            "95": {
                "beforePatchRowNumber": 490,
                "afterPatchRowNumber": 542,
                "PatchRowcode": " "
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 543,
                "PatchRowcode": "+    def sanitize_uri_value(self, value, allowed_protocols):"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 544,
                "PatchRowcode": "+        \"\"\"Checks a uri value to see if it's allowed"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 545,
                "PatchRowcode": "+"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 546,
                "PatchRowcode": "+        :arg value: the uri value to sanitize"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 547,
                "PatchRowcode": "+        :arg allowed_protocols: list of allowed protocols"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 548,
                "PatchRowcode": "+"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 549,
                "PatchRowcode": "+        :returns: allowed value or None"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 550,
                "PatchRowcode": "+"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 551,
                "PatchRowcode": "+        \"\"\""
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 552,
                "PatchRowcode": "+        # NOTE(willkg): This transforms the value into one that's easier to"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 553,
                "PatchRowcode": "+        # match and verify, but shouldn't get returned since it's vastly"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 554,
                "PatchRowcode": "+        # different than the original value."
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 555,
                "PatchRowcode": "+"
            },
            "109": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 556,
                "PatchRowcode": "+        # Convert all character entities in the value"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 557,
                "PatchRowcode": "+        new_value = convert_entities(value)"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 558,
                "PatchRowcode": "+"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 559,
                "PatchRowcode": "+        # Nix backtick, space characters, and control characters"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 560,
                "PatchRowcode": "+        new_value = re.sub("
            },
            "114": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 561,
                "PatchRowcode": "+            \"[`\\000-\\040\\177-\\240\\s]+\","
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 562,
                "PatchRowcode": "+            '',"
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 563,
                "PatchRowcode": "+            new_value"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 564,
                "PatchRowcode": "+        )"
            },
            "118": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 565,
                "PatchRowcode": "+"
            },
            "119": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 566,
                "PatchRowcode": "+        # Remove REPLACEMENT characters"
            },
            "120": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 567,
                "PatchRowcode": "+        new_value = new_value.replace('\\ufffd', '')"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 568,
                "PatchRowcode": "+"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 569,
                "PatchRowcode": "+        # Lowercase it--this breaks the value, but makes it easier to match"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 570,
                "PatchRowcode": "+        # against"
            },
            "124": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 571,
                "PatchRowcode": "+        new_value = new_value.lower()"
            },
            "125": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 572,
                "PatchRowcode": "+"
            },
            "126": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 573,
                "PatchRowcode": "+        # Drop attributes with uri values that have protocols that aren't"
            },
            "127": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 574,
                "PatchRowcode": "+        # allowed"
            },
            "128": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 575,
                "PatchRowcode": "+        parsed = urlparse(new_value)"
            },
            "129": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 576,
                "PatchRowcode": "+        if parsed.scheme:"
            },
            "130": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 577,
                "PatchRowcode": "+            # If urlparse found a scheme, check that"
            },
            "131": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 578,
                "PatchRowcode": "+            if parsed.scheme in allowed_protocols:"
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 579,
                "PatchRowcode": "+                return value"
            },
            "133": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 580,
                "PatchRowcode": "+"
            },
            "134": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 581,
                "PatchRowcode": "+        else:"
            },
            "135": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 582,
                "PatchRowcode": "+            # Allow uris that are just an anchor"
            },
            "136": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 583,
                "PatchRowcode": "+            if new_value.startswith('#'):"
            },
            "137": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 584,
                "PatchRowcode": "+                return value"
            },
            "138": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 585,
                "PatchRowcode": "+"
            },
            "139": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 586,
                "PatchRowcode": "+            # Handle protocols that urlparse doesn't recognize like \"myprotocol\""
            },
            "140": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 587,
                "PatchRowcode": "+            if ':' in new_value and new_value.split(':')[0] in allowed_protocols:"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 588,
                "PatchRowcode": "+                return value"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 589,
                "PatchRowcode": "+"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 590,
                "PatchRowcode": "+            # If there's no protocol/scheme specified, then assume it's \"http\""
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 591,
                "PatchRowcode": "+            # and see if that's allowed"
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 592,
                "PatchRowcode": "+            if 'http' in allowed_protocols:"
            },
            "146": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 593,
                "PatchRowcode": "+                return value"
            },
            "147": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 594,
                "PatchRowcode": "+"
            },
            "148": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 595,
                "PatchRowcode": "+        return None"
            },
            "149": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 596,
                "PatchRowcode": "+"
            },
            "150": {
                "beforePatchRowNumber": 491,
                "afterPatchRowNumber": 597,
                "PatchRowcode": "     def allow_token(self, token):"
            },
            "151": {
                "beforePatchRowNumber": 492,
                "afterPatchRowNumber": 598,
                "PatchRowcode": "         \"\"\"Handles the case where we're allowing the tag\"\"\""
            },
            "152": {
                "beforePatchRowNumber": 493,
                "afterPatchRowNumber": 599,
                "PatchRowcode": "         if 'data' in token:"
            },
            "153": {
                "beforePatchRowNumber": 508,
                "afterPatchRowNumber": 614,
                "PatchRowcode": "                 if not self.attr_filter(token['name'], name, val):"
            },
            "154": {
                "beforePatchRowNumber": 509,
                "afterPatchRowNumber": 615,
                "PatchRowcode": "                     continue"
            },
            "155": {
                "beforePatchRowNumber": 510,
                "afterPatchRowNumber": 616,
                "PatchRowcode": " "
            },
            "156": {
                "beforePatchRowNumber": 511,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # Look at attributes that have uri values"
            },
            "157": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 617,
                "PatchRowcode": "+                # Drop attributes with uri values that use a disallowed protocol"
            },
            "158": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 618,
                "PatchRowcode": "+                # Sanitize attributes with uri values"
            },
            "159": {
                "beforePatchRowNumber": 512,
                "afterPatchRowNumber": 619,
                "PatchRowcode": "                 if namespaced_name in self.attr_val_is_uri:"
            },
            "160": {
                "beforePatchRowNumber": 513,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    val_unescaped = re.sub("
            },
            "161": {
                "beforePatchRowNumber": 514,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        \"[`\\000-\\040\\177-\\240\\s]+\","
            },
            "162": {
                "beforePatchRowNumber": 515,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        '',"
            },
            "163": {
                "beforePatchRowNumber": 516,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        unescape(val)).lower()"
            },
            "164": {
                "beforePatchRowNumber": 517,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "165": {
                "beforePatchRowNumber": 518,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    # Remove replacement characters from unescaped characters."
            },
            "166": {
                "beforePatchRowNumber": 519,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")"
            },
            "167": {
                "beforePatchRowNumber": 520,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "168": {
                "beforePatchRowNumber": 521,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    # Drop attributes with uri values that have protocols that"
            },
            "169": {
                "beforePatchRowNumber": 522,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    # aren't allowed"
            },
            "170": {
                "beforePatchRowNumber": 523,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    if (re.match(r'^[a-z0-9][-+.a-z0-9]*:', val_unescaped) and"
            },
            "171": {
                "beforePatchRowNumber": 524,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            (val_unescaped.split(':')[0] not in self.allowed_protocols)):"
            },
            "172": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 620,
                "PatchRowcode": "+                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)"
            },
            "173": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 621,
                "PatchRowcode": "+                    if new_value is None:"
            },
            "174": {
                "beforePatchRowNumber": 525,
                "afterPatchRowNumber": 622,
                "PatchRowcode": "                         continue"
            },
            "175": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 623,
                "PatchRowcode": "+                    val = new_value"
            },
            "176": {
                "beforePatchRowNumber": 526,
                "afterPatchRowNumber": 624,
                "PatchRowcode": " "
            },
            "177": {
                "beforePatchRowNumber": 527,
                "afterPatchRowNumber": 625,
                "PatchRowcode": "                 # Drop values in svg attrs with non-local IRIs"
            },
            "178": {
                "beforePatchRowNumber": 528,
                "afterPatchRowNumber": 626,
                "PatchRowcode": "                 if namespaced_name in self.svg_attr_val_allows_ref:"
            }
        },
        "frontPatchFile": [
            "from __future__ import unicode_literals",
            "from itertools import chain",
            "import re",
            "import string",
            "",
            "import six",
            "from xml.sax.saxutils import unescape",
            "",
            "import html5lib",
            "from html5lib.constants import (",
            "    entities,",
            "    namespaces,",
            "    prefixes,",
            "    tokenTypes,",
            ")",
            "try:",
            "    from html5lib.constants import ReparseException",
            "except ImportError:",
            "    # html5lib-python 1.0 changed the name",
            "    from html5lib.constants import _ReparseException as ReparseException",
            "from html5lib.filters.base import Filter",
            "from html5lib.filters import sanitizer",
            "from html5lib.serializer import HTMLSerializer",
            "from html5lib._tokenizer import HTMLTokenizer",
            "from html5lib._trie import Trie",
            "",
            "from bleach.utils import alphabetize_attributes, force_unicode",
            "",
            "",
            "#: Trie of html entity string -> character representation",
            "ENTITIES_TRIE = Trie(entities)",
            "",
            "#: List of allowed tags",
            "ALLOWED_TAGS = [",
            "    'a',",
            "    'abbr',",
            "    'acronym',",
            "    'b',",
            "    'blockquote',",
            "    'code',",
            "    'em',",
            "    'i',",
            "    'li',",
            "    'ol',",
            "    'strong',",
            "    'ul',",
            "]",
            "",
            "",
            "#: Map of allowed attributes by tag",
            "ALLOWED_ATTRIBUTES = {",
            "    'a': ['href', 'title'],",
            "    'abbr': ['title'],",
            "    'acronym': ['title'],",
            "}",
            "",
            "",
            "#: List of allowed styles",
            "ALLOWED_STYLES = []",
            "",
            "",
            "#: List of allowed protocols",
            "ALLOWED_PROTOCOLS = ['http', 'https', 'mailto']",
            "",
            "",
            "AMP_SPLIT_RE = re.compile('(&)')",
            "",
            "#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)",
            "INVISIBLE_CHARACTERS = ''.join([chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))])",
            "",
            "#: Regexp for characters that are invisible",
            "INVISIBLE_CHARACTERS_RE = re.compile(",
            "    '[' + INVISIBLE_CHARACTERS + ']',",
            "    re.UNICODE",
            ")",
            "",
            "#: String to replace invisible characters with. This can be a character, a",
            "#: string, or even a function that takes a Python re matchobj",
            "INVISIBLE_REPLACEMENT_CHAR = '?'",
            "",
            "",
            "class BleachHTMLTokenizer(HTMLTokenizer):",
            "    def consumeEntity(self, allowedChar=None, fromAttribute=False):",
            "        # We don't want to consume and convert entities, so this overrides the",
            "        # html5lib tokenizer's consumeEntity so that it's now a no-op.",
            "        #",
            "        # However, when that gets called, it's consumed an &, so we put that in",
            "        # the steam.",
            "        if fromAttribute:",
            "            self.currentToken['data'][-1][1] += '&'",
            "",
            "        else:",
            "            self.tokenQueue.append({\"type\": tokenTypes['Characters'], \"data\": '&'})",
            "",
            "",
            "class BleachHTMLParser(html5lib.HTMLParser):",
            "    def _parse(self, stream, innerHTML=False, container=\"div\", scripting=False, **kwargs):",
            "        # Override HTMLParser so we can swap out the tokenizer for our own.",
            "        self.innerHTMLMode = innerHTML",
            "        self.container = container",
            "        self.scripting = scripting",
            "        self.tokenizer = BleachHTMLTokenizer(stream, parser=self, **kwargs)",
            "        self.reset()",
            "",
            "        try:",
            "            self.mainLoop()",
            "        except ReparseException:",
            "            self.reset()",
            "            self.mainLoop()",
            "",
            "",
            "class Cleaner(object):",
            "    \"\"\"Cleaner for cleaning HTML fragments of malicious content",
            "",
            "    This cleaner is a security-focused function whose sole purpose is to remove",
            "    malicious content from a string such that it can be displayed as content in",
            "    a web page.",
            "",
            "    This cleaner is not designed to use to transform content to be used in",
            "    non-web-page contexts.",
            "",
            "    To use::",
            "",
            "        from bleach.sanitizer import Cleaner",
            "",
            "        cleaner = Cleaner()",
            "",
            "        for text in all_the_yucky_things:",
            "            sanitized = cleaner.clean(text)",
            "",
            "    \"\"\"",
            "",
            "    def __init__(self, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES,",
            "                 styles=ALLOWED_STYLES, protocols=ALLOWED_PROTOCOLS, strip=False,",
            "                 strip_comments=True, filters=None):",
            "        \"\"\"Initializes a Cleaner",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip: whether or not to strip disallowed elements",
            "",
            "        :arg bool strip_comments: whether or not to strip HTML comments",
            "",
            "        :arg list filters: list of html5lib Filter classes to pass streamed content through",
            "",
            "            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters",
            "",
            "            .. Warning::",
            "",
            "               Using filters changes the output of ``bleach.Cleaner.clean``.",
            "               Make sure the way the filters change the output are secure.",
            "",
            "        \"\"\"",
            "        self.tags = tags",
            "        self.attributes = attributes",
            "        self.styles = styles",
            "        self.protocols = protocols",
            "        self.strip = strip",
            "        self.strip_comments = strip_comments",
            "        self.filters = filters or []",
            "",
            "        self.parser = BleachHTMLParser(namespaceHTMLElements=False)",
            "        self.walker = html5lib.getTreeWalker('etree')",
            "        self.serializer = BleachHTMLSerializer(",
            "            quote_attr_values='always',",
            "            omit_optional_tags=False,",
            "            escape_lt_in_attrs=True,",
            "",
            "            # We want to leave entities as they are without escaping or",
            "            # resolving or expanding",
            "            resolve_entities=False,",
            "",
            "            # Bleach has its own sanitizer, so don't use the html5lib one",
            "            sanitize=False,",
            "",
            "            # Bleach sanitizer alphabetizes already, so don't use the html5lib one",
            "            alphabetical_attributes=False,",
            "        )",
            "",
            "    def clean(self, text):",
            "        \"\"\"Cleans text and returns sanitized result as unicode",
            "",
            "        :arg str text: text to be cleaned",
            "",
            "        :returns: sanitized text as unicode",
            "",
            "        :raises TypeError: if ``text`` is not a text type",
            "",
            "        \"\"\"",
            "        if not isinstance(text, six.string_types):",
            "            message = \"argument cannot be of '{name}' type, must be of text type\".format(",
            "                name=text.__class__.__name__)",
            "            raise TypeError(message)",
            "",
            "        if not text:",
            "            return u''",
            "",
            "        text = force_unicode(text)",
            "",
            "        dom = self.parser.parseFragment(text)",
            "        filtered = BleachSanitizerFilter(",
            "            source=self.walker(dom),",
            "",
            "            # Bleach-sanitizer-specific things",
            "            attributes=self.attributes,",
            "            strip_disallowed_elements=self.strip,",
            "            strip_html_comments=self.strip_comments,",
            "",
            "            # html5lib-sanitizer things",
            "            allowed_elements=self.tags,",
            "            allowed_css_properties=self.styles,",
            "            allowed_protocols=self.protocols,",
            "            allowed_svg_properties=[],",
            "        )",
            "",
            "        # Apply any filters after the BleachSanitizerFilter",
            "        for filter_class in self.filters:",
            "            filtered = filter_class(source=filtered)",
            "",
            "        return self.serializer.render(filtered)",
            "",
            "",
            "def attribute_filter_factory(attributes):",
            "    \"\"\"Generates attribute filter function for the given attributes value",
            "",
            "    The attributes value can take one of several shapes. This returns a filter",
            "    function appropriate to the attributes value. One nice thing about this is",
            "    that there's less if/then shenanigans in the ``allow_token`` method.",
            "",
            "    \"\"\"",
            "    if callable(attributes):",
            "        return attributes",
            "",
            "    if isinstance(attributes, dict):",
            "        def _attr_filter(tag, attr, value):",
            "            if tag in attributes:",
            "                attr_val = attributes[tag]",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                if attr in attr_val:",
            "                    return True",
            "",
            "            if '*' in attributes:",
            "                attr_val = attributes['*']",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                return attr in attr_val",
            "",
            "            return False",
            "",
            "        return _attr_filter",
            "",
            "    if isinstance(attributes, list):",
            "        def _attr_filter(tag, attr, value):",
            "            return attr in attributes",
            "",
            "        return _attr_filter",
            "",
            "    raise ValueError('attributes needs to be a callable, a list or a dict')",
            "",
            "",
            "def match_entity(stream):",
            "    \"\"\"Returns first entity in stream or None if no entity exists",
            "",
            "    Note: For Bleach purposes, entities must start with a \"&\" and end with",
            "    a \";\".",
            "",
            "    :arg stream: the character stream",
            "",
            "    :returns: ``None`` or the entity string without \"&\" or \";\"",
            "",
            "    \"\"\"",
            "    # Nix the & at the beginning",
            "    if stream[0] != '&':",
            "        raise ValueError('Stream should begin with \"&\"')",
            "",
            "    stream = stream[1:]",
            "",
            "    stream = list(stream)",
            "    possible_entity = ''",
            "    end_characters = '<&=;' + string.whitespace",
            "",
            "    # Handle number entities",
            "    if stream and stream[0] == '#':",
            "        possible_entity = '#'",
            "        stream.pop(0)",
            "",
            "        if stream and stream[0] in ('x', 'X'):",
            "            allowed = '0123456789abcdefABCDEF'",
            "            possible_entity += stream.pop(0)",
            "        else:",
            "            allowed = '0123456789'",
            "",
            "        # FIXME(willkg): Do we want to make sure these are valid number",
            "        # entities? This doesn't do that currently.",
            "        while stream and stream[0] not in end_characters:",
            "            c = stream.pop(0)",
            "            if c not in allowed:",
            "                break",
            "            possible_entity += c",
            "",
            "        if possible_entity and stream and stream[0] == ';':",
            "            return possible_entity",
            "        return None",
            "",
            "    # Handle character entities",
            "    while stream and stream[0] not in end_characters:",
            "        c = stream.pop(0)",
            "        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):",
            "            break",
            "        possible_entity += c",
            "",
            "    if possible_entity and stream and stream[0] == ';':",
            "        return possible_entity",
            "",
            "    return None",
            "",
            "",
            "def next_possible_entity(text):",
            "    \"\"\"Takes a text and generates a list of possible entities",
            "",
            "    :arg text: the text to look at",
            "",
            "    :returns: generator where each part (except the first) starts with an",
            "        \"&\"",
            "",
            "    \"\"\"",
            "    for i, part in enumerate(AMP_SPLIT_RE.split(text)):",
            "        if i == 0:",
            "            yield part",
            "        elif i % 2 == 0:",
            "            yield '&' + part",
            "",
            "",
            "class BleachSanitizerFilter(sanitizer.Filter):",
            "    \"\"\"html5lib Filter that sanitizes text",
            "",
            "    This filter can be used anywhere html5lib filters can be used.",
            "",
            "    \"\"\"",
            "    def __init__(self, source, attributes=ALLOWED_ATTRIBUTES,",
            "                 strip_disallowed_elements=False, strip_html_comments=True,",
            "                 **kwargs):",
            "        \"\"\"Creates a BleachSanitizerFilter instance",
            "",
            "        :arg Treewalker source: stream",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip_disallowed_elements: whether or not to strip disallowed",
            "            elements",
            "",
            "        :arg bool strip_html_comments: whether or not to strip HTML comments",
            "",
            "        \"\"\"",
            "        self.attr_filter = attribute_filter_factory(attributes)",
            "",
            "        self.strip_disallowed_elements = strip_disallowed_elements",
            "        self.strip_html_comments = strip_html_comments",
            "",
            "        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)",
            "",
            "    def __iter__(self):",
            "        for token in Filter.__iter__(self):",
            "            ret = self.sanitize_token(token)",
            "",
            "            if not ret:",
            "                continue",
            "",
            "            if isinstance(ret, list):",
            "                for subtoken in ret:",
            "                    yield subtoken",
            "            else:",
            "                yield ret",
            "",
            "    def sanitize_token(self, token):",
            "        \"\"\"Sanitize a token either by HTML-encoding or dropping.",
            "",
            "        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':",
            "        ['attribute', 'pairs'], 'tag': callable}.",
            "",
            "        Here callable is a function with two arguments of attribute name and",
            "        value. It should return true of false.",
            "",
            "        Also gives the option to strip tags instead of encoding.",
            "",
            "        :arg dict token: token to sanitize",
            "",
            "        :returns: token or list of tokens",
            "",
            "        \"\"\"",
            "        token_type = token['type']",
            "        if token_type in ['StartTag', 'EndTag', 'EmptyTag']:",
            "            if token['name'] in self.allowed_elements:",
            "                return self.allow_token(token)",
            "",
            "            elif self.strip_disallowed_elements:",
            "                return None",
            "",
            "            else:",
            "                if 'data' in token:",
            "                    # Alphabetize the attributes before calling .disallowed_token()",
            "                    # so that the resulting string is stable",
            "                    token['data'] = alphabetize_attributes(token['data'])",
            "                return self.disallowed_token(token)",
            "",
            "        elif token_type == 'Comment':",
            "            if not self.strip_html_comments:",
            "                return token",
            "            else:",
            "                return None",
            "",
            "        elif token_type == 'Characters':",
            "            return self.sanitize_characters(token)",
            "",
            "        else:",
            "            return token",
            "",
            "    def sanitize_characters(self, token):",
            "        \"\"\"Handles Characters tokens",
            "",
            "        Our overridden tokenizer doesn't do anything with entities. However,",
            "        that means that the serializer will convert all ``&`` in Characters",
            "        tokens to ``&amp;``.",
            "",
            "        Since we don't want that, we extract entities here and convert them to",
            "        Entity tokens so the serializer will let them be.",
            "",
            "        :arg token: the Characters token to work on",
            "",
            "        :returns: a list of tokens",
            "",
            "        \"\"\"",
            "        data = token.get('data', '')",
            "",
            "        if not data:",
            "            return token",
            "",
            "        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)",
            "        token['data'] = data",
            "",
            "        # If there isn't a & in the data, we can return now",
            "        if '&' not in data:",
            "            return token",
            "",
            "        new_tokens = []",
            "",
            "        # For each possible entity that starts with a \"&\", we try to extract an",
            "        # actual entity and re-tokenize accordingly",
            "        for part in next_possible_entity(data):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith('&'):",
            "                entity = match_entity(part)",
            "                if entity is not None:",
            "                    new_tokens.append({'type': 'Entity', 'name': entity})",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and and one for ; at the end",
            "                    part = part[len(entity) + 2:]",
            "                    if part:",
            "                        new_tokens.append({'type': 'Characters', 'data': part})",
            "                    continue",
            "",
            "            new_tokens.append({'type': 'Characters', 'data': part})",
            "",
            "        return new_tokens",
            "",
            "    def allow_token(self, token):",
            "        \"\"\"Handles the case where we're allowing the tag\"\"\"",
            "        if 'data' in token:",
            "            # Loop through all the attributes and drop the ones that are not",
            "            # allowed, are unsafe or break other rules. Additionally, fix",
            "            # attribute values that need fixing.",
            "            #",
            "            # At the end of this loop, we have the final set of attributes",
            "            # we're keeping.",
            "            attrs = {}",
            "            for namespaced_name, val in token['data'].items():",
            "                namespace, name = namespaced_name",
            "",
            "                # Drop attributes that are not explicitly allowed",
            "                #",
            "                # NOTE(willkg): We pass in the attribute name--not a namespaced",
            "                # name.",
            "                if not self.attr_filter(token['name'], name, val):",
            "                    continue",
            "",
            "                # Look at attributes that have uri values",
            "                if namespaced_name in self.attr_val_is_uri:",
            "                    val_unescaped = re.sub(",
            "                        \"[`\\000-\\040\\177-\\240\\s]+\",",
            "                        '',",
            "                        unescape(val)).lower()",
            "",
            "                    # Remove replacement characters from unescaped characters.",
            "                    val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")",
            "",
            "                    # Drop attributes with uri values that have protocols that",
            "                    # aren't allowed",
            "                    if (re.match(r'^[a-z0-9][-+.a-z0-9]*:', val_unescaped) and",
            "                            (val_unescaped.split(':')[0] not in self.allowed_protocols)):",
            "                        continue",
            "",
            "                # Drop values in svg attrs with non-local IRIs",
            "                if namespaced_name in self.svg_attr_val_allows_ref:",
            "                    new_val = re.sub(r'url\\s*\\(\\s*[^#\\s][^)]+?\\)',",
            "                                     ' ',",
            "                                     unescape(val))",
            "                    new_val = new_val.strip()",
            "                    if not new_val:",
            "                        continue",
            "",
            "                    else:",
            "                        # Replace the val with the unescaped version because",
            "                        # it's a iri",
            "                        val = new_val",
            "",
            "                # Drop href and xlink:href attr for svg elements with non-local IRIs",
            "                if (None, token['name']) in self.svg_allow_local_href:",
            "                    if namespaced_name in [(None, 'href'), (namespaces['xlink'], 'href')]:",
            "                        if re.search(r'^\\s*[^#\\s]', val):",
            "                            continue",
            "",
            "                # If it's a style attribute, sanitize it",
            "                if namespaced_name == (None, u'style'):",
            "                    val = self.sanitize_css(val)",
            "",
            "                # At this point, we want to keep the attribute, so add it in",
            "                attrs[namespaced_name] = val",
            "",
            "            token['data'] = alphabetize_attributes(attrs)",
            "",
            "        return token",
            "",
            "    def disallowed_token(self, token):",
            "        token_type = token[\"type\"]",
            "        if token_type == \"EndTag\":",
            "            token[\"data\"] = \"</%s>\" % token[\"name\"]",
            "",
            "        elif token[\"data\"]:",
            "            assert token_type in (\"StartTag\", \"EmptyTag\")",
            "            attrs = []",
            "            for (ns, name), v in token[\"data\"].items():",
            "                attrs.append(' %s=\"%s\"' % (",
            "                    name if ns is None else \"%s:%s\" % (prefixes[ns], name),",
            "                    # NOTE(willkg): HTMLSerializer escapes attribute values",
            "                    # already, so if we do it here (like HTMLSerializer does),",
            "                    # then we end up double-escaping.",
            "                    v)",
            "                )",
            "            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], ''.join(attrs))",
            "",
            "        else:",
            "            token[\"data\"] = \"<%s>\" % token[\"name\"]",
            "",
            "        if token.get(\"selfClosing\"):",
            "            token[\"data\"] = token[\"data\"][:-1] + \"/>\"",
            "",
            "        token[\"type\"] = \"Characters\"",
            "",
            "        del token[\"name\"]",
            "        return token",
            "",
            "    def sanitize_css(self, style):",
            "        \"\"\"Sanitizes css in style tags\"\"\"",
            "        # disallow urls",
            "        style = re.compile('url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*').sub(' ', style)",
            "",
            "        # gauntlet",
            "",
            "        # Validate the css in the style tag and if it's not valid, then drop",
            "        # the whole thing.",
            "        parts = style.split(';')",
            "        gauntlet = re.compile(",
            "            r\"\"\"^([-/:,#%.'\"\\sa-zA-Z0-9!]|\\w-\\w|'[\\s\\w]+'\\s*|\"[\\s\\w]+\"|\\([\\d,%\\.\\s]+\\))*$\"\"\"",
            "        )",
            "",
            "        for part in parts:",
            "            if not gauntlet.match(part):",
            "                return ''",
            "",
            "        if not re.match(\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):",
            "            return ''",
            "",
            "        clean = []",
            "        for prop, value in re.findall('([-\\w]+)\\s*:\\s*([^:;]*)', style):",
            "            if not value:",
            "                continue",
            "",
            "            if prop.lower() in self.allowed_css_properties:",
            "                clean.append(prop + ': ' + value + ';')",
            "",
            "            elif prop.lower() in self.allowed_svg_properties:",
            "                clean.append(prop + ': ' + value + ';')",
            "",
            "        return ' '.join(clean)",
            "",
            "",
            "class BleachHTMLSerializer(HTMLSerializer):",
            "    \"\"\"Wraps the HTMLSerializer and undoes & -> &amp; in attributes\"\"\"",
            "    def escape_base_amp(self, stoken):",
            "        \"\"\"Escapes bare & in HTML attribute values\"\"\"",
            "        # First, undo what the HTMLSerializer did",
            "        stoken = stoken.replace('&amp;', '&')",
            "",
            "        # Then, escape any bare &",
            "        for part in next_possible_entity(stoken):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith('&'):",
            "                entity = match_entity(part)",
            "                if entity is not None:",
            "                    yield '&' + entity + ';'",
            "",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and and one for ; at the end",
            "                    part = part[len(entity) + 2:]",
            "                    if part:",
            "                        yield part",
            "                    continue",
            "",
            "            yield part.replace('&', '&amp;')",
            "",
            "    def serialize(self, treewalker, encoding=None):",
            "        \"\"\"Wrap HTMLSerializer.serialize and escape bare & in attributes\"\"\"",
            "        in_tag = False",
            "        after_equals = False",
            "",
            "        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):",
            "            if in_tag:",
            "                if stoken == '>':",
            "                    in_tag = False",
            "",
            "                elif after_equals:",
            "                    if stoken != '\"':",
            "                        for part in self.escape_base_amp(stoken):",
            "                            yield part",
            "",
            "                        after_equals = False",
            "                        continue",
            "",
            "                elif stoken == '=':",
            "                    after_equals = True",
            "",
            "                yield stoken",
            "            else:",
            "                if stoken.startswith('<'):",
            "                    in_tag = True",
            "                yield stoken"
        ],
        "afterPatchFile": [
            "from __future__ import unicode_literals",
            "from itertools import chain",
            "import re",
            "import string",
            "",
            "import six",
            "from six.moves.urllib.parse import urlparse",
            "from xml.sax.saxutils import unescape",
            "",
            "import html5lib",
            "from html5lib.constants import (",
            "    entities,",
            "    namespaces,",
            "    prefixes,",
            "    tokenTypes,",
            ")",
            "try:",
            "    from html5lib.constants import ReparseException",
            "except ImportError:",
            "    # html5lib-python 1.0 changed the name",
            "    from html5lib.constants import _ReparseException as ReparseException",
            "from html5lib.filters.base import Filter",
            "from html5lib.filters import sanitizer",
            "from html5lib.serializer import HTMLSerializer",
            "from html5lib._tokenizer import HTMLTokenizer",
            "from html5lib._trie import Trie",
            "",
            "from bleach.utils import alphabetize_attributes, force_unicode",
            "",
            "",
            "#: Map of entity name to expanded entity",
            "ENTITIES = entities",
            "",
            "#: Trie of html entity string -> character representation",
            "ENTITIES_TRIE = Trie(ENTITIES)",
            "",
            "#: List of allowed tags",
            "ALLOWED_TAGS = [",
            "    'a',",
            "    'abbr',",
            "    'acronym',",
            "    'b',",
            "    'blockquote',",
            "    'code',",
            "    'em',",
            "    'i',",
            "    'li',",
            "    'ol',",
            "    'strong',",
            "    'ul',",
            "]",
            "",
            "",
            "#: Map of allowed attributes by tag",
            "ALLOWED_ATTRIBUTES = {",
            "    'a': ['href', 'title'],",
            "    'abbr': ['title'],",
            "    'acronym': ['title'],",
            "}",
            "",
            "",
            "#: List of allowed styles",
            "ALLOWED_STYLES = []",
            "",
            "",
            "#: List of allowed protocols",
            "ALLOWED_PROTOCOLS = ['http', 'https', 'mailto']",
            "",
            "",
            "AMP_SPLIT_RE = re.compile('(&)')",
            "",
            "#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)",
            "INVISIBLE_CHARACTERS = ''.join([chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))])",
            "",
            "#: Regexp for characters that are invisible",
            "INVISIBLE_CHARACTERS_RE = re.compile(",
            "    '[' + INVISIBLE_CHARACTERS + ']',",
            "    re.UNICODE",
            ")",
            "",
            "#: String to replace invisible characters with. This can be a character, a",
            "#: string, or even a function that takes a Python re matchobj",
            "INVISIBLE_REPLACEMENT_CHAR = '?'",
            "",
            "",
            "def convert_entity(value):",
            "    \"\"\"Convert an entity (minus the & and ; part) into what it represents",
            "",
            "    This handles numeric, hex, and text entities.",
            "",
            "    :arg value: the string (minus the ``&`` and ``;`` part) to convert",
            "",
            "    :returns: unicode character",
            "",
            "    \"\"\"",
            "    if value[0] == '#':",
            "        if value[1] in ('x', 'X'):",
            "            return six.unichr(int(value[2:], 16))",
            "        return six.unichr(int(value[1:], 10))",
            "",
            "    return ENTITIES[value]",
            "",
            "",
            "def convert_entities(text):",
            "    \"\"\"Converts all found entities in the text",
            "",
            "    :arg text: the text to convert entities in",
            "",
            "    :returns: unicode text with converted entities",
            "",
            "    \"\"\"",
            "    if '&' not in text:",
            "        return text",
            "",
            "    new_text = []",
            "    for part in next_possible_entity(text):",
            "        if not part:",
            "            continue",
            "",
            "        if part.startswith('&'):",
            "            entity = match_entity(part)",
            "            if entity is not None:",
            "                new_text.append(convert_entity(entity))",
            "                remainder = part[len(entity) + 2:]",
            "                if part:",
            "                    new_text.append(remainder)",
            "                continue",
            "",
            "        new_text.append(part)",
            "",
            "    return u''.join(new_text)",
            "",
            "",
            "class BleachHTMLTokenizer(HTMLTokenizer):",
            "    def consumeEntity(self, allowedChar=None, fromAttribute=False):",
            "        # We don't want to consume and convert entities, so this overrides the",
            "        # html5lib tokenizer's consumeEntity so that it's now a no-op.",
            "        #",
            "        # However, when that gets called, it's consumed an &, so we put that in",
            "        # the stream.",
            "        if fromAttribute:",
            "            self.currentToken['data'][-1][1] += '&'",
            "",
            "        else:",
            "            self.tokenQueue.append({\"type\": tokenTypes['Characters'], \"data\": '&'})",
            "",
            "",
            "class BleachHTMLParser(html5lib.HTMLParser):",
            "    def _parse(self, stream, innerHTML=False, container=\"div\", scripting=False, **kwargs):",
            "        # Override HTMLParser so we can swap out the tokenizer for our own.",
            "        self.innerHTMLMode = innerHTML",
            "        self.container = container",
            "        self.scripting = scripting",
            "        self.tokenizer = BleachHTMLTokenizer(stream, parser=self, **kwargs)",
            "        self.reset()",
            "",
            "        try:",
            "            self.mainLoop()",
            "        except ReparseException:",
            "            self.reset()",
            "            self.mainLoop()",
            "",
            "",
            "class Cleaner(object):",
            "    \"\"\"Cleaner for cleaning HTML fragments of malicious content",
            "",
            "    This cleaner is a security-focused function whose sole purpose is to remove",
            "    malicious content from a string such that it can be displayed as content in",
            "    a web page.",
            "",
            "    This cleaner is not designed to use to transform content to be used in",
            "    non-web-page contexts.",
            "",
            "    To use::",
            "",
            "        from bleach.sanitizer import Cleaner",
            "",
            "        cleaner = Cleaner()",
            "",
            "        for text in all_the_yucky_things:",
            "            sanitized = cleaner.clean(text)",
            "",
            "    \"\"\"",
            "",
            "    def __init__(self, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES,",
            "                 styles=ALLOWED_STYLES, protocols=ALLOWED_PROTOCOLS, strip=False,",
            "                 strip_comments=True, filters=None):",
            "        \"\"\"Initializes a Cleaner",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip: whether or not to strip disallowed elements",
            "",
            "        :arg bool strip_comments: whether or not to strip HTML comments",
            "",
            "        :arg list filters: list of html5lib Filter classes to pass streamed content through",
            "",
            "            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters",
            "",
            "            .. Warning::",
            "",
            "               Using filters changes the output of ``bleach.Cleaner.clean``.",
            "               Make sure the way the filters change the output are secure.",
            "",
            "        \"\"\"",
            "        self.tags = tags",
            "        self.attributes = attributes",
            "        self.styles = styles",
            "        self.protocols = protocols",
            "        self.strip = strip",
            "        self.strip_comments = strip_comments",
            "        self.filters = filters or []",
            "",
            "        self.parser = BleachHTMLParser(namespaceHTMLElements=False)",
            "        self.walker = html5lib.getTreeWalker('etree')",
            "        self.serializer = BleachHTMLSerializer(",
            "            quote_attr_values='always',",
            "            omit_optional_tags=False,",
            "            escape_lt_in_attrs=True,",
            "",
            "            # We want to leave entities as they are without escaping or",
            "            # resolving or expanding",
            "            resolve_entities=False,",
            "",
            "            # Bleach has its own sanitizer, so don't use the html5lib one",
            "            sanitize=False,",
            "",
            "            # Bleach sanitizer alphabetizes already, so don't use the html5lib one",
            "            alphabetical_attributes=False,",
            "        )",
            "",
            "    def clean(self, text):",
            "        \"\"\"Cleans text and returns sanitized result as unicode",
            "",
            "        :arg str text: text to be cleaned",
            "",
            "        :returns: sanitized text as unicode",
            "",
            "        :raises TypeError: if ``text`` is not a text type",
            "",
            "        \"\"\"",
            "        if not isinstance(text, six.string_types):",
            "            message = \"argument cannot be of '{name}' type, must be of text type\".format(",
            "                name=text.__class__.__name__)",
            "            raise TypeError(message)",
            "",
            "        if not text:",
            "            return u''",
            "",
            "        text = force_unicode(text)",
            "",
            "        dom = self.parser.parseFragment(text)",
            "        filtered = BleachSanitizerFilter(",
            "            source=self.walker(dom),",
            "",
            "            # Bleach-sanitizer-specific things",
            "            attributes=self.attributes,",
            "            strip_disallowed_elements=self.strip,",
            "            strip_html_comments=self.strip_comments,",
            "",
            "            # html5lib-sanitizer things",
            "            allowed_elements=self.tags,",
            "            allowed_css_properties=self.styles,",
            "            allowed_protocols=self.protocols,",
            "            allowed_svg_properties=[],",
            "        )",
            "",
            "        # Apply any filters after the BleachSanitizerFilter",
            "        for filter_class in self.filters:",
            "            filtered = filter_class(source=filtered)",
            "",
            "        return self.serializer.render(filtered)",
            "",
            "",
            "def attribute_filter_factory(attributes):",
            "    \"\"\"Generates attribute filter function for the given attributes value",
            "",
            "    The attributes value can take one of several shapes. This returns a filter",
            "    function appropriate to the attributes value. One nice thing about this is",
            "    that there's less if/then shenanigans in the ``allow_token`` method.",
            "",
            "    \"\"\"",
            "    if callable(attributes):",
            "        return attributes",
            "",
            "    if isinstance(attributes, dict):",
            "        def _attr_filter(tag, attr, value):",
            "            if tag in attributes:",
            "                attr_val = attributes[tag]",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                if attr in attr_val:",
            "                    return True",
            "",
            "            if '*' in attributes:",
            "                attr_val = attributes['*']",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                return attr in attr_val",
            "",
            "            return False",
            "",
            "        return _attr_filter",
            "",
            "    if isinstance(attributes, list):",
            "        def _attr_filter(tag, attr, value):",
            "            return attr in attributes",
            "",
            "        return _attr_filter",
            "",
            "    raise ValueError('attributes needs to be a callable, a list or a dict')",
            "",
            "",
            "def match_entity(stream):",
            "    \"\"\"Returns first entity in stream or None if no entity exists",
            "",
            "    Note: For Bleach purposes, entities must start with a \"&\" and end with",
            "    a \";\".",
            "",
            "    :arg stream: the character stream",
            "",
            "    :returns: ``None`` or the entity string without \"&\" or \";\"",
            "",
            "    \"\"\"",
            "    # Nix the & at the beginning",
            "    if stream[0] != '&':",
            "        raise ValueError('Stream should begin with \"&\"')",
            "",
            "    stream = stream[1:]",
            "",
            "    stream = list(stream)",
            "    possible_entity = ''",
            "    end_characters = '<&=;' + string.whitespace",
            "",
            "    # Handle number entities",
            "    if stream and stream[0] == '#':",
            "        possible_entity = '#'",
            "        stream.pop(0)",
            "",
            "        if stream and stream[0] in ('x', 'X'):",
            "            allowed = '0123456789abcdefABCDEF'",
            "            possible_entity += stream.pop(0)",
            "        else:",
            "            allowed = '0123456789'",
            "",
            "        # FIXME(willkg): Do we want to make sure these are valid number",
            "        # entities? This doesn't do that currently.",
            "        while stream and stream[0] not in end_characters:",
            "            c = stream.pop(0)",
            "            if c not in allowed:",
            "                break",
            "            possible_entity += c",
            "",
            "        if possible_entity and stream and stream[0] == ';':",
            "            return possible_entity",
            "        return None",
            "",
            "    # Handle character entities",
            "    while stream and stream[0] not in end_characters:",
            "        c = stream.pop(0)",
            "        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):",
            "            break",
            "        possible_entity += c",
            "",
            "    if possible_entity and stream and stream[0] == ';':",
            "        return possible_entity",
            "",
            "    return None",
            "",
            "",
            "def next_possible_entity(text):",
            "    \"\"\"Takes a text and generates a list of possible entities",
            "",
            "    :arg text: the text to look at",
            "",
            "    :returns: generator where each part (except the first) starts with an",
            "        \"&\"",
            "",
            "    \"\"\"",
            "    for i, part in enumerate(AMP_SPLIT_RE.split(text)):",
            "        if i == 0:",
            "            yield part",
            "        elif i % 2 == 0:",
            "            yield '&' + part",
            "",
            "",
            "class BleachSanitizerFilter(sanitizer.Filter):",
            "    \"\"\"html5lib Filter that sanitizes text",
            "",
            "    This filter can be used anywhere html5lib filters can be used.",
            "",
            "    \"\"\"",
            "    def __init__(self, source, attributes=ALLOWED_ATTRIBUTES,",
            "                 strip_disallowed_elements=False, strip_html_comments=True,",
            "                 **kwargs):",
            "        \"\"\"Creates a BleachSanitizerFilter instance",
            "",
            "        :arg Treewalker source: stream",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip_disallowed_elements: whether or not to strip disallowed",
            "            elements",
            "",
            "        :arg bool strip_html_comments: whether or not to strip HTML comments",
            "",
            "        \"\"\"",
            "        self.attr_filter = attribute_filter_factory(attributes)",
            "",
            "        self.strip_disallowed_elements = strip_disallowed_elements",
            "        self.strip_html_comments = strip_html_comments",
            "",
            "        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)",
            "",
            "    def __iter__(self):",
            "        for token in Filter.__iter__(self):",
            "            ret = self.sanitize_token(token)",
            "",
            "            if not ret:",
            "                continue",
            "",
            "            if isinstance(ret, list):",
            "                for subtoken in ret:",
            "                    yield subtoken",
            "            else:",
            "                yield ret",
            "",
            "    def sanitize_token(self, token):",
            "        \"\"\"Sanitize a token either by HTML-encoding or dropping.",
            "",
            "        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':",
            "        ['attribute', 'pairs'], 'tag': callable}.",
            "",
            "        Here callable is a function with two arguments of attribute name and",
            "        value. It should return true of false.",
            "",
            "        Also gives the option to strip tags instead of encoding.",
            "",
            "        :arg dict token: token to sanitize",
            "",
            "        :returns: token or list of tokens",
            "",
            "        \"\"\"",
            "        token_type = token['type']",
            "        if token_type in ['StartTag', 'EndTag', 'EmptyTag']:",
            "            if token['name'] in self.allowed_elements:",
            "                return self.allow_token(token)",
            "",
            "            elif self.strip_disallowed_elements:",
            "                return None",
            "",
            "            else:",
            "                if 'data' in token:",
            "                    # Alphabetize the attributes before calling .disallowed_token()",
            "                    # so that the resulting string is stable",
            "                    token['data'] = alphabetize_attributes(token['data'])",
            "                return self.disallowed_token(token)",
            "",
            "        elif token_type == 'Comment':",
            "            if not self.strip_html_comments:",
            "                return token",
            "            else:",
            "                return None",
            "",
            "        elif token_type == 'Characters':",
            "            return self.sanitize_characters(token)",
            "",
            "        else:",
            "            return token",
            "",
            "    def sanitize_characters(self, token):",
            "        \"\"\"Handles Characters tokens",
            "",
            "        Our overridden tokenizer doesn't do anything with entities. However,",
            "        that means that the serializer will convert all ``&`` in Characters",
            "        tokens to ``&amp;``.",
            "",
            "        Since we don't want that, we extract entities here and convert them to",
            "        Entity tokens so the serializer will let them be.",
            "",
            "        :arg token: the Characters token to work on",
            "",
            "        :returns: a list of tokens",
            "",
            "        \"\"\"",
            "        data = token.get('data', '')",
            "",
            "        if not data:",
            "            return token",
            "",
            "        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)",
            "        token['data'] = data",
            "",
            "        # If there isn't a & in the data, we can return now",
            "        if '&' not in data:",
            "            return token",
            "",
            "        new_tokens = []",
            "",
            "        # For each possible entity that starts with a \"&\", we try to extract an",
            "        # actual entity and re-tokenize accordingly",
            "        for part in next_possible_entity(data):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith('&'):",
            "                entity = match_entity(part)",
            "                if entity is not None:",
            "                    new_tokens.append({'type': 'Entity', 'name': entity})",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and and one for ; at the end",
            "                    remainder = part[len(entity) + 2:]",
            "                    if remainder:",
            "                        new_tokens.append({'type': 'Characters', 'data': remainder})",
            "                    continue",
            "",
            "            new_tokens.append({'type': 'Characters', 'data': part})",
            "",
            "        return new_tokens",
            "",
            "    def sanitize_uri_value(self, value, allowed_protocols):",
            "        \"\"\"Checks a uri value to see if it's allowed",
            "",
            "        :arg value: the uri value to sanitize",
            "        :arg allowed_protocols: list of allowed protocols",
            "",
            "        :returns: allowed value or None",
            "",
            "        \"\"\"",
            "        # NOTE(willkg): This transforms the value into one that's easier to",
            "        # match and verify, but shouldn't get returned since it's vastly",
            "        # different than the original value.",
            "",
            "        # Convert all character entities in the value",
            "        new_value = convert_entities(value)",
            "",
            "        # Nix backtick, space characters, and control characters",
            "        new_value = re.sub(",
            "            \"[`\\000-\\040\\177-\\240\\s]+\",",
            "            '',",
            "            new_value",
            "        )",
            "",
            "        # Remove REPLACEMENT characters",
            "        new_value = new_value.replace('\\ufffd', '')",
            "",
            "        # Lowercase it--this breaks the value, but makes it easier to match",
            "        # against",
            "        new_value = new_value.lower()",
            "",
            "        # Drop attributes with uri values that have protocols that aren't",
            "        # allowed",
            "        parsed = urlparse(new_value)",
            "        if parsed.scheme:",
            "            # If urlparse found a scheme, check that",
            "            if parsed.scheme in allowed_protocols:",
            "                return value",
            "",
            "        else:",
            "            # Allow uris that are just an anchor",
            "            if new_value.startswith('#'):",
            "                return value",
            "",
            "            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"",
            "            if ':' in new_value and new_value.split(':')[0] in allowed_protocols:",
            "                return value",
            "",
            "            # If there's no protocol/scheme specified, then assume it's \"http\"",
            "            # and see if that's allowed",
            "            if 'http' in allowed_protocols:",
            "                return value",
            "",
            "        return None",
            "",
            "    def allow_token(self, token):",
            "        \"\"\"Handles the case where we're allowing the tag\"\"\"",
            "        if 'data' in token:",
            "            # Loop through all the attributes and drop the ones that are not",
            "            # allowed, are unsafe or break other rules. Additionally, fix",
            "            # attribute values that need fixing.",
            "            #",
            "            # At the end of this loop, we have the final set of attributes",
            "            # we're keeping.",
            "            attrs = {}",
            "            for namespaced_name, val in token['data'].items():",
            "                namespace, name = namespaced_name",
            "",
            "                # Drop attributes that are not explicitly allowed",
            "                #",
            "                # NOTE(willkg): We pass in the attribute name--not a namespaced",
            "                # name.",
            "                if not self.attr_filter(token['name'], name, val):",
            "                    continue",
            "",
            "                # Drop attributes with uri values that use a disallowed protocol",
            "                # Sanitize attributes with uri values",
            "                if namespaced_name in self.attr_val_is_uri:",
            "                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)",
            "                    if new_value is None:",
            "                        continue",
            "                    val = new_value",
            "",
            "                # Drop values in svg attrs with non-local IRIs",
            "                if namespaced_name in self.svg_attr_val_allows_ref:",
            "                    new_val = re.sub(r'url\\s*\\(\\s*[^#\\s][^)]+?\\)',",
            "                                     ' ',",
            "                                     unescape(val))",
            "                    new_val = new_val.strip()",
            "                    if not new_val:",
            "                        continue",
            "",
            "                    else:",
            "                        # Replace the val with the unescaped version because",
            "                        # it's a iri",
            "                        val = new_val",
            "",
            "                # Drop href and xlink:href attr for svg elements with non-local IRIs",
            "                if (None, token['name']) in self.svg_allow_local_href:",
            "                    if namespaced_name in [(None, 'href'), (namespaces['xlink'], 'href')]:",
            "                        if re.search(r'^\\s*[^#\\s]', val):",
            "                            continue",
            "",
            "                # If it's a style attribute, sanitize it",
            "                if namespaced_name == (None, u'style'):",
            "                    val = self.sanitize_css(val)",
            "",
            "                # At this point, we want to keep the attribute, so add it in",
            "                attrs[namespaced_name] = val",
            "",
            "            token['data'] = alphabetize_attributes(attrs)",
            "",
            "        return token",
            "",
            "    def disallowed_token(self, token):",
            "        token_type = token[\"type\"]",
            "        if token_type == \"EndTag\":",
            "            token[\"data\"] = \"</%s>\" % token[\"name\"]",
            "",
            "        elif token[\"data\"]:",
            "            assert token_type in (\"StartTag\", \"EmptyTag\")",
            "            attrs = []",
            "            for (ns, name), v in token[\"data\"].items():",
            "                attrs.append(' %s=\"%s\"' % (",
            "                    name if ns is None else \"%s:%s\" % (prefixes[ns], name),",
            "                    # NOTE(willkg): HTMLSerializer escapes attribute values",
            "                    # already, so if we do it here (like HTMLSerializer does),",
            "                    # then we end up double-escaping.",
            "                    v)",
            "                )",
            "            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], ''.join(attrs))",
            "",
            "        else:",
            "            token[\"data\"] = \"<%s>\" % token[\"name\"]",
            "",
            "        if token.get(\"selfClosing\"):",
            "            token[\"data\"] = token[\"data\"][:-1] + \"/>\"",
            "",
            "        token[\"type\"] = \"Characters\"",
            "",
            "        del token[\"name\"]",
            "        return token",
            "",
            "    def sanitize_css(self, style):",
            "        \"\"\"Sanitizes css in style tags\"\"\"",
            "        # disallow urls",
            "        style = re.compile('url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*').sub(' ', style)",
            "",
            "        # gauntlet",
            "",
            "        # Validate the css in the style tag and if it's not valid, then drop",
            "        # the whole thing.",
            "        parts = style.split(';')",
            "        gauntlet = re.compile(",
            "            r\"\"\"^([-/:,#%.'\"\\sa-zA-Z0-9!]|\\w-\\w|'[\\s\\w]+'\\s*|\"[\\s\\w]+\"|\\([\\d,%\\.\\s]+\\))*$\"\"\"",
            "        )",
            "",
            "        for part in parts:",
            "            if not gauntlet.match(part):",
            "                return ''",
            "",
            "        if not re.match(\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):",
            "            return ''",
            "",
            "        clean = []",
            "        for prop, value in re.findall('([-\\w]+)\\s*:\\s*([^:;]*)', style):",
            "            if not value:",
            "                continue",
            "",
            "            if prop.lower() in self.allowed_css_properties:",
            "                clean.append(prop + ': ' + value + ';')",
            "",
            "            elif prop.lower() in self.allowed_svg_properties:",
            "                clean.append(prop + ': ' + value + ';')",
            "",
            "        return ' '.join(clean)",
            "",
            "",
            "class BleachHTMLSerializer(HTMLSerializer):",
            "    \"\"\"Wraps the HTMLSerializer and undoes & -> &amp; in attributes\"\"\"",
            "    def escape_base_amp(self, stoken):",
            "        \"\"\"Escapes bare & in HTML attribute values\"\"\"",
            "        # First, undo what the HTMLSerializer did",
            "        stoken = stoken.replace('&amp;', '&')",
            "",
            "        # Then, escape any bare &",
            "        for part in next_possible_entity(stoken):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith('&'):",
            "                entity = match_entity(part)",
            "                if entity is not None:",
            "                    yield '&' + entity + ';'",
            "",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and and one for ; at the end",
            "                    part = part[len(entity) + 2:]",
            "                    if part:",
            "                        yield part",
            "                    continue",
            "",
            "            yield part.replace('&', '&amp;')",
            "",
            "    def serialize(self, treewalker, encoding=None):",
            "        \"\"\"Wrap HTMLSerializer.serialize and escape bare & in attributes\"\"\"",
            "        in_tag = False",
            "        after_equals = False",
            "",
            "        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):",
            "            if in_tag:",
            "                if stoken == '>':",
            "                    in_tag = False",
            "",
            "                elif after_equals:",
            "                    if stoken != '\"':",
            "                        for part in self.escape_base_amp(stoken):",
            "                            yield part",
            "",
            "                        after_equals = False",
            "                        continue",
            "",
            "                elif stoken == '=':",
            "                    after_equals = True",
            "",
            "                yield stoken",
            "            else:",
            "                if stoken.startswith('<'):",
            "                    in_tag = True",
            "                yield stoken"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "31": [
                "ENTITIES_TRIE"
            ],
            "88": [
                "BleachHTMLTokenizer",
                "consumeEntity"
            ],
            "482": [
                "BleachSanitizerFilter",
                "sanitize_characters"
            ],
            "483": [
                "BleachSanitizerFilter",
                "sanitize_characters"
            ],
            "484": [
                "BleachSanitizerFilter",
                "sanitize_characters"
            ],
            "511": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "513": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "514": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "515": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "516": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "517": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "518": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "519": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "520": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "521": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "522": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "523": [
                "BleachSanitizerFilter",
                "allow_token"
            ],
            "524": [
                "BleachSanitizerFilter",
                "allow_token"
            ]
        },
        "addLocation": []
    }
}