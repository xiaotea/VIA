{
    "nova/tests/virt/xenapi/stubs.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": 299,
                "PatchRowcode": "         vdi_rec['other_config']['nova_disk_type'] = 'root'"
            },
            "1": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": 300,
                "PatchRowcode": "         return {'uuid': vdi_rec['uuid'], 'ref': vdi_ref}"
            },
            "2": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": 301,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+    def fake_wait_for_instance_to_start(self, *args):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 303,
                "PatchRowcode": "+        pass"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 304,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "     def fake_get_vdi(session, vm_ref):"
            },
            "7": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": 306,
                "PatchRowcode": "         vdi_ref_parent = fake.create_vdi('derp-parent', fakesr)"
            },
            "8": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": 307,
                "PatchRowcode": "         vdi_rec_parent = fake.get_record('VDI', vdi_ref_parent)"
            },
            "9": {
                "beforePatchRowNumber": 320,
                "afterPatchRowNumber": 323,
                "PatchRowcode": "         pass"
            },
            "10": {
                "beforePatchRowNumber": 321,
                "afterPatchRowNumber": 324,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 322,
                "afterPatchRowNumber": 325,
                "PatchRowcode": "     stubs.Set(vmops.VMOps, '_destroy', fake_destroy)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 326,
                "PatchRowcode": "+    stubs.Set(vmops.VMOps, '_wait_for_instance_to_start',"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 327,
                "PatchRowcode": "+              fake_wait_for_instance_to_start)"
            },
            "14": {
                "beforePatchRowNumber": 323,
                "afterPatchRowNumber": 328,
                "PatchRowcode": "     stubs.Set(vm_utils, 'move_disks', fake_move_disks)"
            },
            "15": {
                "beforePatchRowNumber": 324,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "     stubs.Set(vm_utils, 'scan_default_sr', fake_sr)"
            },
            "16": {
                "beforePatchRowNumber": 325,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "     stubs.Set(vm_utils, 'get_vdi_for_vm_safely', fake_get_vdi)"
            }
        },
        "frontPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"Stubouts, mocks and fixtures for the test suite.\"\"\"",
            "",
            "import pickle",
            "import random",
            "",
            "from nova.openstack.common import jsonutils",
            "from nova import test",
            "import nova.tests.image.fake",
            "from nova.virt.xenapi import driver as xenapi_conn",
            "from nova.virt.xenapi import fake",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "",
            "",
            "def stubout_firewall_driver(stubs, conn):",
            "",
            "    def fake_none(self, *args):",
            "        return",
            "",
            "    _vmops = conn._vmops",
            "    stubs.Set(_vmops.firewall_driver, 'prepare_instance_filter', fake_none)",
            "    stubs.Set(_vmops.firewall_driver, 'instance_filter_exists', fake_none)",
            "",
            "",
            "def stubout_instance_snapshot(stubs):",
            "    def fake_fetch_image(context, session, instance, name_label, image, type):",
            "        return {'root': dict(uuid=_make_fake_vdi(), file=None),",
            "                'kernel': dict(uuid=_make_fake_vdi(), file=None),",
            "                'ramdisk': dict(uuid=_make_fake_vdi(), file=None)}",
            "",
            "    stubs.Set(vm_utils, '_fetch_image', fake_fetch_image)",
            "",
            "    def fake_wait_for_vhd_coalesce(*args):",
            "        #TODO(sirp): Should we actually fake out the data here",
            "        return \"fakeparent\", \"fakebase\"",
            "",
            "    stubs.Set(vm_utils, '_wait_for_vhd_coalesce', fake_wait_for_vhd_coalesce)",
            "",
            "",
            "def stubout_session(stubs, cls, product_version=(5, 6, 2),",
            "                    product_brand='XenServer', **opt_args):",
            "    \"\"\"Stubs out methods from XenAPISession.\"\"\"",
            "    stubs.Set(xenapi_conn.XenAPISession, '_create_session',",
            "              lambda s, url: cls(url, **opt_args))",
            "    stubs.Set(xenapi_conn.XenAPISession, '_get_product_version_and_brand',",
            "              lambda s: (product_version, product_brand))",
            "",
            "",
            "def stubout_get_this_vm_uuid(stubs):",
            "    def f(session):",
            "        vms = [rec['uuid'] for ref, rec",
            "               in fake.get_all_records('VM').iteritems()",
            "               if rec['is_control_domain']]",
            "        return vms[0]",
            "    stubs.Set(vm_utils, 'get_this_vm_uuid', f)",
            "",
            "",
            "def stubout_image_service_download(stubs):",
            "    def fake_download(*args, **kwargs):",
            "        pass",
            "    stubs.Set(nova.tests.image.fake._FakeImageService,",
            "        'download', fake_download)",
            "",
            "",
            "def stubout_stream_disk(stubs):",
            "    def fake_stream_disk(*args, **kwargs):",
            "        pass",
            "    stubs.Set(vm_utils, '_stream_disk', fake_stream_disk)",
            "",
            "",
            "def stubout_determine_is_pv_objectstore(stubs):",
            "    \"\"\"Assumes VMs stu have PV kernels.\"\"\"",
            "",
            "    def f(*args):",
            "        return False",
            "    stubs.Set(vm_utils, '_determine_is_pv_objectstore', f)",
            "",
            "",
            "def stubout_is_snapshot(stubs):",
            "    \"\"\"Always returns true",
            "",
            "        xenapi fake driver does not create vmrefs for snapshots.",
            "    \"\"\"",
            "",
            "    def f(*args):",
            "        return True",
            "    stubs.Set(vm_utils, 'is_snapshot', f)",
            "",
            "",
            "def stubout_lookup_image(stubs):",
            "    \"\"\"Simulates a failure in lookup image.\"\"\"",
            "    def f(_1, _2, _3, _4):",
            "        raise Exception(\"Test Exception raised by fake lookup_image\")",
            "    stubs.Set(vm_utils, 'lookup_image', f)",
            "",
            "",
            "def stubout_fetch_disk_image(stubs, raise_failure=False):",
            "    \"\"\"Simulates a failure in fetch image_glance_disk.\"\"\"",
            "",
            "    def _fake_fetch_disk_image(context, session, instance, name_label, image,",
            "                               image_type):",
            "        if raise_failure:",
            "            raise fake.Failure(\"Test Exception raised by \"",
            "                               \"fake fetch_image_glance_disk\")",
            "        elif image_type == vm_utils.ImageType.KERNEL:",
            "            filename = \"kernel\"",
            "        elif image_type == vm_utils.ImageType.RAMDISK:",
            "            filename = \"ramdisk\"",
            "        else:",
            "            filename = \"unknown\"",
            "",
            "        vdi_type = vm_utils.ImageType.to_string(image_type)",
            "        return {vdi_type: dict(uuid=None, file=filename)}",
            "",
            "    stubs.Set(vm_utils, '_fetch_disk_image', _fake_fetch_disk_image)",
            "",
            "",
            "def stubout_create_vm(stubs):",
            "    \"\"\"Simulates a failure in create_vm.\"\"\"",
            "",
            "    def f(*args):",
            "        raise fake.Failure(\"Test Exception raised by fake create_vm\")",
            "    stubs.Set(vm_utils, 'create_vm', f)",
            "",
            "",
            "def stubout_attach_disks(stubs):",
            "    \"\"\"Simulates a failure in _attach_disks.\"\"\"",
            "",
            "    def f(*args):",
            "        raise fake.Failure(\"Test Exception raised by fake _attach_disks\")",
            "    stubs.Set(vmops.VMOps, '_attach_disks', f)",
            "",
            "",
            "def _make_fake_vdi():",
            "    sr_ref = fake.get_all('SR')[0]",
            "    vdi_ref = fake.create_vdi('', sr_ref)",
            "    vdi_rec = fake.get_record('VDI', vdi_ref)",
            "    return vdi_rec['uuid']",
            "",
            "",
            "class FakeSessionForVMTests(fake.SessionBase):",
            "    \"\"\"Stubs out a XenAPISession for VM tests.\"\"\"",
            "",
            "    _fake_iptables_save_output = (\"# Generated by iptables-save v1.4.10 on \"",
            "                                  \"Sun Nov  6 22:49:02 2011\\n\"",
            "                                  \"*filter\\n\"",
            "                                  \":INPUT ACCEPT [0:0]\\n\"",
            "                                  \":FORWARD ACCEPT [0:0]\\n\"",
            "                                  \":OUTPUT ACCEPT [0:0]\\n\"",
            "                                  \"COMMIT\\n\"",
            "                                  \"# Completed on Sun Nov  6 22:49:02 2011\\n\")",
            "",
            "    def host_call_plugin(self, _1, _2, plugin, method, _5):",
            "        if (plugin, method) == ('glance', 'download_vhd'):",
            "            root_uuid = _make_fake_vdi()",
            "            return pickle.dumps(dict(root=dict(uuid=root_uuid)))",
            "        elif (plugin, method) == (\"xenhost\", \"iptables_config\"):",
            "            return fake.as_json(out=self._fake_iptables_save_output,",
            "                                err='')",
            "        else:",
            "            return (super(FakeSessionForVMTests, self).",
            "                    host_call_plugin(_1, _2, plugin, method, _5))",
            "",
            "    def VM_start(self, _1, ref, _2, _3):",
            "        vm = fake.get_record('VM', ref)",
            "        if vm['power_state'] != 'Halted':",
            "            raise fake.Failure(['VM_BAD_POWER_STATE', ref, 'Halted',",
            "                                vm['power_state']])",
            "        vm['power_state'] = 'Running'",
            "        vm['is_a_template'] = False",
            "        vm['is_control_domain'] = False",
            "        vm['domid'] = random.randrange(1, 1 << 16)",
            "        return vm",
            "",
            "    def VM_start_on(self, _1, vm_ref, host_ref, _2, _3):",
            "        vm_rec = self.VM_start(_1, vm_ref, _2, _3)",
            "        vm_rec['resident_on'] = host_ref",
            "",
            "    def VDI_snapshot(self, session_ref, vm_ref, _1):",
            "        sr_ref = \"fakesr\"",
            "        return fake.create_vdi('fakelabel', sr_ref, read_only=True)",
            "",
            "    def SR_scan(self, session_ref, sr_ref):",
            "        pass",
            "",
            "",
            "class FakeSessionForFirewallTests(FakeSessionForVMTests):",
            "    \"\"\"Stubs out a XenApi Session for doing IPTable Firewall tests.\"\"\"",
            "",
            "    def __init__(self, uri, test_case=None):",
            "        super(FakeSessionForFirewallTests, self).__init__(uri)",
            "        if hasattr(test_case, '_in_rules'):",
            "            self._in_rules = test_case._in_rules",
            "        if hasattr(test_case, '_in6_filter_rules'):",
            "            self._in6_filter_rules = test_case._in6_filter_rules",
            "        self._test_case = test_case",
            "",
            "    def host_call_plugin(self, _1, _2, plugin, method, args):",
            "        \"\"\"Mock method four host_call_plugin to be used in unit tests",
            "           for the dom0 iptables Firewall drivers for XenAPI",
            "",
            "        \"\"\"",
            "        if plugin == \"xenhost\" and method == \"iptables_config\":",
            "            # The command to execute is a json-encoded list",
            "            cmd_args = args.get('cmd_args', None)",
            "            cmd = jsonutils.loads(cmd_args)",
            "            if not cmd:",
            "                ret_str = ''",
            "            else:",
            "                output = ''",
            "                process_input = args.get('process_input', None)",
            "                if cmd == ['ip6tables-save', '-c']:",
            "                    output = '\\n'.join(self._in6_filter_rules)",
            "                if cmd == ['iptables-save', '-c']:",
            "                    output = '\\n'.join(self._in_rules)",
            "                if cmd == ['iptables-restore', '-c', ]:",
            "                    lines = process_input.split('\\n')",
            "                    if '*filter' in lines:",
            "                        if self._test_case is not None:",
            "                            self._test_case._out_rules = lines",
            "                        output = '\\n'.join(lines)",
            "                if cmd == ['ip6tables-restore', '-c', ]:",
            "                    lines = process_input.split('\\n')",
            "                    if '*filter' in lines:",
            "                        output = '\\n'.join(lines)",
            "                ret_str = fake.as_json(out=output, err='')",
            "        return ret_str",
            "",
            "",
            "def stub_out_vm_methods(stubs):",
            "    def fake_acquire_bootlock(self, vm):",
            "        pass",
            "",
            "    def fake_release_bootlock(self, vm):",
            "        pass",
            "",
            "    def fake_generate_ephemeral(*args):",
            "        pass",
            "",
            "    def fake_wait_for_device(dev):",
            "        pass",
            "",
            "    stubs.Set(vmops.VMOps, \"_acquire_bootlock\", fake_acquire_bootlock)",
            "    stubs.Set(vmops.VMOps, \"_release_bootlock\", fake_release_bootlock)",
            "    stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "    stubs.Set(vm_utils, '_wait_for_device', fake_wait_for_device)",
            "",
            "",
            "class FakeSessionForVolumeTests(fake.SessionBase):",
            "    \"\"\"Stubs out a XenAPISession for Volume tests.\"\"\"",
            "    def VDI_introduce(self, _1, uuid, _2, _3, _4, _5,",
            "                      _6, _7, _8, _9, _10, _11):",
            "        valid_vdi = False",
            "        refs = fake.get_all('VDI')",
            "        for ref in refs:",
            "            rec = fake.get_record('VDI', ref)",
            "            if rec['uuid'] == uuid:",
            "                valid_vdi = True",
            "        if not valid_vdi:",
            "            raise fake.Failure([['INVALID_VDI', 'session', self._session]])",
            "",
            "",
            "class FakeSessionForVolumeFailedTests(FakeSessionForVolumeTests):",
            "    \"\"\"Stubs out a XenAPISession for Volume tests: it injects failures.\"\"\"",
            "    def VDI_introduce(self, _1, uuid, _2, _3, _4, _5,",
            "                      _6, _7, _8, _9, _10, _11):",
            "        # This is for testing failure",
            "        raise fake.Failure([['INVALID_VDI', 'session', self._session]])",
            "",
            "    def PBD_unplug(self, _1, ref):",
            "        rec = fake.get_record('PBD', ref)",
            "        rec['currently-attached'] = False",
            "",
            "    def SR_forget(self, _1, ref):",
            "        pass",
            "",
            "",
            "def stub_out_migration_methods(stubs):",
            "    fakesr = fake.create_sr()",
            "",
            "    def fake_move_disks(self, instance, disk_info):",
            "        vdi_ref = fake.create_vdi(instance['name'], fakesr)",
            "        vdi_rec = fake.get_record('VDI', vdi_ref)",
            "        vdi_rec['other_config']['nova_disk_type'] = 'root'",
            "        return {'uuid': vdi_rec['uuid'], 'ref': vdi_ref}",
            "",
            "    def fake_get_vdi(session, vm_ref):",
            "        vdi_ref_parent = fake.create_vdi('derp-parent', fakesr)",
            "        vdi_rec_parent = fake.get_record('VDI', vdi_ref_parent)",
            "        vdi_ref = fake.create_vdi('derp', fakesr,",
            "                sm_config={'vhd-parent': vdi_rec_parent['uuid']})",
            "        vdi_rec = session.call_xenapi(\"VDI.get_record\", vdi_ref)",
            "        return vdi_ref, vdi_rec",
            "",
            "    def fake_sr(session, *args):",
            "        return fakesr",
            "",
            "    def fake_get_sr_path(*args):",
            "        return \"fake\"",
            "",
            "    def fake_destroy(*args, **kwargs):",
            "        pass",
            "",
            "    def fake_generate_ephemeral(*args):",
            "        pass",
            "",
            "    stubs.Set(vmops.VMOps, '_destroy', fake_destroy)",
            "    stubs.Set(vm_utils, 'move_disks', fake_move_disks)",
            "    stubs.Set(vm_utils, 'scan_default_sr', fake_sr)",
            "    stubs.Set(vm_utils, 'get_vdi_for_vm_safely', fake_get_vdi)",
            "    stubs.Set(vm_utils, 'get_sr_path', fake_get_sr_path)",
            "    stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "",
            "",
            "class FakeSessionForFailedMigrateTests(FakeSessionForVMTests):",
            "    def VM_assert_can_migrate(self, session, vmref, migrate_data,",
            "                              live, vdi_map, vif_map, options):",
            "        raise fake.Failure(\"XenAPI VM.assert_can_migrate failed\")",
            "",
            "    def host_migrate_receive(self, session, hostref, networkref, options):",
            "        raise fake.Failure(\"XenAPI host.migrate_receive failed\")",
            "",
            "    def VM_migrate_send(self, session, vmref, migrate_data, islive, vdi_map,",
            "                        vif_map, options):",
            "        raise fake.Failure(\"XenAPI VM.migrate_send failed\")",
            "",
            "",
            "class XenAPITestBase(test.TestCase):",
            "    def setUp(self):",
            "        super(XenAPITestBase, self).setUp()",
            "",
            "        self.useFixture(test.ReplaceModule('XenAPI', fake))",
            "",
            "        fake.reset()"
        ],
        "afterPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"Stubouts, mocks and fixtures for the test suite.\"\"\"",
            "",
            "import pickle",
            "import random",
            "",
            "from nova.openstack.common import jsonutils",
            "from nova import test",
            "import nova.tests.image.fake",
            "from nova.virt.xenapi import driver as xenapi_conn",
            "from nova.virt.xenapi import fake",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "",
            "",
            "def stubout_firewall_driver(stubs, conn):",
            "",
            "    def fake_none(self, *args):",
            "        return",
            "",
            "    _vmops = conn._vmops",
            "    stubs.Set(_vmops.firewall_driver, 'prepare_instance_filter', fake_none)",
            "    stubs.Set(_vmops.firewall_driver, 'instance_filter_exists', fake_none)",
            "",
            "",
            "def stubout_instance_snapshot(stubs):",
            "    def fake_fetch_image(context, session, instance, name_label, image, type):",
            "        return {'root': dict(uuid=_make_fake_vdi(), file=None),",
            "                'kernel': dict(uuid=_make_fake_vdi(), file=None),",
            "                'ramdisk': dict(uuid=_make_fake_vdi(), file=None)}",
            "",
            "    stubs.Set(vm_utils, '_fetch_image', fake_fetch_image)",
            "",
            "    def fake_wait_for_vhd_coalesce(*args):",
            "        #TODO(sirp): Should we actually fake out the data here",
            "        return \"fakeparent\", \"fakebase\"",
            "",
            "    stubs.Set(vm_utils, '_wait_for_vhd_coalesce', fake_wait_for_vhd_coalesce)",
            "",
            "",
            "def stubout_session(stubs, cls, product_version=(5, 6, 2),",
            "                    product_brand='XenServer', **opt_args):",
            "    \"\"\"Stubs out methods from XenAPISession.\"\"\"",
            "    stubs.Set(xenapi_conn.XenAPISession, '_create_session',",
            "              lambda s, url: cls(url, **opt_args))",
            "    stubs.Set(xenapi_conn.XenAPISession, '_get_product_version_and_brand',",
            "              lambda s: (product_version, product_brand))",
            "",
            "",
            "def stubout_get_this_vm_uuid(stubs):",
            "    def f(session):",
            "        vms = [rec['uuid'] for ref, rec",
            "               in fake.get_all_records('VM').iteritems()",
            "               if rec['is_control_domain']]",
            "        return vms[0]",
            "    stubs.Set(vm_utils, 'get_this_vm_uuid', f)",
            "",
            "",
            "def stubout_image_service_download(stubs):",
            "    def fake_download(*args, **kwargs):",
            "        pass",
            "    stubs.Set(nova.tests.image.fake._FakeImageService,",
            "        'download', fake_download)",
            "",
            "",
            "def stubout_stream_disk(stubs):",
            "    def fake_stream_disk(*args, **kwargs):",
            "        pass",
            "    stubs.Set(vm_utils, '_stream_disk', fake_stream_disk)",
            "",
            "",
            "def stubout_determine_is_pv_objectstore(stubs):",
            "    \"\"\"Assumes VMs stu have PV kernels.\"\"\"",
            "",
            "    def f(*args):",
            "        return False",
            "    stubs.Set(vm_utils, '_determine_is_pv_objectstore', f)",
            "",
            "",
            "def stubout_is_snapshot(stubs):",
            "    \"\"\"Always returns true",
            "",
            "        xenapi fake driver does not create vmrefs for snapshots.",
            "    \"\"\"",
            "",
            "    def f(*args):",
            "        return True",
            "    stubs.Set(vm_utils, 'is_snapshot', f)",
            "",
            "",
            "def stubout_lookup_image(stubs):",
            "    \"\"\"Simulates a failure in lookup image.\"\"\"",
            "    def f(_1, _2, _3, _4):",
            "        raise Exception(\"Test Exception raised by fake lookup_image\")",
            "    stubs.Set(vm_utils, 'lookup_image', f)",
            "",
            "",
            "def stubout_fetch_disk_image(stubs, raise_failure=False):",
            "    \"\"\"Simulates a failure in fetch image_glance_disk.\"\"\"",
            "",
            "    def _fake_fetch_disk_image(context, session, instance, name_label, image,",
            "                               image_type):",
            "        if raise_failure:",
            "            raise fake.Failure(\"Test Exception raised by \"",
            "                               \"fake fetch_image_glance_disk\")",
            "        elif image_type == vm_utils.ImageType.KERNEL:",
            "            filename = \"kernel\"",
            "        elif image_type == vm_utils.ImageType.RAMDISK:",
            "            filename = \"ramdisk\"",
            "        else:",
            "            filename = \"unknown\"",
            "",
            "        vdi_type = vm_utils.ImageType.to_string(image_type)",
            "        return {vdi_type: dict(uuid=None, file=filename)}",
            "",
            "    stubs.Set(vm_utils, '_fetch_disk_image', _fake_fetch_disk_image)",
            "",
            "",
            "def stubout_create_vm(stubs):",
            "    \"\"\"Simulates a failure in create_vm.\"\"\"",
            "",
            "    def f(*args):",
            "        raise fake.Failure(\"Test Exception raised by fake create_vm\")",
            "    stubs.Set(vm_utils, 'create_vm', f)",
            "",
            "",
            "def stubout_attach_disks(stubs):",
            "    \"\"\"Simulates a failure in _attach_disks.\"\"\"",
            "",
            "    def f(*args):",
            "        raise fake.Failure(\"Test Exception raised by fake _attach_disks\")",
            "    stubs.Set(vmops.VMOps, '_attach_disks', f)",
            "",
            "",
            "def _make_fake_vdi():",
            "    sr_ref = fake.get_all('SR')[0]",
            "    vdi_ref = fake.create_vdi('', sr_ref)",
            "    vdi_rec = fake.get_record('VDI', vdi_ref)",
            "    return vdi_rec['uuid']",
            "",
            "",
            "class FakeSessionForVMTests(fake.SessionBase):",
            "    \"\"\"Stubs out a XenAPISession for VM tests.\"\"\"",
            "",
            "    _fake_iptables_save_output = (\"# Generated by iptables-save v1.4.10 on \"",
            "                                  \"Sun Nov  6 22:49:02 2011\\n\"",
            "                                  \"*filter\\n\"",
            "                                  \":INPUT ACCEPT [0:0]\\n\"",
            "                                  \":FORWARD ACCEPT [0:0]\\n\"",
            "                                  \":OUTPUT ACCEPT [0:0]\\n\"",
            "                                  \"COMMIT\\n\"",
            "                                  \"# Completed on Sun Nov  6 22:49:02 2011\\n\")",
            "",
            "    def host_call_plugin(self, _1, _2, plugin, method, _5):",
            "        if (plugin, method) == ('glance', 'download_vhd'):",
            "            root_uuid = _make_fake_vdi()",
            "            return pickle.dumps(dict(root=dict(uuid=root_uuid)))",
            "        elif (plugin, method) == (\"xenhost\", \"iptables_config\"):",
            "            return fake.as_json(out=self._fake_iptables_save_output,",
            "                                err='')",
            "        else:",
            "            return (super(FakeSessionForVMTests, self).",
            "                    host_call_plugin(_1, _2, plugin, method, _5))",
            "",
            "    def VM_start(self, _1, ref, _2, _3):",
            "        vm = fake.get_record('VM', ref)",
            "        if vm['power_state'] != 'Halted':",
            "            raise fake.Failure(['VM_BAD_POWER_STATE', ref, 'Halted',",
            "                                vm['power_state']])",
            "        vm['power_state'] = 'Running'",
            "        vm['is_a_template'] = False",
            "        vm['is_control_domain'] = False",
            "        vm['domid'] = random.randrange(1, 1 << 16)",
            "        return vm",
            "",
            "    def VM_start_on(self, _1, vm_ref, host_ref, _2, _3):",
            "        vm_rec = self.VM_start(_1, vm_ref, _2, _3)",
            "        vm_rec['resident_on'] = host_ref",
            "",
            "    def VDI_snapshot(self, session_ref, vm_ref, _1):",
            "        sr_ref = \"fakesr\"",
            "        return fake.create_vdi('fakelabel', sr_ref, read_only=True)",
            "",
            "    def SR_scan(self, session_ref, sr_ref):",
            "        pass",
            "",
            "",
            "class FakeSessionForFirewallTests(FakeSessionForVMTests):",
            "    \"\"\"Stubs out a XenApi Session for doing IPTable Firewall tests.\"\"\"",
            "",
            "    def __init__(self, uri, test_case=None):",
            "        super(FakeSessionForFirewallTests, self).__init__(uri)",
            "        if hasattr(test_case, '_in_rules'):",
            "            self._in_rules = test_case._in_rules",
            "        if hasattr(test_case, '_in6_filter_rules'):",
            "            self._in6_filter_rules = test_case._in6_filter_rules",
            "        self._test_case = test_case",
            "",
            "    def host_call_plugin(self, _1, _2, plugin, method, args):",
            "        \"\"\"Mock method four host_call_plugin to be used in unit tests",
            "           for the dom0 iptables Firewall drivers for XenAPI",
            "",
            "        \"\"\"",
            "        if plugin == \"xenhost\" and method == \"iptables_config\":",
            "            # The command to execute is a json-encoded list",
            "            cmd_args = args.get('cmd_args', None)",
            "            cmd = jsonutils.loads(cmd_args)",
            "            if not cmd:",
            "                ret_str = ''",
            "            else:",
            "                output = ''",
            "                process_input = args.get('process_input', None)",
            "                if cmd == ['ip6tables-save', '-c']:",
            "                    output = '\\n'.join(self._in6_filter_rules)",
            "                if cmd == ['iptables-save', '-c']:",
            "                    output = '\\n'.join(self._in_rules)",
            "                if cmd == ['iptables-restore', '-c', ]:",
            "                    lines = process_input.split('\\n')",
            "                    if '*filter' in lines:",
            "                        if self._test_case is not None:",
            "                            self._test_case._out_rules = lines",
            "                        output = '\\n'.join(lines)",
            "                if cmd == ['ip6tables-restore', '-c', ]:",
            "                    lines = process_input.split('\\n')",
            "                    if '*filter' in lines:",
            "                        output = '\\n'.join(lines)",
            "                ret_str = fake.as_json(out=output, err='')",
            "        return ret_str",
            "",
            "",
            "def stub_out_vm_methods(stubs):",
            "    def fake_acquire_bootlock(self, vm):",
            "        pass",
            "",
            "    def fake_release_bootlock(self, vm):",
            "        pass",
            "",
            "    def fake_generate_ephemeral(*args):",
            "        pass",
            "",
            "    def fake_wait_for_device(dev):",
            "        pass",
            "",
            "    stubs.Set(vmops.VMOps, \"_acquire_bootlock\", fake_acquire_bootlock)",
            "    stubs.Set(vmops.VMOps, \"_release_bootlock\", fake_release_bootlock)",
            "    stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "    stubs.Set(vm_utils, '_wait_for_device', fake_wait_for_device)",
            "",
            "",
            "class FakeSessionForVolumeTests(fake.SessionBase):",
            "    \"\"\"Stubs out a XenAPISession for Volume tests.\"\"\"",
            "    def VDI_introduce(self, _1, uuid, _2, _3, _4, _5,",
            "                      _6, _7, _8, _9, _10, _11):",
            "        valid_vdi = False",
            "        refs = fake.get_all('VDI')",
            "        for ref in refs:",
            "            rec = fake.get_record('VDI', ref)",
            "            if rec['uuid'] == uuid:",
            "                valid_vdi = True",
            "        if not valid_vdi:",
            "            raise fake.Failure([['INVALID_VDI', 'session', self._session]])",
            "",
            "",
            "class FakeSessionForVolumeFailedTests(FakeSessionForVolumeTests):",
            "    \"\"\"Stubs out a XenAPISession for Volume tests: it injects failures.\"\"\"",
            "    def VDI_introduce(self, _1, uuid, _2, _3, _4, _5,",
            "                      _6, _7, _8, _9, _10, _11):",
            "        # This is for testing failure",
            "        raise fake.Failure([['INVALID_VDI', 'session', self._session]])",
            "",
            "    def PBD_unplug(self, _1, ref):",
            "        rec = fake.get_record('PBD', ref)",
            "        rec['currently-attached'] = False",
            "",
            "    def SR_forget(self, _1, ref):",
            "        pass",
            "",
            "",
            "def stub_out_migration_methods(stubs):",
            "    fakesr = fake.create_sr()",
            "",
            "    def fake_move_disks(self, instance, disk_info):",
            "        vdi_ref = fake.create_vdi(instance['name'], fakesr)",
            "        vdi_rec = fake.get_record('VDI', vdi_ref)",
            "        vdi_rec['other_config']['nova_disk_type'] = 'root'",
            "        return {'uuid': vdi_rec['uuid'], 'ref': vdi_ref}",
            "",
            "    def fake_wait_for_instance_to_start(self, *args):",
            "        pass",
            "",
            "    def fake_get_vdi(session, vm_ref):",
            "        vdi_ref_parent = fake.create_vdi('derp-parent', fakesr)",
            "        vdi_rec_parent = fake.get_record('VDI', vdi_ref_parent)",
            "        vdi_ref = fake.create_vdi('derp', fakesr,",
            "                sm_config={'vhd-parent': vdi_rec_parent['uuid']})",
            "        vdi_rec = session.call_xenapi(\"VDI.get_record\", vdi_ref)",
            "        return vdi_ref, vdi_rec",
            "",
            "    def fake_sr(session, *args):",
            "        return fakesr",
            "",
            "    def fake_get_sr_path(*args):",
            "        return \"fake\"",
            "",
            "    def fake_destroy(*args, **kwargs):",
            "        pass",
            "",
            "    def fake_generate_ephemeral(*args):",
            "        pass",
            "",
            "    stubs.Set(vmops.VMOps, '_destroy', fake_destroy)",
            "    stubs.Set(vmops.VMOps, '_wait_for_instance_to_start',",
            "              fake_wait_for_instance_to_start)",
            "    stubs.Set(vm_utils, 'move_disks', fake_move_disks)",
            "    stubs.Set(vm_utils, 'scan_default_sr', fake_sr)",
            "    stubs.Set(vm_utils, 'get_vdi_for_vm_safely', fake_get_vdi)",
            "    stubs.Set(vm_utils, 'get_sr_path', fake_get_sr_path)",
            "    stubs.Set(vm_utils, 'generate_ephemeral', fake_generate_ephemeral)",
            "",
            "",
            "class FakeSessionForFailedMigrateTests(FakeSessionForVMTests):",
            "    def VM_assert_can_migrate(self, session, vmref, migrate_data,",
            "                              live, vdi_map, vif_map, options):",
            "        raise fake.Failure(\"XenAPI VM.assert_can_migrate failed\")",
            "",
            "    def host_migrate_receive(self, session, hostref, networkref, options):",
            "        raise fake.Failure(\"XenAPI host.migrate_receive failed\")",
            "",
            "    def VM_migrate_send(self, session, vmref, migrate_data, islive, vdi_map,",
            "                        vif_map, options):",
            "        raise fake.Failure(\"XenAPI VM.migrate_send failed\")",
            "",
            "",
            "class XenAPITestBase(test.TestCase):",
            "    def setUp(self):",
            "        super(XenAPITestBase, self).setUp()",
            "",
            "        self.useFixture(test.ReplaceModule('XenAPI', fake))",
            "",
            "        fake.reset()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "asyncua.server.internal_session"
        ]
    },
    "nova/tests/virt/xenapi/test_vmops.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 404,
                "afterPatchRowNumber": 404,
                "PatchRowcode": "         self.assertRaises(test.TestingException, self._test_spawn,"
            },
            "1": {
                "beforePatchRowNumber": 405,
                "afterPatchRowNumber": 405,
                "PatchRowcode": "                           throw_exception=test.TestingException())"
            },
            "2": {
                "beforePatchRowNumber": 406,
                "afterPatchRowNumber": 406,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 407,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def test_finish_migration(self):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 407,
                "PatchRowcode": "+    def _test_finish_migration(self, power_on=True, resize_instance=True,"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 408,
                "PatchRowcode": "+                               throw_exception=None):"
            },
            "6": {
                "beforePatchRowNumber": 408,
                "afterPatchRowNumber": 409,
                "PatchRowcode": "         self._stub_out_common()"
            },
            "7": {
                "beforePatchRowNumber": 409,
                "afterPatchRowNumber": 410,
                "PatchRowcode": "         self.mox.StubOutWithMock(vm_utils, \"move_disks\")"
            },
            "8": {
                "beforePatchRowNumber": 410,
                "afterPatchRowNumber": 411,
                "PatchRowcode": "         self.mox.StubOutWithMock(self.vmops, \"_attach_mapped_block_devices\")"
            },
            "9": {
                "beforePatchRowNumber": 416,
                "afterPatchRowNumber": 417,
                "PatchRowcode": "         disk_info = \"disk_info\""
            },
            "10": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 418,
                "PatchRowcode": "         network_info = \"net_info\""
            },
            "11": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 419,
                "PatchRowcode": "         image_meta = {\"id\": \"image_id\"}"
            },
            "12": {
                "beforePatchRowNumber": 419,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        resize_instance = True"
            },
            "13": {
                "beforePatchRowNumber": 420,
                "afterPatchRowNumber": 420,
                "PatchRowcode": "         block_device_info = \"bdi\""
            },
            "14": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 421,
                "PatchRowcode": "         session = self.vmops._session"
            },
            "15": {
                "beforePatchRowNumber": 422,
                "afterPatchRowNumber": 422,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 423,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        root_vdi = \"root_vdi\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 423,
                "PatchRowcode": "+        self.vmops._ensure_instance_name_unique(name_label)"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 424,
                "PatchRowcode": "+        self.vmops._ensure_enough_free_mem(instance)"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 425,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 426,
                "PatchRowcode": "+        di_type = \"di_type\""
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 427,
                "PatchRowcode": "+        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 428,
                "PatchRowcode": "+"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 429,
                "PatchRowcode": "+        root_vdi = {\"ref\": \"fake_ref\"}"
            },
            "24": {
                "beforePatchRowNumber": 424,
                "afterPatchRowNumber": 430,
                "PatchRowcode": "         vdis = {\"root\": root_vdi}"
            },
            "25": {
                "beforePatchRowNumber": 425,
                "afterPatchRowNumber": 431,
                "PatchRowcode": "         vm_utils.move_disks(self.vmops._session, instance,"
            },
            "26": {
                "beforePatchRowNumber": 426,
                "afterPatchRowNumber": 432,
                "PatchRowcode": "                             disk_info).AndReturn(root_vdi)"
            },
            "27": {
                "beforePatchRowNumber": 427,
                "afterPatchRowNumber": 433,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": 428,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._resize_up_root_vdi(instance, root_vdi)"
            },
            "29": {
                "beforePatchRowNumber": 429,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "30": {
                "beforePatchRowNumber": 430,
                "afterPatchRowNumber": 434,
                "PatchRowcode": "         kernel_file = \"kernel\""
            },
            "31": {
                "beforePatchRowNumber": 431,
                "afterPatchRowNumber": 435,
                "PatchRowcode": "         ramdisk_file = \"ramdisk\""
            },
            "32": {
                "beforePatchRowNumber": 432,
                "afterPatchRowNumber": 436,
                "PatchRowcode": "         vm_utils.create_kernel_and_ramdisk(context, session,"
            },
            "33": {
                "beforePatchRowNumber": 433,
                "afterPatchRowNumber": 437,
                "PatchRowcode": "                 instance, name_label).AndReturn((kernel_file, ramdisk_file))"
            },
            "34": {
                "beforePatchRowNumber": 434,
                "afterPatchRowNumber": 438,
                "PatchRowcode": " "
            },
            "35": {
                "beforePatchRowNumber": 435,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        di_type = \"di_type\""
            },
            "36": {
                "beforePatchRowNumber": 436,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)"
            },
            "37": {
                "beforePatchRowNumber": 437,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._ensure_instance_name_unique(name_label)"
            },
            "38": {
                "beforePatchRowNumber": 438,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._ensure_enough_free_mem(instance)"
            },
            "39": {
                "beforePatchRowNumber": 439,
                "afterPatchRowNumber": 439,
                "PatchRowcode": "         vm_ref = \"fake_vm_ref\""
            },
            "40": {
                "beforePatchRowNumber": 440,
                "afterPatchRowNumber": 440,
                "PatchRowcode": "         self.vmops._create_vm_record(context, instance, name_label, vdis,"
            },
            "41": {
                "beforePatchRowNumber": 441,
                "afterPatchRowNumber": 441,
                "PatchRowcode": "                 di_type, kernel_file, ramdisk_file).AndReturn(vm_ref)"
            },
            "42": {
                "beforePatchRowNumber": 442,
                "afterPatchRowNumber": 442,
                "PatchRowcode": " "
            },
            "43": {
                "beforePatchRowNumber": 443,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._attach_disks(instance, vm_ref, name_label, vdis, di_type)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 443,
                "PatchRowcode": "+        if resize_instance:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 444,
                "PatchRowcode": "+            self.vmops._resize_up_root_vdi(instance, root_vdi)"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 445,
                "PatchRowcode": "+        self.vmops._attach_disks(instance, vm_ref, name_label, vdis, di_type,"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 446,
                "PatchRowcode": "+                                 None, None)"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 447,
                "PatchRowcode": "+        self.vmops._attach_mapped_block_devices(instance, block_device_info)"
            },
            "49": {
                "beforePatchRowNumber": 444,
                "afterPatchRowNumber": 448,
                "PatchRowcode": " "
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 449,
                "PatchRowcode": "+        self.vmops._inject_instance_metadata(instance, vm_ref)"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 450,
                "PatchRowcode": "+        self.vmops._inject_auto_disk_config(instance, vm_ref)"
            },
            "52": {
                "beforePatchRowNumber": 445,
                "afterPatchRowNumber": 451,
                "PatchRowcode": "         self.vmops._file_inject_vm_settings(instance, vm_ref, vdis,"
            },
            "53": {
                "beforePatchRowNumber": 446,
                "afterPatchRowNumber": 452,
                "PatchRowcode": "                                             network_info)"
            },
            "54": {
                "beforePatchRowNumber": 447,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._create_vifs(instance, vm_ref, network_info)"
            },
            "55": {
                "beforePatchRowNumber": 448,
                "afterPatchRowNumber": 453,
                "PatchRowcode": "         self.vmops.inject_network_info(instance, network_info, vm_ref)"
            },
            "56": {
                "beforePatchRowNumber": 449,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._inject_instance_metadata(instance, vm_ref)"
            },
            "57": {
                "beforePatchRowNumber": 450,
                "afterPatchRowNumber": 454,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 451,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._attach_mapped_block_devices(instance, block_device_info)"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 455,
                "PatchRowcode": "+        self.vmops._create_vifs(instance, vm_ref, network_info)"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 456,
                "PatchRowcode": "+        self.vmops.firewall_driver.setup_basic_filtering(instance,"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 457,
                "PatchRowcode": "+                network_info).AndRaise(NotImplementedError)"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 458,
                "PatchRowcode": "+        self.vmops.firewall_driver.prepare_instance_filter(instance,"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 459,
                "PatchRowcode": "+                                                           network_info)"
            },
            "64": {
                "beforePatchRowNumber": 452,
                "afterPatchRowNumber": 460,
                "PatchRowcode": " "
            },
            "65": {
                "beforePatchRowNumber": 453,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._start(instance, vm_ref)"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 461,
                "PatchRowcode": "+        if power_on:"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 462,
                "PatchRowcode": "+            self.vmops._start(instance, vm_ref)"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 463,
                "PatchRowcode": "+            self.vmops._wait_for_instance_to_start(instance, vm_ref)"
            },
            "69": {
                "beforePatchRowNumber": 454,
                "afterPatchRowNumber": 464,
                "PatchRowcode": " "
            },
            "70": {
                "beforePatchRowNumber": 455,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vmops._update_instance_progress(context, instance,"
            },
            "71": {
                "beforePatchRowNumber": 456,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                             step=5, total_steps=5)"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 465,
                "PatchRowcode": "+        self.vmops.firewall_driver.apply_instance_filter(instance,"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 466,
                "PatchRowcode": "+                                                         network_info)"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 467,
                "PatchRowcode": "+"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 468,
                "PatchRowcode": "+        last_call = self.vmops._update_instance_progress(context, instance,"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 469,
                "PatchRowcode": "+                                                        step=5, total_steps=5)"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 470,
                "PatchRowcode": "+        if throw_exception:"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 471,
                "PatchRowcode": "+            last_call.AndRaise(throw_exception)"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 472,
                "PatchRowcode": "+            self.vmops._destroy(instance, vm_ref, network_info=network_info)"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 473,
                "PatchRowcode": "+            vm_utils.destroy_kernel_ramdisk(self.vmops._session, instance,"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 474,
                "PatchRowcode": "+                                            kernel_file, ramdisk_file)"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 475,
                "PatchRowcode": "+            vm_utils.safe_destroy_vdis(self.vmops._session, [\"fake_ref\"])"
            },
            "83": {
                "beforePatchRowNumber": 457,
                "afterPatchRowNumber": 476,
                "PatchRowcode": " "
            },
            "84": {
                "beforePatchRowNumber": 458,
                "afterPatchRowNumber": 477,
                "PatchRowcode": "         self.mox.ReplayAll()"
            },
            "85": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": 478,
                "PatchRowcode": "         self.vmops.finish_migration(context, migration, instance, disk_info,"
            },
            "86": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": 479,
                "PatchRowcode": "                                     network_info, image_meta, resize_instance,"
            },
            "87": {
                "beforePatchRowNumber": 461,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    block_device_info)"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 480,
                "PatchRowcode": "+                                    block_device_info, power_on)"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 481,
                "PatchRowcode": "+"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 482,
                "PatchRowcode": "+    def test_finish_migration(self):"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 483,
                "PatchRowcode": "+        self._test_finish_migration()"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 484,
                "PatchRowcode": "+"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 485,
                "PatchRowcode": "+    def test_finish_migration_no_power_on(self):"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 486,
                "PatchRowcode": "+        self._test_finish_migration(power_on=False, resize_instance=False)"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 487,
                "PatchRowcode": "+"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 488,
                "PatchRowcode": "+    def test_finish_migrate_performs_rollback_on_error(self):"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 489,
                "PatchRowcode": "+        self.assertRaises(test.TestingException, self._test_finish_migration,"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 490,
                "PatchRowcode": "+                          power_on=False, resize_instance=False,"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 491,
                "PatchRowcode": "+                          throw_exception=test.TestingException())"
            },
            "100": {
                "beforePatchRowNumber": 462,
                "afterPatchRowNumber": 492,
                "PatchRowcode": " "
            },
            "101": {
                "beforePatchRowNumber": 463,
                "afterPatchRowNumber": 493,
                "PatchRowcode": "     def test_remove_hostname(self):"
            },
            "102": {
                "beforePatchRowNumber": 464,
                "afterPatchRowNumber": 494,
                "PatchRowcode": "         vm, vm_ref = self.create_vm(\"dummy\")"
            }
        },
        "frontPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright 2013 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_mode",
            "from nova import exception",
            "from nova import test",
            "from nova.tests.virt.xenapi import stubs",
            "from nova.virt import fake",
            "from nova.virt.xenapi import driver as xenapi_conn",
            "from nova.virt.xenapi import fake as xenapi_fake",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "",
            "",
            "class VMOpsTestBase(stubs.XenAPITestBase):",
            "    def setUp(self):",
            "        super(VMOpsTestBase, self).setUp()",
            "        self._setup_mock_vmops()",
            "        self.vms = []",
            "",
            "    def _setup_mock_vmops(self, product_brand=None, product_version=None):",
            "        stubs.stubout_session(self.stubs, xenapi_fake.SessionBase)",
            "        self._session = xenapi_conn.XenAPISession('test_url', 'root',",
            "                                                  'test_pass',",
            "                                                  fake.FakeVirtAPI())",
            "        self.vmops = vmops.VMOps(self._session, fake.FakeVirtAPI())",
            "",
            "    def create_vm(self, name, state=\"running\"):",
            "        vm_ref = xenapi_fake.create_vm(name, state)",
            "        self.vms.append(vm_ref)",
            "        vm = xenapi_fake.get_record(\"VM\", vm_ref)",
            "        return vm, vm_ref",
            "",
            "    def tearDown(self):",
            "        super(VMOpsTestBase, self).tearDown()",
            "        for vm in self.vms:",
            "            xenapi_fake.destroy_vm(vm)",
            "",
            "",
            "class VMOpsTestCase(test.TestCase):",
            "    def setUp(self):",
            "        super(VMOpsTestCase, self).setUp()",
            "        self._setup_mock_vmops()",
            "",
            "    def _setup_mock_vmops(self, product_brand=None, product_version=None):",
            "        self._session = self._get_mock_session(product_brand, product_version)",
            "        self._vmops = vmops.VMOps(self._session, fake.FakeVirtAPI())",
            "",
            "    def _get_mock_session(self, product_brand, product_version):",
            "        class Mock(object):",
            "            pass",
            "",
            "        mock_session = Mock()",
            "        mock_session.product_brand = product_brand",
            "        mock_session.product_version = product_version",
            "        return mock_session",
            "",
            "    def test_check_resize_func_name_defaults_to_VDI_resize(self):",
            "        self.assertEquals(",
            "            'VDI.resize',",
            "            self._vmops.check_resize_func_name())",
            "",
            "    def _test_finish_revert_migration_after_crash(self, backup_made, new_made):",
            "        instance = {'name': 'foo',",
            "                    'task_state': task_states.RESIZE_MIGRATING}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        self.mox.StubOutWithMock(self._vmops, '_destroy')",
            "        self.mox.StubOutWithMock(vm_utils, 'set_vm_name_label')",
            "        self.mox.StubOutWithMock(self._vmops, '_attach_mapped_block_devices')",
            "        self.mox.StubOutWithMock(self._vmops, '_start')",
            "",
            "        vm_utils.lookup(self._session, 'foo-orig').AndReturn(",
            "            backup_made and 'foo' or None)",
            "        vm_utils.lookup(self._session, 'foo').AndReturn(",
            "            (not backup_made or new_made) and 'foo' or None)",
            "        if backup_made:",
            "            if new_made:",
            "                self._vmops._destroy(instance, 'foo')",
            "            vm_utils.set_vm_name_label(self._session, 'foo', 'foo')",
            "            self._vmops._attach_mapped_block_devices(instance, [])",
            "        self._vmops._start(instance, 'foo')",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        self._vmops.finish_revert_migration(instance, [])",
            "",
            "    def test_finish_revert_migration_after_crash(self):",
            "        self._test_finish_revert_migration_after_crash(True, True)",
            "",
            "    def test_finish_revert_migration_after_crash_before_new(self):",
            "        self._test_finish_revert_migration_after_crash(True, False)",
            "",
            "    def test_finish_revert_migration_after_crash_before_backup(self):",
            "        self._test_finish_revert_migration_after_crash(False, False)",
            "",
            "    def test_determine_vm_mode_returns_xen(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "",
            "        fake_instance = \"instance\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(vm_mode.XEN)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.XEN,",
            "            self._vmops._determine_vm_mode(fake_instance, None, None))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_hvm(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "",
            "        fake_instance = \"instance\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(vm_mode.HVM)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.HVM,",
            "            self._vmops._determine_vm_mode(fake_instance, None, None))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_is_pv(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_is_pv')",
            "",
            "        fake_instance = {\"os_type\": \"foo\"}",
            "        fake_vdis = {'root': {\"ref\": 'fake'}}",
            "        fake_disk_type = \"disk\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(None)",
            "        vm_utils.determine_is_pv(self._session, \"fake\", fake_disk_type,",
            "            \"foo\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.XEN,",
            "            self._vmops._determine_vm_mode(fake_instance, fake_vdis,",
            "                                     fake_disk_type))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_is_not_pv(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_is_pv')",
            "",
            "        fake_instance = {\"os_type\": \"foo\"}",
            "        fake_vdis = {'root': {\"ref\": 'fake'}}",
            "        fake_disk_type = \"disk\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(None)",
            "        vm_utils.determine_is_pv(self._session, \"fake\", fake_disk_type,",
            "            \"foo\").AndReturn(False)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.HVM,",
            "            self._vmops._determine_vm_mode(fake_instance, fake_vdis,",
            "                                     fake_disk_type))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_is_not_pv_no_root_disk(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_is_pv')",
            "",
            "        fake_instance = {\"os_type\": \"foo\"}",
            "        fake_vdis = {'iso': {\"ref\": 'fake'}}",
            "        fake_disk_type = \"disk\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(None)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.HVM,",
            "            self._vmops._determine_vm_mode(fake_instance, fake_vdis,",
            "                                     fake_disk_type))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_xsm_sr_check_relaxed_cached(self):",
            "        self.make_plugin_call_count = 0",
            "",
            "        def fake_make_plugin_call(plugin, method, **args):",
            "            self.make_plugin_call_count = self.make_plugin_call_count + 1",
            "            return \"true\"",
            "",
            "        self.stubs.Set(self._vmops, \"_make_plugin_call\",",
            "                       fake_make_plugin_call)",
            "",
            "        self.assertTrue(self._vmops._is_xsm_sr_check_relaxed())",
            "        self.assertTrue(self._vmops._is_xsm_sr_check_relaxed())",
            "",
            "        self.assertEqual(self.make_plugin_call_count, 1)",
            "",
            "",
            "class InjectAutoDiskConfigTestCase(VMOpsTestBase):",
            "    def setUp(self):",
            "        super(InjectAutoDiskConfigTestCase, self).setUp()",
            "",
            "    def test_inject_auto_disk_config_when_present(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"1234\", \"auto_disk_config\": True}",
            "        self.vmops._inject_auto_disk_config(instance, vm_ref)",
            "        xenstore_data = vm['xenstore_data']",
            "        self.assertEquals(xenstore_data['vm-data/auto-disk-config'], 'True')",
            "",
            "    def test_inject_auto_disk_config_none_as_false(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"1234\", \"auto_disk_config\": None}",
            "        self.vmops._inject_auto_disk_config(instance, vm_ref)",
            "        xenstore_data = vm['xenstore_data']",
            "        self.assertEquals(xenstore_data['vm-data/auto-disk-config'], 'False')",
            "",
            "",
            "class GetConsoleOutputTestCase(VMOpsTestBase):",
            "    def setUp(self):",
            "        super(GetConsoleOutputTestCase, self).setUp()",
            "",
            "    def test_get_console_output_works(self):",
            "        self.mox.StubOutWithMock(self.vmops, '_get_dom_id')",
            "",
            "        instance = {\"name\": \"dummy\"}",
            "        self.vmops._get_dom_id(instance, check_rescue=True).AndReturn(42)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertEqual(\"dom_id: 42\", self.vmops.get_console_output(instance))",
            "",
            "    def test_get_console_output_throws_nova_exception(self):",
            "        self.mox.StubOutWithMock(self.vmops, '_get_dom_id')",
            "",
            "        instance = {\"name\": \"dummy\"}",
            "        # dom_id=0 used to trigger exception in fake XenAPI",
            "        self.vmops._get_dom_id(instance, check_rescue=True).AndReturn(0)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertRaises(exception.NovaException,",
            "                self.vmops.get_console_output, instance)",
            "",
            "    def test_get_dom_id_works(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        self.assertEqual(vm[\"domid\"], self.vmops._get_dom_id(instance))",
            "",
            "    def test_get_dom_id_works_with_rescue_vm(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        vm, vm_ref = self.create_vm(\"dummy-rescue\")",
            "        self.assertEqual(vm[\"domid\"],",
            "                self.vmops._get_dom_id(instance, check_rescue=True))",
            "",
            "    def test_get_dom_id_raises_not_found(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        self.create_vm(\"not-dummy\")",
            "        self.assertRaises(exception.NotFound, self.vmops._get_dom_id, instance)",
            "",
            "    def test_get_dom_id_works_with_vmref(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        self.assertEqual(vm[\"domid\"],",
            "                         self.vmops._get_dom_id(vm_ref=vm_ref))",
            "",
            "",
            "class SpawnTestCase(VMOpsTestBase):",
            "    def _stub_out_common(self):",
            "        self.mox.StubOutWithMock(self.vmops, '_ensure_instance_name_unique')",
            "        self.mox.StubOutWithMock(self.vmops, '_ensure_enough_free_mem')",
            "        self.mox.StubOutWithMock(self.vmops, '_update_instance_progress')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_disk_image_type')",
            "        self.mox.StubOutWithMock(vm_utils, 'get_vdis_for_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'safe_destroy_vdis')",
            "        self.mox.StubOutWithMock(self.vmops, '_resize_up_root_vdi')",
            "        self.mox.StubOutWithMock(vm_utils,",
            "                                 'create_kernel_and_ramdisk')",
            "        self.mox.StubOutWithMock(vm_utils, 'destroy_kernel_ramdisk')",
            "        self.mox.StubOutWithMock(self.vmops, '_create_vm_record')",
            "        self.mox.StubOutWithMock(self.vmops, '_destroy')",
            "        self.mox.StubOutWithMock(self.vmops, '_attach_disks')",
            "        self.mox.StubOutWithMock(self.vmops, '_attach_orig_disk_for_rescue')",
            "        self.mox.StubOutWithMock(self.vmops, 'inject_network_info')",
            "        self.mox.StubOutWithMock(self.vmops, '_inject_hostname')",
            "        self.mox.StubOutWithMock(self.vmops, '_inject_instance_metadata')",
            "        self.mox.StubOutWithMock(self.vmops, '_inject_auto_disk_config')",
            "        self.mox.StubOutWithMock(self.vmops, '_file_inject_vm_settings')",
            "        self.mox.StubOutWithMock(self.vmops, '_create_vifs')",
            "        self.mox.StubOutWithMock(self.vmops.firewall_driver,",
            "                                 'setup_basic_filtering')",
            "        self.mox.StubOutWithMock(self.vmops.firewall_driver,",
            "                                 'prepare_instance_filter')",
            "        self.mox.StubOutWithMock(self.vmops, '_start')",
            "        self.mox.StubOutWithMock(self.vmops, '_wait_for_instance_to_start')",
            "        self.mox.StubOutWithMock(self.vmops,",
            "                                 '_configure_new_instance_with_agent')",
            "        self.mox.StubOutWithMock(self.vmops, '_remove_hostname')",
            "        self.mox.StubOutWithMock(self.vmops.firewall_driver,",
            "                                 'apply_instance_filter')",
            "",
            "    def _test_spawn(self, name_label_param=None, block_device_info_param=None,",
            "                    rescue=False, include_root_vdi=True,",
            "                    throw_exception=None):",
            "        self._stub_out_common()",
            "",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"fake_uuid\"}",
            "        name_label = name_label_param",
            "        if name_label is None:",
            "            name_label = \"dummy\"",
            "        image_meta = {\"id\": \"image_id\"}",
            "        context = \"context\"",
            "        session = self.vmops._session",
            "        injected_files = \"fake_files\"",
            "        admin_password = \"password\"",
            "        network_info = \"net_info\"",
            "        steps = 10",
            "        block_device_info = block_device_info_param",
            "        if block_device_info and not block_device_info['root_device_name']:",
            "            block_device_info = dict(block_device_info_param)",
            "            block_device_info['root_device_name'] = \\",
            "                                                self.vmops.default_root_dev",
            "",
            "        di_type = \"di_type\"",
            "        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)",
            "        self.vmops._update_instance_progress(context, instance, 1, steps)",
            "",
            "        vdis = {\"other\": {\"ref\": \"fake_ref_2\", \"osvol\": True}}",
            "        if include_root_vdi:",
            "            vdis[\"root\"] = {\"ref\": \"fake_ref\"}",
            "        vm_utils.get_vdis_for_instance(context, session, instance, name_label,",
            "                    \"image_id\", di_type,",
            "                    block_device_info=block_device_info).AndReturn(vdis)",
            "        if include_root_vdi:",
            "            self.vmops._resize_up_root_vdi(instance, vdis[\"root\"])",
            "        self.vmops._update_instance_progress(context, instance, 2, steps)",
            "",
            "        kernel_file = \"kernel\"",
            "        ramdisk_file = \"ramdisk\"",
            "        vm_utils.create_kernel_and_ramdisk(context, session,",
            "                instance, name_label).AndReturn((kernel_file, ramdisk_file))",
            "        self.vmops._update_instance_progress(context, instance, 3, steps)",
            "",
            "        vm_ref = \"fake_vm_ref\"",
            "        self.vmops._ensure_instance_name_unique(name_label)",
            "        self.vmops._ensure_enough_free_mem(instance)",
            "        self.vmops._create_vm_record(context, instance, name_label, vdis,",
            "                di_type, kernel_file, ramdisk_file).AndReturn(vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 4, steps)",
            "",
            "        self.vmops._attach_disks(instance, vm_ref, name_label, vdis, di_type,",
            "                          admin_password, injected_files)",
            "        if rescue:",
            "            self.vmops._attach_orig_disk_for_rescue(instance, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 5, steps)",
            "",
            "        self.vmops._inject_instance_metadata(instance, vm_ref)",
            "        self.vmops._inject_auto_disk_config(instance, vm_ref)",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue)",
            "        self.vmops._file_inject_vm_settings(instance, vm_ref, vdis,",
            "                                            network_info)",
            "        self.vmops.inject_network_info(instance, network_info, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 6, steps)",
            "",
            "        self.vmops._create_vifs(instance, vm_ref, network_info)",
            "        self.vmops.firewall_driver.setup_basic_filtering(instance,",
            "                network_info).AndRaise(NotImplementedError)",
            "        self.vmops.firewall_driver.prepare_instance_filter(instance,",
            "                                                           network_info)",
            "        self.vmops._update_instance_progress(context, instance, 7, steps)",
            "",
            "        self.vmops._start(instance, vm_ref)",
            "        self.vmops._wait_for_instance_to_start(instance, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 8, steps)",
            "",
            "        self.vmops._configure_new_instance_with_agent(instance, vm_ref,",
            "                injected_files, admin_password)",
            "        self.vmops._remove_hostname(instance, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 9, steps)",
            "",
            "        self.vmops.firewall_driver.apply_instance_filter(instance,",
            "                                                         network_info)",
            "        last_call = self.vmops._update_instance_progress(context, instance,",
            "                                                         steps, steps)",
            "        if throw_exception:",
            "            last_call.AndRaise(throw_exception)",
            "            self.vmops._destroy(instance, vm_ref, network_info=network_info)",
            "            vm_utils.destroy_kernel_ramdisk(self.vmops._session, instance,",
            "                                            kernel_file, ramdisk_file)",
            "            vm_utils.safe_destroy_vdis(self.vmops._session, [\"fake_ref\"])",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops.spawn(context, instance, image_meta, injected_files,",
            "                         admin_password, network_info,",
            "                         block_device_info_param, name_label_param, rescue)",
            "",
            "    def test_spawn(self):",
            "        self._test_spawn()",
            "",
            "    def test_spawn_with_alternate_options(self):",
            "        self._test_spawn(include_root_vdi=False, rescue=True,",
            "                         name_label_param=\"bob\",",
            "                         block_device_info_param={\"root_device_name\": \"\"})",
            "",
            "    def test_spawn_performs_rollback_and_throws_exception(self):",
            "        self.assertRaises(test.TestingException, self._test_spawn,",
            "                          throw_exception=test.TestingException())",
            "",
            "    def test_finish_migration(self):",
            "        self._stub_out_common()",
            "        self.mox.StubOutWithMock(vm_utils, \"move_disks\")",
            "        self.mox.StubOutWithMock(self.vmops, \"_attach_mapped_block_devices\")",
            "",
            "        context = \"context\"",
            "        migration = {}",
            "        name_label = \"dummy\"",
            "        instance = {\"name\": name_label, \"uuid\": \"fake_uuid\"}",
            "        disk_info = \"disk_info\"",
            "        network_info = \"net_info\"",
            "        image_meta = {\"id\": \"image_id\"}",
            "        resize_instance = True",
            "        block_device_info = \"bdi\"",
            "        session = self.vmops._session",
            "",
            "        root_vdi = \"root_vdi\"",
            "        vdis = {\"root\": root_vdi}",
            "        vm_utils.move_disks(self.vmops._session, instance,",
            "                            disk_info).AndReturn(root_vdi)",
            "",
            "        self.vmops._resize_up_root_vdi(instance, root_vdi)",
            "",
            "        kernel_file = \"kernel\"",
            "        ramdisk_file = \"ramdisk\"",
            "        vm_utils.create_kernel_and_ramdisk(context, session,",
            "                instance, name_label).AndReturn((kernel_file, ramdisk_file))",
            "",
            "        di_type = \"di_type\"",
            "        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)",
            "        self.vmops._ensure_instance_name_unique(name_label)",
            "        self.vmops._ensure_enough_free_mem(instance)",
            "        vm_ref = \"fake_vm_ref\"",
            "        self.vmops._create_vm_record(context, instance, name_label, vdis,",
            "                di_type, kernel_file, ramdisk_file).AndReturn(vm_ref)",
            "",
            "        self.vmops._attach_disks(instance, vm_ref, name_label, vdis, di_type)",
            "",
            "        self.vmops._file_inject_vm_settings(instance, vm_ref, vdis,",
            "                                            network_info)",
            "        self.vmops._create_vifs(instance, vm_ref, network_info)",
            "        self.vmops.inject_network_info(instance, network_info, vm_ref)",
            "        self.vmops._inject_instance_metadata(instance, vm_ref)",
            "",
            "        self.vmops._attach_mapped_block_devices(instance, block_device_info)",
            "",
            "        self.vmops._start(instance, vm_ref)",
            "",
            "        self.vmops._update_instance_progress(context, instance,",
            "                                             step=5, total_steps=5)",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops.finish_migration(context, migration, instance, disk_info,",
            "                                    network_info, image_meta, resize_instance,",
            "                                    block_device_info)",
            "",
            "    def test_remove_hostname(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"1234\", \"auto_disk_config\": None}",
            "        self.mox.StubOutWithMock(self._session, 'call_xenapi')",
            "        self._session.call_xenapi(\"VM.remove_from_xenstore_data\", vm_ref,",
            "                                  \"vm-data/hostname\")",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._remove_hostname(instance, vm_ref)",
            "        self.mox.VerifyAll()",
            "",
            "    def test_inject_hostname(self):",
            "        instance = {\"hostname\": \"dummy\", \"os_type\": \"fake\", \"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')",
            "        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname', 'dummy')",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue=False)",
            "",
            "    def test_inject_hostname_with_rescue_prefix(self):",
            "        instance = {\"hostname\": \"dummy\", \"os_type\": \"fake\", \"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')",
            "        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname',",
            "                                          'RESCUE-dummy')",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue=True)",
            "",
            "    def test_inject_hostname_with_windows_name_truncation(self):",
            "        instance = {\"hostname\": \"dummydummydummydummydummy\",",
            "                    \"os_type\": \"windows\", \"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')",
            "        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname',",
            "                                          'RESCUE-dummydum')",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue=True)",
            "",
            "    def test_wait_for_instance_to_start(self):",
            "        instance = {\"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, 'get_info')",
            "        self.vmops.get_info(instance, vm_ref).AndReturn({\"state\": \"asdf\"})",
            "        self.vmops.get_info(instance, vm_ref).AndReturn({",
            "                                            \"state\": power_state.RUNNING})",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._wait_for_instance_to_start(instance, vm_ref)",
            "",
            "    def test_attach_orig_disk_for_rescue(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        self.mox.StubOutWithMock(self.vmops, '_find_root_vdi_ref')",
            "        self.mox.StubOutWithMock(vm_utils, 'create_vbd')",
            "",
            "        vm_utils.lookup(self.vmops._session, \"dummy\").AndReturn(\"ref\")",
            "        self.vmops._find_root_vdi_ref(\"ref\").AndReturn(\"vdi_ref\")",
            "        vm_utils.create_vbd(self.vmops._session, vm_ref, \"vdi_ref\",",
            "                            vmops.DEVICE_RESCUE, bootable=False)",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._attach_orig_disk_for_rescue(instance, vm_ref)"
        ],
        "afterPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright 2013 OpenStack Foundation",
            "# All Rights Reserved.",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_mode",
            "from nova import exception",
            "from nova import test",
            "from nova.tests.virt.xenapi import stubs",
            "from nova.virt import fake",
            "from nova.virt.xenapi import driver as xenapi_conn",
            "from nova.virt.xenapi import fake as xenapi_fake",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import vmops",
            "",
            "",
            "class VMOpsTestBase(stubs.XenAPITestBase):",
            "    def setUp(self):",
            "        super(VMOpsTestBase, self).setUp()",
            "        self._setup_mock_vmops()",
            "        self.vms = []",
            "",
            "    def _setup_mock_vmops(self, product_brand=None, product_version=None):",
            "        stubs.stubout_session(self.stubs, xenapi_fake.SessionBase)",
            "        self._session = xenapi_conn.XenAPISession('test_url', 'root',",
            "                                                  'test_pass',",
            "                                                  fake.FakeVirtAPI())",
            "        self.vmops = vmops.VMOps(self._session, fake.FakeVirtAPI())",
            "",
            "    def create_vm(self, name, state=\"running\"):",
            "        vm_ref = xenapi_fake.create_vm(name, state)",
            "        self.vms.append(vm_ref)",
            "        vm = xenapi_fake.get_record(\"VM\", vm_ref)",
            "        return vm, vm_ref",
            "",
            "    def tearDown(self):",
            "        super(VMOpsTestBase, self).tearDown()",
            "        for vm in self.vms:",
            "            xenapi_fake.destroy_vm(vm)",
            "",
            "",
            "class VMOpsTestCase(test.TestCase):",
            "    def setUp(self):",
            "        super(VMOpsTestCase, self).setUp()",
            "        self._setup_mock_vmops()",
            "",
            "    def _setup_mock_vmops(self, product_brand=None, product_version=None):",
            "        self._session = self._get_mock_session(product_brand, product_version)",
            "        self._vmops = vmops.VMOps(self._session, fake.FakeVirtAPI())",
            "",
            "    def _get_mock_session(self, product_brand, product_version):",
            "        class Mock(object):",
            "            pass",
            "",
            "        mock_session = Mock()",
            "        mock_session.product_brand = product_brand",
            "        mock_session.product_version = product_version",
            "        return mock_session",
            "",
            "    def test_check_resize_func_name_defaults_to_VDI_resize(self):",
            "        self.assertEquals(",
            "            'VDI.resize',",
            "            self._vmops.check_resize_func_name())",
            "",
            "    def _test_finish_revert_migration_after_crash(self, backup_made, new_made):",
            "        instance = {'name': 'foo',",
            "                    'task_state': task_states.RESIZE_MIGRATING}",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        self.mox.StubOutWithMock(self._vmops, '_destroy')",
            "        self.mox.StubOutWithMock(vm_utils, 'set_vm_name_label')",
            "        self.mox.StubOutWithMock(self._vmops, '_attach_mapped_block_devices')",
            "        self.mox.StubOutWithMock(self._vmops, '_start')",
            "",
            "        vm_utils.lookup(self._session, 'foo-orig').AndReturn(",
            "            backup_made and 'foo' or None)",
            "        vm_utils.lookup(self._session, 'foo').AndReturn(",
            "            (not backup_made or new_made) and 'foo' or None)",
            "        if backup_made:",
            "            if new_made:",
            "                self._vmops._destroy(instance, 'foo')",
            "            vm_utils.set_vm_name_label(self._session, 'foo', 'foo')",
            "            self._vmops._attach_mapped_block_devices(instance, [])",
            "        self._vmops._start(instance, 'foo')",
            "",
            "        self.mox.ReplayAll()",
            "",
            "        self._vmops.finish_revert_migration(instance, [])",
            "",
            "    def test_finish_revert_migration_after_crash(self):",
            "        self._test_finish_revert_migration_after_crash(True, True)",
            "",
            "    def test_finish_revert_migration_after_crash_before_new(self):",
            "        self._test_finish_revert_migration_after_crash(True, False)",
            "",
            "    def test_finish_revert_migration_after_crash_before_backup(self):",
            "        self._test_finish_revert_migration_after_crash(False, False)",
            "",
            "    def test_determine_vm_mode_returns_xen(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "",
            "        fake_instance = \"instance\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(vm_mode.XEN)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.XEN,",
            "            self._vmops._determine_vm_mode(fake_instance, None, None))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_hvm(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "",
            "        fake_instance = \"instance\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(vm_mode.HVM)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.HVM,",
            "            self._vmops._determine_vm_mode(fake_instance, None, None))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_is_pv(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_is_pv')",
            "",
            "        fake_instance = {\"os_type\": \"foo\"}",
            "        fake_vdis = {'root': {\"ref\": 'fake'}}",
            "        fake_disk_type = \"disk\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(None)",
            "        vm_utils.determine_is_pv(self._session, \"fake\", fake_disk_type,",
            "            \"foo\").AndReturn(True)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.XEN,",
            "            self._vmops._determine_vm_mode(fake_instance, fake_vdis,",
            "                                     fake_disk_type))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_is_not_pv(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_is_pv')",
            "",
            "        fake_instance = {\"os_type\": \"foo\"}",
            "        fake_vdis = {'root': {\"ref\": 'fake'}}",
            "        fake_disk_type = \"disk\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(None)",
            "        vm_utils.determine_is_pv(self._session, \"fake\", fake_disk_type,",
            "            \"foo\").AndReturn(False)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.HVM,",
            "            self._vmops._determine_vm_mode(fake_instance, fake_vdis,",
            "                                     fake_disk_type))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_determine_vm_mode_returns_is_not_pv_no_root_disk(self):",
            "        self.mox.StubOutWithMock(vm_mode, 'get_from_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_is_pv')",
            "",
            "        fake_instance = {\"os_type\": \"foo\"}",
            "        fake_vdis = {'iso': {\"ref\": 'fake'}}",
            "        fake_disk_type = \"disk\"",
            "        vm_mode.get_from_instance(fake_instance).AndReturn(None)",
            "",
            "        self.mox.ReplayAll()",
            "        self.assertEquals(vm_mode.HVM,",
            "            self._vmops._determine_vm_mode(fake_instance, fake_vdis,",
            "                                     fake_disk_type))",
            "        self.mox.VerifyAll()",
            "",
            "    def test_xsm_sr_check_relaxed_cached(self):",
            "        self.make_plugin_call_count = 0",
            "",
            "        def fake_make_plugin_call(plugin, method, **args):",
            "            self.make_plugin_call_count = self.make_plugin_call_count + 1",
            "            return \"true\"",
            "",
            "        self.stubs.Set(self._vmops, \"_make_plugin_call\",",
            "                       fake_make_plugin_call)",
            "",
            "        self.assertTrue(self._vmops._is_xsm_sr_check_relaxed())",
            "        self.assertTrue(self._vmops._is_xsm_sr_check_relaxed())",
            "",
            "        self.assertEqual(self.make_plugin_call_count, 1)",
            "",
            "",
            "class InjectAutoDiskConfigTestCase(VMOpsTestBase):",
            "    def setUp(self):",
            "        super(InjectAutoDiskConfigTestCase, self).setUp()",
            "",
            "    def test_inject_auto_disk_config_when_present(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"1234\", \"auto_disk_config\": True}",
            "        self.vmops._inject_auto_disk_config(instance, vm_ref)",
            "        xenstore_data = vm['xenstore_data']",
            "        self.assertEquals(xenstore_data['vm-data/auto-disk-config'], 'True')",
            "",
            "    def test_inject_auto_disk_config_none_as_false(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"1234\", \"auto_disk_config\": None}",
            "        self.vmops._inject_auto_disk_config(instance, vm_ref)",
            "        xenstore_data = vm['xenstore_data']",
            "        self.assertEquals(xenstore_data['vm-data/auto-disk-config'], 'False')",
            "",
            "",
            "class GetConsoleOutputTestCase(VMOpsTestBase):",
            "    def setUp(self):",
            "        super(GetConsoleOutputTestCase, self).setUp()",
            "",
            "    def test_get_console_output_works(self):",
            "        self.mox.StubOutWithMock(self.vmops, '_get_dom_id')",
            "",
            "        instance = {\"name\": \"dummy\"}",
            "        self.vmops._get_dom_id(instance, check_rescue=True).AndReturn(42)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertEqual(\"dom_id: 42\", self.vmops.get_console_output(instance))",
            "",
            "    def test_get_console_output_throws_nova_exception(self):",
            "        self.mox.StubOutWithMock(self.vmops, '_get_dom_id')",
            "",
            "        instance = {\"name\": \"dummy\"}",
            "        # dom_id=0 used to trigger exception in fake XenAPI",
            "        self.vmops._get_dom_id(instance, check_rescue=True).AndReturn(0)",
            "        self.mox.ReplayAll()",
            "",
            "        self.assertRaises(exception.NovaException,",
            "                self.vmops.get_console_output, instance)",
            "",
            "    def test_get_dom_id_works(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        self.assertEqual(vm[\"domid\"], self.vmops._get_dom_id(instance))",
            "",
            "    def test_get_dom_id_works_with_rescue_vm(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        vm, vm_ref = self.create_vm(\"dummy-rescue\")",
            "        self.assertEqual(vm[\"domid\"],",
            "                self.vmops._get_dom_id(instance, check_rescue=True))",
            "",
            "    def test_get_dom_id_raises_not_found(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        self.create_vm(\"not-dummy\")",
            "        self.assertRaises(exception.NotFound, self.vmops._get_dom_id, instance)",
            "",
            "    def test_get_dom_id_works_with_vmref(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        self.assertEqual(vm[\"domid\"],",
            "                         self.vmops._get_dom_id(vm_ref=vm_ref))",
            "",
            "",
            "class SpawnTestCase(VMOpsTestBase):",
            "    def _stub_out_common(self):",
            "        self.mox.StubOutWithMock(self.vmops, '_ensure_instance_name_unique')",
            "        self.mox.StubOutWithMock(self.vmops, '_ensure_enough_free_mem')",
            "        self.mox.StubOutWithMock(self.vmops, '_update_instance_progress')",
            "        self.mox.StubOutWithMock(vm_utils, 'determine_disk_image_type')",
            "        self.mox.StubOutWithMock(vm_utils, 'get_vdis_for_instance')",
            "        self.mox.StubOutWithMock(vm_utils, 'safe_destroy_vdis')",
            "        self.mox.StubOutWithMock(self.vmops, '_resize_up_root_vdi')",
            "        self.mox.StubOutWithMock(vm_utils,",
            "                                 'create_kernel_and_ramdisk')",
            "        self.mox.StubOutWithMock(vm_utils, 'destroy_kernel_ramdisk')",
            "        self.mox.StubOutWithMock(self.vmops, '_create_vm_record')",
            "        self.mox.StubOutWithMock(self.vmops, '_destroy')",
            "        self.mox.StubOutWithMock(self.vmops, '_attach_disks')",
            "        self.mox.StubOutWithMock(self.vmops, '_attach_orig_disk_for_rescue')",
            "        self.mox.StubOutWithMock(self.vmops, 'inject_network_info')",
            "        self.mox.StubOutWithMock(self.vmops, '_inject_hostname')",
            "        self.mox.StubOutWithMock(self.vmops, '_inject_instance_metadata')",
            "        self.mox.StubOutWithMock(self.vmops, '_inject_auto_disk_config')",
            "        self.mox.StubOutWithMock(self.vmops, '_file_inject_vm_settings')",
            "        self.mox.StubOutWithMock(self.vmops, '_create_vifs')",
            "        self.mox.StubOutWithMock(self.vmops.firewall_driver,",
            "                                 'setup_basic_filtering')",
            "        self.mox.StubOutWithMock(self.vmops.firewall_driver,",
            "                                 'prepare_instance_filter')",
            "        self.mox.StubOutWithMock(self.vmops, '_start')",
            "        self.mox.StubOutWithMock(self.vmops, '_wait_for_instance_to_start')",
            "        self.mox.StubOutWithMock(self.vmops,",
            "                                 '_configure_new_instance_with_agent')",
            "        self.mox.StubOutWithMock(self.vmops, '_remove_hostname')",
            "        self.mox.StubOutWithMock(self.vmops.firewall_driver,",
            "                                 'apply_instance_filter')",
            "",
            "    def _test_spawn(self, name_label_param=None, block_device_info_param=None,",
            "                    rescue=False, include_root_vdi=True,",
            "                    throw_exception=None):",
            "        self._stub_out_common()",
            "",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"fake_uuid\"}",
            "        name_label = name_label_param",
            "        if name_label is None:",
            "            name_label = \"dummy\"",
            "        image_meta = {\"id\": \"image_id\"}",
            "        context = \"context\"",
            "        session = self.vmops._session",
            "        injected_files = \"fake_files\"",
            "        admin_password = \"password\"",
            "        network_info = \"net_info\"",
            "        steps = 10",
            "        block_device_info = block_device_info_param",
            "        if block_device_info and not block_device_info['root_device_name']:",
            "            block_device_info = dict(block_device_info_param)",
            "            block_device_info['root_device_name'] = \\",
            "                                                self.vmops.default_root_dev",
            "",
            "        di_type = \"di_type\"",
            "        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)",
            "        self.vmops._update_instance_progress(context, instance, 1, steps)",
            "",
            "        vdis = {\"other\": {\"ref\": \"fake_ref_2\", \"osvol\": True}}",
            "        if include_root_vdi:",
            "            vdis[\"root\"] = {\"ref\": \"fake_ref\"}",
            "        vm_utils.get_vdis_for_instance(context, session, instance, name_label,",
            "                    \"image_id\", di_type,",
            "                    block_device_info=block_device_info).AndReturn(vdis)",
            "        if include_root_vdi:",
            "            self.vmops._resize_up_root_vdi(instance, vdis[\"root\"])",
            "        self.vmops._update_instance_progress(context, instance, 2, steps)",
            "",
            "        kernel_file = \"kernel\"",
            "        ramdisk_file = \"ramdisk\"",
            "        vm_utils.create_kernel_and_ramdisk(context, session,",
            "                instance, name_label).AndReturn((kernel_file, ramdisk_file))",
            "        self.vmops._update_instance_progress(context, instance, 3, steps)",
            "",
            "        vm_ref = \"fake_vm_ref\"",
            "        self.vmops._ensure_instance_name_unique(name_label)",
            "        self.vmops._ensure_enough_free_mem(instance)",
            "        self.vmops._create_vm_record(context, instance, name_label, vdis,",
            "                di_type, kernel_file, ramdisk_file).AndReturn(vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 4, steps)",
            "",
            "        self.vmops._attach_disks(instance, vm_ref, name_label, vdis, di_type,",
            "                          admin_password, injected_files)",
            "        if rescue:",
            "            self.vmops._attach_orig_disk_for_rescue(instance, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 5, steps)",
            "",
            "        self.vmops._inject_instance_metadata(instance, vm_ref)",
            "        self.vmops._inject_auto_disk_config(instance, vm_ref)",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue)",
            "        self.vmops._file_inject_vm_settings(instance, vm_ref, vdis,",
            "                                            network_info)",
            "        self.vmops.inject_network_info(instance, network_info, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 6, steps)",
            "",
            "        self.vmops._create_vifs(instance, vm_ref, network_info)",
            "        self.vmops.firewall_driver.setup_basic_filtering(instance,",
            "                network_info).AndRaise(NotImplementedError)",
            "        self.vmops.firewall_driver.prepare_instance_filter(instance,",
            "                                                           network_info)",
            "        self.vmops._update_instance_progress(context, instance, 7, steps)",
            "",
            "        self.vmops._start(instance, vm_ref)",
            "        self.vmops._wait_for_instance_to_start(instance, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 8, steps)",
            "",
            "        self.vmops._configure_new_instance_with_agent(instance, vm_ref,",
            "                injected_files, admin_password)",
            "        self.vmops._remove_hostname(instance, vm_ref)",
            "        self.vmops._update_instance_progress(context, instance, 9, steps)",
            "",
            "        self.vmops.firewall_driver.apply_instance_filter(instance,",
            "                                                         network_info)",
            "        last_call = self.vmops._update_instance_progress(context, instance,",
            "                                                         steps, steps)",
            "        if throw_exception:",
            "            last_call.AndRaise(throw_exception)",
            "            self.vmops._destroy(instance, vm_ref, network_info=network_info)",
            "            vm_utils.destroy_kernel_ramdisk(self.vmops._session, instance,",
            "                                            kernel_file, ramdisk_file)",
            "            vm_utils.safe_destroy_vdis(self.vmops._session, [\"fake_ref\"])",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops.spawn(context, instance, image_meta, injected_files,",
            "                         admin_password, network_info,",
            "                         block_device_info_param, name_label_param, rescue)",
            "",
            "    def test_spawn(self):",
            "        self._test_spawn()",
            "",
            "    def test_spawn_with_alternate_options(self):",
            "        self._test_spawn(include_root_vdi=False, rescue=True,",
            "                         name_label_param=\"bob\",",
            "                         block_device_info_param={\"root_device_name\": \"\"})",
            "",
            "    def test_spawn_performs_rollback_and_throws_exception(self):",
            "        self.assertRaises(test.TestingException, self._test_spawn,",
            "                          throw_exception=test.TestingException())",
            "",
            "    def _test_finish_migration(self, power_on=True, resize_instance=True,",
            "                               throw_exception=None):",
            "        self._stub_out_common()",
            "        self.mox.StubOutWithMock(vm_utils, \"move_disks\")",
            "        self.mox.StubOutWithMock(self.vmops, \"_attach_mapped_block_devices\")",
            "",
            "        context = \"context\"",
            "        migration = {}",
            "        name_label = \"dummy\"",
            "        instance = {\"name\": name_label, \"uuid\": \"fake_uuid\"}",
            "        disk_info = \"disk_info\"",
            "        network_info = \"net_info\"",
            "        image_meta = {\"id\": \"image_id\"}",
            "        block_device_info = \"bdi\"",
            "        session = self.vmops._session",
            "",
            "        self.vmops._ensure_instance_name_unique(name_label)",
            "        self.vmops._ensure_enough_free_mem(instance)",
            "",
            "        di_type = \"di_type\"",
            "        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)",
            "",
            "        root_vdi = {\"ref\": \"fake_ref\"}",
            "        vdis = {\"root\": root_vdi}",
            "        vm_utils.move_disks(self.vmops._session, instance,",
            "                            disk_info).AndReturn(root_vdi)",
            "",
            "        kernel_file = \"kernel\"",
            "        ramdisk_file = \"ramdisk\"",
            "        vm_utils.create_kernel_and_ramdisk(context, session,",
            "                instance, name_label).AndReturn((kernel_file, ramdisk_file))",
            "",
            "        vm_ref = \"fake_vm_ref\"",
            "        self.vmops._create_vm_record(context, instance, name_label, vdis,",
            "                di_type, kernel_file, ramdisk_file).AndReturn(vm_ref)",
            "",
            "        if resize_instance:",
            "            self.vmops._resize_up_root_vdi(instance, root_vdi)",
            "        self.vmops._attach_disks(instance, vm_ref, name_label, vdis, di_type,",
            "                                 None, None)",
            "        self.vmops._attach_mapped_block_devices(instance, block_device_info)",
            "",
            "        self.vmops._inject_instance_metadata(instance, vm_ref)",
            "        self.vmops._inject_auto_disk_config(instance, vm_ref)",
            "        self.vmops._file_inject_vm_settings(instance, vm_ref, vdis,",
            "                                            network_info)",
            "        self.vmops.inject_network_info(instance, network_info, vm_ref)",
            "",
            "        self.vmops._create_vifs(instance, vm_ref, network_info)",
            "        self.vmops.firewall_driver.setup_basic_filtering(instance,",
            "                network_info).AndRaise(NotImplementedError)",
            "        self.vmops.firewall_driver.prepare_instance_filter(instance,",
            "                                                           network_info)",
            "",
            "        if power_on:",
            "            self.vmops._start(instance, vm_ref)",
            "            self.vmops._wait_for_instance_to_start(instance, vm_ref)",
            "",
            "        self.vmops.firewall_driver.apply_instance_filter(instance,",
            "                                                         network_info)",
            "",
            "        last_call = self.vmops._update_instance_progress(context, instance,",
            "                                                        step=5, total_steps=5)",
            "        if throw_exception:",
            "            last_call.AndRaise(throw_exception)",
            "            self.vmops._destroy(instance, vm_ref, network_info=network_info)",
            "            vm_utils.destroy_kernel_ramdisk(self.vmops._session, instance,",
            "                                            kernel_file, ramdisk_file)",
            "            vm_utils.safe_destroy_vdis(self.vmops._session, [\"fake_ref\"])",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops.finish_migration(context, migration, instance, disk_info,",
            "                                    network_info, image_meta, resize_instance,",
            "                                    block_device_info, power_on)",
            "",
            "    def test_finish_migration(self):",
            "        self._test_finish_migration()",
            "",
            "    def test_finish_migration_no_power_on(self):",
            "        self._test_finish_migration(power_on=False, resize_instance=False)",
            "",
            "    def test_finish_migrate_performs_rollback_on_error(self):",
            "        self.assertRaises(test.TestingException, self._test_finish_migration,",
            "                          power_on=False, resize_instance=False,",
            "                          throw_exception=test.TestingException())",
            "",
            "    def test_remove_hostname(self):",
            "        vm, vm_ref = self.create_vm(\"dummy\")",
            "        instance = {\"name\": \"dummy\", \"uuid\": \"1234\", \"auto_disk_config\": None}",
            "        self.mox.StubOutWithMock(self._session, 'call_xenapi')",
            "        self._session.call_xenapi(\"VM.remove_from_xenstore_data\", vm_ref,",
            "                                  \"vm-data/hostname\")",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._remove_hostname(instance, vm_ref)",
            "        self.mox.VerifyAll()",
            "",
            "    def test_inject_hostname(self):",
            "        instance = {\"hostname\": \"dummy\", \"os_type\": \"fake\", \"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')",
            "        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname', 'dummy')",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue=False)",
            "",
            "    def test_inject_hostname_with_rescue_prefix(self):",
            "        instance = {\"hostname\": \"dummy\", \"os_type\": \"fake\", \"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')",
            "        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname',",
            "                                          'RESCUE-dummy')",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue=True)",
            "",
            "    def test_inject_hostname_with_windows_name_truncation(self):",
            "        instance = {\"hostname\": \"dummydummydummydummydummy\",",
            "                    \"os_type\": \"windows\", \"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')",
            "        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname',",
            "                                          'RESCUE-dummydum')",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._inject_hostname(instance, vm_ref, rescue=True)",
            "",
            "    def test_wait_for_instance_to_start(self):",
            "        instance = {\"uuid\": \"uuid\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(self.vmops, 'get_info')",
            "        self.vmops.get_info(instance, vm_ref).AndReturn({\"state\": \"asdf\"})",
            "        self.vmops.get_info(instance, vm_ref).AndReturn({",
            "                                            \"state\": power_state.RUNNING})",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._wait_for_instance_to_start(instance, vm_ref)",
            "",
            "    def test_attach_orig_disk_for_rescue(self):",
            "        instance = {\"name\": \"dummy\"}",
            "        vm_ref = \"vm_ref\"",
            "",
            "        self.mox.StubOutWithMock(vm_utils, 'lookup')",
            "        self.mox.StubOutWithMock(self.vmops, '_find_root_vdi_ref')",
            "        self.mox.StubOutWithMock(vm_utils, 'create_vbd')",
            "",
            "        vm_utils.lookup(self.vmops._session, \"dummy\").AndReturn(\"ref\")",
            "        self.vmops._find_root_vdi_ref(\"ref\").AndReturn(\"vdi_ref\")",
            "        vm_utils.create_vbd(self.vmops._session, vm_ref, \"vdi_ref\",",
            "                            vmops.DEVICE_RESCUE, bootable=False)",
            "",
            "        self.mox.ReplayAll()",
            "        self.vmops._attach_orig_disk_for_rescue(instance, vm_ref)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "0",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "407": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "419": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "423": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "428": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "429": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "435": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "436": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "437": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "438": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "443": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "447": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "449": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "451": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "453": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "455": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "456": [
                "SpawnTestCase",
                "test_finish_migration"
            ],
            "461": [
                "SpawnTestCase",
                "test_finish_migration"
            ]
        },
        "addLocation": []
    },
    "nova/virt/xenapi/vmops.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": 274,
                "PatchRowcode": "     def finish_migration(self, context, migration, instance, disk_info,"
            },
            "1": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 275,
                "PatchRowcode": "                          network_info, image_meta, resize_instance,"
            },
            "2": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 276,
                "PatchRowcode": "                          block_device_info=None, power_on=True):"
            },
            "3": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        root_vdi = vm_utils.move_disks(self._session, instance, disk_info)"
            },
            "4": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        vdis = {'root': root_vdi}"
            },
            "5": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 277,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if resize_instance:"
            },
            "7": {
                "beforePatchRowNumber": 281,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._resize_up_root_vdi(instance, root_vdi)"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 278,
                "PatchRowcode": "+        def null_step_decorator(f):"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 279,
                "PatchRowcode": "+            return f"
            },
            "10": {
                "beforePatchRowNumber": 282,
                "afterPatchRowNumber": 280,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 283,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        name_label = instance['name']"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 281,
                "PatchRowcode": "+        def create_disks_step(undo_mgr, disk_image_type, image_meta,"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 282,
                "PatchRowcode": "+                              name_label):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 283,
                "PatchRowcode": "+            #TODO(johngarbutt) clean up the move_disks if this is not run"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 284,
                "PatchRowcode": "+            root_vdi = vm_utils.move_disks(self._session, instance, disk_info)"
            },
            "16": {
                "beforePatchRowNumber": 284,
                "afterPatchRowNumber": 285,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 285,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        kernel_file, ramdisk_file = vm_utils.create_kernel_and_ramdisk("
            },
            "18": {
                "beforePatchRowNumber": 286,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                context, self._session, instance, name_label)"
            },
            "19": {
                "beforePatchRowNumber": 287,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "20": {
                "beforePatchRowNumber": 288,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        disk_image_type = vm_utils.determine_disk_image_type(image_meta)"
            },
            "21": {
                "beforePatchRowNumber": 289,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._ensure_instance_name_unique(name_label)"
            },
            "22": {
                "beforePatchRowNumber": 290,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._ensure_enough_free_mem(instance)"
            },
            "23": {
                "beforePatchRowNumber": 291,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        vm_ref = self._create_vm_record(context, instance, name_label,"
            },
            "24": {
                "beforePatchRowNumber": 292,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                vdis, disk_image_type, kernel_file, ramdisk_file)"
            },
            "25": {
                "beforePatchRowNumber": 293,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "26": {
                "beforePatchRowNumber": 294,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._attach_disks(instance, vm_ref, name_label, vdis,"
            },
            "27": {
                "beforePatchRowNumber": 295,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                           disk_image_type)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 286,
                "PatchRowcode": "+            def undo_create_disks():"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 287,
                "PatchRowcode": "+                vm_utils.safe_destroy_vdis(self._session, [root_vdi['ref']])"
            },
            "30": {
                "beforePatchRowNumber": 296,
                "afterPatchRowNumber": 288,
                "PatchRowcode": " "
            },
            "31": {
                "beforePatchRowNumber": 297,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._file_inject_vm_settings(instance, vm_ref, vdis, network_info)"
            },
            "32": {
                "beforePatchRowNumber": 298,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._create_vifs(instance, vm_ref, network_info)"
            },
            "33": {
                "beforePatchRowNumber": 299,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.inject_network_info(instance, network_info, vm_ref)"
            },
            "34": {
                "beforePatchRowNumber": 300,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._inject_instance_metadata(instance, vm_ref)"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 289,
                "PatchRowcode": "+            undo_mgr.undo_with(undo_create_disks)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 290,
                "PatchRowcode": "+            return {'root': root_vdi}"
            },
            "37": {
                "beforePatchRowNumber": 301,
                "afterPatchRowNumber": 291,
                "PatchRowcode": " "
            },
            "38": {
                "beforePatchRowNumber": 302,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._attach_mapped_block_devices(instance, block_device_info)"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 292,
                "PatchRowcode": "+        def completed_callback():"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 293,
                "PatchRowcode": "+            self._update_instance_progress(context, instance,"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 294,
                "PatchRowcode": "+                                           step=5,"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 295,
                "PatchRowcode": "+                                           total_steps=RESIZE_TOTAL_STEPS)"
            },
            "43": {
                "beforePatchRowNumber": 303,
                "afterPatchRowNumber": 296,
                "PatchRowcode": " "
            },
            "44": {
                "beforePatchRowNumber": 304,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # 5. Start VM"
            },
            "45": {
                "beforePatchRowNumber": 305,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if power_on:"
            },
            "46": {
                "beforePatchRowNumber": 306,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._start(instance, vm_ref)"
            },
            "47": {
                "beforePatchRowNumber": 307,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._update_instance_progress(context, instance,"
            },
            "48": {
                "beforePatchRowNumber": 308,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                       step=5,"
            },
            "49": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                       total_steps=RESIZE_TOTAL_STEPS)"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 297,
                "PatchRowcode": "+        self._spawn(context, instance, image_meta, null_step_decorator,"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 298,
                "PatchRowcode": "+                    create_disks_step, first_boot=False, injected_files=None,"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 299,
                "PatchRowcode": "+                    admin_password=None, network_info=network_info,"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 300,
                "PatchRowcode": "+                    block_device_info=block_device_info, name_label=None,"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 301,
                "PatchRowcode": "+                    rescue=False, power_on=power_on, resize=resize_instance,"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 302,
                "PatchRowcode": "+                    completed_callback=completed_callback)"
            },
            "56": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": 303,
                "PatchRowcode": " "
            },
            "57": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 304,
                "PatchRowcode": "     def _start(self, instance, vm_ref=None, bad_volumes_callback=None):"
            },
            "58": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 305,
                "PatchRowcode": "         \"\"\"Power on a VM instance.\"\"\""
            },
            "59": {
                "beforePatchRowNumber": 336,
                "afterPatchRowNumber": 329,
                "PatchRowcode": "     def spawn(self, context, instance, image_meta, injected_files,"
            },
            "60": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "               admin_password, network_info=None, block_device_info=None,"
            },
            "61": {
                "beforePatchRowNumber": 338,
                "afterPatchRowNumber": 331,
                "PatchRowcode": "               name_label=None, rescue=False):"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+"
            },
            "63": {
                "beforePatchRowNumber": 339,
                "afterPatchRowNumber": 333,
                "PatchRowcode": "         if block_device_info:"
            },
            "64": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": 334,
                "PatchRowcode": "             LOG.debug(_(\"Block device information present: %s\")"
            },
            "65": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": 335,
                "PatchRowcode": "                       % block_device_info, instance=instance)"
            },
            "66": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "         step = make_step_decorator(context, instance,"
            },
            "67": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 340,
                "PatchRowcode": "                                    self._update_instance_progress)"
            },
            "68": {
                "beforePatchRowNumber": 347,
                "afterPatchRowNumber": 341,
                "PatchRowcode": " "
            },
            "69": {
                "beforePatchRowNumber": 348,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if name_label is None:"
            },
            "70": {
                "beforePatchRowNumber": 349,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            name_label = instance['name']"
            },
            "71": {
                "beforePatchRowNumber": 350,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "72": {
                "beforePatchRowNumber": 351,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._ensure_instance_name_unique(name_label)"
            },
            "73": {
                "beforePatchRowNumber": 352,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self._ensure_enough_free_mem(instance)"
            },
            "74": {
                "beforePatchRowNumber": 353,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "75": {
                "beforePatchRowNumber": 354,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        @step"
            },
            "76": {
                "beforePatchRowNumber": 355,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        def determine_disk_image_type_step(undo_mgr):"
            },
            "77": {
                "beforePatchRowNumber": 356,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return vm_utils.determine_disk_image_type(image_meta)"
            },
            "78": {
                "beforePatchRowNumber": 357,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "79": {
                "beforePatchRowNumber": 358,
                "afterPatchRowNumber": 342,
                "PatchRowcode": "         @step"
            },
            "80": {
                "beforePatchRowNumber": 359,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        def create_disks_step(undo_mgr, disk_image_type, image_meta):"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 343,
                "PatchRowcode": "+        def create_disks_step(undo_mgr, disk_image_type, image_meta,"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 344,
                "PatchRowcode": "+                              name_label):"
            },
            "83": {
                "beforePatchRowNumber": 360,
                "afterPatchRowNumber": 345,
                "PatchRowcode": "             vdis = vm_utils.get_vdis_for_instance(context, self._session,"
            },
            "84": {
                "beforePatchRowNumber": 361,
                "afterPatchRowNumber": 346,
                "PatchRowcode": "                         instance, name_label, image_meta.get('id'),"
            },
            "85": {
                "beforePatchRowNumber": 362,
                "afterPatchRowNumber": 347,
                "PatchRowcode": "                         disk_image_type, block_device_info=block_device_info)"
            },
            "86": {
                "beforePatchRowNumber": 369,
                "afterPatchRowNumber": 354,
                "PatchRowcode": "             undo_mgr.undo_with(undo_create_disks)"
            },
            "87": {
                "beforePatchRowNumber": 370,
                "afterPatchRowNumber": 355,
                "PatchRowcode": "             return vdis"
            },
            "88": {
                "beforePatchRowNumber": 371,
                "afterPatchRowNumber": 356,
                "PatchRowcode": " "
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 357,
                "PatchRowcode": "+        self._spawn(context, instance, image_meta, step, create_disks_step,"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 358,
                "PatchRowcode": "+                    True, injected_files, admin_password,"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 359,
                "PatchRowcode": "+                    network_info, block_device_info, name_label, rescue)"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 360,
                "PatchRowcode": "+"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 361,
                "PatchRowcode": "+    def _spawn(self, context, instance, image_meta, step, create_disks_step,"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 362,
                "PatchRowcode": "+               first_boot, injected_files=None, admin_password=None,"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 363,
                "PatchRowcode": "+               network_info=None, block_device_info=None,"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 364,
                "PatchRowcode": "+               name_label=None, rescue=False, power_on=True, resize=True,"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 365,
                "PatchRowcode": "+               completed_callback=None):"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 366,
                "PatchRowcode": "+        if name_label is None:"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 367,
                "PatchRowcode": "+            name_label = instance['name']"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 368,
                "PatchRowcode": "+"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 369,
                "PatchRowcode": "+        self._ensure_instance_name_unique(name_label)"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 370,
                "PatchRowcode": "+        self._ensure_enough_free_mem(instance)"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 371,
                "PatchRowcode": "+"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 372,
                "PatchRowcode": "+        @step"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 373,
                "PatchRowcode": "+        def determine_disk_image_type_step(undo_mgr):"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 374,
                "PatchRowcode": "+            return vm_utils.determine_disk_image_type(image_meta)"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 375,
                "PatchRowcode": "+"
            },
            "108": {
                "beforePatchRowNumber": 372,
                "afterPatchRowNumber": 376,
                "PatchRowcode": "         @step"
            },
            "109": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 377,
                "PatchRowcode": "         def create_kernel_ramdisk_step(undo_mgr):"
            },
            "110": {
                "beforePatchRowNumber": 374,
                "afterPatchRowNumber": 378,
                "PatchRowcode": "             kernel_file, ramdisk_file = vm_utils.create_kernel_and_ramdisk("
            },
            "111": {
                "beforePatchRowNumber": 410,
                "afterPatchRowNumber": 414,
                "PatchRowcode": "                                 instance=instance)"
            },
            "112": {
                "beforePatchRowNumber": 411,
                "afterPatchRowNumber": 415,
                "PatchRowcode": " "
            },
            "113": {
                "beforePatchRowNumber": 412,
                "afterPatchRowNumber": 416,
                "PatchRowcode": "             root_vdi = vdis.get('root')"
            },
            "114": {
                "beforePatchRowNumber": 413,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if root_vdi:"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 417,
                "PatchRowcode": "+            if root_vdi and resize:"
            },
            "116": {
                "beforePatchRowNumber": 414,
                "afterPatchRowNumber": 418,
                "PatchRowcode": "                 self._resize_up_root_vdi(instance, root_vdi)"
            },
            "117": {
                "beforePatchRowNumber": 415,
                "afterPatchRowNumber": 419,
                "PatchRowcode": " "
            },
            "118": {
                "beforePatchRowNumber": 416,
                "afterPatchRowNumber": 420,
                "PatchRowcode": "             self._attach_disks(instance, vm_ref, name_label, vdis,"
            },
            "119": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 421,
                "PatchRowcode": "                                disk_image_type, admin_password,"
            },
            "120": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 422,
                "PatchRowcode": "                                injected_files)"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 423,
                "PatchRowcode": "+            if not first_boot:"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 424,
                "PatchRowcode": "+                self._attach_mapped_block_devices(instance,"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 425,
                "PatchRowcode": "+                                                  block_device_info)"
            },
            "124": {
                "beforePatchRowNumber": 419,
                "afterPatchRowNumber": 426,
                "PatchRowcode": " "
            },
            "125": {
                "beforePatchRowNumber": 420,
                "afterPatchRowNumber": 427,
                "PatchRowcode": "             if rescue:"
            },
            "126": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 428,
                "PatchRowcode": "                 # NOTE(johannes): Attach root disk to rescue VM now, before"
            },
            "127": {
                "beforePatchRowNumber": 427,
                "afterPatchRowNumber": 434,
                "PatchRowcode": "         def inject_instance_data_step(undo_mgr, vm_ref, vdis):"
            },
            "128": {
                "beforePatchRowNumber": 428,
                "afterPatchRowNumber": 435,
                "PatchRowcode": "             self._inject_instance_metadata(instance, vm_ref)"
            },
            "129": {
                "beforePatchRowNumber": 429,
                "afterPatchRowNumber": 436,
                "PatchRowcode": "             self._inject_auto_disk_config(instance, vm_ref)"
            },
            "130": {
                "beforePatchRowNumber": 430,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._inject_hostname(instance, vm_ref, rescue)"
            },
            "131": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 437,
                "PatchRowcode": "+            if first_boot:"
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 438,
                "PatchRowcode": "+                self._inject_hostname(instance, vm_ref, rescue)"
            },
            "133": {
                "beforePatchRowNumber": 431,
                "afterPatchRowNumber": 439,
                "PatchRowcode": "             self._file_inject_vm_settings(instance, vm_ref, vdis, network_info)"
            },
            "134": {
                "beforePatchRowNumber": 432,
                "afterPatchRowNumber": 440,
                "PatchRowcode": "             self.inject_network_info(instance, network_info, vm_ref)"
            },
            "135": {
                "beforePatchRowNumber": 433,
                "afterPatchRowNumber": 441,
                "PatchRowcode": " "
            },
            "136": {
                "beforePatchRowNumber": 449,
                "afterPatchRowNumber": 457,
                "PatchRowcode": " "
            },
            "137": {
                "beforePatchRowNumber": 450,
                "afterPatchRowNumber": 458,
                "PatchRowcode": "         @step"
            },
            "138": {
                "beforePatchRowNumber": 451,
                "afterPatchRowNumber": 459,
                "PatchRowcode": "         def boot_instance_step(undo_mgr, vm_ref):"
            },
            "139": {
                "beforePatchRowNumber": 452,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._start(instance, vm_ref)"
            },
            "140": {
                "beforePatchRowNumber": 453,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._wait_for_instance_to_start(instance, vm_ref)"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 460,
                "PatchRowcode": "+            if power_on:"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 461,
                "PatchRowcode": "+                self._start(instance, vm_ref)"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 462,
                "PatchRowcode": "+                self._wait_for_instance_to_start(instance, vm_ref)"
            },
            "144": {
                "beforePatchRowNumber": 454,
                "afterPatchRowNumber": 463,
                "PatchRowcode": " "
            },
            "145": {
                "beforePatchRowNumber": 455,
                "afterPatchRowNumber": 464,
                "PatchRowcode": "         @step"
            },
            "146": {
                "beforePatchRowNumber": 456,
                "afterPatchRowNumber": 465,
                "PatchRowcode": "         def configure_booted_instance_step(undo_mgr, vm_ref):"
            },
            "147": {
                "beforePatchRowNumber": 457,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._configure_new_instance_with_agent(instance, vm_ref,"
            },
            "148": {
                "beforePatchRowNumber": 458,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    injected_files, admin_password)"
            },
            "149": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self._remove_hostname(instance, vm_ref)"
            },
            "150": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 466,
                "PatchRowcode": "+            if first_boot:"
            },
            "151": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 467,
                "PatchRowcode": "+                self._configure_new_instance_with_agent(instance, vm_ref,"
            },
            "152": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 468,
                "PatchRowcode": "+                        injected_files, admin_password)"
            },
            "153": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 469,
                "PatchRowcode": "+                self._remove_hostname(instance, vm_ref)"
            },
            "154": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": 470,
                "PatchRowcode": " "
            },
            "155": {
                "beforePatchRowNumber": 461,
                "afterPatchRowNumber": 471,
                "PatchRowcode": "         @step"
            },
            "156": {
                "beforePatchRowNumber": 462,
                "afterPatchRowNumber": 472,
                "PatchRowcode": "         def apply_security_group_filters_step(undo_mgr):"
            },
            "157": {
                "beforePatchRowNumber": 471,
                "afterPatchRowNumber": 481,
                "PatchRowcode": "             # first step is something that completes rather quickly."
            },
            "158": {
                "beforePatchRowNumber": 472,
                "afterPatchRowNumber": 482,
                "PatchRowcode": "             disk_image_type = determine_disk_image_type_step(undo_mgr)"
            },
            "159": {
                "beforePatchRowNumber": 473,
                "afterPatchRowNumber": 483,
                "PatchRowcode": " "
            },
            "160": {
                "beforePatchRowNumber": 474,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            vdis = create_disks_step(undo_mgr, disk_image_type, image_meta)"
            },
            "161": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 484,
                "PatchRowcode": "+            vdis = create_disks_step(undo_mgr, disk_image_type, image_meta,"
            },
            "162": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 485,
                "PatchRowcode": "+                                     name_label)"
            },
            "163": {
                "beforePatchRowNumber": 475,
                "afterPatchRowNumber": 486,
                "PatchRowcode": "             kernel_file, ramdisk_file = create_kernel_ramdisk_step(undo_mgr)"
            },
            "164": {
                "beforePatchRowNumber": 476,
                "afterPatchRowNumber": 487,
                "PatchRowcode": " "
            },
            "165": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": 488,
                "PatchRowcode": "             vm_ref = create_vm_record_step(undo_mgr, vdis, disk_image_type,"
            },
            "166": {
                "beforePatchRowNumber": 485,
                "afterPatchRowNumber": 496,
                "PatchRowcode": " "
            },
            "167": {
                "beforePatchRowNumber": 486,
                "afterPatchRowNumber": 497,
                "PatchRowcode": "             configure_booted_instance_step(undo_mgr, vm_ref)"
            },
            "168": {
                "beforePatchRowNumber": 487,
                "afterPatchRowNumber": 498,
                "PatchRowcode": "             apply_security_group_filters_step(undo_mgr)"
            },
            "169": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 499,
                "PatchRowcode": "+"
            },
            "170": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 500,
                "PatchRowcode": "+            if completed_callback:"
            },
            "171": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 501,
                "PatchRowcode": "+                completed_callback()"
            },
            "172": {
                "beforePatchRowNumber": 488,
                "afterPatchRowNumber": 502,
                "PatchRowcode": "         except Exception:"
            },
            "173": {
                "beforePatchRowNumber": 489,
                "afterPatchRowNumber": 503,
                "PatchRowcode": "             msg = _(\"Failed to spawn, rolling back\")"
            },
            "174": {
                "beforePatchRowNumber": 490,
                "afterPatchRowNumber": 504,
                "PatchRowcode": "             undo_mgr.rollback_and_reraise(msg=msg, instance=instance)"
            }
        },
        "frontPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "# Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Management class for VM-related functions (spawn, reboot, etc).",
            "\"\"\"",
            "",
            "import base64",
            "import functools",
            "import itertools",
            "import time",
            "import zlib",
            "",
            "from eventlet import greenthread",
            "import netaddr",
            "from oslo.config import cfg",
            "",
            "from nova import block_device",
            "from nova import compute",
            "from nova.compute import flavors",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_mode",
            "from nova.compute import vm_states",
            "from nova import context as nova_context",
            "from nova import exception",
            "from nova.openstack.common import excutils",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import importutils",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova.openstack.common import strutils",
            "from nova.openstack.common import timeutils",
            "from nova import utils",
            "from nova.virt import configdrive",
            "from nova.virt import driver as virt_driver",
            "from nova.virt import firewall",
            "from nova.virt.xenapi import agent as xapi_agent",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import volume_utils",
            "from nova.virt.xenapi import volumeops",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "xenapi_vmops_opts = [",
            "    cfg.IntOpt('xenapi_running_timeout',",
            "               default=60,",
            "               help='number of seconds to wait for instance '",
            "                    'to go to running state'),",
            "    cfg.StrOpt('xenapi_vif_driver',",
            "               default='nova.virt.xenapi.vif.XenAPIBridgeDriver',",
            "               help='The XenAPI VIF driver using XenServer Network APIs.'),",
            "    cfg.StrOpt('xenapi_image_upload_handler',",
            "                default='nova.virt.xenapi.image.glance.GlanceStore',",
            "                help='Dom0 plugin driver used to handle image uploads.'),",
            "    ]",
            "",
            "CONF = cfg.CONF",
            "CONF.register_opts(xenapi_vmops_opts)",
            "CONF.import_opt('host', 'nova.netconf')",
            "CONF.import_opt('vncserver_proxyclient_address', 'nova.vnc')",
            "",
            "DEFAULT_FIREWALL_DRIVER = \"%s.%s\" % (",
            "    firewall.__name__,",
            "    firewall.IptablesFirewallDriver.__name__)",
            "",
            "RESIZE_TOTAL_STEPS = 5",
            "",
            "DEVICE_ROOT = '0'",
            "DEVICE_RESCUE = '1'",
            "DEVICE_SWAP = '2'",
            "DEVICE_CONFIGDRIVE = '3'",
            "# Note(johngarbutt) HVM guests only support four devices",
            "# until the PV tools activate, when others before available",
            "# As such, ephemeral disk only available once PV tools load",
            "# Note(johngarbutt) When very large ephemeral storage is required,",
            "# multiple disks may be added. In this case the device id below",
            "# is the used for the first disk. The second disk will be given",
            "# next device id, i.e. 5, and so on, until enough space is added.",
            "DEVICE_EPHEMERAL = '4'",
            "# Note(johngarbutt) Currently don't support ISO boot during rescue",
            "# and we must have the ISO visible before the PV drivers start",
            "DEVICE_CD = '1'",
            "",
            "",
            "def cmp_version(a, b):",
            "    \"\"\"Compare two version strings (eg 0.0.1.10 > 0.0.1.9).\"\"\"",
            "    a = a.split('.')",
            "    b = b.split('.')",
            "",
            "    # Compare each individual portion of both version strings",
            "    for va, vb in zip(a, b):",
            "        ret = int(va) - int(vb)",
            "        if ret:",
            "            return ret",
            "",
            "    # Fallback to comparing length last",
            "    return len(a) - len(b)",
            "",
            "",
            "def make_step_decorator(context, instance, update_instance_progress):",
            "    \"\"\"Factory to create a decorator that records instance progress as a series",
            "    of discrete steps.",
            "",
            "    Each time the decorator is invoked we bump the total-step-count, so after::",
            "",
            "        @step",
            "        def step1():",
            "            ...",
            "",
            "        @step",
            "        def step2():",
            "            ...",
            "",
            "    we have a total-step-count of 2.",
            "",
            "    Each time the step-function (not the step-decorator!) is invoked, we bump",
            "    the current-step-count by 1, so after::",
            "",
            "        step1()",
            "",
            "    the current-step-count would be 1 giving a progress of ``1 / 2 *",
            "    100`` or 50%.",
            "    \"\"\"",
            "    step_info = dict(total=0, current=0)",
            "",
            "    def bump_progress():",
            "        step_info['current'] += 1",
            "        update_instance_progress(context, instance,",
            "                                 step_info['current'], step_info['total'])",
            "",
            "    def step_decorator(f):",
            "        step_info['total'] += 1",
            "",
            "        @functools.wraps(f)",
            "        def inner(*args, **kwargs):",
            "            rv = f(*args, **kwargs)",
            "            bump_progress()",
            "            return rv",
            "",
            "        return inner",
            "",
            "    return step_decorator",
            "",
            "",
            "class VMOps(object):",
            "    \"\"\"",
            "    Management class for VM-related tasks",
            "    \"\"\"",
            "    def __init__(self, session, virtapi):",
            "        self.compute_api = compute.API()",
            "        self._session = session",
            "        self._virtapi = virtapi",
            "        self._volumeops = volumeops.VolumeOps(self._session)",
            "        self.firewall_driver = firewall.load_driver(",
            "            DEFAULT_FIREWALL_DRIVER,",
            "            self._virtapi,",
            "            xenapi_session=self._session)",
            "        vif_impl = importutils.import_class(CONF.xenapi_vif_driver)",
            "        self.vif_driver = vif_impl(xenapi_session=self._session)",
            "        self.default_root_dev = '/dev/sda'",
            "",
            "        LOG.debug(_(\"Importing image upload handler: %s\"),",
            "                  CONF.xenapi_image_upload_handler)",
            "        self.image_upload_handler = importutils.import_object(",
            "                                CONF.xenapi_image_upload_handler)",
            "",
            "    def agent_enabled(self, instance):",
            "        if CONF.xenapi_disable_agent:",
            "            return False",
            "",
            "        return xapi_agent.should_use_agent(instance)",
            "",
            "    def _get_agent(self, instance, vm_ref):",
            "        if self.agent_enabled(instance):",
            "            return xapi_agent.XenAPIBasedAgent(self._session, self._virtapi,",
            "                                               instance, vm_ref)",
            "        raise exception.NovaException(_(\"Error: Agent is disabled\"))",
            "",
            "    def instance_exists(self, name_label):",
            "        return vm_utils.lookup(self._session, name_label) is not None",
            "",
            "    def list_instances(self):",
            "        \"\"\"List VM instances.\"\"\"",
            "        # TODO(justinsb): Should we just always use the details method?",
            "        #  Seems to be the same number of API calls..",
            "        name_labels = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            name_labels.append(vm_rec[\"name_label\"])",
            "",
            "        return name_labels",
            "",
            "    def list_instance_uuids(self):",
            "        \"\"\"Get the list of nova instance uuids for VMs found on the",
            "        hypervisor.",
            "        \"\"\"",
            "        nova_uuids = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            other_config = vm_rec['other_config']",
            "            nova_uuid = other_config.get('nova_uuid')",
            "            if nova_uuid:",
            "                nova_uuids.append(nova_uuid)",
            "        return nova_uuids",
            "",
            "    def confirm_migration(self, migration, instance, network_info):",
            "        self._destroy_orig_vm(instance, network_info)",
            "",
            "    def _destroy_orig_vm(self, instance, network_info):",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        return self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "    def _attach_mapped_block_devices(self, instance, block_device_info):",
            "        # We are attaching these volumes before start (no hotplugging)",
            "        # because some guests (windows) don't load PV drivers quickly",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.attach_volume(connection_info,",
            "                                          instance['name'],",
            "                                          mount_device,",
            "                                          hotplug=False)",
            "",
            "    def finish_revert_migration(self, instance, block_device_info=None,",
            "                                power_on=True):",
            "        self._restore_orig_vm_and_cleanup_orphan(instance, block_device_info,",
            "                                                 power_on)",
            "",
            "    def _restore_orig_vm_and_cleanup_orphan(self, instance,",
            "                                            block_device_info, power_on=True):",
            "        # NOTE(sirp): the original vm was suffixed with '-orig'; find it using",
            "        # the old suffix, remove the suffix, then power it back on.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "",
            "        # NOTE(danms): if we're reverting migration in the failure case,",
            "        # make sure we don't have a conflicting vm still running here,",
            "        # as might be the case in a failed migrate-to-same-host situation",
            "        new_ref = vm_utils.lookup(self._session, instance['name'])",
            "        if vm_ref is not None:",
            "            if new_ref is not None:",
            "                self._destroy(instance, new_ref)",
            "            # Remove the '-orig' suffix (which was added in case the",
            "            # resized VM ends up on the source host, common during",
            "            # testing)",
            "            name_label = instance['name']",
            "            vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "            self._attach_mapped_block_devices(instance, block_device_info)",
            "        elif new_ref is not None:",
            "            # We crashed before the -orig backup was made",
            "            vm_ref = new_ref",
            "",
            "        if power_on:",
            "            self._start(instance, vm_ref)",
            "",
            "    def finish_migration(self, context, migration, instance, disk_info,",
            "                         network_info, image_meta, resize_instance,",
            "                         block_device_info=None, power_on=True):",
            "        root_vdi = vm_utils.move_disks(self._session, instance, disk_info)",
            "        vdis = {'root': root_vdi}",
            "",
            "        if resize_instance:",
            "            self._resize_up_root_vdi(instance, root_vdi)",
            "",
            "        name_label = instance['name']",
            "",
            "        kernel_file, ramdisk_file = vm_utils.create_kernel_and_ramdisk(",
            "                context, self._session, instance, name_label)",
            "",
            "        disk_image_type = vm_utils.determine_disk_image_type(image_meta)",
            "        self._ensure_instance_name_unique(name_label)",
            "        self._ensure_enough_free_mem(instance)",
            "        vm_ref = self._create_vm_record(context, instance, name_label,",
            "                vdis, disk_image_type, kernel_file, ramdisk_file)",
            "",
            "        self._attach_disks(instance, vm_ref, name_label, vdis,",
            "                           disk_image_type)",
            "",
            "        self._file_inject_vm_settings(instance, vm_ref, vdis, network_info)",
            "        self._create_vifs(instance, vm_ref, network_info)",
            "        self.inject_network_info(instance, network_info, vm_ref)",
            "        self._inject_instance_metadata(instance, vm_ref)",
            "",
            "        self._attach_mapped_block_devices(instance, block_device_info)",
            "",
            "        # 5. Start VM",
            "        if power_on:",
            "            self._start(instance, vm_ref)",
            "        self._update_instance_progress(context, instance,",
            "                                       step=5,",
            "                                       total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "    def _start(self, instance, vm_ref=None, bad_volumes_callback=None):",
            "        \"\"\"Power on a VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Starting instance\"), instance=instance)",
            "",
            "        # Attached volumes that have become non-responsive will prevent a VM",
            "        # from starting, so scan for these before attempting to start",
            "        #",
            "        # In order to make sure this detach is consistent (virt, BDM, cinder),",
            "        # we only detach in the virt-layer if a callback is provided.",
            "        if bad_volumes_callback:",
            "            bad_devices = self._volumeops.find_bad_volumes(vm_ref)",
            "            for device_name in bad_devices:",
            "                self._volumeops.detach_volume(",
            "                        None, instance['name'], device_name)",
            "",
            "        self._session.call_xenapi('VM.start_on', vm_ref,",
            "                                  self._session.get_xenapi_host(),",
            "                                  False, False)",
            "",
            "        # Allow higher-layers a chance to detach bad-volumes as well (in order",
            "        # to cleanup BDM entries and detach in Cinder)",
            "        if bad_volumes_callback and bad_devices:",
            "            bad_volumes_callback(bad_devices)",
            "",
            "    def spawn(self, context, instance, image_meta, injected_files,",
            "              admin_password, network_info=None, block_device_info=None,",
            "              name_label=None, rescue=False):",
            "        if block_device_info:",
            "            LOG.debug(_(\"Block device information present: %s\")",
            "                      % block_device_info, instance=instance)",
            "        if block_device_info and not block_device_info['root_device_name']:",
            "            block_device_info['root_device_name'] = self.default_root_dev",
            "",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        if name_label is None:",
            "            name_label = instance['name']",
            "",
            "        self._ensure_instance_name_unique(name_label)",
            "        self._ensure_enough_free_mem(instance)",
            "",
            "        @step",
            "        def determine_disk_image_type_step(undo_mgr):",
            "            return vm_utils.determine_disk_image_type(image_meta)",
            "",
            "        @step",
            "        def create_disks_step(undo_mgr, disk_image_type, image_meta):",
            "            vdis = vm_utils.get_vdis_for_instance(context, self._session,",
            "                        instance, name_label, image_meta.get('id'),",
            "                        disk_image_type, block_device_info=block_device_info)",
            "",
            "            def undo_create_disks():",
            "                vdi_refs = [vdi['ref'] for vdi in vdis.values()",
            "                        if not vdi.get('osvol')]",
            "                vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "            undo_mgr.undo_with(undo_create_disks)",
            "            return vdis",
            "",
            "        @step",
            "        def create_kernel_ramdisk_step(undo_mgr):",
            "            kernel_file, ramdisk_file = vm_utils.create_kernel_and_ramdisk(",
            "                    context, self._session, instance, name_label)",
            "",
            "            def undo_create_kernel_ramdisk():",
            "                vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                        kernel_file, ramdisk_file)",
            "",
            "            undo_mgr.undo_with(undo_create_kernel_ramdisk)",
            "            return kernel_file, ramdisk_file",
            "",
            "        @step",
            "        def create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                kernel_file, ramdisk_file):",
            "            vm_ref = self._create_vm_record(context, instance, name_label,",
            "                    vdis, disk_image_type, kernel_file, ramdisk_file)",
            "",
            "            def undo_create_vm():",
            "                self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "            undo_mgr.undo_with(undo_create_vm)",
            "            return vm_ref",
            "",
            "        @step",
            "        def attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type):",
            "            try:",
            "                ipxe_boot = strutils.bool_from_string(",
            "                        image_meta['properties']['ipxe_boot'])",
            "            except KeyError:",
            "                ipxe_boot = False",
            "",
            "            if ipxe_boot:",
            "                if 'iso' in vdis:",
            "                    vm_utils.handle_ipxe_iso(",
            "                        self._session, instance, vdis['iso'], network_info)",
            "                else:",
            "                    LOG.warning(_('ipxe_boot is True but no ISO image found'),",
            "                                instance=instance)",
            "",
            "            root_vdi = vdis.get('root')",
            "            if root_vdi:",
            "                self._resize_up_root_vdi(instance, root_vdi)",
            "",
            "            self._attach_disks(instance, vm_ref, name_label, vdis,",
            "                               disk_image_type, admin_password,",
            "                               injected_files)",
            "",
            "            if rescue:",
            "                # NOTE(johannes): Attach root disk to rescue VM now, before",
            "                # booting the VM, since we can't hotplug block devices",
            "                # on non-PV guests",
            "                self._attach_orig_disk_for_rescue(instance, vm_ref)",
            "",
            "        @step",
            "        def inject_instance_data_step(undo_mgr, vm_ref, vdis):",
            "            self._inject_instance_metadata(instance, vm_ref)",
            "            self._inject_auto_disk_config(instance, vm_ref)",
            "            self._inject_hostname(instance, vm_ref, rescue)",
            "            self._file_inject_vm_settings(instance, vm_ref, vdis, network_info)",
            "            self.inject_network_info(instance, network_info, vm_ref)",
            "",
            "        @step",
            "        def setup_network_step(undo_mgr, vm_ref):",
            "            self._create_vifs(instance, vm_ref, network_info)",
            "",
            "            try:",
            "                self.firewall_driver.setup_basic_filtering(",
            "                        instance, network_info)",
            "            except NotImplementedError:",
            "                # NOTE(salvatore-orlando): setup_basic_filtering might be",
            "                # empty or not implemented at all, as basic filter could",
            "                # be implemented with VIF rules created by xapi plugin",
            "                pass",
            "",
            "            self.firewall_driver.prepare_instance_filter(instance,",
            "                                                         network_info)",
            "",
            "        @step",
            "        def boot_instance_step(undo_mgr, vm_ref):",
            "            self._start(instance, vm_ref)",
            "            self._wait_for_instance_to_start(instance, vm_ref)",
            "",
            "        @step",
            "        def configure_booted_instance_step(undo_mgr, vm_ref):",
            "            self._configure_new_instance_with_agent(instance, vm_ref,",
            "                    injected_files, admin_password)",
            "            self._remove_hostname(instance, vm_ref)",
            "",
            "        @step",
            "        def apply_security_group_filters_step(undo_mgr):",
            "            self.firewall_driver.apply_instance_filter(instance, network_info)",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            # NOTE(sirp): The create_disks() step will potentially take a",
            "            # *very* long time to complete since it has to fetch the image",
            "            # over the network and images can be several gigs in size. To",
            "            # avoid progress remaining at 0% for too long, make sure the",
            "            # first step is something that completes rather quickly.",
            "            disk_image_type = determine_disk_image_type_step(undo_mgr)",
            "",
            "            vdis = create_disks_step(undo_mgr, disk_image_type, image_meta)",
            "            kernel_file, ramdisk_file = create_kernel_ramdisk_step(undo_mgr)",
            "",
            "            vm_ref = create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                    kernel_file, ramdisk_file)",
            "            attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type)",
            "",
            "            inject_instance_data_step(undo_mgr, vm_ref, vdis)",
            "            setup_network_step(undo_mgr, vm_ref)",
            "",
            "            boot_instance_step(undo_mgr, vm_ref)",
            "",
            "            configure_booted_instance_step(undo_mgr, vm_ref)",
            "            apply_security_group_filters_step(undo_mgr)",
            "        except Exception:",
            "            msg = _(\"Failed to spawn, rolling back\")",
            "            undo_mgr.rollback_and_reraise(msg=msg, instance=instance)",
            "",
            "    def _attach_orig_disk_for_rescue(self, instance, vm_ref):",
            "        orig_vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "        vdi_ref = self._find_root_vdi_ref(orig_vm_ref)",
            "        vm_utils.create_vbd(self._session, vm_ref, vdi_ref, DEVICE_RESCUE,",
            "                            bootable=False)",
            "",
            "    def _file_inject_vm_settings(self, instance, vm_ref, vdis, network_info):",
            "        if CONF.flat_injected:",
            "            vm_utils.preconfigure_instance(self._session, instance,",
            "                                           vdis['root']['ref'], network_info)",
            "",
            "    def _ensure_instance_name_unique(self, name_label):",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        if vm_ref is not None:",
            "            raise exception.InstanceExists(name=name_label)",
            "",
            "    def _ensure_enough_free_mem(self, instance):",
            "        if not vm_utils.is_enough_free_mem(self._session, instance):",
            "            raise exception.InsufficientFreeMemory(uuid=instance['uuid'])",
            "",
            "    def _create_vm_record(self, context, instance, name_label, vdis,",
            "            disk_image_type, kernel_file, ramdisk_file):",
            "        \"\"\"Create the VM record in Xen, making sure that we do not create",
            "        a duplicate name-label.  Also do a rough sanity check on memory",
            "        to try to short-circuit a potential failure later.  (The memory",
            "        check only accounts for running VMs, so it can miss other builds",
            "        that are in progress.)",
            "        \"\"\"",
            "        mode = self._determine_vm_mode(instance, vdis, disk_image_type)",
            "        if instance['vm_mode'] != mode:",
            "            # Update database with normalized (or determined) value",
            "            self._virtapi.instance_update(context,",
            "                                          instance['uuid'], {'vm_mode': mode})",
            "",
            "        use_pv_kernel = (mode == vm_mode.XEN)",
            "        vm_ref = vm_utils.create_vm(self._session, instance, name_label,",
            "                                    kernel_file, ramdisk_file, use_pv_kernel)",
            "        return vm_ref",
            "",
            "    def _determine_vm_mode(self, instance, vdis, disk_image_type):",
            "        current_mode = vm_mode.get_from_instance(instance)",
            "        if current_mode == vm_mode.XEN or current_mode == vm_mode.HVM:",
            "            return current_mode",
            "",
            "        is_pv = False",
            "        if 'root' in vdis:",
            "            os_type = instance['os_type']",
            "            vdi_ref = vdis['root']['ref']",
            "            is_pv = vm_utils.determine_is_pv(self._session, vdi_ref,",
            "                                             disk_image_type, os_type)",
            "        if is_pv:",
            "            return vm_mode.XEN",
            "        else:",
            "            return vm_mode.HVM",
            "",
            "    def _attach_disks(self, instance, vm_ref, name_label, vdis,",
            "                      disk_image_type, admin_password=None, files=None):",
            "        ctx = nova_context.get_admin_context()",
            "        instance_type = flavors.extract_flavor(instance)",
            "",
            "        # Attach (required) root disk",
            "        if disk_image_type == vm_utils.ImageType.DISK_ISO:",
            "            # DISK_ISO needs two VBDs: the ISO disk and a blank RW disk",
            "            root_disk_size = instance_type['root_gb']",
            "            if root_disk_size > 0:",
            "                vm_utils.generate_iso_blank_root_disk(self._session, instance,",
            "                    vm_ref, DEVICE_ROOT, name_label, root_disk_size)",
            "",
            "            cd_vdi = vdis.pop('iso')",
            "            vm_utils.attach_cd(self._session, vm_ref, cd_vdi['ref'],",
            "                               DEVICE_CD)",
            "        else:",
            "            root_vdi = vdis['root']",
            "",
            "            if instance['auto_disk_config']:",
            "                LOG.debug(_(\"Auto configuring disk, attempting to \"",
            "                            \"resize partition...\"), instance=instance)",
            "                vm_utils.try_auto_configure_disk(self._session,",
            "                                                 root_vdi['ref'],",
            "                                                 instance_type['root_gb'])",
            "",
            "            vm_utils.create_vbd(self._session, vm_ref, root_vdi['ref'],",
            "                                DEVICE_ROOT, bootable=True,",
            "                                osvol=root_vdi.get('osvol'))",
            "",
            "        # Attach (optional) additional block-devices",
            "        for type_, vdi_info in vdis.items():",
            "            # Additional block-devices for boot use their device-name as the",
            "            # type.",
            "            if not type_.startswith('/dev'):",
            "                continue",
            "",
            "            # Convert device name to userdevice number, e.g. /dev/xvdb -> 1",
            "            userdevice = ord(block_device.strip_prefix(type_)) - ord('a')",
            "            vm_utils.create_vbd(self._session, vm_ref, vdi_info['ref'],",
            "                                userdevice, bootable=False,",
            "                                osvol=vdi_info.get('osvol'))",
            "",
            "        # Attach (optional) swap disk",
            "        swap_mb = instance_type['swap']",
            "        if swap_mb:",
            "            vm_utils.generate_swap(self._session, instance, vm_ref,",
            "                                   DEVICE_SWAP, name_label, swap_mb)",
            "",
            "        # Attach (optional) ephemeral disk",
            "        ephemeral_gb = instance_type['ephemeral_gb']",
            "        if ephemeral_gb:",
            "            vm_utils.generate_ephemeral(self._session, instance, vm_ref,",
            "                                        DEVICE_EPHEMERAL, name_label,",
            "                                        ephemeral_gb)",
            "",
            "        # Attach (optional) configdrive v2 disk",
            "        if configdrive.required_by(instance):",
            "            vm_utils.generate_configdrive(self._session, instance, vm_ref,",
            "                                          DEVICE_CONFIGDRIVE,",
            "                                          admin_password=admin_password,",
            "                                          files=files)",
            "",
            "    def _wait_for_instance_to_start(self, instance, vm_ref):",
            "        LOG.debug(_('Waiting for instance state to become running'),",
            "                  instance=instance)",
            "        expiration = time.time() + CONF.xenapi_running_timeout",
            "        while time.time() < expiration:",
            "            state = self.get_info(instance, vm_ref)['state']",
            "            if state == power_state.RUNNING:",
            "                break",
            "            greenthread.sleep(0.5)",
            "",
            "    def _configure_new_instance_with_agent(self, instance, vm_ref,",
            "                                           injected_files, admin_password):",
            "        if self.agent_enabled(instance):",
            "            ctx = nova_context.get_admin_context()",
            "            agent_build = self._virtapi.agent_build_get_by_triple(",
            "                ctx, 'xen', instance['os_type'], instance['architecture'])",
            "            if agent_build:",
            "                LOG.info(_('Latest agent build for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s is %(version)s') % agent_build)",
            "            else:",
            "                LOG.info(_('No agent build found for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s') % {",
            "                            'hypervisor': 'xen',",
            "                            'os': instance['os_type'],",
            "                            'architecture': instance['architecture']})",
            "",
            "            # Update agent, if necessary",
            "            # This also waits until the agent starts",
            "            agent = self._get_agent(instance, vm_ref)",
            "            version = agent.get_agent_version()",
            "            if version:",
            "                LOG.info(_('Instance agent version: %s'), version,",
            "                         instance=instance)",
            "",
            "            if (version and agent_build and",
            "                    cmp_version(version, agent_build['version']) < 0):",
            "                agent.agent_update(agent_build)",
            "",
            "            # if the guest agent is not available, configure the",
            "            # instance, but skip the admin password configuration",
            "            no_agent = version is None",
            "",
            "            # Inject ssh key.",
            "            agent.inject_ssh_key()",
            "",
            "            # Inject files, if necessary",
            "            if injected_files:",
            "                # Inject any files, if specified",
            "                agent.inject_files(injected_files)",
            "",
            "            # Set admin password, if necessary",
            "            if admin_password and not no_agent:",
            "                agent.set_admin_password(admin_password)",
            "",
            "            # Reset network config",
            "            agent.resetnetwork()",
            "",
            "    def _get_vm_opaque_ref(self, instance, check_rescue=False):",
            "        \"\"\"Get xapi OpaqueRef from a db record.",
            "        :param check_rescue: if True will return the 'name'-rescue vm if it",
            "                             exists, instead of just 'name'",
            "        \"\"\"",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'], check_rescue)",
            "        if vm_ref is None:",
            "            raise exception.NotFound(_('Could not find VM with name %s') %",
            "                                     instance['name'])",
            "        return vm_ref",
            "",
            "    def _acquire_bootlock(self, vm):",
            "        \"\"\"Prevent an instance from booting.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.set_blocked_operations\",",
            "            vm,",
            "            {\"start\": \"\"})",
            "",
            "    def _release_bootlock(self, vm):",
            "        \"\"\"Allow an instance to boot.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.remove_from_blocked_operations\",",
            "            vm,",
            "            \"start\")",
            "",
            "    def snapshot(self, context, instance, image_id, update_task_state):",
            "        \"\"\"Create snapshot from a running VM instance.",
            "",
            "        :param context: request context",
            "        :param instance: instance to be snapshotted",
            "        :param image_id: id of image to upload to",
            "",
            "        Steps involved in a XenServer snapshot:",
            "",
            "        1. XAPI-Snapshot: Snapshotting the instance using XenAPI. This",
            "           creates: Snapshot (Template) VM, Snapshot VBD, Snapshot VDI,",
            "           Snapshot VHD",
            "",
            "        2. Wait-for-coalesce: The Snapshot VDI and Instance VDI both point to",
            "           a 'base-copy' VDI.  The base_copy is immutable and may be chained",
            "           with other base_copies.  If chained, the base_copies",
            "           coalesce together, so, we must wait for this coalescing to occur to",
            "           get a stable representation of the data on disk.",
            "",
            "        3. Push-to-data-store: Once coalesced, we call",
            "           'xenapi_image_upload_handler' to upload the images.",
            "",
            "        \"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        label = \"%s-snapshot\" % instance['name']",
            "",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label,",
            "                update_task_state) as vdi_uuids:",
            "            update_task_state(task_state=task_states.IMAGE_UPLOADING,",
            "                              expected_state=task_states.IMAGE_PENDING_UPLOAD)",
            "            self.image_upload_handler.upload_image(context,",
            "                                                   self._session,",
            "                                                   instance,",
            "                                                   vdi_uuids,",
            "                                                   image_id)",
            "",
            "        LOG.debug(_(\"Finished snapshot and upload for VM\"),",
            "                  instance=instance)",
            "",
            "    def _migrate_vhd(self, instance, vdi_uuid, dest, sr_path, seq_num):",
            "        LOG.debug(_(\"Migrating VHD '%(vdi_uuid)s' with seq_num %(seq_num)d\"),",
            "                  {'vdi_uuid': vdi_uuid, 'seq_num': seq_num},",
            "                  instance=instance)",
            "        instance_uuid = instance['uuid']",
            "        try:",
            "            self._session.call_plugin_serialized('migration', 'transfer_vhd',",
            "                    instance_uuid=instance_uuid, host=dest, vdi_uuid=vdi_uuid,",
            "                    sr_path=sr_path, seq_num=seq_num)",
            "        except self._session.XenAPI.Failure:",
            "            msg = _(\"Failed to transfer vhd to new host\")",
            "            raise exception.MigrationError(reason=msg)",
            "",
            "    def _get_orig_vm_name_label(self, instance):",
            "        return instance['name'] + '-orig'",
            "",
            "    def _update_instance_progress(self, context, instance, step, total_steps):",
            "        \"\"\"Update instance progress percent to reflect current step number",
            "        \"\"\"",
            "        # FIXME(sirp): for now we're taking a KISS approach to instance",
            "        # progress:",
            "        # Divide the action's workflow into discrete steps and \"bump\" the",
            "        # instance's progress field as each step is completed.",
            "        #",
            "        # For a first cut this should be fine, however, for large VM images,",
            "        # the get_vdis_for_instance step begins to dominate the equation. A",
            "        # better approximation would use the percentage of the VM image that",
            "        # has been streamed to the destination host.",
            "        progress = round(float(step) / total_steps * 100)",
            "        LOG.debug(_(\"Updating progress to %d\"), progress,",
            "                  instance=instance)",
            "        self._virtapi.instance_update(context, instance['uuid'],",
            "                                      {'progress': progress})",
            "",
            "    def _resize_ensure_vm_is_shutdown(self, instance, vm_ref):",
            "        if vm_utils.is_vm_shutdown(self._session, vm_ref):",
            "            LOG.debug(_(\"VM was already shutdown.\"), instance=instance)",
            "            return",
            "",
            "        if not vm_utils.clean_shutdown_vm(self._session, instance, vm_ref):",
            "            LOG.debug(_(\"Clean shutdown did not complete successfully, \"",
            "                        \"trying hard shutdown.\"), instance=instance)",
            "            if not vm_utils.hard_shutdown_vm(self._session, instance, vm_ref):",
            "                raise exception.ResizeError(",
            "                    reason=_(\"Unable to terminate instance.\"))",
            "",
            "    def _migrate_disk_resizing_down(self, context, instance, dest,",
            "                                    instance_type, vm_ref, sr_path):",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        @step",
            "        def fake_step_to_match_resizing_up():",
            "            pass",
            "",
            "        @step",
            "        def rename_and_power_off_vm(undo_mgr):",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "            def restore_orig_vm():",
            "                # Do not need to restore block devices, not yet been removed",
            "                self._restore_orig_vm_and_cleanup_orphan(instance, None)",
            "",
            "            undo_mgr.undo_with(restore_orig_vm)",
            "",
            "        @step",
            "        def create_copy_vdi_and_resize(undo_mgr, old_vdi_ref):",
            "            new_vdi_ref, new_vdi_uuid = vm_utils.resize_disk(self._session,",
            "                instance, old_vdi_ref, instance_type)",
            "",
            "            def cleanup_vdi_copy():",
            "                vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "            undo_mgr.undo_with(cleanup_vdi_copy)",
            "",
            "            return new_vdi_ref, new_vdi_uuid",
            "",
            "        @step",
            "        def transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid):",
            "            self._migrate_vhd(instance, new_vdi_uuid, dest, sr_path, 0)",
            "            # Clean up VDI now that it's been copied",
            "            vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "        @step",
            "        def fake_step_to_be_executed_by_finish_migration():",
            "            pass",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            fake_step_to_match_resizing_up()",
            "            rename_and_power_off_vm(undo_mgr)",
            "            old_vdi_ref, _ignore = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            new_vdi_ref, new_vdi_uuid = create_copy_vdi_and_resize(",
            "                undo_mgr, old_vdi_ref)",
            "            transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid)",
            "        except Exception as error:",
            "            LOG.exception(_(\"_migrate_disk_resizing_down failed. \"",
            "                            \"Restoring orig vm due_to: %s.\"), error,",
            "                          instance=instance)",
            "            undo_mgr._rollback()",
            "            raise exception.InstanceFaultRollback(error)",
            "",
            "    def _migrate_disk_resizing_up(self, context, instance, dest, vm_ref,",
            "                                  sr_path):",
            "        self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "        # 1. Create Snapshot",
            "        label = \"%s-snapshot\" % instance['name']",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label) as vdi_uuids:",
            "            self._update_instance_progress(context, instance,",
            "                                           step=1,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 2. Transfer the immutable VHDs (base-copies)",
            "            #",
            "            # The first VHD will be the leaf (aka COW) that is being used by",
            "            # the VM. For this step, we're only interested in the immutable",
            "            # VHDs which are all of the parents of the leaf VHD.",
            "            for seq_num, vdi_uuid in itertools.islice(",
            "                    enumerate(vdi_uuids), 1, None):",
            "                self._migrate_vhd(instance, vdi_uuid, dest, sr_path, seq_num)",
            "                self._update_instance_progress(context, instance,",
            "                                               step=2,",
            "                                               total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 3. Now power down the instance",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=3,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 4. Transfer the COW VHD",
            "            vdi_ref, vm_vdi_rec = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            cow_uuid = vm_vdi_rec['uuid']",
            "            self._migrate_vhd(instance, cow_uuid, dest, sr_path, 0)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=4,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "    def _apply_orig_vm_name_label(self, instance, vm_ref):",
            "        # NOTE(sirp): in case we're resizing to the same host (for dev",
            "        # purposes), apply a suffix to name-label so the two VM records",
            "        # extant until a confirm_resize don't collide.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "",
            "    def migrate_disk_and_power_off(self, context, instance, dest,",
            "                                   instance_type, block_device_info):",
            "        \"\"\"Copies a VHD from one host machine to another, possibly",
            "        resizing filesystem before hand.",
            "",
            "        :param instance: the instance that owns the VHD in question.",
            "        :param dest: the destination host machine.",
            "        :param instance_type: instance_type to resize to",
            "        \"\"\"",
            "        # 0. Zero out the progress to begin",
            "        self._update_instance_progress(context, instance,",
            "                                       step=0,",
            "                                       total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        sr_path = vm_utils.get_sr_path(self._session)",
            "",
            "        old_gb = instance['root_gb']",
            "        new_gb = instance_type['root_gb']",
            "        resize_down = old_gb > new_gb",
            "",
            "        if resize_down:",
            "            self._migrate_disk_resizing_down(",
            "                    context, instance, dest, instance_type, vm_ref, sr_path)",
            "        else:",
            "            self._migrate_disk_resizing_up(",
            "                    context, instance, dest, vm_ref, sr_path)",
            "",
            "        self._detach_block_devices_from_orig_vm(instance, block_device_info)",
            "",
            "        # NOTE(sirp): disk_info isn't used by the xenapi driver, instead it",
            "        # uses a staging-area (/images/instance<uuid>) and sequence-numbered",
            "        # VHDs to figure out how to reconstruct the VDI chain after syncing",
            "        disk_info = {}",
            "        return disk_info",
            "",
            "    def _detach_block_devices_from_orig_vm(self, instance, block_device_info):",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.detach_volume(connection_info, name_label,",
            "                                          mount_device)",
            "",
            "    def _resize_up_root_vdi(self, instance, root_vdi):",
            "        \"\"\"Resize an instances root disk.\"\"\"",
            "",
            "        new_disk_size = instance['root_gb'] * 1024 * 1024 * 1024",
            "        if not new_disk_size:",
            "            return",
            "",
            "        # Get current size of VDI",
            "        virtual_size = self._session.call_xenapi('VDI.get_virtual_size',",
            "                                                 root_vdi['ref'])",
            "        virtual_size = int(virtual_size)",
            "",
            "        old_gb = virtual_size / (1024 * 1024 * 1024)",
            "        new_gb = instance['root_gb']",
            "",
            "        if virtual_size < new_disk_size:",
            "            # Resize up. Simple VDI resize will do the trick",
            "            vdi_uuid = root_vdi['uuid']",
            "            LOG.debug(_(\"Resizing up VDI %(vdi_uuid)s from %(old_gb)dGB to \"",
            "                        \"%(new_gb)dGB\"),",
            "                      {'vdi_uuid': vdi_uuid, 'old_gb': old_gb,",
            "                       'new_gb': new_gb}, instance=instance)",
            "            resize_func_name = self.check_resize_func_name()",
            "            self._session.call_xenapi(resize_func_name, root_vdi['ref'],",
            "                    str(new_disk_size))",
            "            LOG.debug(_(\"Resize complete\"), instance=instance)",
            "",
            "    def check_resize_func_name(self):",
            "        \"\"\"Check the function name used to resize an instance based",
            "        on product_brand and product_version.",
            "        \"\"\"",
            "",
            "        brand = self._session.product_brand",
            "        version = self._session.product_version",
            "",
            "        # To maintain backwards compatibility. All recent versions",
            "        # should use VDI.resize",
            "        if bool(version) and bool(brand):",
            "            xcp = brand == 'XCP'",
            "            r1_2_or_above = (",
            "                (",
            "                    version[0] == 1",
            "                    and version[1] > 1",
            "                )",
            "                or version[0] > 1)",
            "",
            "            xenserver = brand == 'XenServer'",
            "            r6_or_above = version[0] > 5",
            "",
            "            if (xcp and not r1_2_or_above) or (xenserver and not r6_or_above):",
            "                return 'VDI.resize_online'",
            "",
            "        return 'VDI.resize'",
            "",
            "    def reboot(self, instance, reboot_type, bad_volumes_callback=None):",
            "        \"\"\"Reboot VM instance.\"\"\"",
            "        # Note (salvatore-orlando): security group rules are not re-enforced",
            "        # upon reboot, since this action on the XenAPI drivers does not",
            "        # remove existing filters",
            "        vm_ref = self._get_vm_opaque_ref(instance, check_rescue=True)",
            "",
            "        try:",
            "            if reboot_type == \"HARD\":",
            "                self._session.call_xenapi('VM.hard_reboot', vm_ref)",
            "            else:",
            "                self._session.call_xenapi('VM.clean_reboot', vm_ref)",
            "        except self._session.XenAPI.Failure as exc:",
            "            details = exc.details",
            "            if (details[0] == 'VM_BAD_POWER_STATE' and",
            "                    details[-1] == 'halted'):",
            "                LOG.info(_(\"Starting halted instance found during reboot\"),",
            "                    instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            elif details[0] == 'SR_BACKEND_FAILURE_46':",
            "                LOG.warn(_(\"Reboot failed due to bad volumes, detaching bad\"",
            "                           \" volumes and starting halted instance\"),",
            "                         instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            else:",
            "                raise",
            "",
            "    def set_admin_password(self, instance, new_pass):",
            "        \"\"\"Set the root/admin password on the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.set_admin_password(new_pass)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def inject_file(self, instance, path, contents):",
            "        \"\"\"Write a file to the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.inject_file(path, contents)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def _sanitize_xenstore_key(key):",
            "        \"\"\"",
            "        Xenstore only allows the following characters as keys:",
            "",
            "        ABCDEFGHIJKLMNOPQRSTUVWXYZ",
            "        abcdefghijklmnopqrstuvwxyz",
            "        0123456789-/_@",
            "",
            "        So convert the others to _",
            "",
            "        Also convert / to _, because that is somewhat like a path",
            "        separator.",
            "        \"\"\"",
            "        allowed_chars = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"",
            "                         \"abcdefghijklmnopqrstuvwxyz\"",
            "                         \"0123456789-_@\")",
            "        return ''.join([x in allowed_chars and x or '_' for x in key])",
            "",
            "    def _inject_instance_metadata(self, instance, vm_ref):",
            "        \"\"\"Inject instance metadata into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_meta(topdir, data_dict):",
            "            for key, value in data_dict.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                value = value or ''",
            "                self._add_to_param_xenstore(vm_ref, '%s/%s' % (topdir, key),",
            "                                            jsonutils.dumps(value))",
            "",
            "        # Store user metadata",
            "        store_meta('vm-data/user-metadata', utils.instance_meta(instance))",
            "",
            "    def _inject_auto_disk_config(self, instance, vm_ref):",
            "        \"\"\"Inject instance's auto_disk_config attribute into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_auto_disk_config(key, value):",
            "            value = value and True or False",
            "            self._add_to_param_xenstore(vm_ref, key, str(value))",
            "",
            "        store_auto_disk_config('vm-data/auto-disk-config',",
            "                               instance['auto_disk_config'])",
            "",
            "    def change_instance_metadata(self, instance, diff):",
            "        \"\"\"Apply changes to instance metadata to xenstore.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            # NOTE(johngarbutt) race conditions mean we can still get here",
            "            # during operations where the VM is not present, like resize.",
            "            # Skip the update when not possible, as the updated metadata will",
            "            # get added when the VM is being booted up at the end of the",
            "            # resize or rebuild.",
            "            LOG.warn(_(\"Unable to update metadata, VM not found.\"),",
            "                     instance=instance, exc_info=True)",
            "            return",
            "",
            "        def process_change(location, change):",
            "            if change[0] == '-':",
            "                self._remove_from_param_xenstore(vm_ref, location)",
            "                try:",
            "                    self._delete_from_xenstore(instance, location,",
            "                                               vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "            elif change[0] == '+':",
            "                self._add_to_param_xenstore(vm_ref, location,",
            "                                            jsonutils.dumps(change[1]))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, change[1],",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_meta():",
            "            for key, change in diff.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                location = 'vm-data/user-metadata/%s' % key",
            "                process_change(location, change)",
            "        update_meta()",
            "",
            "    def _find_root_vdi_ref(self, vm_ref):",
            "        \"\"\"Find and return the root vdi ref for a VM.\"\"\"",
            "        if not vm_ref:",
            "            return None",
            "",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        for vbd_uuid in vbd_refs:",
            "            vbd = self._session.call_xenapi(\"VBD.get_record\", vbd_uuid)",
            "            if vbd[\"userdevice\"] == DEVICE_ROOT:",
            "                return vbd[\"VDI\"]",
            "",
            "        raise exception.NotFound(_(\"Unable to find root VBD/VDI for VM\"))",
            "",
            "    def _destroy_vdis(self, instance, vm_ref):",
            "        \"\"\"Destroys all VDIs associated with a VM.\"\"\"",
            "        LOG.debug(_(\"Destroying VDIs\"), instance=instance)",
            "",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, vm_ref)",
            "        if not vdi_refs:",
            "            return",
            "        for vdi_ref in vdi_refs:",
            "            try:",
            "                vm_utils.destroy_vdi(self._session, vdi_ref)",
            "            except volume_utils.StorageError as exc:",
            "                LOG.error(exc)",
            "",
            "    def _destroy_kernel_ramdisk(self, instance, vm_ref):",
            "        \"\"\"Three situations can occur:",
            "",
            "            1. We have neither a ramdisk nor a kernel, in which case we are a",
            "               RAW image and can omit this step",
            "",
            "            2. We have one or the other, in which case, we should flag as an",
            "               error",
            "",
            "            3. We have both, in which case we safely remove both the kernel",
            "               and the ramdisk.",
            "",
            "        \"\"\"",
            "        instance_uuid = instance['uuid']",
            "        if not instance['kernel_id'] and not instance['ramdisk_id']:",
            "            # 1. No kernel or ramdisk",
            "            LOG.debug(_(\"Using RAW or VHD, skipping kernel and ramdisk \"",
            "                        \"deletion\"), instance=instance)",
            "            return",
            "",
            "        if not (instance['kernel_id'] and instance['ramdisk_id']):",
            "            # 2. We only have kernel xor ramdisk",
            "            raise exception.InstanceUnacceptable(instance_id=instance_uuid,",
            "               reason=_(\"instance has a kernel or ramdisk but not both\"))",
            "",
            "        # 3. We have both kernel and ramdisk",
            "        (kernel, ramdisk) = vm_utils.lookup_kernel_ramdisk(self._session,",
            "                                                           vm_ref)",
            "        if kernel or ramdisk:",
            "            vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                                            kernel, ramdisk)",
            "            LOG.debug(_(\"kernel/ramdisk files removed\"), instance=instance)",
            "",
            "    def _destroy_rescue_instance(self, rescue_vm_ref, original_vm_ref):",
            "        \"\"\"Destroy a rescue instance.\"\"\"",
            "        # Shutdown Rescue VM",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", rescue_vm_ref)",
            "        state = vm_utils.compile_info(vm_rec)['state']",
            "        if state != power_state.SHUTDOWN:",
            "            self._session.call_xenapi(\"VM.hard_shutdown\", rescue_vm_ref)",
            "",
            "        # Destroy Rescue VDIs",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, rescue_vm_ref)",
            "        root_vdi_ref = self._find_root_vdi_ref(original_vm_ref)",
            "        vdi_refs = [vdi_ref for vdi_ref in vdi_refs if vdi_ref != root_vdi_ref]",
            "        vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "        # Destroy Rescue VM",
            "        self._session.call_xenapi(\"VM.destroy\", rescue_vm_ref)",
            "",
            "    def destroy(self, instance, network_info, block_device_info=None,",
            "                destroy_disks=True):",
            "        \"\"\"Destroy VM instance.",
            "",
            "        This is the method exposed by xenapi_conn.destroy(). The rest of the",
            "        destroy_* methods are internal.",
            "",
            "        \"\"\"",
            "        LOG.info(_(\"Destroying VM\"), instance=instance)",
            "",
            "        # We don't use _get_vm_opaque_ref because the instance may",
            "        # truly not exist because of a failure during build. A valid",
            "        # vm_ref is checked correctly where necessary.",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if rescue_vm_ref:",
            "            self._destroy_rescue_instance(rescue_vm_ref, vm_ref)",
            "",
            "        # NOTE(sirp): `block_device_info` is not used, information about which",
            "        # volumes should be detached is determined by the",
            "        # VBD.other_config['osvol'] attribute",
            "        return self._destroy(instance, vm_ref, network_info=network_info,",
            "                             destroy_disks=destroy_disks)",
            "",
            "    def _destroy(self, instance, vm_ref, network_info=None,",
            "                 destroy_disks=True):",
            "        \"\"\"Destroys VM instance by performing:",
            "",
            "            1. A shutdown",
            "            2. Destroying associated VDIs.",
            "            3. Destroying kernel and ramdisk files (if necessary).",
            "            4. Destroying that actual VM record.",
            "",
            "        \"\"\"",
            "        if vm_ref is None:",
            "            LOG.warning(_(\"VM is not present, skipping destroy...\"),",
            "                        instance=instance)",
            "            return",
            "",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "        if destroy_disks:",
            "            self._volumeops.detach_all(vm_ref)",
            "            self._destroy_vdis(instance, vm_ref)",
            "            self._destroy_kernel_ramdisk(instance, vm_ref)",
            "",
            "        vm_utils.destroy_vm(self._session, instance, vm_ref)",
            "",
            "        self.unplug_vifs(instance, network_info)",
            "        self.firewall_driver.unfilter_instance(",
            "                instance, network_info=network_info)",
            "",
            "    def pause(self, instance):",
            "        \"\"\"Pause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.pause', vm_ref)",
            "",
            "    def unpause(self, instance):",
            "        \"\"\"Unpause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.unpause', vm_ref)",
            "",
            "    def suspend(self, instance):",
            "        \"\"\"Suspend the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._acquire_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.suspend', vm_ref)",
            "",
            "    def resume(self, instance):",
            "        \"\"\"Resume the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.resume', vm_ref, False, True)",
            "",
            "    def rescue(self, context, instance, network_info, image_meta,",
            "               rescue_password):",
            "        \"\"\"Rescue the specified instance.",
            "",
            "            - shutdown the instance VM.",
            "            - set 'bootlock' to prevent the instance from starting in rescue.",
            "            - spawn a rescue VM (the vm name-label will be instance-N-rescue).",
            "",
            "        \"\"\"",
            "        rescue_name_label = '%s-rescue' % instance['name']",
            "        rescue_vm_ref = vm_utils.lookup(self._session, rescue_name_label)",
            "        if rescue_vm_ref:",
            "            raise RuntimeError(_(\"Instance is already in Rescue Mode: %s\")",
            "                               % instance['name'])",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "        self._acquire_bootlock(vm_ref)",
            "        self.spawn(context, instance, image_meta, [], rescue_password,",
            "                   network_info, name_label=rescue_name_label, rescue=True)",
            "",
            "    def unrescue(self, instance):",
            "        \"\"\"Unrescue the specified instance.",
            "",
            "            - unplug the instance VM's disk from the rescue VM.",
            "            - teardown the rescue VM.",
            "            - release the bootlock to allow the instance VM to start.",
            "",
            "        \"\"\"",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if not rescue_vm_ref:",
            "            raise exception.InstanceNotInRescueMode(",
            "                    instance_id=instance['uuid'])",
            "",
            "        original_vm_ref = self._get_vm_opaque_ref(instance)",
            "",
            "        self._destroy_rescue_instance(rescue_vm_ref, original_vm_ref)",
            "        self._release_bootlock(original_vm_ref)",
            "        self._start(instance, original_vm_ref)",
            "",
            "    def soft_delete(self, instance):",
            "        \"\"\"Soft delete the specified instance.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            LOG.warning(_(\"VM is not present, skipping soft delete...\"),",
            "                        instance=instance)",
            "        else:",
            "            vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "            self._acquire_bootlock(vm_ref)",
            "",
            "    def restore(self, instance):",
            "        \"\"\"Restore the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._start(instance, vm_ref)",
            "",
            "    def power_off(self, instance):",
            "        \"\"\"Power off the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "    def power_on(self, instance):",
            "        \"\"\"Power on the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._start(instance, vm_ref)",
            "",
            "    def _cancel_stale_tasks(self, timeout, task):",
            "        \"\"\"Cancel the given tasks that are older than the given timeout.\"\"\"",
            "        task_refs = self._session.call_xenapi(\"task.get_by_name_label\", task)",
            "        for task_ref in task_refs:",
            "            task_rec = self._session.call_xenapi(\"task.get_record\", task_ref)",
            "            task_created = timeutils.parse_strtime(task_rec[\"created\"].value,",
            "                                                   \"%Y%m%dT%H:%M:%SZ\")",
            "",
            "            if timeutils.is_older_than(task_created, timeout):",
            "                self._session.call_xenapi(\"task.cancel\", task_ref)",
            "",
            "    def poll_rebooting_instances(self, timeout, instances):",
            "        \"\"\"Look for expirable rebooting instances.",
            "",
            "            - issue a \"hard\" reboot to any instance that has been stuck in a",
            "              reboot state for >= the given timeout",
            "        \"\"\"",
            "        # NOTE(jk0): All existing clean_reboot tasks must be cancelled before",
            "        # we can kick off the hard_reboot tasks.",
            "        self._cancel_stale_tasks(timeout, 'VM.clean_reboot')",
            "",
            "        ctxt = nova_context.get_admin_context()",
            "",
            "        instances_info = dict(instance_count=len(instances),",
            "                timeout=timeout)",
            "",
            "        if instances_info[\"instance_count\"] > 0:",
            "            LOG.info(_(\"Found %(instance_count)d hung reboots \"",
            "                       \"older than %(timeout)d seconds\") % instances_info)",
            "",
            "        for instance in instances:",
            "            LOG.info(_(\"Automatically hard rebooting\"), instance=instance)",
            "            self.compute_api.reboot(ctxt, instance, \"HARD\")",
            "",
            "    def get_info(self, instance, vm_ref=None):",
            "        \"\"\"Return data about VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_info(vm_rec)",
            "",
            "    def get_diagnostics(self, instance):",
            "        \"\"\"Return data about VM diagnostics.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_diagnostics(vm_rec)",
            "",
            "    def _get_vif_device_map(self, vm_rec):",
            "        vif_map = {}",
            "        for vif in [self._session.call_xenapi(\"VIF.get_record\", vrec)",
            "                    for vrec in vm_rec['VIFs']]:",
            "            vif_map[vif['device']] = vif['MAC']",
            "        return vif_map",
            "",
            "    def get_all_bw_counters(self):",
            "        \"\"\"Return running bandwidth counter for each interface on each",
            "           running VM.",
            "        \"\"\"",
            "        counters = vm_utils.fetch_bandwidth(self._session)",
            "        bw = {}",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            vif_map = self._get_vif_device_map(vm_rec)",
            "            name = vm_rec['name_label']",
            "            if 'nova_uuid' not in vm_rec['other_config']:",
            "                continue",
            "            dom = vm_rec.get('domid')",
            "            if dom is None or dom not in counters:",
            "                continue",
            "            vifs_bw = bw.setdefault(name, {})",
            "            for vif_num, vif_data in counters[dom].iteritems():",
            "                mac = vif_map[vif_num]",
            "                vif_data['mac_address'] = mac",
            "                vifs_bw[mac] = vif_data",
            "        return bw",
            "",
            "    def get_console_output(self, instance):",
            "        \"\"\"Return last few lines of instance console.\"\"\"",
            "        dom_id = self._get_dom_id(instance, check_rescue=True)",
            "",
            "        try:",
            "            raw_console_data = self._session.call_plugin('console',",
            "                    'get_console_log', {'dom_id': dom_id})",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _(\"Guest does not have a console available\")",
            "            raise exception.NovaException(msg)",
            "",
            "        return zlib.decompress(base64.b64decode(raw_console_data))",
            "",
            "    def get_vnc_console(self, instance):",
            "        \"\"\"Return connection info for a vnc console.\"\"\"",
            "        if instance['vm_state'] == vm_states.RESCUED:",
            "            name = '%s-rescue' % instance['name']",
            "            vm_ref = vm_utils.lookup(self._session, name)",
            "            if vm_ref is None:",
            "                # The rescue instance might not be ready at this point.",
            "                raise exception.InstanceNotReady(instance_id=instance['uuid'])",
            "        else:",
            "            vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "            if vm_ref is None:",
            "                # The compute manager expects InstanceNotFound for this case.",
            "                raise exception.InstanceNotFound(instance_id=instance['uuid'])",
            "",
            "        session_id = self._session.get_session_id()",
            "        path = \"/console?ref=%s&session_id=%s\" % (str(vm_ref), session_id)",
            "",
            "        # NOTE: XS5.6sp2+ use http over port 80 for xenapi com",
            "        return {'host': CONF.vncserver_proxyclient_address, 'port': 80,",
            "                'internal_access_path': path}",
            "",
            "    def _vif_xenstore_data(self, vif):",
            "        \"\"\"convert a network info vif to injectable instance data.\"\"\"",
            "",
            "        def get_ip(ip):",
            "            if not ip:",
            "                return None",
            "            return ip['address']",
            "",
            "        def fixed_ip_dict(ip, subnet):",
            "            if ip['version'] == 4:",
            "                netmask = str(subnet.as_netaddr().netmask)",
            "            else:",
            "                netmask = subnet.as_netaddr()._prefixlen",
            "",
            "            return {'ip': ip['address'],",
            "                    'enabled': '1',",
            "                    'netmask': netmask,",
            "                    'gateway': get_ip(subnet['gateway'])}",
            "",
            "        def convert_route(route):",
            "            return {'route': str(netaddr.IPNetwork(route['cidr']).network),",
            "                    'netmask': str(netaddr.IPNetwork(route['cidr']).netmask),",
            "                    'gateway': get_ip(route['gateway'])}",
            "",
            "        network = vif['network']",
            "        v4_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 4]",
            "        v6_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 6]",
            "",
            "        # NOTE(tr3buchet): routes and DNS come from all subnets",
            "        routes = [convert_route(route) for subnet in network['subnets']",
            "                                       for route in subnet['routes']]",
            "        dns = [get_ip(ip) for subnet in network['subnets']",
            "                          for ip in subnet['dns']]",
            "",
            "        info_dict = {'label': network['label'],",
            "                     'mac': vif['address']}",
            "",
            "        if v4_subnets:",
            "            # NOTE(tr3buchet): gateway and broadcast from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway'] = get_ip(v4_subnets[0]['gateway'])",
            "            info_dict['broadcast'] = str(v4_subnets[0].as_netaddr().broadcast)",
            "            info_dict['ips'] = [fixed_ip_dict(ip, subnet)",
            "                                for subnet in v4_subnets",
            "                                for ip in subnet['ips']]",
            "        if v6_subnets:",
            "            # NOTE(tr3buchet): gateway from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway_v6'] = get_ip(v6_subnets[0]['gateway'])",
            "            info_dict['ip6s'] = [fixed_ip_dict(ip, subnet)",
            "                                 for subnet in v6_subnets",
            "                                 for ip in subnet['ips']]",
            "        if routes:",
            "            info_dict['routes'] = routes",
            "",
            "        if dns:",
            "            info_dict['dns'] = list(set(dns))",
            "",
            "        return info_dict",
            "",
            "    def inject_network_info(self, instance, network_info, vm_ref=None):",
            "        \"\"\"",
            "        Generate the network info and make calls to place it into the",
            "        xenstore and the xenstore param list.",
            "        vm_ref can be passed in because it will sometimes be different than",
            "        what vm_utils.lookup(session, instance['name']) will find (ex: rescue)",
            "        \"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Injecting network info to xenstore\"), instance=instance)",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_nwinfo():",
            "            for vif in network_info:",
            "                xs_data = self._vif_xenstore_data(vif)",
            "                location = ('vm-data/networking/%s' %",
            "                            vif['address'].replace(':', ''))",
            "                self._add_to_param_xenstore(vm_ref,",
            "                                            location,",
            "                                            jsonutils.dumps(xs_data))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, xs_data,",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "        update_nwinfo()",
            "",
            "    def _create_vifs(self, instance, vm_ref, network_info):",
            "        \"\"\"Creates vifs for an instance.\"\"\"",
            "",
            "        LOG.debug(_(\"Creating vifs\"), instance=instance)",
            "",
            "        # this function raises if vm_ref is not a vm_opaque_ref",
            "        self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "",
            "        for device, vif in enumerate(network_info):",
            "            vif_rec = self.vif_driver.plug(instance, vif,",
            "                                           vm_ref=vm_ref, device=device)",
            "            network_ref = vif_rec['network']",
            "            LOG.debug(_('Creating VIF for network %s'),",
            "                      network_ref, instance=instance)",
            "            vif_ref = self._session.call_xenapi('VIF.create', vif_rec)",
            "            LOG.debug(_('Created VIF %(vif_ref)s, network %(network_ref)s'),",
            "                      {'vif_ref': vif_ref, 'network_ref': network_ref},",
            "                      instance=instance)",
            "",
            "    def plug_vifs(self, instance, network_info):",
            "        \"\"\"Set up VIF networking on the host.\"\"\"",
            "        for device, vif in enumerate(network_info):",
            "            self.vif_driver.plug(instance, vif, device=device)",
            "",
            "    def unplug_vifs(self, instance, network_info):",
            "        if network_info:",
            "            for vif in network_info:",
            "                self.vif_driver.unplug(instance, vif)",
            "",
            "    def reset_network(self, instance):",
            "        \"\"\"Calls resetnetwork method in agent.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.resetnetwork()",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def _inject_hostname(self, instance, vm_ref, rescue):",
            "        \"\"\"Inject the hostname of the instance into the xenstore.\"\"\"",
            "        hostname = instance['hostname']",
            "        if rescue:",
            "            hostname = 'RESCUE-%s' % hostname",
            "",
            "        if instance['os_type'] == \"windows\":",
            "            # NOTE(jk0): Windows hostnames can only be <= 15 chars.",
            "            hostname = hostname[:15]",
            "",
            "        LOG.debug(_(\"Injecting hostname to xenstore\"), instance=instance)",
            "        self._add_to_param_xenstore(vm_ref, 'vm-data/hostname', hostname)",
            "",
            "    def _remove_hostname(self, instance, vm_ref):",
            "        LOG.debug(_(\"Removing hostname from xenstore\"), instance=instance)",
            "        self._remove_from_param_xenstore(vm_ref, 'vm-data/hostname')",
            "",
            "    def _write_to_xenstore(self, instance, path, value, vm_ref=None):",
            "        \"\"\"",
            "        Writes the passed value to the xenstore record for the given VM",
            "        at the specified location. A XenAPIPlugin.PluginError will be raised",
            "        if any error is encountered in the write process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'write_record', instance,",
            "                                      vm_ref=vm_ref, path=path,",
            "                                      value=jsonutils.dumps(value))",
            "",
            "    def _delete_from_xenstore(self, instance, path, vm_ref=None):",
            "        \"\"\"",
            "        Deletes the value from the xenstore record for the given VM at",
            "        the specified location.  A XenAPIPlugin.PluginError will be",
            "        raised if any error is encountered in the delete process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'delete_record', instance,",
            "                                      vm_ref=vm_ref, path=path)",
            "",
            "    def _make_plugin_call(self, plugin, method, instance=None, vm_ref=None,",
            "                          **addl_args):",
            "        \"\"\"",
            "        Abstracts out the process of calling a method of a xenapi plugin.",
            "        Any errors raised by the plugin will in turn raise a RuntimeError here.",
            "        \"\"\"",
            "        args = {}",
            "        if instance or vm_ref:",
            "            args['dom_id'] = self._get_dom_id(instance, vm_ref)",
            "        args.update(addl_args)",
            "        try:",
            "            return self._session.call_plugin(plugin, method, args)",
            "        except self._session.XenAPI.Failure as e:",
            "            err_msg = e.details[-1].splitlines()[-1]",
            "            if 'TIMEOUT:' in err_msg:",
            "                LOG.error(_('TIMEOUT: The call to %(method)s timed out. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'timeout', 'message': err_msg}",
            "            elif 'NOT IMPLEMENTED:' in err_msg:",
            "                LOG.error(_('NOT IMPLEMENTED: The call to %(method)s is not'",
            "                            ' supported by the agent. args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'notimplemented', 'message': err_msg}",
            "            else:",
            "                LOG.error(_('The call to %(method)s returned an error: %(e)s. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args, 'e': e},",
            "                          instance=instance)",
            "                return {'returncode': 'error', 'message': err_msg}",
            "",
            "    def _get_dom_id(self, instance=None, vm_ref=None, check_rescue=False):",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance, check_rescue)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_rec['domid']",
            "",
            "    def _add_to_param_xenstore(self, vm_ref, key, val):",
            "        \"\"\"",
            "        Takes a key/value pair and adds it to the xenstore parameter",
            "        record for the given vm instance. If the key exists in xenstore,",
            "        it is overwritten",
            "        \"\"\"",
            "        self._remove_from_param_xenstore(vm_ref, key)",
            "        self._session.call_xenapi('VM.add_to_xenstore_data', vm_ref, key, val)",
            "",
            "    def _remove_from_param_xenstore(self, vm_ref, key):",
            "        \"\"\"",
            "        Takes a single key and removes it from the xenstore parameter",
            "        record data for the given VM.",
            "        If the key doesn't exist, the request is ignored.",
            "        \"\"\"",
            "        self._session.call_xenapi('VM.remove_from_xenstore_data', vm_ref, key)",
            "",
            "    def refresh_security_group_rules(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_rules(security_group_id)",
            "",
            "    def refresh_security_group_members(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_members(security_group_id)",
            "",
            "    def refresh_instance_security_rules(self, instance):",
            "        \"\"\"recreates security group rules for specified instance.\"\"\"",
            "        self.firewall_driver.refresh_instance_security_rules(instance)",
            "",
            "    def refresh_provider_fw_rules(self):",
            "        self.firewall_driver.refresh_provider_fw_rules()",
            "",
            "    def unfilter_instance(self, instance_ref, network_info):",
            "        \"\"\"Removes filters for each VIF of the specified instance.\"\"\"",
            "        self.firewall_driver.unfilter_instance(instance_ref,",
            "                                               network_info=network_info)",
            "",
            "    def _get_host_uuid_from_aggregate(self, context, hostname):",
            "        current_aggregate = self._virtapi.aggregate_get_by_host(",
            "            context, CONF.host, key=pool_states.POOL_FLAG)[0]",
            "        if not current_aggregate:",
            "            raise exception.AggregateHostNotFound(host=CONF.host)",
            "        try:",
            "            return current_aggregate.metadetails[hostname]",
            "        except KeyError:",
            "            reason = _('Destination host:%s must be in the same '",
            "                       'aggregate as the source server') % hostname",
            "            raise exception.MigrationPreCheckError(reason=reason)",
            "",
            "    def _ensure_host_in_aggregate(self, context, hostname):",
            "        self._get_host_uuid_from_aggregate(context, hostname)",
            "",
            "    def _get_host_opaque_ref(self, context, hostname):",
            "        host_uuid = self._get_host_uuid_from_aggregate(context, hostname)",
            "        return self._session.call_xenapi(\"host.get_by_uuid\", host_uuid)",
            "",
            "    def _migrate_receive(self, ctxt):",
            "        destref = self._session.get_xenapi_host()",
            "        # Get the network to for migrate.",
            "        # This is the one associated with the pif marked management. From cli:",
            "        # uuid=`xe pif-list --minimal management=true`",
            "        # xe pif-param-get param-name=network-uuid uuid=$uuid",
            "        expr = 'field \"management\" = \"true\"'",
            "        pifs = self._session.call_xenapi('PIF.get_all_records_where',",
            "                                         expr)",
            "        if len(pifs) != 1:",
            "            msg = _('No suitable network for migrate')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "",
            "        nwref = pifs[pifs.keys()[0]]['network']",
            "        try:",
            "            options = {}",
            "            migrate_data = self._session.call_xenapi(\"host.migrate_receive\",",
            "                                                     destref,",
            "                                                     nwref,",
            "                                                     options)",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _('Migrate Receive failed')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "        return migrate_data",
            "",
            "    def _get_iscsi_srs(self, ctxt, instance_ref):",
            "        vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        iscsi_srs = []",
            "",
            "        for vbd_ref in vbd_refs:",
            "            vdi_ref = self._session.call_xenapi(\"VBD.get_VDI\", vbd_ref)",
            "            # Check if it's on an iSCSI SR",
            "            sr_ref = self._session.call_xenapi(\"VDI.get_SR\", vdi_ref)",
            "            if self._session.call_xenapi(\"SR.get_type\", sr_ref) == 'iscsi':",
            "                iscsi_srs.append(sr_ref)",
            "",
            "        return iscsi_srs",
            "",
            "    def check_can_live_migrate_destination(self, ctxt, instance_ref,",
            "                                           block_migration=False,",
            "                                           disk_over_commit=False):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param block_migration: if true, prepare for block migration",
            "        :param disk_over_commit: if true, allow disk over commit",
            "",
            "        \"\"\"",
            "        dest_check_data = {}",
            "        if block_migration:",
            "            migrate_send_data = self._migrate_receive(ctxt)",
            "            destination_sr_ref = vm_utils.safe_find_sr(self._session)",
            "            dest_check_data.update(",
            "                {\"block_migration\": block_migration,",
            "                 \"migrate_data\": {\"migrate_send_data\": migrate_send_data,",
            "                                  \"destination_sr_ref\": destination_sr_ref}})",
            "        else:",
            "            src = instance_ref['host']",
            "            self._ensure_host_in_aggregate(ctxt, src)",
            "            # TODO(johngarbutt) we currently assume",
            "            # instance is on a SR shared with other destination",
            "            # block migration work will be able to resolve this",
            "        return dest_check_data",
            "",
            "    def _is_xsm_sr_check_relaxed(self):",
            "        try:",
            "            return self.cached_xsm_sr_relaxed",
            "        except AttributeError:",
            "            config_value = None",
            "            try:",
            "                config_value = self._make_plugin_call('config_file',",
            "                                                      'get_val',",
            "                                                      key='relax-xsm-sr-check')",
            "            except Exception as exc:",
            "                LOG.exception(exc)",
            "            self.cached_xsm_sr_relaxed = config_value == \"true\"",
            "            return self.cached_xsm_sr_relaxed",
            "",
            "    def check_can_live_migrate_source(self, ctxt, instance_ref,",
            "                                      dest_check_data):",
            "        \"\"\"Check if it's possible to execute live migration on the source side.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param dest_check_data: data returned by the check on the",
            "                                destination, includes block_migration flag",
            "",
            "        \"\"\"",
            "        if len(self._get_iscsi_srs(ctxt, instance_ref)) > 0:",
            "            # XAPI must support the relaxed SR check for live migrating with",
            "            # iSCSI VBDs",
            "            if not self._is_xsm_sr_check_relaxed():",
            "                raise exception.MigrationError(_('XAPI supporting '",
            "                                'relax-xsm-sr-check=true requried'))",
            "",
            "        if 'migrate_data' in dest_check_data:",
            "            vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "            migrate_data = dest_check_data['migrate_data']",
            "            try:",
            "                self._call_live_migrate_command(",
            "                    \"VM.assert_can_migrate\", vm_ref, migrate_data)",
            "            except self._session.XenAPI.Failure as exc:",
            "                LOG.exception(exc)",
            "                msg = _('VM.assert_can_migrate failed')",
            "                raise exception.MigrationPreCheckError(reason=msg)",
            "        return dest_check_data",
            "",
            "    def _generate_vdi_map(self, destination_sr_ref, vm_ref, sr_ref=None):",
            "        \"\"\"generate a vdi_map for _call_live_migrate_command.\"\"\"",
            "        if sr_ref is None:",
            "            sr_ref = vm_utils.safe_find_sr(self._session)",
            "        vm_vdis = vm_utils.get_instance_vdis_for_sr(self._session,",
            "                                                    vm_ref, sr_ref)",
            "        return dict((vdi, destination_sr_ref) for vdi in vm_vdis)",
            "",
            "    def _call_live_migrate_command(self, command_name, vm_ref, migrate_data):",
            "        \"\"\"unpack xapi specific parameters, and call a live migrate command.\"\"\"",
            "        destination_sr_ref = migrate_data['destination_sr_ref']",
            "        migrate_send_data = migrate_data['migrate_send_data']",
            "",
            "        vdi_map = self._generate_vdi_map(destination_sr_ref, vm_ref)",
            "",
            "        # Add destination SR refs for all of the VDIs that we created",
            "        # as part of the pre migration callback",
            "        if 'pre_live_migration_result' in migrate_data:",
            "            pre_migrate_data = migrate_data['pre_live_migration_result']",
            "            sr_uuid_map = pre_migrate_data.get('sr_uuid_map', [])",
            "            for sr_uuid in sr_uuid_map:",
            "                # Source and destination SRs have the same UUID, so get the",
            "                # reference for the local SR",
            "                sr_ref = self._session.call_xenapi(\"SR.get_by_uuid\", sr_uuid)",
            "                vdi_map.update(",
            "                    self._generate_vdi_map(",
            "                        sr_uuid_map[sr_uuid], vm_ref, sr_ref))",
            "        vif_map = {}",
            "        options = {}",
            "        self._session.call_xenapi(command_name, vm_ref,",
            "                                  migrate_send_data, True,",
            "                                  vdi_map, vif_map, options)",
            "",
            "    def live_migrate(self, context, instance, destination_hostname,",
            "                     post_method, recover_method, block_migration,",
            "                     migrate_data=None):",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            if block_migration:",
            "                if not migrate_data:",
            "                    raise exception.InvalidParameterValue('Block Migration '",
            "                                    'requires migrate data from destination')",
            "",
            "                iscsi_srs = self._get_iscsi_srs(context, instance)",
            "                try:",
            "                    self._call_live_migrate_command(",
            "                        \"VM.migrate_send\", vm_ref, migrate_data)",
            "                except self._session.XenAPI.Failure as exc:",
            "                    LOG.exception(exc)",
            "                    raise exception.MigrationError(_('Migrate Send failed'))",
            "",
            "                # Tidy up the iSCSI SRs",
            "                for sr_ref in iscsi_srs:",
            "                    volume_utils.forget_sr(self._session, sr_ref)",
            "            else:",
            "                host_ref = self._get_host_opaque_ref(context,",
            "                                                     destination_hostname)",
            "                self._session.call_xenapi(\"VM.pool_migrate\", vm_ref,",
            "                                          host_ref, {})",
            "            post_method(context, instance, destination_hostname,",
            "                        block_migration)",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                recover_method(context, instance, destination_hostname,",
            "                               block_migration)",
            "",
            "    def get_per_instance_usage(self):",
            "        \"\"\"Get usage info about each active instance.\"\"\"",
            "        usage = {}",
            "",
            "        def _is_active(vm_rec):",
            "            power_state = vm_rec['power_state'].lower()",
            "            return power_state in ['running', 'paused']",
            "",
            "        def _get_uuid(vm_rec):",
            "            other_config = vm_rec['other_config']",
            "            return other_config.get('nova_uuid', None)",
            "",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            uuid = _get_uuid(vm_rec)",
            "",
            "            if _is_active(vm_rec) and uuid is not None:",
            "                memory_mb = int(vm_rec['memory_static_max']) / 1024 / 1024",
            "                usage[uuid] = {'memory_mb': memory_mb, 'uuid': uuid}",
            "",
            "        return usage",
            "",
            "    def attach_block_device_volumes(self, block_device_info):",
            "        sr_uuid_map = {}",
            "        try:",
            "            if block_device_info is not None:",
            "                for block_device_map in block_device_info[",
            "                                                'block_device_mapping']:",
            "                    sr_uuid, _ = self._volumeops.attach_volume(",
            "                        block_device_map['connection_info'],",
            "                        None,",
            "                        block_device_map['mount_device'],",
            "                        hotplug=False)",
            "",
            "                    sr_ref = self._session.call_xenapi('SR.get_by_uuid',",
            "                                                       sr_uuid)",
            "                    sr_uuid_map[sr_uuid] = sr_ref",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                # Disconnect the volumes we just connected",
            "                for sr in sr_uuid_map:",
            "                    volume_utils.forget_sr(self._session, sr_uuid_map[sr_ref])",
            "",
            "        return sr_uuid_map"
        ],
        "afterPatchFile": [
            "# vim: tabstop=4 shiftwidth=4 softtabstop=4",
            "",
            "# Copyright (c) 2010 Citrix Systems, Inc.",
            "# Copyright 2010 OpenStack Foundation",
            "#",
            "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may",
            "#    not use this file except in compliance with the License. You may obtain",
            "#    a copy of the License at",
            "#",
            "#         http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#    Unless required by applicable law or agreed to in writing, software",
            "#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT",
            "#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the",
            "#    License for the specific language governing permissions and limitations",
            "#    under the License.",
            "",
            "\"\"\"",
            "Management class for VM-related functions (spawn, reboot, etc).",
            "\"\"\"",
            "",
            "import base64",
            "import functools",
            "import itertools",
            "import time",
            "import zlib",
            "",
            "from eventlet import greenthread",
            "import netaddr",
            "from oslo.config import cfg",
            "",
            "from nova import block_device",
            "from nova import compute",
            "from nova.compute import flavors",
            "from nova.compute import power_state",
            "from nova.compute import task_states",
            "from nova.compute import vm_mode",
            "from nova.compute import vm_states",
            "from nova import context as nova_context",
            "from nova import exception",
            "from nova.openstack.common import excutils",
            "from nova.openstack.common.gettextutils import _",
            "from nova.openstack.common import importutils",
            "from nova.openstack.common import jsonutils",
            "from nova.openstack.common import log as logging",
            "from nova.openstack.common import strutils",
            "from nova.openstack.common import timeutils",
            "from nova import utils",
            "from nova.virt import configdrive",
            "from nova.virt import driver as virt_driver",
            "from nova.virt import firewall",
            "from nova.virt.xenapi import agent as xapi_agent",
            "from nova.virt.xenapi import pool_states",
            "from nova.virt.xenapi import vm_utils",
            "from nova.virt.xenapi import volume_utils",
            "from nova.virt.xenapi import volumeops",
            "",
            "",
            "LOG = logging.getLogger(__name__)",
            "",
            "xenapi_vmops_opts = [",
            "    cfg.IntOpt('xenapi_running_timeout',",
            "               default=60,",
            "               help='number of seconds to wait for instance '",
            "                    'to go to running state'),",
            "    cfg.StrOpt('xenapi_vif_driver',",
            "               default='nova.virt.xenapi.vif.XenAPIBridgeDriver',",
            "               help='The XenAPI VIF driver using XenServer Network APIs.'),",
            "    cfg.StrOpt('xenapi_image_upload_handler',",
            "                default='nova.virt.xenapi.image.glance.GlanceStore',",
            "                help='Dom0 plugin driver used to handle image uploads.'),",
            "    ]",
            "",
            "CONF = cfg.CONF",
            "CONF.register_opts(xenapi_vmops_opts)",
            "CONF.import_opt('host', 'nova.netconf')",
            "CONF.import_opt('vncserver_proxyclient_address', 'nova.vnc')",
            "",
            "DEFAULT_FIREWALL_DRIVER = \"%s.%s\" % (",
            "    firewall.__name__,",
            "    firewall.IptablesFirewallDriver.__name__)",
            "",
            "RESIZE_TOTAL_STEPS = 5",
            "",
            "DEVICE_ROOT = '0'",
            "DEVICE_RESCUE = '1'",
            "DEVICE_SWAP = '2'",
            "DEVICE_CONFIGDRIVE = '3'",
            "# Note(johngarbutt) HVM guests only support four devices",
            "# until the PV tools activate, when others before available",
            "# As such, ephemeral disk only available once PV tools load",
            "# Note(johngarbutt) When very large ephemeral storage is required,",
            "# multiple disks may be added. In this case the device id below",
            "# is the used for the first disk. The second disk will be given",
            "# next device id, i.e. 5, and so on, until enough space is added.",
            "DEVICE_EPHEMERAL = '4'",
            "# Note(johngarbutt) Currently don't support ISO boot during rescue",
            "# and we must have the ISO visible before the PV drivers start",
            "DEVICE_CD = '1'",
            "",
            "",
            "def cmp_version(a, b):",
            "    \"\"\"Compare two version strings (eg 0.0.1.10 > 0.0.1.9).\"\"\"",
            "    a = a.split('.')",
            "    b = b.split('.')",
            "",
            "    # Compare each individual portion of both version strings",
            "    for va, vb in zip(a, b):",
            "        ret = int(va) - int(vb)",
            "        if ret:",
            "            return ret",
            "",
            "    # Fallback to comparing length last",
            "    return len(a) - len(b)",
            "",
            "",
            "def make_step_decorator(context, instance, update_instance_progress):",
            "    \"\"\"Factory to create a decorator that records instance progress as a series",
            "    of discrete steps.",
            "",
            "    Each time the decorator is invoked we bump the total-step-count, so after::",
            "",
            "        @step",
            "        def step1():",
            "            ...",
            "",
            "        @step",
            "        def step2():",
            "            ...",
            "",
            "    we have a total-step-count of 2.",
            "",
            "    Each time the step-function (not the step-decorator!) is invoked, we bump",
            "    the current-step-count by 1, so after::",
            "",
            "        step1()",
            "",
            "    the current-step-count would be 1 giving a progress of ``1 / 2 *",
            "    100`` or 50%.",
            "    \"\"\"",
            "    step_info = dict(total=0, current=0)",
            "",
            "    def bump_progress():",
            "        step_info['current'] += 1",
            "        update_instance_progress(context, instance,",
            "                                 step_info['current'], step_info['total'])",
            "",
            "    def step_decorator(f):",
            "        step_info['total'] += 1",
            "",
            "        @functools.wraps(f)",
            "        def inner(*args, **kwargs):",
            "            rv = f(*args, **kwargs)",
            "            bump_progress()",
            "            return rv",
            "",
            "        return inner",
            "",
            "    return step_decorator",
            "",
            "",
            "class VMOps(object):",
            "    \"\"\"",
            "    Management class for VM-related tasks",
            "    \"\"\"",
            "    def __init__(self, session, virtapi):",
            "        self.compute_api = compute.API()",
            "        self._session = session",
            "        self._virtapi = virtapi",
            "        self._volumeops = volumeops.VolumeOps(self._session)",
            "        self.firewall_driver = firewall.load_driver(",
            "            DEFAULT_FIREWALL_DRIVER,",
            "            self._virtapi,",
            "            xenapi_session=self._session)",
            "        vif_impl = importutils.import_class(CONF.xenapi_vif_driver)",
            "        self.vif_driver = vif_impl(xenapi_session=self._session)",
            "        self.default_root_dev = '/dev/sda'",
            "",
            "        LOG.debug(_(\"Importing image upload handler: %s\"),",
            "                  CONF.xenapi_image_upload_handler)",
            "        self.image_upload_handler = importutils.import_object(",
            "                                CONF.xenapi_image_upload_handler)",
            "",
            "    def agent_enabled(self, instance):",
            "        if CONF.xenapi_disable_agent:",
            "            return False",
            "",
            "        return xapi_agent.should_use_agent(instance)",
            "",
            "    def _get_agent(self, instance, vm_ref):",
            "        if self.agent_enabled(instance):",
            "            return xapi_agent.XenAPIBasedAgent(self._session, self._virtapi,",
            "                                               instance, vm_ref)",
            "        raise exception.NovaException(_(\"Error: Agent is disabled\"))",
            "",
            "    def instance_exists(self, name_label):",
            "        return vm_utils.lookup(self._session, name_label) is not None",
            "",
            "    def list_instances(self):",
            "        \"\"\"List VM instances.\"\"\"",
            "        # TODO(justinsb): Should we just always use the details method?",
            "        #  Seems to be the same number of API calls..",
            "        name_labels = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            name_labels.append(vm_rec[\"name_label\"])",
            "",
            "        return name_labels",
            "",
            "    def list_instance_uuids(self):",
            "        \"\"\"Get the list of nova instance uuids for VMs found on the",
            "        hypervisor.",
            "        \"\"\"",
            "        nova_uuids = []",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            other_config = vm_rec['other_config']",
            "            nova_uuid = other_config.get('nova_uuid')",
            "            if nova_uuid:",
            "                nova_uuids.append(nova_uuid)",
            "        return nova_uuids",
            "",
            "    def confirm_migration(self, migration, instance, network_info):",
            "        self._destroy_orig_vm(instance, network_info)",
            "",
            "    def _destroy_orig_vm(self, instance, network_info):",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        return self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "    def _attach_mapped_block_devices(self, instance, block_device_info):",
            "        # We are attaching these volumes before start (no hotplugging)",
            "        # because some guests (windows) don't load PV drivers quickly",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.attach_volume(connection_info,",
            "                                          instance['name'],",
            "                                          mount_device,",
            "                                          hotplug=False)",
            "",
            "    def finish_revert_migration(self, instance, block_device_info=None,",
            "                                power_on=True):",
            "        self._restore_orig_vm_and_cleanup_orphan(instance, block_device_info,",
            "                                                 power_on)",
            "",
            "    def _restore_orig_vm_and_cleanup_orphan(self, instance,",
            "                                            block_device_info, power_on=True):",
            "        # NOTE(sirp): the original vm was suffixed with '-orig'; find it using",
            "        # the old suffix, remove the suffix, then power it back on.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "",
            "        # NOTE(danms): if we're reverting migration in the failure case,",
            "        # make sure we don't have a conflicting vm still running here,",
            "        # as might be the case in a failed migrate-to-same-host situation",
            "        new_ref = vm_utils.lookup(self._session, instance['name'])",
            "        if vm_ref is not None:",
            "            if new_ref is not None:",
            "                self._destroy(instance, new_ref)",
            "            # Remove the '-orig' suffix (which was added in case the",
            "            # resized VM ends up on the source host, common during",
            "            # testing)",
            "            name_label = instance['name']",
            "            vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "            self._attach_mapped_block_devices(instance, block_device_info)",
            "        elif new_ref is not None:",
            "            # We crashed before the -orig backup was made",
            "            vm_ref = new_ref",
            "",
            "        if power_on:",
            "            self._start(instance, vm_ref)",
            "",
            "    def finish_migration(self, context, migration, instance, disk_info,",
            "                         network_info, image_meta, resize_instance,",
            "                         block_device_info=None, power_on=True):",
            "",
            "        def null_step_decorator(f):",
            "            return f",
            "",
            "        def create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                              name_label):",
            "            #TODO(johngarbutt) clean up the move_disks if this is not run",
            "            root_vdi = vm_utils.move_disks(self._session, instance, disk_info)",
            "",
            "            def undo_create_disks():",
            "                vm_utils.safe_destroy_vdis(self._session, [root_vdi['ref']])",
            "",
            "            undo_mgr.undo_with(undo_create_disks)",
            "            return {'root': root_vdi}",
            "",
            "        def completed_callback():",
            "            self._update_instance_progress(context, instance,",
            "                                           step=5,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "        self._spawn(context, instance, image_meta, null_step_decorator,",
            "                    create_disks_step, first_boot=False, injected_files=None,",
            "                    admin_password=None, network_info=network_info,",
            "                    block_device_info=block_device_info, name_label=None,",
            "                    rescue=False, power_on=power_on, resize=resize_instance,",
            "                    completed_callback=completed_callback)",
            "",
            "    def _start(self, instance, vm_ref=None, bad_volumes_callback=None):",
            "        \"\"\"Power on a VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Starting instance\"), instance=instance)",
            "",
            "        # Attached volumes that have become non-responsive will prevent a VM",
            "        # from starting, so scan for these before attempting to start",
            "        #",
            "        # In order to make sure this detach is consistent (virt, BDM, cinder),",
            "        # we only detach in the virt-layer if a callback is provided.",
            "        if bad_volumes_callback:",
            "            bad_devices = self._volumeops.find_bad_volumes(vm_ref)",
            "            for device_name in bad_devices:",
            "                self._volumeops.detach_volume(",
            "                        None, instance['name'], device_name)",
            "",
            "        self._session.call_xenapi('VM.start_on', vm_ref,",
            "                                  self._session.get_xenapi_host(),",
            "                                  False, False)",
            "",
            "        # Allow higher-layers a chance to detach bad-volumes as well (in order",
            "        # to cleanup BDM entries and detach in Cinder)",
            "        if bad_volumes_callback and bad_devices:",
            "            bad_volumes_callback(bad_devices)",
            "",
            "    def spawn(self, context, instance, image_meta, injected_files,",
            "              admin_password, network_info=None, block_device_info=None,",
            "              name_label=None, rescue=False):",
            "",
            "        if block_device_info:",
            "            LOG.debug(_(\"Block device information present: %s\")",
            "                      % block_device_info, instance=instance)",
            "        if block_device_info and not block_device_info['root_device_name']:",
            "            block_device_info['root_device_name'] = self.default_root_dev",
            "",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        @step",
            "        def create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                              name_label):",
            "            vdis = vm_utils.get_vdis_for_instance(context, self._session,",
            "                        instance, name_label, image_meta.get('id'),",
            "                        disk_image_type, block_device_info=block_device_info)",
            "",
            "            def undo_create_disks():",
            "                vdi_refs = [vdi['ref'] for vdi in vdis.values()",
            "                        if not vdi.get('osvol')]",
            "                vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "            undo_mgr.undo_with(undo_create_disks)",
            "            return vdis",
            "",
            "        self._spawn(context, instance, image_meta, step, create_disks_step,",
            "                    True, injected_files, admin_password,",
            "                    network_info, block_device_info, name_label, rescue)",
            "",
            "    def _spawn(self, context, instance, image_meta, step, create_disks_step,",
            "               first_boot, injected_files=None, admin_password=None,",
            "               network_info=None, block_device_info=None,",
            "               name_label=None, rescue=False, power_on=True, resize=True,",
            "               completed_callback=None):",
            "        if name_label is None:",
            "            name_label = instance['name']",
            "",
            "        self._ensure_instance_name_unique(name_label)",
            "        self._ensure_enough_free_mem(instance)",
            "",
            "        @step",
            "        def determine_disk_image_type_step(undo_mgr):",
            "            return vm_utils.determine_disk_image_type(image_meta)",
            "",
            "        @step",
            "        def create_kernel_ramdisk_step(undo_mgr):",
            "            kernel_file, ramdisk_file = vm_utils.create_kernel_and_ramdisk(",
            "                    context, self._session, instance, name_label)",
            "",
            "            def undo_create_kernel_ramdisk():",
            "                vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                        kernel_file, ramdisk_file)",
            "",
            "            undo_mgr.undo_with(undo_create_kernel_ramdisk)",
            "            return kernel_file, ramdisk_file",
            "",
            "        @step",
            "        def create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                kernel_file, ramdisk_file):",
            "            vm_ref = self._create_vm_record(context, instance, name_label,",
            "                    vdis, disk_image_type, kernel_file, ramdisk_file)",
            "",
            "            def undo_create_vm():",
            "                self._destroy(instance, vm_ref, network_info=network_info)",
            "",
            "            undo_mgr.undo_with(undo_create_vm)",
            "            return vm_ref",
            "",
            "        @step",
            "        def attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type):",
            "            try:",
            "                ipxe_boot = strutils.bool_from_string(",
            "                        image_meta['properties']['ipxe_boot'])",
            "            except KeyError:",
            "                ipxe_boot = False",
            "",
            "            if ipxe_boot:",
            "                if 'iso' in vdis:",
            "                    vm_utils.handle_ipxe_iso(",
            "                        self._session, instance, vdis['iso'], network_info)",
            "                else:",
            "                    LOG.warning(_('ipxe_boot is True but no ISO image found'),",
            "                                instance=instance)",
            "",
            "            root_vdi = vdis.get('root')",
            "            if root_vdi and resize:",
            "                self._resize_up_root_vdi(instance, root_vdi)",
            "",
            "            self._attach_disks(instance, vm_ref, name_label, vdis,",
            "                               disk_image_type, admin_password,",
            "                               injected_files)",
            "            if not first_boot:",
            "                self._attach_mapped_block_devices(instance,",
            "                                                  block_device_info)",
            "",
            "            if rescue:",
            "                # NOTE(johannes): Attach root disk to rescue VM now, before",
            "                # booting the VM, since we can't hotplug block devices",
            "                # on non-PV guests",
            "                self._attach_orig_disk_for_rescue(instance, vm_ref)",
            "",
            "        @step",
            "        def inject_instance_data_step(undo_mgr, vm_ref, vdis):",
            "            self._inject_instance_metadata(instance, vm_ref)",
            "            self._inject_auto_disk_config(instance, vm_ref)",
            "            if first_boot:",
            "                self._inject_hostname(instance, vm_ref, rescue)",
            "            self._file_inject_vm_settings(instance, vm_ref, vdis, network_info)",
            "            self.inject_network_info(instance, network_info, vm_ref)",
            "",
            "        @step",
            "        def setup_network_step(undo_mgr, vm_ref):",
            "            self._create_vifs(instance, vm_ref, network_info)",
            "",
            "            try:",
            "                self.firewall_driver.setup_basic_filtering(",
            "                        instance, network_info)",
            "            except NotImplementedError:",
            "                # NOTE(salvatore-orlando): setup_basic_filtering might be",
            "                # empty or not implemented at all, as basic filter could",
            "                # be implemented with VIF rules created by xapi plugin",
            "                pass",
            "",
            "            self.firewall_driver.prepare_instance_filter(instance,",
            "                                                         network_info)",
            "",
            "        @step",
            "        def boot_instance_step(undo_mgr, vm_ref):",
            "            if power_on:",
            "                self._start(instance, vm_ref)",
            "                self._wait_for_instance_to_start(instance, vm_ref)",
            "",
            "        @step",
            "        def configure_booted_instance_step(undo_mgr, vm_ref):",
            "            if first_boot:",
            "                self._configure_new_instance_with_agent(instance, vm_ref,",
            "                        injected_files, admin_password)",
            "                self._remove_hostname(instance, vm_ref)",
            "",
            "        @step",
            "        def apply_security_group_filters_step(undo_mgr):",
            "            self.firewall_driver.apply_instance_filter(instance, network_info)",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            # NOTE(sirp): The create_disks() step will potentially take a",
            "            # *very* long time to complete since it has to fetch the image",
            "            # over the network and images can be several gigs in size. To",
            "            # avoid progress remaining at 0% for too long, make sure the",
            "            # first step is something that completes rather quickly.",
            "            disk_image_type = determine_disk_image_type_step(undo_mgr)",
            "",
            "            vdis = create_disks_step(undo_mgr, disk_image_type, image_meta,",
            "                                     name_label)",
            "            kernel_file, ramdisk_file = create_kernel_ramdisk_step(undo_mgr)",
            "",
            "            vm_ref = create_vm_record_step(undo_mgr, vdis, disk_image_type,",
            "                    kernel_file, ramdisk_file)",
            "            attach_disks_step(undo_mgr, vm_ref, vdis, disk_image_type)",
            "",
            "            inject_instance_data_step(undo_mgr, vm_ref, vdis)",
            "            setup_network_step(undo_mgr, vm_ref)",
            "",
            "            boot_instance_step(undo_mgr, vm_ref)",
            "",
            "            configure_booted_instance_step(undo_mgr, vm_ref)",
            "            apply_security_group_filters_step(undo_mgr)",
            "",
            "            if completed_callback:",
            "                completed_callback()",
            "        except Exception:",
            "            msg = _(\"Failed to spawn, rolling back\")",
            "            undo_mgr.rollback_and_reraise(msg=msg, instance=instance)",
            "",
            "    def _attach_orig_disk_for_rescue(self, instance, vm_ref):",
            "        orig_vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "        vdi_ref = self._find_root_vdi_ref(orig_vm_ref)",
            "        vm_utils.create_vbd(self._session, vm_ref, vdi_ref, DEVICE_RESCUE,",
            "                            bootable=False)",
            "",
            "    def _file_inject_vm_settings(self, instance, vm_ref, vdis, network_info):",
            "        if CONF.flat_injected:",
            "            vm_utils.preconfigure_instance(self._session, instance,",
            "                                           vdis['root']['ref'], network_info)",
            "",
            "    def _ensure_instance_name_unique(self, name_label):",
            "        vm_ref = vm_utils.lookup(self._session, name_label)",
            "        if vm_ref is not None:",
            "            raise exception.InstanceExists(name=name_label)",
            "",
            "    def _ensure_enough_free_mem(self, instance):",
            "        if not vm_utils.is_enough_free_mem(self._session, instance):",
            "            raise exception.InsufficientFreeMemory(uuid=instance['uuid'])",
            "",
            "    def _create_vm_record(self, context, instance, name_label, vdis,",
            "            disk_image_type, kernel_file, ramdisk_file):",
            "        \"\"\"Create the VM record in Xen, making sure that we do not create",
            "        a duplicate name-label.  Also do a rough sanity check on memory",
            "        to try to short-circuit a potential failure later.  (The memory",
            "        check only accounts for running VMs, so it can miss other builds",
            "        that are in progress.)",
            "        \"\"\"",
            "        mode = self._determine_vm_mode(instance, vdis, disk_image_type)",
            "        if instance['vm_mode'] != mode:",
            "            # Update database with normalized (or determined) value",
            "            self._virtapi.instance_update(context,",
            "                                          instance['uuid'], {'vm_mode': mode})",
            "",
            "        use_pv_kernel = (mode == vm_mode.XEN)",
            "        vm_ref = vm_utils.create_vm(self._session, instance, name_label,",
            "                                    kernel_file, ramdisk_file, use_pv_kernel)",
            "        return vm_ref",
            "",
            "    def _determine_vm_mode(self, instance, vdis, disk_image_type):",
            "        current_mode = vm_mode.get_from_instance(instance)",
            "        if current_mode == vm_mode.XEN or current_mode == vm_mode.HVM:",
            "            return current_mode",
            "",
            "        is_pv = False",
            "        if 'root' in vdis:",
            "            os_type = instance['os_type']",
            "            vdi_ref = vdis['root']['ref']",
            "            is_pv = vm_utils.determine_is_pv(self._session, vdi_ref,",
            "                                             disk_image_type, os_type)",
            "        if is_pv:",
            "            return vm_mode.XEN",
            "        else:",
            "            return vm_mode.HVM",
            "",
            "    def _attach_disks(self, instance, vm_ref, name_label, vdis,",
            "                      disk_image_type, admin_password=None, files=None):",
            "        ctx = nova_context.get_admin_context()",
            "        instance_type = flavors.extract_flavor(instance)",
            "",
            "        # Attach (required) root disk",
            "        if disk_image_type == vm_utils.ImageType.DISK_ISO:",
            "            # DISK_ISO needs two VBDs: the ISO disk and a blank RW disk",
            "            root_disk_size = instance_type['root_gb']",
            "            if root_disk_size > 0:",
            "                vm_utils.generate_iso_blank_root_disk(self._session, instance,",
            "                    vm_ref, DEVICE_ROOT, name_label, root_disk_size)",
            "",
            "            cd_vdi = vdis.pop('iso')",
            "            vm_utils.attach_cd(self._session, vm_ref, cd_vdi['ref'],",
            "                               DEVICE_CD)",
            "        else:",
            "            root_vdi = vdis['root']",
            "",
            "            if instance['auto_disk_config']:",
            "                LOG.debug(_(\"Auto configuring disk, attempting to \"",
            "                            \"resize partition...\"), instance=instance)",
            "                vm_utils.try_auto_configure_disk(self._session,",
            "                                                 root_vdi['ref'],",
            "                                                 instance_type['root_gb'])",
            "",
            "            vm_utils.create_vbd(self._session, vm_ref, root_vdi['ref'],",
            "                                DEVICE_ROOT, bootable=True,",
            "                                osvol=root_vdi.get('osvol'))",
            "",
            "        # Attach (optional) additional block-devices",
            "        for type_, vdi_info in vdis.items():",
            "            # Additional block-devices for boot use their device-name as the",
            "            # type.",
            "            if not type_.startswith('/dev'):",
            "                continue",
            "",
            "            # Convert device name to userdevice number, e.g. /dev/xvdb -> 1",
            "            userdevice = ord(block_device.strip_prefix(type_)) - ord('a')",
            "            vm_utils.create_vbd(self._session, vm_ref, vdi_info['ref'],",
            "                                userdevice, bootable=False,",
            "                                osvol=vdi_info.get('osvol'))",
            "",
            "        # Attach (optional) swap disk",
            "        swap_mb = instance_type['swap']",
            "        if swap_mb:",
            "            vm_utils.generate_swap(self._session, instance, vm_ref,",
            "                                   DEVICE_SWAP, name_label, swap_mb)",
            "",
            "        # Attach (optional) ephemeral disk",
            "        ephemeral_gb = instance_type['ephemeral_gb']",
            "        if ephemeral_gb:",
            "            vm_utils.generate_ephemeral(self._session, instance, vm_ref,",
            "                                        DEVICE_EPHEMERAL, name_label,",
            "                                        ephemeral_gb)",
            "",
            "        # Attach (optional) configdrive v2 disk",
            "        if configdrive.required_by(instance):",
            "            vm_utils.generate_configdrive(self._session, instance, vm_ref,",
            "                                          DEVICE_CONFIGDRIVE,",
            "                                          admin_password=admin_password,",
            "                                          files=files)",
            "",
            "    def _wait_for_instance_to_start(self, instance, vm_ref):",
            "        LOG.debug(_('Waiting for instance state to become running'),",
            "                  instance=instance)",
            "        expiration = time.time() + CONF.xenapi_running_timeout",
            "        while time.time() < expiration:",
            "            state = self.get_info(instance, vm_ref)['state']",
            "            if state == power_state.RUNNING:",
            "                break",
            "            greenthread.sleep(0.5)",
            "",
            "    def _configure_new_instance_with_agent(self, instance, vm_ref,",
            "                                           injected_files, admin_password):",
            "        if self.agent_enabled(instance):",
            "            ctx = nova_context.get_admin_context()",
            "            agent_build = self._virtapi.agent_build_get_by_triple(",
            "                ctx, 'xen', instance['os_type'], instance['architecture'])",
            "            if agent_build:",
            "                LOG.info(_('Latest agent build for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s is %(version)s') % agent_build)",
            "            else:",
            "                LOG.info(_('No agent build found for %(hypervisor)s/%(os)s'",
            "                           '/%(architecture)s') % {",
            "                            'hypervisor': 'xen',",
            "                            'os': instance['os_type'],",
            "                            'architecture': instance['architecture']})",
            "",
            "            # Update agent, if necessary",
            "            # This also waits until the agent starts",
            "            agent = self._get_agent(instance, vm_ref)",
            "            version = agent.get_agent_version()",
            "            if version:",
            "                LOG.info(_('Instance agent version: %s'), version,",
            "                         instance=instance)",
            "",
            "            if (version and agent_build and",
            "                    cmp_version(version, agent_build['version']) < 0):",
            "                agent.agent_update(agent_build)",
            "",
            "            # if the guest agent is not available, configure the",
            "            # instance, but skip the admin password configuration",
            "            no_agent = version is None",
            "",
            "            # Inject ssh key.",
            "            agent.inject_ssh_key()",
            "",
            "            # Inject files, if necessary",
            "            if injected_files:",
            "                # Inject any files, if specified",
            "                agent.inject_files(injected_files)",
            "",
            "            # Set admin password, if necessary",
            "            if admin_password and not no_agent:",
            "                agent.set_admin_password(admin_password)",
            "",
            "            # Reset network config",
            "            agent.resetnetwork()",
            "",
            "    def _get_vm_opaque_ref(self, instance, check_rescue=False):",
            "        \"\"\"Get xapi OpaqueRef from a db record.",
            "        :param check_rescue: if True will return the 'name'-rescue vm if it",
            "                             exists, instead of just 'name'",
            "        \"\"\"",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'], check_rescue)",
            "        if vm_ref is None:",
            "            raise exception.NotFound(_('Could not find VM with name %s') %",
            "                                     instance['name'])",
            "        return vm_ref",
            "",
            "    def _acquire_bootlock(self, vm):",
            "        \"\"\"Prevent an instance from booting.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.set_blocked_operations\",",
            "            vm,",
            "            {\"start\": \"\"})",
            "",
            "    def _release_bootlock(self, vm):",
            "        \"\"\"Allow an instance to boot.\"\"\"",
            "        self._session.call_xenapi(",
            "            \"VM.remove_from_blocked_operations\",",
            "            vm,",
            "            \"start\")",
            "",
            "    def snapshot(self, context, instance, image_id, update_task_state):",
            "        \"\"\"Create snapshot from a running VM instance.",
            "",
            "        :param context: request context",
            "        :param instance: instance to be snapshotted",
            "        :param image_id: id of image to upload to",
            "",
            "        Steps involved in a XenServer snapshot:",
            "",
            "        1. XAPI-Snapshot: Snapshotting the instance using XenAPI. This",
            "           creates: Snapshot (Template) VM, Snapshot VBD, Snapshot VDI,",
            "           Snapshot VHD",
            "",
            "        2. Wait-for-coalesce: The Snapshot VDI and Instance VDI both point to",
            "           a 'base-copy' VDI.  The base_copy is immutable and may be chained",
            "           with other base_copies.  If chained, the base_copies",
            "           coalesce together, so, we must wait for this coalescing to occur to",
            "           get a stable representation of the data on disk.",
            "",
            "        3. Push-to-data-store: Once coalesced, we call",
            "           'xenapi_image_upload_handler' to upload the images.",
            "",
            "        \"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        label = \"%s-snapshot\" % instance['name']",
            "",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label,",
            "                update_task_state) as vdi_uuids:",
            "            update_task_state(task_state=task_states.IMAGE_UPLOADING,",
            "                              expected_state=task_states.IMAGE_PENDING_UPLOAD)",
            "            self.image_upload_handler.upload_image(context,",
            "                                                   self._session,",
            "                                                   instance,",
            "                                                   vdi_uuids,",
            "                                                   image_id)",
            "",
            "        LOG.debug(_(\"Finished snapshot and upload for VM\"),",
            "                  instance=instance)",
            "",
            "    def _migrate_vhd(self, instance, vdi_uuid, dest, sr_path, seq_num):",
            "        LOG.debug(_(\"Migrating VHD '%(vdi_uuid)s' with seq_num %(seq_num)d\"),",
            "                  {'vdi_uuid': vdi_uuid, 'seq_num': seq_num},",
            "                  instance=instance)",
            "        instance_uuid = instance['uuid']",
            "        try:",
            "            self._session.call_plugin_serialized('migration', 'transfer_vhd',",
            "                    instance_uuid=instance_uuid, host=dest, vdi_uuid=vdi_uuid,",
            "                    sr_path=sr_path, seq_num=seq_num)",
            "        except self._session.XenAPI.Failure:",
            "            msg = _(\"Failed to transfer vhd to new host\")",
            "            raise exception.MigrationError(reason=msg)",
            "",
            "    def _get_orig_vm_name_label(self, instance):",
            "        return instance['name'] + '-orig'",
            "",
            "    def _update_instance_progress(self, context, instance, step, total_steps):",
            "        \"\"\"Update instance progress percent to reflect current step number",
            "        \"\"\"",
            "        # FIXME(sirp): for now we're taking a KISS approach to instance",
            "        # progress:",
            "        # Divide the action's workflow into discrete steps and \"bump\" the",
            "        # instance's progress field as each step is completed.",
            "        #",
            "        # For a first cut this should be fine, however, for large VM images,",
            "        # the get_vdis_for_instance step begins to dominate the equation. A",
            "        # better approximation would use the percentage of the VM image that",
            "        # has been streamed to the destination host.",
            "        progress = round(float(step) / total_steps * 100)",
            "        LOG.debug(_(\"Updating progress to %d\"), progress,",
            "                  instance=instance)",
            "        self._virtapi.instance_update(context, instance['uuid'],",
            "                                      {'progress': progress})",
            "",
            "    def _resize_ensure_vm_is_shutdown(self, instance, vm_ref):",
            "        if vm_utils.is_vm_shutdown(self._session, vm_ref):",
            "            LOG.debug(_(\"VM was already shutdown.\"), instance=instance)",
            "            return",
            "",
            "        if not vm_utils.clean_shutdown_vm(self._session, instance, vm_ref):",
            "            LOG.debug(_(\"Clean shutdown did not complete successfully, \"",
            "                        \"trying hard shutdown.\"), instance=instance)",
            "            if not vm_utils.hard_shutdown_vm(self._session, instance, vm_ref):",
            "                raise exception.ResizeError(",
            "                    reason=_(\"Unable to terminate instance.\"))",
            "",
            "    def _migrate_disk_resizing_down(self, context, instance, dest,",
            "                                    instance_type, vm_ref, sr_path):",
            "        step = make_step_decorator(context, instance,",
            "                                   self._update_instance_progress)",
            "",
            "        @step",
            "        def fake_step_to_match_resizing_up():",
            "            pass",
            "",
            "        @step",
            "        def rename_and_power_off_vm(undo_mgr):",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "            def restore_orig_vm():",
            "                # Do not need to restore block devices, not yet been removed",
            "                self._restore_orig_vm_and_cleanup_orphan(instance, None)",
            "",
            "            undo_mgr.undo_with(restore_orig_vm)",
            "",
            "        @step",
            "        def create_copy_vdi_and_resize(undo_mgr, old_vdi_ref):",
            "            new_vdi_ref, new_vdi_uuid = vm_utils.resize_disk(self._session,",
            "                instance, old_vdi_ref, instance_type)",
            "",
            "            def cleanup_vdi_copy():",
            "                vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "            undo_mgr.undo_with(cleanup_vdi_copy)",
            "",
            "            return new_vdi_ref, new_vdi_uuid",
            "",
            "        @step",
            "        def transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid):",
            "            self._migrate_vhd(instance, new_vdi_uuid, dest, sr_path, 0)",
            "            # Clean up VDI now that it's been copied",
            "            vm_utils.destroy_vdi(self._session, new_vdi_ref)",
            "",
            "        @step",
            "        def fake_step_to_be_executed_by_finish_migration():",
            "            pass",
            "",
            "        undo_mgr = utils.UndoManager()",
            "        try:",
            "            fake_step_to_match_resizing_up()",
            "            rename_and_power_off_vm(undo_mgr)",
            "            old_vdi_ref, _ignore = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            new_vdi_ref, new_vdi_uuid = create_copy_vdi_and_resize(",
            "                undo_mgr, old_vdi_ref)",
            "            transfer_vhd_to_dest(new_vdi_ref, new_vdi_uuid)",
            "        except Exception as error:",
            "            LOG.exception(_(\"_migrate_disk_resizing_down failed. \"",
            "                            \"Restoring orig vm due_to: %s.\"), error,",
            "                          instance=instance)",
            "            undo_mgr._rollback()",
            "            raise exception.InstanceFaultRollback(error)",
            "",
            "    def _migrate_disk_resizing_up(self, context, instance, dest, vm_ref,",
            "                                  sr_path):",
            "        self._apply_orig_vm_name_label(instance, vm_ref)",
            "",
            "        # 1. Create Snapshot",
            "        label = \"%s-snapshot\" % instance['name']",
            "        with vm_utils.snapshot_attached_here(",
            "                self._session, instance, vm_ref, label) as vdi_uuids:",
            "            self._update_instance_progress(context, instance,",
            "                                           step=1,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 2. Transfer the immutable VHDs (base-copies)",
            "            #",
            "            # The first VHD will be the leaf (aka COW) that is being used by",
            "            # the VM. For this step, we're only interested in the immutable",
            "            # VHDs which are all of the parents of the leaf VHD.",
            "            for seq_num, vdi_uuid in itertools.islice(",
            "                    enumerate(vdi_uuids), 1, None):",
            "                self._migrate_vhd(instance, vdi_uuid, dest, sr_path, seq_num)",
            "                self._update_instance_progress(context, instance,",
            "                                               step=2,",
            "                                               total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 3. Now power down the instance",
            "            self._resize_ensure_vm_is_shutdown(instance, vm_ref)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=3,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "            # 4. Transfer the COW VHD",
            "            vdi_ref, vm_vdi_rec = vm_utils.get_vdi_for_vm_safely(",
            "                self._session, vm_ref)",
            "            cow_uuid = vm_vdi_rec['uuid']",
            "            self._migrate_vhd(instance, cow_uuid, dest, sr_path, 0)",
            "            self._update_instance_progress(context, instance,",
            "                                           step=4,",
            "                                           total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "    def _apply_orig_vm_name_label(self, instance, vm_ref):",
            "        # NOTE(sirp): in case we're resizing to the same host (for dev",
            "        # purposes), apply a suffix to name-label so the two VM records",
            "        # extant until a confirm_resize don't collide.",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        vm_utils.set_vm_name_label(self._session, vm_ref, name_label)",
            "",
            "    def migrate_disk_and_power_off(self, context, instance, dest,",
            "                                   instance_type, block_device_info):",
            "        \"\"\"Copies a VHD from one host machine to another, possibly",
            "        resizing filesystem before hand.",
            "",
            "        :param instance: the instance that owns the VHD in question.",
            "        :param dest: the destination host machine.",
            "        :param instance_type: instance_type to resize to",
            "        \"\"\"",
            "        # 0. Zero out the progress to begin",
            "        self._update_instance_progress(context, instance,",
            "                                       step=0,",
            "                                       total_steps=RESIZE_TOTAL_STEPS)",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        sr_path = vm_utils.get_sr_path(self._session)",
            "",
            "        old_gb = instance['root_gb']",
            "        new_gb = instance_type['root_gb']",
            "        resize_down = old_gb > new_gb",
            "",
            "        if resize_down:",
            "            self._migrate_disk_resizing_down(",
            "                    context, instance, dest, instance_type, vm_ref, sr_path)",
            "        else:",
            "            self._migrate_disk_resizing_up(",
            "                    context, instance, dest, vm_ref, sr_path)",
            "",
            "        self._detach_block_devices_from_orig_vm(instance, block_device_info)",
            "",
            "        # NOTE(sirp): disk_info isn't used by the xenapi driver, instead it",
            "        # uses a staging-area (/images/instance<uuid>) and sequence-numbered",
            "        # VHDs to figure out how to reconstruct the VDI chain after syncing",
            "        disk_info = {}",
            "        return disk_info",
            "",
            "    def _detach_block_devices_from_orig_vm(self, instance, block_device_info):",
            "        block_device_mapping = virt_driver.block_device_info_get_mapping(",
            "                block_device_info)",
            "        name_label = self._get_orig_vm_name_label(instance)",
            "        for vol in block_device_mapping:",
            "            connection_info = vol['connection_info']",
            "            mount_device = vol['mount_device'].rpartition(\"/\")[2]",
            "            self._volumeops.detach_volume(connection_info, name_label,",
            "                                          mount_device)",
            "",
            "    def _resize_up_root_vdi(self, instance, root_vdi):",
            "        \"\"\"Resize an instances root disk.\"\"\"",
            "",
            "        new_disk_size = instance['root_gb'] * 1024 * 1024 * 1024",
            "        if not new_disk_size:",
            "            return",
            "",
            "        # Get current size of VDI",
            "        virtual_size = self._session.call_xenapi('VDI.get_virtual_size',",
            "                                                 root_vdi['ref'])",
            "        virtual_size = int(virtual_size)",
            "",
            "        old_gb = virtual_size / (1024 * 1024 * 1024)",
            "        new_gb = instance['root_gb']",
            "",
            "        if virtual_size < new_disk_size:",
            "            # Resize up. Simple VDI resize will do the trick",
            "            vdi_uuid = root_vdi['uuid']",
            "            LOG.debug(_(\"Resizing up VDI %(vdi_uuid)s from %(old_gb)dGB to \"",
            "                        \"%(new_gb)dGB\"),",
            "                      {'vdi_uuid': vdi_uuid, 'old_gb': old_gb,",
            "                       'new_gb': new_gb}, instance=instance)",
            "            resize_func_name = self.check_resize_func_name()",
            "            self._session.call_xenapi(resize_func_name, root_vdi['ref'],",
            "                    str(new_disk_size))",
            "            LOG.debug(_(\"Resize complete\"), instance=instance)",
            "",
            "    def check_resize_func_name(self):",
            "        \"\"\"Check the function name used to resize an instance based",
            "        on product_brand and product_version.",
            "        \"\"\"",
            "",
            "        brand = self._session.product_brand",
            "        version = self._session.product_version",
            "",
            "        # To maintain backwards compatibility. All recent versions",
            "        # should use VDI.resize",
            "        if bool(version) and bool(brand):",
            "            xcp = brand == 'XCP'",
            "            r1_2_or_above = (",
            "                (",
            "                    version[0] == 1",
            "                    and version[1] > 1",
            "                )",
            "                or version[0] > 1)",
            "",
            "            xenserver = brand == 'XenServer'",
            "            r6_or_above = version[0] > 5",
            "",
            "            if (xcp and not r1_2_or_above) or (xenserver and not r6_or_above):",
            "                return 'VDI.resize_online'",
            "",
            "        return 'VDI.resize'",
            "",
            "    def reboot(self, instance, reboot_type, bad_volumes_callback=None):",
            "        \"\"\"Reboot VM instance.\"\"\"",
            "        # Note (salvatore-orlando): security group rules are not re-enforced",
            "        # upon reboot, since this action on the XenAPI drivers does not",
            "        # remove existing filters",
            "        vm_ref = self._get_vm_opaque_ref(instance, check_rescue=True)",
            "",
            "        try:",
            "            if reboot_type == \"HARD\":",
            "                self._session.call_xenapi('VM.hard_reboot', vm_ref)",
            "            else:",
            "                self._session.call_xenapi('VM.clean_reboot', vm_ref)",
            "        except self._session.XenAPI.Failure as exc:",
            "            details = exc.details",
            "            if (details[0] == 'VM_BAD_POWER_STATE' and",
            "                    details[-1] == 'halted'):",
            "                LOG.info(_(\"Starting halted instance found during reboot\"),",
            "                    instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            elif details[0] == 'SR_BACKEND_FAILURE_46':",
            "                LOG.warn(_(\"Reboot failed due to bad volumes, detaching bad\"",
            "                           \" volumes and starting halted instance\"),",
            "                         instance=instance)",
            "                self._start(instance, vm_ref=vm_ref,",
            "                            bad_volumes_callback=bad_volumes_callback)",
            "                return",
            "            else:",
            "                raise",
            "",
            "    def set_admin_password(self, instance, new_pass):",
            "        \"\"\"Set the root/admin password on the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.set_admin_password(new_pass)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def inject_file(self, instance, path, contents):",
            "        \"\"\"Write a file to the VM instance.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.inject_file(path, contents)",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def _sanitize_xenstore_key(key):",
            "        \"\"\"",
            "        Xenstore only allows the following characters as keys:",
            "",
            "        ABCDEFGHIJKLMNOPQRSTUVWXYZ",
            "        abcdefghijklmnopqrstuvwxyz",
            "        0123456789-/_@",
            "",
            "        So convert the others to _",
            "",
            "        Also convert / to _, because that is somewhat like a path",
            "        separator.",
            "        \"\"\"",
            "        allowed_chars = (\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"",
            "                         \"abcdefghijklmnopqrstuvwxyz\"",
            "                         \"0123456789-_@\")",
            "        return ''.join([x in allowed_chars and x or '_' for x in key])",
            "",
            "    def _inject_instance_metadata(self, instance, vm_ref):",
            "        \"\"\"Inject instance metadata into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_meta(topdir, data_dict):",
            "            for key, value in data_dict.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                value = value or ''",
            "                self._add_to_param_xenstore(vm_ref, '%s/%s' % (topdir, key),",
            "                                            jsonutils.dumps(value))",
            "",
            "        # Store user metadata",
            "        store_meta('vm-data/user-metadata', utils.instance_meta(instance))",
            "",
            "    def _inject_auto_disk_config(self, instance, vm_ref):",
            "        \"\"\"Inject instance's auto_disk_config attribute into xenstore.\"\"\"",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def store_auto_disk_config(key, value):",
            "            value = value and True or False",
            "            self._add_to_param_xenstore(vm_ref, key, str(value))",
            "",
            "        store_auto_disk_config('vm-data/auto-disk-config',",
            "                               instance['auto_disk_config'])",
            "",
            "    def change_instance_metadata(self, instance, diff):",
            "        \"\"\"Apply changes to instance metadata to xenstore.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            # NOTE(johngarbutt) race conditions mean we can still get here",
            "            # during operations where the VM is not present, like resize.",
            "            # Skip the update when not possible, as the updated metadata will",
            "            # get added when the VM is being booted up at the end of the",
            "            # resize or rebuild.",
            "            LOG.warn(_(\"Unable to update metadata, VM not found.\"),",
            "                     instance=instance, exc_info=True)",
            "            return",
            "",
            "        def process_change(location, change):",
            "            if change[0] == '-':",
            "                self._remove_from_param_xenstore(vm_ref, location)",
            "                try:",
            "                    self._delete_from_xenstore(instance, location,",
            "                                               vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "            elif change[0] == '+':",
            "                self._add_to_param_xenstore(vm_ref, location,",
            "                                            jsonutils.dumps(change[1]))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, change[1],",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_meta():",
            "            for key, change in diff.items():",
            "                key = self._sanitize_xenstore_key(key)",
            "                location = 'vm-data/user-metadata/%s' % key",
            "                process_change(location, change)",
            "        update_meta()",
            "",
            "    def _find_root_vdi_ref(self, vm_ref):",
            "        \"\"\"Find and return the root vdi ref for a VM.\"\"\"",
            "        if not vm_ref:",
            "            return None",
            "",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        for vbd_uuid in vbd_refs:",
            "            vbd = self._session.call_xenapi(\"VBD.get_record\", vbd_uuid)",
            "            if vbd[\"userdevice\"] == DEVICE_ROOT:",
            "                return vbd[\"VDI\"]",
            "",
            "        raise exception.NotFound(_(\"Unable to find root VBD/VDI for VM\"))",
            "",
            "    def _destroy_vdis(self, instance, vm_ref):",
            "        \"\"\"Destroys all VDIs associated with a VM.\"\"\"",
            "        LOG.debug(_(\"Destroying VDIs\"), instance=instance)",
            "",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, vm_ref)",
            "        if not vdi_refs:",
            "            return",
            "        for vdi_ref in vdi_refs:",
            "            try:",
            "                vm_utils.destroy_vdi(self._session, vdi_ref)",
            "            except volume_utils.StorageError as exc:",
            "                LOG.error(exc)",
            "",
            "    def _destroy_kernel_ramdisk(self, instance, vm_ref):",
            "        \"\"\"Three situations can occur:",
            "",
            "            1. We have neither a ramdisk nor a kernel, in which case we are a",
            "               RAW image and can omit this step",
            "",
            "            2. We have one or the other, in which case, we should flag as an",
            "               error",
            "",
            "            3. We have both, in which case we safely remove both the kernel",
            "               and the ramdisk.",
            "",
            "        \"\"\"",
            "        instance_uuid = instance['uuid']",
            "        if not instance['kernel_id'] and not instance['ramdisk_id']:",
            "            # 1. No kernel or ramdisk",
            "            LOG.debug(_(\"Using RAW or VHD, skipping kernel and ramdisk \"",
            "                        \"deletion\"), instance=instance)",
            "            return",
            "",
            "        if not (instance['kernel_id'] and instance['ramdisk_id']):",
            "            # 2. We only have kernel xor ramdisk",
            "            raise exception.InstanceUnacceptable(instance_id=instance_uuid,",
            "               reason=_(\"instance has a kernel or ramdisk but not both\"))",
            "",
            "        # 3. We have both kernel and ramdisk",
            "        (kernel, ramdisk) = vm_utils.lookup_kernel_ramdisk(self._session,",
            "                                                           vm_ref)",
            "        if kernel or ramdisk:",
            "            vm_utils.destroy_kernel_ramdisk(self._session, instance,",
            "                                            kernel, ramdisk)",
            "            LOG.debug(_(\"kernel/ramdisk files removed\"), instance=instance)",
            "",
            "    def _destroy_rescue_instance(self, rescue_vm_ref, original_vm_ref):",
            "        \"\"\"Destroy a rescue instance.\"\"\"",
            "        # Shutdown Rescue VM",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", rescue_vm_ref)",
            "        state = vm_utils.compile_info(vm_rec)['state']",
            "        if state != power_state.SHUTDOWN:",
            "            self._session.call_xenapi(\"VM.hard_shutdown\", rescue_vm_ref)",
            "",
            "        # Destroy Rescue VDIs",
            "        vdi_refs = vm_utils.lookup_vm_vdis(self._session, rescue_vm_ref)",
            "        root_vdi_ref = self._find_root_vdi_ref(original_vm_ref)",
            "        vdi_refs = [vdi_ref for vdi_ref in vdi_refs if vdi_ref != root_vdi_ref]",
            "        vm_utils.safe_destroy_vdis(self._session, vdi_refs)",
            "",
            "        # Destroy Rescue VM",
            "        self._session.call_xenapi(\"VM.destroy\", rescue_vm_ref)",
            "",
            "    def destroy(self, instance, network_info, block_device_info=None,",
            "                destroy_disks=True):",
            "        \"\"\"Destroy VM instance.",
            "",
            "        This is the method exposed by xenapi_conn.destroy(). The rest of the",
            "        destroy_* methods are internal.",
            "",
            "        \"\"\"",
            "        LOG.info(_(\"Destroying VM\"), instance=instance)",
            "",
            "        # We don't use _get_vm_opaque_ref because the instance may",
            "        # truly not exist because of a failure during build. A valid",
            "        # vm_ref is checked correctly where necessary.",
            "        vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if rescue_vm_ref:",
            "            self._destroy_rescue_instance(rescue_vm_ref, vm_ref)",
            "",
            "        # NOTE(sirp): `block_device_info` is not used, information about which",
            "        # volumes should be detached is determined by the",
            "        # VBD.other_config['osvol'] attribute",
            "        return self._destroy(instance, vm_ref, network_info=network_info,",
            "                             destroy_disks=destroy_disks)",
            "",
            "    def _destroy(self, instance, vm_ref, network_info=None,",
            "                 destroy_disks=True):",
            "        \"\"\"Destroys VM instance by performing:",
            "",
            "            1. A shutdown",
            "            2. Destroying associated VDIs.",
            "            3. Destroying kernel and ramdisk files (if necessary).",
            "            4. Destroying that actual VM record.",
            "",
            "        \"\"\"",
            "        if vm_ref is None:",
            "            LOG.warning(_(\"VM is not present, skipping destroy...\"),",
            "                        instance=instance)",
            "            return",
            "",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "        if destroy_disks:",
            "            self._volumeops.detach_all(vm_ref)",
            "            self._destroy_vdis(instance, vm_ref)",
            "            self._destroy_kernel_ramdisk(instance, vm_ref)",
            "",
            "        vm_utils.destroy_vm(self._session, instance, vm_ref)",
            "",
            "        self.unplug_vifs(instance, network_info)",
            "        self.firewall_driver.unfilter_instance(",
            "                instance, network_info=network_info)",
            "",
            "    def pause(self, instance):",
            "        \"\"\"Pause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.pause', vm_ref)",
            "",
            "    def unpause(self, instance):",
            "        \"\"\"Unpause VM instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._session.call_xenapi('VM.unpause', vm_ref)",
            "",
            "    def suspend(self, instance):",
            "        \"\"\"Suspend the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._acquire_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.suspend', vm_ref)",
            "",
            "    def resume(self, instance):",
            "        \"\"\"Resume the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._session.call_xenapi('VM.resume', vm_ref, False, True)",
            "",
            "    def rescue(self, context, instance, network_info, image_meta,",
            "               rescue_password):",
            "        \"\"\"Rescue the specified instance.",
            "",
            "            - shutdown the instance VM.",
            "            - set 'bootlock' to prevent the instance from starting in rescue.",
            "            - spawn a rescue VM (the vm name-label will be instance-N-rescue).",
            "",
            "        \"\"\"",
            "        rescue_name_label = '%s-rescue' % instance['name']",
            "        rescue_vm_ref = vm_utils.lookup(self._session, rescue_name_label)",
            "        if rescue_vm_ref:",
            "            raise RuntimeError(_(\"Instance is already in Rescue Mode: %s\")",
            "                               % instance['name'])",
            "",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "        self._acquire_bootlock(vm_ref)",
            "        self.spawn(context, instance, image_meta, [], rescue_password,",
            "                   network_info, name_label=rescue_name_label, rescue=True)",
            "",
            "    def unrescue(self, instance):",
            "        \"\"\"Unrescue the specified instance.",
            "",
            "            - unplug the instance VM's disk from the rescue VM.",
            "            - teardown the rescue VM.",
            "            - release the bootlock to allow the instance VM to start.",
            "",
            "        \"\"\"",
            "        rescue_vm_ref = vm_utils.lookup(self._session,",
            "                                        \"%s-rescue\" % instance['name'])",
            "        if not rescue_vm_ref:",
            "            raise exception.InstanceNotInRescueMode(",
            "                    instance_id=instance['uuid'])",
            "",
            "        original_vm_ref = self._get_vm_opaque_ref(instance)",
            "",
            "        self._destroy_rescue_instance(rescue_vm_ref, original_vm_ref)",
            "        self._release_bootlock(original_vm_ref)",
            "        self._start(instance, original_vm_ref)",
            "",
            "    def soft_delete(self, instance):",
            "        \"\"\"Soft delete the specified instance.\"\"\"",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "        except exception.NotFound:",
            "            LOG.warning(_(\"VM is not present, skipping soft delete...\"),",
            "                        instance=instance)",
            "        else:",
            "            vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "            self._acquire_bootlock(vm_ref)",
            "",
            "    def restore(self, instance):",
            "        \"\"\"Restore the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._release_bootlock(vm_ref)",
            "        self._start(instance, vm_ref)",
            "",
            "    def power_off(self, instance):",
            "        \"\"\"Power off the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_utils.hard_shutdown_vm(self._session, instance, vm_ref)",
            "",
            "    def power_on(self, instance):",
            "        \"\"\"Power on the specified instance.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        self._start(instance, vm_ref)",
            "",
            "    def _cancel_stale_tasks(self, timeout, task):",
            "        \"\"\"Cancel the given tasks that are older than the given timeout.\"\"\"",
            "        task_refs = self._session.call_xenapi(\"task.get_by_name_label\", task)",
            "        for task_ref in task_refs:",
            "            task_rec = self._session.call_xenapi(\"task.get_record\", task_ref)",
            "            task_created = timeutils.parse_strtime(task_rec[\"created\"].value,",
            "                                                   \"%Y%m%dT%H:%M:%SZ\")",
            "",
            "            if timeutils.is_older_than(task_created, timeout):",
            "                self._session.call_xenapi(\"task.cancel\", task_ref)",
            "",
            "    def poll_rebooting_instances(self, timeout, instances):",
            "        \"\"\"Look for expirable rebooting instances.",
            "",
            "            - issue a \"hard\" reboot to any instance that has been stuck in a",
            "              reboot state for >= the given timeout",
            "        \"\"\"",
            "        # NOTE(jk0): All existing clean_reboot tasks must be cancelled before",
            "        # we can kick off the hard_reboot tasks.",
            "        self._cancel_stale_tasks(timeout, 'VM.clean_reboot')",
            "",
            "        ctxt = nova_context.get_admin_context()",
            "",
            "        instances_info = dict(instance_count=len(instances),",
            "                timeout=timeout)",
            "",
            "        if instances_info[\"instance_count\"] > 0:",
            "            LOG.info(_(\"Found %(instance_count)d hung reboots \"",
            "                       \"older than %(timeout)d seconds\") % instances_info)",
            "",
            "        for instance in instances:",
            "            LOG.info(_(\"Automatically hard rebooting\"), instance=instance)",
            "            self.compute_api.reboot(ctxt, instance, \"HARD\")",
            "",
            "    def get_info(self, instance, vm_ref=None):",
            "        \"\"\"Return data about VM instance.\"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_info(vm_rec)",
            "",
            "    def get_diagnostics(self, instance):",
            "        \"\"\"Return data about VM diagnostics.\"\"\"",
            "        vm_ref = self._get_vm_opaque_ref(instance)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_utils.compile_diagnostics(vm_rec)",
            "",
            "    def _get_vif_device_map(self, vm_rec):",
            "        vif_map = {}",
            "        for vif in [self._session.call_xenapi(\"VIF.get_record\", vrec)",
            "                    for vrec in vm_rec['VIFs']]:",
            "            vif_map[vif['device']] = vif['MAC']",
            "        return vif_map",
            "",
            "    def get_all_bw_counters(self):",
            "        \"\"\"Return running bandwidth counter for each interface on each",
            "           running VM.",
            "        \"\"\"",
            "        counters = vm_utils.fetch_bandwidth(self._session)",
            "        bw = {}",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            vif_map = self._get_vif_device_map(vm_rec)",
            "            name = vm_rec['name_label']",
            "            if 'nova_uuid' not in vm_rec['other_config']:",
            "                continue",
            "            dom = vm_rec.get('domid')",
            "            if dom is None or dom not in counters:",
            "                continue",
            "            vifs_bw = bw.setdefault(name, {})",
            "            for vif_num, vif_data in counters[dom].iteritems():",
            "                mac = vif_map[vif_num]",
            "                vif_data['mac_address'] = mac",
            "                vifs_bw[mac] = vif_data",
            "        return bw",
            "",
            "    def get_console_output(self, instance):",
            "        \"\"\"Return last few lines of instance console.\"\"\"",
            "        dom_id = self._get_dom_id(instance, check_rescue=True)",
            "",
            "        try:",
            "            raw_console_data = self._session.call_plugin('console',",
            "                    'get_console_log', {'dom_id': dom_id})",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _(\"Guest does not have a console available\")",
            "            raise exception.NovaException(msg)",
            "",
            "        return zlib.decompress(base64.b64decode(raw_console_data))",
            "",
            "    def get_vnc_console(self, instance):",
            "        \"\"\"Return connection info for a vnc console.\"\"\"",
            "        if instance['vm_state'] == vm_states.RESCUED:",
            "            name = '%s-rescue' % instance['name']",
            "            vm_ref = vm_utils.lookup(self._session, name)",
            "            if vm_ref is None:",
            "                # The rescue instance might not be ready at this point.",
            "                raise exception.InstanceNotReady(instance_id=instance['uuid'])",
            "        else:",
            "            vm_ref = vm_utils.lookup(self._session, instance['name'])",
            "            if vm_ref is None:",
            "                # The compute manager expects InstanceNotFound for this case.",
            "                raise exception.InstanceNotFound(instance_id=instance['uuid'])",
            "",
            "        session_id = self._session.get_session_id()",
            "        path = \"/console?ref=%s&session_id=%s\" % (str(vm_ref), session_id)",
            "",
            "        # NOTE: XS5.6sp2+ use http over port 80 for xenapi com",
            "        return {'host': CONF.vncserver_proxyclient_address, 'port': 80,",
            "                'internal_access_path': path}",
            "",
            "    def _vif_xenstore_data(self, vif):",
            "        \"\"\"convert a network info vif to injectable instance data.\"\"\"",
            "",
            "        def get_ip(ip):",
            "            if not ip:",
            "                return None",
            "            return ip['address']",
            "",
            "        def fixed_ip_dict(ip, subnet):",
            "            if ip['version'] == 4:",
            "                netmask = str(subnet.as_netaddr().netmask)",
            "            else:",
            "                netmask = subnet.as_netaddr()._prefixlen",
            "",
            "            return {'ip': ip['address'],",
            "                    'enabled': '1',",
            "                    'netmask': netmask,",
            "                    'gateway': get_ip(subnet['gateway'])}",
            "",
            "        def convert_route(route):",
            "            return {'route': str(netaddr.IPNetwork(route['cidr']).network),",
            "                    'netmask': str(netaddr.IPNetwork(route['cidr']).netmask),",
            "                    'gateway': get_ip(route['gateway'])}",
            "",
            "        network = vif['network']",
            "        v4_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 4]",
            "        v6_subnets = [subnet for subnet in network['subnets']",
            "                             if subnet['version'] == 6]",
            "",
            "        # NOTE(tr3buchet): routes and DNS come from all subnets",
            "        routes = [convert_route(route) for subnet in network['subnets']",
            "                                       for route in subnet['routes']]",
            "        dns = [get_ip(ip) for subnet in network['subnets']",
            "                          for ip in subnet['dns']]",
            "",
            "        info_dict = {'label': network['label'],",
            "                     'mac': vif['address']}",
            "",
            "        if v4_subnets:",
            "            # NOTE(tr3buchet): gateway and broadcast from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway'] = get_ip(v4_subnets[0]['gateway'])",
            "            info_dict['broadcast'] = str(v4_subnets[0].as_netaddr().broadcast)",
            "            info_dict['ips'] = [fixed_ip_dict(ip, subnet)",
            "                                for subnet in v4_subnets",
            "                                for ip in subnet['ips']]",
            "        if v6_subnets:",
            "            # NOTE(tr3buchet): gateway from first subnet",
            "            #                  primary IP will be from first subnet",
            "            #                  subnets are generally unordered :(",
            "            info_dict['gateway_v6'] = get_ip(v6_subnets[0]['gateway'])",
            "            info_dict['ip6s'] = [fixed_ip_dict(ip, subnet)",
            "                                 for subnet in v6_subnets",
            "                                 for ip in subnet['ips']]",
            "        if routes:",
            "            info_dict['routes'] = routes",
            "",
            "        if dns:",
            "            info_dict['dns'] = list(set(dns))",
            "",
            "        return info_dict",
            "",
            "    def inject_network_info(self, instance, network_info, vm_ref=None):",
            "        \"\"\"",
            "        Generate the network info and make calls to place it into the",
            "        xenstore and the xenstore param list.",
            "        vm_ref can be passed in because it will sometimes be different than",
            "        what vm_utils.lookup(session, instance['name']) will find (ex: rescue)",
            "        \"\"\"",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance)",
            "        LOG.debug(_(\"Injecting network info to xenstore\"), instance=instance)",
            "",
            "        @utils.synchronized('xenstore-' + instance['uuid'])",
            "        def update_nwinfo():",
            "            for vif in network_info:",
            "                xs_data = self._vif_xenstore_data(vif)",
            "                location = ('vm-data/networking/%s' %",
            "                            vif['address'].replace(':', ''))",
            "                self._add_to_param_xenstore(vm_ref,",
            "                                            location,",
            "                                            jsonutils.dumps(xs_data))",
            "                try:",
            "                    self._write_to_xenstore(instance, location, xs_data,",
            "                                            vm_ref=vm_ref)",
            "                except KeyError:",
            "                    # catch KeyError for domid if instance isn't running",
            "                    pass",
            "        update_nwinfo()",
            "",
            "    def _create_vifs(self, instance, vm_ref, network_info):",
            "        \"\"\"Creates vifs for an instance.\"\"\"",
            "",
            "        LOG.debug(_(\"Creating vifs\"), instance=instance)",
            "",
            "        # this function raises if vm_ref is not a vm_opaque_ref",
            "        self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "",
            "        for device, vif in enumerate(network_info):",
            "            vif_rec = self.vif_driver.plug(instance, vif,",
            "                                           vm_ref=vm_ref, device=device)",
            "            network_ref = vif_rec['network']",
            "            LOG.debug(_('Creating VIF for network %s'),",
            "                      network_ref, instance=instance)",
            "            vif_ref = self._session.call_xenapi('VIF.create', vif_rec)",
            "            LOG.debug(_('Created VIF %(vif_ref)s, network %(network_ref)s'),",
            "                      {'vif_ref': vif_ref, 'network_ref': network_ref},",
            "                      instance=instance)",
            "",
            "    def plug_vifs(self, instance, network_info):",
            "        \"\"\"Set up VIF networking on the host.\"\"\"",
            "        for device, vif in enumerate(network_info):",
            "            self.vif_driver.plug(instance, vif, device=device)",
            "",
            "    def unplug_vifs(self, instance, network_info):",
            "        if network_info:",
            "            for vif in network_info:",
            "                self.vif_driver.unplug(instance, vif)",
            "",
            "    def reset_network(self, instance):",
            "        \"\"\"Calls resetnetwork method in agent.\"\"\"",
            "        if self.agent_enabled(instance):",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            agent = self._get_agent(instance, vm_ref)",
            "            agent.resetnetwork()",
            "        else:",
            "            raise NotImplementedError()",
            "",
            "    def _inject_hostname(self, instance, vm_ref, rescue):",
            "        \"\"\"Inject the hostname of the instance into the xenstore.\"\"\"",
            "        hostname = instance['hostname']",
            "        if rescue:",
            "            hostname = 'RESCUE-%s' % hostname",
            "",
            "        if instance['os_type'] == \"windows\":",
            "            # NOTE(jk0): Windows hostnames can only be <= 15 chars.",
            "            hostname = hostname[:15]",
            "",
            "        LOG.debug(_(\"Injecting hostname to xenstore\"), instance=instance)",
            "        self._add_to_param_xenstore(vm_ref, 'vm-data/hostname', hostname)",
            "",
            "    def _remove_hostname(self, instance, vm_ref):",
            "        LOG.debug(_(\"Removing hostname from xenstore\"), instance=instance)",
            "        self._remove_from_param_xenstore(vm_ref, 'vm-data/hostname')",
            "",
            "    def _write_to_xenstore(self, instance, path, value, vm_ref=None):",
            "        \"\"\"",
            "        Writes the passed value to the xenstore record for the given VM",
            "        at the specified location. A XenAPIPlugin.PluginError will be raised",
            "        if any error is encountered in the write process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'write_record', instance,",
            "                                      vm_ref=vm_ref, path=path,",
            "                                      value=jsonutils.dumps(value))",
            "",
            "    def _delete_from_xenstore(self, instance, path, vm_ref=None):",
            "        \"\"\"",
            "        Deletes the value from the xenstore record for the given VM at",
            "        the specified location.  A XenAPIPlugin.PluginError will be",
            "        raised if any error is encountered in the delete process.",
            "        \"\"\"",
            "        return self._make_plugin_call('xenstore.py', 'delete_record', instance,",
            "                                      vm_ref=vm_ref, path=path)",
            "",
            "    def _make_plugin_call(self, plugin, method, instance=None, vm_ref=None,",
            "                          **addl_args):",
            "        \"\"\"",
            "        Abstracts out the process of calling a method of a xenapi plugin.",
            "        Any errors raised by the plugin will in turn raise a RuntimeError here.",
            "        \"\"\"",
            "        args = {}",
            "        if instance or vm_ref:",
            "            args['dom_id'] = self._get_dom_id(instance, vm_ref)",
            "        args.update(addl_args)",
            "        try:",
            "            return self._session.call_plugin(plugin, method, args)",
            "        except self._session.XenAPI.Failure as e:",
            "            err_msg = e.details[-1].splitlines()[-1]",
            "            if 'TIMEOUT:' in err_msg:",
            "                LOG.error(_('TIMEOUT: The call to %(method)s timed out. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'timeout', 'message': err_msg}",
            "            elif 'NOT IMPLEMENTED:' in err_msg:",
            "                LOG.error(_('NOT IMPLEMENTED: The call to %(method)s is not'",
            "                            ' supported by the agent. args=%(args)r'),",
            "                          {'method': method, 'args': args}, instance=instance)",
            "                return {'returncode': 'notimplemented', 'message': err_msg}",
            "            else:",
            "                LOG.error(_('The call to %(method)s returned an error: %(e)s. '",
            "                            'args=%(args)r'),",
            "                          {'method': method, 'args': args, 'e': e},",
            "                          instance=instance)",
            "                return {'returncode': 'error', 'message': err_msg}",
            "",
            "    def _get_dom_id(self, instance=None, vm_ref=None, check_rescue=False):",
            "        vm_ref = vm_ref or self._get_vm_opaque_ref(instance, check_rescue)",
            "        vm_rec = self._session.call_xenapi(\"VM.get_record\", vm_ref)",
            "        return vm_rec['domid']",
            "",
            "    def _add_to_param_xenstore(self, vm_ref, key, val):",
            "        \"\"\"",
            "        Takes a key/value pair and adds it to the xenstore parameter",
            "        record for the given vm instance. If the key exists in xenstore,",
            "        it is overwritten",
            "        \"\"\"",
            "        self._remove_from_param_xenstore(vm_ref, key)",
            "        self._session.call_xenapi('VM.add_to_xenstore_data', vm_ref, key, val)",
            "",
            "    def _remove_from_param_xenstore(self, vm_ref, key):",
            "        \"\"\"",
            "        Takes a single key and removes it from the xenstore parameter",
            "        record data for the given VM.",
            "        If the key doesn't exist, the request is ignored.",
            "        \"\"\"",
            "        self._session.call_xenapi('VM.remove_from_xenstore_data', vm_ref, key)",
            "",
            "    def refresh_security_group_rules(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_rules(security_group_id)",
            "",
            "    def refresh_security_group_members(self, security_group_id):",
            "        \"\"\"recreates security group rules for every instance.\"\"\"",
            "        self.firewall_driver.refresh_security_group_members(security_group_id)",
            "",
            "    def refresh_instance_security_rules(self, instance):",
            "        \"\"\"recreates security group rules for specified instance.\"\"\"",
            "        self.firewall_driver.refresh_instance_security_rules(instance)",
            "",
            "    def refresh_provider_fw_rules(self):",
            "        self.firewall_driver.refresh_provider_fw_rules()",
            "",
            "    def unfilter_instance(self, instance_ref, network_info):",
            "        \"\"\"Removes filters for each VIF of the specified instance.\"\"\"",
            "        self.firewall_driver.unfilter_instance(instance_ref,",
            "                                               network_info=network_info)",
            "",
            "    def _get_host_uuid_from_aggregate(self, context, hostname):",
            "        current_aggregate = self._virtapi.aggregate_get_by_host(",
            "            context, CONF.host, key=pool_states.POOL_FLAG)[0]",
            "        if not current_aggregate:",
            "            raise exception.AggregateHostNotFound(host=CONF.host)",
            "        try:",
            "            return current_aggregate.metadetails[hostname]",
            "        except KeyError:",
            "            reason = _('Destination host:%s must be in the same '",
            "                       'aggregate as the source server') % hostname",
            "            raise exception.MigrationPreCheckError(reason=reason)",
            "",
            "    def _ensure_host_in_aggregate(self, context, hostname):",
            "        self._get_host_uuid_from_aggregate(context, hostname)",
            "",
            "    def _get_host_opaque_ref(self, context, hostname):",
            "        host_uuid = self._get_host_uuid_from_aggregate(context, hostname)",
            "        return self._session.call_xenapi(\"host.get_by_uuid\", host_uuid)",
            "",
            "    def _migrate_receive(self, ctxt):",
            "        destref = self._session.get_xenapi_host()",
            "        # Get the network to for migrate.",
            "        # This is the one associated with the pif marked management. From cli:",
            "        # uuid=`xe pif-list --minimal management=true`",
            "        # xe pif-param-get param-name=network-uuid uuid=$uuid",
            "        expr = 'field \"management\" = \"true\"'",
            "        pifs = self._session.call_xenapi('PIF.get_all_records_where',",
            "                                         expr)",
            "        if len(pifs) != 1:",
            "            msg = _('No suitable network for migrate')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "",
            "        nwref = pifs[pifs.keys()[0]]['network']",
            "        try:",
            "            options = {}",
            "            migrate_data = self._session.call_xenapi(\"host.migrate_receive\",",
            "                                                     destref,",
            "                                                     nwref,",
            "                                                     options)",
            "        except self._session.XenAPI.Failure as exc:",
            "            LOG.exception(exc)",
            "            msg = _('Migrate Receive failed')",
            "            raise exception.MigrationPreCheckError(reason=msg)",
            "        return migrate_data",
            "",
            "    def _get_iscsi_srs(self, ctxt, instance_ref):",
            "        vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "        vbd_refs = self._session.call_xenapi(\"VM.get_VBDs\", vm_ref)",
            "",
            "        iscsi_srs = []",
            "",
            "        for vbd_ref in vbd_refs:",
            "            vdi_ref = self._session.call_xenapi(\"VBD.get_VDI\", vbd_ref)",
            "            # Check if it's on an iSCSI SR",
            "            sr_ref = self._session.call_xenapi(\"VDI.get_SR\", vdi_ref)",
            "            if self._session.call_xenapi(\"SR.get_type\", sr_ref) == 'iscsi':",
            "                iscsi_srs.append(sr_ref)",
            "",
            "        return iscsi_srs",
            "",
            "    def check_can_live_migrate_destination(self, ctxt, instance_ref,",
            "                                           block_migration=False,",
            "                                           disk_over_commit=False):",
            "        \"\"\"Check if it is possible to execute live migration.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param block_migration: if true, prepare for block migration",
            "        :param disk_over_commit: if true, allow disk over commit",
            "",
            "        \"\"\"",
            "        dest_check_data = {}",
            "        if block_migration:",
            "            migrate_send_data = self._migrate_receive(ctxt)",
            "            destination_sr_ref = vm_utils.safe_find_sr(self._session)",
            "            dest_check_data.update(",
            "                {\"block_migration\": block_migration,",
            "                 \"migrate_data\": {\"migrate_send_data\": migrate_send_data,",
            "                                  \"destination_sr_ref\": destination_sr_ref}})",
            "        else:",
            "            src = instance_ref['host']",
            "            self._ensure_host_in_aggregate(ctxt, src)",
            "            # TODO(johngarbutt) we currently assume",
            "            # instance is on a SR shared with other destination",
            "            # block migration work will be able to resolve this",
            "        return dest_check_data",
            "",
            "    def _is_xsm_sr_check_relaxed(self):",
            "        try:",
            "            return self.cached_xsm_sr_relaxed",
            "        except AttributeError:",
            "            config_value = None",
            "            try:",
            "                config_value = self._make_plugin_call('config_file',",
            "                                                      'get_val',",
            "                                                      key='relax-xsm-sr-check')",
            "            except Exception as exc:",
            "                LOG.exception(exc)",
            "            self.cached_xsm_sr_relaxed = config_value == \"true\"",
            "            return self.cached_xsm_sr_relaxed",
            "",
            "    def check_can_live_migrate_source(self, ctxt, instance_ref,",
            "                                      dest_check_data):",
            "        \"\"\"Check if it's possible to execute live migration on the source side.",
            "",
            "        :param context: security context",
            "        :param instance_ref: nova.db.sqlalchemy.models.Instance object",
            "        :param dest_check_data: data returned by the check on the",
            "                                destination, includes block_migration flag",
            "",
            "        \"\"\"",
            "        if len(self._get_iscsi_srs(ctxt, instance_ref)) > 0:",
            "            # XAPI must support the relaxed SR check for live migrating with",
            "            # iSCSI VBDs",
            "            if not self._is_xsm_sr_check_relaxed():",
            "                raise exception.MigrationError(_('XAPI supporting '",
            "                                'relax-xsm-sr-check=true requried'))",
            "",
            "        if 'migrate_data' in dest_check_data:",
            "            vm_ref = self._get_vm_opaque_ref(instance_ref)",
            "            migrate_data = dest_check_data['migrate_data']",
            "            try:",
            "                self._call_live_migrate_command(",
            "                    \"VM.assert_can_migrate\", vm_ref, migrate_data)",
            "            except self._session.XenAPI.Failure as exc:",
            "                LOG.exception(exc)",
            "                msg = _('VM.assert_can_migrate failed')",
            "                raise exception.MigrationPreCheckError(reason=msg)",
            "        return dest_check_data",
            "",
            "    def _generate_vdi_map(self, destination_sr_ref, vm_ref, sr_ref=None):",
            "        \"\"\"generate a vdi_map for _call_live_migrate_command.\"\"\"",
            "        if sr_ref is None:",
            "            sr_ref = vm_utils.safe_find_sr(self._session)",
            "        vm_vdis = vm_utils.get_instance_vdis_for_sr(self._session,",
            "                                                    vm_ref, sr_ref)",
            "        return dict((vdi, destination_sr_ref) for vdi in vm_vdis)",
            "",
            "    def _call_live_migrate_command(self, command_name, vm_ref, migrate_data):",
            "        \"\"\"unpack xapi specific parameters, and call a live migrate command.\"\"\"",
            "        destination_sr_ref = migrate_data['destination_sr_ref']",
            "        migrate_send_data = migrate_data['migrate_send_data']",
            "",
            "        vdi_map = self._generate_vdi_map(destination_sr_ref, vm_ref)",
            "",
            "        # Add destination SR refs for all of the VDIs that we created",
            "        # as part of the pre migration callback",
            "        if 'pre_live_migration_result' in migrate_data:",
            "            pre_migrate_data = migrate_data['pre_live_migration_result']",
            "            sr_uuid_map = pre_migrate_data.get('sr_uuid_map', [])",
            "            for sr_uuid in sr_uuid_map:",
            "                # Source and destination SRs have the same UUID, so get the",
            "                # reference for the local SR",
            "                sr_ref = self._session.call_xenapi(\"SR.get_by_uuid\", sr_uuid)",
            "                vdi_map.update(",
            "                    self._generate_vdi_map(",
            "                        sr_uuid_map[sr_uuid], vm_ref, sr_ref))",
            "        vif_map = {}",
            "        options = {}",
            "        self._session.call_xenapi(command_name, vm_ref,",
            "                                  migrate_send_data, True,",
            "                                  vdi_map, vif_map, options)",
            "",
            "    def live_migrate(self, context, instance, destination_hostname,",
            "                     post_method, recover_method, block_migration,",
            "                     migrate_data=None):",
            "        try:",
            "            vm_ref = self._get_vm_opaque_ref(instance)",
            "            if block_migration:",
            "                if not migrate_data:",
            "                    raise exception.InvalidParameterValue('Block Migration '",
            "                                    'requires migrate data from destination')",
            "",
            "                iscsi_srs = self._get_iscsi_srs(context, instance)",
            "                try:",
            "                    self._call_live_migrate_command(",
            "                        \"VM.migrate_send\", vm_ref, migrate_data)",
            "                except self._session.XenAPI.Failure as exc:",
            "                    LOG.exception(exc)",
            "                    raise exception.MigrationError(_('Migrate Send failed'))",
            "",
            "                # Tidy up the iSCSI SRs",
            "                for sr_ref in iscsi_srs:",
            "                    volume_utils.forget_sr(self._session, sr_ref)",
            "            else:",
            "                host_ref = self._get_host_opaque_ref(context,",
            "                                                     destination_hostname)",
            "                self._session.call_xenapi(\"VM.pool_migrate\", vm_ref,",
            "                                          host_ref, {})",
            "            post_method(context, instance, destination_hostname,",
            "                        block_migration)",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                recover_method(context, instance, destination_hostname,",
            "                               block_migration)",
            "",
            "    def get_per_instance_usage(self):",
            "        \"\"\"Get usage info about each active instance.\"\"\"",
            "        usage = {}",
            "",
            "        def _is_active(vm_rec):",
            "            power_state = vm_rec['power_state'].lower()",
            "            return power_state in ['running', 'paused']",
            "",
            "        def _get_uuid(vm_rec):",
            "            other_config = vm_rec['other_config']",
            "            return other_config.get('nova_uuid', None)",
            "",
            "        for vm_ref, vm_rec in vm_utils.list_vms(self._session):",
            "            uuid = _get_uuid(vm_rec)",
            "",
            "            if _is_active(vm_rec) and uuid is not None:",
            "                memory_mb = int(vm_rec['memory_static_max']) / 1024 / 1024",
            "                usage[uuid] = {'memory_mb': memory_mb, 'uuid': uuid}",
            "",
            "        return usage",
            "",
            "    def attach_block_device_volumes(self, block_device_info):",
            "        sr_uuid_map = {}",
            "        try:",
            "            if block_device_info is not None:",
            "                for block_device_map in block_device_info[",
            "                                                'block_device_mapping']:",
            "                    sr_uuid, _ = self._volumeops.attach_volume(",
            "                        block_device_map['connection_info'],",
            "                        None,",
            "                        block_device_map['mount_device'],",
            "                        hotplug=False)",
            "",
            "                    sr_ref = self._session.call_xenapi('SR.get_by_uuid',",
            "                                                       sr_uuid)",
            "                    sr_uuid_map[sr_uuid] = sr_ref",
            "        except Exception:",
            "            with excutils.save_and_reraise_exception():",
            "                # Disconnect the volumes we just connected",
            "                for sr in sr_uuid_map:",
            "                    volume_utils.forget_sr(self._session, sr_uuid_map[sr_ref])",
            "",
            "        return sr_uuid_map"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "277": [
                "VMOps",
                "finish_migration"
            ],
            "278": [
                "VMOps",
                "finish_migration"
            ],
            "280": [
                "VMOps",
                "finish_migration"
            ],
            "281": [
                "VMOps",
                "finish_migration"
            ],
            "283": [
                "VMOps",
                "finish_migration"
            ],
            "285": [
                "VMOps",
                "finish_migration"
            ],
            "286": [
                "VMOps",
                "finish_migration"
            ],
            "287": [
                "VMOps",
                "finish_migration"
            ],
            "288": [
                "VMOps",
                "finish_migration"
            ],
            "289": [
                "VMOps",
                "finish_migration"
            ],
            "290": [
                "VMOps",
                "finish_migration"
            ],
            "291": [
                "VMOps",
                "finish_migration"
            ],
            "292": [
                "VMOps",
                "finish_migration"
            ],
            "293": [
                "VMOps",
                "finish_migration"
            ],
            "294": [
                "VMOps",
                "finish_migration"
            ],
            "295": [
                "VMOps",
                "finish_migration"
            ],
            "297": [
                "VMOps",
                "finish_migration"
            ],
            "298": [
                "VMOps",
                "finish_migration"
            ],
            "299": [
                "VMOps",
                "finish_migration"
            ],
            "300": [
                "VMOps",
                "finish_migration"
            ],
            "302": [
                "VMOps",
                "finish_migration"
            ],
            "304": [
                "VMOps",
                "finish_migration"
            ],
            "305": [
                "VMOps",
                "finish_migration"
            ],
            "306": [
                "VMOps",
                "finish_migration"
            ],
            "307": [
                "VMOps",
                "finish_migration"
            ],
            "308": [
                "VMOps",
                "finish_migration"
            ],
            "309": [
                "VMOps",
                "finish_migration"
            ],
            "348": [
                "VMOps",
                "spawn"
            ],
            "349": [
                "VMOps",
                "spawn"
            ],
            "350": [
                "VMOps",
                "spawn"
            ],
            "351": [
                "VMOps",
                "spawn"
            ],
            "352": [
                "VMOps",
                "spawn"
            ],
            "353": [
                "VMOps",
                "spawn"
            ],
            "354": [
                "VMOps",
                "spawn"
            ],
            "355": [
                "VMOps",
                "spawn",
                "determine_disk_image_type_step"
            ],
            "356": [
                "VMOps",
                "spawn",
                "determine_disk_image_type_step"
            ],
            "357": [
                "VMOps",
                "spawn"
            ],
            "359": [
                "VMOps",
                "spawn",
                "create_disks_step"
            ],
            "413": [
                "VMOps",
                "spawn",
                "attach_disks_step"
            ],
            "430": [
                "VMOps",
                "spawn",
                "inject_instance_data_step"
            ],
            "452": [
                "VMOps",
                "spawn",
                "boot_instance_step"
            ],
            "453": [
                "VMOps",
                "spawn",
                "boot_instance_step"
            ],
            "457": [
                "VMOps",
                "spawn",
                "configure_booted_instance_step"
            ],
            "458": [
                "VMOps",
                "spawn",
                "configure_booted_instance_step"
            ],
            "459": [
                "VMOps",
                "spawn",
                "configure_booted_instance_step"
            ],
            "474": [
                "VMOps",
                "spawn"
            ]
        },
        "addLocation": []
    }
}