{
    "docs/conf.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 441,
                "afterPatchRowNumber": 441,
                "PatchRowcode": "     [\"sphinx_immaterial\", r\".*Parameter name '\\*\\*params'.*\"],"
            },
            "1": {
                "beforePatchRowNumber": 442,
                "afterPatchRowNumber": 442,
                "PatchRowcode": "     # The annotations for `PipelineMeta` seem to have their own set of issues, ignore here:"
            },
            "2": {
                "beforePatchRowNumber": 443,
                "afterPatchRowNumber": 443,
                "PatchRowcode": "     [\"sphinx_immaterial\", r\".*Parameter name.*PipelineMeta.*\"],"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 444,
                "PatchRowcode": "+    # Some warnings caused ultimately by pytorch docstrings:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 445,
                "PatchRowcode": "+    r\".*undefined label: 'extending-autograd'.*\","
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 446,
                "PatchRowcode": "+    r\".*undefined label: 'combining-forward-context'.*\","
            },
            "6": {
                "beforePatchRowNumber": 444,
                "afterPatchRowNumber": 447,
                "PatchRowcode": " ]"
            },
            "7": {
                "beforePatchRowNumber": 445,
                "afterPatchRowNumber": 448,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 446,
                "afterPatchRowNumber": 449,
                "PatchRowcode": " # `sphinx-zeta-suppress` (more specific warnings suppression) configuration [end]."
            }
        },
        "frontPatchFile": [
            "# This file is execfile()d with the current directory set to its containing dir.",
            "#",
            "# This file only contains a selection of the most common options. For a full",
            "# list see the documentation:",
            "# https://www.sphinx-doc.org/en/master/usage/configuration.html",
            "#",
            "# All configuration values have a default; values that are commented out",
            "# serve to show the default.",
            "",
            "import os",
            "import shutil",
            "import sys",
            "import time",
            "",
            "# -- Path setup --------------------------------------------------------------",
            "",
            "__location__ = os.path.dirname(__file__)",
            "",
            "# If extensions (or modules to document with autodoc) are in another directory,",
            "# add these directories to sys.path here. If the directory is relative to the",
            "# documentation root, use os.path.abspath to make it absolute, like shown here.",
            "sys.path.insert(0, os.path.join(__location__, \"../src\"))",
            "",
            "# Any custom sphinx extensions for TemporAI live in docs/custom-sphinx-ext/:",
            "sys.path.insert(0, os.path.join(__location__, \"custom-sphinx-ext\"))",
            "",
            "# -- Run sphinx-apidoc -------------------------------------------------------",
            "# This hack is necessary since RTD does not issue `sphinx-apidoc` before running",
            "# `sphinx-build -b html . _build/html`. See Issue:",
            "# https://github.com/readthedocs/readthedocs.org/issues/1139",
            "# DON'T FORGET: Check the box \"Install your project inside a virtualenv using",
            "# setup.py install\" in the RTD Advanced Settings.",
            "# Additionally it helps us to avoid running apidoc manually",
            "",
            "try:  # for Sphinx >= 1.7",
            "    from sphinx.ext import apidoc",
            "except ImportError:",
            "    from sphinx import apidoc  # type: ignore",
            "",
            "output_dir = os.path.join(__location__, \"api\")",
            "module_dir = os.path.join(__location__, \"../src/tempor\")",
            "try:",
            "    shutil.rmtree(output_dir)",
            "except FileNotFoundError:",
            "    pass",
            "",
            "try:",
            "    import sphinx",
            "",
            "    cmd_line = f\"sphinx-apidoc --implicit-namespaces -e -f -o {output_dir} {module_dir}\"",
            "",
            "    args = cmd_line.split(\" \")",
            "    if tuple(sphinx.__version__.split(\".\")) >= (\"1\", \"7\"):",
            "        # This is a rudimentary parse_version to avoid external dependencies",
            "        args = args[1:]",
            "",
            "    apidoc.main(args)",
            "except Exception as e:  # pylint: disable=broad-except",
            "    print(\"Running `sphinx-apidoc` failed!\\n{}\".format(e))",
            "",
            "# -- General configuration ---------------------------------------------------",
            "",
            "# If your documentation needs a minimal Sphinx version, state it here.",
            "needs_sphinx = \"5.0.0\"",
            "",
            "# Add any Sphinx extension module names here, as strings. They can be extensions",
            "# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.",
            "extensions = [",
            "    \"sphinx.ext.autodoc\",",
            "    \"sphinx.ext.intersphinx\",",
            "    \"sphinx.ext.todo\",",
            "    \"sphinx.ext.autosummary\",",
            "    \"sphinx.ext.viewcode\",",
            "    \"sphinx.ext.coverage\",",
            "    \"sphinx.ext.doctest\",",
            "    \"sphinx.ext.ifconfig\",",
            "    \"sphinx.ext.mathjax\",",
            "    \"sphinx.ext.napoleon\",",
            "    \"sphinx_immaterial\",",
            "    # \"sphinx_immaterial.apidoc.python.apigen\"",
            "    # ^ Enable this if wishing to use https://jbms.github.io/sphinx-immaterial/apidoc/python/apigen.html",
            "    \"nbsphinx\",",
            "    # --- Custom extensions from here ---",
            "    \"sphinx-zeta-suppress\",  # More specific warnings suppression.",
            "]",
            "",
            "# Add any paths that contain templates here, relative to this directory.",
            "templates_path = [\"_templates\"]",
            "",
            "",
            "# Enable markdown",
            "extensions.append(\"myst_parser\")",
            "",
            "# Configure MyST-Parser",
            "myst_enable_extensions = [",
            "    \"amsmath\",",
            "    \"colon_fence\",",
            "    \"deflist\",",
            "    \"dollarmath\",",
            "    \"html_image\",",
            "    \"linkify\",",
            "    \"replacements\",",
            "    \"smartquotes\",",
            "    \"substitution\",",
            "    \"tasklist\",",
            "    \"attrs_inline\",",
            "    \"attrs_block\",",
            "]",
            "",
            "# MyST URL schemes.",
            "myst_url_schemes = {",
            "    \"http\": None,",
            "    \"https\": None,",
            "    \"ftp\": None,",
            "    \"mailto\": None,",
            "    \"repo-code\": \"https://github.com/vanderschaarlab/temporai/tree/main/{{path}}#{{fragment}}\",",
            "    # \"doi\": \"https://doi.org/{{path}}\",",
            "    # \"gh-issue\": {",
            "    #     \"url\": \"https://github.com/executablebooks/MyST-Parser/issue/{{path}}#{{fragment}}\",",
            "    #     \"title\": \"Issue #{{path}}\",",
            "    #     \"classes\": [\"github\"],",
            "    # },",
            "}",
            "",
            "# The suffix of source filenames.",
            "source_suffix = [\".rst\", \".md\"]",
            "",
            "# The encoding of source files.",
            "# source_encoding = 'utf-8-sig'",
            "",
            "# The master toctree document.",
            "master_doc = \"index\"",
            "",
            "# General information about the project.",
            "project = \"TemporAI\"",
            "copyright = f\"{time.strftime('%Y')}, van der Schaar Lab\"  # pylint: disable=redefined-builtin",
            "",
            "# The version info for the project you're documenting, acts as replacement for",
            "# |version| and |release|, also used in various other places throughout the",
            "# built documents.",
            "#",
            "# version: The short X.Y version.",
            "# release: The full version, including alpha/beta/rc tags.",
            "# If you don\u2019t need the separation provided between version and release,",
            "# just set them both to the same value.",
            "try:",
            "    from tempor import __version__ as version",
            "except ImportError:",
            "    version = \"\"",
            "",
            "if not version or version.lower() == \"unknown\":",
            "    version = os.getenv(\"READTHEDOCS_VERSION\", \"unknown\")  # automatically set by RTD",
            "",
            "release = version",
            "",
            "# The language for content autogenerated by Sphinx. Refer to documentation",
            "# for a list of supported languages.",
            "# language = None",
            "",
            "# There are two options for replacing |today|: either, you set today to some",
            "# non-false value, then it is used:",
            "# today = ''",
            "# Else, today_fmt is used as the format for a strftime call.",
            "# today_fmt = '%B %d, %Y'",
            "",
            "# List of patterns, relative to source directory, that match files and",
            "# directories to ignore when looking for source files.",
            "exclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\", \".venv\", \".dev\"]",
            "",
            "# The reST default role (used for this markup: `text`) to use for all documents.",
            "default_role = \"py:obj\"",
            "",
            "# If true, '()' will be appended to :func: etc. cross-reference text.",
            "# add_function_parentheses = True",
            "",
            "# If true, the current module name will be prepended to all description",
            "# unit titles (such as .. function::).",
            "# add_module_names = True",
            "",
            "# If true, sectionauthor and moduleauthor directives will be shown in the",
            "# output. They are ignored by default.",
            "# show_authors = False",
            "",
            "# The name of the Pygments (syntax highlighting) style to use.",
            "# https://pygments.org/styles/",
            "pygments_style = \"tango\"",
            "",
            "# A list of ignored prefixes for module index sorting.",
            "# modindex_common_prefix = []",
            "",
            "# If true, keep warnings as \"system message\" paragraphs in the built documents.",
            "# keep_warnings = False",
            "",
            "# A list of warning types to suppress arbitrary warning messages.",
            "# https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-suppress_warnings",
            "suppress_warnings = [\"misc.highlighting_failure\"]",
            "# ^ Highlighting failures seem to be common under this setup when types appear in literal blocks, since these are not",
            "# critical, we suppress highlighting warnings.",
            "",
            "# If this is True, todo emits a warning for each TODO entries. The default is False.",
            "todo_emit_warnings = True",
            "",
            "",
            "# -- Configure autodoc ---------------------------------------------",
            "",
            "autoclass_content = \"both\"",
            "",
            "autodoc_member_order = \"bysource\"",
            "",
            "# autodoc_mock_imports = [\"sklearn\"]  # Update as needed.",
            "",
            "# -- Configure autodoc (end) ---------------------------------------",
            "",
            "",
            "# The theme to use for HTML and HTML Help pages.  See the documentation for",
            "# a list of builtin themes.",
            "# We use this theme: https://jbms.github.io/sphinx-immaterial/",
            "html_theme = \"sphinx_immaterial\"",
            "",
            "# Theme options are theme-specific and customize the look and feel of a theme",
            "# further.  For a list of options available for each theme, see the",
            "# documentation.",
            "",
            "# Material theme options (see theme.conf for more information)",
            "html_theme_options = {",
            "    \"icon\": {",
            "        \"repo\": \"fontawesome/brands/github\",",
            "        \"edit\": \"material/file-edit-outline\",",
            "    },",
            "    \"site_url\": \"https://www.temporai.vanderschaar-lab.com/\",",
            "    \"repo_url\": \"https://github.com/vanderschaarlab/temporai/\",",
            "    \"repo_name\": \"TemporAI\",",
            "    \"edit_uri\": \"blob/main/docs\",",
            "    \"globaltoc_collapse\": True,",
            "    \"features\": [",
            "        \"navigation.expand\",",
            "        # \"navigation.tabs\",",
            "        # \"toc.integrate\",",
            "        \"navigation.sections\",",
            "        # \"navigation.instant\",",
            "        # \"header.autohide\",",
            "        \"navigation.top\",",
            "        # \"navigation.tracking\",",
            "        # \"search.highlight\",",
            "        \"search.share\",",
            "        \"toc.follow\",",
            "        \"toc.sticky\",",
            "        \"content.tabs.link\",",
            "        \"announce.dismiss\",",
            "    ],",
            "    \"palette\": [",
            "        {",
            "            \"media\": \"(prefers-color-scheme: light)\",",
            "            \"scheme\": \"default\",",
            "            \"primary\": \"light-blue\",",
            "            \"accent\": \"indigo\",",
            "            \"toggle\": {",
            "                \"icon\": \"material/lightbulb-outline\",",
            "                \"name\": \"Switch to dark mode\",",
            "            },",
            "        },",
            "        {",
            "            \"media\": \"(prefers-color-scheme: dark)\",",
            "            \"scheme\": \"slate\",",
            "            \"primary\": \"indigo\",",
            "            \"accent\": \"deep-purple\",",
            "            \"toggle\": {",
            "                \"icon\": \"material/lightbulb\",",
            "                \"name\": \"Switch to light mode\",",
            "            },",
            "        },",
            "    ],",
            "    # BEGIN: version_dropdown",
            "    \"version_dropdown\": False,",
            "    \"version_info\": [",
            "        {",
            "            \"version\": \"https://temporai.readthedocs.io/en/latest/\",",
            "            \"title\": \"ReadTheDocs\",",
            "            \"aliases\": [],",
            "        },",
            "        # {",
            "        #     \"version\": \"https://jbms.github.io/sphinx-immaterial\",",
            "        #     \"title\": \"Github Pages\",",
            "        #     \"aliases\": [],",
            "        # },",
            "    ],",
            "    # END: version_dropdown",
            "    \"toc_title_is_page_title\": True,",
            "    # BEGIN: social icons",
            "    \"social\": [",
            "        {",
            "            \"icon\": \"fontawesome/brands/github\",",
            "            \"link\": \"https://github.com/vanderschaarlab/temporai/\",",
            "            \"name\": \"Source on github.com\",",
            "        },",
            "        {",
            "            \"icon\": \"fontawesome/brands/python\",",
            "            \"link\": \"https://pypi.org/project/temporai/\",",
            "        },",
            "    ],",
            "    # END: social icons",
            "}",
            "",
            "# Sphinx immaterial theme's python apigen options:",
            "# python_apigen_modules = {",
            "#     \"tempor\": \"src/tempor/\",",
            "# }",
            "",
            "# Add any paths that contain custom themes here, relative to this directory.",
            "# html_theme_path = []",
            "",
            "# The name for this set of Sphinx documents.  If None, it defaults to",
            "# \"<project> v<release> documentation\".",
            "html_title = \"TemporAI documentation\"",
            "",
            "# A shorter title for the navigation bar.  Default is the same as html_title.",
            "html_short_title = \"TemporAI\"",
            "",
            "# The name of an image file (relative to this directory) to place at the top",
            "# of the sidebar.",
            "html_logo = \"assets/TemporAI_Logo_Icon.ico\"",
            "",
            "# The name of an image file (within the static path) to use as favicon of the",
            "# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
            "# pixels large.",
            "html_favicon = \"assets/TemporAI_Logo_Icon.ico\"",
            "",
            "# Add any paths that contain custom static files (such as style sheets) here,",
            "# relative to this directory. They are copied after the builtin static files,",
            "# so a file named \"default.css\" will overwrite the builtin \"default.css\".",
            "html_static_path = [\"_static\"]",
            "",
            "# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,",
            "# using the given strftime format.",
            "# html_last_updated_fmt = '%b %d, %Y'",
            "",
            "# If true, SmartyPants will be used to convert quotes and dashes to",
            "# typographically correct entities.",
            "# html_use_smartypants = True",
            "",
            "# Custom sidebar templates, maps document names to template names.",
            "# html_sidebars = {\"**\": [\"logo-text.html\", \"globaltoc.html\", \"localtoc.html\", \"searchbox.html\"]}",
            "",
            "# Additional templates that should be rendered to pages, maps page names to",
            "# template names.",
            "# html_additional_pages = {}",
            "",
            "# If false, no module index is generated.",
            "# html_domain_indices = True",
            "",
            "# If false, no index is generated.",
            "# html_use_index = True",
            "",
            "# If true, the index is split into individual pages for each letter.",
            "# html_split_index = False",
            "",
            "# If true, links to the reST sources are added to the pages.",
            "# html_show_sourcelink = True",
            "",
            "# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.",
            "# html_show_sphinx = True",
            "",
            "# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.",
            "# html_show_copyright = True",
            "",
            "# If true, an OpenSearch description file will be output, and all pages will",
            "# contain a <link> tag referring to it.  The value of this option must be the",
            "# base URL from which the finished HTML is served.",
            "# html_use_opensearch = ''",
            "",
            "# This is the file name suffix for HTML files (e.g. \".xhtml\").",
            "# html_file_suffix = None",
            "",
            "# Output file base name for HTML help builder.",
            "htmlhelp_basename = \"temporai-doc\"",
            "",
            "",
            "# -- Options for LaTeX output ------------------------------------------------",
            "",
            "latex_elements = {  # type: ignore",
            "    # The paper size (\"letterpaper\" or \"a4paper\").",
            "    # \"papersize\": \"letterpaper\",",
            "    # The font size (\"10pt\", \"11pt\" or \"12pt\").",
            "    # \"pointsize\": \"10pt\",",
            "    # Additional stuff for the LaTeX preamble.",
            "    # \"preamble\": \"\",",
            "}",
            "",
            "# Grouping the document tree into LaTeX files. List of tuples",
            "# (source start file, target name, title, author, documentclass [howto/manual]).",
            "latex_documents = [(\"index\", \"user_guide.tex\", \"TemporAI Documentation\", \"Evgeny Saveliev\", \"manual\")]",
            "",
            "# The name of an image file (relative to this directory) to place at the top of",
            "# the title page.",
            "# latex_logo = \"\"",
            "",
            "# For \"manual\" documents, if this is true, then toplevel headings are parts,",
            "# not chapters.",
            "# latex_use_parts = False",
            "",
            "# If true, show page references after internal links.",
            "# latex_show_pagerefs = False",
            "",
            "# If true, show URL addresses after external links.",
            "# latex_show_urls = False",
            "",
            "# Documents to append as an appendix to all manuals.",
            "# latex_appendices = []",
            "",
            "# If false, no module index is generated.",
            "# latex_domain_indices = True",
            "",
            "",
            "# `sphinx-zeta-suppress` (more specific warnings suppression) configuration.",
            "#",
            "# See:",
            "# - https://sphinx-zeta-suppress.readthedocs.io/",
            "#",
            "# See also:",
            "# - https://github.com/picnixz/sphinx-zeta-suppress",
            "# - https://github.com/sphinx-doc/sphinx/issues/11325",
            "# Note that since there is no PyPI package for sphinx-zeta-suppress, we add its python module as a custom extension,",
            "# see: docs/custom-sphinx-ext/sphinx-zeta-suppress.py.",
            "",
            "zeta_suppress_protect = [",
            "    \"sphinx_immaterial\"",
            "    # `sphinx-zeta-suppress` is not compatible with `sphinx_immaterial`. It will throw an error when registering the",
            "    # filters (`_setup_filters` function). However, the problem can be overcome by adding `sphinx_immaterial` to the",
            "    # `zeta_suppress_protect` list.",
            "    # Note we can still suppress warnings specific to `sphinx_immaterial` by adding it to",
            "    # the `zeta_suppress_records` list below.",
            "]",
            "",
            "zeta_suppress_records = [",
            "    # The following warnings caused ultimately by sphinx_immaterial are caused by having *args/**kwargs in the",
            "    # docstrings. We want to have those in docstrings, so we suppress these warnings.",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*args'.*\"],",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*\\*kwargs'.*\"],",
            "    # Similar but for other variable names used for the variadics:",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*dims'.*\"],",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*\\*params'.*\"],",
            "    # The annotations for `PipelineMeta` seem to have their own set of issues, ignore here:",
            "    [\"sphinx_immaterial\", r\".*Parameter name.*PipelineMeta.*\"],",
            "]",
            "",
            "# `sphinx-zeta-suppress` (more specific warnings suppression) configuration [end].",
            "",
            "",
            "# -- External mapping --------------------------------------------------------",
            "python_version = \".\".join(map(str, sys.version_info[0:2]))",
            "intersphinx_mapping = {",
            "    \"sphinx\": (\"https://www.sphinx-doc.org/en/master\", None),",
            "    \"python\": (\"https://docs.python.org/\" + python_version, None),",
            "    \"matplotlib\": (\"https://matplotlib.org\", None),",
            "    \"numpy\": (\"https://numpy.org/doc/stable\", None),",
            "    \"sklearn\": (\"https://scikit-learn.org/stable\", None),",
            "    \"pandas\": (\"https://pandas.pydata.org/docs\", None),",
            "    \"pandera\": (\"https://pandera.readthedocs.io/en/stable\", None),",
            "    \"scipy\": (\"https://docs.scipy.org/doc/scipy/reference\", None),",
            "    \"setuptools\": (\"https://setuptools.pypa.io/en/stable/\", None),",
            "    \"pyscaffold\": (\"https://pyscaffold.org/en/stable\", None),",
            "    \"hyperimpute\": (\"https://hyperimpute.readthedocs.io/en/latest/\", None),",
            "    \"xgbse\": (\"https://loft-br.github.io/xgboost-survival-embeddings/\", None),",
            "    \"lifelines\": (\"https://lifelines.readthedocs.io/en/stable/\", None),",
            "    \"optuna\": (\"https://optuna.readthedocs.io/en/stable/\", None),",
            "}",
            "",
            "print(f\"loading configurations for {project} {version} ...\", file=sys.stderr)"
        ],
        "afterPatchFile": [
            "# This file is execfile()d with the current directory set to its containing dir.",
            "#",
            "# This file only contains a selection of the most common options. For a full",
            "# list see the documentation:",
            "# https://www.sphinx-doc.org/en/master/usage/configuration.html",
            "#",
            "# All configuration values have a default; values that are commented out",
            "# serve to show the default.",
            "",
            "import os",
            "import shutil",
            "import sys",
            "import time",
            "",
            "# -- Path setup --------------------------------------------------------------",
            "",
            "__location__ = os.path.dirname(__file__)",
            "",
            "# If extensions (or modules to document with autodoc) are in another directory,",
            "# add these directories to sys.path here. If the directory is relative to the",
            "# documentation root, use os.path.abspath to make it absolute, like shown here.",
            "sys.path.insert(0, os.path.join(__location__, \"../src\"))",
            "",
            "# Any custom sphinx extensions for TemporAI live in docs/custom-sphinx-ext/:",
            "sys.path.insert(0, os.path.join(__location__, \"custom-sphinx-ext\"))",
            "",
            "# -- Run sphinx-apidoc -------------------------------------------------------",
            "# This hack is necessary since RTD does not issue `sphinx-apidoc` before running",
            "# `sphinx-build -b html . _build/html`. See Issue:",
            "# https://github.com/readthedocs/readthedocs.org/issues/1139",
            "# DON'T FORGET: Check the box \"Install your project inside a virtualenv using",
            "# setup.py install\" in the RTD Advanced Settings.",
            "# Additionally it helps us to avoid running apidoc manually",
            "",
            "try:  # for Sphinx >= 1.7",
            "    from sphinx.ext import apidoc",
            "except ImportError:",
            "    from sphinx import apidoc  # type: ignore",
            "",
            "output_dir = os.path.join(__location__, \"api\")",
            "module_dir = os.path.join(__location__, \"../src/tempor\")",
            "try:",
            "    shutil.rmtree(output_dir)",
            "except FileNotFoundError:",
            "    pass",
            "",
            "try:",
            "    import sphinx",
            "",
            "    cmd_line = f\"sphinx-apidoc --implicit-namespaces -e -f -o {output_dir} {module_dir}\"",
            "",
            "    args = cmd_line.split(\" \")",
            "    if tuple(sphinx.__version__.split(\".\")) >= (\"1\", \"7\"):",
            "        # This is a rudimentary parse_version to avoid external dependencies",
            "        args = args[1:]",
            "",
            "    apidoc.main(args)",
            "except Exception as e:  # pylint: disable=broad-except",
            "    print(\"Running `sphinx-apidoc` failed!\\n{}\".format(e))",
            "",
            "# -- General configuration ---------------------------------------------------",
            "",
            "# If your documentation needs a minimal Sphinx version, state it here.",
            "needs_sphinx = \"5.0.0\"",
            "",
            "# Add any Sphinx extension module names here, as strings. They can be extensions",
            "# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.",
            "extensions = [",
            "    \"sphinx.ext.autodoc\",",
            "    \"sphinx.ext.intersphinx\",",
            "    \"sphinx.ext.todo\",",
            "    \"sphinx.ext.autosummary\",",
            "    \"sphinx.ext.viewcode\",",
            "    \"sphinx.ext.coverage\",",
            "    \"sphinx.ext.doctest\",",
            "    \"sphinx.ext.ifconfig\",",
            "    \"sphinx.ext.mathjax\",",
            "    \"sphinx.ext.napoleon\",",
            "    \"sphinx_immaterial\",",
            "    # \"sphinx_immaterial.apidoc.python.apigen\"",
            "    # ^ Enable this if wishing to use https://jbms.github.io/sphinx-immaterial/apidoc/python/apigen.html",
            "    \"nbsphinx\",",
            "    # --- Custom extensions from here ---",
            "    \"sphinx-zeta-suppress\",  # More specific warnings suppression.",
            "]",
            "",
            "# Add any paths that contain templates here, relative to this directory.",
            "templates_path = [\"_templates\"]",
            "",
            "",
            "# Enable markdown",
            "extensions.append(\"myst_parser\")",
            "",
            "# Configure MyST-Parser",
            "myst_enable_extensions = [",
            "    \"amsmath\",",
            "    \"colon_fence\",",
            "    \"deflist\",",
            "    \"dollarmath\",",
            "    \"html_image\",",
            "    \"linkify\",",
            "    \"replacements\",",
            "    \"smartquotes\",",
            "    \"substitution\",",
            "    \"tasklist\",",
            "    \"attrs_inline\",",
            "    \"attrs_block\",",
            "]",
            "",
            "# MyST URL schemes.",
            "myst_url_schemes = {",
            "    \"http\": None,",
            "    \"https\": None,",
            "    \"ftp\": None,",
            "    \"mailto\": None,",
            "    \"repo-code\": \"https://github.com/vanderschaarlab/temporai/tree/main/{{path}}#{{fragment}}\",",
            "    # \"doi\": \"https://doi.org/{{path}}\",",
            "    # \"gh-issue\": {",
            "    #     \"url\": \"https://github.com/executablebooks/MyST-Parser/issue/{{path}}#{{fragment}}\",",
            "    #     \"title\": \"Issue #{{path}}\",",
            "    #     \"classes\": [\"github\"],",
            "    # },",
            "}",
            "",
            "# The suffix of source filenames.",
            "source_suffix = [\".rst\", \".md\"]",
            "",
            "# The encoding of source files.",
            "# source_encoding = 'utf-8-sig'",
            "",
            "# The master toctree document.",
            "master_doc = \"index\"",
            "",
            "# General information about the project.",
            "project = \"TemporAI\"",
            "copyright = f\"{time.strftime('%Y')}, van der Schaar Lab\"  # pylint: disable=redefined-builtin",
            "",
            "# The version info for the project you're documenting, acts as replacement for",
            "# |version| and |release|, also used in various other places throughout the",
            "# built documents.",
            "#",
            "# version: The short X.Y version.",
            "# release: The full version, including alpha/beta/rc tags.",
            "# If you don\u2019t need the separation provided between version and release,",
            "# just set them both to the same value.",
            "try:",
            "    from tempor import __version__ as version",
            "except ImportError:",
            "    version = \"\"",
            "",
            "if not version or version.lower() == \"unknown\":",
            "    version = os.getenv(\"READTHEDOCS_VERSION\", \"unknown\")  # automatically set by RTD",
            "",
            "release = version",
            "",
            "# The language for content autogenerated by Sphinx. Refer to documentation",
            "# for a list of supported languages.",
            "# language = None",
            "",
            "# There are two options for replacing |today|: either, you set today to some",
            "# non-false value, then it is used:",
            "# today = ''",
            "# Else, today_fmt is used as the format for a strftime call.",
            "# today_fmt = '%B %d, %Y'",
            "",
            "# List of patterns, relative to source directory, that match files and",
            "# directories to ignore when looking for source files.",
            "exclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\", \".venv\", \".dev\"]",
            "",
            "# The reST default role (used for this markup: `text`) to use for all documents.",
            "default_role = \"py:obj\"",
            "",
            "# If true, '()' will be appended to :func: etc. cross-reference text.",
            "# add_function_parentheses = True",
            "",
            "# If true, the current module name will be prepended to all description",
            "# unit titles (such as .. function::).",
            "# add_module_names = True",
            "",
            "# If true, sectionauthor and moduleauthor directives will be shown in the",
            "# output. They are ignored by default.",
            "# show_authors = False",
            "",
            "# The name of the Pygments (syntax highlighting) style to use.",
            "# https://pygments.org/styles/",
            "pygments_style = \"tango\"",
            "",
            "# A list of ignored prefixes for module index sorting.",
            "# modindex_common_prefix = []",
            "",
            "# If true, keep warnings as \"system message\" paragraphs in the built documents.",
            "# keep_warnings = False",
            "",
            "# A list of warning types to suppress arbitrary warning messages.",
            "# https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-suppress_warnings",
            "suppress_warnings = [\"misc.highlighting_failure\"]",
            "# ^ Highlighting failures seem to be common under this setup when types appear in literal blocks, since these are not",
            "# critical, we suppress highlighting warnings.",
            "",
            "# If this is True, todo emits a warning for each TODO entries. The default is False.",
            "todo_emit_warnings = True",
            "",
            "",
            "# -- Configure autodoc ---------------------------------------------",
            "",
            "autoclass_content = \"both\"",
            "",
            "autodoc_member_order = \"bysource\"",
            "",
            "# autodoc_mock_imports = [\"sklearn\"]  # Update as needed.",
            "",
            "# -- Configure autodoc (end) ---------------------------------------",
            "",
            "",
            "# The theme to use for HTML and HTML Help pages.  See the documentation for",
            "# a list of builtin themes.",
            "# We use this theme: https://jbms.github.io/sphinx-immaterial/",
            "html_theme = \"sphinx_immaterial\"",
            "",
            "# Theme options are theme-specific and customize the look and feel of a theme",
            "# further.  For a list of options available for each theme, see the",
            "# documentation.",
            "",
            "# Material theme options (see theme.conf for more information)",
            "html_theme_options = {",
            "    \"icon\": {",
            "        \"repo\": \"fontawesome/brands/github\",",
            "        \"edit\": \"material/file-edit-outline\",",
            "    },",
            "    \"site_url\": \"https://www.temporai.vanderschaar-lab.com/\",",
            "    \"repo_url\": \"https://github.com/vanderschaarlab/temporai/\",",
            "    \"repo_name\": \"TemporAI\",",
            "    \"edit_uri\": \"blob/main/docs\",",
            "    \"globaltoc_collapse\": True,",
            "    \"features\": [",
            "        \"navigation.expand\",",
            "        # \"navigation.tabs\",",
            "        # \"toc.integrate\",",
            "        \"navigation.sections\",",
            "        # \"navigation.instant\",",
            "        # \"header.autohide\",",
            "        \"navigation.top\",",
            "        # \"navigation.tracking\",",
            "        # \"search.highlight\",",
            "        \"search.share\",",
            "        \"toc.follow\",",
            "        \"toc.sticky\",",
            "        \"content.tabs.link\",",
            "        \"announce.dismiss\",",
            "    ],",
            "    \"palette\": [",
            "        {",
            "            \"media\": \"(prefers-color-scheme: light)\",",
            "            \"scheme\": \"default\",",
            "            \"primary\": \"light-blue\",",
            "            \"accent\": \"indigo\",",
            "            \"toggle\": {",
            "                \"icon\": \"material/lightbulb-outline\",",
            "                \"name\": \"Switch to dark mode\",",
            "            },",
            "        },",
            "        {",
            "            \"media\": \"(prefers-color-scheme: dark)\",",
            "            \"scheme\": \"slate\",",
            "            \"primary\": \"indigo\",",
            "            \"accent\": \"deep-purple\",",
            "            \"toggle\": {",
            "                \"icon\": \"material/lightbulb\",",
            "                \"name\": \"Switch to light mode\",",
            "            },",
            "        },",
            "    ],",
            "    # BEGIN: version_dropdown",
            "    \"version_dropdown\": False,",
            "    \"version_info\": [",
            "        {",
            "            \"version\": \"https://temporai.readthedocs.io/en/latest/\",",
            "            \"title\": \"ReadTheDocs\",",
            "            \"aliases\": [],",
            "        },",
            "        # {",
            "        #     \"version\": \"https://jbms.github.io/sphinx-immaterial\",",
            "        #     \"title\": \"Github Pages\",",
            "        #     \"aliases\": [],",
            "        # },",
            "    ],",
            "    # END: version_dropdown",
            "    \"toc_title_is_page_title\": True,",
            "    # BEGIN: social icons",
            "    \"social\": [",
            "        {",
            "            \"icon\": \"fontawesome/brands/github\",",
            "            \"link\": \"https://github.com/vanderschaarlab/temporai/\",",
            "            \"name\": \"Source on github.com\",",
            "        },",
            "        {",
            "            \"icon\": \"fontawesome/brands/python\",",
            "            \"link\": \"https://pypi.org/project/temporai/\",",
            "        },",
            "    ],",
            "    # END: social icons",
            "}",
            "",
            "# Sphinx immaterial theme's python apigen options:",
            "# python_apigen_modules = {",
            "#     \"tempor\": \"src/tempor/\",",
            "# }",
            "",
            "# Add any paths that contain custom themes here, relative to this directory.",
            "# html_theme_path = []",
            "",
            "# The name for this set of Sphinx documents.  If None, it defaults to",
            "# \"<project> v<release> documentation\".",
            "html_title = \"TemporAI documentation\"",
            "",
            "# A shorter title for the navigation bar.  Default is the same as html_title.",
            "html_short_title = \"TemporAI\"",
            "",
            "# The name of an image file (relative to this directory) to place at the top",
            "# of the sidebar.",
            "html_logo = \"assets/TemporAI_Logo_Icon.ico\"",
            "",
            "# The name of an image file (within the static path) to use as favicon of the",
            "# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
            "# pixels large.",
            "html_favicon = \"assets/TemporAI_Logo_Icon.ico\"",
            "",
            "# Add any paths that contain custom static files (such as style sheets) here,",
            "# relative to this directory. They are copied after the builtin static files,",
            "# so a file named \"default.css\" will overwrite the builtin \"default.css\".",
            "html_static_path = [\"_static\"]",
            "",
            "# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,",
            "# using the given strftime format.",
            "# html_last_updated_fmt = '%b %d, %Y'",
            "",
            "# If true, SmartyPants will be used to convert quotes and dashes to",
            "# typographically correct entities.",
            "# html_use_smartypants = True",
            "",
            "# Custom sidebar templates, maps document names to template names.",
            "# html_sidebars = {\"**\": [\"logo-text.html\", \"globaltoc.html\", \"localtoc.html\", \"searchbox.html\"]}",
            "",
            "# Additional templates that should be rendered to pages, maps page names to",
            "# template names.",
            "# html_additional_pages = {}",
            "",
            "# If false, no module index is generated.",
            "# html_domain_indices = True",
            "",
            "# If false, no index is generated.",
            "# html_use_index = True",
            "",
            "# If true, the index is split into individual pages for each letter.",
            "# html_split_index = False",
            "",
            "# If true, links to the reST sources are added to the pages.",
            "# html_show_sourcelink = True",
            "",
            "# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.",
            "# html_show_sphinx = True",
            "",
            "# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.",
            "# html_show_copyright = True",
            "",
            "# If true, an OpenSearch description file will be output, and all pages will",
            "# contain a <link> tag referring to it.  The value of this option must be the",
            "# base URL from which the finished HTML is served.",
            "# html_use_opensearch = ''",
            "",
            "# This is the file name suffix for HTML files (e.g. \".xhtml\").",
            "# html_file_suffix = None",
            "",
            "# Output file base name for HTML help builder.",
            "htmlhelp_basename = \"temporai-doc\"",
            "",
            "",
            "# -- Options for LaTeX output ------------------------------------------------",
            "",
            "latex_elements = {  # type: ignore",
            "    # The paper size (\"letterpaper\" or \"a4paper\").",
            "    # \"papersize\": \"letterpaper\",",
            "    # The font size (\"10pt\", \"11pt\" or \"12pt\").",
            "    # \"pointsize\": \"10pt\",",
            "    # Additional stuff for the LaTeX preamble.",
            "    # \"preamble\": \"\",",
            "}",
            "",
            "# Grouping the document tree into LaTeX files. List of tuples",
            "# (source start file, target name, title, author, documentclass [howto/manual]).",
            "latex_documents = [(\"index\", \"user_guide.tex\", \"TemporAI Documentation\", \"Evgeny Saveliev\", \"manual\")]",
            "",
            "# The name of an image file (relative to this directory) to place at the top of",
            "# the title page.",
            "# latex_logo = \"\"",
            "",
            "# For \"manual\" documents, if this is true, then toplevel headings are parts,",
            "# not chapters.",
            "# latex_use_parts = False",
            "",
            "# If true, show page references after internal links.",
            "# latex_show_pagerefs = False",
            "",
            "# If true, show URL addresses after external links.",
            "# latex_show_urls = False",
            "",
            "# Documents to append as an appendix to all manuals.",
            "# latex_appendices = []",
            "",
            "# If false, no module index is generated.",
            "# latex_domain_indices = True",
            "",
            "",
            "# `sphinx-zeta-suppress` (more specific warnings suppression) configuration.",
            "#",
            "# See:",
            "# - https://sphinx-zeta-suppress.readthedocs.io/",
            "#",
            "# See also:",
            "# - https://github.com/picnixz/sphinx-zeta-suppress",
            "# - https://github.com/sphinx-doc/sphinx/issues/11325",
            "# Note that since there is no PyPI package for sphinx-zeta-suppress, we add its python module as a custom extension,",
            "# see: docs/custom-sphinx-ext/sphinx-zeta-suppress.py.",
            "",
            "zeta_suppress_protect = [",
            "    \"sphinx_immaterial\"",
            "    # `sphinx-zeta-suppress` is not compatible with `sphinx_immaterial`. It will throw an error when registering the",
            "    # filters (`_setup_filters` function). However, the problem can be overcome by adding `sphinx_immaterial` to the",
            "    # `zeta_suppress_protect` list.",
            "    # Note we can still suppress warnings specific to `sphinx_immaterial` by adding it to",
            "    # the `zeta_suppress_records` list below.",
            "]",
            "",
            "zeta_suppress_records = [",
            "    # The following warnings caused ultimately by sphinx_immaterial are caused by having *args/**kwargs in the",
            "    # docstrings. We want to have those in docstrings, so we suppress these warnings.",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*args'.*\"],",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*\\*kwargs'.*\"],",
            "    # Similar but for other variable names used for the variadics:",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*dims'.*\"],",
            "    [\"sphinx_immaterial\", r\".*Parameter name '\\*\\*params'.*\"],",
            "    # The annotations for `PipelineMeta` seem to have their own set of issues, ignore here:",
            "    [\"sphinx_immaterial\", r\".*Parameter name.*PipelineMeta.*\"],",
            "    # Some warnings caused ultimately by pytorch docstrings:",
            "    r\".*undefined label: 'extending-autograd'.*\",",
            "    r\".*undefined label: 'combining-forward-context'.*\",",
            "]",
            "",
            "# `sphinx-zeta-suppress` (more specific warnings suppression) configuration [end].",
            "",
            "",
            "# -- External mapping --------------------------------------------------------",
            "python_version = \".\".join(map(str, sys.version_info[0:2]))",
            "intersphinx_mapping = {",
            "    \"sphinx\": (\"https://www.sphinx-doc.org/en/master\", None),",
            "    \"python\": (\"https://docs.python.org/\" + python_version, None),",
            "    \"matplotlib\": (\"https://matplotlib.org\", None),",
            "    \"numpy\": (\"https://numpy.org/doc/stable\", None),",
            "    \"sklearn\": (\"https://scikit-learn.org/stable\", None),",
            "    \"pandas\": (\"https://pandas.pydata.org/docs\", None),",
            "    \"pandera\": (\"https://pandera.readthedocs.io/en/stable\", None),",
            "    \"scipy\": (\"https://docs.scipy.org/doc/scipy/reference\", None),",
            "    \"setuptools\": (\"https://setuptools.pypa.io/en/stable/\", None),",
            "    \"pyscaffold\": (\"https://pyscaffold.org/en/stable\", None),",
            "    \"hyperimpute\": (\"https://hyperimpute.readthedocs.io/en/latest/\", None),",
            "    \"xgbse\": (\"https://loft-br.github.io/xgboost-survival-embeddings/\", None),",
            "    \"lifelines\": (\"https://lifelines.readthedocs.io/en/stable/\", None),",
            "    \"optuna\": (\"https://optuna.readthedocs.io/en/stable/\", None),",
            "}",
            "",
            "print(f\"loading configurations for {project} {version} ...\", file=sys.stderr)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "src/tempor/data/clv2conv.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import List, Type"
            },
            "1": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import pandas as pd"
            },
            "3": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.data import Dataset as Clairvoyance2Dataset"
            },
            "4": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from tempor.data import dataset, samples"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+from tempor.models.clairvoyance2.data import Dataset as Clairvoyance2Dataset"
            },
            "7": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from . import utils"
            },
            "9": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\"Utilities for converting to and from ``clairvoyance2`` datasets.\"\"\"",
            "",
            "from typing import List, Type",
            "",
            "import pandas as pd",
            "from clairvoyance2.data import Dataset as Clairvoyance2Dataset",
            "",
            "from tempor.data import dataset, samples",
            "",
            "from . import utils",
            "",
            "",
            "def _from_clv2_static(df: pd.DataFrame) -> pd.DataFrame:",
            "    df = utils.set_df_column_names_inplace(df, names=[str(c) for c in df.columns])",
            "    df.index.name = \"sample_idx\"",
            "    return df",
            "",
            "",
            "def _from_clv2_time_series(df: pd.DataFrame) -> pd.DataFrame:",
            "    df = utils.set_df_column_names_inplace(df, names=[str(c) for c in df.columns])",
            "    df.index.names = [\"sample_idx\", \"time_idx\"]",
            "    return df",
            "",
            "",
            "def _from_clv2_event(df: pd.DataFrame) -> pd.DataFrame:",
            "    column_names = [str(c) for c in df.columns]",
            "    sample_index = df.index.get_level_values(0)",
            "    sample_index.name = \"sample_idx\"",
            "    event_times = df.index.get_level_values(1)",
            "    data_ = {k: list(zip(event_times, df[k].astype(bool))) for k in column_names}",
            "    df_out = pd.DataFrame(data=data_, index=sample_index, columns=column_names)",
            "    return df_out",
            "",
            "",
            "def clairvoyance2_dataset_to_tempor_dataset(data: Clairvoyance2Dataset) -> dataset.BaseDataset:",
            "    \"\"\"A utility function to convert a ``clairvoyance2`` dataset to a TemporAI dataset.",
            "",
            "    Args:",
            "        data (Clairvoyance2Dataset): The ``clairvoyance2`` dataset to convert.",
            "",
            "    Returns:",
            "        dataset.BaseDataset: The converted dataset.",
            "    \"\"\"",
            "    if (",
            "        data.temporal_targets is None",
            "        and data.temporal_treatments is None",
            "        and data.event_targets is None",
            "        and data.event_treatments is None",
            "    ):",
            "        raise ValueError(",
            "            \"`clairvoyance2` dataset did not have any predictive data (targets or treatments), \"",
            "            \"this case is not supported\"",
            "        )",
            "    if data.temporal_targets is not None and data.event_targets is not None:",
            "        raise ValueError(",
            "            \"`clairvoyance2` dataset had both `temporal_targets` and `event_targets`, this case is not supported\"",
            "        )",
            "    if data.temporal_treatments is not None and data.event_treatments is not None:",
            "        raise ValueError(",
            "            \"`clairvoyance2` dataset had both `temporal_treatments` and `event_treatments`, this case is not supported\"",
            "        )",
            "    if data.event_covariates is not None:",
            "        raise ValueError(\"`clairvoyance2` dataset had `event_covariates`, this case is not supported.\")",
            "",
            "    # Covariates.",
            "    static_df = data.static_covariates.df if data.static_covariates is not None else None",
            "    time_series_df = data.temporal_covariates.to_multi_index_dataframe()",
            "",
            "    # Targets.",
            "    if data.temporal_targets is not None:",
            "        targets_df = data.temporal_targets.to_multi_index_dataframe()",
            "    elif data.event_targets is not None:",
            "        targets_df = data.event_targets.df",
            "    else:  # pragma: no cover",
            "        # Case caught by exceptions above.",
            "        targets_df = None",
            "",
            "    # Treatments.",
            "    if data.temporal_treatments is not None:",
            "        treatments_df = data.temporal_treatments.to_multi_index_dataframe()",
            "    elif data.event_treatments is not None:",
            "        treatments_df = data.event_treatments.df",
            "    else:",
            "        treatments_df = None",
            "",
            "    # Conversions.",
            "    if static_df is not None:",
            "        static_df = _from_clv2_static(static_df)",
            "    time_series_df = _from_clv2_time_series(time_series_df)",
            "    if data.temporal_targets is not None:",
            "        targets_df = _from_clv2_time_series(targets_df)  # pyright: ignore",
            "    elif data.event_targets is not None:",
            "        targets_df = _from_clv2_event(targets_df)  # pyright: ignore",
            "    else:  # pragma: no cover",
            "        # Case caught by exceptions above.",
            "        targets_df = None",
            "    if data.temporal_treatments is not None:",
            "        treatments_df = _from_clv2_time_series(treatments_df)  # pyright: ignore",
            "    elif data.event_treatments is not None:",
            "        treatments_df = _from_clv2_event(treatments_df)  # pyright: ignore",
            "",
            "    # Determine tempor.Dataset subclass.",
            "    if data.temporal_targets is not None and data.temporal_treatments is None and data.event_treatments is None:",
            "        TemporDatasetCls: Type[dataset.BaseDataset] = dataset.TemporalPredictionDataset",
            "    elif data.event_targets is not None and data.temporal_treatments is None and data.event_treatments is None:",
            "        TemporDatasetCls = dataset.TimeToEventAnalysisDataset",
            "    elif data.temporal_targets is not None and data.event_treatments is not None:",
            "        TemporDatasetCls = dataset.OneOffTreatmentEffectsDataset",
            "    elif data.temporal_targets is not None and data.temporal_treatments is not None:",
            "        TemporDatasetCls = dataset.TemporalTreatmentEffectsDataset",
            "    else:  # pragma: no cover",
            "        raise ValueError(",
            "            \"Cannot convert a clairvoyance2 dataset to tempor dataset in this case, as it is \"",
            "            f\"not supported, see clairvoyance2 dataset components:\\n{data}\"",
            "        )",
            "",
            "    return TemporDatasetCls(",
            "        time_series=time_series_df,",
            "        static=static_df,",
            "        targets=targets_df,  # pyright: ignore",
            "        treatments=treatments_df,  # pyright: ignore",
            "    )",
            "",
            "",
            "def _to_clv2_static(s: samples.StaticSamplesBase) -> pd.DataFrame:",
            "    int_sample_index = list(range(s.num_samples))",
            "    return s.dataframe().set_index(keys=pd.Index(int_sample_index), drop=True)",
            "",
            "",
            "def _to_clv2_time_series(s: samples.TimeSeriesSamplesBase) -> List[pd.DataFrame]:",
            "    return [df.droplevel(0) for df in s.list_of_dataframes()]",
            "",
            "",
            "def _to_clv2_event(s: samples.EventSamplesBase) -> pd.DataFrame:",
            "    int_sample_index = list(range(s.num_samples))",
            "",
            "    df_event_times, df_event_values = s.split_as_two_dataframes()",
            "",
            "    all_event_times_match = df_event_times.eq(df_event_times.iloc[:, 0], axis=0).all(1).all()",
            "    # ^ Check all time columns equal else exception.",
            "    if not all_event_times_match:",
            "        raise ValueError(",
            "            \"Event times must be the same for all features of each sample in order to \"",
            "            \"be convertible to a clairvoyance2 dataset\"",
            "        )",
            "    times = df_event_times.iloc[:, 0].to_list()",
            "",
            "    df = df_event_values.astype(int).set_index(keys=[pd.Index(int_sample_index), times])  # pyright: ignore",
            "",
            "    return df",
            "",
            "",
            "def tempor_dataset_to_clairvoyance2_dataset(data: dataset.BaseDataset) -> Clairvoyance2Dataset:",
            "    \"\"\"A utility function to convert a TemporAI dataset to a ``clairvoyance2`` dataset.",
            "",
            "    Args:",
            "        data (dataset.BaseDataset): The TemporAI dataset to convert.",
            "",
            "    Returns:",
            "        Clairvoyance2Dataset: The converted dataset.",
            "    \"\"\"",
            "    if isinstance(data, dataset.OneOffPredictionDataset):",
            "        raise ValueError(",
            "            \"Cannot convert a `OneOffPredictionDataset` to a clairvoyance2 dataset, as this setting is not supported\"",
            "        )",
            "",
            "    def has_temporal_targets(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.targets, samples.TimeSeriesSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    def has_temporal_treatments(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.treatments, samples.TimeSeriesSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    def has_event_targets(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.targets, samples.EventSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    def has_event_treatments(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.treatments, samples.EventSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    return Clairvoyance2Dataset(",
            "        temporal_covariates=_to_clv2_time_series(data.time_series),",
            "        static_covariates=_to_clv2_static(data.static) if data.static is not None else None,",
            "        event_covariates=None,",
            "        temporal_targets=(",
            "            _to_clv2_time_series(data.predictive.targets) if has_temporal_targets(data) else None  # type: ignore",
            "        ),",
            "        temporal_treatments=(",
            "            _to_clv2_time_series(data.predictive.treatments) if has_temporal_treatments(data) else None  # type: ignore",
            "        ),",
            "        event_targets=(_to_clv2_event(data.predictive.targets) if has_event_targets(data) else None),  # type: ignore",
            "        event_treatments=(",
            "            _to_clv2_event(data.predictive.treatments) if has_event_treatments(data) else None  # type: ignore",
            "        ),",
            "    )"
        ],
        "afterPatchFile": [
            "\"\"\"Utilities for converting to and from ``clairvoyance2`` datasets.\"\"\"",
            "",
            "from typing import List, Type",
            "",
            "import pandas as pd",
            "",
            "from tempor.data import dataset, samples",
            "from tempor.models.clairvoyance2.data import Dataset as Clairvoyance2Dataset",
            "",
            "from . import utils",
            "",
            "",
            "def _from_clv2_static(df: pd.DataFrame) -> pd.DataFrame:",
            "    df = utils.set_df_column_names_inplace(df, names=[str(c) for c in df.columns])",
            "    df.index.name = \"sample_idx\"",
            "    return df",
            "",
            "",
            "def _from_clv2_time_series(df: pd.DataFrame) -> pd.DataFrame:",
            "    df = utils.set_df_column_names_inplace(df, names=[str(c) for c in df.columns])",
            "    df.index.names = [\"sample_idx\", \"time_idx\"]",
            "    return df",
            "",
            "",
            "def _from_clv2_event(df: pd.DataFrame) -> pd.DataFrame:",
            "    column_names = [str(c) for c in df.columns]",
            "    sample_index = df.index.get_level_values(0)",
            "    sample_index.name = \"sample_idx\"",
            "    event_times = df.index.get_level_values(1)",
            "    data_ = {k: list(zip(event_times, df[k].astype(bool))) for k in column_names}",
            "    df_out = pd.DataFrame(data=data_, index=sample_index, columns=column_names)",
            "    return df_out",
            "",
            "",
            "def clairvoyance2_dataset_to_tempor_dataset(data: Clairvoyance2Dataset) -> dataset.BaseDataset:",
            "    \"\"\"A utility function to convert a ``clairvoyance2`` dataset to a TemporAI dataset.",
            "",
            "    Args:",
            "        data (Clairvoyance2Dataset): The ``clairvoyance2`` dataset to convert.",
            "",
            "    Returns:",
            "        dataset.BaseDataset: The converted dataset.",
            "    \"\"\"",
            "    if (",
            "        data.temporal_targets is None",
            "        and data.temporal_treatments is None",
            "        and data.event_targets is None",
            "        and data.event_treatments is None",
            "    ):",
            "        raise ValueError(",
            "            \"`clairvoyance2` dataset did not have any predictive data (targets or treatments), \"",
            "            \"this case is not supported\"",
            "        )",
            "    if data.temporal_targets is not None and data.event_targets is not None:",
            "        raise ValueError(",
            "            \"`clairvoyance2` dataset had both `temporal_targets` and `event_targets`, this case is not supported\"",
            "        )",
            "    if data.temporal_treatments is not None and data.event_treatments is not None:",
            "        raise ValueError(",
            "            \"`clairvoyance2` dataset had both `temporal_treatments` and `event_treatments`, this case is not supported\"",
            "        )",
            "    if data.event_covariates is not None:",
            "        raise ValueError(\"`clairvoyance2` dataset had `event_covariates`, this case is not supported.\")",
            "",
            "    # Covariates.",
            "    static_df = data.static_covariates.df if data.static_covariates is not None else None",
            "    time_series_df = data.temporal_covariates.to_multi_index_dataframe()",
            "",
            "    # Targets.",
            "    if data.temporal_targets is not None:",
            "        targets_df = data.temporal_targets.to_multi_index_dataframe()",
            "    elif data.event_targets is not None:",
            "        targets_df = data.event_targets.df",
            "    else:  # pragma: no cover",
            "        # Case caught by exceptions above.",
            "        targets_df = None",
            "",
            "    # Treatments.",
            "    if data.temporal_treatments is not None:",
            "        treatments_df = data.temporal_treatments.to_multi_index_dataframe()",
            "    elif data.event_treatments is not None:",
            "        treatments_df = data.event_treatments.df",
            "    else:",
            "        treatments_df = None",
            "",
            "    # Conversions.",
            "    if static_df is not None:",
            "        static_df = _from_clv2_static(static_df)",
            "    time_series_df = _from_clv2_time_series(time_series_df)",
            "    if data.temporal_targets is not None:",
            "        targets_df = _from_clv2_time_series(targets_df)  # pyright: ignore",
            "    elif data.event_targets is not None:",
            "        targets_df = _from_clv2_event(targets_df)  # pyright: ignore",
            "    else:  # pragma: no cover",
            "        # Case caught by exceptions above.",
            "        targets_df = None",
            "    if data.temporal_treatments is not None:",
            "        treatments_df = _from_clv2_time_series(treatments_df)  # pyright: ignore",
            "    elif data.event_treatments is not None:",
            "        treatments_df = _from_clv2_event(treatments_df)  # pyright: ignore",
            "",
            "    # Determine tempor.Dataset subclass.",
            "    if data.temporal_targets is not None and data.temporal_treatments is None and data.event_treatments is None:",
            "        TemporDatasetCls: Type[dataset.BaseDataset] = dataset.TemporalPredictionDataset",
            "    elif data.event_targets is not None and data.temporal_treatments is None and data.event_treatments is None:",
            "        TemporDatasetCls = dataset.TimeToEventAnalysisDataset",
            "    elif data.temporal_targets is not None and data.event_treatments is not None:",
            "        TemporDatasetCls = dataset.OneOffTreatmentEffectsDataset",
            "    elif data.temporal_targets is not None and data.temporal_treatments is not None:",
            "        TemporDatasetCls = dataset.TemporalTreatmentEffectsDataset",
            "    else:  # pragma: no cover",
            "        raise ValueError(",
            "            \"Cannot convert a clairvoyance2 dataset to tempor dataset in this case, as it is \"",
            "            f\"not supported, see clairvoyance2 dataset components:\\n{data}\"",
            "        )",
            "",
            "    return TemporDatasetCls(",
            "        time_series=time_series_df,",
            "        static=static_df,",
            "        targets=targets_df,  # pyright: ignore",
            "        treatments=treatments_df,  # pyright: ignore",
            "    )",
            "",
            "",
            "def _to_clv2_static(s: samples.StaticSamplesBase) -> pd.DataFrame:",
            "    int_sample_index = list(range(s.num_samples))",
            "    return s.dataframe().set_index(keys=pd.Index(int_sample_index), drop=True)",
            "",
            "",
            "def _to_clv2_time_series(s: samples.TimeSeriesSamplesBase) -> List[pd.DataFrame]:",
            "    return [df.droplevel(0) for df in s.list_of_dataframes()]",
            "",
            "",
            "def _to_clv2_event(s: samples.EventSamplesBase) -> pd.DataFrame:",
            "    int_sample_index = list(range(s.num_samples))",
            "",
            "    df_event_times, df_event_values = s.split_as_two_dataframes()",
            "",
            "    all_event_times_match = df_event_times.eq(df_event_times.iloc[:, 0], axis=0).all(1).all()",
            "    # ^ Check all time columns equal else exception.",
            "    if not all_event_times_match:",
            "        raise ValueError(",
            "            \"Event times must be the same for all features of each sample in order to \"",
            "            \"be convertible to a clairvoyance2 dataset\"",
            "        )",
            "    times = df_event_times.iloc[:, 0].to_list()",
            "",
            "    df = df_event_values.astype(int).set_index(keys=[pd.Index(int_sample_index), times])  # pyright: ignore",
            "",
            "    return df",
            "",
            "",
            "def tempor_dataset_to_clairvoyance2_dataset(data: dataset.BaseDataset) -> Clairvoyance2Dataset:",
            "    \"\"\"A utility function to convert a TemporAI dataset to a ``clairvoyance2`` dataset.",
            "",
            "    Args:",
            "        data (dataset.BaseDataset): The TemporAI dataset to convert.",
            "",
            "    Returns:",
            "        Clairvoyance2Dataset: The converted dataset.",
            "    \"\"\"",
            "    if isinstance(data, dataset.OneOffPredictionDataset):",
            "        raise ValueError(",
            "            \"Cannot convert a `OneOffPredictionDataset` to a clairvoyance2 dataset, as this setting is not supported\"",
            "        )",
            "",
            "    def has_temporal_targets(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.targets, samples.TimeSeriesSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    def has_temporal_treatments(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.treatments, samples.TimeSeriesSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    def has_event_targets(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.targets, samples.EventSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    def has_event_treatments(d: dataset.BaseDataset) -> bool:",
            "        if d.predictive is not None:",
            "            return isinstance(d.predictive.treatments, samples.EventSamplesBase)",
            "        else:",
            "            return False",
            "",
            "    return Clairvoyance2Dataset(",
            "        temporal_covariates=_to_clv2_time_series(data.time_series),",
            "        static_covariates=_to_clv2_static(data.static) if data.static is not None else None,",
            "        event_covariates=None,",
            "        temporal_targets=(",
            "            _to_clv2_time_series(data.predictive.targets) if has_temporal_targets(data) else None  # type: ignore",
            "        ),",
            "        temporal_treatments=(",
            "            _to_clv2_time_series(data.predictive.treatments) if has_temporal_treatments(data) else None  # type: ignore",
            "        ),",
            "        event_targets=(_to_clv2_event(data.predictive.targets) if has_event_targets(data) else None),  # type: ignore",
            "        event_treatments=(",
            "            _to_clv2_event(data.predictive.treatments) if has_event_treatments(data) else None  # type: ignore",
            "        ),",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "6": []
        },
        "addLocation": []
    },
    "src/tempor/data/samples.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "         return data_typing.DataModality.TIME_SERIES"
            },
            "1": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 214,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "     @abc.abstractmethod"
            },
            "3": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def time_indexes(self) -> data_typing.TimeIndexList:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+    def time_indexes(self) -> data_typing.TimeIndexList:  # pragma: no cover"
            },
            "5": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "         \"\"\"Get a list containing time indexes for each sample. Each time index is represented as a list of time step"
            },
            "6": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "         elements."
            },
            "7": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 219,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 223,
                "PatchRowcode": "         ..."
            },
            "9": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 224,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "     @abc.abstractmethod"
            },
            "11": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 226,
                "PatchRowcode": "+    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:  # pragma: no cover"
            },
            "13": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 227,
                "PatchRowcode": "         \"\"\"Get a dictionary mapping each sample index to its time index. Time index is represented as a list of time"
            },
            "14": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 228,
                "PatchRowcode": "         step elements."
            },
            "15": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 229,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "         ..."
            },
            "17": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": 234,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "     @abc.abstractmethod"
            },
            "19": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def time_indexes_float(self) -> List[np.ndarray]:"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 236,
                "PatchRowcode": "+    def time_indexes_float(self) -> List[np.ndarray]:  # pragma: no cover"
            },
            "21": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 237,
                "PatchRowcode": "         \"\"\"Return time indexes but converting their elements to `float` values."
            },
            "22": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": 238,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "         Date-time time index will be converted using :obj:`~tempor.data.utils.datetime_time_index_to_float`."
            },
            "24": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "         ..."
            },
            "25": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 245,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "     @abc.abstractmethod"
            },
            "27": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def num_timesteps(self) -> List[int]:"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+    def num_timesteps(self) -> List[int]:  # pragma: no cover"
            },
            "29": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 248,
                "PatchRowcode": "         \"\"\"Get the number of timesteps for each sample."
            },
            "30": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 249,
                "PatchRowcode": " "
            },
            "31": {
                "beforePatchRowNumber": 250,
                "afterPatchRowNumber": 250,
                "PatchRowcode": "         Returns:"
            },
            "32": {
                "beforePatchRowNumber": 253,
                "afterPatchRowNumber": 253,
                "PatchRowcode": "         ..."
            },
            "33": {
                "beforePatchRowNumber": 254,
                "afterPatchRowNumber": 254,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": 255,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "     @abc.abstractmethod"
            },
            "35": {
                "beforePatchRowNumber": 256,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 256,
                "PatchRowcode": "+    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:  # pragma: no cover"
            },
            "37": {
                "beforePatchRowNumber": 257,
                "afterPatchRowNumber": 257,
                "PatchRowcode": "         \"\"\"Get a dictionary mapping each sample index to its the number of timesteps."
            },
            "38": {
                "beforePatchRowNumber": 258,
                "afterPatchRowNumber": 258,
                "PatchRowcode": " "
            },
            "39": {
                "beforePatchRowNumber": 259,
                "afterPatchRowNumber": 259,
                "PatchRowcode": "         Returns:"
            },
            "40": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 262,
                "PatchRowcode": "         ..."
            },
            "41": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 263,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 264,
                "PatchRowcode": "     @abc.abstractmethod"
            },
            "43": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def num_timesteps_equal(self) -> bool:"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 265,
                "PatchRowcode": "+    def num_timesteps_equal(self) -> bool:  # pragma: no cover"
            },
            "45": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 266,
                "PatchRowcode": "         \"\"\"Returns `True` if all samples share the same number of timesteps, `False` otherwise."
            },
            "46": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 267,
                "PatchRowcode": " "
            },
            "47": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "         Returns:"
            },
            "48": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": 271,
                "PatchRowcode": "         ..."
            },
            "49": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 272,
                "PatchRowcode": " "
            },
            "50": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 273,
                "PatchRowcode": "     @abc.abstractmethod"
            },
            "51": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def list_of_dataframes(self) -> List[pd.DataFrame]:"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 274,
                "PatchRowcode": "+    def list_of_dataframes(self) -> List[pd.DataFrame]:  # pragma: no cover"
            },
            "53": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 275,
                "PatchRowcode": "         \"\"\"Returns a list of dataframes where each dataframe has the data for each sample."
            },
            "54": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 276,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": 277,
                "PatchRowcode": "         Returns:"
            }
        },
        "frontPatchFile": [
            "\"\"\"Data handling for different data samples modalities supported by TemporAI.\"\"\"",
            "",
            "# pylint: disable=unnecessary-ellipsis",
            "",
            "import abc",
            "import contextlib",
            "from typing import TYPE_CHECKING, Any, Generator, List, Optional, Tuple",
            "",
            "import numpy as np",
            "import pandas as pd",
            "import pandera as pa",
            "import pydantic",
            "from packaging.version import Version",
            "from typing_extensions import Self",
            "",
            "import tempor.exc",
            "from tempor.core import plugins, pydantic_utils",
            "from tempor.log import log_helpers, logger",
            "",
            "from . import data_typing, pandera_utils, utils",
            "from .settings import DATA_SETTINGS",
            "",
            "",
            "class DataSamples(plugins.Plugin, abc.ABC):",
            "    _data: Any",
            "",
            "    @property",
            "    @abc.abstractmethod",
            "    def modality(self) -> data_typing.DataModality:  # pragma: no cover",
            "        \"\"\"Return the data modality enum corresponding to the class",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum.",
            "        \"\"\"",
            "        ...",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,  # pylint: disable=unused-argument",
            "        **kwargs: Any,",
            "    ) -> None:  # pragma: no cover",
            "        \"\"\"The abstract base class for all data samples classes.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer): The data container.",
            "            **kwargs (Any): Any additional keyword arguments.",
            "        \"\"\"",
            "        plugins.Plugin.__init__(self)",
            "",
            "        if \"_skip_validate\" not in kwargs:",
            "            # For efficiency, pass `_skip_validate` internally (e.g. in `__getitem__`)",
            "            # when there is no need to validate.",
            "            self.validate()",
            "",
            "    def __repr__(self) -> str:",
            "        \"\"\"The `repr()` representation of the class.",
            "",
            "        Returns:",
            "            str: The representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__} with data:\\n{self.dataframe()}\"",
            "",
            "    def _repr_html_(self) -> str:",
            "        \"\"\"Return a HTML representation of the object, used in Jupyter notebooks.",
            "",
            "        Returns:",
            "            str: The HTML representation of the object.",
            "        \"\"\"",
            "        repr_ = (",
            "            # pylint: disable-next=protected-access",
            "            f'<p><span style=\"font-family: monospace;\">{self.__class__.__name__}</span> with data:</p>'",
            "            f\"{self.dataframe()._repr_html_()}\"  # pyright: ignore",
            "        )",
            "        return repr_",
            "",
            "    def validate(self) -> None:",
            "        \"\"\"Validate the data contained.",
            "",
            "        Raises:",
            "            tempor.exc.DataValidationException: Raised if data validation fails.",
            "        \"\"\"",
            "        with log_helpers.exc_to_log():",
            "            try:",
            "                self._validate()",
            "            except (",
            "                pa.errors.SchemaError,  # pyright: ignore",
            "                pa.errors.SchemaErrors,  # pyright: ignore",
            "                ValueError,",
            "                TypeError,",
            "            ) as ex:",
            "                raise tempor.exc.DataValidationException(",
            "                    \"Data validation failed, see traceback for more details\"",
            "                ) from ex",
            "",
            "    @abc.abstractmethod",
            "    def _validate(self) -> None:  # pragma: no cover",
            "        \"\"\"Validate integrity of the data samples. Raise any of `ValueError`, `TypeError`,",
            "        `pandera.errors.SchemaError`, `pandera.errors.SchemaErrors` (or exceptions derived from these) to indicate",
            "        validation failure.",
            "        \"\"\"",
            "        ...",
            "",
            "    @staticmethod",
            "    @abc.abstractmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"DataSamples\":  # pragma: no cover",
            "        \"\"\"Create :class:`DataSamples` from `numpy.ndarray`.",
            "",
            "        Args:",
            "            array (np.ndarray):",
            "                The array that represents the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                List with sample (row) index for each sample. Optional, if `None`, will be of form ``[0, 1, ...]``.",
            "                Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                List with feature (column) index for each feature. Optional, if `None`, will be of form",
            "                ``[\"feat_0\", \"feat_1\", ...]``. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments.",
            "",
            "        Returns:",
            "            DataSamples: :class:`DataSamples` object from ``array``.",
            "        \"\"\"",
            "        ...",
            "",
            "    @staticmethod",
            "    @abc.abstractmethod",
            "    def from_dataframe(dataframe: pd.DataFrame, **kwargs: Any) -> \"DataSamples\":  # pragma: no cover",
            "        \"\"\"Create :class:`DataSamples` from `pandas.DataFrame`.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:  # pragma: no cover",
            "        \"\"\"Return `numpy.ndarray` representation of the data.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:  # pragma: no cover",
            "        \"\"\"Return `pandas.DataFrame` representation of the data.\"\"\"",
            "        ...",
            "",
            "    @property",
            "    @abc.abstractmethod",
            "    def num_samples(self) -> int:  # pragma: no cover",
            "        \"\"\"Return number of samples.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def sample_index(self) -> data_typing.SampleIndex:  # pragma: no cover",
            "        \"\"\"Return a list representing sample indexes.\"\"\"",
            "        ...",
            "",
            "    def __len__(self) -> int:",
            "        \"\"\"The length, which is the number of samples.",
            "",
            "        Returns:",
            "            int: The number of samples.",
            "        \"\"\"",
            "        return self.num_samples",
            "",
            "    @property",
            "    @abc.abstractmethod",
            "    def num_features(self) -> int:  # pragma: no cover",
            "        \"\"\"Return number of features.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def short_repr(self) -> str:  # pragma: no cover",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short string representation of the object.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:  # pragma: no cover",
            "        \"\"\"Return a new subset :class:`DataSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`DataSamples` object.",
            "        \"\"\"",
            "        ...",
            "",
            "",
            "class StaticSamplesBase(DataSamples):",
            "    @property",
            "    def modality(self) -> data_typing.DataModality:",
            "        \"\"\"Return the data modality enum corresponding to the class. Here, ``STATIC``.",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum. Here, ``STATIC``.",
            "        \"\"\"",
            "        return data_typing.DataModality.STATIC",
            "",
            "",
            "class TimeSeriesSamplesBase(DataSamples):",
            "    @property",
            "    def modality(self) -> data_typing.DataModality:",
            "        \"\"\"Return the data modality enum corresponding to the class. Here, ``TIME_SERIES``.",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum. Here, ``TIME_SERIES``.",
            "        \"\"\"",
            "        return data_typing.DataModality.TIME_SERIES",
            "",
            "    @abc.abstractmethod",
            "    def time_indexes(self) -> data_typing.TimeIndexList:",
            "        \"\"\"Get a list containing time indexes for each sample. Each time index is represented as a list of time step",
            "        elements.",
            "",
            "        Returns:",
            "            data_typing.TimeIndexList: A list containing time indexes for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its time index. Time index is represented as a list of time",
            "        step elements.",
            "",
            "        Returns:",
            "            data_typing.SampleToTimeIndexDict: The dictionary mapping each sample index to its time index.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def time_indexes_float(self) -> List[np.ndarray]:",
            "        \"\"\"Return time indexes but converting their elements to `float` values.",
            "",
            "        Date-time time index will be converted using :obj:`~tempor.data.utils.datetime_time_index_to_float`.",
            "",
            "        Returns:",
            "            List[np.ndarray]: List of 1D `numpy.ndarray` s of `float` values, corresponding to the time index.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def num_timesteps(self) -> List[int]:",
            "        \"\"\"Get the number of timesteps for each sample.",
            "",
            "        Returns:",
            "            List[int]: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its the number of timesteps.",
            "",
            "        Returns:",
            "            data_typing.SampleToNumTimestepsDict: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def num_timesteps_equal(self) -> bool:",
            "        \"\"\"Returns `True` if all samples share the same number of timesteps, `False` otherwise.",
            "",
            "        Returns:",
            "            bool: whether all samples share the same number of timesteps.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def list_of_dataframes(self) -> List[pd.DataFrame]:",
            "        \"\"\"Returns a list of dataframes where each dataframe has the data for each sample.",
            "",
            "        Returns:",
            "            List[pd.DataFrame]: List of dataframes for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "",
            "_DEFAULT_EVENTS_TIME_FEATURE_SUFFIX = \"_time\"",
            "",
            "",
            "class EventSamplesBase(DataSamples):",
            "    @property",
            "    def modality(self) -> data_typing.DataModality:",
            "        \"\"\"Return the data modality enum corresponding to the class. Here, ``EVENT``.",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum. Here, ``EVENT``.",
            "        \"\"\"",
            "        return data_typing.DataModality.EVENT",
            "",
            "    @abc.abstractmethod",
            "    def split(self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX) -> pd.DataFrame:",
            "        \"\"\"Return a `pandas.DataFrame` where the time component of each event feature has been split off to its own",
            "        column. The new columns that contain the times will be named ``\"<original column name><time_feature_suffix>\"``",
            "        and will be inserted before each corresponding ``<original column name>`` column. The ``<original column name>``",
            "        columns will contain only the event value.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            pd.DataFrame: The output dataframe.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def split_as_two_dataframes(",
            "        self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX",
            "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:",
            "        \"\"\"Analogous to :func:`~tempor.data.samples.EventSamples.split` but returns two `pandas.DataFrame` s:",
            "            - first dataframe contains the event times of each feature.",
            "            - second dataframe contains the event values (`True`/`False`) of each feature.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            Tuple[pd.DataFrame, pd.DataFrame]: Two `pandas.DataFrame` s containing event times and values respectively.",
            "        \"\"\"",
            "        ...",
            "",
            "",
            "def _array_default_sample_index(array: np.ndarray) -> List[int]:",
            "    n_samples, *_ = array.shape",
            "    return list(range(0, n_samples))",
            "",
            "",
            "def _array_default_feature_index(array: np.ndarray) -> List[str]:",
            "    *_, n_features = array.shape",
            "    return [f\"feat_{x}\" for x in range(0, n_features)]",
            "",
            "",
            "def _array_default_time_indexes(array: np.ndarray, padding_indicator: Any) -> List[List[int]]:",
            "    lengths = utils.get_seq_lengths_timeseries_array3d(array, padding_indicator)",
            "    return [list(range(x)) for x in lengths]",
            "",
            "",
            "plugins.register_plugin_category(\"static_samples\", StaticSamplesBase, plugin_type=\"dataformat\")",
            "plugins.register_plugin_category(\"time_series_samples\", TimeSeriesSamplesBase, plugin_type=\"dataformat\")",
            "plugins.register_plugin_category(\"event_samples\", EventSamplesBase, plugin_type=\"dataformat\")",
            "",
            "",
            "@plugins.register_plugin(name=\"static_samples_df\", category=\"static_samples\", plugin_type=\"dataformat\")",
            "class StaticSamples(StaticSamplesBase):",
            "    _data: pd.DataFrame",
            "    _schema: pa.DataFrameSchema",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create a :class:`StaticSamples` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List with sample (row) index for each sample. Optional,",
            "                if `None`, will be of form ``[0, 1, ...]``. Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`.  List with feature (column) index for each feature.",
            "                Optional, if `None`, will be of form ``[\"feat_0\", \"feat_1\", ...]``. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        if isinstance(data, pd.DataFrame):",
            "            self._data = data",
            "        elif isinstance(data, np.ndarray):",
            "            self._data = self._array_to_df(data, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        schema = pandera_utils.init_schema(self._data, coerce=False)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(schema, pa.DataFrameSchema)  # nosec B101",
            "        logger.debug(f\"Inferred schema:\\n{schema}\")",
            "",
            "        # DataFrame-level validation:",
            "        schema = pandera_utils.add_df_checks(",
            "            schema,",
            "            checks_list=[",
            "                pandera_utils.checks.forbid_multiindex_index,",
            "                pandera_utils.checks.forbid_multiindex_columns,",
            "                pandera_utils.checks.configurable.column_index_satisfies_dtype(",
            "                    pandera_utils.UnionDtype[DATA_SETTINGS.feature_index_dtypes],  # type: ignore",
            "                    nullable=DATA_SETTINGS.feature_index_nullable,",
            "                ),",
            "            ],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Values validation:",
            "        schema = pandera_utils.add_regex_column_checks(",
            "            schema,",
            "            regex=\".*\",",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.static_value_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.static_values_nullable,",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Index validation:",
            "        schema, data = pandera_utils.set_up_index(",
            "            schema,",
            "            self._data,",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.sample_index_dtypes],  # type: ignore",
            "            name=DATA_SETTINGS.sample_index_name,",
            "            nullable=DATA_SETTINGS.sample_index_nullable,",
            "            coerce=False,",
            "            unique=DATA_SETTINGS.sample_index_unique,",
            "        )",
            "        self._data = schema.validate(data)",
            "",
            "        logger.debug(f\"Final schema:\\n{schema}\")",
            "        self._schema = schema",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`StaticSamples` from `pandas.DataFrame`. The rows represent samples, the columns represent",
            "        features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that represents the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            StaticSamples: :class:`StaticSamples` object from ``dataframe``.",
            "        \"\"\"",
            "        return StaticSamples(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`StaticSamples` from `numpy.ndarray`. The 0th dimension represents samples, the 1st dimension",
            "        represents features.",
            "",
            "        Args:",
            "            array (np.ndarray): The array with the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional): Sample indices to assign. Defaults to None.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional): Feature indices to assign. Defaults to None.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            StaticSamples: :class:`StaticSamples` object created from the ``array``.",
            "        \"\"\"",
            "        return StaticSamples(array, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "",
            "    @staticmethod",
            "    def _array_to_df(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> pd.DataFrame:",
            "        if sample_index is None:",
            "            sample_index = _array_default_sample_index(array)  # pyright: ignore",
            "        if feature_index is None:",
            "            feature_index = _array_default_feature_index(array)",
            "        return pd.DataFrame(data=array, index=sample_index, columns=feature_index, **kwargs)",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return self._data.to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The dataframe.",
            "        \"\"\"",
            "        return self._data",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index)  # pyright: ignore",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0]",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`StaticSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`StaticSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return StaticSamples(  # type: ignore [return-value]",
            "            self._data.iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "@contextlib.contextmanager",
            "def workaround_pandera_pd2_1_0_multiindex_compatibility(schema: pa.DataFrameSchema, data: pd.DataFrame) -> Generator:",
            "    \"\"\"A version compatibility issue exists between pandera and pandas 2.1.0, as reported here:",
            "    https://github.com/unionai-oss/pandera/issues/1328",
            "",
            "    The error pertains to multiindex uniqueness validation giving an unexpected error.",
            "",
            "    This is a workaround that will \"manually\" throw an error that is expected from pandera.",
            "    \"\"\"",
            "",
            "    def problem_versions() -> bool:  # pragma: no cover",
            "        return Version(pd.__version__) >= Version(\"2.1.0\")",
            "        # TODO: When/if fixed in pandera, add the below condition:",
            "        # and Version(pa.__version__) < Version(\"0.XX.YY\")",
            "",
            "    try:",
            "        yield",
            "",
            "    except ValueError as ex:",
            "        if problem_versions() and (",
            "            \"Columns with duplicate values are not supported in stack\" in str(ex)",
            "        ):  # pragma: no cover",
            "            cols = data.index.names",
            "            raise pa.errors.SchemaError(  # type: ignore [no-untyped-call]",
            "                schema=schema,",
            "                data=data,",
            "                message=f\"columns {cols} not unique\",",
            "            )",
            "        else:  # pragma: no cover",
            "            raise",
            "",
            "    finally:",
            "        pass",
            "",
            "",
            "@plugins.register_plugin(name=\"time_series_samples_df\", category=\"time_series_samples\", plugin_type=\"dataformat\")",
            "class TimeSeriesSamples(TimeSeriesSamplesBase):",
            "    _data: pd.DataFrame",
            "    _schema: pa.DataFrameSchema",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        *,",
            "        padding_indicator: Any = None,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        time_indexes: Optional[data_typing.TimeIndexList] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create a :class:`TimeSeriesSamples` object from the ``data``.",
            "",
            "        If ``data`` is a `pandas.DataFrame`, this should be a 2-level multiindex (sample, timestep) dataframe.",
            "",
            "        If ``data`` is a `numpy.ndarray`, this should be a 3D array, with dimensions ``(sample, timestep, feature)``.",
            "        Optionally, padding values of ``padding_indicator`` can be set inside the array to pad out the length of arrays",
            "        of different samples in case they differ. Padding needs to go at the end of the timesteps (dim 1). Padding must",
            "        be the same across the feature dimension (dim 2) for each sample.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            padding_indicator (Any, optional):",
            "                Padding indicator used in ``data`` to indicate padding. Defaults to `None`.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List with sample (row) index for each sample.",
            "                Optional, if `None`, will be of form ``[0, 1, ...]``. Defaults to `None`.",
            "            time_indexes (Optional[data_typing.TimeIndexList], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List of lists containing timesteps for each sample (outer",
            "                list should be the same length as dim 0 of `data`, inner list should contain as many elements as each",
            "                sample has timesteps). Optional, if `None`, will be of form ``[[0, 1, ...], [0, 1, ...], ...]``",
            "                Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`.  List with feature (column) index for each feature.",
            "                Optional, if `None`, will be of form ``[\"feat_0\", \"feat_1\", ...]``.",
            "                Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        if isinstance(data, pd.DataFrame):",
            "            self._data = data",
            "        elif isinstance(data, np.ndarray):",
            "            self._data = self._array_to_df(",
            "                data,",
            "                padding_indicator=padding_indicator,",
            "                sample_index=sample_index,",
            "                time_indexes=time_indexes,",
            "                feature_index=feature_index,",
            "                **kwargs,",
            "            )",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        schema = pandera_utils.init_schema(self._data, coerce=False)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(schema, pa.DataFrameSchema)  # nosec B101",
            "        logger.debug(f\"Inferred schema:\\n{schema}\")",
            "",
            "        # DataFrame-level validation:",
            "        schema = pandera_utils.add_df_checks(",
            "            schema,",
            "            checks_list=[",
            "                pandera_utils.checks.forbid_multiindex_columns,",
            "                pandera_utils.checks.require_2level_multiindex_index,",
            "                pandera_utils.checks.configurable.column_index_satisfies_dtype(",
            "                    pandera_utils.UnionDtype[DATA_SETTINGS.feature_index_dtypes],  # type: ignore",
            "                    nullable=DATA_SETTINGS.feature_index_nullable,",
            "                ),",
            "            ],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Values validation:",
            "        schema = pandera_utils.add_regex_column_checks(",
            "            schema,",
            "            regex=\".*\",",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.time_series_value_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.time_series_values_nullable,",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Index validation:",
            "        if not (DATA_SETTINGS.sample_index_unique and DATA_SETTINGS.sample_timestep_index_unique):",
            "            raise NotImplementedError(\"Only supported case: unique sample and unique timestep indexes\")",
            "        multiindex_unique_def = (DATA_SETTINGS.sample_index_name, DATA_SETTINGS.time_index_name)",
            "        schema, data = pandera_utils.set_up_2level_multiindex(",
            "            schema,",
            "            self._data,",
            "            dtypes=(",
            "                pandera_utils.UnionDtype[DATA_SETTINGS.sample_index_dtypes],  # type: ignore",
            "                pandera_utils.UnionDtype[DATA_SETTINGS.time_index_dtypes],  # type: ignore",
            "            ),",
            "            names=(DATA_SETTINGS.sample_index_name, DATA_SETTINGS.time_index_name),",
            "            nullable=(DATA_SETTINGS.sample_index_nullable, DATA_SETTINGS.time_index_nullable),",
            "            coerce=False,",
            "            unique=multiindex_unique_def,",
            "        )",
            "        with workaround_pandera_pd2_1_0_multiindex_compatibility(schema, data):",
            "            self._data = schema.validate(data)",
            "",
            "        logger.debug(f\"Final schema:\\n{schema}\")",
            "        self._schema = schema",
            "",
            "        # TODO:",
            "        # Possible additional validation checks:",
            "        # - Ensure time index sorted ascending within each sample.",
            "        # - Time index float / int expected non-negative values.",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`TimeSeriesSamples` from `pandas.DataFrame`. This row index of the dataframe should be a",
            "        2-level multiindex (sample, timestep). The columns should be the features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            TimeSeriesSamples: The :class:`TimeSeriesSamples` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return TimeSeriesSamples(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        padding_indicator: Any = None,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        time_indexes: Optional[data_typing.TimeIndexList] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`TimeSeriesSamples` from `numpy.ndarray`.",
            "",
            "        This should be a 3D array, with dimensions ``(sample, timestep, feature)``.",
            "",
            "        Optionally, padding values of ``padding_indicator`` can be set inside the array to pad out the length of arrays",
            "        of different samples in case they differ. Padding needs to go at the end of the timesteps (dim 1). Padding must",
            "        be the same across the feature dimension (dim 2) for each sample.",
            "",
            "        Args:",
            "            array (np.ndarray):",
            "                The array that contains the data.",
            "            padding_indicator (Any, optional):",
            "                The padding indicator value. Defaults to `None`.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Sample indexes as a list. Defaults to `None`.",
            "            time_indexes (Optional[data_typing.TimeIndexList], optional):",
            "                Time indexes as a list of list (that is, time indexes per sample). Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Feature indexes as a list. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments.",
            "",
            "        Returns:",
            "            TimeSeriesSamples: The :class:`TimeSeriesSamples` object created from the ``array``.",
            "        \"\"\"",
            "        return TimeSeriesSamples(",
            "            array,",
            "            padding_indicator=padding_indicator,",
            "            sample_index=sample_index,",
            "            time_indexes=time_indexes,",
            "            feature_index=feature_index,",
            "            **kwargs,",
            "        )",
            "",
            "    @staticmethod",
            "    def _array_to_df(  # pylint: disable=unused-argument",
            "        array: np.ndarray,",
            "        *,",
            "        padding_indicator: Any,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        time_indexes: Optional[data_typing.TimeIndexList] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> pd.DataFrame:",
            "        if sample_index is None:",
            "            sample_index = _array_default_sample_index(array)  # pyright: ignore",
            "        if feature_index is None:",
            "            feature_index = _array_default_feature_index(array)",
            "        if time_indexes is None:",
            "            time_indexes = _array_default_time_indexes(array, padding_indicator)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert sample_index is not None and feature_index is not None and time_indexes is not None  # nosec B101",
            "        return utils.array3d_to_multiindex_timeseries_dataframe(",
            "            array,",
            "            sample_index=sample_index,",
            "            time_indexes=time_indexes,",
            "            feature_index=feature_index,",
            "            padding_indicator=padding_indicator,",
            "        )",
            "",
            "    def numpy(self, *, padding_indicator: Any = DATA_SETTINGS.default_padding_indicator, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            padding_indicator (Any, optional):",
            "                Padding indicator value. Defaults to `DATA_SETTINGS.default_padding_indicator`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_array3d(",
            "            df=self._data, padding_indicator=padding_indicator, max_timesteps=None",
            "        )",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return self._data",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Get a list containing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: A list containing sample indexes.",
            "        \"\"\"",
            "        return list(utils.get_df_index_level0_unique(self._data))  # pyright: ignore",
            "",
            "    def time_indexes(self) -> data_typing.TimeIndexList:",
            "        \"\"\"Get a list containing time indexes for each sample. Each time index is represented as a list of time step",
            "        elements.",
            "",
            "        Returns:",
            "            data_typing.TimeIndexList: A list containing time indexes for each sample.",
            "        \"\"\"",
            "        return list(self.time_indexes_as_dict().values())  # pyright: ignore",
            "",
            "    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its time index. Time index is represented as a list of time",
            "        step elements.",
            "",
            "        Returns:",
            "            data_typing.SampleToTimeIndexDict: The dictionary mapping each sample index to its time index.",
            "        \"\"\"",
            "        multiindex = self._data.index",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(multiindex, pd.MultiIndex)  # nosec B101",
            "        sample_index = self.sample_index()",
            "        d = dict()",
            "        for s in sample_index:",
            "            time_index_locs = multiindex.get_locs([s, slice(None)])",
            "            d[s] = list(multiindex.get_level_values(1)[time_index_locs])",
            "        return d  # type: ignore[return-value]",
            "",
            "    def time_indexes_float(self) -> List[np.ndarray]:",
            "        \"\"\"Return time indexes but converting their elements to `float` values.",
            "",
            "        Date-time time index will be converted using :obj:`~tempor.data.utils.datetime_time_index_to_float`.",
            "",
            "        Returns:",
            "            List[np.ndarray]: List of 1D `numpy.ndarray` s of `float` values, corresponding to the time index.",
            "        \"\"\"",
            "        return [utils.datetime_time_index_to_float(ti) for ti in self.time_indexes()]",
            "",
            "    def num_timesteps(self) -> List[int]:",
            "        \"\"\"Get the number of timesteps for each sample.",
            "",
            "        Returns:",
            "            List[int]: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return [len(x) for x in self.time_indexes()]",
            "",
            "    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its the number of timesteps.",
            "",
            "        Returns:",
            "            data_typing.SampleToNumTimestepsDict: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return {key: len(x) for key, x in self.time_indexes_as_dict().items()}  # type: ignore",
            "",
            "    def num_timesteps_equal(self) -> bool:",
            "        \"\"\"Returns `True` if all samples share the same number of timesteps, `False` otherwise.",
            "",
            "        Returns:",
            "            bool: whether all samples share the same number of timesteps.",
            "        \"\"\"",
            "        timesteps = self.num_timesteps()",
            "        return True if len(timesteps) == 0 else all([x == timesteps[0] for x in timesteps])",
            "",
            "    def list_of_dataframes(self) -> List[pd.DataFrame]:",
            "        \"\"\"Returns a list of dataframes where each dataframe has the data for each sample.",
            "",
            "        Returns:",
            "            List[pd.DataFrame]: List of dataframes for each sample.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_list_of_dataframes(self._data)",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        sample_ids = utils.get_df_index_level0_unique(self._data)",
            "        return len(sample_ids)",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, *, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a subset :class:`TimeSeriesSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`TimeSeriesSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        sample_index = utils.get_df_index_level0_unique(self._data)",
            "        selected = list(sample_index[key_])  # pyright: ignore",
            "        return TimeSeriesSamples(  # type: ignore [return-value]",
            "            self._data.loc[(selected, slice(None)), :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "@plugins.register_plugin(name=\"event_samples_df\", category=\"event_samples\", plugin_type=\"dataformat\")",
            "class EventSamples(EventSamplesBase):",
            "    _data: pd.DataFrame",
            "    _schema: pa.DataFrameSchema",
            "    _schema_split: pa.DataFrameSchema",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create an :class:`EventSamples` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List with sample (row) index for each sample. Optional,",
            "                if `None`, will be of form ``[0, 1, ...]``. Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`.  List with feature (column) index for each feature.",
            "                Optional, if `None`, will be of form ``[\"feat_0\", \"feat_1\", ...]``. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        if isinstance(data, pd.DataFrame):",
            "            self._data = data",
            "        elif isinstance(data, np.ndarray):",
            "            self._data = self._array_to_df(data, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        schema = pandera_utils.init_schema(self._data, coerce=False)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(schema, pa.DataFrameSchema)  # nosec B101",
            "        logger.debug(f\"Inferred schema:\\n{schema}\")",
            "",
            "        # DataFrame-level validation:",
            "        schema = pandera_utils.add_df_checks(",
            "            schema,",
            "            checks_list=[",
            "                pandera_utils.checks.forbid_multiindex_index,",
            "                pandera_utils.checks.forbid_multiindex_columns,",
            "                pandera_utils.checks.configurable.column_index_satisfies_dtype(",
            "                    pandera_utils.UnionDtype[DATA_SETTINGS.feature_index_dtypes],  # type: ignore",
            "                    nullable=DATA_SETTINGS.feature_index_nullable,",
            "                ),",
            "            ],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Values validation:",
            "        schema = pandera_utils.add_regex_column_checks(",
            "            schema,",
            "            regex=\".*\",",
            "            dtype=None,",
            "            nullable=DATA_SETTINGS.event_values_nullable,",
            "            checks_list=[pandera_utils.checks.require_element_len_2],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "        # Validate event time and value components:",
            "        suffix = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX",
            "        data_split = self.split(time_feature_suffix=suffix)",
            "        schema_split = pandera_utils.init_schema(data_split, coerce=False)",
            "        schema_split = pandera_utils.add_regex_column_checks(",
            "            schema_split,",
            "            regex=f\".*{suffix}$\",  # Event time columns, end in \"_time\".",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.time_index_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.time_index_nullable,",
            "        )",
            "        schema_split = pandera_utils.add_regex_column_checks(",
            "            schema_split,",
            "            regex=f\"^((?!{suffix}$).)*$\",  # Event value columns, do not end in \"_time\".",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.event_value_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.event_values_nullable,",
            "        )",
            "        logger.debug(f\"Time split-off schema (checks event time and values separately):\\n{schema_split}\")",
            "        schema_split.validate(data_split)",
            "        self._schema_split = schema_split",
            "",
            "        # Index validation:",
            "        schema, data = pandera_utils.set_up_index(",
            "            schema,",
            "            self._data,",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.sample_index_dtypes],  # type: ignore",
            "            name=DATA_SETTINGS.sample_index_name,",
            "            nullable=DATA_SETTINGS.sample_index_nullable,",
            "            coerce=False,",
            "            unique=DATA_SETTINGS.sample_index_unique,",
            "        )",
            "        self._data = schema.validate(data)",
            "",
            "        logger.debug(f\"Final schema:\\n{schema}\")",
            "        self._schema = schema",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`EventSamples` from `pandas.DataFrame`. The row index of the dataframe should be the sample",
            "        indexes. The columns should be the features. Each feature should contain a tuple of ``(time, value)``",
            "        representing the event.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            EventSamples: The :class:`EventSamples` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return EventSamples(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`EventSamples` from `numpy.ndarray`. The array should be a 2D array, with dimensions",
            "        ``(sample, feature)``. Each element should contain a tuple of ``(time, value)`` representing the event.",
            "",
            "        Args:",
            "            array (np.ndarray): The array that contains the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional): Sample indexes. Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional): Feature index. Defaults to `None`.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            EventSamples: The :class:`EventSamples` object created from the ``array``.",
            "        \"\"\"",
            "        return EventSamples(array, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "",
            "    @staticmethod",
            "    def _array_to_df(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> pd.DataFrame:",
            "        if sample_index is None:",
            "            sample_index = _array_default_sample_index(array)  # pyright: ignore",
            "        if feature_index is None:",
            "            feature_index = _array_default_feature_index(array)",
            "        return pd.DataFrame(data=array, index=sample_index, columns=feature_index, **kwargs)",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        # TODO: May want at option to return a scikit-survive -style array.",
            "        return self._data.to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return self._data",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index)  # pyright: ignore",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0]",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def split(self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX) -> pd.DataFrame:",
            "        \"\"\"Return a `pandas.DataFrame` where the time component of each event feature has been split off to its own",
            "        column. The new columns that contain the times will be named ``\"<original column name><time_feature_suffix>\"``",
            "        and will be inserted before each corresponding ``<original column name>`` column. The ``<original column name>``",
            "        columns will contain only the event value.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            pd.DataFrame: The output dataframe.",
            "        \"\"\"",
            "        df = self._data.copy()",
            "        features = list(df.columns)",
            "        if any(time_feature_suffix in str(c) for c in features):",
            "            raise ValueError(f\"Column names must not contain '{time_feature_suffix}'\")",
            "        for f_idx, f in enumerate(features):",
            "            df.insert(f_idx * 2, f\"{f}{time_feature_suffix}\", df[f].apply(lambda x: x[0]))",
            "        for f in features:",
            "            df[f] = df[f].apply(lambda x: x[1])",
            "        return df",
            "",
            "    def split_as_two_dataframes(",
            "        self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX",
            "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:",
            "        \"\"\"Analogous to :func:`~tempor.data.samples.EventSamples.split` but returns two `pandas.DataFrame` s:",
            "            - first dataframe contains the event times of each feature.",
            "            - second dataframe contains the event values (`True`/`False`) of each feature.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            Tuple[pd.DataFrame, pd.DataFrame]: Two `pandas.DataFrame` s containing event times and values respectively.",
            "        \"\"\"",
            "        df_split = self.split(time_feature_suffix=time_feature_suffix)",
            "        df_event_times = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix in c]]",
            "        df_event_values = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix not in c]]",
            "        return df_event_times, df_event_values",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`EventSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`EventSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return EventSamples(  # type: ignore [return-value]",
            "            self._data.iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )"
        ],
        "afterPatchFile": [
            "\"\"\"Data handling for different data samples modalities supported by TemporAI.\"\"\"",
            "",
            "# pylint: disable=unnecessary-ellipsis",
            "",
            "import abc",
            "import contextlib",
            "from typing import TYPE_CHECKING, Any, Generator, List, Optional, Tuple",
            "",
            "import numpy as np",
            "import pandas as pd",
            "import pandera as pa",
            "import pydantic",
            "from packaging.version import Version",
            "from typing_extensions import Self",
            "",
            "import tempor.exc",
            "from tempor.core import plugins, pydantic_utils",
            "from tempor.log import log_helpers, logger",
            "",
            "from . import data_typing, pandera_utils, utils",
            "from .settings import DATA_SETTINGS",
            "",
            "",
            "class DataSamples(plugins.Plugin, abc.ABC):",
            "    _data: Any",
            "",
            "    @property",
            "    @abc.abstractmethod",
            "    def modality(self) -> data_typing.DataModality:  # pragma: no cover",
            "        \"\"\"Return the data modality enum corresponding to the class",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum.",
            "        \"\"\"",
            "        ...",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,  # pylint: disable=unused-argument",
            "        **kwargs: Any,",
            "    ) -> None:  # pragma: no cover",
            "        \"\"\"The abstract base class for all data samples classes.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer): The data container.",
            "            **kwargs (Any): Any additional keyword arguments.",
            "        \"\"\"",
            "        plugins.Plugin.__init__(self)",
            "",
            "        if \"_skip_validate\" not in kwargs:",
            "            # For efficiency, pass `_skip_validate` internally (e.g. in `__getitem__`)",
            "            # when there is no need to validate.",
            "            self.validate()",
            "",
            "    def __repr__(self) -> str:",
            "        \"\"\"The `repr()` representation of the class.",
            "",
            "        Returns:",
            "            str: The representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__} with data:\\n{self.dataframe()}\"",
            "",
            "    def _repr_html_(self) -> str:",
            "        \"\"\"Return a HTML representation of the object, used in Jupyter notebooks.",
            "",
            "        Returns:",
            "            str: The HTML representation of the object.",
            "        \"\"\"",
            "        repr_ = (",
            "            # pylint: disable-next=protected-access",
            "            f'<p><span style=\"font-family: monospace;\">{self.__class__.__name__}</span> with data:</p>'",
            "            f\"{self.dataframe()._repr_html_()}\"  # pyright: ignore",
            "        )",
            "        return repr_",
            "",
            "    def validate(self) -> None:",
            "        \"\"\"Validate the data contained.",
            "",
            "        Raises:",
            "            tempor.exc.DataValidationException: Raised if data validation fails.",
            "        \"\"\"",
            "        with log_helpers.exc_to_log():",
            "            try:",
            "                self._validate()",
            "            except (",
            "                pa.errors.SchemaError,  # pyright: ignore",
            "                pa.errors.SchemaErrors,  # pyright: ignore",
            "                ValueError,",
            "                TypeError,",
            "            ) as ex:",
            "                raise tempor.exc.DataValidationException(",
            "                    \"Data validation failed, see traceback for more details\"",
            "                ) from ex",
            "",
            "    @abc.abstractmethod",
            "    def _validate(self) -> None:  # pragma: no cover",
            "        \"\"\"Validate integrity of the data samples. Raise any of `ValueError`, `TypeError`,",
            "        `pandera.errors.SchemaError`, `pandera.errors.SchemaErrors` (or exceptions derived from these) to indicate",
            "        validation failure.",
            "        \"\"\"",
            "        ...",
            "",
            "    @staticmethod",
            "    @abc.abstractmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"DataSamples\":  # pragma: no cover",
            "        \"\"\"Create :class:`DataSamples` from `numpy.ndarray`.",
            "",
            "        Args:",
            "            array (np.ndarray):",
            "                The array that represents the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                List with sample (row) index for each sample. Optional, if `None`, will be of form ``[0, 1, ...]``.",
            "                Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                List with feature (column) index for each feature. Optional, if `None`, will be of form",
            "                ``[\"feat_0\", \"feat_1\", ...]``. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments.",
            "",
            "        Returns:",
            "            DataSamples: :class:`DataSamples` object from ``array``.",
            "        \"\"\"",
            "        ...",
            "",
            "    @staticmethod",
            "    @abc.abstractmethod",
            "    def from_dataframe(dataframe: pd.DataFrame, **kwargs: Any) -> \"DataSamples\":  # pragma: no cover",
            "        \"\"\"Create :class:`DataSamples` from `pandas.DataFrame`.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:  # pragma: no cover",
            "        \"\"\"Return `numpy.ndarray` representation of the data.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:  # pragma: no cover",
            "        \"\"\"Return `pandas.DataFrame` representation of the data.\"\"\"",
            "        ...",
            "",
            "    @property",
            "    @abc.abstractmethod",
            "    def num_samples(self) -> int:  # pragma: no cover",
            "        \"\"\"Return number of samples.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def sample_index(self) -> data_typing.SampleIndex:  # pragma: no cover",
            "        \"\"\"Return a list representing sample indexes.\"\"\"",
            "        ...",
            "",
            "    def __len__(self) -> int:",
            "        \"\"\"The length, which is the number of samples.",
            "",
            "        Returns:",
            "            int: The number of samples.",
            "        \"\"\"",
            "        return self.num_samples",
            "",
            "    @property",
            "    @abc.abstractmethod",
            "    def num_features(self) -> int:  # pragma: no cover",
            "        \"\"\"Return number of features.\"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def short_repr(self) -> str:  # pragma: no cover",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short string representation of the object.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:  # pragma: no cover",
            "        \"\"\"Return a new subset :class:`DataSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`DataSamples` object.",
            "        \"\"\"",
            "        ...",
            "",
            "",
            "class StaticSamplesBase(DataSamples):",
            "    @property",
            "    def modality(self) -> data_typing.DataModality:",
            "        \"\"\"Return the data modality enum corresponding to the class. Here, ``STATIC``.",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum. Here, ``STATIC``.",
            "        \"\"\"",
            "        return data_typing.DataModality.STATIC",
            "",
            "",
            "class TimeSeriesSamplesBase(DataSamples):",
            "    @property",
            "    def modality(self) -> data_typing.DataModality:",
            "        \"\"\"Return the data modality enum corresponding to the class. Here, ``TIME_SERIES``.",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum. Here, ``TIME_SERIES``.",
            "        \"\"\"",
            "        return data_typing.DataModality.TIME_SERIES",
            "",
            "    @abc.abstractmethod",
            "    def time_indexes(self) -> data_typing.TimeIndexList:  # pragma: no cover",
            "        \"\"\"Get a list containing time indexes for each sample. Each time index is represented as a list of time step",
            "        elements.",
            "",
            "        Returns:",
            "            data_typing.TimeIndexList: A list containing time indexes for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:  # pragma: no cover",
            "        \"\"\"Get a dictionary mapping each sample index to its time index. Time index is represented as a list of time",
            "        step elements.",
            "",
            "        Returns:",
            "            data_typing.SampleToTimeIndexDict: The dictionary mapping each sample index to its time index.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def time_indexes_float(self) -> List[np.ndarray]:  # pragma: no cover",
            "        \"\"\"Return time indexes but converting their elements to `float` values.",
            "",
            "        Date-time time index will be converted using :obj:`~tempor.data.utils.datetime_time_index_to_float`.",
            "",
            "        Returns:",
            "            List[np.ndarray]: List of 1D `numpy.ndarray` s of `float` values, corresponding to the time index.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def num_timesteps(self) -> List[int]:  # pragma: no cover",
            "        \"\"\"Get the number of timesteps for each sample.",
            "",
            "        Returns:",
            "            List[int]: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:  # pragma: no cover",
            "        \"\"\"Get a dictionary mapping each sample index to its the number of timesteps.",
            "",
            "        Returns:",
            "            data_typing.SampleToNumTimestepsDict: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def num_timesteps_equal(self) -> bool:  # pragma: no cover",
            "        \"\"\"Returns `True` if all samples share the same number of timesteps, `False` otherwise.",
            "",
            "        Returns:",
            "            bool: whether all samples share the same number of timesteps.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def list_of_dataframes(self) -> List[pd.DataFrame]:  # pragma: no cover",
            "        \"\"\"Returns a list of dataframes where each dataframe has the data for each sample.",
            "",
            "        Returns:",
            "            List[pd.DataFrame]: List of dataframes for each sample.",
            "        \"\"\"",
            "        ...",
            "",
            "",
            "_DEFAULT_EVENTS_TIME_FEATURE_SUFFIX = \"_time\"",
            "",
            "",
            "class EventSamplesBase(DataSamples):",
            "    @property",
            "    def modality(self) -> data_typing.DataModality:",
            "        \"\"\"Return the data modality enum corresponding to the class. Here, ``EVENT``.",
            "",
            "        Returns:",
            "            data_typing.DataModality: The data modality enum. Here, ``EVENT``.",
            "        \"\"\"",
            "        return data_typing.DataModality.EVENT",
            "",
            "    @abc.abstractmethod",
            "    def split(self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX) -> pd.DataFrame:",
            "        \"\"\"Return a `pandas.DataFrame` where the time component of each event feature has been split off to its own",
            "        column. The new columns that contain the times will be named ``\"<original column name><time_feature_suffix>\"``",
            "        and will be inserted before each corresponding ``<original column name>`` column. The ``<original column name>``",
            "        columns will contain only the event value.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            pd.DataFrame: The output dataframe.",
            "        \"\"\"",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def split_as_two_dataframes(",
            "        self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX",
            "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:",
            "        \"\"\"Analogous to :func:`~tempor.data.samples.EventSamples.split` but returns two `pandas.DataFrame` s:",
            "            - first dataframe contains the event times of each feature.",
            "            - second dataframe contains the event values (`True`/`False`) of each feature.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            Tuple[pd.DataFrame, pd.DataFrame]: Two `pandas.DataFrame` s containing event times and values respectively.",
            "        \"\"\"",
            "        ...",
            "",
            "",
            "def _array_default_sample_index(array: np.ndarray) -> List[int]:",
            "    n_samples, *_ = array.shape",
            "    return list(range(0, n_samples))",
            "",
            "",
            "def _array_default_feature_index(array: np.ndarray) -> List[str]:",
            "    *_, n_features = array.shape",
            "    return [f\"feat_{x}\" for x in range(0, n_features)]",
            "",
            "",
            "def _array_default_time_indexes(array: np.ndarray, padding_indicator: Any) -> List[List[int]]:",
            "    lengths = utils.get_seq_lengths_timeseries_array3d(array, padding_indicator)",
            "    return [list(range(x)) for x in lengths]",
            "",
            "",
            "plugins.register_plugin_category(\"static_samples\", StaticSamplesBase, plugin_type=\"dataformat\")",
            "plugins.register_plugin_category(\"time_series_samples\", TimeSeriesSamplesBase, plugin_type=\"dataformat\")",
            "plugins.register_plugin_category(\"event_samples\", EventSamplesBase, plugin_type=\"dataformat\")",
            "",
            "",
            "@plugins.register_plugin(name=\"static_samples_df\", category=\"static_samples\", plugin_type=\"dataformat\")",
            "class StaticSamples(StaticSamplesBase):",
            "    _data: pd.DataFrame",
            "    _schema: pa.DataFrameSchema",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create a :class:`StaticSamples` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List with sample (row) index for each sample. Optional,",
            "                if `None`, will be of form ``[0, 1, ...]``. Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`.  List with feature (column) index for each feature.",
            "                Optional, if `None`, will be of form ``[\"feat_0\", \"feat_1\", ...]``. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        if isinstance(data, pd.DataFrame):",
            "            self._data = data",
            "        elif isinstance(data, np.ndarray):",
            "            self._data = self._array_to_df(data, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        schema = pandera_utils.init_schema(self._data, coerce=False)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(schema, pa.DataFrameSchema)  # nosec B101",
            "        logger.debug(f\"Inferred schema:\\n{schema}\")",
            "",
            "        # DataFrame-level validation:",
            "        schema = pandera_utils.add_df_checks(",
            "            schema,",
            "            checks_list=[",
            "                pandera_utils.checks.forbid_multiindex_index,",
            "                pandera_utils.checks.forbid_multiindex_columns,",
            "                pandera_utils.checks.configurable.column_index_satisfies_dtype(",
            "                    pandera_utils.UnionDtype[DATA_SETTINGS.feature_index_dtypes],  # type: ignore",
            "                    nullable=DATA_SETTINGS.feature_index_nullable,",
            "                ),",
            "            ],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Values validation:",
            "        schema = pandera_utils.add_regex_column_checks(",
            "            schema,",
            "            regex=\".*\",",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.static_value_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.static_values_nullable,",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Index validation:",
            "        schema, data = pandera_utils.set_up_index(",
            "            schema,",
            "            self._data,",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.sample_index_dtypes],  # type: ignore",
            "            name=DATA_SETTINGS.sample_index_name,",
            "            nullable=DATA_SETTINGS.sample_index_nullable,",
            "            coerce=False,",
            "            unique=DATA_SETTINGS.sample_index_unique,",
            "        )",
            "        self._data = schema.validate(data)",
            "",
            "        logger.debug(f\"Final schema:\\n{schema}\")",
            "        self._schema = schema",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`StaticSamples` from `pandas.DataFrame`. The rows represent samples, the columns represent",
            "        features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that represents the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            StaticSamples: :class:`StaticSamples` object from ``dataframe``.",
            "        \"\"\"",
            "        return StaticSamples(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`StaticSamples` from `numpy.ndarray`. The 0th dimension represents samples, the 1st dimension",
            "        represents features.",
            "",
            "        Args:",
            "            array (np.ndarray): The array with the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional): Sample indices to assign. Defaults to None.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional): Feature indices to assign. Defaults to None.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            StaticSamples: :class:`StaticSamples` object created from the ``array``.",
            "        \"\"\"",
            "        return StaticSamples(array, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "",
            "    @staticmethod",
            "    def _array_to_df(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> pd.DataFrame:",
            "        if sample_index is None:",
            "            sample_index = _array_default_sample_index(array)  # pyright: ignore",
            "        if feature_index is None:",
            "            feature_index = _array_default_feature_index(array)",
            "        return pd.DataFrame(data=array, index=sample_index, columns=feature_index, **kwargs)",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return self._data.to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The dataframe.",
            "        \"\"\"",
            "        return self._data",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index)  # pyright: ignore",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0]",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`StaticSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`StaticSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return StaticSamples(  # type: ignore [return-value]",
            "            self._data.iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "@contextlib.contextmanager",
            "def workaround_pandera_pd2_1_0_multiindex_compatibility(schema: pa.DataFrameSchema, data: pd.DataFrame) -> Generator:",
            "    \"\"\"A version compatibility issue exists between pandera and pandas 2.1.0, as reported here:",
            "    https://github.com/unionai-oss/pandera/issues/1328",
            "",
            "    The error pertains to multiindex uniqueness validation giving an unexpected error.",
            "",
            "    This is a workaround that will \"manually\" throw an error that is expected from pandera.",
            "    \"\"\"",
            "",
            "    def problem_versions() -> bool:  # pragma: no cover",
            "        return Version(pd.__version__) >= Version(\"2.1.0\")",
            "        # TODO: When/if fixed in pandera, add the below condition:",
            "        # and Version(pa.__version__) < Version(\"0.XX.YY\")",
            "",
            "    try:",
            "        yield",
            "",
            "    except ValueError as ex:",
            "        if problem_versions() and (",
            "            \"Columns with duplicate values are not supported in stack\" in str(ex)",
            "        ):  # pragma: no cover",
            "            cols = data.index.names",
            "            raise pa.errors.SchemaError(  # type: ignore [no-untyped-call]",
            "                schema=schema,",
            "                data=data,",
            "                message=f\"columns {cols} not unique\",",
            "            )",
            "        else:  # pragma: no cover",
            "            raise",
            "",
            "    finally:",
            "        pass",
            "",
            "",
            "@plugins.register_plugin(name=\"time_series_samples_df\", category=\"time_series_samples\", plugin_type=\"dataformat\")",
            "class TimeSeriesSamples(TimeSeriesSamplesBase):",
            "    _data: pd.DataFrame",
            "    _schema: pa.DataFrameSchema",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        *,",
            "        padding_indicator: Any = None,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        time_indexes: Optional[data_typing.TimeIndexList] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create a :class:`TimeSeriesSamples` object from the ``data``.",
            "",
            "        If ``data`` is a `pandas.DataFrame`, this should be a 2-level multiindex (sample, timestep) dataframe.",
            "",
            "        If ``data`` is a `numpy.ndarray`, this should be a 3D array, with dimensions ``(sample, timestep, feature)``.",
            "        Optionally, padding values of ``padding_indicator`` can be set inside the array to pad out the length of arrays",
            "        of different samples in case they differ. Padding needs to go at the end of the timesteps (dim 1). Padding must",
            "        be the same across the feature dimension (dim 2) for each sample.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            padding_indicator (Any, optional):",
            "                Padding indicator used in ``data`` to indicate padding. Defaults to `None`.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List with sample (row) index for each sample.",
            "                Optional, if `None`, will be of form ``[0, 1, ...]``. Defaults to `None`.",
            "            time_indexes (Optional[data_typing.TimeIndexList], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List of lists containing timesteps for each sample (outer",
            "                list should be the same length as dim 0 of `data`, inner list should contain as many elements as each",
            "                sample has timesteps). Optional, if `None`, will be of form ``[[0, 1, ...], [0, 1, ...], ...]``",
            "                Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`.  List with feature (column) index for each feature.",
            "                Optional, if `None`, will be of form ``[\"feat_0\", \"feat_1\", ...]``.",
            "                Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        if isinstance(data, pd.DataFrame):",
            "            self._data = data",
            "        elif isinstance(data, np.ndarray):",
            "            self._data = self._array_to_df(",
            "                data,",
            "                padding_indicator=padding_indicator,",
            "                sample_index=sample_index,",
            "                time_indexes=time_indexes,",
            "                feature_index=feature_index,",
            "                **kwargs,",
            "            )",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        schema = pandera_utils.init_schema(self._data, coerce=False)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(schema, pa.DataFrameSchema)  # nosec B101",
            "        logger.debug(f\"Inferred schema:\\n{schema}\")",
            "",
            "        # DataFrame-level validation:",
            "        schema = pandera_utils.add_df_checks(",
            "            schema,",
            "            checks_list=[",
            "                pandera_utils.checks.forbid_multiindex_columns,",
            "                pandera_utils.checks.require_2level_multiindex_index,",
            "                pandera_utils.checks.configurable.column_index_satisfies_dtype(",
            "                    pandera_utils.UnionDtype[DATA_SETTINGS.feature_index_dtypes],  # type: ignore",
            "                    nullable=DATA_SETTINGS.feature_index_nullable,",
            "                ),",
            "            ],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Values validation:",
            "        schema = pandera_utils.add_regex_column_checks(",
            "            schema,",
            "            regex=\".*\",",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.time_series_value_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.time_series_values_nullable,",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Index validation:",
            "        if not (DATA_SETTINGS.sample_index_unique and DATA_SETTINGS.sample_timestep_index_unique):",
            "            raise NotImplementedError(\"Only supported case: unique sample and unique timestep indexes\")",
            "        multiindex_unique_def = (DATA_SETTINGS.sample_index_name, DATA_SETTINGS.time_index_name)",
            "        schema, data = pandera_utils.set_up_2level_multiindex(",
            "            schema,",
            "            self._data,",
            "            dtypes=(",
            "                pandera_utils.UnionDtype[DATA_SETTINGS.sample_index_dtypes],  # type: ignore",
            "                pandera_utils.UnionDtype[DATA_SETTINGS.time_index_dtypes],  # type: ignore",
            "            ),",
            "            names=(DATA_SETTINGS.sample_index_name, DATA_SETTINGS.time_index_name),",
            "            nullable=(DATA_SETTINGS.sample_index_nullable, DATA_SETTINGS.time_index_nullable),",
            "            coerce=False,",
            "            unique=multiindex_unique_def,",
            "        )",
            "        with workaround_pandera_pd2_1_0_multiindex_compatibility(schema, data):",
            "            self._data = schema.validate(data)",
            "",
            "        logger.debug(f\"Final schema:\\n{schema}\")",
            "        self._schema = schema",
            "",
            "        # TODO:",
            "        # Possible additional validation checks:",
            "        # - Ensure time index sorted ascending within each sample.",
            "        # - Time index float / int expected non-negative values.",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`TimeSeriesSamples` from `pandas.DataFrame`. This row index of the dataframe should be a",
            "        2-level multiindex (sample, timestep). The columns should be the features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            TimeSeriesSamples: The :class:`TimeSeriesSamples` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return TimeSeriesSamples(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        padding_indicator: Any = None,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        time_indexes: Optional[data_typing.TimeIndexList] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`TimeSeriesSamples` from `numpy.ndarray`.",
            "",
            "        This should be a 3D array, with dimensions ``(sample, timestep, feature)``.",
            "",
            "        Optionally, padding values of ``padding_indicator`` can be set inside the array to pad out the length of arrays",
            "        of different samples in case they differ. Padding needs to go at the end of the timesteps (dim 1). Padding must",
            "        be the same across the feature dimension (dim 2) for each sample.",
            "",
            "        Args:",
            "            array (np.ndarray):",
            "                The array that contains the data.",
            "            padding_indicator (Any, optional):",
            "                The padding indicator value. Defaults to `None`.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Sample indexes as a list. Defaults to `None`.",
            "            time_indexes (Optional[data_typing.TimeIndexList], optional):",
            "                Time indexes as a list of list (that is, time indexes per sample). Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Feature indexes as a list. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments.",
            "",
            "        Returns:",
            "            TimeSeriesSamples: The :class:`TimeSeriesSamples` object created from the ``array``.",
            "        \"\"\"",
            "        return TimeSeriesSamples(",
            "            array,",
            "            padding_indicator=padding_indicator,",
            "            sample_index=sample_index,",
            "            time_indexes=time_indexes,",
            "            feature_index=feature_index,",
            "            **kwargs,",
            "        )",
            "",
            "    @staticmethod",
            "    def _array_to_df(  # pylint: disable=unused-argument",
            "        array: np.ndarray,",
            "        *,",
            "        padding_indicator: Any,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        time_indexes: Optional[data_typing.TimeIndexList] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> pd.DataFrame:",
            "        if sample_index is None:",
            "            sample_index = _array_default_sample_index(array)  # pyright: ignore",
            "        if feature_index is None:",
            "            feature_index = _array_default_feature_index(array)",
            "        if time_indexes is None:",
            "            time_indexes = _array_default_time_indexes(array, padding_indicator)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert sample_index is not None and feature_index is not None and time_indexes is not None  # nosec B101",
            "        return utils.array3d_to_multiindex_timeseries_dataframe(",
            "            array,",
            "            sample_index=sample_index,",
            "            time_indexes=time_indexes,",
            "            feature_index=feature_index,",
            "            padding_indicator=padding_indicator,",
            "        )",
            "",
            "    def numpy(self, *, padding_indicator: Any = DATA_SETTINGS.default_padding_indicator, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            padding_indicator (Any, optional):",
            "                Padding indicator value. Defaults to `DATA_SETTINGS.default_padding_indicator`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_array3d(",
            "            df=self._data, padding_indicator=padding_indicator, max_timesteps=None",
            "        )",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return self._data",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Get a list containing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: A list containing sample indexes.",
            "        \"\"\"",
            "        return list(utils.get_df_index_level0_unique(self._data))  # pyright: ignore",
            "",
            "    def time_indexes(self) -> data_typing.TimeIndexList:",
            "        \"\"\"Get a list containing time indexes for each sample. Each time index is represented as a list of time step",
            "        elements.",
            "",
            "        Returns:",
            "            data_typing.TimeIndexList: A list containing time indexes for each sample.",
            "        \"\"\"",
            "        return list(self.time_indexes_as_dict().values())  # pyright: ignore",
            "",
            "    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its time index. Time index is represented as a list of time",
            "        step elements.",
            "",
            "        Returns:",
            "            data_typing.SampleToTimeIndexDict: The dictionary mapping each sample index to its time index.",
            "        \"\"\"",
            "        multiindex = self._data.index",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(multiindex, pd.MultiIndex)  # nosec B101",
            "        sample_index = self.sample_index()",
            "        d = dict()",
            "        for s in sample_index:",
            "            time_index_locs = multiindex.get_locs([s, slice(None)])",
            "            d[s] = list(multiindex.get_level_values(1)[time_index_locs])",
            "        return d  # type: ignore[return-value]",
            "",
            "    def time_indexes_float(self) -> List[np.ndarray]:",
            "        \"\"\"Return time indexes but converting their elements to `float` values.",
            "",
            "        Date-time time index will be converted using :obj:`~tempor.data.utils.datetime_time_index_to_float`.",
            "",
            "        Returns:",
            "            List[np.ndarray]: List of 1D `numpy.ndarray` s of `float` values, corresponding to the time index.",
            "        \"\"\"",
            "        return [utils.datetime_time_index_to_float(ti) for ti in self.time_indexes()]",
            "",
            "    def num_timesteps(self) -> List[int]:",
            "        \"\"\"Get the number of timesteps for each sample.",
            "",
            "        Returns:",
            "            List[int]: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return [len(x) for x in self.time_indexes()]",
            "",
            "    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its the number of timesteps.",
            "",
            "        Returns:",
            "            data_typing.SampleToNumTimestepsDict: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return {key: len(x) for key, x in self.time_indexes_as_dict().items()}  # type: ignore",
            "",
            "    def num_timesteps_equal(self) -> bool:",
            "        \"\"\"Returns `True` if all samples share the same number of timesteps, `False` otherwise.",
            "",
            "        Returns:",
            "            bool: whether all samples share the same number of timesteps.",
            "        \"\"\"",
            "        timesteps = self.num_timesteps()",
            "        return True if len(timesteps) == 0 else all([x == timesteps[0] for x in timesteps])",
            "",
            "    def list_of_dataframes(self) -> List[pd.DataFrame]:",
            "        \"\"\"Returns a list of dataframes where each dataframe has the data for each sample.",
            "",
            "        Returns:",
            "            List[pd.DataFrame]: List of dataframes for each sample.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_list_of_dataframes(self._data)",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        sample_ids = utils.get_df_index_level0_unique(self._data)",
            "        return len(sample_ids)",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, *, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a subset :class:`TimeSeriesSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`TimeSeriesSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        sample_index = utils.get_df_index_level0_unique(self._data)",
            "        selected = list(sample_index[key_])  # pyright: ignore",
            "        return TimeSeriesSamples(  # type: ignore [return-value]",
            "            self._data.loc[(selected, slice(None)), :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "@plugins.register_plugin(name=\"event_samples_df\", category=\"event_samples\", plugin_type=\"dataformat\")",
            "class EventSamples(EventSamplesBase):",
            "    _data: pd.DataFrame",
            "    _schema: pa.DataFrameSchema",
            "    _schema_split: pa.DataFrameSchema",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create an :class:`EventSamples` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`. List with sample (row) index for each sample. Optional,",
            "                if `None`, will be of form ``[0, 1, ...]``. Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional):",
            "                Used only if ``data`` is a `numpy.ndarray`.  List with feature (column) index for each feature.",
            "                Optional, if `None`, will be of form ``[\"feat_0\", \"feat_1\", ...]``. Defaults to `None`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        if isinstance(data, pd.DataFrame):",
            "            self._data = data",
            "        elif isinstance(data, np.ndarray):",
            "            self._data = self._array_to_df(data, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        schema = pandera_utils.init_schema(self._data, coerce=False)",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(schema, pa.DataFrameSchema)  # nosec B101",
            "        logger.debug(f\"Inferred schema:\\n{schema}\")",
            "",
            "        # DataFrame-level validation:",
            "        schema = pandera_utils.add_df_checks(",
            "            schema,",
            "            checks_list=[",
            "                pandera_utils.checks.forbid_multiindex_index,",
            "                pandera_utils.checks.forbid_multiindex_columns,",
            "                pandera_utils.checks.configurable.column_index_satisfies_dtype(",
            "                    pandera_utils.UnionDtype[DATA_SETTINGS.feature_index_dtypes],  # type: ignore",
            "                    nullable=DATA_SETTINGS.feature_index_nullable,",
            "                ),",
            "            ],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "",
            "        # Values validation:",
            "        schema = pandera_utils.add_regex_column_checks(",
            "            schema,",
            "            regex=\".*\",",
            "            dtype=None,",
            "            nullable=DATA_SETTINGS.event_values_nullable,",
            "            checks_list=[pandera_utils.checks.require_element_len_2],",
            "        )",
            "        self._data = schema.validate(self._data)",
            "        # Validate event time and value components:",
            "        suffix = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX",
            "        data_split = self.split(time_feature_suffix=suffix)",
            "        schema_split = pandera_utils.init_schema(data_split, coerce=False)",
            "        schema_split = pandera_utils.add_regex_column_checks(",
            "            schema_split,",
            "            regex=f\".*{suffix}$\",  # Event time columns, end in \"_time\".",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.time_index_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.time_index_nullable,",
            "        )",
            "        schema_split = pandera_utils.add_regex_column_checks(",
            "            schema_split,",
            "            regex=f\"^((?!{suffix}$).)*$\",  # Event value columns, do not end in \"_time\".",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.event_value_dtypes],  # type: ignore",
            "            nullable=DATA_SETTINGS.event_values_nullable,",
            "        )",
            "        logger.debug(f\"Time split-off schema (checks event time and values separately):\\n{schema_split}\")",
            "        schema_split.validate(data_split)",
            "        self._schema_split = schema_split",
            "",
            "        # Index validation:",
            "        schema, data = pandera_utils.set_up_index(",
            "            schema,",
            "            self._data,",
            "            dtype=pandera_utils.UnionDtype[DATA_SETTINGS.sample_index_dtypes],  # type: ignore",
            "            name=DATA_SETTINGS.sample_index_name,",
            "            nullable=DATA_SETTINGS.sample_index_nullable,",
            "            coerce=False,",
            "            unique=DATA_SETTINGS.sample_index_unique,",
            "        )",
            "        self._data = schema.validate(data)",
            "",
            "        logger.debug(f\"Final schema:\\n{schema}\")",
            "        self._schema = schema",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`EventSamples` from `pandas.DataFrame`. The row index of the dataframe should be the sample",
            "        indexes. The columns should be the features. Each feature should contain a tuple of ``(time, value)``",
            "        representing the event.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            EventSamples: The :class:`EventSamples` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return EventSamples(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamples\":  # pyright: ignore",
            "        \"\"\"Create :class:`EventSamples` from `numpy.ndarray`. The array should be a 2D array, with dimensions",
            "        ``(sample, feature)``. Each element should contain a tuple of ``(time, value)`` representing the event.",
            "",
            "        Args:",
            "            array (np.ndarray): The array that contains the data.",
            "            sample_index (Optional[data_typing.SampleIndex], optional): Sample indexes. Defaults to `None`.",
            "            feature_index (Optional[data_typing.FeatureIndex], optional): Feature index. Defaults to `None`.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            EventSamples: The :class:`EventSamples` object created from the ``array``.",
            "        \"\"\"",
            "        return EventSamples(array, sample_index=sample_index, feature_index=feature_index, **kwargs)",
            "",
            "    @staticmethod",
            "    def _array_to_df(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> pd.DataFrame:",
            "        if sample_index is None:",
            "            sample_index = _array_default_sample_index(array)  # pyright: ignore",
            "        if feature_index is None:",
            "            feature_index = _array_default_feature_index(array)",
            "        return pd.DataFrame(data=array, index=sample_index, columns=feature_index, **kwargs)",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        # TODO: May want at option to return a scikit-survive -style array.",
            "        return self._data.to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return self._data",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index)  # pyright: ignore",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0]",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    @pydantic_utils.validate_arguments(config=pydantic.ConfigDict(arbitrary_types_allowed=True))",
            "    def split(self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX) -> pd.DataFrame:",
            "        \"\"\"Return a `pandas.DataFrame` where the time component of each event feature has been split off to its own",
            "        column. The new columns that contain the times will be named ``\"<original column name><time_feature_suffix>\"``",
            "        and will be inserted before each corresponding ``<original column name>`` column. The ``<original column name>``",
            "        columns will contain only the event value.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            pd.DataFrame: The output dataframe.",
            "        \"\"\"",
            "        df = self._data.copy()",
            "        features = list(df.columns)",
            "        if any(time_feature_suffix in str(c) for c in features):",
            "            raise ValueError(f\"Column names must not contain '{time_feature_suffix}'\")",
            "        for f_idx, f in enumerate(features):",
            "            df.insert(f_idx * 2, f\"{f}{time_feature_suffix}\", df[f].apply(lambda x: x[0]))",
            "        for f in features:",
            "            df[f] = df[f].apply(lambda x: x[1])",
            "        return df",
            "",
            "    def split_as_two_dataframes(",
            "        self, time_feature_suffix: str = _DEFAULT_EVENTS_TIME_FEATURE_SUFFIX",
            "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:",
            "        \"\"\"Analogous to :func:`~tempor.data.samples.EventSamples.split` but returns two `pandas.DataFrame` s:",
            "            - first dataframe contains the event times of each feature.",
            "            - second dataframe contains the event values (`True`/`False`) of each feature.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            Tuple[pd.DataFrame, pd.DataFrame]: Two `pandas.DataFrame` s containing event times and values respectively.",
            "        \"\"\"",
            "        df_split = self.split(time_feature_suffix=time_feature_suffix)",
            "        df_event_times = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix in c]]",
            "        df_event_values = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix not in c]]",
            "        return df_event_times, df_event_values",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`EventSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`EventSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return EventSamples(  # type: ignore [return-value]",
            "            self._data.iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "216": [
                "TimeSeriesSamplesBase",
                "time_indexes"
            ],
            "226": [
                "TimeSeriesSamplesBase",
                "time_indexes_as_dict"
            ],
            "236": [
                "TimeSeriesSamplesBase",
                "time_indexes_float"
            ],
            "247": [
                "TimeSeriesSamplesBase",
                "num_timesteps"
            ],
            "256": [
                "TimeSeriesSamplesBase",
                "num_timesteps_as_dict"
            ],
            "265": [
                "TimeSeriesSamplesBase",
                "num_timesteps_equal"
            ],
            "274": [
                "TimeSeriesSamplesBase",
                "list_of_dataframes"
            ]
        },
        "addLocation": []
    },
    "src/tempor/data/samples_experimental.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "         sample_index: Optional[data_typing.SampleIndex] = None,"
            },
            "1": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "         feature_index: Optional[data_typing.FeatureIndex] = None,"
            },
            "2": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 94,
                "PatchRowcode": "         **kwargs: Any,"
            },
            "3": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> \"StaticSamplesDask\":  # pyright: ignore  # noqa: D102"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+    ) -> \"StaticSamplesDask\":  # pyright: ignore"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+        \"\"\"Not implemented yet.\"\"\""
            },
            "6": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "         raise NotImplementedError(\"`StaticSamplesDask` does not support `numpy.ndarray` input yet.\")"
            },
            "7": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 98,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     def numpy(self, **kwargs: Any) -> np.ndarray:"
            },
            "9": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "     def from_numpy("
            },
            "10": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "         array: np.ndarray,"
            },
            "11": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "         **kwargs: Any,"
            },
            "12": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> \"TimeSeriesSamplesDask\":  # pyright: ignore  # noqa: D102"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+    ) -> \"TimeSeriesSamplesDask\":  # pyright: ignore"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+        \"\"\"Not implemented yet.\"\"\""
            },
            "15": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 249,
                "PatchRowcode": "         raise NotImplementedError(\"`TimeSeriesSamples` does not support `numpy.ndarray` input yet.\")"
            },
            "16": {
                "beforePatchRowNumber": 248,
                "afterPatchRowNumber": 250,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 249,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "     def numpy(self, *, padding_indicator: Any = DATA_SETTINGS.default_padding_indicator, **kwargs: Any) -> np.ndarray:"
            },
            "18": {
                "beforePatchRowNumber": 455,
                "afterPatchRowNumber": 457,
                "PatchRowcode": "     def from_numpy("
            },
            "19": {
                "beforePatchRowNumber": 456,
                "afterPatchRowNumber": 458,
                "PatchRowcode": "         array: np.ndarray,"
            },
            "20": {
                "beforePatchRowNumber": 457,
                "afterPatchRowNumber": 459,
                "PatchRowcode": "         **kwargs: Any,"
            },
            "21": {
                "beforePatchRowNumber": 458,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ) -> \"EventSamplesDask\":  # pyright: ignore  # noqa: D102"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 460,
                "PatchRowcode": "+    ) -> \"EventSamplesDask\":  # pyright: ignore"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 461,
                "PatchRowcode": "+        \"\"\"Not implemented yet.\"\"\""
            },
            "24": {
                "beforePatchRowNumber": 459,
                "afterPatchRowNumber": 462,
                "PatchRowcode": "         raise NotImplementedError(\"`EventSamplesDask` does not support `numpy.ndarray` input yet.\")"
            },
            "25": {
                "beforePatchRowNumber": 460,
                "afterPatchRowNumber": 463,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 461,
                "afterPatchRowNumber": 464,
                "PatchRowcode": "     def numpy(self, **kwargs: Any) -> np.ndarray:"
            }
        },
        "frontPatchFile": [
            "\"\"\"Module with experimental samples implementations.\"\"\"",
            "",
            "# pyright: reportPrivateImportUsage=false",
            "",
            "from typing import TYPE_CHECKING, Any, List, Optional, Tuple",
            "",
            "import dask.dataframe as dd",
            "import numpy as np",
            "import pandas as pd",
            "from typing_extensions import Self",
            "",
            "from tempor.core import plugins",
            "from tempor.log import logger",
            "",
            "from . import data_typing, samples, utils",
            "from .settings import DATA_SETTINGS",
            "",
            "# NOTE: Dask samples implementations are a work in progress and do not yet fully leverage Dask capabilities.",
            "",
            "",
            "def _process_npartitions_chunksize(**kwargs: Any) -> Tuple[Optional[int], Optional[int], Any]:",
            "    if \"npartitions\" in kwargs:",
            "        npartitions = kwargs.pop(\"npartitions\")",
            "        chunksize = None",
            "    elif \"chunksize\" in kwargs:",
            "        chunksize = kwargs.pop(\"chunksize\")",
            "        npartitions = None",
            "    else:",
            "        npartitions = 1",
            "        chunksize = None",
            "    return npartitions, chunksize, kwargs",
            "",
            "",
            "@plugins.register_plugin(name=\"static_samples_dask\", category=\"static_samples\", plugin_type=\"dataformat\")",
            "class StaticSamplesDask(samples.StaticSamplesBase):",
            "    _data: dd.DataFrame  # type: ignore[name-defined]",
            "    # _schema: pa.DataFrameSchema",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create a :class:`StaticSamplesDask` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        npartitions, chunksize, kwargs = _process_npartitions_chunksize(**kwargs)",
            "        if isinstance(data, dd.DataFrame):  # type: ignore[attr-defined]",
            "            self._data = data",
            "        elif isinstance(data, pd.DataFrame):",
            "            self._data = dd.from_pandas(  # type: ignore[attr-defined]",
            "                data,",
            "                npartitions=npartitions,",
            "                chunksize=chunksize,",
            "            )",
            "        elif isinstance(data, np.ndarray):",
            "            raise NotImplementedError(\"`StaticSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        # TODO: Validation analogous to the DF implementation.",
            "        logger.info(\"Validation not yet implemented for Dask data format. Data format consistency is not guaranteed.\")",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamplesDask\":  # pyright: ignore",
            "        \"\"\"Create :class:`StaticSamplesDask` from `pandas.DataFrame`. The rows represent samples, the columns represent",
            "        features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that represents the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            StaticSamplesDask: :class:`StaticSamples` object from ``dataframe``.",
            "        \"\"\"",
            "        return StaticSamplesDask(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamplesDask\":  # pyright: ignore  # noqa: D102",
            "        raise NotImplementedError(\"`StaticSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return self._data.compute().to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The dataframe.",
            "        \"\"\"",
            "        return self._data.compute()",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index.compute())",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0].compute()",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`StaticSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`StaticSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return StaticSamplesDask(  # type: ignore [return-value]",
            "            self._data.compute().iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "def multiindex_df_to_compatible_ddf(",
            "    df: pd.DataFrame,",
            "    **kwargs: Any,",
            ") -> dd.DataFrame:  # type: ignore[name-defined]",
            "    \"\"\"Convert a multiindex dataframe to a dask dataframe with a single tuple index.\"\"\"",
            "    compatible_df = pd.DataFrame(data=df.to_numpy(), index=df.index.to_list(), columns=df.columns.to_list())",
            "    return dd.from_pandas(  # type: ignore[attr-defined]",
            "        compatible_df,",
            "        **kwargs,",
            "    )",
            "",
            "",
            "def compatible_ddf_to_multiindex_df(",
            "    ddf: dd.DataFrame,  # type: ignore[name-defined]",
            ") -> pd.DataFrame:",
            "    \"\"\"Convert a dask dataframe with a single tuple index to a multiindex dataframe.\"\"\"",
            "    asdf = ddf.compute()",
            "    df = pd.DataFrame(data=asdf.to_numpy(), columns=asdf.columns.to_list())",
            "    df.index = pd.MultiIndex.from_tuples(asdf.index.to_list())",
            "    return df",
            "",
            "",
            "@plugins.register_plugin(name=\"time_series_samples_dask\", category=\"time_series_samples\", plugin_type=\"dataformat\")",
            "class TimeSeriesSamplesDask(samples.TimeSeriesSamplesBase):",
            "    _data: dd.DataFrame  # type: ignore[name-defined]",
            "    # _schema: pa.DataFrameSchema",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create an :class:`TimeSeriesSamplesDask` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        npartitions, chunksize, kwargs = _process_npartitions_chunksize(**kwargs)",
            "        if isinstance(data, dd.DataFrame):  # type: ignore[attr-defined]",
            "            self._data = data",
            "        elif isinstance(data, pd.DataFrame):",
            "            self._data = multiindex_df_to_compatible_ddf(data, npartitions=npartitions, chunksize=chunksize)",
            "        elif isinstance(data, np.ndarray):",
            "            raise NotImplementedError(\"`TimeSeriesSamples` does not support `numpy.ndarray` input yet.\")",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        # TODO: Validation analogous to the DF implementation.",
            "        logger.info(\"Validation not yet implemented for Dask data format. Data format consistency is not guaranteed.\")",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamplesDask\":  # pyright: ignore",
            "        \"\"\"Create :class:`TimeSeriesSamplesDask` from `pandas.DataFrame`. This row index of the dataframe should be a",
            "        2-level multiindex (sample, timestep). The columns should be the features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            TimeSeriesSamplesDask: The :class:`TimeSeriesSamples` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return TimeSeriesSamplesDask(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamplesDask\":  # pyright: ignore  # noqa: D102",
            "        raise NotImplementedError(\"`TimeSeriesSamples` does not support `numpy.ndarray` input yet.\")",
            "",
            "    def numpy(self, *, padding_indicator: Any = DATA_SETTINGS.default_padding_indicator, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            padding_indicator (Any, optional):",
            "                Padding indicator value. Defaults to `DATA_SETTINGS.default_padding_indicator`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_array3d(",
            "            df=compatible_ddf_to_multiindex_df(self._data), padding_indicator=padding_indicator, max_timesteps=None",
            "        )",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return compatible_ddf_to_multiindex_df(self._data)",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Get a list containing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: A list containing sample indexes.",
            "        \"\"\"",
            "        return list(utils.get_df_index_level0_unique(compatible_ddf_to_multiindex_df(self._data)))  # pyright: ignore",
            "",
            "    def time_indexes(self) -> data_typing.TimeIndexList:",
            "        \"\"\"Get a list containing time indexes for each sample. Each time index is represented as a list of time step",
            "        elements.",
            "",
            "        Returns:",
            "            data_typing.TimeIndexList: A list containing time indexes for each sample.",
            "        \"\"\"",
            "        return list(self.time_indexes_as_dict().values())  # pyright: ignore",
            "",
            "    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its time index. Time index is represented as a list of time",
            "        step elements.",
            "",
            "        Returns:",
            "            data_typing.SampleToTimeIndexDict: The dictionary mapping each sample index to its time index.",
            "        \"\"\"",
            "        multiindex = compatible_ddf_to_multiindex_df(self._data).index",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(multiindex, pd.MultiIndex)  # nosec B101",
            "        sample_index = self.sample_index()",
            "        d = dict()",
            "        for s in sample_index:",
            "            time_index_locs = multiindex.get_locs([s, slice(None)])",
            "            d[s] = list(multiindex.get_level_values(1)[time_index_locs])",
            "        return d  # type: ignore[return-value]",
            "",
            "    def time_indexes_float(self) -> List[np.ndarray]:",
            "        \"\"\"Return time indexes but converting their elements to `float` values.",
            "",
            "        Date-time time index will be converted using :obj:`~tempor.data.utils.datetime_time_index_to_float`.",
            "",
            "        Returns:",
            "            List[np.ndarray]: List of 1D `numpy.ndarray` s of `float` values, corresponding to the time index.",
            "        \"\"\"",
            "        return [utils.datetime_time_index_to_float(ti) for ti in self.time_indexes()]",
            "",
            "    def num_timesteps(self) -> List[int]:",
            "        \"\"\"Get the number of timesteps for each sample.",
            "",
            "        Returns:",
            "            List[int]: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return [len(x) for x in self.time_indexes()]",
            "",
            "    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its the number of timesteps.",
            "",
            "        Returns:",
            "            data_typing.SampleToNumTimestepsDict: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return {key: len(x) for key, x in self.time_indexes_as_dict().items()}  # type: ignore",
            "",
            "    def num_timesteps_equal(self) -> bool:",
            "        \"\"\"Returns `True` if all samples share the same number of timesteps, `False` otherwise.",
            "",
            "        Returns:",
            "            bool: whether all samples share the same number of timesteps.",
            "        \"\"\"",
            "        timesteps = self.num_timesteps()",
            "        return True if len(timesteps) == 0 else all([x == timesteps[0] for x in timesteps])",
            "",
            "    def list_of_dataframes(self) -> List[pd.DataFrame]:",
            "        \"\"\"Returns a list of dataframes where each dataframe has the data for each sample.",
            "",
            "        Returns:",
            "            List[pd.DataFrame]: List of dataframes for each sample.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_list_of_dataframes(compatible_ddf_to_multiindex_df(self._data))",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        sample_ids = utils.get_df_index_level0_unique(compatible_ddf_to_multiindex_df(self._data))",
            "        return len(sample_ids)",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, *, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a subset :class:`TimeSeriesSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`TimeSeriesSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        sample_index = utils.get_df_index_level0_unique(compatible_ddf_to_multiindex_df(self._data))",
            "        selected = list(sample_index[key_])  # pyright: ignore",
            "        return TimeSeriesSamplesDask(  # type: ignore [return-value]",
            "            compatible_ddf_to_multiindex_df(self._data).loc[(selected, slice(None)), :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "@plugins.register_plugin(name=\"event_samples_dask\", category=\"event_samples\", plugin_type=\"dataformat\")",
            "class EventSamplesDask(samples.EventSamplesBase):",
            "    _data: dd.DataFrame  # type: ignore[name-defined]",
            "    # _schema: pa.DataFrameSchema",
            "    # _schema_split: pa.DataFrameSchema",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create an :class:`EventSamplesDask` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        npartitions, chunksize, kwargs = _process_npartitions_chunksize(**kwargs)",
            "        if isinstance(data, dd.DataFrame):  # type: ignore[attr-defined]",
            "            self._data = data",
            "        elif isinstance(data, pd.DataFrame):",
            "            self._data = dd.from_pandas(  # type: ignore[attr-defined]",
            "                data,",
            "                npartitions=npartitions,",
            "                chunksize=chunksize,",
            "            )",
            "        elif isinstance(data, np.ndarray):",
            "            raise NotImplementedError(\"`EventSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        # TODO: Validation analogous to the DF implementation.",
            "        logger.info(\"Validation not yet implemented for Dask data format. Data format consistency is not guaranteed.\")",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamplesDask\":  # pyright: ignore",
            "        \"\"\"Create :class:`EventSamples` from `pandas.DataFrame`. The row index of the dataframe should be the sample",
            "        indexes. The columns should be the features. Each feature should contain a tuple of ``(time, value)``",
            "        representing the event.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            EventSamplesDask: The :class:`EventSamplesDask` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return EventSamplesDask(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamplesDask\":  # pyright: ignore  # noqa: D102",
            "        raise NotImplementedError(\"`EventSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return self._data.compute().to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return self._data.compute()",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index.compute())",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0].compute()",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def split(",
            "        self,",
            "        time_feature_suffix: str = samples._DEFAULT_EVENTS_TIME_FEATURE_SUFFIX,  # pylint: disable=protected-access",
            "    ) -> pd.DataFrame:",
            "        \"\"\"Return a `pandas.DataFrame` where the time component of each event feature has been split off to its own",
            "        column. The new columns that contain the times will be named ``\"<original column name><time_feature_suffix>\"``",
            "        and will be inserted before each corresponding ``<original column name>`` column. The ``<original column name>``",
            "        columns will contain only the event value.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            pd.DataFrame: The output dataframe.",
            "        \"\"\"",
            "        df = self._data.compute().copy()",
            "        features = list(df.columns)",
            "        if any(time_feature_suffix in str(c) for c in features):  # pragma: no cover",
            "            raise ValueError(f\"Column names must not contain '{time_feature_suffix}'\")",
            "        for f_idx, f in enumerate(features):",
            "            df.insert(f_idx * 2, f\"{f}{time_feature_suffix}\", df[f].apply(lambda x: x[0]))",
            "        for f in features:",
            "            df[f] = df[f].apply(lambda x: x[1])",
            "        return df",
            "",
            "    def split_as_two_dataframes(",
            "        self,",
            "        time_feature_suffix: str = samples._DEFAULT_EVENTS_TIME_FEATURE_SUFFIX,  # pylint: disable=protected-access",
            "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:",
            "        \"\"\"Analogous to :func:`~tempor.data.samples.EventSamples.split` but returns two `pandas.DataFrame` s:",
            "            - first dataframe contains the event times of each feature.",
            "            - second dataframe contains the event values (`True`/`False`) of each feature.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            Tuple[pd.DataFrame, pd.DataFrame]: Two `pandas.DataFrame` s containing event times and values respectively.",
            "        \"\"\"",
            "        df_split = self.split(time_feature_suffix=time_feature_suffix)",
            "        df_event_times = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix in c]]",
            "        df_event_values = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix not in c]]",
            "        return df_event_times, df_event_values",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`EventSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`EventSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return EventSamplesDask(  # type: ignore [return-value]",
            "            self._data.compute().iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )"
        ],
        "afterPatchFile": [
            "\"\"\"Module with experimental samples implementations.\"\"\"",
            "",
            "# pyright: reportPrivateImportUsage=false",
            "",
            "from typing import TYPE_CHECKING, Any, List, Optional, Tuple",
            "",
            "import dask.dataframe as dd",
            "import numpy as np",
            "import pandas as pd",
            "from typing_extensions import Self",
            "",
            "from tempor.core import plugins",
            "from tempor.log import logger",
            "",
            "from . import data_typing, samples, utils",
            "from .settings import DATA_SETTINGS",
            "",
            "# NOTE: Dask samples implementations are a work in progress and do not yet fully leverage Dask capabilities.",
            "",
            "",
            "def _process_npartitions_chunksize(**kwargs: Any) -> Tuple[Optional[int], Optional[int], Any]:",
            "    if \"npartitions\" in kwargs:",
            "        npartitions = kwargs.pop(\"npartitions\")",
            "        chunksize = None",
            "    elif \"chunksize\" in kwargs:",
            "        chunksize = kwargs.pop(\"chunksize\")",
            "        npartitions = None",
            "    else:",
            "        npartitions = 1",
            "        chunksize = None",
            "    return npartitions, chunksize, kwargs",
            "",
            "",
            "@plugins.register_plugin(name=\"static_samples_dask\", category=\"static_samples\", plugin_type=\"dataformat\")",
            "class StaticSamplesDask(samples.StaticSamplesBase):",
            "    _data: dd.DataFrame  # type: ignore[name-defined]",
            "    # _schema: pa.DataFrameSchema",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create a :class:`StaticSamplesDask` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        npartitions, chunksize, kwargs = _process_npartitions_chunksize(**kwargs)",
            "        if isinstance(data, dd.DataFrame):  # type: ignore[attr-defined]",
            "            self._data = data",
            "        elif isinstance(data, pd.DataFrame):",
            "            self._data = dd.from_pandas(  # type: ignore[attr-defined]",
            "                data,",
            "                npartitions=npartitions,",
            "                chunksize=chunksize,",
            "            )",
            "        elif isinstance(data, np.ndarray):",
            "            raise NotImplementedError(\"`StaticSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        # TODO: Validation analogous to the DF implementation.",
            "        logger.info(\"Validation not yet implemented for Dask data format. Data format consistency is not guaranteed.\")",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamplesDask\":  # pyright: ignore",
            "        \"\"\"Create :class:`StaticSamplesDask` from `pandas.DataFrame`. The rows represent samples, the columns represent",
            "        features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that represents the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            StaticSamplesDask: :class:`StaticSamples` object from ``dataframe``.",
            "        \"\"\"",
            "        return StaticSamplesDask(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        *,",
            "        sample_index: Optional[data_typing.SampleIndex] = None,",
            "        feature_index: Optional[data_typing.FeatureIndex] = None,",
            "        **kwargs: Any,",
            "    ) -> \"StaticSamplesDask\":  # pyright: ignore",
            "        \"\"\"Not implemented yet.\"\"\"",
            "        raise NotImplementedError(\"`StaticSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return self._data.compute().to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The dataframe.",
            "        \"\"\"",
            "        return self._data.compute()",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index.compute())",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0].compute()",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`StaticSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`StaticSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return StaticSamplesDask(  # type: ignore [return-value]",
            "            self._data.compute().iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "def multiindex_df_to_compatible_ddf(",
            "    df: pd.DataFrame,",
            "    **kwargs: Any,",
            ") -> dd.DataFrame:  # type: ignore[name-defined]",
            "    \"\"\"Convert a multiindex dataframe to a dask dataframe with a single tuple index.\"\"\"",
            "    compatible_df = pd.DataFrame(data=df.to_numpy(), index=df.index.to_list(), columns=df.columns.to_list())",
            "    return dd.from_pandas(  # type: ignore[attr-defined]",
            "        compatible_df,",
            "        **kwargs,",
            "    )",
            "",
            "",
            "def compatible_ddf_to_multiindex_df(",
            "    ddf: dd.DataFrame,  # type: ignore[name-defined]",
            ") -> pd.DataFrame:",
            "    \"\"\"Convert a dask dataframe with a single tuple index to a multiindex dataframe.\"\"\"",
            "    asdf = ddf.compute()",
            "    df = pd.DataFrame(data=asdf.to_numpy(), columns=asdf.columns.to_list())",
            "    df.index = pd.MultiIndex.from_tuples(asdf.index.to_list())",
            "    return df",
            "",
            "",
            "@plugins.register_plugin(name=\"time_series_samples_dask\", category=\"time_series_samples\", plugin_type=\"dataformat\")",
            "class TimeSeriesSamplesDask(samples.TimeSeriesSamplesBase):",
            "    _data: dd.DataFrame  # type: ignore[name-defined]",
            "    # _schema: pa.DataFrameSchema",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create an :class:`TimeSeriesSamplesDask` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        npartitions, chunksize, kwargs = _process_npartitions_chunksize(**kwargs)",
            "        if isinstance(data, dd.DataFrame):  # type: ignore[attr-defined]",
            "            self._data = data",
            "        elif isinstance(data, pd.DataFrame):",
            "            self._data = multiindex_df_to_compatible_ddf(data, npartitions=npartitions, chunksize=chunksize)",
            "        elif isinstance(data, np.ndarray):",
            "            raise NotImplementedError(\"`TimeSeriesSamples` does not support `numpy.ndarray` input yet.\")",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        # TODO: Validation analogous to the DF implementation.",
            "        logger.info(\"Validation not yet implemented for Dask data format. Data format consistency is not guaranteed.\")",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamplesDask\":  # pyright: ignore",
            "        \"\"\"Create :class:`TimeSeriesSamplesDask` from `pandas.DataFrame`. This row index of the dataframe should be a",
            "        2-level multiindex (sample, timestep). The columns should be the features.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            TimeSeriesSamplesDask: The :class:`TimeSeriesSamples` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return TimeSeriesSamplesDask(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        **kwargs: Any,",
            "    ) -> \"TimeSeriesSamplesDask\":  # pyright: ignore",
            "        \"\"\"Not implemented yet.\"\"\"",
            "        raise NotImplementedError(\"`TimeSeriesSamples` does not support `numpy.ndarray` input yet.\")",
            "",
            "    def numpy(self, *, padding_indicator: Any = DATA_SETTINGS.default_padding_indicator, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            padding_indicator (Any, optional):",
            "                Padding indicator value. Defaults to `DATA_SETTINGS.default_padding_indicator`.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_array3d(",
            "            df=compatible_ddf_to_multiindex_df(self._data), padding_indicator=padding_indicator, max_timesteps=None",
            "        )",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return compatible_ddf_to_multiindex_df(self._data)",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Get a list containing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: A list containing sample indexes.",
            "        \"\"\"",
            "        return list(utils.get_df_index_level0_unique(compatible_ddf_to_multiindex_df(self._data)))  # pyright: ignore",
            "",
            "    def time_indexes(self) -> data_typing.TimeIndexList:",
            "        \"\"\"Get a list containing time indexes for each sample. Each time index is represented as a list of time step",
            "        elements.",
            "",
            "        Returns:",
            "            data_typing.TimeIndexList: A list containing time indexes for each sample.",
            "        \"\"\"",
            "        return list(self.time_indexes_as_dict().values())  # pyright: ignore",
            "",
            "    def time_indexes_as_dict(self) -> data_typing.SampleToTimeIndexDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its time index. Time index is represented as a list of time",
            "        step elements.",
            "",
            "        Returns:",
            "            data_typing.SampleToTimeIndexDict: The dictionary mapping each sample index to its time index.",
            "        \"\"\"",
            "        multiindex = compatible_ddf_to_multiindex_df(self._data).index",
            "        if TYPE_CHECKING:  # pragma: no cover",
            "            assert isinstance(multiindex, pd.MultiIndex)  # nosec B101",
            "        sample_index = self.sample_index()",
            "        d = dict()",
            "        for s in sample_index:",
            "            time_index_locs = multiindex.get_locs([s, slice(None)])",
            "            d[s] = list(multiindex.get_level_values(1)[time_index_locs])",
            "        return d  # type: ignore[return-value]",
            "",
            "    def time_indexes_float(self) -> List[np.ndarray]:",
            "        \"\"\"Return time indexes but converting their elements to `float` values.",
            "",
            "        Date-time time index will be converted using :obj:`~tempor.data.utils.datetime_time_index_to_float`.",
            "",
            "        Returns:",
            "            List[np.ndarray]: List of 1D `numpy.ndarray` s of `float` values, corresponding to the time index.",
            "        \"\"\"",
            "        return [utils.datetime_time_index_to_float(ti) for ti in self.time_indexes()]",
            "",
            "    def num_timesteps(self) -> List[int]:",
            "        \"\"\"Get the number of timesteps for each sample.",
            "",
            "        Returns:",
            "            List[int]: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return [len(x) for x in self.time_indexes()]",
            "",
            "    def num_timesteps_as_dict(self) -> data_typing.SampleToNumTimestepsDict:",
            "        \"\"\"Get a dictionary mapping each sample index to its the number of timesteps.",
            "",
            "        Returns:",
            "            data_typing.SampleToNumTimestepsDict: List containing the number of timesteps for each sample.",
            "        \"\"\"",
            "        return {key: len(x) for key, x in self.time_indexes_as_dict().items()}  # type: ignore",
            "",
            "    def num_timesteps_equal(self) -> bool:",
            "        \"\"\"Returns `True` if all samples share the same number of timesteps, `False` otherwise.",
            "",
            "        Returns:",
            "            bool: whether all samples share the same number of timesteps.",
            "        \"\"\"",
            "        timesteps = self.num_timesteps()",
            "        return True if len(timesteps) == 0 else all([x == timesteps[0] for x in timesteps])",
            "",
            "    def list_of_dataframes(self) -> List[pd.DataFrame]:",
            "        \"\"\"Returns a list of dataframes where each dataframe has the data for each sample.",
            "",
            "        Returns:",
            "            List[pd.DataFrame]: List of dataframes for each sample.",
            "        \"\"\"",
            "        return utils.multiindex_timeseries_dataframe_to_list_of_dataframes(compatible_ddf_to_multiindex_df(self._data))",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        sample_ids = utils.get_df_index_level0_unique(compatible_ddf_to_multiindex_df(self._data))",
            "        return len(sample_ids)",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, *, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a subset :class:`TimeSeriesSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`TimeSeriesSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        sample_index = utils.get_df_index_level0_unique(compatible_ddf_to_multiindex_df(self._data))",
            "        selected = list(sample_index[key_])  # pyright: ignore",
            "        return TimeSeriesSamplesDask(  # type: ignore [return-value]",
            "            compatible_ddf_to_multiindex_df(self._data).loc[(selected, slice(None)), :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )",
            "",
            "",
            "@plugins.register_plugin(name=\"event_samples_dask\", category=\"event_samples\", plugin_type=\"dataformat\")",
            "class EventSamplesDask(samples.EventSamplesBase):",
            "    _data: dd.DataFrame  # type: ignore[name-defined]",
            "    # _schema: pa.DataFrameSchema",
            "    # _schema_split: pa.DataFrameSchema",
            "",
            "    def __init__(",
            "        self,",
            "        data: data_typing.DataContainer,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Create an :class:`EventSamplesDask` object from the ``data``.",
            "",
            "        Args:",
            "            data (data_typing.DataContainer):",
            "                A container with the data.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments to pass to the constructor.",
            "        \"\"\"",
            "        npartitions, chunksize, kwargs = _process_npartitions_chunksize(**kwargs)",
            "        if isinstance(data, dd.DataFrame):  # type: ignore[attr-defined]",
            "            self._data = data",
            "        elif isinstance(data, pd.DataFrame):",
            "            self._data = dd.from_pandas(  # type: ignore[attr-defined]",
            "                data,",
            "                npartitions=npartitions,",
            "                chunksize=chunksize,",
            "            )",
            "        elif isinstance(data, np.ndarray):",
            "            raise NotImplementedError(\"`EventSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "        else:  # pragma: no cover  # Prevented by pydantic check.",
            "            raise ValueError(f\"Data object {type(data)} not supported\")",
            "        super().__init__(data, **kwargs)",
            "",
            "    def _validate(self) -> None:",
            "        # TODO: Validation analogous to the DF implementation.",
            "        logger.info(\"Validation not yet implemented for Dask data format. Data format consistency is not guaranteed.\")",
            "",
            "    @staticmethod",
            "    def from_dataframe(",
            "        dataframe: pd.DataFrame,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamplesDask\":  # pyright: ignore",
            "        \"\"\"Create :class:`EventSamples` from `pandas.DataFrame`. The row index of the dataframe should be the sample",
            "        indexes. The columns should be the features. Each feature should contain a tuple of ``(time, value)``",
            "        representing the event.",
            "",
            "        Args:",
            "            dataframe (pd.DataFrame): The dataframe that contains the data.",
            "            **kwargs (Any): Any additional keyword arguments to pass to the constructor.",
            "",
            "        Returns:",
            "            EventSamplesDask: The :class:`EventSamplesDask` object created from the ``dataframe``.",
            "        \"\"\"",
            "        return EventSamplesDask(dataframe, **kwargs)",
            "",
            "    @staticmethod",
            "    def from_numpy(",
            "        array: np.ndarray,",
            "        **kwargs: Any,",
            "    ) -> \"EventSamplesDask\":  # pyright: ignore",
            "        \"\"\"Not implemented yet.\"\"\"",
            "        raise NotImplementedError(\"`EventSamplesDask` does not support `numpy.ndarray` input yet.\")",
            "",
            "    def numpy(self, **kwargs: Any) -> np.ndarray:",
            "        \"\"\"Return the data as a `numpy.ndarray`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            np.ndarray: The `numpy.ndarray`.",
            "        \"\"\"",
            "        return self._data.compute().to_numpy()",
            "",
            "    def dataframe(self, **kwargs: Any) -> pd.DataFrame:",
            "        \"\"\"Return the data as a `pandas.DataFrame`.",
            "",
            "        Args:",
            "            **kwargs (Any): Any additional keyword arguments. Currently unused.",
            "",
            "        Returns:",
            "            pd.DataFrame: The `pandas.DataFrame`.",
            "        \"\"\"",
            "        return self._data.compute()",
            "",
            "    def sample_index(self) -> data_typing.SampleIndex:",
            "        \"\"\"Return a list representing sample indexes.",
            "",
            "        Returns:",
            "            data_typing.SampleIndex: Sample indexes.",
            "        \"\"\"",
            "        return list(self._data.index.compute())",
            "",
            "    @property",
            "    def num_samples(self) -> int:",
            "        \"\"\"Return number of samples.",
            "",
            "        Returns:",
            "            int: Number of samples.",
            "        \"\"\"",
            "        return self._data.shape[0].compute()",
            "",
            "    @property",
            "    def num_features(self) -> int:",
            "        \"\"\"Return number of features.",
            "",
            "        Returns:",
            "            int: Number of features.",
            "        \"\"\"",
            "        return self._data.shape[1]",
            "",
            "    def split(",
            "        self,",
            "        time_feature_suffix: str = samples._DEFAULT_EVENTS_TIME_FEATURE_SUFFIX,  # pylint: disable=protected-access",
            "    ) -> pd.DataFrame:",
            "        \"\"\"Return a `pandas.DataFrame` where the time component of each event feature has been split off to its own",
            "        column. The new columns that contain the times will be named ``\"<original column name><time_feature_suffix>\"``",
            "        and will be inserted before each corresponding ``<original column name>`` column. The ``<original column name>``",
            "        columns will contain only the event value.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            pd.DataFrame: The output dataframe.",
            "        \"\"\"",
            "        df = self._data.compute().copy()",
            "        features = list(df.columns)",
            "        if any(time_feature_suffix in str(c) for c in features):  # pragma: no cover",
            "            raise ValueError(f\"Column names must not contain '{time_feature_suffix}'\")",
            "        for f_idx, f in enumerate(features):",
            "            df.insert(f_idx * 2, f\"{f}{time_feature_suffix}\", df[f].apply(lambda x: x[0]))",
            "        for f in features:",
            "            df[f] = df[f].apply(lambda x: x[1])",
            "        return df",
            "",
            "    def split_as_two_dataframes(",
            "        self,",
            "        time_feature_suffix: str = samples._DEFAULT_EVENTS_TIME_FEATURE_SUFFIX,  # pylint: disable=protected-access",
            "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:",
            "        \"\"\"Analogous to :func:`~tempor.data.samples.EventSamples.split` but returns two `pandas.DataFrame` s:",
            "            - first dataframe contains the event times of each feature.",
            "            - second dataframe contains the event values (`True`/`False`) of each feature.",
            "",
            "        Args:",
            "            time_feature_suffix (str, optional):",
            "                A column name suffix string to identify the time columns that will be split off. Defaults to",
            "                ``\"_time\"``.",
            "",
            "        Returns:",
            "            Tuple[pd.DataFrame, pd.DataFrame]: Two `pandas.DataFrame` s containing event times and values respectively.",
            "        \"\"\"",
            "        df_split = self.split(time_feature_suffix=time_feature_suffix)",
            "        df_event_times = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix in c]]",
            "        df_event_values = df_split.loc[:, [c for c in df_split.columns if time_feature_suffix not in c]]",
            "        return df_event_times, df_event_values",
            "",
            "    def short_repr(self) -> str:",
            "        \"\"\"A short string representation of the object.",
            "",
            "        Returns:",
            "            str: The short representation.",
            "        \"\"\"",
            "        return f\"{self.__class__.__name__}([{self.num_samples}, {self.num_features}])\"",
            "",
            "    def __getitem__(self, key: data_typing.GetItemKey) -> Self:",
            "        \"\"\"Return a new subset :class:`EventSamples` object with the data indexed by the ``key``.",
            "",
            "        Args:",
            "            key (data_typing.GetItemKey): The key to index the data by.",
            "",
            "        Returns:",
            "            Self: A new subset :class:`EventSamples` object.",
            "        \"\"\"",
            "        key_ = utils.ensure_pd_iloc_key_returns_df(key)",
            "        return EventSamplesDask(  # type: ignore [return-value]",
            "            self._data.compute().iloc[key_, :],  # pyright: ignore",
            "            _skip_validate=True,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "95": [
                "StaticSamplesDask",
                "from_numpy"
            ],
            "246": [
                "TimeSeriesSamplesDask",
                "from_numpy"
            ],
            "458": [
                "EventSamplesDask",
                "from_numpy"
            ]
        },
        "addLocation": []
    },
    "src/tempor/datasources/prediction/temporal/plugin_dummy_prediction.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import Any, Optional, cast"
            },
            "2": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.datasets.dummy import dummy_dataset"
            },
            "4": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "5": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from tempor.core import plugins"
            },
            "6": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from tempor.data import dataset"
            },
            "7": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset"
            },
            "8": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from tempor.datasources import datasource"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+from tempor.models.clairvoyance2.datasets.dummy import dummy_dataset"
            },
            "10": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " @plugins.register_plugin(name=\"dummy_prediction\", category=\"prediction.temporal\", plugin_type=\"datasource\")"
            }
        },
        "frontPatchFile": [
            "\"\"\"Module with the dummy data source for temporal prediction.\"\"\"",
            "",
            "from typing import Any, Optional, cast",
            "",
            "from clairvoyance2.datasets.dummy import dummy_dataset",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "",
            "",
            "@plugins.register_plugin(name=\"dummy_prediction\", category=\"prediction.temporal\", plugin_type=\"datasource\")",
            "class DummyTemporalPredictionDataSource(datasource.TemporalPredictionDataSource):",
            "    def __init__(",
            "        self,",
            "        n_samples: int = 100,",
            "        temporal_covariates_n_features: int = 5,",
            "        temporal_covariates_max_len: int = 20,",
            "        temporal_covariates_missing_prob: float = 0.1,",
            "        static_covariates_n_features: int = 3,",
            "        static_covariates_missing_prob: float = 0.0,",
            "        temporal_targets_n_features: int = 2,",
            "        temporal_targets_n_categories: Optional[int] = None,",
            "        random_state: int = 12345,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Dummy data source for temporal prediction tasks; generates a dataset with random data.",
            "",
            "        Args:",
            "            n_samples (int, optional):",
            "                Number of samples. Defaults to ``100``.",
            "            temporal_covariates_n_features (int, optional):",
            "                Number of time series covariates features. Defaults to ``5``.",
            "            temporal_covariates_max_len (int, optional):",
            "                Maximum number of time steps in time series covariates. Defaults to ``20``.",
            "            temporal_covariates_missing_prob (float, optional):",
            "                The missingness probability of time series covariates. Defaults to ``0.1``.",
            "            static_covariates_n_features (int, optional):",
            "                Number of static covariates features. Defaults to ``3``.",
            "            static_covariates_missing_prob (float, optional):",
            "                The missingness probability of static covariates. Defaults to ``0.0``.",
            "            temporal_targets_n_features (int, optional):",
            "                Number of time series target features. Defaults to 2.",
            "            temporal_targets_n_categories (Optional[int], optional):",
            "                Number of categories in time series targets features. If `None`, the target features will be real \\",
            "                rather than categorical. Defaults to `None`.",
            "            random_state (int, optional):",
            "                Random state to use. Defaults to ``12345``.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments will be passed to `~tempor.datasources.DataSource`.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "",
            "        self.n_samples = n_samples",
            "        self.temporal_covariates_n_features = temporal_covariates_n_features",
            "        self.temporal_covariates_max_len = temporal_covariates_max_len",
            "        self.temporal_covariates_missing_prob = temporal_covariates_missing_prob",
            "        self.static_covariates_n_features = static_covariates_n_features",
            "        self.static_covariates_missing_prob = static_covariates_missing_prob",
            "        self.temporal_targets_n_features = temporal_targets_n_features",
            "        self.temporal_targets_n_categories = temporal_targets_n_categories",
            "        self.random_state = random_state",
            "",
            "    @staticmethod",
            "    def url() -> None:  # noqa: D102",
            "        return None",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> None:  # noqa: D102",
            "        return None",
            "",
            "    def load(self, **kwargs: Any) -> dataset.TemporalPredictionDataset:  # noqa: D102",
            "        clv_dataset = dummy_dataset(",
            "            n_samples=self.n_samples,",
            "            temporal_covariates_n_features=self.temporal_covariates_n_features,",
            "            temporal_covariates_max_len=self.temporal_covariates_max_len,",
            "            temporal_covariates_missing_prob=self.temporal_covariates_missing_prob,",
            "            static_covariates_n_features=self.static_covariates_n_features,",
            "            temporal_targets_n_features=self.temporal_targets_n_features,",
            "            temporal_targets_n_categories=self.temporal_targets_n_categories,",
            "            random_seed=self.random_state,",
            "            # No temporal treatments features in this case:",
            "            temporal_treatments_n_features=0,",
            "            temporal_treatments_n_categories=None,",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.TemporalPredictionDataset, data)",
            "        return data"
        ],
        "afterPatchFile": [
            "\"\"\"Module with the dummy data source for temporal prediction.\"\"\"",
            "",
            "from typing import Any, Optional, cast",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "from tempor.models.clairvoyance2.datasets.dummy import dummy_dataset",
            "",
            "",
            "@plugins.register_plugin(name=\"dummy_prediction\", category=\"prediction.temporal\", plugin_type=\"datasource\")",
            "class DummyTemporalPredictionDataSource(datasource.TemporalPredictionDataSource):",
            "    def __init__(",
            "        self,",
            "        n_samples: int = 100,",
            "        temporal_covariates_n_features: int = 5,",
            "        temporal_covariates_max_len: int = 20,",
            "        temporal_covariates_missing_prob: float = 0.1,",
            "        static_covariates_n_features: int = 3,",
            "        static_covariates_missing_prob: float = 0.0,",
            "        temporal_targets_n_features: int = 2,",
            "        temporal_targets_n_categories: Optional[int] = None,",
            "        random_state: int = 12345,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Dummy data source for temporal prediction tasks; generates a dataset with random data.",
            "",
            "        Args:",
            "            n_samples (int, optional):",
            "                Number of samples. Defaults to ``100``.",
            "            temporal_covariates_n_features (int, optional):",
            "                Number of time series covariates features. Defaults to ``5``.",
            "            temporal_covariates_max_len (int, optional):",
            "                Maximum number of time steps in time series covariates. Defaults to ``20``.",
            "            temporal_covariates_missing_prob (float, optional):",
            "                The missingness probability of time series covariates. Defaults to ``0.1``.",
            "            static_covariates_n_features (int, optional):",
            "                Number of static covariates features. Defaults to ``3``.",
            "            static_covariates_missing_prob (float, optional):",
            "                The missingness probability of static covariates. Defaults to ``0.0``.",
            "            temporal_targets_n_features (int, optional):",
            "                Number of time series target features. Defaults to 2.",
            "            temporal_targets_n_categories (Optional[int], optional):",
            "                Number of categories in time series targets features. If `None`, the target features will be real \\",
            "                rather than categorical. Defaults to `None`.",
            "            random_state (int, optional):",
            "                Random state to use. Defaults to ``12345``.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments will be passed to `~tempor.datasources.DataSource`.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "",
            "        self.n_samples = n_samples",
            "        self.temporal_covariates_n_features = temporal_covariates_n_features",
            "        self.temporal_covariates_max_len = temporal_covariates_max_len",
            "        self.temporal_covariates_missing_prob = temporal_covariates_missing_prob",
            "        self.static_covariates_n_features = static_covariates_n_features",
            "        self.static_covariates_missing_prob = static_covariates_missing_prob",
            "        self.temporal_targets_n_features = temporal_targets_n_features",
            "        self.temporal_targets_n_categories = temporal_targets_n_categories",
            "        self.random_state = random_state",
            "",
            "    @staticmethod",
            "    def url() -> None:  # noqa: D102",
            "        return None",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> None:  # noqa: D102",
            "        return None",
            "",
            "    def load(self, **kwargs: Any) -> dataset.TemporalPredictionDataset:  # noqa: D102",
            "        clv_dataset = dummy_dataset(",
            "            n_samples=self.n_samples,",
            "            temporal_covariates_n_features=self.temporal_covariates_n_features,",
            "            temporal_covariates_max_len=self.temporal_covariates_max_len,",
            "            temporal_covariates_missing_prob=self.temporal_covariates_missing_prob,",
            "            static_covariates_n_features=self.static_covariates_n_features,",
            "            temporal_targets_n_features=self.temporal_targets_n_features,",
            "            temporal_targets_n_categories=self.temporal_targets_n_categories,",
            "            random_seed=self.random_state,",
            "            # No temporal treatments features in this case:",
            "            temporal_treatments_n_features=0,",
            "            temporal_treatments_n_categories=None,",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.TemporalPredictionDataset, data)",
            "        return data"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "5": [],
            "6": []
        },
        "addLocation": []
    },
    "src/tempor/datasources/prediction/temporal/plugin_uci_diabetes.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from typing import Any, Tuple, cast"
            },
            "1": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " import requests"
            },
            "3": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.datasets.uci import uci_diabetes"
            },
            "4": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.preprocessing.convenience import TemporalTargetsExtractor"
            },
            "5": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from tempor.core import plugins"
            },
            "7": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " from tempor.data import dataset"
            },
            "8": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset"
            },
            "9": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from tempor.datasources import datasource"
            },
            "10": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from tempor.log import logger"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from tempor.models.clairvoyance2.datasets.uci import uci_diabetes"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+from tempor.models.clairvoyance2.preprocessing.convenience import TemporalTargetsExtractor"
            },
            "13": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from ...datasource import monkeypatch_ssl_error_workaround"
            },
            "15": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\"Module defining the UCI diabetes data source.\"\"\"",
            "",
            "import time",
            "import traceback",
            "import urllib.error",
            "from pathlib import Path",
            "from typing import Any, Tuple, cast",
            "",
            "import requests",
            "from clairvoyance2.datasets.uci import uci_diabetes",
            "from clairvoyance2.preprocessing.convenience import TemporalTargetsExtractor",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "from tempor.log import logger",
            "",
            "from ...datasource import monkeypatch_ssl_error_workaround",
            "",
            "",
            "@plugins.register_plugin(name=\"uci_diabetes\", category=\"prediction.temporal\", plugin_type=\"datasource\")",
            "class UCIDiabetesDataSource(datasource.TemporalPredictionDataSource):",
            "    def __init__(",
            "        self,",
            "        make_regular: bool = False,",
            "        use_int_index: bool = True,",
            "        targets: Tuple[str, ...] = (\"hypoglycemic_symptoms\",),",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"UCI diabetes data source.",
            "",
            "        See: https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes",
            "",
            "        Args:",
            "            make_regular (bool, optional):",
            "                Whether to reindex the dataset to have regular timesteps. Defaults to `False`.",
            "            use_int_index (bool, optional):",
            "                Whether to use integer index. Defaults to `True`.",
            "            targets (Tuple[str, ...], optional):",
            "                The target feature(s). Defaults to ``(\"hypoglycemic_symptoms\",)``.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments will be passed to parent constructor.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.make_regular = make_regular",
            "        self.use_int_index = use_int_index",
            "        self.targets = targets",
            "",
            "    @staticmethod",
            "    def url() -> str:  # noqa: D102",
            "        return \"https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes\"",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> str:  # noqa: D102",
            "        return str(Path(UCIDiabetesDataSource.data_root_dir) / \"uci_diabetes\")",
            "",
            "    def _load(self) -> Any:",
            "        return uci_diabetes(",
            "            data_home=UCIDiabetesDataSource.data_root_dir,",
            "            refresh_cache=True,",
            "            redownload=False,",
            "            make_regular=self.make_regular,",
            "            use_int_index=self.use_int_index,",
            "        )",
            "",
            "    def load(self, **kwargs: Any) -> dataset.TemporalPredictionDataset:  # noqa: D102",
            "        download_retries = 3",
            "        download_pause_sec = 5",
            "        for retry in range(download_retries):",
            "            # NOTE: Connection to archive.ics.uci.edu tends to be flaky, attempt download retries.",
            "            # TODO: May wish to exclude this from tests / download files from a more stable location /",
            "            # make tests resilient to internet connection failures.",
            "            try:",
            "                clv_dataset = self._load()",
            "            except urllib.error.URLError as ex:  # pragma: no cover",
            "                if \"SSL\" in str(ex) or \"ssl\" in str(ex):",
            "                    with monkeypatch_ssl_error_workaround():",
            "                        clv_dataset = self._load()",
            "                else:",
            "                    raise",
            "            except (  # pylint: disable=duplicate-except",
            "                requests.exceptions.RequestException,",
            "                urllib.error.URLError,",
            "            ) as ex:  # pragma: no cover",
            "                if retry + 1 == download_retries:",
            "                    logger.error(f\"Failed to download UCI diabetes dataset after {download_retries} retries.\")",
            "                    raise",
            "                logger.debug(",
            "                    f\"Caught exception and will retry ({retry + 1}/{download_retries}): \"",
            "                    f\"{ex}\\n{traceback.format_exc()}\"",
            "                )",
            "                time.sleep(download_pause_sec)",
            "        clv_dataset = TemporalTargetsExtractor(params={\"targets\": self.targets}).fit_transform(",
            "            clv_dataset  # pyright: ignore",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.TemporalPredictionDataset, data)",
            "        return data"
        ],
        "afterPatchFile": [
            "\"\"\"Module defining the UCI diabetes data source.\"\"\"",
            "",
            "import time",
            "import traceback",
            "import urllib.error",
            "from pathlib import Path",
            "from typing import Any, Tuple, cast",
            "",
            "import requests",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "from tempor.log import logger",
            "from tempor.models.clairvoyance2.datasets.uci import uci_diabetes",
            "from tempor.models.clairvoyance2.preprocessing.convenience import TemporalTargetsExtractor",
            "",
            "from ...datasource import monkeypatch_ssl_error_workaround",
            "",
            "",
            "@plugins.register_plugin(name=\"uci_diabetes\", category=\"prediction.temporal\", plugin_type=\"datasource\")",
            "class UCIDiabetesDataSource(datasource.TemporalPredictionDataSource):",
            "    def __init__(",
            "        self,",
            "        make_regular: bool = False,",
            "        use_int_index: bool = True,",
            "        targets: Tuple[str, ...] = (\"hypoglycemic_symptoms\",),",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"UCI diabetes data source.",
            "",
            "        See: https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes",
            "",
            "        Args:",
            "            make_regular (bool, optional):",
            "                Whether to reindex the dataset to have regular timesteps. Defaults to `False`.",
            "            use_int_index (bool, optional):",
            "                Whether to use integer index. Defaults to `True`.",
            "            targets (Tuple[str, ...], optional):",
            "                The target feature(s). Defaults to ``(\"hypoglycemic_symptoms\",)``.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments will be passed to parent constructor.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "        self.make_regular = make_regular",
            "        self.use_int_index = use_int_index",
            "        self.targets = targets",
            "",
            "    @staticmethod",
            "    def url() -> str:  # noqa: D102",
            "        return \"https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes\"",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> str:  # noqa: D102",
            "        return str(Path(UCIDiabetesDataSource.data_root_dir) / \"uci_diabetes\")",
            "",
            "    def _load(self) -> Any:",
            "        return uci_diabetes(",
            "            data_home=UCIDiabetesDataSource.data_root_dir,",
            "            refresh_cache=True,",
            "            redownload=False,",
            "            make_regular=self.make_regular,",
            "            use_int_index=self.use_int_index,",
            "        )",
            "",
            "    def load(self, **kwargs: Any) -> dataset.TemporalPredictionDataset:  # noqa: D102",
            "        download_retries = 3",
            "        download_pause_sec = 5",
            "        for retry in range(download_retries):",
            "            # NOTE: Connection to archive.ics.uci.edu tends to be flaky, attempt download retries.",
            "            # TODO: May wish to exclude this from tests / download files from a more stable location /",
            "            # make tests resilient to internet connection failures.",
            "            try:",
            "                clv_dataset = self._load()",
            "            except urllib.error.URLError as ex:  # pragma: no cover",
            "                if \"SSL\" in str(ex) or \"ssl\" in str(ex):",
            "                    with monkeypatch_ssl_error_workaround():",
            "                        clv_dataset = self._load()",
            "                else:",
            "                    raise",
            "            except (  # pylint: disable=duplicate-except",
            "                requests.exceptions.RequestException,",
            "                urllib.error.URLError,",
            "            ) as ex:  # pragma: no cover",
            "                if retry + 1 == download_retries:",
            "                    logger.error(f\"Failed to download UCI diabetes dataset after {download_retries} retries.\")",
            "                    raise",
            "                logger.debug(",
            "                    f\"Caught exception and will retry ({retry + 1}/{download_retries}): \"",
            "                    f\"{ex}\\n{traceback.format_exc()}\"",
            "                )",
            "                time.sleep(download_pause_sec)",
            "        clv_dataset = TemporalTargetsExtractor(params={\"targets\": self.targets}).fit_transform(",
            "            clv_dataset  # pyright: ignore",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.TemporalPredictionDataset, data)",
            "        return data"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "10": [],
            "11": []
        },
        "addLocation": []
    },
    "src/tempor/datasources/treatments/one_off/plugin_pkpd.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import Any, cast"
            },
            "2": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.datasets.simulated.simple_pkpd import simple_pkpd_dataset"
            },
            "4": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "5": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from tempor.core import plugins"
            },
            "6": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from tempor.data import dataset"
            },
            "7": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset"
            },
            "8": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from tempor.datasources import datasource"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+from tempor.models.clairvoyance2.datasets.simulated.simple_pkpd import simple_pkpd_dataset"
            },
            "10": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " @plugins.register_plugin(name=\"pkpd\", category=\"treatments.one_off\", plugin_type=\"datasource\")"
            }
        },
        "frontPatchFile": [
            "\"\"\"Module for the PKPD data source plugin.\"\"\"",
            "",
            "from typing import Any, cast",
            "",
            "from clairvoyance2.datasets.simulated.simple_pkpd import simple_pkpd_dataset",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "",
            "",
            "@plugins.register_plugin(name=\"pkpd\", category=\"treatments.one_off\", plugin_type=\"datasource\")",
            "class PKPDDataSource(datasource.OneOffTreatmentEffectsDataSource):",
            "    def __init__(",
            "        self,",
            "        n_timesteps: int = 10,",
            "        time_index_treatment_event: int = 7,",
            "        n_control_samples: int = 20,",
            "        n_treated_samples: int = 20,",
            "        random_state: int = 100,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"PKPD data source for one-off treatment effects tasks.",
            "",
            "        Adapted from: https://github.com/ZhaozhiQIAN/SyncTwin-NeurIPS-2021",
            "",
            "        Args:",
            "            n_timesteps (int, optional): Number of timesteps. Defaults to ``10``.",
            "            time_index_treatment_event (int, optional): Time index of the treatment event. Defaults to ``7``.",
            "            n_control_samples (int, optional): Number of control samples to generate. Defaults to ``20``.",
            "            n_treated_samples (int, optional): Number of treated samples to generate. Defaults to ``20``.",
            "            random_state (int, optional): Random state to use. Defaults to ``100``.",
            "            **kwargs (Any): Any additional keyword arguments will be passed to parent constructor.",
            "",
            "        Reference:",
            "            Qian, Z., Zhang, Y., Bica, I., Wood, A., & van der Schaar, M. (2021). Synctwin: Treatment effect \\",
            "            estimation with longitudinal outcomes. Advances in Neural Information Processing Systems, 34, 3178-3190.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "",
            "        self.n_timesteps = n_timesteps",
            "        self.time_index_treatment_event = time_index_treatment_event",
            "        self.n_control_samples = n_control_samples",
            "        self.n_treated_samples = n_treated_samples",
            "        self.random_state = random_state",
            "",
            "    @staticmethod",
            "    def url() -> None:  # noqa: D102",
            "        return None",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> None:  # noqa: D102",
            "        return None",
            "",
            "    def load(self, **kwargs: Any) -> dataset.OneOffTreatmentEffectsDataset:  # noqa: D102",
            "        clv_dataset = simple_pkpd_dataset(",
            "            n_timesteps=self.n_timesteps,",
            "            time_index_treatment_event=self.time_index_treatment_event,",
            "            n_control_samples=self.n_control_samples,",
            "            n_treated_samples=self.n_treated_samples,",
            "            seed=self.random_state,",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.OneOffTreatmentEffectsDataset, data)",
            "        return data"
        ],
        "afterPatchFile": [
            "\"\"\"Module for the PKPD data source plugin.\"\"\"",
            "",
            "from typing import Any, cast",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "from tempor.models.clairvoyance2.datasets.simulated.simple_pkpd import simple_pkpd_dataset",
            "",
            "",
            "@plugins.register_plugin(name=\"pkpd\", category=\"treatments.one_off\", plugin_type=\"datasource\")",
            "class PKPDDataSource(datasource.OneOffTreatmentEffectsDataSource):",
            "    def __init__(",
            "        self,",
            "        n_timesteps: int = 10,",
            "        time_index_treatment_event: int = 7,",
            "        n_control_samples: int = 20,",
            "        n_treated_samples: int = 20,",
            "        random_state: int = 100,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"PKPD data source for one-off treatment effects tasks.",
            "",
            "        Adapted from: https://github.com/ZhaozhiQIAN/SyncTwin-NeurIPS-2021",
            "",
            "        Args:",
            "            n_timesteps (int, optional): Number of timesteps. Defaults to ``10``.",
            "            time_index_treatment_event (int, optional): Time index of the treatment event. Defaults to ``7``.",
            "            n_control_samples (int, optional): Number of control samples to generate. Defaults to ``20``.",
            "            n_treated_samples (int, optional): Number of treated samples to generate. Defaults to ``20``.",
            "            random_state (int, optional): Random state to use. Defaults to ``100``.",
            "            **kwargs (Any): Any additional keyword arguments will be passed to parent constructor.",
            "",
            "        Reference:",
            "            Qian, Z., Zhang, Y., Bica, I., Wood, A., & van der Schaar, M. (2021). Synctwin: Treatment effect \\",
            "            estimation with longitudinal outcomes. Advances in Neural Information Processing Systems, 34, 3178-3190.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "",
            "        self.n_timesteps = n_timesteps",
            "        self.time_index_treatment_event = time_index_treatment_event",
            "        self.n_control_samples = n_control_samples",
            "        self.n_treated_samples = n_treated_samples",
            "        self.random_state = random_state",
            "",
            "    @staticmethod",
            "    def url() -> None:  # noqa: D102",
            "        return None",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> None:  # noqa: D102",
            "        return None",
            "",
            "    def load(self, **kwargs: Any) -> dataset.OneOffTreatmentEffectsDataset:  # noqa: D102",
            "        clv_dataset = simple_pkpd_dataset(",
            "            n_timesteps=self.n_timesteps,",
            "            time_index_treatment_event=self.time_index_treatment_event,",
            "            n_control_samples=self.n_control_samples,",
            "            n_treated_samples=self.n_treated_samples,",
            "            seed=self.random_state,",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.OneOffTreatmentEffectsDataset, data)",
            "        return data"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "5": [],
            "6": []
        },
        "addLocation": []
    },
    "src/tempor/datasources/treatments/temporal/plugin_dummy_treatments.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import Any, Optional, cast"
            },
            "2": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.datasets.dummy import dummy_dataset"
            },
            "4": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "5": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from tempor.core import plugins"
            },
            "6": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from tempor.data import dataset"
            },
            "7": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset"
            },
            "8": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from tempor.datasources import datasource"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+from tempor.models.clairvoyance2.datasets.dummy import dummy_dataset"
            },
            "10": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " @plugins.register_plugin(name=\"dummy_treatments\", category=\"treatments.temporal\", plugin_type=\"datasource\")"
            }
        },
        "frontPatchFile": [
            "\"\"\"Module with the dummy data source for temporal treatment effects.\"\"\"",
            "",
            "from typing import Any, Optional, cast",
            "",
            "from clairvoyance2.datasets.dummy import dummy_dataset",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "",
            "",
            "@plugins.register_plugin(name=\"dummy_treatments\", category=\"treatments.temporal\", plugin_type=\"datasource\")",
            "class DummyTemporalTreatmentEffectsDataSource(datasource.TemporalTreatmentEffectsDataSource):",
            "    def __init__(",
            "        self,",
            "        n_samples: int = 100,",
            "        temporal_covariates_n_features: int = 5,",
            "        temporal_covariates_max_len: int = 20,",
            "        temporal_covariates_missing_prob: float = 0.1,",
            "        static_covariates_n_features: int = 3,",
            "        static_covariates_missing_prob: float = 0.0,",
            "        temporal_targets_n_features: int = 2,",
            "        temporal_targets_n_categories: Optional[int] = None,",
            "        temporal_treatments_n_features: int = 2,",
            "        temporal_treatments_n_categories: Optional[int] = None,",
            "        random_state: int = 12345,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Dummy data source for temporal treatment effects tasks; generates a dataset with random data.",
            "",
            "        Args:",
            "            n_samples (int, optional):",
            "                Number of samples. Defaults to ``100``.",
            "            temporal_covariates_n_features (int, optional):",
            "                Number of time series covariates features. Defaults to ``5``.",
            "            temporal_covariates_max_len (int, optional):",
            "                Maximum number of time steps in time series covariates. Defaults to ``20``.",
            "            temporal_covariates_missing_prob (float, optional):",
            "                The missingness probability of time series covariates. Defaults to ``0.1``.",
            "            static_covariates_n_features (int, optional):",
            "                Number of static covariates features. Defaults to ``3``.",
            "            static_covariates_missing_prob (float, optional):",
            "                The missingness probability of static covariates. Defaults to ``0.0``.",
            "            temporal_targets_n_features (int, optional):",
            "                Number of time series target features. Defaults to 2.",
            "            temporal_targets_n_categories (Optional[int], optional):",
            "                Number of categories in time series targets features. If `None`, the target features will be real \\",
            "                rather than categorical. Defaults to `None`.",
            "            temporal_treatments_n_features (int, optional):",
            "                Number of time series treatment features. Defaults to ``2``.",
            "            temporal_treatments_n_categories (Optional[int], optional):",
            "                Number of categories in time series treatment features. If `None`, the target features will be real \\",
            "                rather than categorical. Defaults to `None`.",
            "            random_state (int, optional):",
            "                Random state to use. Defaults to ``12345``.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments will be passed to `~tempor.datasources.DataSource`.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "",
            "        self.n_samples = n_samples",
            "        self.temporal_covariates_n_features = temporal_covariates_n_features",
            "        self.temporal_covariates_max_len = temporal_covariates_max_len",
            "        self.temporal_covariates_missing_prob = temporal_covariates_missing_prob",
            "        self.static_covariates_n_features = static_covariates_n_features",
            "        self.static_covariates_missing_prob = static_covariates_missing_prob",
            "        self.temporal_targets_n_features = temporal_targets_n_features",
            "        self.temporal_targets_n_categories = temporal_targets_n_categories",
            "        self.temporal_treatments_n_features = temporal_treatments_n_features",
            "        self.temporal_treatments_n_categories = temporal_treatments_n_categories",
            "        self.random_state = random_state",
            "",
            "    @staticmethod",
            "    def url() -> None:  # noqa: D102",
            "        return None",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> None:  # noqa: D102",
            "        return None",
            "",
            "    def load(self, **kwargs: Any) -> dataset.TemporalTreatmentEffectsDataset:  # noqa: D102",
            "        clv_dataset = dummy_dataset(",
            "            n_samples=self.n_samples,",
            "            temporal_covariates_n_features=self.temporal_covariates_n_features,",
            "            temporal_covariates_max_len=self.temporal_covariates_max_len,",
            "            temporal_covariates_missing_prob=self.temporal_covariates_missing_prob,",
            "            static_covariates_n_features=self.static_covariates_n_features,",
            "            temporal_targets_n_features=self.temporal_targets_n_features,",
            "            temporal_targets_n_categories=self.temporal_targets_n_categories,",
            "            random_seed=self.random_state,",
            "            # There are treatments features in this case:",
            "            temporal_treatments_n_features=self.temporal_treatments_n_features,",
            "            temporal_treatments_n_categories=self.temporal_treatments_n_categories,",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.TemporalTreatmentEffectsDataset, data)",
            "        return data"
        ],
        "afterPatchFile": [
            "\"\"\"Module with the dummy data source for temporal treatment effects.\"\"\"",
            "",
            "from typing import Any, Optional, cast",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset",
            "from tempor.data.clv2conv import clairvoyance2_dataset_to_tempor_dataset",
            "from tempor.datasources import datasource",
            "from tempor.models.clairvoyance2.datasets.dummy import dummy_dataset",
            "",
            "",
            "@plugins.register_plugin(name=\"dummy_treatments\", category=\"treatments.temporal\", plugin_type=\"datasource\")",
            "class DummyTemporalTreatmentEffectsDataSource(datasource.TemporalTreatmentEffectsDataSource):",
            "    def __init__(",
            "        self,",
            "        n_samples: int = 100,",
            "        temporal_covariates_n_features: int = 5,",
            "        temporal_covariates_max_len: int = 20,",
            "        temporal_covariates_missing_prob: float = 0.1,",
            "        static_covariates_n_features: int = 3,",
            "        static_covariates_missing_prob: float = 0.0,",
            "        temporal_targets_n_features: int = 2,",
            "        temporal_targets_n_categories: Optional[int] = None,",
            "        temporal_treatments_n_features: int = 2,",
            "        temporal_treatments_n_categories: Optional[int] = None,",
            "        random_state: int = 12345,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"Dummy data source for temporal treatment effects tasks; generates a dataset with random data.",
            "",
            "        Args:",
            "            n_samples (int, optional):",
            "                Number of samples. Defaults to ``100``.",
            "            temporal_covariates_n_features (int, optional):",
            "                Number of time series covariates features. Defaults to ``5``.",
            "            temporal_covariates_max_len (int, optional):",
            "                Maximum number of time steps in time series covariates. Defaults to ``20``.",
            "            temporal_covariates_missing_prob (float, optional):",
            "                The missingness probability of time series covariates. Defaults to ``0.1``.",
            "            static_covariates_n_features (int, optional):",
            "                Number of static covariates features. Defaults to ``3``.",
            "            static_covariates_missing_prob (float, optional):",
            "                The missingness probability of static covariates. Defaults to ``0.0``.",
            "            temporal_targets_n_features (int, optional):",
            "                Number of time series target features. Defaults to 2.",
            "            temporal_targets_n_categories (Optional[int], optional):",
            "                Number of categories in time series targets features. If `None`, the target features will be real \\",
            "                rather than categorical. Defaults to `None`.",
            "            temporal_treatments_n_features (int, optional):",
            "                Number of time series treatment features. Defaults to ``2``.",
            "            temporal_treatments_n_categories (Optional[int], optional):",
            "                Number of categories in time series treatment features. If `None`, the target features will be real \\",
            "                rather than categorical. Defaults to `None`.",
            "            random_state (int, optional):",
            "                Random state to use. Defaults to ``12345``.",
            "            **kwargs (Any):",
            "                Any additional keyword arguments will be passed to `~tempor.datasources.DataSource`.",
            "        \"\"\"",
            "        super().__init__(**kwargs)",
            "",
            "        self.n_samples = n_samples",
            "        self.temporal_covariates_n_features = temporal_covariates_n_features",
            "        self.temporal_covariates_max_len = temporal_covariates_max_len",
            "        self.temporal_covariates_missing_prob = temporal_covariates_missing_prob",
            "        self.static_covariates_n_features = static_covariates_n_features",
            "        self.static_covariates_missing_prob = static_covariates_missing_prob",
            "        self.temporal_targets_n_features = temporal_targets_n_features",
            "        self.temporal_targets_n_categories = temporal_targets_n_categories",
            "        self.temporal_treatments_n_features = temporal_treatments_n_features",
            "        self.temporal_treatments_n_categories = temporal_treatments_n_categories",
            "        self.random_state = random_state",
            "",
            "    @staticmethod",
            "    def url() -> None:  # noqa: D102",
            "        return None",
            "",
            "    @staticmethod",
            "    def dataset_dir() -> None:  # noqa: D102",
            "        return None",
            "",
            "    def load(self, **kwargs: Any) -> dataset.TemporalTreatmentEffectsDataset:  # noqa: D102",
            "        clv_dataset = dummy_dataset(",
            "            n_samples=self.n_samples,",
            "            temporal_covariates_n_features=self.temporal_covariates_n_features,",
            "            temporal_covariates_max_len=self.temporal_covariates_max_len,",
            "            temporal_covariates_missing_prob=self.temporal_covariates_missing_prob,",
            "            static_covariates_n_features=self.static_covariates_n_features,",
            "            temporal_targets_n_features=self.temporal_targets_n_features,",
            "            temporal_targets_n_categories=self.temporal_targets_n_categories,",
            "            random_seed=self.random_state,",
            "            # There are treatments features in this case:",
            "            temporal_treatments_n_features=self.temporal_treatments_n_features,",
            "            temporal_treatments_n_categories=self.temporal_treatments_n_categories,",
            "        )",
            "        data = clairvoyance2_dataset_to_tempor_dataset(clv_dataset)",
            "        data = cast(dataset.TemporalTreatmentEffectsDataset, data)",
            "        return data"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "5": [],
            "6": []
        },
        "addLocation": []
    },
    "src/tempor/methods/constants.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import dataclasses"
            },
            "1": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " from typing import Any, Dict, List, Optional"
            },
            "2": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.data import DEFAULT_PADDING_INDICATOR"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 6,
                "PatchRowcode": "+from tempor.models.clairvoyance2.data import DEFAULT_PADDING_INDICATOR"
            },
            "5": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " @dataclasses.dataclass"
            }
        },
        "frontPatchFile": [
            "\"\"\"Common constants for methods.\"\"\"",
            "",
            "import dataclasses",
            "from typing import Any, Dict, List, Optional",
            "",
            "from clairvoyance2.data import DEFAULT_PADDING_INDICATOR",
            "",
            "",
            "@dataclasses.dataclass",
            "class Seq2seqParams:",
            "    # Encoder:",
            "    encoder_rnn_type: str = \"LSTM\"",
            "    \"\"\"Encoder RNN type. Available options: ``\"LSTM\"``, ``\"GRU\"``, ``\"RNN\"``.\"\"\"",
            "    encoder_hidden_size: int = 100",
            "    \"\"\"Encoder hidden size.\"\"\"",
            "    encoder_num_layers: int = 1",
            "    \"\"\"Encoder number of layers.\"\"\"",
            "    encoder_bias: bool = True",
            "    \"\"\"Encoder bias enabled/disabled.\"\"\"",
            "    encoder_dropout: float = 0.0",
            "    \"\"\"Encoder dropout values (0.0 to 1.0)\"\"\"",
            "    encoder_bidirectional: bool = False",
            "    \"\"\"Encoder bidirectional enabled/disabled.\"\"\"",
            "    encoder_nonlinearity: Optional[str] = None",
            "    \"\"\"Encoder nonlinearity.\"\"\"",
            "    encoder_proj_size: Optional[int] = None",
            "    \"\"\"Encoder projection size.\"\"\"",
            "    # Decoder:",
            "    decoder_rnn_type: str = \"LSTM\"",
            "    \"\"\"Decoder RNN type. Available options: ``\"LSTM\"``, ``\"GRU\"``, ``\"RNN\"``.\"\"\"",
            "    decoder_hidden_size: int = 100",
            "    \"\"\"Decoder hidden size.\"\"\"",
            "    decoder_num_layers: int = 1",
            "    \"\"\"Decoder number of layers.\"\"\"",
            "    decoder_bias: bool = True",
            "    \"\"\"Decoder bias enabled/disabled.\"\"\"",
            "    decoder_dropout: float = 0.0",
            "    \"\"\"Decoder dropout values (0.0 to 1.0)\"\"\"",
            "    decoder_bidirectional: bool = False",
            "    \"\"\"Decoder bidirectional enabled/disabled.\"\"\"",
            "    decoder_nonlinearity: Optional[str] = None",
            "    \"\"\"Decoder nonlinearity.\"\"\"",
            "    decoder_proj_size: Optional[int] = None",
            "    \"\"\"Decoder projection size.\"\"\"",
            "    # Adapter FF NN:",
            "    adapter_hidden_dims: List[int] = dataclasses.field(default_factory=lambda: [50])",
            "    \"\"\"Adapter hidden dimensions, as a list of integers corresponding to different layers.\"\"\"",
            "    adapter_out_activation: Optional[str] = \"Tanh\"",
            "    \"\"\"Adapter output activation function.\"\"\"",
            "    # Predictor FF NN:",
            "    predictor_hidden_dims: List[int] = dataclasses.field(default_factory=lambda: [])",
            "    \"\"\"Predictor hidden dimensions, as a list of integers corresponding to different layers.\"\"\"",
            "    predictor_out_activation: Optional[str] = None",
            "    \"\"\"Predictor output activation function.\"\"\"",
            "    # Misc:",
            "    max_len: Optional[int] = None",
            "    \"\"\"Maximum length of the input sequence.\"\"\"",
            "    optimizer_str: str = \"Adam\"",
            "    \"\"\"Optimizer type.\"\"\"",
            "    optimizer_kwargs: Dict[str, Any] = dataclasses.field(default_factory=lambda: dict(lr=0.01, weight_decay=1e-5))",
            "    \"\"\"Optimizer kwargs.\"\"\"",
            "    batch_size: int = 32",
            "    \"\"\"Batch size.\"\"\"",
            "    epochs: int = 100",
            "    \"\"\"Number of epochs.\"\"\"",
            "    padding_indicator: float = DEFAULT_PADDING_INDICATOR",
            "    \"\"\"Padding indicator value.\"\"\""
        ],
        "afterPatchFile": [
            "\"\"\"Common constants for methods.\"\"\"",
            "",
            "import dataclasses",
            "from typing import Any, Dict, List, Optional",
            "",
            "from tempor.models.clairvoyance2.data import DEFAULT_PADDING_INDICATOR",
            "",
            "",
            "@dataclasses.dataclass",
            "class Seq2seqParams:",
            "    # Encoder:",
            "    encoder_rnn_type: str = \"LSTM\"",
            "    \"\"\"Encoder RNN type. Available options: ``\"LSTM\"``, ``\"GRU\"``, ``\"RNN\"``.\"\"\"",
            "    encoder_hidden_size: int = 100",
            "    \"\"\"Encoder hidden size.\"\"\"",
            "    encoder_num_layers: int = 1",
            "    \"\"\"Encoder number of layers.\"\"\"",
            "    encoder_bias: bool = True",
            "    \"\"\"Encoder bias enabled/disabled.\"\"\"",
            "    encoder_dropout: float = 0.0",
            "    \"\"\"Encoder dropout values (0.0 to 1.0)\"\"\"",
            "    encoder_bidirectional: bool = False",
            "    \"\"\"Encoder bidirectional enabled/disabled.\"\"\"",
            "    encoder_nonlinearity: Optional[str] = None",
            "    \"\"\"Encoder nonlinearity.\"\"\"",
            "    encoder_proj_size: Optional[int] = None",
            "    \"\"\"Encoder projection size.\"\"\"",
            "    # Decoder:",
            "    decoder_rnn_type: str = \"LSTM\"",
            "    \"\"\"Decoder RNN type. Available options: ``\"LSTM\"``, ``\"GRU\"``, ``\"RNN\"``.\"\"\"",
            "    decoder_hidden_size: int = 100",
            "    \"\"\"Decoder hidden size.\"\"\"",
            "    decoder_num_layers: int = 1",
            "    \"\"\"Decoder number of layers.\"\"\"",
            "    decoder_bias: bool = True",
            "    \"\"\"Decoder bias enabled/disabled.\"\"\"",
            "    decoder_dropout: float = 0.0",
            "    \"\"\"Decoder dropout values (0.0 to 1.0)\"\"\"",
            "    decoder_bidirectional: bool = False",
            "    \"\"\"Decoder bidirectional enabled/disabled.\"\"\"",
            "    decoder_nonlinearity: Optional[str] = None",
            "    \"\"\"Decoder nonlinearity.\"\"\"",
            "    decoder_proj_size: Optional[int] = None",
            "    \"\"\"Decoder projection size.\"\"\"",
            "    # Adapter FF NN:",
            "    adapter_hidden_dims: List[int] = dataclasses.field(default_factory=lambda: [50])",
            "    \"\"\"Adapter hidden dimensions, as a list of integers corresponding to different layers.\"\"\"",
            "    adapter_out_activation: Optional[str] = \"Tanh\"",
            "    \"\"\"Adapter output activation function.\"\"\"",
            "    # Predictor FF NN:",
            "    predictor_hidden_dims: List[int] = dataclasses.field(default_factory=lambda: [])",
            "    \"\"\"Predictor hidden dimensions, as a list of integers corresponding to different layers.\"\"\"",
            "    predictor_out_activation: Optional[str] = None",
            "    \"\"\"Predictor output activation function.\"\"\"",
            "    # Misc:",
            "    max_len: Optional[int] = None",
            "    \"\"\"Maximum length of the input sequence.\"\"\"",
            "    optimizer_str: str = \"Adam\"",
            "    \"\"\"Optimizer type.\"\"\"",
            "    optimizer_kwargs: Dict[str, Any] = dataclasses.field(default_factory=lambda: dict(lr=0.01, weight_decay=1e-5))",
            "    \"\"\"Optimizer kwargs.\"\"\"",
            "    batch_size: int = 32",
            "    \"\"\"Batch size.\"\"\"",
            "    epochs: int = 100",
            "    \"\"\"Number of epochs.\"\"\"",
            "    padding_indicator: float = DEFAULT_PADDING_INDICATOR",
            "    \"\"\"Padding indicator value.\"\"\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "6": []
        },
        "addLocation": []
    },
    "src/tempor/methods/core/params.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 168,
                "PatchRowcode": "         return [self.name, self.low, self.high, self.step]"
            },
            "1": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": 169,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 170,
                "PatchRowcode": "     def _sample_optuna_trial(self, trial: Trial) -> Any:"
            },
            "3": {
                "beforePatchRowNumber": 171,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return trial.suggest_int(self.name, self.low, self.high, self.step)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 171,
                "PatchRowcode": "+        return trial.suggest_int(self.name, self.low, self.high, step=self.step)"
            },
            "5": {
                "beforePatchRowNumber": 172,
                "afterPatchRowNumber": 172,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 173,
                "afterPatchRowNumber": 173,
                "PatchRowcode": "     def _sample_default(self) -> Any:"
            },
            "7": {
                "beforePatchRowNumber": 174,
                "afterPatchRowNumber": 174,
                "PatchRowcode": "         return random.SystemRandom().choice(self.choices)"
            }
        },
        "frontPatchFile": [
            "\"\"\"Module defining `Params` classes used for sampling in hyperparameter tuning.\"\"\"",
            "",
            "import abc",
            "import random",
            "from typing import Any, Generator, List, Optional, Tuple",
            "",
            "import rich.pretty",
            "from optuna.trial import Trial",
            "",
            "RESERVED_ARG_NAMES = (\"trail\", \"override\")",
            "",
            "",
            "class Params(abc.ABC):",
            "    def __init__(self, name: str, bounds: Tuple[Any, Any]) -> None:",
            "        \"\"\"Abstract base class for all hyperparameter sampling classes. A helper for describing the hyperparameters for",
            "        each estimator.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            bounds (Tuple[Any, Any]): The bounds (lower, higher) of the hyperparameter.",
            "        \"\"\"",
            "        if name in RESERVED_ARG_NAMES:",
            "            raise ValueError(f\"The hyperparameter name '{name}' is not allowed, as it is a special argument\")",
            "        self.name = name",
            "        self.bounds = bounds",
            "",
            "    @abc.abstractmethod",
            "    def get(self) -> List[Any]:  # pragma: no cover",
            "        \"\"\"Returns the hyperparameter name and properties as a list.",
            "",
            "        Returns:",
            "            List[Any]: The hyperparameter name and properties as a list.",
            "        \"\"\"",
            "        ...  # pylint: disable=unnecessary-ellipsis",
            "",
            "    def sample(self, trial: Optional[Trial] = None) -> Any:",
            "        \"\"\"Sample the hyperparameter. If `trial` is not `None`, dispatch to ``_sample_optuna_trial``. Otherwise,",
            "        dispatch to ``_sample_default``.",
            "",
            "        Args:",
            "            trial (Optional[Trial], optional): Trial object, e.g `optuna.trial`. Defaults to None.",
            "",
            "        Returns:",
            "            Any: The sampled hyperparameter.",
            "        \"\"\"",
            "        if trial is not None:",
            "            return self._sample_optuna_trial(trial)",
            "        else:",
            "            return self._sample_default()",
            "        # NOTE: Could support more parameter sampling implementations.",
            "",
            "    @abc.abstractmethod",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:  # pragma: no cover",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def _sample_default(self) -> Any:  # pragma: no cover",
            "        ...",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"bounds\", self.bounds",
            "",
            "    def __repr__(self) -> str:",
            "        \"\"\"The `repr()` representation of the class.",
            "",
            "        Returns:",
            "            str: The representation.",
            "        \"\"\"",
            "        return rich.pretty.pretty_repr(self)",
            "",
            "",
            "class CategoricalParams(Params):",
            "    def __init__(self, name: str, choices: List[Any]) -> None:",
            "        \"\"\"Sample from a categorical distribution.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            choices (List[Any]): The choices to sample from.",
            "        \"\"\"",
            "        super().__init__(name, (min(choices), max(choices)))",
            "        self.name = name",
            "        self.choices = choices",
            "",
            "    def get(self) -> List[Any]:  # noqa: D102",
            "        return [self.name, self.choices]",
            "",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:",
            "        return trial.suggest_categorical(self.name, self.choices)",
            "",
            "    def _sample_default(self) -> Any:",
            "        return random.SystemRandom().choice(self.choices)",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"choices\", self.choices",
            "",
            "",
            "class FloatParams(Params):",
            "    def __init__(self, name: str, low: float, high: float) -> None:",
            "        \"\"\"Sample from a float distribution.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            low (float): Lower bound.",
            "            high (float): Upper bound.",
            "        \"\"\"",
            "        low = float(low)",
            "        high = float(high)",
            "",
            "        super().__init__(name, (low, high))",
            "        self.name = name",
            "        self.low = low",
            "        self.high = high",
            "",
            "    def get(self) -> List[Any]:  # noqa: D102",
            "        return [self.name, self.low, self.high]",
            "",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:",
            "        return trial.suggest_float(self.name, self.low, self.high)",
            "",
            "    def _sample_default(self) -> Any:",
            "        return random.uniform(self.low, self.high)  # nosec",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"low\", self.low",
            "        yield \"high\", self.high",
            "",
            "",
            "class IntegerParams(Params):",
            "    def __init__(self, name: str, low: int, high: int, step: int = 1) -> None:",
            "        \"\"\"Sample from an integer distribution.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            low (int): Lower bound.",
            "            high (int): Upper bound.",
            "            step (int, optional): Step. Defaults to ``1``.",
            "        \"\"\"",
            "        self.low = low",
            "        self.high = high",
            "        self.step = step",
            "",
            "        super().__init__(name, (low, high))",
            "        self.name = name",
            "        self.low = low",
            "        self.high = high",
            "        self.step = step",
            "        self.choices = [val for val in range(low, high + 1, step)]",
            "",
            "    def get(self) -> List[Any]:  # noqa: D102",
            "        return [self.name, self.low, self.high, self.step]",
            "",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:",
            "        return trial.suggest_int(self.name, self.low, self.high, self.step)",
            "",
            "    def _sample_default(self) -> Any:",
            "        return random.SystemRandom().choice(self.choices)",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"low\", self.low",
            "        yield \"high\", self.high",
            "        yield \"step\", self.step"
        ],
        "afterPatchFile": [
            "\"\"\"Module defining `Params` classes used for sampling in hyperparameter tuning.\"\"\"",
            "",
            "import abc",
            "import random",
            "from typing import Any, Generator, List, Optional, Tuple",
            "",
            "import rich.pretty",
            "from optuna.trial import Trial",
            "",
            "RESERVED_ARG_NAMES = (\"trail\", \"override\")",
            "",
            "",
            "class Params(abc.ABC):",
            "    def __init__(self, name: str, bounds: Tuple[Any, Any]) -> None:",
            "        \"\"\"Abstract base class for all hyperparameter sampling classes. A helper for describing the hyperparameters for",
            "        each estimator.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            bounds (Tuple[Any, Any]): The bounds (lower, higher) of the hyperparameter.",
            "        \"\"\"",
            "        if name in RESERVED_ARG_NAMES:",
            "            raise ValueError(f\"The hyperparameter name '{name}' is not allowed, as it is a special argument\")",
            "        self.name = name",
            "        self.bounds = bounds",
            "",
            "    @abc.abstractmethod",
            "    def get(self) -> List[Any]:  # pragma: no cover",
            "        \"\"\"Returns the hyperparameter name and properties as a list.",
            "",
            "        Returns:",
            "            List[Any]: The hyperparameter name and properties as a list.",
            "        \"\"\"",
            "        ...  # pylint: disable=unnecessary-ellipsis",
            "",
            "    def sample(self, trial: Optional[Trial] = None) -> Any:",
            "        \"\"\"Sample the hyperparameter. If `trial` is not `None`, dispatch to ``_sample_optuna_trial``. Otherwise,",
            "        dispatch to ``_sample_default``.",
            "",
            "        Args:",
            "            trial (Optional[Trial], optional): Trial object, e.g `optuna.trial`. Defaults to None.",
            "",
            "        Returns:",
            "            Any: The sampled hyperparameter.",
            "        \"\"\"",
            "        if trial is not None:",
            "            return self._sample_optuna_trial(trial)",
            "        else:",
            "            return self._sample_default()",
            "        # NOTE: Could support more parameter sampling implementations.",
            "",
            "    @abc.abstractmethod",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:  # pragma: no cover",
            "        ...",
            "",
            "    @abc.abstractmethod",
            "    def _sample_default(self) -> Any:  # pragma: no cover",
            "        ...",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"bounds\", self.bounds",
            "",
            "    def __repr__(self) -> str:",
            "        \"\"\"The `repr()` representation of the class.",
            "",
            "        Returns:",
            "            str: The representation.",
            "        \"\"\"",
            "        return rich.pretty.pretty_repr(self)",
            "",
            "",
            "class CategoricalParams(Params):",
            "    def __init__(self, name: str, choices: List[Any]) -> None:",
            "        \"\"\"Sample from a categorical distribution.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            choices (List[Any]): The choices to sample from.",
            "        \"\"\"",
            "        super().__init__(name, (min(choices), max(choices)))",
            "        self.name = name",
            "        self.choices = choices",
            "",
            "    def get(self) -> List[Any]:  # noqa: D102",
            "        return [self.name, self.choices]",
            "",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:",
            "        return trial.suggest_categorical(self.name, self.choices)",
            "",
            "    def _sample_default(self) -> Any:",
            "        return random.SystemRandom().choice(self.choices)",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"choices\", self.choices",
            "",
            "",
            "class FloatParams(Params):",
            "    def __init__(self, name: str, low: float, high: float) -> None:",
            "        \"\"\"Sample from a float distribution.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            low (float): Lower bound.",
            "            high (float): Upper bound.",
            "        \"\"\"",
            "        low = float(low)",
            "        high = float(high)",
            "",
            "        super().__init__(name, (low, high))",
            "        self.name = name",
            "        self.low = low",
            "        self.high = high",
            "",
            "    def get(self) -> List[Any]:  # noqa: D102",
            "        return [self.name, self.low, self.high]",
            "",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:",
            "        return trial.suggest_float(self.name, self.low, self.high)",
            "",
            "    def _sample_default(self) -> Any:",
            "        return random.uniform(self.low, self.high)  # nosec",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"low\", self.low",
            "        yield \"high\", self.high",
            "",
            "",
            "class IntegerParams(Params):",
            "    def __init__(self, name: str, low: int, high: int, step: int = 1) -> None:",
            "        \"\"\"Sample from an integer distribution.",
            "",
            "        Args:",
            "            name (str): Hyperparameter name.",
            "            low (int): Lower bound.",
            "            high (int): Upper bound.",
            "            step (int, optional): Step. Defaults to ``1``.",
            "        \"\"\"",
            "        self.low = low",
            "        self.high = high",
            "        self.step = step",
            "",
            "        super().__init__(name, (low, high))",
            "        self.name = name",
            "        self.low = low",
            "        self.high = high",
            "        self.step = step",
            "        self.choices = [val for val in range(low, high + 1, step)]",
            "",
            "    def get(self) -> List[Any]:  # noqa: D102",
            "        return [self.name, self.low, self.high, self.step]",
            "",
            "    def _sample_optuna_trial(self, trial: Trial) -> Any:",
            "        return trial.suggest_int(self.name, self.low, self.high, step=self.step)",
            "",
            "    def _sample_default(self) -> Any:",
            "        return random.SystemRandom().choice(self.choices)",
            "",
            "    def __rich_repr__(self) -> Generator:",
            "        \"\"\"A `rich` representation of the class.",
            "",
            "        Yields:",
            "            Generator: The fields and their values fed to `rich`.",
            "        \"\"\"",
            "        yield \"name\", self.name",
            "        yield \"low\", self.low",
            "        yield \"high\", self.high",
            "        yield \"step\", self.step"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "171": [
                "IntegerParams",
                "_sample_optuna_trial"
            ]
        },
        "addLocation": []
    },
    "src/tempor/methods/prediction/temporal/classification/plugin_seq2seq_classifier.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from typing import Any, List, cast"
            },
            "2": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import clairvoyance2.data.dataformat as cl_dataformat"
            },
            "4": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from clairvoyance2.prediction.seq2seq import Seq2SeqClassifier, TimeIndexHorizon"
            },
            "5": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from typing_extensions import Self"
            },
            "6": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+import tempor.models.clairvoyance2.data.dataformat as cl_dataformat"
            },
            "8": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from tempor.core import plugins"
            },
            "9": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from tempor.data import dataset, samples"
            },
            "10": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from tempor.data.clv2conv import _from_clv2_time_series, tempor_dataset_to_clairvoyance2_dataset"
            },
            "11": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from tempor.methods.constants import Seq2seqParams"
            },
            "12": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " from tempor.methods.core.params import CategoricalParams, FloatParams, IntegerParams, Params"
            },
            "13": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " from tempor.methods.prediction.temporal.classification import BaseTemporalClassifier"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 14,
                "PatchRowcode": "+from tempor.models.clairvoyance2.interface.horizon import TimeIndexHorizon"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 15,
                "PatchRowcode": "+from tempor.models.clairvoyance2.prediction.seq2seq import Seq2SeqClassifier"
            },
            "16": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " @plugins.register_plugin(name=\"seq2seq_classifier\", category=\"prediction.temporal.classification\")"
            },
            "19": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "         \"\"\""
            },
            "20": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 58,
                "PatchRowcode": "         super().__init__(**params)"
            },
            "21": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 59,
                "PatchRowcode": "         self.model = Seq2SeqClassifier("
            },
            "22": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            params=self.params,  # pyright: ignore"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+            params=self.params,  # type: ignore [arg-type]  # pyright: ignore"
            },
            "24": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "         )"
            },
            "25": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 62,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "     def _fit("
            }
        },
        "frontPatchFile": [
            "\"\"\"Temporal classification estimator based on Seq2Seq model.\"\"\"",
            "",
            "from typing import Any, List, cast",
            "",
            "import clairvoyance2.data.dataformat as cl_dataformat",
            "from clairvoyance2.prediction.seq2seq import Seq2SeqClassifier, TimeIndexHorizon",
            "from typing_extensions import Self",
            "",
            "from tempor.core import plugins",
            "from tempor.data import dataset, samples",
            "from tempor.data.clv2conv import _from_clv2_time_series, tempor_dataset_to_clairvoyance2_dataset",
            "from tempor.methods.constants import Seq2seqParams",
            "from tempor.methods.core.params import CategoricalParams, FloatParams, IntegerParams, Params",
            "from tempor.methods.prediction.temporal.classification import BaseTemporalClassifier",
            "",
            "",
            "@plugins.register_plugin(name=\"seq2seq_classifier\", category=\"prediction.temporal.classification\")",
            "class Seq2seqClassifier(BaseTemporalClassifier):",
            "    ParamsDefinition = Seq2seqParams",
            "    params: Seq2seqParams  # type: ignore",
            "",
            "    def __init__(",
            "        self,",
            "        **params: Any,",
            "    ) -> None:",
            "        \"\"\"Seq2seq classifier.",
            "",
            "        Args:",
            "            **params (Any):",
            "                Parameters and defaults as defined in :class:`Seq2seqParams`.",
            "",
            "        Example:",
            "            >>> import doctest; doctest.ELLIPSIS_MARKER = \"[...]\"  # Doctest config, ignore.",
            "            >>>",
            "            >>> from tempor.data import dataset",
            "            >>> from tempor import plugin_loader",
            "            >>>",
            "            >>> raw_data = plugin_loader.get(\"prediction.one_off.sine\", plugin_type=\"datasource\", temporal_dim=5).load()",
            "            >>> data = dataset.TemporalPredictionDataset(",
            "            ...    time_series=raw_data.time_series.dataframe(),",
            "            ...    static=raw_data.static.dataframe(),",
            "            ...    targets=raw_data.time_series.dataframe(),",
            "            ... )",
            "            >>>",
            "            >>> # Load the model:",
            "            >>> model = plugin_loader.get(\"prediction.temporal.classification.seq2seq_classifier\", epochs=10)",
            "            >>>",
            "            >>> # Train:",
            "            >>> model.fit(data)",
            "            [...]",
            "            >>>",
            "            >>> # Predict:",
            "            >>> assert model.predict(data, n_future_steps = 10).numpy().shape == (len(data), 10, 5)",
            "            >>>",
            "            >>> doctest.ELLIPSIS_MARKER = \"...\"  # Doctest config, ignore.",
            "        \"\"\"",
            "        super().__init__(**params)",
            "        self.model = Seq2SeqClassifier(",
            "            params=self.params,  # pyright: ignore",
            "        )",
            "",
            "    def _fit(",
            "        self,",
            "        data: dataset.BaseDataset,",
            "        *args: Any,",
            "        **kwargs: Any,",
            "    ) -> Self:",
            "        cl_dataset = tempor_dataset_to_clairvoyance2_dataset(data)",
            "        self.model.fit(cl_dataset, **kwargs)",
            "        return self",
            "",
            "    def _predict(",
            "        self,",
            "        data: dataset.PredictiveDataset,",
            "        n_future_steps: int,",
            "        *args: Any,",
            "        time_delta: int = 1,",
            "        **kwargs: Any,",
            "    ) -> samples.TimeSeriesSamplesBase:",
            "        if self.model is None:",
            "            raise RuntimeError(\"Fit the model first\")",
            "        cl_dataset = tempor_dataset_to_clairvoyance2_dataset(data)",
            "",
            "        horizons = TimeIndexHorizon.future_horizon_from_dataset(",
            "            cl_dataset,",
            "            forecast_n_future_steps=n_future_steps,",
            "            time_delta=time_delta,",
            "        )",
            "",
            "        preds_cl = cast(cl_dataformat.TimeSeriesSamples, self.model.predict(cl_dataset, horizons))",
            "        preds = _from_clv2_time_series(preds_cl.to_multi_index_dataframe())",
            "        return samples.TimeSeriesSamples.from_dataframe(preds)",
            "",
            "    def _predict_proba(",
            "        self,",
            "        data: dataset.PredictiveDataset,",
            "        *args: Any,",
            "        **kwargs: Any,",
            "    ) -> samples.TimeSeriesSamplesBase:",
            "        raise NotImplementedError(\"Not currently supported\")",
            "",
            "    @staticmethod",
            "    def hyperparameter_space(*args: Any, **kwargs: Any) -> List[Params]:  # noqa: D102",
            "        return [",
            "            IntegerParams(name=\"encoder_hidden_size\", low=10, high=500),",
            "            IntegerParams(name=\"encoder_num_layers\", low=1, high=10),",
            "            FloatParams(name=\"encoder_dropout\", low=0, high=0.2),",
            "            CategoricalParams(name=\"encoder_rnn_type\", choices=[\"LSTM\", \"GRU\", \"RNN\"]),",
            "            IntegerParams(name=\"decoder_hidden_size\", low=10, high=500),",
            "            IntegerParams(name=\"decoder_num_layers\", low=1, high=10),",
            "            FloatParams(name=\"decoder_dropout\", low=0, high=0.2),",
            "            CategoricalParams(name=\"decoder_rnn_type\", choices=[\"LSTM\", \"GRU\", \"RNN\"]),",
            "        ]"
        ],
        "afterPatchFile": [
            "\"\"\"Temporal classification estimator based on Seq2Seq model.\"\"\"",
            "",
            "from typing import Any, List, cast",
            "",
            "from typing_extensions import Self",
            "",
            "import tempor.models.clairvoyance2.data.dataformat as cl_dataformat",
            "from tempor.core import plugins",
            "from tempor.data import dataset, samples",
            "from tempor.data.clv2conv import _from_clv2_time_series, tempor_dataset_to_clairvoyance2_dataset",
            "from tempor.methods.constants import Seq2seqParams",
            "from tempor.methods.core.params import CategoricalParams, FloatParams, IntegerParams, Params",
            "from tempor.methods.prediction.temporal.classification import BaseTemporalClassifier",
            "from tempor.models.clairvoyance2.interface.horizon import TimeIndexHorizon",
            "from tempor.models.clairvoyance2.prediction.seq2seq import Seq2SeqClassifier",
            "",
            "",
            "@plugins.register_plugin(name=\"seq2seq_classifier\", category=\"prediction.temporal.classification\")",
            "class Seq2seqClassifier(BaseTemporalClassifier):",
            "    ParamsDefinition = Seq2seqParams",
            "    params: Seq2seqParams  # type: ignore",
            "",
            "    def __init__(",
            "        self,",
            "        **params: Any,",
            "    ) -> None:",
            "        \"\"\"Seq2seq classifier.",
            "",
            "        Args:",
            "            **params (Any):",
            "                Parameters and defaults as defined in :class:`Seq2seqParams`.",
            "",
            "        Example:",
            "            >>> import doctest; doctest.ELLIPSIS_MARKER = \"[...]\"  # Doctest config, ignore.",
            "            >>>",
            "            >>> from tempor.data import dataset",
            "            >>> from tempor import plugin_loader",
            "            >>>",
            "            >>> raw_data = plugin_loader.get(\"prediction.one_off.sine\", plugin_type=\"datasource\", temporal_dim=5).load()",
            "            >>> data = dataset.TemporalPredictionDataset(",
            "            ...    time_series=raw_data.time_series.dataframe(),",
            "            ...    static=raw_data.static.dataframe(),",
            "            ...    targets=raw_data.time_series.dataframe(),",
            "            ... )",
            "            >>>",
            "            >>> # Load the model:",
            "            >>> model = plugin_loader.get(\"prediction.temporal.classification.seq2seq_classifier\", epochs=10)",
            "            >>>",
            "            >>> # Train:",
            "            >>> model.fit(data)",
            "            [...]",
            "            >>>",
            "            >>> # Predict:",
            "            >>> assert model.predict(data, n_future_steps = 10).numpy().shape == (len(data), 10, 5)",
            "            >>>",
            "            >>> doctest.ELLIPSIS_MARKER = \"...\"  # Doctest config, ignore.",
            "        \"\"\"",
            "        super().__init__(**params)",
            "        self.model = Seq2SeqClassifier(",
            "            params=self.params,  # type: ignore [arg-type]  # pyright: ignore",
            "        )",
            "",
            "    def _fit(",
            "        self,",
            "        data: dataset.BaseDataset,",
            "        *args: Any,",
            "        **kwargs: Any,",
            "    ) -> Self:",
            "        cl_dataset = tempor_dataset_to_clairvoyance2_dataset(data)",
            "        self.model.fit(cl_dataset, **kwargs)",
            "        return self",
            "",
            "    def _predict(",
            "        self,",
            "        data: dataset.PredictiveDataset,",
            "        n_future_steps: int,",
            "        *args: Any,",
            "        time_delta: int = 1,",
            "        **kwargs: Any,",
            "    ) -> samples.TimeSeriesSamplesBase:",
            "        if self.model is None:",
            "            raise RuntimeError(\"Fit the model first\")",
            "        cl_dataset = tempor_dataset_to_clairvoyance2_dataset(data)",
            "",
            "        horizons = TimeIndexHorizon.future_horizon_from_dataset(",
            "            cl_dataset,",
            "            forecast_n_future_steps=n_future_steps,",
            "            time_delta=time_delta,",
            "        )",
            "",
            "        preds_cl = cast(cl_dataformat.TimeSeriesSamples, self.model.predict(cl_dataset, horizons))",
            "        preds = _from_clv2_time_series(preds_cl.to_multi_index_dataframe())",
            "        return samples.TimeSeriesSamples.from_dataframe(preds)",
            "",
            "    def _predict_proba(",
            "        self,",
            "        data: dataset.PredictiveDataset,",
            "        *args: Any,",
            "        **kwargs: Any,",
            "    ) -> samples.TimeSeriesSamplesBase:",
            "        raise NotImplementedError(\"Not currently supported\")",
            "",
            "    @staticmethod",
            "    def hyperparameter_space(*args: Any, **kwargs: Any) -> List[Params]:  # noqa: D102",
            "        return [",
            "            IntegerParams(name=\"encoder_hidden_size\", low=10, high=500),",
            "            IntegerParams(name=\"encoder_num_layers\", low=1, high=10),",
            "            FloatParams(name=\"encoder_dropout\", low=0, high=0.2),",
            "            CategoricalParams(name=\"encoder_rnn_type\", choices=[\"LSTM\", \"GRU\", \"RNN\"]),",
            "            IntegerParams(name=\"decoder_hidden_size\", low=10, high=500),",
            "            IntegerParams(name=\"decoder_num_layers\", low=1, high=10),",
            "            FloatParams(name=\"decoder_dropout\", low=0, high=0.2),",
            "            CategoricalParams(name=\"decoder_rnn_type\", choices=[\"LSTM\", \"GRU\", \"RNN\"]),",
            "        ]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "1",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "5": [],
            "6": [],
            "59": [
                "Seq2seqClassifier",
                "__init__"
            ]
        },
        "addLocation": []
    }
}