{
    "setup.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "         \"slack_sdk>=3.19.0, <4\","
            },
            "1": {
                "beforePatchRowNumber": 126,
                "afterPatchRowNumber": 126,
                "PatchRowcode": "         \"sqlalchemy>=1.4, <2\","
            },
            "2": {
                "beforePatchRowNumber": 127,
                "afterPatchRowNumber": 127,
                "PatchRowcode": "         \"sqlalchemy-utils>=0.38.3, <0.39\","
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        \"sqlglot>=20,<21\","
            },
            "4": {
                "beforePatchRowNumber": 128,
                "afterPatchRowNumber": 129,
                "PatchRowcode": "         \"sqlparse>=0.4.4, <0.5\","
            },
            "5": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "         \"tabulate>=0.8.9, <0.9\","
            },
            "6": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "         \"typing-extensions>=4, <5\","
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import json",
            "import os",
            "import subprocess",
            "",
            "from setuptools import find_packages, setup",
            "",
            "BASE_DIR = os.path.abspath(os.path.dirname(__file__))",
            "PACKAGE_JSON = os.path.join(BASE_DIR, \"superset-frontend\", \"package.json\")",
            "",
            "with open(PACKAGE_JSON) as package_file:",
            "    version_string = json.load(package_file)[\"version\"]",
            "",
            "with open(\"README.md\", encoding=\"utf-8\") as f:",
            "    long_description = f.read()",
            "",
            "",
            "def get_git_sha() -> str:",
            "    try:",
            "        s = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])",
            "        return s.decode().strip()",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "GIT_SHA = get_git_sha()",
            "version_info = {\"GIT_SHA\": GIT_SHA, \"version\": version_string}",
            "print(\"-==-\" * 15)",
            "print(\"VERSION: \" + version_string)",
            "print(\"GIT SHA: \" + GIT_SHA)",
            "print(\"-==-\" * 15)",
            "",
            "VERSION_INFO_FILE = os.path.join(BASE_DIR, \"superset\", \"static\", \"version_info.json\")",
            "",
            "with open(VERSION_INFO_FILE, \"w\") as version_file:",
            "    json.dump(version_info, version_file)",
            "",
            "setup(",
            "    name=\"apache-superset\",",
            "    description=\"A modern, enterprise-ready business intelligence web application\",",
            "    long_description=long_description,",
            "    long_description_content_type=\"text/markdown\",",
            "    version=version_string,",
            "    packages=find_packages(),",
            "    include_package_data=True,",
            "    zip_safe=False,",
            "    entry_points={",
            "        \"console_scripts\": [\"superset=superset.cli.main:superset\"],",
            "        # the `postgres` and `postgres+psycopg2://` schemes were removed in SQLAlchemy 1.4",
            "        # add an alias here to prevent breaking existing databases",
            "        \"sqlalchemy.dialects\": [",
            "            \"postgres.psycopg2 = sqlalchemy.dialects.postgresql:dialect\",",
            "            \"postgres = sqlalchemy.dialects.postgresql:dialect\",",
            "            \"superset = superset.extensions.metadb:SupersetAPSWDialect\",",
            "        ],",
            "        \"shillelagh.adapter\": [",
            "            \"superset=superset.extensions.metadb:SupersetShillelaghAdapter\"",
            "        ],",
            "    },",
            "    install_requires=[",
            "        \"backoff>=1.8.0\",",
            "        \"celery>=5.2.2, <6.0.0\",",
            "        \"click>=8.0.3\",",
            "        \"click-option-group\",",
            "        \"colorama\",",
            "        \"croniter>=0.3.28\",",
            "        \"cron-descriptor\",",
            "        \"cryptography>=41.0.2, <41.1.0\",",
            "        \"deprecation>=2.1.0, <2.2.0\",",
            "        \"flask>=2.2.5, <3.0.0\",",
            "        \"flask-appbuilder>=4.3.10, <5.0.0\",",
            "        \"flask-caching>=2.1.0, <3\",",
            "        \"flask-compress>=1.13, <2.0\",",
            "        \"flask-talisman>=1.0.0, <2.0\",",
            "        \"flask-login>=0.6.0, < 1.0\",",
            "        \"flask-migrate>=3.1.0, <4.0\",",
            "        \"flask-session>=0.4.0, <1.0\",",
            "        \"flask-wtf>=1.1.0, <2.0\",",
            "        \"func_timeout\",",
            "        \"geopy\",",
            "        \"gunicorn>=21.2.0, <22.0; sys_platform != 'win32'\",",
            "        \"hashids>=1.3.1, <2\",",
            "        \"holidays>=0.25, <0.26\",",
            "        \"humanize\",",
            "        \"importlib_metadata\",",
            "        \"isodate\",",
            "        \"Mako>=1.2.2\",",
            "        \"markdown>=3.0\",",
            "        \"msgpack>=1.0.0, <1.1\",",
            "        \"nh3>=0.2.11, <0.3\",",
            "        \"numpy==1.23.5\",",
            "        \"packaging\",",
            "        \"pandas[performance]>=2.0.3, <2.1\",",
            "        \"parsedatetime\",",
            "        \"pgsanity\",",
            "        \"polyline>=2.0.0, <3.0\",",
            "        \"pyparsing>=3.0.6, <4\",",
            "        \"python-dateutil\",",
            "        \"python-dotenv\",",
            "        \"python-geohash\",",
            "        \"pyarrow>=14.0.1, <15\",",
            "        \"pyyaml>=6.0.0, <7.0.0\",",
            "        \"PyJWT>=2.4.0, <3.0\",",
            "        \"redis>=4.5.4, <5.0\",",
            "        \"selenium>=3.141.0, <4.10.0\",",
            "        \"shillelagh>=1.2.10, <2.0\",",
            "        \"shortid\",",
            "        \"sshtunnel>=0.4.0, <0.5\",",
            "        \"simplejson>=3.15.0\",",
            "        \"slack_sdk>=3.19.0, <4\",",
            "        \"sqlalchemy>=1.4, <2\",",
            "        \"sqlalchemy-utils>=0.38.3, <0.39\",",
            "        \"sqlparse>=0.4.4, <0.5\",",
            "        \"tabulate>=0.8.9, <0.9\",",
            "        \"typing-extensions>=4, <5\",",
            "        \"waitress; sys_platform == 'win32'\",",
            "        \"werkzeug>=2.3.3, <3\",",
            "        \"wtforms>=2.3.3, <4\",",
            "        \"wtforms-json\",",
            "        \"xlsxwriter>=3.0.7, <3.1\",",
            "    ],",
            "    extras_require={",
            "        \"athena\": [\"pyathena[pandas]>=2, <3\"],",
            "        \"aurora-data-api\": [\"preset-sqlalchemy-aurora-data-api>=0.2.8,<0.3\"],",
            "        \"bigquery\": [",
            "            \"pandas-gbq>=0.19.1\",",
            "            \"sqlalchemy-bigquery>=1.6.1\",",
            "            \"google-cloud-bigquery>=3.10.0\",",
            "        ],",
            "        \"clickhouse\": [\"clickhouse-connect>=0.5.14, <1.0\"],",
            "        \"cockroachdb\": [\"cockroachdb>=0.3.5, <0.4\"],",
            "        \"cors\": [\"flask-cors>=2.0.0\"],",
            "        \"crate\": [\"crate[sqlalchemy]>=0.26.0, <0.27\"],",
            "        \"databend\": [\"databend-sqlalchemy>=0.3.2, <1.0\"],",
            "        \"databricks\": [",
            "            \"databricks-sql-connector>=2.0.2, <3\",",
            "            \"sqlalchemy-databricks>=0.2.0\",",
            "        ],",
            "        \"db2\": [\"ibm-db-sa>0.3.8, <=0.4.0\"],",
            "        \"dremio\": [\"sqlalchemy-dremio>=1.1.5, <1.3\"],",
            "        \"drill\": [\"sqlalchemy-drill==0.1.dev\"],",
            "        \"druid\": [\"pydruid>=0.6.5,<0.7\"],",
            "        \"duckdb\": [\"duckdb-engine>=0.9.5, <0.10\"],",
            "        \"dynamodb\": [\"pydynamodb>=0.4.2\"],",
            "        \"solr\": [\"sqlalchemy-solr >= 0.2.0\"],",
            "        \"elasticsearch\": [\"elasticsearch-dbapi>=0.2.9, <0.3.0\"],",
            "        \"exasol\": [\"sqlalchemy-exasol >= 2.4.0, <3.0\"],",
            "        \"excel\": [\"xlrd>=1.2.0, <1.3\"],",
            "        \"firebird\": [\"sqlalchemy-firebird>=0.7.0, <0.8\"],",
            "        \"firebolt\": [\"firebolt-sqlalchemy>=0.0.1\"],",
            "        \"gsheets\": [\"shillelagh[gsheetsapi]>=1.2.10, <2\"],",
            "        \"hana\": [\"hdbcli==2.4.162\", \"sqlalchemy_hana==0.4.0\"],",
            "        \"hive\": [",
            "            \"pyhive[hive]>=0.6.5;python_version<'3.11'\",",
            "            \"pyhive[hive_pure_sasl]>=0.7.0\",",
            "            \"tableschema\",",
            "            \"thrift>=0.14.1, <1.0.0\",",
            "        ],",
            "        \"impala\": [\"impyla>0.16.2, <0.17\"],",
            "        \"kusto\": [\"sqlalchemy-kusto>=2.0.0, <3\"],",
            "        \"kylin\": [\"kylinpy>=2.8.1, <2.9\"],",
            "        \"mssql\": [\"pymssql>=2.2.8, <3\"],",
            "        \"mysql\": [\"mysqlclient>=2.1.0, <3\"],",
            "        \"ocient\": [",
            "            \"sqlalchemy-ocient>=1.0.0\",",
            "            \"pyocient>=1.0.15, <2\",",
            "            \"shapely\",",
            "            \"geojson\",",
            "        ],",
            "        \"oracle\": [\"cx-Oracle>8.0.0, <8.1\"],",
            "        \"pinot\": [\"pinotdb>=0.3.3, <0.4\"],",
            "        \"playwright\": [\"playwright>=1.37.0, <2\"],",
            "        \"postgres\": [\"psycopg2-binary==2.9.6\"],",
            "        \"presto\": [\"pyhive[presto]>=0.6.5\"],",
            "        \"trino\": [\"trino>=0.324.0\"],",
            "        \"prophet\": [\"prophet>=1.1.5, <2\"],",
            "        \"redshift\": [\"sqlalchemy-redshift>=0.8.1, <0.9\"],",
            "        \"rockset\": [\"rockset-sqlalchemy>=0.0.1, <1\"],",
            "        \"shillelagh\": [",
            "            \"shillelagh[datasetteapi,gsheetsapi,socrata,weatherapi]>=1.2.10, <2\"",
            "        ],",
            "        \"snowflake\": [\"snowflake-sqlalchemy>=1.2.4, <2\"],",
            "        \"spark\": [",
            "            \"pyhive[hive]>=0.6.5;python_version<'3.11'\",",
            "            \"pyhive[hive_pure_sasl]>=0.7\",",
            "            \"tableschema\",",
            "            \"thrift>=0.14.1, <1\",",
            "        ],",
            "        \"teradata\": [\"teradatasql>=16.20.0.23\"],",
            "        \"thumbnails\": [\"Pillow>=10.0.1, <11\"],",
            "        \"vertica\": [\"sqlalchemy-vertica-python>=0.5.9, < 0.6\"],",
            "        \"netezza\": [\"nzalchemy>=11.0.2\"],",
            "        \"starrocks\": [\"starrocks>=1.0.0\"],",
            "        \"doris\": [\"pydoris>=1.0.0, <2.0.0\"],",
            "    },",
            "    python_requires=\"~=3.9\",",
            "    author=\"Apache Software Foundation\",",
            "    author_email=\"dev@superset.apache.org\",",
            "    url=\"https://superset.apache.org/\",",
            "    download_url=\"https://www.apache.org/dist/superset/\" + version_string,",
            "    classifiers=[",
            "        \"Programming Language :: Python :: 3.9\",",
            "        \"Programming Language :: Python :: 3.10\",",
            "        \"Programming Language :: Python :: 3.11\",",
            "    ],",
            ")"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import json",
            "import os",
            "import subprocess",
            "",
            "from setuptools import find_packages, setup",
            "",
            "BASE_DIR = os.path.abspath(os.path.dirname(__file__))",
            "PACKAGE_JSON = os.path.join(BASE_DIR, \"superset-frontend\", \"package.json\")",
            "",
            "with open(PACKAGE_JSON) as package_file:",
            "    version_string = json.load(package_file)[\"version\"]",
            "",
            "with open(\"README.md\", encoding=\"utf-8\") as f:",
            "    long_description = f.read()",
            "",
            "",
            "def get_git_sha() -> str:",
            "    try:",
            "        s = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])",
            "        return s.decode().strip()",
            "    except Exception:",
            "        return \"\"",
            "",
            "",
            "GIT_SHA = get_git_sha()",
            "version_info = {\"GIT_SHA\": GIT_SHA, \"version\": version_string}",
            "print(\"-==-\" * 15)",
            "print(\"VERSION: \" + version_string)",
            "print(\"GIT SHA: \" + GIT_SHA)",
            "print(\"-==-\" * 15)",
            "",
            "VERSION_INFO_FILE = os.path.join(BASE_DIR, \"superset\", \"static\", \"version_info.json\")",
            "",
            "with open(VERSION_INFO_FILE, \"w\") as version_file:",
            "    json.dump(version_info, version_file)",
            "",
            "setup(",
            "    name=\"apache-superset\",",
            "    description=\"A modern, enterprise-ready business intelligence web application\",",
            "    long_description=long_description,",
            "    long_description_content_type=\"text/markdown\",",
            "    version=version_string,",
            "    packages=find_packages(),",
            "    include_package_data=True,",
            "    zip_safe=False,",
            "    entry_points={",
            "        \"console_scripts\": [\"superset=superset.cli.main:superset\"],",
            "        # the `postgres` and `postgres+psycopg2://` schemes were removed in SQLAlchemy 1.4",
            "        # add an alias here to prevent breaking existing databases",
            "        \"sqlalchemy.dialects\": [",
            "            \"postgres.psycopg2 = sqlalchemy.dialects.postgresql:dialect\",",
            "            \"postgres = sqlalchemy.dialects.postgresql:dialect\",",
            "            \"superset = superset.extensions.metadb:SupersetAPSWDialect\",",
            "        ],",
            "        \"shillelagh.adapter\": [",
            "            \"superset=superset.extensions.metadb:SupersetShillelaghAdapter\"",
            "        ],",
            "    },",
            "    install_requires=[",
            "        \"backoff>=1.8.0\",",
            "        \"celery>=5.2.2, <6.0.0\",",
            "        \"click>=8.0.3\",",
            "        \"click-option-group\",",
            "        \"colorama\",",
            "        \"croniter>=0.3.28\",",
            "        \"cron-descriptor\",",
            "        \"cryptography>=41.0.2, <41.1.0\",",
            "        \"deprecation>=2.1.0, <2.2.0\",",
            "        \"flask>=2.2.5, <3.0.0\",",
            "        \"flask-appbuilder>=4.3.10, <5.0.0\",",
            "        \"flask-caching>=2.1.0, <3\",",
            "        \"flask-compress>=1.13, <2.0\",",
            "        \"flask-talisman>=1.0.0, <2.0\",",
            "        \"flask-login>=0.6.0, < 1.0\",",
            "        \"flask-migrate>=3.1.0, <4.0\",",
            "        \"flask-session>=0.4.0, <1.0\",",
            "        \"flask-wtf>=1.1.0, <2.0\",",
            "        \"func_timeout\",",
            "        \"geopy\",",
            "        \"gunicorn>=21.2.0, <22.0; sys_platform != 'win32'\",",
            "        \"hashids>=1.3.1, <2\",",
            "        \"holidays>=0.25, <0.26\",",
            "        \"humanize\",",
            "        \"importlib_metadata\",",
            "        \"isodate\",",
            "        \"Mako>=1.2.2\",",
            "        \"markdown>=3.0\",",
            "        \"msgpack>=1.0.0, <1.1\",",
            "        \"nh3>=0.2.11, <0.3\",",
            "        \"numpy==1.23.5\",",
            "        \"packaging\",",
            "        \"pandas[performance]>=2.0.3, <2.1\",",
            "        \"parsedatetime\",",
            "        \"pgsanity\",",
            "        \"polyline>=2.0.0, <3.0\",",
            "        \"pyparsing>=3.0.6, <4\",",
            "        \"python-dateutil\",",
            "        \"python-dotenv\",",
            "        \"python-geohash\",",
            "        \"pyarrow>=14.0.1, <15\",",
            "        \"pyyaml>=6.0.0, <7.0.0\",",
            "        \"PyJWT>=2.4.0, <3.0\",",
            "        \"redis>=4.5.4, <5.0\",",
            "        \"selenium>=3.141.0, <4.10.0\",",
            "        \"shillelagh>=1.2.10, <2.0\",",
            "        \"shortid\",",
            "        \"sshtunnel>=0.4.0, <0.5\",",
            "        \"simplejson>=3.15.0\",",
            "        \"slack_sdk>=3.19.0, <4\",",
            "        \"sqlalchemy>=1.4, <2\",",
            "        \"sqlalchemy-utils>=0.38.3, <0.39\",",
            "        \"sqlglot>=20,<21\",",
            "        \"sqlparse>=0.4.4, <0.5\",",
            "        \"tabulate>=0.8.9, <0.9\",",
            "        \"typing-extensions>=4, <5\",",
            "        \"waitress; sys_platform == 'win32'\",",
            "        \"werkzeug>=2.3.3, <3\",",
            "        \"wtforms>=2.3.3, <4\",",
            "        \"wtforms-json\",",
            "        \"xlsxwriter>=3.0.7, <3.1\",",
            "    ],",
            "    extras_require={",
            "        \"athena\": [\"pyathena[pandas]>=2, <3\"],",
            "        \"aurora-data-api\": [\"preset-sqlalchemy-aurora-data-api>=0.2.8,<0.3\"],",
            "        \"bigquery\": [",
            "            \"pandas-gbq>=0.19.1\",",
            "            \"sqlalchemy-bigquery>=1.6.1\",",
            "            \"google-cloud-bigquery>=3.10.0\",",
            "        ],",
            "        \"clickhouse\": [\"clickhouse-connect>=0.5.14, <1.0\"],",
            "        \"cockroachdb\": [\"cockroachdb>=0.3.5, <0.4\"],",
            "        \"cors\": [\"flask-cors>=2.0.0\"],",
            "        \"crate\": [\"crate[sqlalchemy]>=0.26.0, <0.27\"],",
            "        \"databend\": [\"databend-sqlalchemy>=0.3.2, <1.0\"],",
            "        \"databricks\": [",
            "            \"databricks-sql-connector>=2.0.2, <3\",",
            "            \"sqlalchemy-databricks>=0.2.0\",",
            "        ],",
            "        \"db2\": [\"ibm-db-sa>0.3.8, <=0.4.0\"],",
            "        \"dremio\": [\"sqlalchemy-dremio>=1.1.5, <1.3\"],",
            "        \"drill\": [\"sqlalchemy-drill==0.1.dev\"],",
            "        \"druid\": [\"pydruid>=0.6.5,<0.7\"],",
            "        \"duckdb\": [\"duckdb-engine>=0.9.5, <0.10\"],",
            "        \"dynamodb\": [\"pydynamodb>=0.4.2\"],",
            "        \"solr\": [\"sqlalchemy-solr >= 0.2.0\"],",
            "        \"elasticsearch\": [\"elasticsearch-dbapi>=0.2.9, <0.3.0\"],",
            "        \"exasol\": [\"sqlalchemy-exasol >= 2.4.0, <3.0\"],",
            "        \"excel\": [\"xlrd>=1.2.0, <1.3\"],",
            "        \"firebird\": [\"sqlalchemy-firebird>=0.7.0, <0.8\"],",
            "        \"firebolt\": [\"firebolt-sqlalchemy>=0.0.1\"],",
            "        \"gsheets\": [\"shillelagh[gsheetsapi]>=1.2.10, <2\"],",
            "        \"hana\": [\"hdbcli==2.4.162\", \"sqlalchemy_hana==0.4.0\"],",
            "        \"hive\": [",
            "            \"pyhive[hive]>=0.6.5;python_version<'3.11'\",",
            "            \"pyhive[hive_pure_sasl]>=0.7.0\",",
            "            \"tableschema\",",
            "            \"thrift>=0.14.1, <1.0.0\",",
            "        ],",
            "        \"impala\": [\"impyla>0.16.2, <0.17\"],",
            "        \"kusto\": [\"sqlalchemy-kusto>=2.0.0, <3\"],",
            "        \"kylin\": [\"kylinpy>=2.8.1, <2.9\"],",
            "        \"mssql\": [\"pymssql>=2.2.8, <3\"],",
            "        \"mysql\": [\"mysqlclient>=2.1.0, <3\"],",
            "        \"ocient\": [",
            "            \"sqlalchemy-ocient>=1.0.0\",",
            "            \"pyocient>=1.0.15, <2\",",
            "            \"shapely\",",
            "            \"geojson\",",
            "        ],",
            "        \"oracle\": [\"cx-Oracle>8.0.0, <8.1\"],",
            "        \"pinot\": [\"pinotdb>=0.3.3, <0.4\"],",
            "        \"playwright\": [\"playwright>=1.37.0, <2\"],",
            "        \"postgres\": [\"psycopg2-binary==2.9.6\"],",
            "        \"presto\": [\"pyhive[presto]>=0.6.5\"],",
            "        \"trino\": [\"trino>=0.324.0\"],",
            "        \"prophet\": [\"prophet>=1.1.5, <2\"],",
            "        \"redshift\": [\"sqlalchemy-redshift>=0.8.1, <0.9\"],",
            "        \"rockset\": [\"rockset-sqlalchemy>=0.0.1, <1\"],",
            "        \"shillelagh\": [",
            "            \"shillelagh[datasetteapi,gsheetsapi,socrata,weatherapi]>=1.2.10, <2\"",
            "        ],",
            "        \"snowflake\": [\"snowflake-sqlalchemy>=1.2.4, <2\"],",
            "        \"spark\": [",
            "            \"pyhive[hive]>=0.6.5;python_version<'3.11'\",",
            "            \"pyhive[hive_pure_sasl]>=0.7\",",
            "            \"tableschema\",",
            "            \"thrift>=0.14.1, <1\",",
            "        ],",
            "        \"teradata\": [\"teradatasql>=16.20.0.23\"],",
            "        \"thumbnails\": [\"Pillow>=10.0.1, <11\"],",
            "        \"vertica\": [\"sqlalchemy-vertica-python>=0.5.9, < 0.6\"],",
            "        \"netezza\": [\"nzalchemy>=11.0.2\"],",
            "        \"starrocks\": [\"starrocks>=1.0.0\"],",
            "        \"doris\": [\"pydoris>=1.0.0, <2.0.0\"],",
            "    },",
            "    python_requires=\"~=3.9\",",
            "    author=\"Apache Software Foundation\",",
            "    author_email=\"dev@superset.apache.org\",",
            "    url=\"https://superset.apache.org/\",",
            "    download_url=\"https://www.apache.org/dist/superset/\" + version_string,",
            "    classifiers=[",
            "        \"Programming Language :: Python :: 3.9\",",
            "        \"Programming Language :: Python :: 3.10\",",
            "        \"Programming Language :: Python :: 3.11\",",
            "    ],",
            ")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "superset/commands/dataset/duplicate.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "             table.normalize_columns = self._base_model.normalize_columns"
            },
            "1": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "             table.always_filter_main_dttm = self._base_model.always_filter_main_dttm"
            },
            "2": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "             table.is_sqllab_view = True"
            },
            "3": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            table.sql = ParsedQuery(self._base_model.sql).stripped()"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 73,
                "PatchRowcode": "+            table.sql = ParsedQuery("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 74,
                "PatchRowcode": "+                self._base_model.sql,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+                engine=database.db_engine_spec.engine,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+            ).stripped()"
            },
            "8": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "             db.session.add(table)"
            },
            "9": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 78,
                "PatchRowcode": "             cols = []"
            },
            "10": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 79,
                "PatchRowcode": "             for config_ in self._base_model.columns:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "from typing import Any",
            "",
            "from flask_appbuilder.models.sqla import Model",
            "from flask_babel import gettext as __",
            "from marshmallow import ValidationError",
            "from sqlalchemy.exc import SQLAlchemyError",
            "",
            "from superset.commands.base import BaseCommand, CreateMixin",
            "from superset.commands.dataset.exceptions import (",
            "    DatasetDuplicateFailedError,",
            "    DatasetExistsValidationError,",
            "    DatasetInvalidError,",
            "    DatasetNotFoundError,",
            ")",
            "from superset.commands.exceptions import DatasourceTypeInvalidError",
            "from superset.connectors.sqla.models import SqlaTable, SqlMetric, TableColumn",
            "from superset.daos.dataset import DatasetDAO",
            "from superset.daos.exceptions import DAOCreateFailedError",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorException",
            "from superset.extensions import db",
            "from superset.models.core import Database",
            "from superset.sql_parse import ParsedQuery",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DuplicateDatasetCommand(CreateMixin, BaseCommand):",
            "    def __init__(self, data: dict[str, Any]) -> None:",
            "        self._base_model: SqlaTable = SqlaTable()",
            "        self._properties = data.copy()",
            "",
            "    def run(self) -> Model:",
            "        self.validate()",
            "        try:",
            "            database_id = self._base_model.database_id",
            "            table_name = self._properties[\"table_name\"]",
            "            owners = self._properties[\"owners\"]",
            "            database = db.session.query(Database).get(database_id)",
            "            if not database:",
            "                raise SupersetErrorException(",
            "                    SupersetError(",
            "                        message=__(\"The database was not found.\"),",
            "                        error_type=SupersetErrorType.DATABASE_NOT_FOUND_ERROR,",
            "                        level=ErrorLevel.ERROR,",
            "                    ),",
            "                    status=404,",
            "                )",
            "            table = SqlaTable(table_name=table_name, owners=owners)",
            "            table.database = database",
            "            table.schema = self._base_model.schema",
            "            table.template_params = self._base_model.template_params",
            "            table.normalize_columns = self._base_model.normalize_columns",
            "            table.always_filter_main_dttm = self._base_model.always_filter_main_dttm",
            "            table.is_sqllab_view = True",
            "            table.sql = ParsedQuery(self._base_model.sql).stripped()",
            "            db.session.add(table)",
            "            cols = []",
            "            for config_ in self._base_model.columns:",
            "                column_name = config_.column_name",
            "                col = TableColumn(",
            "                    column_name=column_name,",
            "                    verbose_name=config_.verbose_name,",
            "                    expression=config_.expression,",
            "                    filterable=True,",
            "                    groupby=True,",
            "                    is_dttm=config_.is_dttm,",
            "                    type=config_.type,",
            "                    description=config_.description,",
            "                )",
            "                cols.append(col)",
            "            table.columns = cols",
            "            mets = []",
            "            for config_ in self._base_model.metrics:",
            "                metric_name = config_.metric_name",
            "                met = SqlMetric(",
            "                    metric_name=metric_name,",
            "                    verbose_name=config_.verbose_name,",
            "                    expression=config_.expression,",
            "                    metric_type=config_.metric_type,",
            "                    description=config_.description,",
            "                )",
            "                mets.append(met)",
            "            table.metrics = mets",
            "            db.session.commit()",
            "        except (SQLAlchemyError, DAOCreateFailedError) as ex:",
            "            logger.warning(ex, exc_info=True)",
            "            db.session.rollback()",
            "            raise DatasetDuplicateFailedError() from ex",
            "        return table",
            "",
            "    def validate(self) -> None:",
            "        exceptions: list[ValidationError] = []",
            "        base_model_id = self._properties[\"base_model_id\"]",
            "        duplicate_name = self._properties[\"table_name\"]",
            "",
            "        base_model = DatasetDAO.find_by_id(base_model_id)",
            "        if not base_model:",
            "            exceptions.append(DatasetNotFoundError())",
            "        else:",
            "            self._base_model = base_model",
            "",
            "        if self._base_model and self._base_model.kind != \"virtual\":",
            "            exceptions.append(DatasourceTypeInvalidError())",
            "",
            "        if DatasetDAO.find_one_or_none(table_name=duplicate_name):",
            "            exceptions.append(DatasetExistsValidationError(table_name=duplicate_name))",
            "",
            "        try:",
            "            owners = self.populate_owners()",
            "            self._properties[\"owners\"] = owners",
            "        except ValidationError as ex:",
            "            exceptions.append(ex)",
            "",
            "        if exceptions:",
            "            raise DatasetInvalidError(exceptions=exceptions)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import logging",
            "from typing import Any",
            "",
            "from flask_appbuilder.models.sqla import Model",
            "from flask_babel import gettext as __",
            "from marshmallow import ValidationError",
            "from sqlalchemy.exc import SQLAlchemyError",
            "",
            "from superset.commands.base import BaseCommand, CreateMixin",
            "from superset.commands.dataset.exceptions import (",
            "    DatasetDuplicateFailedError,",
            "    DatasetExistsValidationError,",
            "    DatasetInvalidError,",
            "    DatasetNotFoundError,",
            ")",
            "from superset.commands.exceptions import DatasourceTypeInvalidError",
            "from superset.connectors.sqla.models import SqlaTable, SqlMetric, TableColumn",
            "from superset.daos.dataset import DatasetDAO",
            "from superset.daos.exceptions import DAOCreateFailedError",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorException",
            "from superset.extensions import db",
            "from superset.models.core import Database",
            "from superset.sql_parse import ParsedQuery",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DuplicateDatasetCommand(CreateMixin, BaseCommand):",
            "    def __init__(self, data: dict[str, Any]) -> None:",
            "        self._base_model: SqlaTable = SqlaTable()",
            "        self._properties = data.copy()",
            "",
            "    def run(self) -> Model:",
            "        self.validate()",
            "        try:",
            "            database_id = self._base_model.database_id",
            "            table_name = self._properties[\"table_name\"]",
            "            owners = self._properties[\"owners\"]",
            "            database = db.session.query(Database).get(database_id)",
            "            if not database:",
            "                raise SupersetErrorException(",
            "                    SupersetError(",
            "                        message=__(\"The database was not found.\"),",
            "                        error_type=SupersetErrorType.DATABASE_NOT_FOUND_ERROR,",
            "                        level=ErrorLevel.ERROR,",
            "                    ),",
            "                    status=404,",
            "                )",
            "            table = SqlaTable(table_name=table_name, owners=owners)",
            "            table.database = database",
            "            table.schema = self._base_model.schema",
            "            table.template_params = self._base_model.template_params",
            "            table.normalize_columns = self._base_model.normalize_columns",
            "            table.always_filter_main_dttm = self._base_model.always_filter_main_dttm",
            "            table.is_sqllab_view = True",
            "            table.sql = ParsedQuery(",
            "                self._base_model.sql,",
            "                engine=database.db_engine_spec.engine,",
            "            ).stripped()",
            "            db.session.add(table)",
            "            cols = []",
            "            for config_ in self._base_model.columns:",
            "                column_name = config_.column_name",
            "                col = TableColumn(",
            "                    column_name=column_name,",
            "                    verbose_name=config_.verbose_name,",
            "                    expression=config_.expression,",
            "                    filterable=True,",
            "                    groupby=True,",
            "                    is_dttm=config_.is_dttm,",
            "                    type=config_.type,",
            "                    description=config_.description,",
            "                )",
            "                cols.append(col)",
            "            table.columns = cols",
            "            mets = []",
            "            for config_ in self._base_model.metrics:",
            "                metric_name = config_.metric_name",
            "                met = SqlMetric(",
            "                    metric_name=metric_name,",
            "                    verbose_name=config_.verbose_name,",
            "                    expression=config_.expression,",
            "                    metric_type=config_.metric_type,",
            "                    description=config_.description,",
            "                )",
            "                mets.append(met)",
            "            table.metrics = mets",
            "            db.session.commit()",
            "        except (SQLAlchemyError, DAOCreateFailedError) as ex:",
            "            logger.warning(ex, exc_info=True)",
            "            db.session.rollback()",
            "            raise DatasetDuplicateFailedError() from ex",
            "        return table",
            "",
            "    def validate(self) -> None:",
            "        exceptions: list[ValidationError] = []",
            "        base_model_id = self._properties[\"base_model_id\"]",
            "        duplicate_name = self._properties[\"table_name\"]",
            "",
            "        base_model = DatasetDAO.find_by_id(base_model_id)",
            "        if not base_model:",
            "            exceptions.append(DatasetNotFoundError())",
            "        else:",
            "            self._base_model = base_model",
            "",
            "        if self._base_model and self._base_model.kind != \"virtual\":",
            "            exceptions.append(DatasourceTypeInvalidError())",
            "",
            "        if DatasetDAO.find_one_or_none(table_name=duplicate_name):",
            "            exceptions.append(DatasetExistsValidationError(table_name=duplicate_name))",
            "",
            "        try:",
            "            owners = self.populate_owners()",
            "            self._properties[\"owners\"] = owners",
            "        except ValidationError as ex:",
            "            exceptions.append(ex)",
            "",
            "        if exceptions:",
            "            raise DatasetInvalidError(exceptions=exceptions)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "73": [
                "DuplicateDatasetCommand",
                "run"
            ]
        },
        "addLocation": []
    },
    "superset/commands/sql_lab/export.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "                 limit = None"
            },
            "1": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "             else:"
            },
            "2": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "                 sql = self._query.executed_sql"
            },
            "3": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                limit = ParsedQuery(sql).limit"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+                limit = ParsedQuery("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+                    sql,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+                    engine=self._query.database.db_engine_spec.engine,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+                ).limit"
            },
            "8": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 122,
                "PatchRowcode": "             if limit is not None and self._query.limiting_factor in {"
            },
            "9": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 123,
                "PatchRowcode": "                 LimitingFactor.QUERY,"
            },
            "10": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 124,
                "PatchRowcode": "                 LimitingFactor.DROPDOWN,"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import logging",
            "from typing import Any, cast, TypedDict",
            "",
            "import pandas as pd",
            "from flask_babel import gettext as __",
            "",
            "from superset import app, db, results_backend, results_backend_use_msgpack",
            "from superset.commands.base import BaseCommand",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorException, SupersetSecurityException",
            "from superset.models.sql_lab import Query",
            "from superset.sql_parse import ParsedQuery",
            "from superset.sqllab.limiting_factor import LimitingFactor",
            "from superset.utils import core as utils, csv",
            "from superset.views.utils import _deserialize_results_payload",
            "",
            "config = app.config",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SqlExportResult(TypedDict):",
            "    query: Query",
            "    count: int",
            "    data: list[Any]",
            "",
            "",
            "class SqlResultExportCommand(BaseCommand):",
            "    _client_id: str",
            "    _query: Query",
            "",
            "    def __init__(",
            "        self,",
            "        client_id: str,",
            "    ) -> None:",
            "        self._client_id = client_id",
            "",
            "    def validate(self) -> None:",
            "        self._query = (",
            "            db.session.query(Query).filter_by(client_id=self._client_id).one_or_none()",
            "        )",
            "        if self._query is None:",
            "            raise SupersetErrorException(",
            "                SupersetError(",
            "                    message=__(",
            "                        \"The query associated with these results could not be found. \"",
            "                        \"You need to re-run the original query.\"",
            "                    ),",
            "                    error_type=SupersetErrorType.RESULTS_BACKEND_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                ),",
            "                status=404,",
            "            )",
            "",
            "        try:",
            "            self._query.raise_for_access()",
            "        except SupersetSecurityException as ex:",
            "            raise SupersetErrorException(",
            "                SupersetError(",
            "                    message=__(\"Cannot access the query\"),",
            "                    error_type=SupersetErrorType.QUERY_SECURITY_ACCESS_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                ),",
            "                status=403,",
            "            ) from ex",
            "",
            "    def run(",
            "        self,",
            "    ) -> SqlExportResult:",
            "        self.validate()",
            "        blob = None",
            "        if results_backend and self._query.results_key:",
            "            logger.info(",
            "                \"Fetching CSV from results backend [%s]\", self._query.results_key",
            "            )",
            "            blob = results_backend.get(self._query.results_key)",
            "        if blob:",
            "            logger.info(\"Decompressing\")",
            "            payload = utils.zlib_decompress(",
            "                blob, decode=not results_backend_use_msgpack",
            "            )",
            "            obj = _deserialize_results_payload(",
            "                payload, self._query, cast(bool, results_backend_use_msgpack)",
            "            )",
            "",
            "            df = pd.DataFrame(",
            "                data=obj[\"data\"],",
            "                dtype=object,",
            "                columns=[c[\"name\"] for c in obj[\"columns\"]],",
            "            )",
            "",
            "            logger.info(\"Using pandas to convert to CSV\")",
            "        else:",
            "            logger.info(\"Running a query to turn into CSV\")",
            "            if self._query.select_sql:",
            "                sql = self._query.select_sql",
            "                limit = None",
            "            else:",
            "                sql = self._query.executed_sql",
            "                limit = ParsedQuery(sql).limit",
            "            if limit is not None and self._query.limiting_factor in {",
            "                LimitingFactor.QUERY,",
            "                LimitingFactor.DROPDOWN,",
            "                LimitingFactor.QUERY_AND_DROPDOWN,",
            "            }:",
            "                # remove extra row from `increased_limit`",
            "                limit -= 1",
            "            df = self._query.database.get_df(sql, self._query.schema)[:limit]",
            "",
            "        csv_data = csv.df_to_escaped_csv(df, index=False, **config[\"CSV_EXPORT\"])",
            "",
            "        return {",
            "            \"query\": self._query,",
            "            \"count\": len(df.index),",
            "            \"data\": csv_data,",
            "        }"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import logging",
            "from typing import Any, cast, TypedDict",
            "",
            "import pandas as pd",
            "from flask_babel import gettext as __",
            "",
            "from superset import app, db, results_backend, results_backend_use_msgpack",
            "from superset.commands.base import BaseCommand",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorException, SupersetSecurityException",
            "from superset.models.sql_lab import Query",
            "from superset.sql_parse import ParsedQuery",
            "from superset.sqllab.limiting_factor import LimitingFactor",
            "from superset.utils import core as utils, csv",
            "from superset.views.utils import _deserialize_results_payload",
            "",
            "config = app.config",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SqlExportResult(TypedDict):",
            "    query: Query",
            "    count: int",
            "    data: list[Any]",
            "",
            "",
            "class SqlResultExportCommand(BaseCommand):",
            "    _client_id: str",
            "    _query: Query",
            "",
            "    def __init__(",
            "        self,",
            "        client_id: str,",
            "    ) -> None:",
            "        self._client_id = client_id",
            "",
            "    def validate(self) -> None:",
            "        self._query = (",
            "            db.session.query(Query).filter_by(client_id=self._client_id).one_or_none()",
            "        )",
            "        if self._query is None:",
            "            raise SupersetErrorException(",
            "                SupersetError(",
            "                    message=__(",
            "                        \"The query associated with these results could not be found. \"",
            "                        \"You need to re-run the original query.\"",
            "                    ),",
            "                    error_type=SupersetErrorType.RESULTS_BACKEND_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                ),",
            "                status=404,",
            "            )",
            "",
            "        try:",
            "            self._query.raise_for_access()",
            "        except SupersetSecurityException as ex:",
            "            raise SupersetErrorException(",
            "                SupersetError(",
            "                    message=__(\"Cannot access the query\"),",
            "                    error_type=SupersetErrorType.QUERY_SECURITY_ACCESS_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                ),",
            "                status=403,",
            "            ) from ex",
            "",
            "    def run(",
            "        self,",
            "    ) -> SqlExportResult:",
            "        self.validate()",
            "        blob = None",
            "        if results_backend and self._query.results_key:",
            "            logger.info(",
            "                \"Fetching CSV from results backend [%s]\", self._query.results_key",
            "            )",
            "            blob = results_backend.get(self._query.results_key)",
            "        if blob:",
            "            logger.info(\"Decompressing\")",
            "            payload = utils.zlib_decompress(",
            "                blob, decode=not results_backend_use_msgpack",
            "            )",
            "            obj = _deserialize_results_payload(",
            "                payload, self._query, cast(bool, results_backend_use_msgpack)",
            "            )",
            "",
            "            df = pd.DataFrame(",
            "                data=obj[\"data\"],",
            "                dtype=object,",
            "                columns=[c[\"name\"] for c in obj[\"columns\"]],",
            "            )",
            "",
            "            logger.info(\"Using pandas to convert to CSV\")",
            "        else:",
            "            logger.info(\"Running a query to turn into CSV\")",
            "            if self._query.select_sql:",
            "                sql = self._query.select_sql",
            "                limit = None",
            "            else:",
            "                sql = self._query.executed_sql",
            "                limit = ParsedQuery(",
            "                    sql,",
            "                    engine=self._query.database.db_engine_spec.engine,",
            "                ).limit",
            "            if limit is not None and self._query.limiting_factor in {",
            "                LimitingFactor.QUERY,",
            "                LimitingFactor.DROPDOWN,",
            "                LimitingFactor.QUERY_AND_DROPDOWN,",
            "            }:",
            "                # remove extra row from `increased_limit`",
            "                limit -= 1",
            "            df = self._query.database.get_df(sql, self._query.schema)[:limit]",
            "",
            "        csv_data = csv.df_to_escaped_csv(df, index=False, **config[\"CSV_EXPORT\"])",
            "",
            "        return {",
            "            \"query\": self._query,",
            "            \"count\": len(df.index),",
            "            \"data\": csv_data,",
            "        }"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "118": [
                "SqlResultExportCommand",
                "run"
            ]
        },
        "addLocation": []
    },
    "superset/connectors/sqla/models.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1458,
                "afterPatchRowNumber": 1458,
                "PatchRowcode": "             return self.get_sqla_table(), None"
            },
            "1": {
                "beforePatchRowNumber": 1459,
                "afterPatchRowNumber": 1459,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 1460,
                "afterPatchRowNumber": 1460,
                "PatchRowcode": "         from_sql = self.get_rendered_sql(template_processor)"
            },
            "3": {
                "beforePatchRowNumber": 1461,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        parsed_query = ParsedQuery(from_sql)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1461,
                "PatchRowcode": "+        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)"
            },
            "5": {
                "beforePatchRowNumber": 1462,
                "afterPatchRowNumber": 1462,
                "PatchRowcode": "         if not ("
            },
            "6": {
                "beforePatchRowNumber": 1463,
                "afterPatchRowNumber": 1463,
                "PatchRowcode": "             parsed_query.is_unknown()"
            },
            "7": {
                "beforePatchRowNumber": 1464,
                "afterPatchRowNumber": 1464,
                "PatchRowcode": "             or self.db_engine_spec.is_readonly_query(parsed_query)"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from dataclasses import dataclass, field",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, Callable, cast",
            "",
            "import dateutil.parser",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "import sqlparse",
            "from flask import escape, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import (",
            "    and_,",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    inspect,",
            "    Integer,",
            "    or_,",
            "    String,",
            "    Table,",
            "    Text,",
            "    update,",
            ")",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.ext.hybrid import hybrid_property",
            "from sqlalchemy.orm import (",
            "    backref,",
            "    foreign,",
            "    Mapped,",
            "    Query,",
            "    reconstructor,",
            "    relationship,",
            "    RelationshipProperty,",
            "    Session,",
            ")",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.schema import UniqueConstraint",
            "from sqlalchemy.sql import column, ColumnElement, literal_column, table",
            "from sqlalchemy.sql.elements import ColumnClause, TextClause",
            "from sqlalchemy.sql.expression import Label, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.commands.dataset.exceptions import DatasetNotFoundError",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.connectors.sqla.utils import (",
            "    get_columns_description,",
            "    get_physical_table_metadata,",
            "    get_virtual_table_metadata,",
            ")",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import BaseEngineSpec, TimestampExpression",
            "from superset.exceptions import (",
            "    ColumnNotFoundException,",
            "    DatasetInvalidPermissionEvaluationException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.jinja_context import (",
            "    BaseTemplateProcessor,",
            "    ExtraCache,",
            "    get_template_processor,",
            ")",
            "from superset.models.annotations import Annotation",
            "from superset.models.core import Database",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    CertificationMixin,",
            "    ExploreMixin,",
            "    ImportExportMixin,",
            "    QueryResult,",
            "    QueryStringExtended,",
            "    validate_adhoc_subquery,",
            ")",
            "from superset.models.slice import Slice",
            "from superset.sql_parse import ParsedQuery, sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    QueryObjectDict,",
            "    ResultSetColumnType,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.backports import StrEnum",
            "from superset.utils.core import GenericDataType, MediumText",
            "",
            "config = app.config",
            "metadata = Model.metadata  # pylint: disable=no-member",
            "logger = logging.getLogger(__name__)",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "",
            "# a non-exhaustive set of additive metrics",
            "ADDITIVE_METRIC_TYPES = {",
            "    \"count\",",
            "    \"sum\",",
            "    \"doubleSum\",",
            "}",
            "ADDITIVE_METRIC_TYPES_LOWER = {op.lower() for op in ADDITIVE_METRIC_TYPES}",
            "",
            "",
            "@dataclass",
            "class MetadataResult:",
            "    added: list[str] = field(default_factory=list)",
            "    removed: list[str] = field(default_factory=list)",
            "    modified: list[str] = field(default_factory=list)",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "METRIC_FORM_DATA_PARAMS = [",
            "    \"metric\",",
            "    \"metric_2\",",
            "    \"metrics\",",
            "    \"metrics_b\",",
            "    \"percent_metrics\",",
            "    \"secondary_metric\",",
            "    \"size\",",
            "    \"timeseries_limit_metric\",",
            "    \"x\",",
            "    \"y\",",
            "]",
            "",
            "COLUMN_FORM_DATA_PARAMS = [",
            "    \"all_columns\",",
            "    \"all_columns_x\",",
            "    \"columns\",",
            "    \"entity\",",
            "    \"groupby\",",
            "    \"order_by_cols\",",
            "    \"series\",",
            "]",
            "",
            "",
            "class DatasourceKind(StrEnum):",
            "    VIRTUAL = \"virtual\"",
            "    PHYSICAL = \"physical\"",
            "",
            "",
            "class BaseDatasource(",
            "    AuditMixinNullable, ImportExportMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"A common interface to objects that are queryable",
            "    (tables and datasources)\"\"\"",
            "",
            "    # ---------------------------------------------------------------",
            "    # class attributes to define when deriving BaseDatasource",
            "    # ---------------------------------------------------------------",
            "    __tablename__: str | None = None  # {connector_name}_datasource",
            "    baselink: str | None = None  # url portion pointing to ModelView endpoint",
            "",
            "    owner_class: User | None = None",
            "",
            "    # Used to do code highlighting when displaying the query in the UI",
            "    query_language: str | None = None",
            "",
            "    # Only some datasources support Row Level Security",
            "    is_rls_supported: bool = False",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        # can be a Column or a property pointing to one",
            "        raise NotImplementedError()",
            "",
            "    # ---------------------------------------------------------------",
            "",
            "    # Columns",
            "    id = Column(Integer, primary_key=True)",
            "    description = Column(Text)",
            "    default_endpoint = Column(Text)",
            "    is_featured = Column(Boolean, default=False)  # TODO deprecating",
            "    filter_select_enabled = Column(Boolean, default=True)",
            "    offset = Column(Integer, default=0)",
            "    cache_timeout = Column(Integer)",
            "    params = Column(String(1000))",
            "    perm = Column(String(1000))",
            "    schema_perm = Column(String(1000))",
            "    is_managed_externally = Column(Boolean, nullable=False, default=False)",
            "    external_url = Column(Text, nullable=True)",
            "",
            "    sql: str | None = None",
            "    owners: list[User]",
            "    update_from_object_fields: list[str]",
            "",
            "    extra_import_fields = [\"is_managed_externally\", \"external_url\"]",
            "",
            "    @property",
            "    def kind(self) -> DatasourceKind:",
            "        return DatasourceKind.VIRTUAL if self.sql else DatasourceKind.PHYSICAL",
            "",
            "    @property",
            "    def owners_data(self) -> list[dict[str, Any]]:",
            "        return [",
            "            {",
            "                \"first_name\": o.first_name,",
            "                \"last_name\": o.last_name,",
            "                \"username\": o.username,",
            "                \"id\": o.id,",
            "            }",
            "            for o in self.owners",
            "        ]",
            "",
            "    @property",
            "    def is_virtual(self) -> bool:",
            "        return self.kind == DatasourceKind.VIRTUAL",
            "",
            "    @declared_attr",
            "    def slices(self) -> RelationshipProperty:",
            "        return relationship(",
            "            \"Slice\",",
            "            overlaps=\"table\",",
            "            primaryjoin=lambda: and_(",
            "                foreign(Slice.datasource_id) == self.id,",
            "                foreign(Slice.datasource_type) == self.type,",
            "            ),",
            "        )",
            "",
            "    columns: list[TableColumn] = []",
            "    metrics: list[SqlMetric] = []",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        \"\"\"Unique id across datasource types\"\"\"",
            "        return f\"{self.id}__{self.type}\"",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns], key=lambda x: x or \"\")",
            "",
            "    @property",
            "    def columns_types(self) -> dict[str, str]:",
            "        return {c.column_name: c.type for c in self.columns}",
            "",
            "    @property",
            "    def main_dttm_col(self) -> str:",
            "        return \"timestamp\"",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def connection(self) -> str | None:",
            "        \"\"\"String representing the context of the Datasource\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def schema(self) -> str | None:",
            "        \"\"\"String representing the schema of the Datasource (if it applies)\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def filterable_column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns if c.filterable])",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        return []",
            "",
            "    @property",
            "    def url(self) -> str:",
            "        return f\"/{self.baselink}/edit/{self.id}\"",
            "",
            "    @property",
            "    def explore_url(self) -> str:",
            "        if self.default_endpoint:",
            "            return self.default_endpoint",
            "        return f\"/explore/?datasource_type={self.type}&datasource_id={self.id}\"",
            "",
            "    @property",
            "    def column_formats(self) -> dict[str, str | None]:",
            "        return {m.metric_name: m.d3format for m in self.metrics if m.d3format}",
            "",
            "    @property",
            "    def currency_formats(self) -> dict[str, dict[str, str | None] | None]:",
            "        return {m.metric_name: m.currency_json for m in self.metrics if m.currency_json}",
            "",
            "    def add_missing_metrics(self, metrics: list[SqlMetric]) -> None:",
            "        existing_metrics = {m.metric_name for m in self.metrics}",
            "        for metric in metrics:",
            "            if metric.metric_name not in existing_metrics:",
            "                metric.table_id = self.id",
            "                self.metrics.append(metric)",
            "",
            "    @property",
            "    def short_data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            \"edit_url\": self.url,",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"schema\": self.schema,",
            "            \"name\": self.name,",
            "            \"type\": self.type,",
            "            \"connection\": self.connection,",
            "            \"creator\": str(self.created_by),",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        pass",
            "",
            "    @property",
            "    def order_by_choices(self) -> list[tuple[str, str]]:",
            "        choices = []",
            "        # self.column_names return sorted column_names",
            "        for column_name in self.column_names:",
            "            column_name = str(column_name or \"\")",
            "            choices.append(",
            "                (json.dumps([column_name, True]), f\"{column_name} \" + __(\"[asc]\"))",
            "            )",
            "            choices.append(",
            "                (json.dumps([column_name, False]), f\"{column_name} \" + __(\"[desc]\"))",
            "            )",
            "        return choices",
            "",
            "    @property",
            "    def verbose_map(self) -> dict[str, str]:",
            "        verb_map = {\"__timestamp\": \"Time\"}",
            "        verb_map.update(",
            "            {o.metric_name: o.verbose_name or o.metric_name for o in self.metrics}",
            "        )",
            "        verb_map.update(",
            "            {o.column_name: o.verbose_name or o.column_name for o in self.columns}",
            "        )",
            "        return verb_map",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            # simple fields",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"column_formats\": self.column_formats,",
            "            \"currency_formats\": self.currency_formats,",
            "            \"description\": self.description,",
            "            \"database\": self.database.data,  # pylint: disable=no-member",
            "            \"default_endpoint\": self.default_endpoint,",
            "            \"filter_select\": self.filter_select_enabled,  # TODO deprecate",
            "            \"filter_select_enabled\": self.filter_select_enabled,",
            "            \"name\": self.name,",
            "            \"datasource_name\": self.datasource_name,",
            "            \"table_name\": self.datasource_name,",
            "            \"type\": self.type,",
            "            \"schema\": self.schema,",
            "            \"offset\": self.offset,",
            "            \"cache_timeout\": self.cache_timeout,",
            "            \"params\": self.params,",
            "            \"perm\": self.perm,",
            "            \"edit_url\": self.url,",
            "            # sqla-specific",
            "            \"sql\": self.sql,",
            "            # one to many",
            "            \"columns\": [o.data for o in self.columns],",
            "            \"metrics\": [o.data for o in self.metrics],",
            "            # TODO deprecate, move logic to JS",
            "            \"order_by_choices\": self.order_by_choices,",
            "            \"owners\": [owner.id for owner in self.owners],",
            "            \"verbose_map\": self.verbose_map,",
            "            \"select_star\": self.select_star,",
            "        }",
            "",
            "    def data_for_slices(  # pylint: disable=too-many-locals",
            "        self, slices: list[Slice]",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        The representation of the datasource containing only the required data",
            "        to render the provided slices.",
            "",
            "        Used to reduce the payload when loading a dashboard.",
            "        \"\"\"",
            "        data = self.data",
            "        metric_names = set()",
            "        column_names = set()",
            "        for slc in slices:",
            "            form_data = slc.form_data",
            "            # pull out all required metrics from the form_data",
            "            for metric_param in METRIC_FORM_DATA_PARAMS:",
            "                for metric in utils.as_list(form_data.get(metric_param) or []):",
            "                    metric_names.add(utils.get_metric_name(metric))",
            "                    if utils.is_adhoc_metric(metric):",
            "                        column_ = metric.get(\"column\") or {}",
            "                        if column_name := column_.get(\"column_name\"):",
            "                            column_names.add(column_name)",
            "",
            "            # Columns used in query filters",
            "            column_names.update(",
            "                filter_[\"subject\"]",
            "                for filter_ in form_data.get(\"adhoc_filters\") or []",
            "                if filter_.get(\"clause\") == \"WHERE\" and filter_.get(\"subject\")",
            "            )",
            "",
            "            # columns used by Filter Box",
            "            column_names.update(",
            "                filter_config[\"column\"]",
            "                for filter_config in form_data.get(\"filter_configs\") or []",
            "                if \"column\" in filter_config",
            "            )",
            "",
            "            # for legacy dashboard imports which have the wrong query_context in them",
            "            try:",
            "                query_context = slc.get_query_context()",
            "            except DatasetNotFoundError:",
            "                query_context = None",
            "",
            "            # legacy charts don't have query_context charts",
            "            if query_context:",
            "                column_names.update(",
            "                    [",
            "                        utils.get_column_name(column_)",
            "                        for query in query_context.queries",
            "                        for column_ in query.columns",
            "                    ]",
            "                    or []",
            "                )",
            "            else:",
            "                _columns = [",
            "                    utils.get_column_name(column_)",
            "                    if utils.is_adhoc_column(column_)",
            "                    else column_",
            "                    for column_param in COLUMN_FORM_DATA_PARAMS",
            "                    for column_ in utils.as_list(form_data.get(column_param) or [])",
            "                ]",
            "                column_names.update(_columns)",
            "",
            "        filtered_metrics = [",
            "            metric",
            "            for metric in data[\"metrics\"]",
            "            if metric[\"metric_name\"] in metric_names",
            "        ]",
            "",
            "        filtered_columns: list[Column] = []",
            "        column_types: set[GenericDataType] = set()",
            "        for column_ in data[\"columns\"]:",
            "            generic_type = column_.get(\"type_generic\")",
            "            if generic_type is not None:",
            "                column_types.add(generic_type)",
            "            if column_[\"column_name\"] in column_names:",
            "                filtered_columns.append(column_)",
            "",
            "        data[\"column_types\"] = list(column_types)",
            "        del data[\"description\"]",
            "        data.update({\"metrics\": filtered_metrics})",
            "        data.update({\"columns\": filtered_columns})",
            "        verbose_map = {\"__timestamp\": \"Time\"}",
            "        verbose_map.update(",
            "            {",
            "                metric[\"metric_name\"]: metric[\"verbose_name\"] or metric[\"metric_name\"]",
            "                for metric in filtered_metrics",
            "            }",
            "        )",
            "        verbose_map.update(",
            "            {",
            "                column_[\"column_name\"]: column_[\"verbose_name\"]",
            "                or column_[\"column_name\"]",
            "                for column_ in filtered_columns",
            "            }",
            "        )",
            "        data[\"verbose_map\"] = verbose_map",
            "",
            "        return data",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: FilterValues | None,",
            "        operator: str,",
            "        target_generic_type: GenericDataType,",
            "        target_native_type: str | None = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: builtins.type[BaseEngineSpec] | None = None,",
            "        db_extra: dict[str, Any] | None = None,",
            "    ) -> FilterValues | None:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: FilterValue | None) -> FilterValue | None:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        \"\"\"Returns column information from the external system\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        \"\"\"Returns a query as a string",
            "",
            "        This is used to be displayed to the user so that they can",
            "        understand what is taking place behind the scene\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        \"\"\"Executes the query and returns a dataframe",
            "",
            "        query_obj is a dictionary representing Superset's query interface.",
            "        Should return a ``superset.models.helpers.QueryResult``",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry",
            "",
            "    def get_column(self, column_name: str | None) -> TableColumn | None:",
            "        if not column_name:",
            "            return None",
            "        for col in self.columns:",
            "            if col.column_name == column_name:",
            "                return col",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_fk_many_from_list(",
            "        object_list: list[Any],",
            "        fkmany: list[Column],",
            "        fkmany_class: builtins.type[TableColumn | SqlMetric],",
            "        key_attr: str,",
            "    ) -> list[Column]:",
            "        \"\"\"Update ORM one-to-many list from object list",
            "",
            "        Used for syncing metrics and columns using the same code\"\"\"",
            "",
            "        object_dict = {o.get(key_attr): o for o in object_list}",
            "",
            "        # delete fks that have been removed",
            "        fkmany = [o for o in fkmany if getattr(o, key_attr) in object_dict]",
            "",
            "        # sync existing fks",
            "        for fk in fkmany:",
            "            obj = object_dict.get(getattr(fk, key_attr))",
            "            if obj:",
            "                for attr in fkmany_class.update_from_object_fields:",
            "                    setattr(fk, attr, obj.get(attr))",
            "",
            "        # create new fks",
            "        new_fks = []",
            "        orm_keys = [getattr(o, key_attr) for o in fkmany]",
            "        for obj in object_list:",
            "            key = obj.get(key_attr)",
            "            if key not in orm_keys:",
            "                del obj[\"id\"]",
            "                orm_kwargs = {}",
            "                for k in obj:",
            "                    if k in fkmany_class.update_from_object_fields and k in obj:",
            "                        orm_kwargs[k] = obj[k]",
            "                new_obj = fkmany_class(**orm_kwargs)",
            "                new_fks.append(new_obj)",
            "        fkmany += new_fks",
            "        return fkmany",
            "",
            "    def update_from_object(self, obj: dict[str, Any]) -> None:",
            "        \"\"\"Update datasource from a data structure",
            "",
            "        The UI's table editor crafts a complex data structure that",
            "        contains most of the datasource's properties as well as",
            "        an array of metrics and columns objects. This method",
            "        receives the object from the UI and syncs the datasource to",
            "        match it. Since the fields are different for the different",
            "        connectors, the implementation uses ``update_from_object_fields``",
            "        which can be defined for each connector and",
            "        defines which fields should be synced\"\"\"",
            "        for attr in self.update_from_object_fields:",
            "            setattr(self, attr, obj.get(attr))",
            "",
            "        self.owners = obj.get(\"owners\", [])",
            "",
            "        # Syncing metrics",
            "        metrics = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"metrics\"], self.metrics, SqlMetric, \"metric_name\"",
            "            )",
            "            if \"metrics\" in obj",
            "            else []",
            "        )",
            "        self.metrics = metrics",
            "",
            "        # Syncing columns",
            "        self.columns = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"columns\"], self.columns, TableColumn, \"column_name\"",
            "            )",
            "            if \"columns\" in obj",
            "            else []",
            "        )",
            "",
            "    def get_extra_cache_keys(",
            "        self, query_obj: QueryObjectDict  # pylint: disable=unused-argument",
            "    ) -> list[Hashable]:",
            "        \"\"\"If a datasource needs to provide additional keys for calculation of",
            "        cache keys, those can be provided via this method",
            "",
            "        :param query_obj: The dict representation of a query object",
            "        :return: list of keys",
            "        \"\"\"",
            "        return []",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(self.uid)",
            "",
            "    def __eq__(self, other: object) -> bool:",
            "        if not isinstance(other, BaseDatasource):",
            "            return NotImplemented",
            "        return self.uid == other.uid",
            "",
            "    def raise_for_access(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        security_manager.raise_for_access(datasource=self)",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls, session: Session, datasource_name: str, schema: str, database_name: str",
            "    ) -> BaseDatasource | None:",
            "        raise NotImplementedError()",
            "",
            "",
            "class AnnotationDatasource(BaseDatasource):",
            "    \"\"\"Dummy object so we can query annotations using 'Viz' objects just like",
            "    regular datasources.",
            "    \"\"\"",
            "",
            "    cache_timeout = 0",
            "    changed_on = None",
            "    type = \"annotation\"",
            "    column_names = [",
            "        \"created_on\",",
            "        \"changed_on\",",
            "        \"id\",",
            "        \"start_dttm\",",
            "        \"end_dttm\",",
            "        \"layer_id\",",
            "        \"short_descr\",",
            "        \"long_descr\",",
            "        \"json_metadata\",",
            "        \"created_by_fk\",",
            "        \"changed_by_fk\",",
            "    ]",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        error_message = None",
            "        qry = db.session.query(Annotation)",
            "        qry = qry.filter(Annotation.layer_id == query_obj[\"filter\"][0][\"val\"])",
            "        if query_obj[\"from_dttm\"]:",
            "            qry = qry.filter(Annotation.start_dttm >= query_obj[\"from_dttm\"])",
            "        if query_obj[\"to_dttm\"]:",
            "            qry = qry.filter(Annotation.end_dttm <= query_obj[\"to_dttm\"])",
            "        status = QueryStatus.SUCCESS",
            "        try:",
            "            df = pd.read_sql_query(qry.statement, db.engine)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.exception(ex)",
            "            error_message = utils.error_msg_from_exception(ex)",
            "        return QueryResult(",
            "            status=status,",
            "            df=df,",
            "            duration=timedelta(0),",
            "            query=\"\",",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        raise NotImplementedError()",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "",
            "class TableColumn(Model, AuditMixinNullable, ImportExportMixin, CertificationMixin):",
            "",
            "    \"\"\"ORM object for table columns, each table can have multiple columns\"\"\"",
            "",
            "    __tablename__ = \"table_columns\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"column_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    column_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    is_active = Column(Boolean, default=True)",
            "    type = Column(Text)",
            "    advanced_data_type = Column(String(255))",
            "    groupby = Column(Boolean, default=True)",
            "    filterable = Column(Boolean, default=True)",
            "    description = Column(MediumText())",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    is_dttm = Column(Boolean, default=False)",
            "    expression = Column(MediumText())",
            "    python_date_format = Column(String(255))",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"columns\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"table_id\",",
            "        \"column_name\",",
            "        \"verbose_name\",",
            "        \"is_dttm\",",
            "        \"is_active\",",
            "        \"type\",",
            "        \"advanced_data_type\",",
            "        \"groupby\",",
            "        \"filterable\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"python_date_format\",",
            "        \"extra\",",
            "    ]",
            "",
            "    update_from_object_fields = [s for s in export_fields if s not in (\"table_id\",)]",
            "    export_parent = \"table\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object.",
            "",
            "        Historically a TableColumn object (from an ORM perspective) was tightly bound to",
            "        a SqlaTable object, however with the introduction of the Query datasource this",
            "        is no longer true, i.e., the SqlaTable relationship is optional.",
            "",
            "        Now the TableColumn is either directly associated with the Database object (",
            "        which is unknown to the ORM) or indirectly via the SqlaTable object (courtesy of",
            "        the ORM) depending on the context.",
            "        \"\"\"",
            "",
            "        self._database: Database | None = kwargs.pop(\"database\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    @reconstructor",
            "    def init_on_load(self) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object when invoked via the SQLAlchemy ORM.",
            "        \"\"\"",
            "",
            "        self._database = None",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.column_name)",
            "",
            "    @property",
            "    def is_boolean(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a boolean datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.BOOLEAN",
            "",
            "    @property",
            "    def is_numeric(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a numeric datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.NUMERIC",
            "",
            "    @property",
            "    def is_string(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a string datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.STRING",
            "",
            "    @property",
            "    def is_temporal(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a temporal datatype. If column has been set as",
            "        temporal/non-temporal (`is_dttm` is True or False respectively), return that",
            "        value. This usually happens during initial metadata fetching or when a column",
            "        is manually set as temporal (for this `python_date_format` needs to be set).",
            "        \"\"\"",
            "        if self.is_dttm is not None:",
            "            return self.is_dttm",
            "        return self.type_generic == GenericDataType.TEMPORAL",
            "",
            "    @property",
            "    def database(self) -> Database:",
            "        return self.table.database if self.table else self._database",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @property",
            "    def type_generic(self) -> utils.GenericDataType | None:",
            "        if self.is_dttm:",
            "            return GenericDataType.TEMPORAL",
            "",
            "        return (",
            "            column_spec.generic_type",
            "            if (",
            "                column_spec := self.db_engine_spec.get_column_spec(",
            "                    self.type,",
            "                    db_extra=self.db_extra,",
            "                )",
            "            )",
            "            else None",
            "        )",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        col = self.database.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    @property",
            "    def datasource(self) -> RelationshipProperty:",
            "        return self.table",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        time_grain: str | None,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TimestampExpression | Label:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "",
            "        pdf = self.python_date_format",
            "        is_epoch = pdf in (\"epoch_s\", \"epoch_ms\")",
            "        column_spec = self.db_engine_spec.get_column_spec(",
            "            self.type, db_extra=self.db_extra",
            "        )",
            "        type_ = column_spec.sqla_type if column_spec else DateTime",
            "        if not self.expression and not time_grain and not is_epoch:",
            "            sqla_col = column(self.column_name, type_=type_)",
            "            return self.database.make_sqla_column_compatible(sqla_col, label)",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, pdf, time_grain)",
            "        return self.database.make_sqla_column_compatible(time_expr, label)",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"advanced_data_type\",",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"column_name\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"filterable\",",
            "            \"groupby\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"is_dttm\",",
            "            \"python_date_format\",",
            "            \"type\",",
            "            \"type_generic\",",
            "            \"verbose_name\",",
            "            \"warning_markdown\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs if hasattr(self, s)}",
            "",
            "",
            "class SqlMetric(Model, AuditMixinNullable, ImportExportMixin, CertificationMixin):",
            "",
            "    \"\"\"ORM object for metrics, each table can have multiple metrics\"\"\"",
            "",
            "    __tablename__ = \"sql_metrics\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"metric_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    metric_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    metric_type = Column(String(32))",
            "    description = Column(MediumText())",
            "    d3format = Column(String(128))",
            "    currency = Column(String(128))",
            "    warning_text = Column(Text)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    expression = Column(MediumText(), nullable=False)",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"metrics\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"metric_name\",",
            "        \"verbose_name\",",
            "        \"metric_type\",",
            "        \"table_id\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"d3format\",",
            "        \"currency\",",
            "        \"extra\",",
            "        \"warning_text\",",
            "    ]",
            "    update_from_object_fields = list(s for s in export_fields if s != \"table_id\")",
            "    export_parent = \"table\"",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.metric_name)",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.metric_name",
            "        expression = self.expression",
            "        if template_processor:",
            "            expression = template_processor.process_template(expression)",
            "",
            "        sqla_col: ColumnClause = literal_column(expression)",
            "        return self.table.database.make_sqla_column_compatible(sqla_col, label)",
            "",
            "    @property",
            "    def perm(self) -> str | None:",
            "        return (",
            "            (\"{parent_name}.[{obj.metric_name}](id:{obj.id})\").format(",
            "                obj=self, parent_name=self.table.full_name",
            "            )",
            "            if self.table",
            "            else None",
            "        )",
            "",
            "    def get_perm(self) -> str | None:",
            "        return self.perm",
            "",
            "    @property",
            "    def currency_json(self) -> dict[str, str | None] | None:",
            "        try:",
            "            return json.loads(self.currency or \"{}\") or None",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load currency json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return None",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"currency\",",
            "            \"d3format\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"metric_name\",",
            "            \"warning_markdown\",",
            "            \"warning_text\",",
            "            \"verbose_name\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs}",
            "",
            "",
            "sqlatable_user = Table(",
            "    \"sqlatable_user\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"user_id\", Integer, ForeignKey(\"ab_user.id\", ondelete=\"CASCADE\")),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\")),",
            ")",
            "",
            "",
            "def _process_sql_expression(",
            "    expression: str | None,",
            "    database_id: int,",
            "    schema: str,",
            "    template_processor: BaseTemplateProcessor | None = None,",
            ") -> str | None:",
            "    if template_processor and expression:",
            "        expression = template_processor.process_template(expression)",
            "    if expression:",
            "        try:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            expression = sanitize_clause(expression)",
            "        except (QueryClauseValidationException, SupersetSecurityException) as ex:",
            "            raise QueryObjectValidationError(ex.message) from ex",
            "    return expression",
            "",
            "",
            "class SqlaTable(",
            "    Model, BaseDatasource, ExploreMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"An ORM object for SqlAlchemy table references\"\"\"",
            "",
            "    type = \"table\"",
            "    query_language = \"sql\"",
            "    is_rls_supported = True",
            "    columns: Mapped[list[TableColumn]] = relationship(",
            "        TableColumn,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metrics: Mapped[list[SqlMetric]] = relationship(",
            "        SqlMetric,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metric_class = SqlMetric",
            "    column_class = TableColumn",
            "    owner_class = security_manager.user_model",
            "",
            "    __tablename__ = \"tables\"",
            "",
            "    # Note this uniqueness constraint is not part of the physical schema, i.e., it does",
            "    # not exist in the migrations, but is required by `import_from_dict` to ensure the",
            "    # correct filters are applied in order to identify uniqueness.",
            "    #",
            "    # The reason it does not physically exist is MySQL, PostgreSQL, etc. have a",
            "    # different interpretation of uniqueness when it comes to NULL which is problematic",
            "    # given the schema is optional.",
            "    __table_args__ = (UniqueConstraint(\"database_id\", \"schema\", \"table_name\"),)",
            "",
            "    table_name = Column(String(250), nullable=False)",
            "    main_dttm_col = Column(String(250))",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "    fetch_values_predicate = Column(Text)",
            "    owners = relationship(owner_class, secondary=sqlatable_user, backref=\"tables\")",
            "    database: Database = relationship(",
            "        \"Database\",",
            "        backref=backref(\"tables\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[database_id],",
            "    )",
            "    schema = Column(String(255))",
            "    sql = Column(MediumText())",
            "    is_sqllab_view = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    extra = Column(Text)",
            "    normalize_columns = Column(Boolean, default=False)",
            "    always_filter_main_dttm = Column(Boolean, default=False)",
            "",
            "    baselink = \"tablemodelview\"",
            "",
            "    export_fields = [",
            "        \"table_name\",",
            "        \"main_dttm_col\",",
            "        \"description\",",
            "        \"default_endpoint\",",
            "        \"database_id\",",
            "        \"offset\",",
            "        \"cache_timeout\",",
            "        \"schema\",",
            "        \"sql\",",
            "        \"params\",",
            "        \"template_params\",",
            "        \"filter_select_enabled\",",
            "        \"fetch_values_predicate\",",
            "        \"extra\",",
            "        \"normalize_columns\",",
            "        \"always_filter_main_dttm\",",
            "    ]",
            "    update_from_object_fields = [f for f in export_fields if f != \"database_id\"]",
            "    export_parent = \"database\"",
            "    export_children = [\"metrics\", \"columns\"]",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "",
            "    def __repr__(self) -> str:  # pylint: disable=invalid-repr-returned",
            "        return self.name",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: str | None) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    @property",
            "    def db_engine_spec(self) -> __builtins__.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if not self.changed_by:",
            "            return \"\"",
            "        return str(self.changed_by)",
            "",
            "    @property",
            "    def connection(self) -> str:",
            "        return str(self.database)",
            "",
            "    @property",
            "    def description_markeddown(self) -> str:",
            "        return utils.markdown(self.description)",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        return self.table_name",
            "",
            "    @property",
            "    def datasource_type(self) -> str:",
            "        return self.type",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls,",
            "        session: Session,",
            "        datasource_name: str,",
            "        schema: str | None,",
            "        database_name: str,",
            "    ) -> SqlaTable | None:",
            "        schema = schema or None",
            "        query = (",
            "            session.query(cls)",
            "            .join(Database)",
            "            .filter(cls.table_name == datasource_name)",
            "            .filter(Database.database_name == database_name)",
            "        )",
            "        # Handling schema being '' or None, which is easier to handle",
            "        # in python than in the SQLA query in a multi-dialect way",
            "        for tbl in query.all():",
            "            if schema == (tbl.schema or None):",
            "                return tbl",
            "        return None",
            "",
            "    @property",
            "    def link(self) -> Markup:",
            "        name = escape(self.name)",
            "        anchor = f'<a target=\"_blank\" href=\"{self.explore_url}\">{name}</a>'",
            "        return Markup(anchor)",
            "",
            "    def get_schema_perm(self) -> str | None:",
            "        \"\"\"Returns schema permission if present, database one otherwise.\"\"\"",
            "        return security_manager.get_schema_perm(self.database, self.schema)",
            "",
            "    def get_perm(self) -> str:",
            "        \"\"\"",
            "        Return this dataset permission name",
            "        :return: dataset permission name",
            "        :raises DatasetInvalidPermissionEvaluationException: When database is missing",
            "        \"\"\"",
            "        if self.database is None:",
            "            raise DatasetInvalidPermissionEvaluationException()",
            "        return f\"[{self.database}].[{self.table_name}](id:{self.id})\"",
            "",
            "    @hybrid_property",
            "    def name(self) -> str:  # pylint: disable=invalid-overridden-method",
            "        return self.schema + \".\" + self.table_name if self.schema else self.table_name",
            "",
            "    @property",
            "    def full_name(self) -> str:",
            "        return utils.get_datasource_full_name(",
            "            self.database, self.table_name, schema=self.schema",
            "        )",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        l = [c.column_name for c in self.columns if c.is_dttm]",
            "        if self.main_dttm_col and self.main_dttm_col not in l:",
            "            l.append(self.main_dttm_col)",
            "        return l",
            "",
            "    @property",
            "    def num_cols(self) -> list[str]:",
            "        return [c.column_name for c in self.columns if c.is_numeric]",
            "",
            "    @property",
            "    def any_dttm_col(self) -> str | None:",
            "        cols = self.dttm_cols",
            "        return cols[0] if cols else None",
            "",
            "    @property",
            "    def html(self) -> str:",
            "        df = pd.DataFrame((c.column_name, c.type) for c in self.columns)",
            "        df.columns = [\"field\", \"type\"]",
            "        return df.to_html(",
            "            index=False,",
            "            classes=(\"dataframe table table-striped table-bordered \" \"table-condensed\"),",
            "        )",
            "",
            "    @property",
            "    def sql_url(self) -> str:",
            "        return self.database.sql_url + \"?table_name=\" + str(self.table_name)",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        # todo(yongjie): create a physical table column type in a separate PR",
            "        if self.sql:",
            "            return get_virtual_table_metadata(dataset=self)",
            "        return get_physical_table_metadata(",
            "            database=self.database,",
            "            table_name=self.table_name,",
            "            schema_name=self.schema,",
            "            normalize_columns=self.normalize_columns,",
            "        )",
            "",
            "    @property",
            "    def time_column_grains(self) -> dict[str, Any]:",
            "        return {",
            "            \"time_columns\": self.dttm_cols,",
            "            \"time_grains\": [grain.name for grain in self.database.grains()],",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        # show_cols and latest_partition set to false to avoid",
            "        # the expensive cost of inspecting the DB",
            "        return self.database.select_star(",
            "            self.table_name, schema=self.schema, show_cols=False, latest_partition=False",
            "        )",
            "",
            "    @property",
            "    def health_check_message(self) -> str | None:",
            "        check = config[\"DATASET_HEALTH_CHECK\"]",
            "        return check(self) if check else None",
            "",
            "    @property",
            "    def granularity_sqla(self) -> list[tuple[Any, Any]]:",
            "        return utils.choicify(self.dttm_cols)",
            "",
            "    @property",
            "    def time_grain_sqla(self) -> list[tuple[Any, Any]]:",
            "        return [(g.duration, g.name) for g in self.database.grains() or []]",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        data_ = super().data",
            "        if self.type == \"table\":",
            "            data_[\"granularity_sqla\"] = self.granularity_sqla",
            "            data_[\"time_grain_sqla\"] = self.time_grain_sqla",
            "            data_[\"main_dttm_col\"] = self.main_dttm_col",
            "            data_[\"fetch_values_predicate\"] = self.fetch_values_predicate",
            "            data_[\"template_params\"] = self.template_params",
            "            data_[\"is_sqllab_view\"] = self.is_sqllab_view",
            "            data_[\"health_check_message\"] = self.health_check_message",
            "            data_[\"extra\"] = self.extra",
            "            data_[\"owners\"] = self.owners_data",
            "            data_[\"always_filter_main_dttm\"] = self.always_filter_main_dttm",
            "            data_[\"normalize_columns\"] = self.normalize_columns",
            "        return data_",
            "",
            "    @property",
            "    def extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TextClause:",
            "        fetch_values_predicate = self.fetch_values_predicate",
            "        if template_processor:",
            "            fetch_values_predicate = template_processor.process_template(",
            "                fetch_values_predicate",
            "            )",
            "        try:",
            "            return self.text(fetch_values_predicate)",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in fetch values predicate: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        sql_query_mutator = config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(table=self, database=self.database, **kwargs)",
            "",
            "    def get_query_str_extended(",
            "        self,",
            "        query_obj: QueryObjectDict,",
            "        mutate: bool = True,",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def get_sqla_table(self) -> TableClause:",
            "        tbl = table(self.table_name)",
            "        if self.schema:",
            "            tbl.schema = self.schema",
            "        return tbl",
            "",
            "    def get_from_clause(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> tuple[TableClause | Alias, str | None]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "        if not self.is_virtual:",
            "            return self.get_sqla_table(), None",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            table_column: TableColumn | None = columns_by_name.get(column_name)",
            "            if table_column:",
            "                sqla_column = table_column.get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "            else:",
            "                sqla_column = column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = _process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    def adhoc_column_to_sqla(  # pylint: disable=too-many-locals",
            "        self,",
            "        col: AdhocColumn,",
            "        force_type_check: bool = False,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "",
            "        :param col: Adhoc column definition",
            "        :param force_type_check: Should the column type be checked in the db.",
            "               This is needed to validate if a filter with an adhoc column",
            "               is applicable.",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = utils.get_column_name(col)",
            "        expression = _process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        time_grain = col.get(\"timeGrain\")",
            "        has_timegrain = col.get(\"columnType\") == \"BASE_AXIS\" and time_grain",
            "        is_dttm = False",
            "        pdf = None",
            "        if col_in_metadata := self.get_column(expression):",
            "            sqla_column = col_in_metadata.get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "            is_dttm = col_in_metadata.is_temporal",
            "            pdf = col_in_metadata.python_date_format",
            "        else:",
            "            sqla_column = literal_column(expression)",
            "            if has_timegrain or force_type_check:",
            "                try:",
            "                    # probe adhoc column type",
            "                    tbl, _ = self.get_from_clause(template_processor)",
            "                    qry = sa.select([sqla_column]).limit(1).select_from(tbl)",
            "                    sql = self.database.compile_sqla_query(qry)",
            "                    col_desc = get_columns_description(self.database, self.schema, sql)",
            "                    if not col_desc:",
            "                        raise SupersetGenericDBErrorException(\"Column not found\")",
            "                    is_dttm = col_desc[0][\"is_dttm\"]  # type: ignore",
            "                except SupersetGenericDBErrorException as ex:",
            "                    raise ColumnNotFoundException(message=str(ex)) from ex",
            "",
            "        if is_dttm and has_timegrain:",
            "            sqla_column = self.db_engine_spec.get_timestamp_expr(",
            "                col=sqla_column,",
            "                pdf=pdf,",
            "                time_grain=time_grain,",
            "            )",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[int | str, list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, SqlMetric],",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> str | int | float | bool | Text:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if column_.type and column_.is_temporal and isinstance(value, str):",
            "            sql = self.db_engine_spec.convert_dttm(",
            "                column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "            )",
            "",
            "            if sql:",
            "                value = self.text(sql)",
            "",
            "        return value",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> pd.DataFrame | None:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_sqla_table_object(self) -> Table:",
            "        return self.database.get_table(self.table_name, schema=self.schema)",
            "",
            "    def fetch_metadata(self, commit: bool = True) -> MetadataResult:",
            "        \"\"\"",
            "        Fetches the metadata for the table and merges it in",
            "",
            "        :param commit: should the changes be committed or not.",
            "        :return: Tuple with lists of added, removed and modified column names.",
            "        \"\"\"",
            "        new_columns = self.external_metadata()",
            "        metrics = [",
            "            SqlMetric(**metric)",
            "            for metric in self.database.get_metrics(self.table_name, self.schema)",
            "        ]",
            "        any_date_col = None",
            "        db_engine_spec = self.db_engine_spec",
            "",
            "        # If no `self.id`, then this is a new table, no need to fetch columns",
            "        # from db.  Passing in `self.id` to query will actually automatically",
            "        # generate a new id, which can be tricky during certain transactions.",
            "        old_columns = (",
            "            (",
            "                db.session.query(TableColumn)",
            "                .filter(TableColumn.table_id == self.id)",
            "                .all()",
            "            )",
            "            if self.id",
            "            else self.columns",
            "        )",
            "",
            "        old_columns_by_name: dict[str, TableColumn] = {",
            "            col.column_name: col for col in old_columns",
            "        }",
            "        results = MetadataResult(",
            "            removed=[",
            "                col",
            "                for col in old_columns_by_name",
            "                if col not in {col[\"column_name\"] for col in new_columns}",
            "            ]",
            "        )",
            "",
            "        # clear old columns before adding modified columns back",
            "        columns = []",
            "        for col in new_columns:",
            "            old_column = old_columns_by_name.pop(col[\"column_name\"], None)",
            "            if not old_column:",
            "                results.added.append(col[\"column_name\"])",
            "                new_column = TableColumn(",
            "                    column_name=col[\"column_name\"],",
            "                    type=col[\"type\"],",
            "                    table=self,",
            "                )",
            "                new_column.is_dttm = new_column.is_temporal",
            "                db_engine_spec.alter_new_orm_column(new_column)",
            "            else:",
            "                new_column = old_column",
            "                if new_column.type != col[\"type\"]:",
            "                    results.modified.append(col[\"column_name\"])",
            "                new_column.type = col[\"type\"]",
            "                new_column.expression = \"\"",
            "            new_column.groupby = True",
            "            new_column.filterable = True",
            "            columns.append(new_column)",
            "            if not any_date_col and new_column.is_temporal:",
            "                any_date_col = col[\"column_name\"]",
            "",
            "        # add back calculated (virtual) columns",
            "        columns.extend([col for col in old_columns if col.expression])",
            "        self.columns = columns",
            "",
            "        if not self.main_dttm_col:",
            "            self.main_dttm_col = any_date_col",
            "        self.add_missing_metrics(metrics)",
            "",
            "        # Apply config supplied mutations.",
            "        config[\"SQLA_TABLE_MUTATOR\"](self)",
            "",
            "        db.session.merge(self)",
            "        if commit:",
            "            db.session.commit()",
            "        return results",
            "",
            "    @classmethod",
            "    def query_datasources_by_name(",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        datasource_name: str,",
            "        schema: str | None = None,",
            "    ) -> list[SqlaTable]:",
            "        query = (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter_by(table_name=datasource_name)",
            "        )",
            "        if schema:",
            "            query = query.filter_by(schema=schema)",
            "        return query.all()",
            "",
            "    @classmethod",
            "    def query_datasources_by_permissions(  # pylint: disable=invalid-name",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        permissions: set[str],",
            "        schema_perms: set[str],",
            "    ) -> list[SqlaTable]:",
            "        # TODO(hughhhh): add unit test",
            "        return (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter(",
            "                or_(",
            "                    SqlaTable.perm.in_(permissions),",
            "                    SqlaTable.schema_perm.in_(schema_perms),",
            "                )",
            "            )",
            "            .all()",
            "        )",
            "",
            "    @classmethod",
            "    def get_eager_sqlatable_datasource(",
            "        cls, session: Session, datasource_id: int",
            "    ) -> SqlaTable:",
            "        \"\"\"Returns SqlaTable with columns and metrics.\"\"\"",
            "        return (",
            "            session.query(cls)",
            "            .options(",
            "                sa.orm.subqueryload(cls.columns),",
            "                sa.orm.subqueryload(cls.metrics),",
            "            )",
            "            .filter_by(id=datasource_id)",
            "            .one()",
            "        )",
            "",
            "    @classmethod",
            "    def get_all_datasources(cls, session: Session) -> list[SqlaTable]:",
            "        qry = session.query(cls)",
            "        qry = cls.default_query(qry)",
            "        return qry.all()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry.filter_by(is_sqllab_view=False)",
            "",
            "    def has_extra_cache_key_calls(self, query_obj: QueryObjectDict) -> bool:",
            "        \"\"\"",
            "        Detects the presence of calls to `ExtraCache` methods in items in query_obj that",
            "        can be templated. If any are present, the query must be evaluated to extract",
            "        additional keys for the cache key. This method is needed to avoid executing the",
            "        template code unnecessarily, as it may contain expensive calls, e.g. to extract",
            "        the latest partition of a database.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: True if there are call(s) to an `ExtraCache` method, False otherwise",
            "        \"\"\"",
            "        templatable_statements: list[str] = []",
            "        if self.sql:",
            "            templatable_statements.append(self.sql)",
            "        if self.fetch_values_predicate:",
            "            templatable_statements.append(self.fetch_values_predicate)",
            "        extras = query_obj.get(\"extras\", {})",
            "        if \"where\" in extras:",
            "            templatable_statements.append(extras[\"where\"])",
            "        if \"having\" in extras:",
            "            templatable_statements.append(extras[\"having\"])",
            "        if self.is_rls_supported:",
            "            templatable_statements += [",
            "                f.clause for f in security_manager.get_rls_filters(self)",
            "            ]",
            "        for statement in templatable_statements:",
            "            if ExtraCache.regex.search(statement):",
            "                return True",
            "        return False",
            "",
            "    def get_extra_cache_keys(self, query_obj: QueryObjectDict) -> list[Hashable]:",
            "        \"\"\"",
            "        The cache key of a SqlaTable needs to consider any keys added by the parent",
            "        class and any keys added via `ExtraCache`.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: The extra cache keys",
            "        \"\"\"",
            "        extra_cache_keys = super().get_extra_cache_keys(query_obj)",
            "        if self.has_extra_cache_key_calls(query_obj):",
            "            sqla_query = self.get_sqla_query(**query_obj)",
            "            extra_cache_keys += sqla_query.extra_cache_keys",
            "        return extra_cache_keys",
            "",
            "    @property",
            "    def quote_identifier(self) -> Callable[[str], str]:",
            "        return self.database.quote_identifier",
            "",
            "    @staticmethod",
            "    def before_update(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Note this listener is called when any fields are being updated",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        :raises Exception: If the target table is not unique",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_before_update(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def update_column(  # pylint: disable=unused-argument",
            "        mapper: Mapper, connection: Connection, target: SqlMetric | TableColumn",
            "    ) -> None:",
            "        \"\"\"",
            "        :param mapper: Unused.",
            "        :param connection: Unused.",
            "        :param target: The metric or column that was updated.",
            "        \"\"\"",
            "        inspector = inspect(target)",
            "        session = inspector.session",
            "",
            "        # Forces an update to the table's changed_on value when a metric or column on the",
            "        # table is updated. This busts the cache key for all charts that use the table.",
            "        session.execute(update(SqlaTable).where(SqlaTable.id == target.table.id))",
            "",
            "    @staticmethod",
            "    def after_insert(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after insert",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_after_insert(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def after_delete(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after delete",
            "        \"\"\"",
            "        security_manager.dataset_after_delete(mapper, connection, sqla_table)",
            "",
            "    def load_database(self: SqlaTable) -> None:",
            "        # somehow the database attribute is not loaded on access",
            "        if self.database_id and (",
            "            not self.database or self.database.id != self.database_id",
            "        ):",
            "            session = inspect(self).session",
            "            self.database = session.query(Database).filter_by(id=self.database_id).one()",
            "",
            "",
            "sa.event.listen(SqlaTable, \"before_update\", SqlaTable.before_update)",
            "sa.event.listen(SqlaTable, \"after_insert\", SqlaTable.after_insert)",
            "sa.event.listen(SqlaTable, \"after_delete\", SqlaTable.after_delete)",
            "sa.event.listen(SqlMetric, \"after_update\", SqlaTable.update_column)",
            "sa.event.listen(TableColumn, \"after_update\", SqlaTable.update_column)",
            "",
            "RLSFilterRoles = Table(",
            "    \"rls_filter_roles\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"role_id\", Integer, ForeignKey(\"ab_role.id\"), nullable=False),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "RLSFilterTables = Table(",
            "    \"rls_filter_tables\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\")),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "",
            "class RowLevelSecurityFilter(Model, AuditMixinNullable):",
            "    \"\"\"",
            "    Custom where clauses attached to Tables and Roles.",
            "    \"\"\"",
            "",
            "    __tablename__ = \"row_level_security_filters\"",
            "    id = Column(Integer, primary_key=True)",
            "    name = Column(String(255), unique=True, nullable=False)",
            "    description = Column(Text)",
            "    filter_type = Column(",
            "        Enum(*[filter_type.value for filter_type in utils.RowLevelSecurityFilterType])",
            "    )",
            "    group_key = Column(String(255), nullable=True)",
            "    roles = relationship(",
            "        security_manager.role_model,",
            "        secondary=RLSFilterRoles,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    tables = relationship(",
            "        SqlaTable,",
            "        overlaps=\"table\",",
            "        secondary=RLSFilterTables,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    clause = Column(Text, nullable=False)"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "from __future__ import annotations",
            "",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from dataclasses import dataclass, field",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, Callable, cast",
            "",
            "import dateutil.parser",
            "import numpy as np",
            "import pandas as pd",
            "import sqlalchemy as sa",
            "import sqlparse",
            "from flask import escape, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import (",
            "    and_,",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    inspect,",
            "    Integer,",
            "    or_,",
            "    String,",
            "    Table,",
            "    Text,",
            "    update,",
            ")",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.ext.hybrid import hybrid_property",
            "from sqlalchemy.orm import (",
            "    backref,",
            "    foreign,",
            "    Mapped,",
            "    Query,",
            "    reconstructor,",
            "    relationship,",
            "    RelationshipProperty,",
            "    Session,",
            ")",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.schema import UniqueConstraint",
            "from sqlalchemy.sql import column, ColumnElement, literal_column, table",
            "from sqlalchemy.sql.elements import ColumnClause, TextClause",
            "from sqlalchemy.sql.expression import Label, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "",
            "from superset import app, db, is_feature_enabled, security_manager",
            "from superset.commands.dataset.exceptions import DatasetNotFoundError",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.connectors.sqla.utils import (",
            "    get_columns_description,",
            "    get_physical_table_metadata,",
            "    get_virtual_table_metadata,",
            ")",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import BaseEngineSpec, TimestampExpression",
            "from superset.exceptions import (",
            "    ColumnNotFoundException,",
            "    DatasetInvalidPermissionEvaluationException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.jinja_context import (",
            "    BaseTemplateProcessor,",
            "    ExtraCache,",
            "    get_template_processor,",
            ")",
            "from superset.models.annotations import Annotation",
            "from superset.models.core import Database",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    CertificationMixin,",
            "    ExploreMixin,",
            "    ImportExportMixin,",
            "    QueryResult,",
            "    QueryStringExtended,",
            "    validate_adhoc_subquery,",
            ")",
            "from superset.models.slice import Slice",
            "from superset.sql_parse import ParsedQuery, sanitize_clause",
            "from superset.superset_typing import (",
            "    AdhocColumn,",
            "    AdhocMetric,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    QueryObjectDict,",
            "    ResultSetColumnType,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.backports import StrEnum",
            "from superset.utils.core import GenericDataType, MediumText",
            "",
            "config = app.config",
            "metadata = Model.metadata  # pylint: disable=no-member",
            "logger = logging.getLogger(__name__)",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "",
            "# a non-exhaustive set of additive metrics",
            "ADDITIVE_METRIC_TYPES = {",
            "    \"count\",",
            "    \"sum\",",
            "    \"doubleSum\",",
            "}",
            "ADDITIVE_METRIC_TYPES_LOWER = {op.lower() for op in ADDITIVE_METRIC_TYPES}",
            "",
            "",
            "@dataclass",
            "class MetadataResult:",
            "    added: list[str] = field(default_factory=list)",
            "    removed: list[str] = field(default_factory=list)",
            "    modified: list[str] = field(default_factory=list)",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "METRIC_FORM_DATA_PARAMS = [",
            "    \"metric\",",
            "    \"metric_2\",",
            "    \"metrics\",",
            "    \"metrics_b\",",
            "    \"percent_metrics\",",
            "    \"secondary_metric\",",
            "    \"size\",",
            "    \"timeseries_limit_metric\",",
            "    \"x\",",
            "    \"y\",",
            "]",
            "",
            "COLUMN_FORM_DATA_PARAMS = [",
            "    \"all_columns\",",
            "    \"all_columns_x\",",
            "    \"columns\",",
            "    \"entity\",",
            "    \"groupby\",",
            "    \"order_by_cols\",",
            "    \"series\",",
            "]",
            "",
            "",
            "class DatasourceKind(StrEnum):",
            "    VIRTUAL = \"virtual\"",
            "    PHYSICAL = \"physical\"",
            "",
            "",
            "class BaseDatasource(",
            "    AuditMixinNullable, ImportExportMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"A common interface to objects that are queryable",
            "    (tables and datasources)\"\"\"",
            "",
            "    # ---------------------------------------------------------------",
            "    # class attributes to define when deriving BaseDatasource",
            "    # ---------------------------------------------------------------",
            "    __tablename__: str | None = None  # {connector_name}_datasource",
            "    baselink: str | None = None  # url portion pointing to ModelView endpoint",
            "",
            "    owner_class: User | None = None",
            "",
            "    # Used to do code highlighting when displaying the query in the UI",
            "    query_language: str | None = None",
            "",
            "    # Only some datasources support Row Level Security",
            "    is_rls_supported: bool = False",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        # can be a Column or a property pointing to one",
            "        raise NotImplementedError()",
            "",
            "    # ---------------------------------------------------------------",
            "",
            "    # Columns",
            "    id = Column(Integer, primary_key=True)",
            "    description = Column(Text)",
            "    default_endpoint = Column(Text)",
            "    is_featured = Column(Boolean, default=False)  # TODO deprecating",
            "    filter_select_enabled = Column(Boolean, default=True)",
            "    offset = Column(Integer, default=0)",
            "    cache_timeout = Column(Integer)",
            "    params = Column(String(1000))",
            "    perm = Column(String(1000))",
            "    schema_perm = Column(String(1000))",
            "    is_managed_externally = Column(Boolean, nullable=False, default=False)",
            "    external_url = Column(Text, nullable=True)",
            "",
            "    sql: str | None = None",
            "    owners: list[User]",
            "    update_from_object_fields: list[str]",
            "",
            "    extra_import_fields = [\"is_managed_externally\", \"external_url\"]",
            "",
            "    @property",
            "    def kind(self) -> DatasourceKind:",
            "        return DatasourceKind.VIRTUAL if self.sql else DatasourceKind.PHYSICAL",
            "",
            "    @property",
            "    def owners_data(self) -> list[dict[str, Any]]:",
            "        return [",
            "            {",
            "                \"first_name\": o.first_name,",
            "                \"last_name\": o.last_name,",
            "                \"username\": o.username,",
            "                \"id\": o.id,",
            "            }",
            "            for o in self.owners",
            "        ]",
            "",
            "    @property",
            "    def is_virtual(self) -> bool:",
            "        return self.kind == DatasourceKind.VIRTUAL",
            "",
            "    @declared_attr",
            "    def slices(self) -> RelationshipProperty:",
            "        return relationship(",
            "            \"Slice\",",
            "            overlaps=\"table\",",
            "            primaryjoin=lambda: and_(",
            "                foreign(Slice.datasource_id) == self.id,",
            "                foreign(Slice.datasource_type) == self.type,",
            "            ),",
            "        )",
            "",
            "    columns: list[TableColumn] = []",
            "    metrics: list[SqlMetric] = []",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        \"\"\"Unique id across datasource types\"\"\"",
            "        return f\"{self.id}__{self.type}\"",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns], key=lambda x: x or \"\")",
            "",
            "    @property",
            "    def columns_types(self) -> dict[str, str]:",
            "        return {c.column_name: c.type for c in self.columns}",
            "",
            "    @property",
            "    def main_dttm_col(self) -> str:",
            "        return \"timestamp\"",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def connection(self) -> str | None:",
            "        \"\"\"String representing the context of the Datasource\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def schema(self) -> str | None:",
            "        \"\"\"String representing the schema of the Datasource (if it applies)\"\"\"",
            "        return None",
            "",
            "    @property",
            "    def filterable_column_names(self) -> list[str]:",
            "        return sorted([c.column_name for c in self.columns if c.filterable])",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        return []",
            "",
            "    @property",
            "    def url(self) -> str:",
            "        return f\"/{self.baselink}/edit/{self.id}\"",
            "",
            "    @property",
            "    def explore_url(self) -> str:",
            "        if self.default_endpoint:",
            "            return self.default_endpoint",
            "        return f\"/explore/?datasource_type={self.type}&datasource_id={self.id}\"",
            "",
            "    @property",
            "    def column_formats(self) -> dict[str, str | None]:",
            "        return {m.metric_name: m.d3format for m in self.metrics if m.d3format}",
            "",
            "    @property",
            "    def currency_formats(self) -> dict[str, dict[str, str | None] | None]:",
            "        return {m.metric_name: m.currency_json for m in self.metrics if m.currency_json}",
            "",
            "    def add_missing_metrics(self, metrics: list[SqlMetric]) -> None:",
            "        existing_metrics = {m.metric_name for m in self.metrics}",
            "        for metric in metrics:",
            "            if metric.metric_name not in existing_metrics:",
            "                metric.table_id = self.id",
            "                self.metrics.append(metric)",
            "",
            "    @property",
            "    def short_data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            \"edit_url\": self.url,",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"schema\": self.schema,",
            "            \"name\": self.name,",
            "            \"type\": self.type,",
            "            \"connection\": self.connection,",
            "            \"creator\": str(self.created_by),",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        pass",
            "",
            "    @property",
            "    def order_by_choices(self) -> list[tuple[str, str]]:",
            "        choices = []",
            "        # self.column_names return sorted column_names",
            "        for column_name in self.column_names:",
            "            column_name = str(column_name or \"\")",
            "            choices.append(",
            "                (json.dumps([column_name, True]), f\"{column_name} \" + __(\"[asc]\"))",
            "            )",
            "            choices.append(",
            "                (json.dumps([column_name, False]), f\"{column_name} \" + __(\"[desc]\"))",
            "            )",
            "        return choices",
            "",
            "    @property",
            "    def verbose_map(self) -> dict[str, str]:",
            "        verb_map = {\"__timestamp\": \"Time\"}",
            "        verb_map.update(",
            "            {o.metric_name: o.verbose_name or o.metric_name for o in self.metrics}",
            "        )",
            "        verb_map.update(",
            "            {o.column_name: o.verbose_name or o.column_name for o in self.columns}",
            "        )",
            "        return verb_map",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        \"\"\"Data representation of the datasource sent to the frontend\"\"\"",
            "        return {",
            "            # simple fields",
            "            \"id\": self.id,",
            "            \"uid\": self.uid,",
            "            \"column_formats\": self.column_formats,",
            "            \"currency_formats\": self.currency_formats,",
            "            \"description\": self.description,",
            "            \"database\": self.database.data,  # pylint: disable=no-member",
            "            \"default_endpoint\": self.default_endpoint,",
            "            \"filter_select\": self.filter_select_enabled,  # TODO deprecate",
            "            \"filter_select_enabled\": self.filter_select_enabled,",
            "            \"name\": self.name,",
            "            \"datasource_name\": self.datasource_name,",
            "            \"table_name\": self.datasource_name,",
            "            \"type\": self.type,",
            "            \"schema\": self.schema,",
            "            \"offset\": self.offset,",
            "            \"cache_timeout\": self.cache_timeout,",
            "            \"params\": self.params,",
            "            \"perm\": self.perm,",
            "            \"edit_url\": self.url,",
            "            # sqla-specific",
            "            \"sql\": self.sql,",
            "            # one to many",
            "            \"columns\": [o.data for o in self.columns],",
            "            \"metrics\": [o.data for o in self.metrics],",
            "            # TODO deprecate, move logic to JS",
            "            \"order_by_choices\": self.order_by_choices,",
            "            \"owners\": [owner.id for owner in self.owners],",
            "            \"verbose_map\": self.verbose_map,",
            "            \"select_star\": self.select_star,",
            "        }",
            "",
            "    def data_for_slices(  # pylint: disable=too-many-locals",
            "        self, slices: list[Slice]",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        The representation of the datasource containing only the required data",
            "        to render the provided slices.",
            "",
            "        Used to reduce the payload when loading a dashboard.",
            "        \"\"\"",
            "        data = self.data",
            "        metric_names = set()",
            "        column_names = set()",
            "        for slc in slices:",
            "            form_data = slc.form_data",
            "            # pull out all required metrics from the form_data",
            "            for metric_param in METRIC_FORM_DATA_PARAMS:",
            "                for metric in utils.as_list(form_data.get(metric_param) or []):",
            "                    metric_names.add(utils.get_metric_name(metric))",
            "                    if utils.is_adhoc_metric(metric):",
            "                        column_ = metric.get(\"column\") or {}",
            "                        if column_name := column_.get(\"column_name\"):",
            "                            column_names.add(column_name)",
            "",
            "            # Columns used in query filters",
            "            column_names.update(",
            "                filter_[\"subject\"]",
            "                for filter_ in form_data.get(\"adhoc_filters\") or []",
            "                if filter_.get(\"clause\") == \"WHERE\" and filter_.get(\"subject\")",
            "            )",
            "",
            "            # columns used by Filter Box",
            "            column_names.update(",
            "                filter_config[\"column\"]",
            "                for filter_config in form_data.get(\"filter_configs\") or []",
            "                if \"column\" in filter_config",
            "            )",
            "",
            "            # for legacy dashboard imports which have the wrong query_context in them",
            "            try:",
            "                query_context = slc.get_query_context()",
            "            except DatasetNotFoundError:",
            "                query_context = None",
            "",
            "            # legacy charts don't have query_context charts",
            "            if query_context:",
            "                column_names.update(",
            "                    [",
            "                        utils.get_column_name(column_)",
            "                        for query in query_context.queries",
            "                        for column_ in query.columns",
            "                    ]",
            "                    or []",
            "                )",
            "            else:",
            "                _columns = [",
            "                    utils.get_column_name(column_)",
            "                    if utils.is_adhoc_column(column_)",
            "                    else column_",
            "                    for column_param in COLUMN_FORM_DATA_PARAMS",
            "                    for column_ in utils.as_list(form_data.get(column_param) or [])",
            "                ]",
            "                column_names.update(_columns)",
            "",
            "        filtered_metrics = [",
            "            metric",
            "            for metric in data[\"metrics\"]",
            "            if metric[\"metric_name\"] in metric_names",
            "        ]",
            "",
            "        filtered_columns: list[Column] = []",
            "        column_types: set[GenericDataType] = set()",
            "        for column_ in data[\"columns\"]:",
            "            generic_type = column_.get(\"type_generic\")",
            "            if generic_type is not None:",
            "                column_types.add(generic_type)",
            "            if column_[\"column_name\"] in column_names:",
            "                filtered_columns.append(column_)",
            "",
            "        data[\"column_types\"] = list(column_types)",
            "        del data[\"description\"]",
            "        data.update({\"metrics\": filtered_metrics})",
            "        data.update({\"columns\": filtered_columns})",
            "        verbose_map = {\"__timestamp\": \"Time\"}",
            "        verbose_map.update(",
            "            {",
            "                metric[\"metric_name\"]: metric[\"verbose_name\"] or metric[\"metric_name\"]",
            "                for metric in filtered_metrics",
            "            }",
            "        )",
            "        verbose_map.update(",
            "            {",
            "                column_[\"column_name\"]: column_[\"verbose_name\"]",
            "                or column_[\"column_name\"]",
            "                for column_ in filtered_columns",
            "            }",
            "        )",
            "        data[\"verbose_map\"] = verbose_map",
            "",
            "        return data",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: FilterValues | None,",
            "        operator: str,",
            "        target_generic_type: GenericDataType,",
            "        target_native_type: str | None = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: builtins.type[BaseEngineSpec] | None = None,",
            "        db_extra: dict[str, Any] | None = None,",
            "    ) -> FilterValues | None:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: FilterValue | None) -> FilterValue | None:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        \"\"\"Returns column information from the external system\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        \"\"\"Returns a query as a string",
            "",
            "        This is used to be displayed to the user so that they can",
            "        understand what is taking place behind the scene\"\"\"",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        \"\"\"Executes the query and returns a dataframe",
            "",
            "        query_obj is a dictionary representing Superset's query interface.",
            "        Should return a ``superset.models.helpers.QueryResult``",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry",
            "",
            "    def get_column(self, column_name: str | None) -> TableColumn | None:",
            "        if not column_name:",
            "            return None",
            "        for col in self.columns:",
            "            if col.column_name == column_name:",
            "                return col",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_fk_many_from_list(",
            "        object_list: list[Any],",
            "        fkmany: list[Column],",
            "        fkmany_class: builtins.type[TableColumn | SqlMetric],",
            "        key_attr: str,",
            "    ) -> list[Column]:",
            "        \"\"\"Update ORM one-to-many list from object list",
            "",
            "        Used for syncing metrics and columns using the same code\"\"\"",
            "",
            "        object_dict = {o.get(key_attr): o for o in object_list}",
            "",
            "        # delete fks that have been removed",
            "        fkmany = [o for o in fkmany if getattr(o, key_attr) in object_dict]",
            "",
            "        # sync existing fks",
            "        for fk in fkmany:",
            "            obj = object_dict.get(getattr(fk, key_attr))",
            "            if obj:",
            "                for attr in fkmany_class.update_from_object_fields:",
            "                    setattr(fk, attr, obj.get(attr))",
            "",
            "        # create new fks",
            "        new_fks = []",
            "        orm_keys = [getattr(o, key_attr) for o in fkmany]",
            "        for obj in object_list:",
            "            key = obj.get(key_attr)",
            "            if key not in orm_keys:",
            "                del obj[\"id\"]",
            "                orm_kwargs = {}",
            "                for k in obj:",
            "                    if k in fkmany_class.update_from_object_fields and k in obj:",
            "                        orm_kwargs[k] = obj[k]",
            "                new_obj = fkmany_class(**orm_kwargs)",
            "                new_fks.append(new_obj)",
            "        fkmany += new_fks",
            "        return fkmany",
            "",
            "    def update_from_object(self, obj: dict[str, Any]) -> None:",
            "        \"\"\"Update datasource from a data structure",
            "",
            "        The UI's table editor crafts a complex data structure that",
            "        contains most of the datasource's properties as well as",
            "        an array of metrics and columns objects. This method",
            "        receives the object from the UI and syncs the datasource to",
            "        match it. Since the fields are different for the different",
            "        connectors, the implementation uses ``update_from_object_fields``",
            "        which can be defined for each connector and",
            "        defines which fields should be synced\"\"\"",
            "        for attr in self.update_from_object_fields:",
            "            setattr(self, attr, obj.get(attr))",
            "",
            "        self.owners = obj.get(\"owners\", [])",
            "",
            "        # Syncing metrics",
            "        metrics = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"metrics\"], self.metrics, SqlMetric, \"metric_name\"",
            "            )",
            "            if \"metrics\" in obj",
            "            else []",
            "        )",
            "        self.metrics = metrics",
            "",
            "        # Syncing columns",
            "        self.columns = (",
            "            self.get_fk_many_from_list(",
            "                obj[\"columns\"], self.columns, TableColumn, \"column_name\"",
            "            )",
            "            if \"columns\" in obj",
            "            else []",
            "        )",
            "",
            "    def get_extra_cache_keys(",
            "        self, query_obj: QueryObjectDict  # pylint: disable=unused-argument",
            "    ) -> list[Hashable]:",
            "        \"\"\"If a datasource needs to provide additional keys for calculation of",
            "        cache keys, those can be provided via this method",
            "",
            "        :param query_obj: The dict representation of a query object",
            "        :return: list of keys",
            "        \"\"\"",
            "        return []",
            "",
            "    def __hash__(self) -> int:",
            "        return hash(self.uid)",
            "",
            "    def __eq__(self, other: object) -> bool:",
            "        if not isinstance(other, BaseDatasource):",
            "            return NotImplemented",
            "        return self.uid == other.uid",
            "",
            "    def raise_for_access(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        security_manager.raise_for_access(datasource=self)",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls, session: Session, datasource_name: str, schema: str, database_name: str",
            "    ) -> BaseDatasource | None:",
            "        raise NotImplementedError()",
            "",
            "",
            "class AnnotationDatasource(BaseDatasource):",
            "    \"\"\"Dummy object so we can query annotations using 'Viz' objects just like",
            "    regular datasources.",
            "    \"\"\"",
            "",
            "    cache_timeout = 0",
            "    changed_on = None",
            "    type = \"annotation\"",
            "    column_names = [",
            "        \"created_on\",",
            "        \"changed_on\",",
            "        \"id\",",
            "        \"start_dttm\",",
            "        \"end_dttm\",",
            "        \"layer_id\",",
            "        \"short_descr\",",
            "        \"long_descr\",",
            "        \"json_metadata\",",
            "        \"created_by_fk\",",
            "        \"changed_by_fk\",",
            "    ]",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        error_message = None",
            "        qry = db.session.query(Annotation)",
            "        qry = qry.filter(Annotation.layer_id == query_obj[\"filter\"][0][\"val\"])",
            "        if query_obj[\"from_dttm\"]:",
            "            qry = qry.filter(Annotation.start_dttm >= query_obj[\"from_dttm\"])",
            "        if query_obj[\"to_dttm\"]:",
            "            qry = qry.filter(Annotation.end_dttm <= query_obj[\"to_dttm\"])",
            "        status = QueryStatus.SUCCESS",
            "        try:",
            "            df = pd.read_sql_query(qry.statement, db.engine)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.exception(ex)",
            "            error_message = utils.error_msg_from_exception(ex)",
            "        return QueryResult(",
            "            status=status,",
            "            df=df,",
            "            duration=timedelta(0),",
            "            query=\"\",",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        raise NotImplementedError()",
            "",
            "    def values_for_column(self, column_name: str, limit: int = 10000) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "",
            "class TableColumn(Model, AuditMixinNullable, ImportExportMixin, CertificationMixin):",
            "",
            "    \"\"\"ORM object for table columns, each table can have multiple columns\"\"\"",
            "",
            "    __tablename__ = \"table_columns\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"column_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    column_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    is_active = Column(Boolean, default=True)",
            "    type = Column(Text)",
            "    advanced_data_type = Column(String(255))",
            "    groupby = Column(Boolean, default=True)",
            "    filterable = Column(Boolean, default=True)",
            "    description = Column(MediumText())",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    is_dttm = Column(Boolean, default=False)",
            "    expression = Column(MediumText())",
            "    python_date_format = Column(String(255))",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"columns\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"table_id\",",
            "        \"column_name\",",
            "        \"verbose_name\",",
            "        \"is_dttm\",",
            "        \"is_active\",",
            "        \"type\",",
            "        \"advanced_data_type\",",
            "        \"groupby\",",
            "        \"filterable\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"python_date_format\",",
            "        \"extra\",",
            "    ]",
            "",
            "    update_from_object_fields = [s for s in export_fields if s not in (\"table_id\",)]",
            "    export_parent = \"table\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object.",
            "",
            "        Historically a TableColumn object (from an ORM perspective) was tightly bound to",
            "        a SqlaTable object, however with the introduction of the Query datasource this",
            "        is no longer true, i.e., the SqlaTable relationship is optional.",
            "",
            "        Now the TableColumn is either directly associated with the Database object (",
            "        which is unknown to the ORM) or indirectly via the SqlaTable object (courtesy of",
            "        the ORM) depending on the context.",
            "        \"\"\"",
            "",
            "        self._database: Database | None = kwargs.pop(\"database\", None)",
            "        super().__init__(**kwargs)",
            "",
            "    @reconstructor",
            "    def init_on_load(self) -> None:",
            "        \"\"\"",
            "        Construct a TableColumn object when invoked via the SQLAlchemy ORM.",
            "        \"\"\"",
            "",
            "        self._database = None",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.column_name)",
            "",
            "    @property",
            "    def is_boolean(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a boolean datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.BOOLEAN",
            "",
            "    @property",
            "    def is_numeric(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a numeric datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.NUMERIC",
            "",
            "    @property",
            "    def is_string(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a string datatype.",
            "        \"\"\"",
            "        return self.type_generic == GenericDataType.STRING",
            "",
            "    @property",
            "    def is_temporal(self) -> bool:",
            "        \"\"\"",
            "        Check if the column has a temporal datatype. If column has been set as",
            "        temporal/non-temporal (`is_dttm` is True or False respectively), return that",
            "        value. This usually happens during initial metadata fetching or when a column",
            "        is manually set as temporal (for this `python_date_format` needs to be set).",
            "        \"\"\"",
            "        if self.is_dttm is not None:",
            "            return self.is_dttm",
            "        return self.type_generic == GenericDataType.TEMPORAL",
            "",
            "    @property",
            "    def database(self) -> Database:",
            "        return self.table.database if self.table else self._database",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @property",
            "    def type_generic(self) -> utils.GenericDataType | None:",
            "        if self.is_dttm:",
            "            return GenericDataType.TEMPORAL",
            "",
            "        return (",
            "            column_spec.generic_type",
            "            if (",
            "                column_spec := self.db_engine_spec.get_column_spec(",
            "                    self.type,",
            "                    db_extra=self.db_extra,",
            "                )",
            "            )",
            "            else None",
            "        )",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        col = self.database.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    @property",
            "    def datasource(self) -> RelationshipProperty:",
            "        return self.table",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        time_grain: str | None,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TimestampExpression | Label:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "",
            "        pdf = self.python_date_format",
            "        is_epoch = pdf in (\"epoch_s\", \"epoch_ms\")",
            "        column_spec = self.db_engine_spec.get_column_spec(",
            "            self.type, db_extra=self.db_extra",
            "        )",
            "        type_ = column_spec.sqla_type if column_spec else DateTime",
            "        if not self.expression and not time_grain and not is_epoch:",
            "            sqla_col = column(self.column_name, type_=type_)",
            "            return self.database.make_sqla_column_compatible(sqla_col, label)",
            "        if expression := self.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = column(self.column_name, type_=type_)",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, pdf, time_grain)",
            "        return self.database.make_sqla_column_compatible(time_expr, label)",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"advanced_data_type\",",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"column_name\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"filterable\",",
            "            \"groupby\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"is_dttm\",",
            "            \"python_date_format\",",
            "            \"type\",",
            "            \"type_generic\",",
            "            \"verbose_name\",",
            "            \"warning_markdown\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs if hasattr(self, s)}",
            "",
            "",
            "class SqlMetric(Model, AuditMixinNullable, ImportExportMixin, CertificationMixin):",
            "",
            "    \"\"\"ORM object for metrics, each table can have multiple metrics\"\"\"",
            "",
            "    __tablename__ = \"sql_metrics\"",
            "    __table_args__ = (UniqueConstraint(\"table_id\", \"metric_name\"),)",
            "",
            "    id = Column(Integer, primary_key=True)",
            "    metric_name = Column(String(255), nullable=False)",
            "    verbose_name = Column(String(1024))",
            "    metric_type = Column(String(32))",
            "    description = Column(MediumText())",
            "    d3format = Column(String(128))",
            "    currency = Column(String(128))",
            "    warning_text = Column(Text)",
            "    table_id = Column(Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\"))",
            "    expression = Column(MediumText(), nullable=False)",
            "    extra = Column(Text)",
            "",
            "    table: Mapped[SqlaTable] = relationship(",
            "        \"SqlaTable\",",
            "        back_populates=\"metrics\",",
            "    )",
            "",
            "    export_fields = [",
            "        \"metric_name\",",
            "        \"verbose_name\",",
            "        \"metric_type\",",
            "        \"table_id\",",
            "        \"expression\",",
            "        \"description\",",
            "        \"d3format\",",
            "        \"currency\",",
            "        \"extra\",",
            "        \"warning_text\",",
            "    ]",
            "    update_from_object_fields = list(s for s in export_fields if s != \"table_id\")",
            "    export_parent = \"table\"",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.metric_name)",
            "",
            "    def get_sqla_col(",
            "        self,",
            "        label: str | None = None,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        label = label or self.metric_name",
            "        expression = self.expression",
            "        if template_processor:",
            "            expression = template_processor.process_template(expression)",
            "",
            "        sqla_col: ColumnClause = literal_column(expression)",
            "        return self.table.database.make_sqla_column_compatible(sqla_col, label)",
            "",
            "    @property",
            "    def perm(self) -> str | None:",
            "        return (",
            "            (\"{parent_name}.[{obj.metric_name}](id:{obj.id})\").format(",
            "                obj=self, parent_name=self.table.full_name",
            "            )",
            "            if self.table",
            "            else None",
            "        )",
            "",
            "    def get_perm(self) -> str | None:",
            "        return self.perm",
            "",
            "    @property",
            "    def currency_json(self) -> dict[str, str | None] | None:",
            "        try:",
            "            return json.loads(self.currency or \"{}\") or None",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load currency json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return None",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        attrs = (",
            "            \"certification_details\",",
            "            \"certified_by\",",
            "            \"currency\",",
            "            \"d3format\",",
            "            \"description\",",
            "            \"expression\",",
            "            \"id\",",
            "            \"is_certified\",",
            "            \"metric_name\",",
            "            \"warning_markdown\",",
            "            \"warning_text\",",
            "            \"verbose_name\",",
            "        )",
            "",
            "        return {s: getattr(self, s) for s in attrs}",
            "",
            "",
            "sqlatable_user = Table(",
            "    \"sqlatable_user\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"user_id\", Integer, ForeignKey(\"ab_user.id\", ondelete=\"CASCADE\")),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\", ondelete=\"CASCADE\")),",
            ")",
            "",
            "",
            "def _process_sql_expression(",
            "    expression: str | None,",
            "    database_id: int,",
            "    schema: str,",
            "    template_processor: BaseTemplateProcessor | None = None,",
            ") -> str | None:",
            "    if template_processor and expression:",
            "        expression = template_processor.process_template(expression)",
            "    if expression:",
            "        try:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            expression = sanitize_clause(expression)",
            "        except (QueryClauseValidationException, SupersetSecurityException) as ex:",
            "            raise QueryObjectValidationError(ex.message) from ex",
            "    return expression",
            "",
            "",
            "class SqlaTable(",
            "    Model, BaseDatasource, ExploreMixin",
            "):  # pylint: disable=too-many-public-methods",
            "    \"\"\"An ORM object for SqlAlchemy table references\"\"\"",
            "",
            "    type = \"table\"",
            "    query_language = \"sql\"",
            "    is_rls_supported = True",
            "    columns: Mapped[list[TableColumn]] = relationship(",
            "        TableColumn,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metrics: Mapped[list[SqlMetric]] = relationship(",
            "        SqlMetric,",
            "        back_populates=\"table\",",
            "        cascade=\"all, delete-orphan\",",
            "        passive_deletes=True,",
            "    )",
            "    metric_class = SqlMetric",
            "    column_class = TableColumn",
            "    owner_class = security_manager.user_model",
            "",
            "    __tablename__ = \"tables\"",
            "",
            "    # Note this uniqueness constraint is not part of the physical schema, i.e., it does",
            "    # not exist in the migrations, but is required by `import_from_dict` to ensure the",
            "    # correct filters are applied in order to identify uniqueness.",
            "    #",
            "    # The reason it does not physically exist is MySQL, PostgreSQL, etc. have a",
            "    # different interpretation of uniqueness when it comes to NULL which is problematic",
            "    # given the schema is optional.",
            "    __table_args__ = (UniqueConstraint(\"database_id\", \"schema\", \"table_name\"),)",
            "",
            "    table_name = Column(String(250), nullable=False)",
            "    main_dttm_col = Column(String(250))",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "    fetch_values_predicate = Column(Text)",
            "    owners = relationship(owner_class, secondary=sqlatable_user, backref=\"tables\")",
            "    database: Database = relationship(",
            "        \"Database\",",
            "        backref=backref(\"tables\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[database_id],",
            "    )",
            "    schema = Column(String(255))",
            "    sql = Column(MediumText())",
            "    is_sqllab_view = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    extra = Column(Text)",
            "    normalize_columns = Column(Boolean, default=False)",
            "    always_filter_main_dttm = Column(Boolean, default=False)",
            "",
            "    baselink = \"tablemodelview\"",
            "",
            "    export_fields = [",
            "        \"table_name\",",
            "        \"main_dttm_col\",",
            "        \"description\",",
            "        \"default_endpoint\",",
            "        \"database_id\",",
            "        \"offset\",",
            "        \"cache_timeout\",",
            "        \"schema\",",
            "        \"sql\",",
            "        \"params\",",
            "        \"template_params\",",
            "        \"filter_select_enabled\",",
            "        \"fetch_values_predicate\",",
            "        \"extra\",",
            "        \"normalize_columns\",",
            "        \"always_filter_main_dttm\",",
            "    ]",
            "    update_from_object_fields = [f for f in export_fields if f != \"database_id\"]",
            "    export_parent = \"database\"",
            "    export_children = [\"metrics\", \"columns\"]",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "",
            "    def __repr__(self) -> str:  # pylint: disable=invalid-repr-returned",
            "        return self.name",
            "",
            "    @property",
            "    def db_extra(self) -> dict[str, Any]:",
            "        return self.database.get_extra()",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: str | None) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    @property",
            "    def db_engine_spec(self) -> __builtins__.type[BaseEngineSpec]:",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if not self.changed_by:",
            "            return \"\"",
            "        return str(self.changed_by)",
            "",
            "    @property",
            "    def connection(self) -> str:",
            "        return str(self.database)",
            "",
            "    @property",
            "    def description_markeddown(self) -> str:",
            "        return utils.markdown(self.description)",
            "",
            "    @property",
            "    def datasource_name(self) -> str:",
            "        return self.table_name",
            "",
            "    @property",
            "    def datasource_type(self) -> str:",
            "        return self.type",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @classmethod",
            "    def get_datasource_by_name(",
            "        cls,",
            "        session: Session,",
            "        datasource_name: str,",
            "        schema: str | None,",
            "        database_name: str,",
            "    ) -> SqlaTable | None:",
            "        schema = schema or None",
            "        query = (",
            "            session.query(cls)",
            "            .join(Database)",
            "            .filter(cls.table_name == datasource_name)",
            "            .filter(Database.database_name == database_name)",
            "        )",
            "        # Handling schema being '' or None, which is easier to handle",
            "        # in python than in the SQLA query in a multi-dialect way",
            "        for tbl in query.all():",
            "            if schema == (tbl.schema or None):",
            "                return tbl",
            "        return None",
            "",
            "    @property",
            "    def link(self) -> Markup:",
            "        name = escape(self.name)",
            "        anchor = f'<a target=\"_blank\" href=\"{self.explore_url}\">{name}</a>'",
            "        return Markup(anchor)",
            "",
            "    def get_schema_perm(self) -> str | None:",
            "        \"\"\"Returns schema permission if present, database one otherwise.\"\"\"",
            "        return security_manager.get_schema_perm(self.database, self.schema)",
            "",
            "    def get_perm(self) -> str:",
            "        \"\"\"",
            "        Return this dataset permission name",
            "        :return: dataset permission name",
            "        :raises DatasetInvalidPermissionEvaluationException: When database is missing",
            "        \"\"\"",
            "        if self.database is None:",
            "            raise DatasetInvalidPermissionEvaluationException()",
            "        return f\"[{self.database}].[{self.table_name}](id:{self.id})\"",
            "",
            "    @hybrid_property",
            "    def name(self) -> str:  # pylint: disable=invalid-overridden-method",
            "        return self.schema + \".\" + self.table_name if self.schema else self.table_name",
            "",
            "    @property",
            "    def full_name(self) -> str:",
            "        return utils.get_datasource_full_name(",
            "            self.database, self.table_name, schema=self.schema",
            "        )",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        l = [c.column_name for c in self.columns if c.is_dttm]",
            "        if self.main_dttm_col and self.main_dttm_col not in l:",
            "            l.append(self.main_dttm_col)",
            "        return l",
            "",
            "    @property",
            "    def num_cols(self) -> list[str]:",
            "        return [c.column_name for c in self.columns if c.is_numeric]",
            "",
            "    @property",
            "    def any_dttm_col(self) -> str | None:",
            "        cols = self.dttm_cols",
            "        return cols[0] if cols else None",
            "",
            "    @property",
            "    def html(self) -> str:",
            "        df = pd.DataFrame((c.column_name, c.type) for c in self.columns)",
            "        df.columns = [\"field\", \"type\"]",
            "        return df.to_html(",
            "            index=False,",
            "            classes=(\"dataframe table table-striped table-bordered \" \"table-condensed\"),",
            "        )",
            "",
            "    @property",
            "    def sql_url(self) -> str:",
            "        return self.database.sql_url + \"?table_name=\" + str(self.table_name)",
            "",
            "    def external_metadata(self) -> list[ResultSetColumnType]:",
            "        # todo(yongjie): create a physical table column type in a separate PR",
            "        if self.sql:",
            "            return get_virtual_table_metadata(dataset=self)",
            "        return get_physical_table_metadata(",
            "            database=self.database,",
            "            table_name=self.table_name,",
            "            schema_name=self.schema,",
            "            normalize_columns=self.normalize_columns,",
            "        )",
            "",
            "    @property",
            "    def time_column_grains(self) -> dict[str, Any]:",
            "        return {",
            "            \"time_columns\": self.dttm_cols,",
            "            \"time_grains\": [grain.name for grain in self.database.grains()],",
            "        }",
            "",
            "    @property",
            "    def select_star(self) -> str | None:",
            "        # show_cols and latest_partition set to false to avoid",
            "        # the expensive cost of inspecting the DB",
            "        return self.database.select_star(",
            "            self.table_name, schema=self.schema, show_cols=False, latest_partition=False",
            "        )",
            "",
            "    @property",
            "    def health_check_message(self) -> str | None:",
            "        check = config[\"DATASET_HEALTH_CHECK\"]",
            "        return check(self) if check else None",
            "",
            "    @property",
            "    def granularity_sqla(self) -> list[tuple[Any, Any]]:",
            "        return utils.choicify(self.dttm_cols)",
            "",
            "    @property",
            "    def time_grain_sqla(self) -> list[tuple[Any, Any]]:",
            "        return [(g.duration, g.name) for g in self.database.grains() or []]",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        data_ = super().data",
            "        if self.type == \"table\":",
            "            data_[\"granularity_sqla\"] = self.granularity_sqla",
            "            data_[\"time_grain_sqla\"] = self.time_grain_sqla",
            "            data_[\"main_dttm_col\"] = self.main_dttm_col",
            "            data_[\"fetch_values_predicate\"] = self.fetch_values_predicate",
            "            data_[\"template_params\"] = self.template_params",
            "            data_[\"is_sqllab_view\"] = self.is_sqllab_view",
            "            data_[\"health_check_message\"] = self.health_check_message",
            "            data_[\"extra\"] = self.extra",
            "            data_[\"owners\"] = self.owners_data",
            "            data_[\"always_filter_main_dttm\"] = self.always_filter_main_dttm",
            "            data_[\"normalize_columns\"] = self.normalize_columns",
            "        return data_",
            "",
            "    @property",
            "    def extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> TextClause:",
            "        fetch_values_predicate = self.fetch_values_predicate",
            "        if template_processor:",
            "            fetch_values_predicate = template_processor.process_template(",
            "                fetch_values_predicate",
            "            )",
            "        try:",
            "            return self.text(fetch_values_predicate)",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in fetch values predicate: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        sql_query_mutator = config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(table=self, database=self.database, **kwargs)",
            "",
            "    def get_query_str_extended(",
            "        self,",
            "        query_obj: QueryObjectDict,",
            "        mutate: bool = True,",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def get_sqla_table(self) -> TableClause:",
            "        tbl = table(self.table_name)",
            "        if self.schema:",
            "            tbl.schema = self.schema",
            "        return tbl",
            "",
            "    def get_from_clause(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> tuple[TableClause | Alias, str | None]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "        if not self.is_virtual:",
            "            return self.get_sqla_table(), None",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: BaseTemplateProcessor | None = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            table_column: TableColumn | None = columns_by_name.get(column_name)",
            "            if table_column:",
            "                sqla_column = table_column.get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "            else:",
            "                sqla_column = column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = _process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    def adhoc_column_to_sqla(  # pylint: disable=too-many-locals",
            "        self,",
            "        col: AdhocColumn,",
            "        force_type_check: bool = False,",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "",
            "        :param col: Adhoc column definition",
            "        :param force_type_check: Should the column type be checked in the db.",
            "               This is needed to validate if a filter with an adhoc column",
            "               is applicable.",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = utils.get_column_name(col)",
            "        expression = _process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        time_grain = col.get(\"timeGrain\")",
            "        has_timegrain = col.get(\"columnType\") == \"BASE_AXIS\" and time_grain",
            "        is_dttm = False",
            "        pdf = None",
            "        if col_in_metadata := self.get_column(expression):",
            "            sqla_column = col_in_metadata.get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "            is_dttm = col_in_metadata.is_temporal",
            "            pdf = col_in_metadata.python_date_format",
            "        else:",
            "            sqla_column = literal_column(expression)",
            "            if has_timegrain or force_type_check:",
            "                try:",
            "                    # probe adhoc column type",
            "                    tbl, _ = self.get_from_clause(template_processor)",
            "                    qry = sa.select([sqla_column]).limit(1).select_from(tbl)",
            "                    sql = self.database.compile_sqla_query(qry)",
            "                    col_desc = get_columns_description(self.database, self.schema, sql)",
            "                    if not col_desc:",
            "                        raise SupersetGenericDBErrorException(\"Column not found\")",
            "                    is_dttm = col_desc[0][\"is_dttm\"]  # type: ignore",
            "                except SupersetGenericDBErrorException as ex:",
            "                    raise ColumnNotFoundException(message=str(ex)) from ex",
            "",
            "        if is_dttm and has_timegrain:",
            "            sqla_column = self.db_engine_spec.get_timestamp_expr(",
            "                col=sqla_column,",
            "                pdf=pdf,",
            "                time_grain=time_grain,",
            "            )",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[int | str, list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, SqlMetric],",
            "        columns_by_name: dict[str, TableColumn],",
            "        template_processor: BaseTemplateProcessor | None = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> str | int | float | bool | Text:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if column_.type and column_.is_temporal and isinstance(value, str):",
            "            sql = self.db_engine_spec.convert_dttm(",
            "                column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "            )",
            "",
            "            if sql:",
            "                value = self.text(sql)",
            "",
            "        return value",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, TableColumn],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> pd.DataFrame | None:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_sqla_table_object(self) -> Table:",
            "        return self.database.get_table(self.table_name, schema=self.schema)",
            "",
            "    def fetch_metadata(self, commit: bool = True) -> MetadataResult:",
            "        \"\"\"",
            "        Fetches the metadata for the table and merges it in",
            "",
            "        :param commit: should the changes be committed or not.",
            "        :return: Tuple with lists of added, removed and modified column names.",
            "        \"\"\"",
            "        new_columns = self.external_metadata()",
            "        metrics = [",
            "            SqlMetric(**metric)",
            "            for metric in self.database.get_metrics(self.table_name, self.schema)",
            "        ]",
            "        any_date_col = None",
            "        db_engine_spec = self.db_engine_spec",
            "",
            "        # If no `self.id`, then this is a new table, no need to fetch columns",
            "        # from db.  Passing in `self.id` to query will actually automatically",
            "        # generate a new id, which can be tricky during certain transactions.",
            "        old_columns = (",
            "            (",
            "                db.session.query(TableColumn)",
            "                .filter(TableColumn.table_id == self.id)",
            "                .all()",
            "            )",
            "            if self.id",
            "            else self.columns",
            "        )",
            "",
            "        old_columns_by_name: dict[str, TableColumn] = {",
            "            col.column_name: col for col in old_columns",
            "        }",
            "        results = MetadataResult(",
            "            removed=[",
            "                col",
            "                for col in old_columns_by_name",
            "                if col not in {col[\"column_name\"] for col in new_columns}",
            "            ]",
            "        )",
            "",
            "        # clear old columns before adding modified columns back",
            "        columns = []",
            "        for col in new_columns:",
            "            old_column = old_columns_by_name.pop(col[\"column_name\"], None)",
            "            if not old_column:",
            "                results.added.append(col[\"column_name\"])",
            "                new_column = TableColumn(",
            "                    column_name=col[\"column_name\"],",
            "                    type=col[\"type\"],",
            "                    table=self,",
            "                )",
            "                new_column.is_dttm = new_column.is_temporal",
            "                db_engine_spec.alter_new_orm_column(new_column)",
            "            else:",
            "                new_column = old_column",
            "                if new_column.type != col[\"type\"]:",
            "                    results.modified.append(col[\"column_name\"])",
            "                new_column.type = col[\"type\"]",
            "                new_column.expression = \"\"",
            "            new_column.groupby = True",
            "            new_column.filterable = True",
            "            columns.append(new_column)",
            "            if not any_date_col and new_column.is_temporal:",
            "                any_date_col = col[\"column_name\"]",
            "",
            "        # add back calculated (virtual) columns",
            "        columns.extend([col for col in old_columns if col.expression])",
            "        self.columns = columns",
            "",
            "        if not self.main_dttm_col:",
            "            self.main_dttm_col = any_date_col",
            "        self.add_missing_metrics(metrics)",
            "",
            "        # Apply config supplied mutations.",
            "        config[\"SQLA_TABLE_MUTATOR\"](self)",
            "",
            "        db.session.merge(self)",
            "        if commit:",
            "            db.session.commit()",
            "        return results",
            "",
            "    @classmethod",
            "    def query_datasources_by_name(",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        datasource_name: str,",
            "        schema: str | None = None,",
            "    ) -> list[SqlaTable]:",
            "        query = (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter_by(table_name=datasource_name)",
            "        )",
            "        if schema:",
            "            query = query.filter_by(schema=schema)",
            "        return query.all()",
            "",
            "    @classmethod",
            "    def query_datasources_by_permissions(  # pylint: disable=invalid-name",
            "        cls,",
            "        session: Session,",
            "        database: Database,",
            "        permissions: set[str],",
            "        schema_perms: set[str],",
            "    ) -> list[SqlaTable]:",
            "        # TODO(hughhhh): add unit test",
            "        return (",
            "            session.query(cls)",
            "            .filter_by(database_id=database.id)",
            "            .filter(",
            "                or_(",
            "                    SqlaTable.perm.in_(permissions),",
            "                    SqlaTable.schema_perm.in_(schema_perms),",
            "                )",
            "            )",
            "            .all()",
            "        )",
            "",
            "    @classmethod",
            "    def get_eager_sqlatable_datasource(",
            "        cls, session: Session, datasource_id: int",
            "    ) -> SqlaTable:",
            "        \"\"\"Returns SqlaTable with columns and metrics.\"\"\"",
            "        return (",
            "            session.query(cls)",
            "            .options(",
            "                sa.orm.subqueryload(cls.columns),",
            "                sa.orm.subqueryload(cls.metrics),",
            "            )",
            "            .filter_by(id=datasource_id)",
            "            .one()",
            "        )",
            "",
            "    @classmethod",
            "    def get_all_datasources(cls, session: Session) -> list[SqlaTable]:",
            "        qry = session.query(cls)",
            "        qry = cls.default_query(qry)",
            "        return qry.all()",
            "",
            "    @staticmethod",
            "    def default_query(qry: Query) -> Query:",
            "        return qry.filter_by(is_sqllab_view=False)",
            "",
            "    def has_extra_cache_key_calls(self, query_obj: QueryObjectDict) -> bool:",
            "        \"\"\"",
            "        Detects the presence of calls to `ExtraCache` methods in items in query_obj that",
            "        can be templated. If any are present, the query must be evaluated to extract",
            "        additional keys for the cache key. This method is needed to avoid executing the",
            "        template code unnecessarily, as it may contain expensive calls, e.g. to extract",
            "        the latest partition of a database.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: True if there are call(s) to an `ExtraCache` method, False otherwise",
            "        \"\"\"",
            "        templatable_statements: list[str] = []",
            "        if self.sql:",
            "            templatable_statements.append(self.sql)",
            "        if self.fetch_values_predicate:",
            "            templatable_statements.append(self.fetch_values_predicate)",
            "        extras = query_obj.get(\"extras\", {})",
            "        if \"where\" in extras:",
            "            templatable_statements.append(extras[\"where\"])",
            "        if \"having\" in extras:",
            "            templatable_statements.append(extras[\"having\"])",
            "        if self.is_rls_supported:",
            "            templatable_statements += [",
            "                f.clause for f in security_manager.get_rls_filters(self)",
            "            ]",
            "        for statement in templatable_statements:",
            "            if ExtraCache.regex.search(statement):",
            "                return True",
            "        return False",
            "",
            "    def get_extra_cache_keys(self, query_obj: QueryObjectDict) -> list[Hashable]:",
            "        \"\"\"",
            "        The cache key of a SqlaTable needs to consider any keys added by the parent",
            "        class and any keys added via `ExtraCache`.",
            "",
            "        :param query_obj: query object to analyze",
            "        :return: The extra cache keys",
            "        \"\"\"",
            "        extra_cache_keys = super().get_extra_cache_keys(query_obj)",
            "        if self.has_extra_cache_key_calls(query_obj):",
            "            sqla_query = self.get_sqla_query(**query_obj)",
            "            extra_cache_keys += sqla_query.extra_cache_keys",
            "        return extra_cache_keys",
            "",
            "    @property",
            "    def quote_identifier(self) -> Callable[[str], str]:",
            "        return self.database.quote_identifier",
            "",
            "    @staticmethod",
            "    def before_update(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Note this listener is called when any fields are being updated",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        :raises Exception: If the target table is not unique",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_before_update(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def update_column(  # pylint: disable=unused-argument",
            "        mapper: Mapper, connection: Connection, target: SqlMetric | TableColumn",
            "    ) -> None:",
            "        \"\"\"",
            "        :param mapper: Unused.",
            "        :param connection: Unused.",
            "        :param target: The metric or column that was updated.",
            "        \"\"\"",
            "        inspector = inspect(target)",
            "        session = inspector.session",
            "",
            "        # Forces an update to the table's changed_on value when a metric or column on the",
            "        # table is updated. This busts the cache key for all charts that use the table.",
            "        session.execute(update(SqlaTable).where(SqlaTable.id == target.table.id))",
            "",
            "    @staticmethod",
            "    def after_insert(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after insert",
            "        \"\"\"",
            "        target.load_database()",
            "        security_manager.dataset_after_insert(mapper, connection, target)",
            "",
            "    @staticmethod",
            "    def after_delete(",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        sqla_table: SqlaTable,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update dataset permissions after delete",
            "        \"\"\"",
            "        security_manager.dataset_after_delete(mapper, connection, sqla_table)",
            "",
            "    def load_database(self: SqlaTable) -> None:",
            "        # somehow the database attribute is not loaded on access",
            "        if self.database_id and (",
            "            not self.database or self.database.id != self.database_id",
            "        ):",
            "            session = inspect(self).session",
            "            self.database = session.query(Database).filter_by(id=self.database_id).one()",
            "",
            "",
            "sa.event.listen(SqlaTable, \"before_update\", SqlaTable.before_update)",
            "sa.event.listen(SqlaTable, \"after_insert\", SqlaTable.after_insert)",
            "sa.event.listen(SqlaTable, \"after_delete\", SqlaTable.after_delete)",
            "sa.event.listen(SqlMetric, \"after_update\", SqlaTable.update_column)",
            "sa.event.listen(TableColumn, \"after_update\", SqlaTable.update_column)",
            "",
            "RLSFilterRoles = Table(",
            "    \"rls_filter_roles\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"role_id\", Integer, ForeignKey(\"ab_role.id\"), nullable=False),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "RLSFilterTables = Table(",
            "    \"rls_filter_tables\",",
            "    metadata,",
            "    Column(\"id\", Integer, primary_key=True),",
            "    Column(\"table_id\", Integer, ForeignKey(\"tables.id\")),",
            "    Column(\"rls_filter_id\", Integer, ForeignKey(\"row_level_security_filters.id\")),",
            ")",
            "",
            "",
            "class RowLevelSecurityFilter(Model, AuditMixinNullable):",
            "    \"\"\"",
            "    Custom where clauses attached to Tables and Roles.",
            "    \"\"\"",
            "",
            "    __tablename__ = \"row_level_security_filters\"",
            "    id = Column(Integer, primary_key=True)",
            "    name = Column(String(255), unique=True, nullable=False)",
            "    description = Column(Text)",
            "    filter_type = Column(",
            "        Enum(*[filter_type.value for filter_type in utils.RowLevelSecurityFilterType])",
            "    )",
            "    group_key = Column(String(255), nullable=True)",
            "    roles = relationship(",
            "        security_manager.role_model,",
            "        secondary=RLSFilterRoles,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    tables = relationship(",
            "        SqlaTable,",
            "        overlaps=\"table\",",
            "        secondary=RLSFilterTables,",
            "        backref=\"row_level_security_filters\",",
            "    )",
            "    clause = Column(Text, nullable=False)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1461": [
                "SqlaTable",
                "get_from_clause"
            ]
        },
        "addLocation": []
    },
    "superset/connectors/sqla/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "     sql = dataset.get_template_processor().process_template("
            },
            "1": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "         dataset.sql, **dataset.template_params_dict"
            },
            "2": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 113,
                "PatchRowcode": "     )"
            },
            "3": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    parsed_query = ParsedQuery(sql)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+    parsed_query = ParsedQuery(sql, engine=db_engine_spec.engine)"
            },
            "5": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "     if not db_engine_spec.is_readonly_query(parsed_query):"
            },
            "6": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "         raise SupersetSecurityException("
            },
            "7": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "             SupersetError("
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import logging",
            "from collections.abc import Iterable, Iterator",
            "from functools import lru_cache",
            "from typing import Callable, TYPE_CHECKING, TypeVar",
            "from uuid import UUID",
            "",
            "from flask_babel import lazy_gettext as _",
            "from sqlalchemy.engine.url import URL as SqlaURL",
            "from sqlalchemy.exc import NoSuchTableError",
            "from sqlalchemy.ext.declarative import DeclarativeMeta",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.exc import ObjectDeletedError",
            "from sqlalchemy.sql.type_api import TypeEngine",
            "",
            "from superset.constants import LRU_CACHE_MAX_SIZE",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.models.core import Database",
            "from superset.result_set import SupersetResultSet",
            "from superset.sql_parse import ParsedQuery",
            "from superset.superset_typing import ResultSetColumnType",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import SqlaTable",
            "",
            "",
            "def get_physical_table_metadata(",
            "    database: Database,",
            "    table_name: str,",
            "    normalize_columns: bool,",
            "    schema_name: str | None = None,",
            ") -> list[ResultSetColumnType]:",
            "    \"\"\"Use SQLAlchemy inspector to get table metadata\"\"\"",
            "    db_engine_spec = database.db_engine_spec",
            "    db_dialect = database.get_dialect()",
            "    # ensure empty schema",
            "    _schema_name = schema_name if schema_name else None",
            "    # Table does not exist or is not visible to a connection.",
            "",
            "    if not (",
            "        database.has_table_by_name(table_name=table_name, schema=_schema_name)",
            "        or database.has_view_by_name(view_name=table_name, schema=_schema_name)",
            "    ):",
            "        raise NoSuchTableError",
            "",
            "    cols = database.get_columns(table_name, schema=_schema_name)",
            "    for col in cols:",
            "        try:",
            "            if isinstance(col[\"type\"], TypeEngine):",
            "                name = col[\"column_name\"]",
            "                if not normalize_columns:",
            "                    name = db_engine_spec.denormalize_name(db_dialect, name)",
            "",
            "                db_type = db_engine_spec.column_datatype_to_string(",
            "                    col[\"type\"], db_dialect",
            "                )",
            "                type_spec = db_engine_spec.get_column_spec(",
            "                    db_type, db_extra=database.get_extra()",
            "                )",
            "                col.update(",
            "                    {",
            "                        \"name\": name,",
            "                        \"column_name\": name,",
            "                        \"type\": db_type,",
            "                        \"type_generic\": type_spec.generic_type if type_spec else None,",
            "                        \"is_dttm\": type_spec.is_dttm if type_spec else None,",
            "                    }",
            "                )",
            "        # Broad exception catch, because there are multiple possible exceptions",
            "        # from different drivers that fall outside CompileError",
            "        except Exception:  # pylint: disable=broad-except",
            "            col.update(",
            "                {",
            "                    \"type\": \"UNKNOWN\",",
            "                    \"type_generic\": None,",
            "                    \"is_dttm\": None,",
            "                }",
            "            )",
            "    return cols",
            "",
            "",
            "def get_virtual_table_metadata(dataset: SqlaTable) -> list[ResultSetColumnType]:",
            "    \"\"\"Use SQLparser to get virtual dataset metadata\"\"\"",
            "    if not dataset.sql:",
            "        raise SupersetGenericDBErrorException(",
            "            message=_(\"Virtual dataset query cannot be empty\"),",
            "        )",
            "",
            "    db_engine_spec = dataset.database.db_engine_spec",
            "    sql = dataset.get_template_processor().process_template(",
            "        dataset.sql, **dataset.template_params_dict",
            "    )",
            "    parsed_query = ParsedQuery(sql)",
            "    if not db_engine_spec.is_readonly_query(parsed_query):",
            "        raise SupersetSecurityException(",
            "            SupersetError(",
            "                error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "                message=_(\"Only `SELECT` statements are allowed\"),",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    statements = parsed_query.get_statements()",
            "    if len(statements) > 1:",
            "        raise SupersetSecurityException(",
            "            SupersetError(",
            "                error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "                message=_(\"Only single queries supported\"),",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    return get_columns_description(dataset.database, dataset.schema, statements[0])",
            "",
            "",
            "def get_columns_description(",
            "    database: Database,",
            "    schema: str | None,",
            "    query: str,",
            ") -> list[ResultSetColumnType]:",
            "    # TODO(villebro): refactor to use same code that's used by",
            "    #  sql_lab.py:execute_sql_statements",
            "    db_engine_spec = database.db_engine_spec",
            "    try:",
            "        with database.get_raw_connection(schema=schema) as conn:",
            "            cursor = conn.cursor()",
            "            query = database.apply_limit_to_sql(query, limit=1)",
            "            cursor.execute(query)",
            "            db_engine_spec.execute(cursor, query)",
            "            result = db_engine_spec.fetch_data(cursor, limit=1)",
            "            result_set = SupersetResultSet(result, cursor.description, db_engine_spec)",
            "            return result_set.columns",
            "    except Exception as ex:",
            "        raise SupersetGenericDBErrorException(message=str(ex)) from ex",
            "",
            "",
            "@lru_cache(maxsize=LRU_CACHE_MAX_SIZE)",
            "def get_dialect_name(drivername: str) -> str:",
            "    return SqlaURL.create(drivername).get_dialect().name",
            "",
            "",
            "@lru_cache(maxsize=LRU_CACHE_MAX_SIZE)",
            "def get_identifier_quoter(drivername: str) -> dict[str, Callable[[str], str]]:",
            "    return SqlaURL.create(drivername).get_dialect()().identifier_preparer.quote",
            "",
            "",
            "DeclarativeModel = TypeVar(\"DeclarativeModel\", bound=DeclarativeMeta)",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def find_cached_objects_in_session(",
            "    session: Session,",
            "    cls: type[DeclarativeModel],",
            "    ids: Iterable[int] | None = None,",
            "    uuids: Iterable[UUID] | None = None,",
            ") -> Iterator[DeclarativeModel]:",
            "    \"\"\"Find known ORM instances in cached SQLA session states.",
            "",
            "    :param session: a SQLA session",
            "    :param cls: a SQLA DeclarativeModel",
            "    :param ids: ids of the desired model instances (optional)",
            "    :param uuids: uuids of the desired instances, will be ignored if `ids` are provides",
            "    \"\"\"",
            "    if not ids and not uuids:",
            "        return iter([])",
            "    uuids = uuids or []",
            "    try:",
            "        items = list(session)",
            "    except ObjectDeletedError:",
            "        logger.warning(\"ObjectDeletedError\", exc_info=True)",
            "        return iter(())",
            "",
            "    return (",
            "        item",
            "        # `session` is an iterator of all known items",
            "        for item in items",
            "        if isinstance(item, cls) and (item.id in ids if ids else item.uuid in uuids)",
            "    )"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "from __future__ import annotations",
            "",
            "import logging",
            "from collections.abc import Iterable, Iterator",
            "from functools import lru_cache",
            "from typing import Callable, TYPE_CHECKING, TypeVar",
            "from uuid import UUID",
            "",
            "from flask_babel import lazy_gettext as _",
            "from sqlalchemy.engine.url import URL as SqlaURL",
            "from sqlalchemy.exc import NoSuchTableError",
            "from sqlalchemy.ext.declarative import DeclarativeMeta",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.orm.exc import ObjectDeletedError",
            "from sqlalchemy.sql.type_api import TypeEngine",
            "",
            "from superset.constants import LRU_CACHE_MAX_SIZE",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    SupersetGenericDBErrorException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.models.core import Database",
            "from superset.result_set import SupersetResultSet",
            "from superset.sql_parse import ParsedQuery",
            "from superset.superset_typing import ResultSetColumnType",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import SqlaTable",
            "",
            "",
            "def get_physical_table_metadata(",
            "    database: Database,",
            "    table_name: str,",
            "    normalize_columns: bool,",
            "    schema_name: str | None = None,",
            ") -> list[ResultSetColumnType]:",
            "    \"\"\"Use SQLAlchemy inspector to get table metadata\"\"\"",
            "    db_engine_spec = database.db_engine_spec",
            "    db_dialect = database.get_dialect()",
            "    # ensure empty schema",
            "    _schema_name = schema_name if schema_name else None",
            "    # Table does not exist or is not visible to a connection.",
            "",
            "    if not (",
            "        database.has_table_by_name(table_name=table_name, schema=_schema_name)",
            "        or database.has_view_by_name(view_name=table_name, schema=_schema_name)",
            "    ):",
            "        raise NoSuchTableError",
            "",
            "    cols = database.get_columns(table_name, schema=_schema_name)",
            "    for col in cols:",
            "        try:",
            "            if isinstance(col[\"type\"], TypeEngine):",
            "                name = col[\"column_name\"]",
            "                if not normalize_columns:",
            "                    name = db_engine_spec.denormalize_name(db_dialect, name)",
            "",
            "                db_type = db_engine_spec.column_datatype_to_string(",
            "                    col[\"type\"], db_dialect",
            "                )",
            "                type_spec = db_engine_spec.get_column_spec(",
            "                    db_type, db_extra=database.get_extra()",
            "                )",
            "                col.update(",
            "                    {",
            "                        \"name\": name,",
            "                        \"column_name\": name,",
            "                        \"type\": db_type,",
            "                        \"type_generic\": type_spec.generic_type if type_spec else None,",
            "                        \"is_dttm\": type_spec.is_dttm if type_spec else None,",
            "                    }",
            "                )",
            "        # Broad exception catch, because there are multiple possible exceptions",
            "        # from different drivers that fall outside CompileError",
            "        except Exception:  # pylint: disable=broad-except",
            "            col.update(",
            "                {",
            "                    \"type\": \"UNKNOWN\",",
            "                    \"type_generic\": None,",
            "                    \"is_dttm\": None,",
            "                }",
            "            )",
            "    return cols",
            "",
            "",
            "def get_virtual_table_metadata(dataset: SqlaTable) -> list[ResultSetColumnType]:",
            "    \"\"\"Use SQLparser to get virtual dataset metadata\"\"\"",
            "    if not dataset.sql:",
            "        raise SupersetGenericDBErrorException(",
            "            message=_(\"Virtual dataset query cannot be empty\"),",
            "        )",
            "",
            "    db_engine_spec = dataset.database.db_engine_spec",
            "    sql = dataset.get_template_processor().process_template(",
            "        dataset.sql, **dataset.template_params_dict",
            "    )",
            "    parsed_query = ParsedQuery(sql, engine=db_engine_spec.engine)",
            "    if not db_engine_spec.is_readonly_query(parsed_query):",
            "        raise SupersetSecurityException(",
            "            SupersetError(",
            "                error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "                message=_(\"Only `SELECT` statements are allowed\"),",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    statements = parsed_query.get_statements()",
            "    if len(statements) > 1:",
            "        raise SupersetSecurityException(",
            "            SupersetError(",
            "                error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "                message=_(\"Only single queries supported\"),",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    return get_columns_description(dataset.database, dataset.schema, statements[0])",
            "",
            "",
            "def get_columns_description(",
            "    database: Database,",
            "    schema: str | None,",
            "    query: str,",
            ") -> list[ResultSetColumnType]:",
            "    # TODO(villebro): refactor to use same code that's used by",
            "    #  sql_lab.py:execute_sql_statements",
            "    db_engine_spec = database.db_engine_spec",
            "    try:",
            "        with database.get_raw_connection(schema=schema) as conn:",
            "            cursor = conn.cursor()",
            "            query = database.apply_limit_to_sql(query, limit=1)",
            "            cursor.execute(query)",
            "            db_engine_spec.execute(cursor, query)",
            "            result = db_engine_spec.fetch_data(cursor, limit=1)",
            "            result_set = SupersetResultSet(result, cursor.description, db_engine_spec)",
            "            return result_set.columns",
            "    except Exception as ex:",
            "        raise SupersetGenericDBErrorException(message=str(ex)) from ex",
            "",
            "",
            "@lru_cache(maxsize=LRU_CACHE_MAX_SIZE)",
            "def get_dialect_name(drivername: str) -> str:",
            "    return SqlaURL.create(drivername).get_dialect().name",
            "",
            "",
            "@lru_cache(maxsize=LRU_CACHE_MAX_SIZE)",
            "def get_identifier_quoter(drivername: str) -> dict[str, Callable[[str], str]]:",
            "    return SqlaURL.create(drivername).get_dialect()().identifier_preparer.quote",
            "",
            "",
            "DeclarativeModel = TypeVar(\"DeclarativeModel\", bound=DeclarativeMeta)",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "def find_cached_objects_in_session(",
            "    session: Session,",
            "    cls: type[DeclarativeModel],",
            "    ids: Iterable[int] | None = None,",
            "    uuids: Iterable[UUID] | None = None,",
            ") -> Iterator[DeclarativeModel]:",
            "    \"\"\"Find known ORM instances in cached SQLA session states.",
            "",
            "    :param session: a SQLA session",
            "    :param cls: a SQLA DeclarativeModel",
            "    :param ids: ids of the desired model instances (optional)",
            "    :param uuids: uuids of the desired instances, will be ignored if `ids` are provides",
            "    \"\"\"",
            "    if not ids and not uuids:",
            "        return iter([])",
            "    uuids = uuids or []",
            "    try:",
            "        items = list(session)",
            "    except ObjectDeletedError:",
            "        logger.warning(\"ObjectDeletedError\", exc_info=True)",
            "        return iter(())",
            "",
            "    return (",
            "        item",
            "        # `session` is an iterator of all known items",
            "        for item in items",
            "        if isinstance(item, cls) and (item.id in ids if ids else item.uuid in uuids)",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "114": [
                "get_virtual_table_metadata"
            ]
        },
        "addLocation": []
    },
    "superset/db_engine_specs/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 900,
                "afterPatchRowNumber": 900,
                "PatchRowcode": "             return database.compile_sqla_query(qry)"
            },
            "1": {
                "beforePatchRowNumber": 901,
                "afterPatchRowNumber": 901,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 902,
                "afterPatchRowNumber": 902,
                "PatchRowcode": "         if cls.limit_method == LimitMethod.FORCE_LIMIT:"
            },
            "3": {
                "beforePatchRowNumber": 903,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            parsed_query = sql_parse.ParsedQuery(sql)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 903,
                "PatchRowcode": "+            parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)"
            },
            "5": {
                "beforePatchRowNumber": 904,
                "afterPatchRowNumber": 904,
                "PatchRowcode": "             sql = parsed_query.set_or_update_query_limit(limit, force=force)"
            },
            "6": {
                "beforePatchRowNumber": 905,
                "afterPatchRowNumber": 905,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 906,
                "afterPatchRowNumber": 906,
                "PatchRowcode": "         return sql"
            },
            "8": {
                "beforePatchRowNumber": 981,
                "afterPatchRowNumber": 981,
                "PatchRowcode": "         :param sql: SQL query"
            },
            "9": {
                "beforePatchRowNumber": 982,
                "afterPatchRowNumber": 982,
                "PatchRowcode": "         :return: Value of limit clause in query"
            },
            "10": {
                "beforePatchRowNumber": 983,
                "afterPatchRowNumber": 983,
                "PatchRowcode": "         \"\"\""
            },
            "11": {
                "beforePatchRowNumber": 984,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        parsed_query = sql_parse.ParsedQuery(sql)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 984,
                "PatchRowcode": "+        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)"
            },
            "13": {
                "beforePatchRowNumber": 985,
                "afterPatchRowNumber": 985,
                "PatchRowcode": "         return parsed_query.limit"
            },
            "14": {
                "beforePatchRowNumber": 986,
                "afterPatchRowNumber": 986,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 987,
                "afterPatchRowNumber": 987,
                "PatchRowcode": "     @classmethod"
            },
            "16": {
                "beforePatchRowNumber": 993,
                "afterPatchRowNumber": 993,
                "PatchRowcode": "         :param limit: New limit to insert/replace into query"
            },
            "17": {
                "beforePatchRowNumber": 994,
                "afterPatchRowNumber": 994,
                "PatchRowcode": "         :return: Query with new limit"
            },
            "18": {
                "beforePatchRowNumber": 995,
                "afterPatchRowNumber": 995,
                "PatchRowcode": "         \"\"\""
            },
            "19": {
                "beforePatchRowNumber": 996,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        parsed_query = sql_parse.ParsedQuery(sql)"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 996,
                "PatchRowcode": "+        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)"
            },
            "21": {
                "beforePatchRowNumber": 997,
                "afterPatchRowNumber": 997,
                "PatchRowcode": "         return parsed_query.set_or_update_query_limit(limit)"
            },
            "22": {
                "beforePatchRowNumber": 998,
                "afterPatchRowNumber": 998,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 999,
                "afterPatchRowNumber": 999,
                "PatchRowcode": "     @classmethod"
            },
            "24": {
                "beforePatchRowNumber": 1490,
                "afterPatchRowNumber": 1490,
                "PatchRowcode": "         :param database: Database instance"
            },
            "25": {
                "beforePatchRowNumber": 1491,
                "afterPatchRowNumber": 1491,
                "PatchRowcode": "         :return: Dictionary with different costs"
            },
            "26": {
                "beforePatchRowNumber": 1492,
                "afterPatchRowNumber": 1492,
                "PatchRowcode": "         \"\"\""
            },
            "27": {
                "beforePatchRowNumber": 1493,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        parsed_query = ParsedQuery(statement)"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1493,
                "PatchRowcode": "+        parsed_query = ParsedQuery(statement, engine=cls.engine)"
            },
            "29": {
                "beforePatchRowNumber": 1494,
                "afterPatchRowNumber": 1494,
                "PatchRowcode": "         sql = parsed_query.stripped()"
            },
            "30": {
                "beforePatchRowNumber": 1495,
                "afterPatchRowNumber": 1495,
                "PatchRowcode": "         sql_query_mutator = current_app.config[\"SQL_QUERY_MUTATOR\"]"
            },
            "31": {
                "beforePatchRowNumber": 1496,
                "afterPatchRowNumber": 1496,
                "PatchRowcode": "         mutate_after_split = current_app.config[\"MUTATE_AFTER_SPLIT\"]"
            },
            "32": {
                "beforePatchRowNumber": 1525,
                "afterPatchRowNumber": 1525,
                "PatchRowcode": "                 \"Database does not support cost estimation\""
            },
            "33": {
                "beforePatchRowNumber": 1526,
                "afterPatchRowNumber": 1526,
                "PatchRowcode": "             )"
            },
            "34": {
                "beforePatchRowNumber": 1527,
                "afterPatchRowNumber": 1527,
                "PatchRowcode": " "
            },
            "35": {
                "beforePatchRowNumber": 1528,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        parsed_query = sql_parse.ParsedQuery(sql)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1528,
                "PatchRowcode": "+        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)"
            },
            "37": {
                "beforePatchRowNumber": 1529,
                "afterPatchRowNumber": 1529,
                "PatchRowcode": "         statements = parsed_query.get_statements()"
            },
            "38": {
                "beforePatchRowNumber": 1530,
                "afterPatchRowNumber": 1530,
                "PatchRowcode": " "
            },
            "39": {
                "beforePatchRowNumber": 1531,
                "afterPatchRowNumber": 1531,
                "PatchRowcode": "         costs = []"
            },
            "40": {
                "beforePatchRowNumber": 1586,
                "afterPatchRowNumber": 1586,
                "PatchRowcode": "         :return:"
            },
            "41": {
                "beforePatchRowNumber": 1587,
                "afterPatchRowNumber": 1587,
                "PatchRowcode": "         \"\"\""
            },
            "42": {
                "beforePatchRowNumber": 1588,
                "afterPatchRowNumber": 1588,
                "PatchRowcode": "         if not cls.allows_sql_comments:"
            },
            "43": {
                "beforePatchRowNumber": 1589,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            query = sql_parse.strip_comments_from_sql(query)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1589,
                "PatchRowcode": "+            query = sql_parse.strip_comments_from_sql(query, engine=cls.engine)"
            },
            "45": {
                "beforePatchRowNumber": 1590,
                "afterPatchRowNumber": 1590,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 1591,
                "afterPatchRowNumber": 1591,
                "PatchRowcode": "         if cls.arraysize:"
            },
            "47": {
                "beforePatchRowNumber": 1592,
                "afterPatchRowNumber": 1592,
                "PatchRowcode": "             cursor.arraysize = cls.arraysize"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Match, Pattern",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    cast,",
            "    ContextManager,",
            "    NamedTuple,",
            "    TYPE_CHECKING,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "import pandas as pd",
            "import sqlparse",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask import current_app",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from marshmallow import fields, Schema",
            "from marshmallow.validate import Range",
            "from sqlalchemy import column, select, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.interfaces import Compiled, Dialect",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.ext.compiler import compiles",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.sql import literal_column, quoted_name, text",
            "from sqlalchemy.sql.expression import ColumnClause, Select, TextAsFrom, TextClause",
            "from sqlalchemy.types import TypeEngine",
            "from sqlparse.tokens import CTE",
            "",
            "from superset import security_manager, sql_parse",
            "from superset.constants import TimeGrain as TimeGrainConstants",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.sql_parse import ParsedQuery, Table",
            "from superset.superset_typing import ResultSetColumnType, SQLAColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.core import ColumnSpec, GenericDataType",
            "from superset.utils.hashing import md5_sha_from_str",
            "from superset.utils.network import is_hostname_valid, is_port_open",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.models.core import Database",
            "    from superset.models.sql_lab import Query",
            "",
            "ColumnTypeMapping = tuple[",
            "    Pattern[str],",
            "    Union[TypeEngine, Callable[[Match[str]], TypeEngine]],",
            "    GenericDataType,",
            "]",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "def convert_inspector_columns(cols: list[SQLAColumnType]) -> list[ResultSetColumnType]:",
            "    result_set_columns: list[ResultSetColumnType] = []",
            "    for col in cols:",
            "        result_set_columns.append({\"column_name\": col.get(\"name\"), **col})  # type: ignore",
            "    return result_set_columns",
            "",
            "",
            "class TimeGrain(NamedTuple):",
            "    name: str  # TODO: redundant field, remove",
            "    label: str",
            "    function: str",
            "    duration: str | None",
            "",
            "",
            "builtin_time_grains: dict[str | None, str] = {",
            "    TimeGrainConstants.SECOND: __(\"Second\"),",
            "    TimeGrainConstants.FIVE_SECONDS: __(\"5 second\"),",
            "    TimeGrainConstants.THIRTY_SECONDS: __(\"30 second\"),",
            "    TimeGrainConstants.MINUTE: __(\"Minute\"),",
            "    TimeGrainConstants.FIVE_MINUTES: __(\"5 minute\"),",
            "    TimeGrainConstants.TEN_MINUTES: __(\"10 minute\"),",
            "    TimeGrainConstants.FIFTEEN_MINUTES: __(\"15 minute\"),",
            "    TimeGrainConstants.THIRTY_MINUTES: __(\"30 minute\"),",
            "    TimeGrainConstants.HOUR: __(\"Hour\"),",
            "    TimeGrainConstants.SIX_HOURS: __(\"6 hour\"),",
            "    TimeGrainConstants.DAY: __(\"Day\"),",
            "    TimeGrainConstants.WEEK: __(\"Week\"),",
            "    TimeGrainConstants.MONTH: __(\"Month\"),",
            "    TimeGrainConstants.QUARTER: __(\"Quarter\"),",
            "    TimeGrainConstants.YEAR: __(\"Year\"),",
            "    TimeGrainConstants.WEEK_STARTING_SUNDAY: __(\"Week starting Sunday\"),",
            "    TimeGrainConstants.WEEK_STARTING_MONDAY: __(\"Week starting Monday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SATURDAY: __(\"Week ending Saturday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SUNDAY: __(\"Week ending Sunday\"),",
            "}",
            "",
            "",
            "class TimestampExpression(",
            "    ColumnClause",
            "):  # pylint: disable=abstract-method, too-many-ancestors",
            "    def __init__(self, expr: str, col: ColumnClause, **kwargs: Any) -> None:",
            "        \"\"\"Sqlalchemy class that can be used to render native column elements respecting",
            "        engine-specific quoting rules as part of a string-based expression.",
            "",
            "        :param expr: Sql expression with '{col}' denoting the locations where the col",
            "        object will be rendered.",
            "        :param col: the target column",
            "        \"\"\"",
            "        super().__init__(expr, **kwargs)",
            "        self.col = col",
            "",
            "    @property",
            "    def _constructor(self) -> ColumnClause:",
            "        # Needed to ensure that the column label is rendered correctly when",
            "        # proxied to the outer query.",
            "        # See https://github.com/sqlalchemy/sqlalchemy/issues/4730",
            "        return ColumnClause",
            "",
            "",
            "@compiles(TimestampExpression)",
            "def compile_timegrain_expression(",
            "    element: TimestampExpression, compiler: Compiled, **kwargs: Any",
            ") -> str:",
            "    return element.name.replace(\"{col}\", compiler.process(element.col, **kwargs))",
            "",
            "",
            "class LimitMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"Enum the ways that limits can be applied\"\"\"",
            "",
            "    FETCH_MANY = \"fetch_many\"",
            "    WRAP_SQL = \"wrap_sql\"",
            "    FORCE_LIMIT = \"force_limit\"",
            "",
            "",
            "class MetricType(TypedDict, total=False):",
            "    \"\"\"",
            "    Type for metrics return by `get_metrics`.",
            "    \"\"\"",
            "",
            "    metric_name: str",
            "    expression: str",
            "    verbose_name: str | None",
            "    metric_type: str | None",
            "    description: str | None",
            "    d3format: str | None",
            "    currency: str | None",
            "    warning_text: str | None",
            "    extra: str | None",
            "",
            "",
            "class BaseEngineSpec:  # pylint: disable=too-many-public-methods",
            "    \"\"\"Abstract class for database engine specific configurations",
            "",
            "    Attributes:",
            "        allows_alias_to_source_column: Whether the engine is able to pick the",
            "                                       source column for aggregation clauses",
            "                                       used in ORDER BY when a column in SELECT",
            "                                       has an alias that is the same as a source",
            "                                       column.",
            "        allows_hidden_orderby_agg:     Whether the engine allows ORDER BY to",
            "                                       directly use aggregation clauses, without",
            "                                       having to add the same aggregation in SELECT.",
            "    \"\"\"",
            "",
            "    engine_name: str | None = None  # for user messages, overridden in child classes",
            "",
            "    # These attributes map the DB engine spec to one or more SQLAlchemy dialects/drivers;",
            "    # see the ``supports_url`` and ``supports_backend`` methods below.",
            "    engine = \"base\"  # str as defined in sqlalchemy.engine.engine",
            "    engine_aliases: set[str] = set()",
            "    drivers: dict[str, str] = {}",
            "    default_driver: str | None = None",
            "",
            "    # placeholder with the SQLAlchemy URI template",
            "    sqlalchemy_uri_placeholder = (",
            "        \"engine+driver://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "",
            "    disable_ssh_tunneling = False",
            "",
            "    _date_trunc_functions: dict[str, str] = {}",
            "    _time_grain_expressions: dict[str | None, str] = {}",
            "    _default_column_type_mappings: tuple[ColumnTypeMapping, ...] = (",
            "        (",
            "            re.compile(r\"^string\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^n((var)?char|text)\", re.IGNORECASE),",
            "            types.UnicodeText(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(var)?char\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(tiny|medium|long)?text\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallint\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^int(eger)?\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigint\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^long\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^decimal\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^numeric\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^float\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^double\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^real\", re.IGNORECASE),",
            "            types.REAL,",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallserial\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^serial\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigserial\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^money\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^timestamp\", re.IGNORECASE),",
            "            types.TIMESTAMP(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^datetime\", re.IGNORECASE),",
            "            types.DateTime(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^date\", re.IGNORECASE),",
            "            types.Date(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^time\", re.IGNORECASE),",
            "            types.Time(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^interval\", re.IGNORECASE),",
            "            types.Interval(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^bool(ean)?\", re.IGNORECASE),",
            "            types.Boolean(),",
            "            GenericDataType.BOOLEAN,",
            "        ),",
            "    )",
            "    # engine-specific type mappings to check prior to the defaults",
            "    column_type_mappings: tuple[ColumnTypeMapping, ...] = ()",
            "",
            "    # type-specific functions to mutate values received from the database.",
            "    # Needed on certain databases that return values in an unexpected format",
            "    column_type_mutators: dict[TypeEngine, Callable[[Any], Any]] = {}",
            "",
            "    # Does database support join-free timeslot grouping",
            "    time_groupby_inline = False",
            "    limit_method = LimitMethod.FORCE_LIMIT",
            "    allows_joins = True",
            "    allows_subqueries = True",
            "    allows_alias_in_select = True",
            "    allows_alias_in_orderby = True",
            "    allows_sql_comments = True",
            "    allows_escaped_colons = True",
            "",
            "    # Whether ORDER BY clause can use aliases created in SELECT",
            "    # that are the same as a source column",
            "    allows_alias_to_source_column = True",
            "",
            "    # Whether ORDER BY clause must appear in SELECT",
            "    # if True, then it doesn't have to.",
            "    allows_hidden_orderby_agg = True",
            "",
            "    # Whether ORDER BY clause can use sql calculated expression",
            "    # if True, use alias of select column for `order by`",
            "    # the True is safely for most database",
            "    # But for backward compatibility, False by default",
            "    allows_hidden_cc_in_orderby = False",
            "",
            "    # Whether allow CTE as subquery or regular CTE",
            "    # If True, then it will allow  in subquery ,",
            "    # if False it will allow as regular CTE",
            "    allows_cte_in_subquery = True",
            "    # Define alias for CTE",
            "    cte_alias = \"__cte\"",
            "    # Whether allow LIMIT clause in the SQL",
            "    # If True, then the database engine is allowed for LIMIT clause",
            "    # If False, then the database engine is allowed for TOP clause",
            "    allow_limit_clause = True",
            "    # This set will give keywords for select statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    select_keywords: set[str] = {\"SELECT\"}",
            "    # This set will give the keywords for data limit statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    top_keywords: set[str] = {\"TOP\"}",
            "    # A set of disallowed connection query parameters by driver name",
            "    disallow_uri_query_params: dict[str, set[str]] = {}",
            "    # A Dict of query parameters that will always be used on every connection",
            "    # by driver name",
            "    enforce_uri_query_params: dict[str, dict[str, Any]] = {}",
            "",
            "    force_column_alias_quotes = False",
            "    arraysize = 0",
            "    max_column_name_length: int | None = None",
            "    try_remove_schema_from_table_name = True  # pylint: disable=invalid-name",
            "    run_multiple_statements_as_one = False",
            "    custom_errors: dict[",
            "        Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]",
            "    ] = {}",
            "",
            "    # Whether the engine supports file uploads",
            "    # if True, database will be listed as option in the upload file form",
            "    supports_file_upload = True",
            "",
            "    # Is the DB engine spec able to change the default schema? This requires implementing",
            "    # a custom `adjust_engine_params` method.",
            "    supports_dynamic_schema = False",
            "",
            "    # Does the DB support catalogs? A catalog here is a group of schemas, and has",
            "    # different names depending on the DB: BigQuery calles it a \"project\", Postgres calls",
            "    # it a \"database\", Trino calls it a \"catalog\", etc.",
            "    supports_catalog = False",
            "",
            "    # Can the catalog be changed on a per-query basis?",
            "    supports_dynamic_catalog = False",
            "",
            "    @classmethod",
            "    def get_allows_alias_in_select(",
            "        cls, database: Database  # pylint: disable=unused-argument",
            "    ) -> bool:",
            "        \"\"\"",
            "        Method for dynamic `allows_alias_in_select`.",
            "",
            "        In Dremio this atribute is version-dependent, so Superset needs to inspect the",
            "        database configuration in order to determine it. This method allows engine-specs",
            "        to define dynamic values for the attribute.",
            "        \"\"\"",
            "        return cls.allows_alias_in_select",
            "",
            "    @classmethod",
            "    def supports_url(cls, url: URL) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy URL.",
            "",
            "        As an example, if a given DB engine spec has:",
            "",
            "            class PostgresDBEngineSpec:",
            "                engine = \"postgresql\"",
            "                engine_aliases = \"postgres\"",
            "                drivers = {",
            "                    \"psycopg2\": \"The default Postgres driver\",",
            "                    \"asyncpg\": \"An asynchronous Postgres driver\",",
            "                }",
            "",
            "        It would be used for all the following SQLAlchemy URIs:",
            "",
            "            - postgres://user:password@host/db",
            "            - postgresql://user:password@host/db",
            "            - postgres+asyncpg://user:password@host/db",
            "            - postgres+psycopg2://user:password@host/db",
            "            - postgresql+asyncpg://user:password@host/db",
            "            - postgresql+psycopg2://user:password@host/db",
            "",
            "        Note that SQLAlchemy has a default driver even if one is not specified:",
            "",
            "            >>> from sqlalchemy.engine.url import make_url",
            "            >>> make_url('postgres://').get_driver_name()",
            "            'psycopg2'",
            "",
            "        \"\"\"",
            "        backend = url.get_backend_name()",
            "        driver = url.get_driver_name()",
            "        return cls.supports_backend(backend, driver)",
            "",
            "    @classmethod",
            "    def supports_backend(cls, backend: str, driver: str | None = None) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy backend/driver.",
            "        \"\"\"",
            "        # check the backend first",
            "        if backend != cls.engine and backend not in cls.engine_aliases:",
            "            return False",
            "",
            "        # originally DB engine specs didn't declare any drivers and the check was made",
            "        # only on the engine; if that's the case, ignore the driver for backwards",
            "        # compatibility",
            "        if not cls.drivers or driver is None:",
            "            return True",
            "",
            "        return driver in cls.drivers",
            "",
            "    @classmethod",
            "    def get_default_schema(cls, database: Database) -> str | None:",
            "        \"\"\"",
            "        Return the default schema in a given database.",
            "        \"\"\"",
            "        with database.get_inspector_with_context() as inspector:",
            "            return inspector.default_schema_name",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the schema configured in a SQLALchemy URI and connection arguments, if any.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This is used to determine the schema of tables that aren't fully qualified, eg:",
            "",
            "            SELECT * FROM foo;",
            "",
            "        In the example above, the schema where the `foo` table lives depends on a few",
            "        factors:",
            "",
            "            1. For DB engine specs that allow dynamically changing the schema based on the",
            "               query we should use the query schema.",
            "            2. For DB engine specs that don't support dynamically changing the schema and",
            "               have the schema hardcoded in the SQLAlchemy URI we should use the schema",
            "               from the URI.",
            "            3. For DB engine specs that don't connect to a specific schema and can't",
            "               change it dynamically we need to probe the database for the default schema.",
            "",
            "        Determining the correct schema is crucial for managing access to data, so please",
            "        make sure you understand this logic when working on a new DB engine spec.",
            "        \"\"\"",
            "        # dynamic schema varies on a per-query basis",
            "        if cls.supports_dynamic_schema:",
            "            return query.schema",
            "",
            "        # check if the schema is stored in the SQLAlchemy URI or connection arguments",
            "        try:",
            "            connect_args = database.get_extra()[\"engine_params\"][\"connect_args\"]",
            "        except KeyError:",
            "            connect_args = {}",
            "        sqlalchemy_uri = make_url_safe(database.sqlalchemy_uri)",
            "        if schema := cls.get_schema_from_engine_params(sqlalchemy_uri, connect_args):",
            "            return schema",
            "",
            "        # return the default schema of the database",
            "        return cls.get_default_schema(database)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific exceptions into",
            "        Superset DBAPI exceptions",
            "",
            "        Note: On python 3.9 this method can be changed to a classmethod property",
            "        without the need of implementing a metaclass type",
            "",
            "        :return: A map of driver specific exception to superset custom exceptions",
            "        \"\"\"",
            "        return {}",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific parser method",
            "",
            "        :return: An Exception with a parsed string off the original exception",
            "        \"\"\"",
            "        return exception",
            "",
            "    @classmethod",
            "    def get_dbapi_mapped_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Get a superset custom DBAPI exception from the driver specific exception.",
            "",
            "        Override if the engine needs to perform extra changes to the exception, for",
            "        example change the exception message or implement custom more complex logic",
            "",
            "        :param exception: The driver specific exception",
            "        :return: Superset custom DBAPI exception",
            "        \"\"\"",
            "        new_exception = cls.get_dbapi_exception_mapping().get(type(exception))",
            "        if not new_exception:",
            "            return cls.parse_error_exception(exception)",
            "        return new_exception(str(exception))",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(  # pylint: disable=unused-argument",
            "        cls,",
            "        extra: dict[str, Any],",
            "    ) -> bool:",
            "        return False",
            "",
            "    @classmethod",
            "    def get_text_clause(cls, clause: str) -> TextClause:",
            "        \"\"\"",
            "        SQLAlchemy wrapper to ensure text clauses are escaped properly",
            "",
            "        :param clause: string clause with potentially unescaped characters",
            "        :return: text clause with escaped characters",
            "        \"\"\"",
            "        if cls.allows_escaped_colons:",
            "            clause = clause.replace(\":\", \"\\\\:\")",
            "        return text(clause)",
            "",
            "    @classmethod",
            "    def get_engine(",
            "        cls,",
            "        database: Database,",
            "        schema: str | None = None,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> ContextManager[Engine]:",
            "        \"\"\"",
            "        Return an engine context manager.",
            "",
            "            >>> with DBEngineSpec.get_engine(database, schema, source) as engine:",
            "            ...     connection = engine.connect()",
            "            ...     connection.execute(sql)",
            "",
            "        \"\"\"",
            "        return database.get_sqla_engine_with_context(schema=schema, source=source)",
            "",
            "    @classmethod",
            "    def get_timestamp_expr(",
            "        cls,",
            "        col: ColumnClause,",
            "        pdf: str | None,",
            "        time_grain: str | None,",
            "    ) -> TimestampExpression:",
            "        \"\"\"",
            "        Construct a TimestampExpression to be used in a SQLAlchemy query.",
            "",
            "        :param col: Target column for the TimestampExpression",
            "        :param pdf: date format (seconds or milliseconds)",
            "        :param time_grain: time grain, e.g. P1Y for 1 year",
            "        :return: TimestampExpression object",
            "        \"\"\"",
            "        if time_grain:",
            "            type_ = str(getattr(col, \"type\", \"\"))",
            "            time_expr = cls.get_time_grain_expressions().get(time_grain)",
            "            if not time_expr:",
            "                raise NotImplementedError(",
            "                    f\"No grain spec for {time_grain} for database {cls.engine}\"",
            "                )",
            "            if type_ and \"{func}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{func}\", date_trunc_function)",
            "            if type_ and \"{type}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{type}\", type_)",
            "        else:",
            "            time_expr = \"{col}\"",
            "",
            "        # if epoch, translate to DATE using db specific conf",
            "        if pdf == \"epoch_s\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_to_dttm())",
            "        elif pdf == \"epoch_ms\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_ms_to_dttm())",
            "",
            "        return TimestampExpression(time_expr, col, type_=col.type)",
            "",
            "    @classmethod",
            "    def get_time_grains(cls) -> tuple[TimeGrain, ...]:",
            "        \"\"\"",
            "        Generate a tuple of supported time grains.",
            "",
            "        :return: All time grains supported by the engine",
            "        \"\"\"",
            "",
            "        ret_list = []",
            "        time_grains = builtin_time_grains.copy()",
            "        time_grains.update(current_app.config[\"TIME_GRAIN_ADDONS\"])",
            "        for duration, func in cls.get_time_grain_expressions().items():",
            "            if duration in time_grains:",
            "                name = time_grains[duration]",
            "                ret_list.append(TimeGrain(name, _(name), func, duration))",
            "        return tuple(ret_list)",
            "",
            "    @classmethod",
            "    def _sort_time_grains(",
            "        cls, val: tuple[str | None, str], index: int",
            "    ) -> float | int | str:",
            "        \"\"\"",
            "        Return an ordered time-based value of a portion of a time grain",
            "        for sorting",
            "        Values are expected to be either None or start with P or PT",
            "        Have a numerical value in the middle and end with",
            "        a value for the time interval",
            "        It can also start or end with epoch start time denoting a range",
            "        i.e, week beginning or ending with a day",
            "        \"\"\"",
            "        pos = {",
            "            \"FIRST\": 0,",
            "            \"SECOND\": 1,",
            "            \"THIRD\": 2,",
            "            \"LAST\": 3,",
            "        }",
            "",
            "        if val[0] is None:",
            "            return pos[\"FIRST\"]",
            "",
            "        prog = re.compile(r\"(.*\\/)?(P|PT)([0-9\\.]+)(S|M|H|D|W|M|Y)(\\/.*)?\")",
            "        result = prog.match(val[0])",
            "",
            "        # for any time grains that don't match the format, put them at the end",
            "        if result is None:",
            "            return pos[\"LAST\"]",
            "",
            "        second_minute_hour = [\"S\", \"M\", \"H\"]",
            "        day_week_month_year = [\"D\", \"W\", \"M\", \"Y\"]",
            "        is_less_than_day = result.group(2) == \"PT\"",
            "        interval = result.group(4)",
            "        epoch_time_start_string = result.group(1) or result.group(5)",
            "        has_starting_or_ending = bool(len(epoch_time_start_string or \"\"))",
            "",
            "        def sort_day_week() -> int:",
            "            if has_starting_or_ending:",
            "                return pos[\"LAST\"]",
            "            if is_less_than_day:",
            "                return pos[\"SECOND\"]",
            "            return pos[\"THIRD\"]",
            "",
            "        def sort_interval() -> float:",
            "            if is_less_than_day:",
            "                return second_minute_hour.index(interval)",
            "            return day_week_month_year.index(interval)",
            "",
            "        # 0: all \"PT\" values should come before \"P\" values (i.e, PT10M)",
            "        # 1: order values within the above arrays (\"D\" before \"W\")",
            "        # 2: sort by numeric value (PT10M before PT15M)",
            "        # 3: sort by any week starting/ending values",
            "        plist = {",
            "            0: sort_day_week(),",
            "            1: pos[\"SECOND\"] if is_less_than_day else pos[\"THIRD\"],",
            "            2: sort_interval(),",
            "            3: float(result.group(3)),",
            "        }",
            "",
            "        return plist.get(index, 0)",
            "",
            "    @classmethod",
            "    def get_time_grain_expressions(cls) -> dict[str | None, str]:",
            "        \"\"\"",
            "        Return a dict of all supported time grains including any potential added grains",
            "        but excluding any potentially disabled grains in the config file.",
            "",
            "        :return: All time grain expressions supported by the engine",
            "        \"\"\"",
            "        # TODO: use @memoize decorator or similar to avoid recomputation on every call",
            "        time_grain_expressions = cls._time_grain_expressions.copy()",
            "        grain_addon_expressions = current_app.config[\"TIME_GRAIN_ADDON_EXPRESSIONS\"]",
            "        time_grain_expressions.update(grain_addon_expressions.get(cls.engine, {}))",
            "        denylist: list[str] = current_app.config[\"TIME_GRAIN_DENYLIST\"]",
            "        for key in denylist:",
            "            time_grain_expressions.pop(key, None)",
            "",
            "        return dict(",
            "            sorted(",
            "                time_grain_expressions.items(),",
            "                key=lambda x: (",
            "                    cls._sort_time_grains(x, 0),",
            "                    cls._sort_time_grains(x, 1),",
            "                    cls._sort_time_grains(x, 2),",
            "                    cls._sort_time_grains(x, 3),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "",
            "        :param cursor: Cursor instance",
            "        :param limit: Maximum number of rows to be returned by the cursor",
            "        :return: Result of query",
            "        \"\"\"",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            if cls.limit_method == LimitMethod.FETCH_MANY and limit:",
            "                return cursor.fetchmany(limit)",
            "            data = cursor.fetchall()",
            "            description = cursor.description or []",
            "            # Create a mapping between column name and a mutator function to normalize",
            "            # values with. The first two items in the description row are",
            "            # the column name and type.",
            "            column_mutators = {",
            "                row[0]: func",
            "                for row in description",
            "                if (",
            "                    func := cls.column_type_mutators.get(",
            "                        type(cls.get_sqla_column_type(cls.get_datatype(row[1])))",
            "                    )",
            "                )",
            "            }",
            "            if column_mutators:",
            "                indexes = {row[0]: idx for idx, row in enumerate(description)}",
            "                for row_idx, row in enumerate(data):",
            "                    new_row = list(row)",
            "                    for col, func in column_mutators.items():",
            "                        col_idx = indexes[col]",
            "                        new_row[col_idx] = func(row[col_idx])",
            "                    data[row_idx] = tuple(new_row)",
            "",
            "            return data",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def expand_data(",
            "        cls, columns: list[ResultSetColumnType], data: list[dict[Any, Any]]",
            "    ) -> tuple[",
            "        list[ResultSetColumnType], list[dict[Any, Any]], list[ResultSetColumnType]",
            "    ]:",
            "        \"\"\"",
            "        Some engines support expanding nested fields. See implementation in Presto",
            "        spec for details.",
            "",
            "        :param columns: columns selected in the query",
            "        :param data: original data set",
            "        :return: list of all columns(selected columns and their nested fields),",
            "                 expanded data set, listed of nested fields",
            "        \"\"\"",
            "        return columns, data, []",
            "",
            "    @classmethod",
            "    def alter_new_orm_column(cls, orm_col: TableColumn) -> None:",
            "        \"\"\"Allow altering default column attributes when first detected/added",
            "",
            "        For instance special column like `__time` for Druid can be",
            "        set to is_dttm=True. Note that this only gets called when new",
            "        columns are detected/created\"\"\"",
            "        # TODO: Fix circular import caused by importing TableColumn",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (seconds) to datetime that can be used in a",
            "        query. The reference column should be denoted as `{col}` in the return",
            "        expression, e.g. \"FROM_UNIXTIME({col})\"",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (milliseconds) to datetime that can be used",
            "        in a query.",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        return cls.epoch_to_dttm().replace(\"{col}\", \"({col}/1000)\")",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        \"\"\"",
            "        Change column type code from cursor description to string representation.",
            "",
            "        :param type_code: Type code from cursor description",
            "        :return: String representation of type code",
            "        \"\"\"",
            "        if isinstance(type_code, str) and type_code != \"\":",
            "            return type_code.upper()",
            "        return None",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        noop by default",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        return indexes",
            "",
            "    @classmethod",
            "    def extra_table_metadata(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Returns engine-specific table metadata",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name",
            "        :param schema_name: Schema name",
            "        :return: Engine-specific table metadata",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        return {}",
            "",
            "    @classmethod",
            "    def apply_limit_to_sql(",
            "        cls, sql: str, limit: int, database: Database, force: bool = False",
            "    ) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a LIMIT clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param database: Database instance",
            "        :return: SQL query with limit clause",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        if cls.limit_method == LimitMethod.WRAP_SQL:",
            "            sql = sql.strip(\"\\t\\n ;\")",
            "            qry = (",
            "                select(\"*\")",
            "                .select_from(TextAsFrom(text(sql), [\"*\"]).alias(\"inner_qry\"))",
            "                .limit(limit)",
            "            )",
            "            return database.compile_sqla_query(qry)",
            "",
            "        if cls.limit_method == LimitMethod.FORCE_LIMIT:",
            "            parsed_query = sql_parse.ParsedQuery(sql)",
            "            sql = parsed_query.set_or_update_query_limit(limit, force=force)",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def apply_top_to_sql(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a TOP clause",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param sql: SQL query",
            "        :return: SQL query with top clause",
            "        \"\"\"",
            "",
            "        cte = None",
            "        sql_remainder = None",
            "        sql = sql.strip(\" \\t\\n;\")",
            "        sql_statement = sqlparse.format(sql, strip_comments=True)",
            "        query_limit: int | None = sql_parse.extract_top_from_query(",
            "            sql_statement, cls.top_keywords",
            "        )",
            "        if not limit:",
            "            final_limit = query_limit",
            "        elif int(query_limit or 0) < limit and query_limit is not None:",
            "            final_limit = query_limit",
            "        else:",
            "            final_limit = limit",
            "        if not cls.allows_cte_in_subquery:",
            "            cte, sql_remainder = sql_parse.get_cte_remainder_query(sql_statement)",
            "        if cte:",
            "            str_statement = str(sql_remainder)",
            "            cte = cte + \"\\n\"",
            "        else:",
            "            cte = \"\"",
            "            str_statement = str(sql)",
            "        str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "",
            "        tokens = str_statement.rstrip().split(\" \")",
            "        tokens = [token for token in tokens if token]",
            "        if cls.top_not_in_sql(str_statement):",
            "            selects = [",
            "                i",
            "                for i, word in enumerate(tokens)",
            "                if word.upper() in cls.select_keywords",
            "            ]",
            "            first_select = selects[0]",
            "            if tokens[first_select + 1].upper() == \"DISTINCT\":",
            "                first_select += 1",
            "",
            "            tokens.insert(first_select + 1, \"TOP\")",
            "            tokens.insert(first_select + 2, str(final_limit))",
            "",
            "        next_is_limit_token = False",
            "        new_tokens = []",
            "",
            "        for token in tokens:",
            "            if token in cls.top_keywords:",
            "                next_is_limit_token = True",
            "            elif next_is_limit_token:",
            "                if token.isdigit():",
            "                    token = str(final_limit)",
            "                    next_is_limit_token = False",
            "            new_tokens.append(token)",
            "        sql = \" \".join(new_tokens)",
            "        return cte + sql",
            "",
            "    @classmethod",
            "    def top_not_in_sql(cls, sql: str) -> bool:",
            "        for top_word in cls.top_keywords:",
            "            if top_word.upper() in sql.upper():",
            "                return False",
            "        return True",
            "",
            "    @classmethod",
            "    def get_limit_from_sql(cls, sql: str) -> int | None:",
            "        \"\"\"",
            "        Extract limit from SQL query",
            "",
            "        :param sql: SQL query",
            "        :return: Value of limit clause in query",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql)",
            "        return parsed_query.limit",
            "",
            "    @classmethod",
            "    def set_or_update_query_limit(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Create a query based on original query but with new limit clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: New limit to insert/replace into query",
            "        :return: Query with new limit",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql)",
            "        return parsed_query.set_or_update_query_limit(limit)",
            "",
            "    @classmethod",
            "    def get_cte_query(cls, sql: str) -> str | None:",
            "        \"\"\"",
            "        Convert the input CTE based SQL to the SQL for virtual table conversion",
            "",
            "        :param sql: SQL query",
            "        :return: CTE with the main select query aliased as `__cte`",
            "",
            "        \"\"\"",
            "        if not cls.allows_cte_in_subquery:",
            "            stmt = sqlparse.parse(sql)[0]",
            "",
            "            # The first meaningful token for CTE will be with WITH",
            "            idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "            if not (token and token.ttype == CTE):",
            "                return None",
            "            idx, token = stmt.token_next(idx)",
            "            idx = stmt.token_index(token) + 1",
            "",
            "            # extract rest of the SQLs after CTE",
            "            remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "            return f\"WITH {token.value},\\n{cls.cte_alias} AS (\\n{remainder}\\n)\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: Database,",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        For regular engines this calls the `pandas.DataFrame.to_sql` method. Can be",
            "        overridden for engines that don't work well with this method, e.g. Hive and",
            "        BigQuery.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "",
            "        to_sql_kwargs[\"name\"] = table.table",
            "",
            "        if table.schema:",
            "            # Only add schema when it is preset and non-empty.",
            "            to_sql_kwargs[\"schema\"] = table.schema",
            "",
            "        with cls.get_engine(database) as engine:",
            "            if engine.dialect.supports_multivalues_insert:",
            "                to_sql_kwargs[\"method\"] = \"multi\"",
            "",
            "            df.to_sql(con=engine, **to_sql_kwargs)",
            "",
            "    @classmethod",
            "    def convert_dttm(  # pylint: disable=unused-argument",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Convert a Python `datetime` object to a SQL expression.",
            "",
            "        :param target_type: The target type of expression",
            "        :param dttm: The datetime object",
            "        :param db_extra: The database extra object",
            "        :return: The SQL expression",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Any, query: Query, session: Session) -> None:",
            "        \"\"\"Handle a live cursor between the execute and fetchall calls",
            "",
            "        The flow works without this method doing anything, but it allows",
            "        for handling the cursor and updating progress information in the",
            "        query object\"\"\"",
            "        # TODO: Fix circular import error caused by importing sql_lab.Query",
            "",
            "    @classmethod",
            "    def execute_with_cursor(",
            "        cls, cursor: Any, sql: str, query: Query, session: Session",
            "    ) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        For most implementations this just makes calls to `execute` and",
            "        `handle_cursor` consecutively, but in some engines (e.g. Trino) we may",
            "        need to handle client limitations such as lack of async support and",
            "        perform a more complicated operation to get information from the cursor",
            "        in a timely manner and facilitate operations such as query stop",
            "        \"\"\"",
            "        logger.debug(\"Query %d: Running query: %s\", query.id, sql)",
            "        cls.execute(cursor, sql, async_=True)",
            "        logger.debug(\"Query %d: Handling cursor\", query.id)",
            "        cls.handle_cursor(cursor, query, session)",
            "",
            "    @classmethod",
            "    def extract_error_message(cls, ex: Exception) -> str:",
            "        return f\"{cls.engine} error: {cls._extract_error_message(ex)}\"",
            "",
            "    @classmethod",
            "    def _extract_error_message(cls, ex: Exception) -> str:",
            "        \"\"\"Extract error message for queries\"\"\"",
            "        return utils.error_msg_from_exception(ex)",
            "",
            "    @classmethod",
            "    def extract_errors(",
            "        cls, ex: Exception, context: dict[str, Any] | None = None",
            "    ) -> list[SupersetError]:",
            "        raw_message = cls._extract_error_message(ex)",
            "",
            "        context = context or {}",
            "        for regex, (message, error_type, extra) in cls.custom_errors.items():",
            "            if match := regex.search(raw_message):",
            "                params = {**context, **match.groupdict()}",
            "                extra[\"engine_name\"] = cls.engine_name",
            "                return [",
            "                    SupersetError(",
            "                        error_type=error_type,",
            "                        message=message % params,",
            "                        level=ErrorLevel.ERROR,",
            "                        extra=extra,",
            "                    )",
            "                ]",
            "",
            "        return [",
            "            SupersetError(",
            "                error_type=SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "                message=cls._extract_error_message(ex),",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"engine_name\": cls.engine_name},",
            "            )",
            "        ]",
            "",
            "    @classmethod",
            "    def adjust_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        uri: URL,",
            "        connect_args: dict[str, Any],",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> tuple[URL, dict[str, Any]]:",
            "        \"\"\"",
            "        Return a new URL and ``connect_args`` for a specific catalog/schema.",
            "",
            "        This is used in SQL Lab, allowing users to select a schema from the list of",
            "        schemas available in a given database, and have the query run with that schema as",
            "        the default one.",
            "",
            "        For some databases (like MySQL, Presto, Snowflake) this requires modifying the",
            "        SQLAlchemy URI before creating the connection. For others (like Postgres), it",
            "        requires additional parameters in ``connect_args`` or running pre-session",
            "        queries with ``set`` parameters.",
            "",
            "        When a DB engine spec implements this method or ``get_prequeries`` (see below) it",
            "        should also have the attribute ``supports_dynamic_schema`` set to true, so that",
            "        Superset knows in which schema a given query is running in order to enforce",
            "        permissions (see #23385 and #23401).",
            "",
            "        Currently, changing the catalog is not supported. The method accepts a catalog so",
            "        that when catalog support is added to Superset the interface remains the same.",
            "        This is important because DB engine specs can be installed from 3rd party",
            "        packages, so we want to keep these methods as stable as possible.",
            "        \"\"\"",
            "        return uri, {",
            "            **connect_args,",
            "            **cls.enforce_uri_query_params.get(uri.get_driver_name(), {}),",
            "        }",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,  # pylint: disable=unused-argument",
            "        schema: str | None = None,  # pylint: disable=unused-argument",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return pre-session queries.",
            "",
            "        These are currently used as an alternative to ``adjust_engine_params`` for",
            "        databases where the selected schema cannot be specified in the SQLAlchemy URI or",
            "        connection arguments.",
            "",
            "        For example, in order to specify a default schema in RDS we need to run a query",
            "        at the beginning of the session:",
            "",
            "            sql> set search_path = my_schema;",
            "",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def patch(cls) -> None:",
            "        \"\"\"",
            "        TODO: Improve docstring and refactor implementation in Hive",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def get_catalog_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs from database.",
            "",
            "        This needs to be implemented per database, since SQLAlchemy doesn't offer an",
            "        abstraction.",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def get_schema_names(cls, inspector: Inspector) -> list[str]:",
            "        \"\"\"",
            "        Get all schemas from database",
            "",
            "        :param inspector: SqlAlchemy inspector",
            "        :return: All schemas in the database",
            "        \"\"\"",
            "        return sorted(inspector.get_schema_names())",
            "",
            "    @classmethod",
            "    def get_table_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the real table names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The physical table names",
            "        \"\"\"",
            "",
            "        try:",
            "            tables = set(inspector.get_table_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            tables = {re.sub(f\"^{schema}\\\\.\", \"\", table) for table in tables}",
            "        return tables",
            "",
            "    @classmethod",
            "    def get_view_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the view names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The view names",
            "        \"\"\"",
            "",
            "        try:",
            "            views = set(inspector.get_view_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            views = {re.sub(f\"^{schema}\\\\.\", \"\", view) for view in views}",
            "        return views",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,  # pylint: disable=unused-argument",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return inspector.get_indexes(table_name, schema)",
            "",
            "    @classmethod",
            "    def get_table_comment(",
            "        cls, inspector: Inspector, table_name: str, schema: str | None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Get comment of table from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :return: comment of table",
            "        \"\"\"",
            "        comment = None",
            "        try:",
            "            comment = inspector.get_table_comment(table_name, schema)",
            "            comment = comment.get(\"text\") if isinstance(comment, dict) else None",
            "        except NotImplementedError:",
            "            # It's expected that some dialects don't implement the comment method",
            "            pass",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.error(\"Unexpected error while fetching table comment\", exc_info=True)",
            "            logger.exception(ex)",
            "        return comment",
            "",
            "    @classmethod",
            "    def get_columns(  # pylint: disable=unused-argument",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Get all columns from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :param options: Extra options to customise the display of columns in",
            "                        some databases",
            "        :return: All columns in table",
            "        \"\"\"",
            "        return convert_inspector_columns(",
            "            cast(list[SQLAColumnType], inspector.get_columns(table_name, schema))",
            "        )",
            "",
            "    @classmethod",
            "    def get_metrics(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[MetricType]:",
            "        \"\"\"",
            "        Get all metrics from a given schema and table.",
            "        \"\"\"",
            "        return [",
            "            {",
            "                \"metric_name\": \"count\",",
            "                \"verbose_name\": \"COUNT(*)\",",
            "                \"metric_type\": \"count\",",
            "                \"expression\": \"COUNT(*)\",",
            "            }",
            "        ]",
            "",
            "    @classmethod",
            "    def where_latest_partition(  # pylint: disable=too-many-arguments,unused-argument",
            "        cls,",
            "        table_name: str,",
            "        schema: str | None,",
            "        database: Database,",
            "        query: Select,",
            "        columns: list[ResultSetColumnType] | None = None,",
            "    ) -> Select | None:",
            "        \"\"\"",
            "        Add a where clause to a query to reference only the most recent partition",
            "",
            "        :param table_name: Table name",
            "        :param schema: Schema name",
            "        :param database: Database instance",
            "        :param query: SqlAlchemy query",
            "        :param columns: List of TableColumns",
            "        :return: SqlAlchemy query with additional where clause referencing the latest",
            "        partition",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database, TableColumn",
            "        return None",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        return [",
            "            literal_column(query_as)",
            "            if (query_as := c.get(\"query_as\"))",
            "            else column(c[\"column_name\"])",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments,too-many-locals",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: str | None = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: list[ResultSetColumnType] | None = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Generate a \"SELECT * from [schema.]table_name\" query with appropriate limit.",
            "",
            "        WARNING: expects only unquoted table and schema names.",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name, unquoted",
            "        :param engine: SqlAlchemy Engine instance",
            "        :param schema: Schema, unquoted",
            "        :param limit: limit to impose on query",
            "        :param show_cols: Show columns in query; otherwise use \"*\"",
            "        :param indent: Add indentation to query",
            "        :param latest_partition: Only query the latest partition",
            "        :param cols: Columns to include in query",
            "        :return: SQL query",
            "        \"\"\"",
            "        # pylint: disable=redefined-outer-name",
            "        fields: str | list[Any] = \"*\"",
            "        cols = cols or []",
            "        if (show_cols or latest_partition) and not cols:",
            "            cols = database.get_columns(table_name, schema)",
            "",
            "        if show_cols:",
            "            fields = cls._get_fields(cols)",
            "        quote = engine.dialect.identifier_preparer.quote",
            "        quote_schema = engine.dialect.identifier_preparer.quote_schema",
            "        if schema:",
            "            full_table_name = quote_schema(schema) + \".\" + quote(table_name)",
            "        else:",
            "            full_table_name = quote(table_name)",
            "",
            "        qry = select(fields).select_from(text(full_table_name))",
            "",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "        if latest_partition:",
            "            partition_query = cls.where_latest_partition(",
            "                table_name, schema, database, qry, columns=cols",
            "            )",
            "            if partition_query is not None:",
            "                qry = partition_query",
            "        sql = database.compile_sqla_query(qry)",
            "        if indent:",
            "            sql = sqlparse.format(sql, reindent=True)",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        \"\"\"",
            "        Generate a SQL query that estimates the cost of a given statement.",
            "",
            "        :param statement: A single SQL statement",
            "        :param cursor: Cursor instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        \"\"\"",
            "        Format cost estimate.",
            "",
            "        :param raw_cost: Raw estimate from `estimate_query_cost`",
            "        :return: Human readable cost estimate",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def process_statement(cls, statement: str, database: Database) -> str:",
            "        \"\"\"",
            "        Process a SQL statement by stripping and mutating it.",
            "",
            "        :param statement: A single SQL statement",
            "        :param database: Database instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        parsed_query = ParsedQuery(statement)",
            "        sql = parsed_query.stripped()",
            "        sql_query_mutator = current_app.config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = current_app.config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=database,",
            "            )",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: Database,",
            "        schema: str,",
            "        sql: str,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise Exception(  # pylint: disable=broad-exception-raised",
            "                \"Database does not support cost estimation\"",
            "            )",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql)",
            "        statements = parsed_query.get_statements()",
            "",
            "        costs = []",
            "        with database.get_raw_connection(schema=schema, source=source) as conn:",
            "            cursor = conn.cursor()",
            "            for statement in statements:",
            "                processed_statement = cls.process_statement(statement, database)",
            "                costs.append(cls.estimate_statement_cost(processed_statement, cursor))",
            "",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        if impersonate_user and username is not None:",
            "            url = url.set(username=username)",
            "",
            "        return url",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "",
            "        :param connect_args: config to be updated",
            "        :param uri: URI",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def execute(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: str,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"",
            "        Execute a SQL query",
            "",
            "        :param cursor: Cursor instance",
            "        :param query: Query to execute",
            "        :param kwargs: kwargs to be passed to cursor.execute()",
            "        :return:",
            "        \"\"\"",
            "        if not cls.allows_sql_comments:",
            "            query = sql_parse.strip_comments_from_sql(query)",
            "",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            cursor.execute(query)",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def make_label_compatible(cls, label: str) -> str | quoted_name:",
            "        \"\"\"",
            "        Conditionally mutate and/or quote a sqlalchemy expression label. If",
            "        force_column_alias_quotes is set to True, return the label as a",
            "        sqlalchemy.sql.elements.quoted_name object to ensure that the select query",
            "        and query results have same case. Otherwise, return the mutated label as a",
            "        regular string. If maximum supported column name length is exceeded,",
            "        generate a truncated label by calling truncate_label().",
            "",
            "        :param label: expected expression label/alias",
            "        :return: conditionally mutated label supported by the db engine",
            "        \"\"\"",
            "        label_mutated = cls._mutate_label(label)",
            "        if (",
            "            cls.max_column_name_length",
            "            and len(label_mutated) > cls.max_column_name_length",
            "        ):",
            "            label_mutated = cls._truncate_label(label)",
            "        if cls.force_column_alias_quotes:",
            "            label_mutated = quoted_name(label_mutated, True)",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def get_column_types(",
            "        cls,",
            "        column_type: str | None,",
            "    ) -> tuple[TypeEngine, GenericDataType] | None:",
            "        \"\"\"",
            "        Return a sqlalchemy native column type and generic data type that corresponds",
            "        to the column type defined in the data source (return None to use default type",
            "        inferred by SQLAlchemy). Override `column_type_mappings` for specific needs",
            "        (see MSSQL for example of NCHAR/NVARCHAR handling).",
            "",
            "        :param column_type: Column type returned by inspector",
            "        :return: SQLAlchemy and generic Superset column types",
            "        \"\"\"",
            "        if not column_type:",
            "            return None",
            "",
            "        for regex, sqla_type, generic_type in (",
            "            cls.column_type_mappings + cls._default_column_type_mappings",
            "        ):",
            "            match = regex.match(column_type)",
            "            if not match:",
            "                continue",
            "            if callable(sqla_type):",
            "                return sqla_type(match), generic_type",
            "            return sqla_type, generic_type",
            "        return None",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        Most engines support mixed case aliases that can include numbers",
            "        and special characters, like commas, parentheses etc. For engines that",
            "        have restrictions on what types of aliases are supported, this method",
            "        can be overridden to ensure that labels conform to the engine's",
            "        limitations. Mutated labels should be deterministic (input label A always",
            "        yields output label X) and unique (input labels A and B don't yield the same",
            "        output label X).",
            "",
            "        :param label: Preferred expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        return label",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"",
            "        In the case that a label exceeds the max length supported by the engine,",
            "        this method is used to construct a deterministic and unique label based on",
            "        the original label. By default, this returns a md5 hash of the original label,",
            "        conditionally truncated if the length of the hash exceeds the max column length",
            "        of the engine.",
            "",
            "        :param label: Expected expression label",
            "        :return: Truncated label",
            "        \"\"\"",
            "        label = md5_sha_from_str(label)",
            "        # truncate hash if it exceeds max length",
            "        if cls.max_column_name_length and len(label) > cls.max_column_name_length:",
            "            label = label[: cls.max_column_name_length]",
            "        return label",
            "",
            "    @classmethod",
            "    def column_datatype_to_string(",
            "        cls, sqla_column_type: TypeEngine, dialect: Dialect",
            "    ) -> str:",
            "        \"\"\"",
            "        Convert sqlalchemy column type to string representation.",
            "        By default, removes collation and character encoding info to avoid",
            "        unnecessarily long datatypes.",
            "",
            "        :param sqla_column_type: SqlAlchemy column type",
            "        :param dialect: Sqlalchemy dialect",
            "        :return: Compiled column type",
            "        \"\"\"",
            "        sqla_column_type = sqla_column_type.copy()",
            "        if hasattr(sqla_column_type, \"collation\"):",
            "            sqla_column_type.collation = None",
            "        if hasattr(sqla_column_type, \"charset\"):",
            "            sqla_column_type.charset = None",
            "        return sqla_column_type.compile(dialect=dialect).upper()",
            "",
            "    @classmethod",
            "    def get_function_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get a list of function names that are able to be called on the database.",
            "        Used for SQL Lab autocomplete.",
            "",
            "        :param database: The database to get functions for",
            "        :return: A list of function names useable in the database",
            "        \"\"\"",
            "        return []",
            "",
            "    @staticmethod",
            "    def pyodbc_rows_to_tuples(data: list[Any]) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "        Convert pyodbc.Row objects from `fetch_data` to tuples.",
            "",
            "        :param data: List of tuples or pyodbc.Row objects",
            "        :return: List of tuples",
            "        \"\"\"",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [tuple(row) for row in data]",
            "        return data",
            "",
            "    @staticmethod",
            "    def mutate_db_for_connection_test(  # pylint: disable=unused-argument",
            "        database: Database,",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require passing additional parameters for validating database",
            "        connections. This method makes it possible to mutate the database instance prior",
            "        to testing if a connection is ok.",
            "",
            "        :param database: instance to be mutated",
            "        \"\"\"",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = {}",
            "        if database.extra:",
            "            try:",
            "                extra = json.loads(database.extra)",
            "            except json.JSONDecodeError as ex:",
            "                logger.error(ex, exc_info=True)",
            "                raise ex",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(  # pylint: disable=invalid-name",
            "        database: Database, params: dict[str, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require some sensitive information which do not conform to",
            "        the username:password syntax normally used by SQLAlchemy.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :param params: params to be updated",
            "        \"\"\"",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            params.update(encrypted_extra)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def is_readonly_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"Pessimistic readonly, 100% sure statement won't mutate anything\"\"\"",
            "        return (",
            "            parsed_query.is_select()",
            "            or parsed_query.is_explain()",
            "            or parsed_query.is_show()",
            "        )",
            "",
            "    @classmethod",
            "    def is_select_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"",
            "        Determine if the statement should be considered as SELECT statement.",
            "        Some query dialects do not contain \"SELECT\" word in queries (eg. Kusto)",
            "        \"\"\"",
            "        return parsed_query.is_select()",
            "",
            "    @classmethod",
            "    def get_column_spec(  # pylint: disable=unused-argument",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> ColumnSpec | None:",
            "        \"\"\"",
            "        Get generic type related specs regarding a native column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        if col_types := cls.get_column_types(native_type):",
            "            column_type, generic_type = col_types",
            "            is_dttm = generic_type == GenericDataType.TEMPORAL",
            "            return ColumnSpec(",
            "                sqla_type=column_type, generic_type=generic_type, is_dttm=is_dttm",
            "            )",
            "        return None",
            "",
            "    @classmethod",
            "    def get_sqla_column_type(",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> TypeEngine | None:",
            "        \"\"\"",
            "        Converts native database type to sqlalchemy column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        column_spec = cls.get_column_spec(",
            "            native_type=native_type,",
            "            db_extra=db_extra,",
            "            source=source,",
            "        )",
            "        return column_spec.sqla_type if column_spec else None",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query, session: Session) -> None:",
            "        \"\"\"",
            "        Some databases may acquire the query cancelation id after the query",
            "        cancelation request has been received. For those cases, the db engine spec",
            "        can record the cancelation intent so that the query can either be stopped",
            "        prior to execution, or canceled once the query id is acquired.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def has_implicit_cancel(cls) -> bool:",
            "        \"\"\"",
            "        Return True if the live cursor handles the implicit cancelation of the query,",
            "        False otherwise.",
            "",
            "        :return: Whether the live cursor implicitly cancels the query",
            "        :see: handle_cursor",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Select identifiers from the database engine that uniquely identifies the",
            "        queries to cancel. The identifier is typically a session id, process id",
            "        or similar.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Query identifier",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def cancel_query(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "        cancel_query_id: str,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Value returned by get_cancel_query_payload or set in",
            "        other life-cycle methods of the query",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def parse_sql(cls, sql: str) -> list[str]:",
            "        return [str(s).strip(\" ;\") for s in sqlparse.parse(sql)]",
            "",
            "    @classmethod",
            "    def get_impersonation_key(cls, user: User | None) -> Any:",
            "        \"\"\"",
            "        Construct an impersonation key, by default it's the given username.",
            "",
            "        :param user: logged-in user",
            "",
            "        :returns: username if given user is not null",
            "        \"\"\"",
            "        return user.username if user else None",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: str | None) -> str | None:",
            "        \"\"\"",
            "        Mask ``encrypted_extra``.",
            "",
            "        This is used to remove any sensitive data in ``encrypted_extra`` when presenting",
            "        it to the user. For example, a private key might be replaced with a masked value",
            "        \"XXXXXXXXXX\". If the masked value is changed the corresponding entry is updated,",
            "        otherwise the old value is used (see ``unmask_encrypted_extra`` below).",
            "        \"\"\"",
            "        return encrypted_extra",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def unmask_encrypted_extra(cls, old: str | None, new: str | None) -> str | None:",
            "        \"\"\"",
            "        Remove masks from ``encrypted_extra``.",
            "",
            "        This method allows reusing existing values from the current encrypted extra on",
            "        updates. It's useful for reusing masked passwords, allowing keys to be updated",
            "        without having to provide sensitive data to the client.",
            "        \"\"\"",
            "        return new",
            "",
            "    @classmethod",
            "    def get_public_information(cls) -> dict[str, Any]:",
            "        \"\"\"",
            "        Construct a Dict with properties we want to expose.",
            "",
            "        :returns: Dict with properties of our class like supports_file_upload",
            "        and disable_ssh_tunneling",
            "        \"\"\"",
            "        return {",
            "            \"supports_file_upload\": cls.supports_file_upload,",
            "            \"disable_ssh_tunneling\": cls.disable_ssh_tunneling,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_database_uri(cls, sqlalchemy_uri: URL) -> None:",
            "        \"\"\"",
            "        Validates a database SQLAlchemy URI per engine spec.",
            "        Use this to implement a final validation for unwanted connection configuration",
            "",
            "        :param sqlalchemy_uri:",
            "        \"\"\"",
            "        if existing_disallowed := cls.disallow_uri_query_params.get(",
            "            sqlalchemy_uri.get_driver_name(), set()",
            "        ).intersection(sqlalchemy_uri.query):",
            "            raise ValueError(f\"Forbidden query parameter(s): {existing_disallowed}\")",
            "",
            "    @classmethod",
            "    def denormalize_name(cls, dialect: Dialect, name: str) -> str:",
            "        if (",
            "            hasattr(dialect, \"requires_name_normalize\")",
            "            and dialect.requires_name_normalize",
            "        ):",
            "            return dialect.denormalize_name(name)",
            "",
            "        return name",
            "",
            "",
            "# schema for adding a database by providing parameters instead of the",
            "# full SQLAlchemy URI",
            "class BasicParametersSchema(Schema):",
            "    username = fields.String(",
            "        required=True, allow_none=True, metadata={\"description\": __(\"Username\")}",
            "    )",
            "    password = fields.String(allow_none=True, metadata={\"description\": __(\"Password\")})",
            "    host = fields.String(",
            "        required=True, metadata={\"description\": __(\"Hostname or IP address\")}",
            "    )",
            "    port = fields.Integer(",
            "        required=True,",
            "        metadata={\"description\": __(\"Database port\")},",
            "        validate=Range(min=0, max=2**16, max_inclusive=False),",
            "    )",
            "    database = fields.String(",
            "        required=True, metadata={\"description\": __(\"Database name\")}",
            "    )",
            "    query = fields.Dict(",
            "        keys=fields.Str(),",
            "        values=fields.Raw(),",
            "        metadata={\"description\": __(\"Additional parameters\")},",
            "    )",
            "    encryption = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an encrypted connection to the database\")},",
            "    )",
            "    ssh = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an ssh tunnel connection to the database\")},",
            "    )",
            "",
            "",
            "class BasicParametersType(TypedDict, total=False):",
            "    username: str | None",
            "    password: str | None",
            "    host: str",
            "    port: int",
            "    database: str",
            "    query: dict[str, Any]",
            "    encryption: bool",
            "",
            "",
            "class BasicPropertiesType(TypedDict):",
            "    parameters: BasicParametersType",
            "",
            "",
            "class BasicParametersMixin:",
            "    \"\"\"",
            "    Mixin for configuring DB engine specs via a dictionary.",
            "",
            "    With this mixin the SQLAlchemy engine can be configured through",
            "    individual parameters, instead of the full SQLAlchemy URI. This",
            "    mixin is for the most common pattern of URI:",
            "",
            "        engine+driver://user:password@host:port/dbname[?key=value&key=value...]",
            "",
            "    \"\"\"",
            "",
            "    # schema describing the parameters used to configure the DB",
            "    parameters_schema = BasicParametersSchema()",
            "",
            "    # recommended driver name for the DB engine spec",
            "    default_driver = \"\"",
            "",
            "    # query parameter to enable encryption in the database connection",
            "    # for Postgres this would be `{\"sslmode\": \"verify-ca\"}`, eg.",
            "    encryption_parameters: dict[str, str] = {}",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(  # pylint: disable=unused-argument",
            "        cls,",
            "        parameters: BasicParametersType,",
            "        encrypted_extra: dict[str, str] | None = None,",
            "    ) -> str:",
            "        # make a copy so that we don't update the original",
            "        query = parameters.get(\"query\", {}).copy()",
            "        if parameters.get(\"encryption\"):",
            "            if not cls.encryption_parameters:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Unable to build a URL with encryption enabled\"",
            "                )",
            "            query.update(cls.encryption_parameters)",
            "",
            "        return str(",
            "            URL.create(",
            "                f\"{cls.engine}+{cls.default_driver}\".rstrip(\"+\"),  # type: ignore",
            "                username=parameters.get(\"username\"),",
            "                password=parameters.get(\"password\"),",
            "                host=parameters[\"host\"],",
            "                port=parameters[\"port\"],",
            "                database=parameters[\"database\"],",
            "                query=query,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(  # pylint: disable=unused-argument",
            "        cls, uri: str, encrypted_extra: dict[str, Any] | None = None",
            "    ) -> BasicParametersType:",
            "        url = make_url_safe(uri)",
            "        query = {",
            "            key: value",
            "            for (key, value) in url.query.items()",
            "            if (key, value) not in cls.encryption_parameters.items()",
            "        }",
            "        encryption = all(",
            "            item in url.query.items() for item in cls.encryption_parameters.items()",
            "        )",
            "        return {",
            "            \"username\": url.username,",
            "            \"password\": url.password,",
            "            \"host\": url.host,",
            "            \"port\": url.port,",
            "            \"database\": url.database,",
            "            \"query\": query,",
            "            \"encryption\": encryption,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls, properties: BasicPropertiesType",
            "    ) -> list[SupersetError]:",
            "        \"\"\"",
            "        Validates any number of parameters, for progressive validation.",
            "",
            "        If only the hostname is present it will check if the name is resolvable. As more",
            "        parameters are present in the request, more validation is done.",
            "        \"\"\"",
            "        errors: list[SupersetError] = []",
            "",
            "        required = {\"host\", \"port\", \"username\", \"database\"}",
            "        parameters = properties.get(\"parameters\", {})",
            "        present = {key for key in parameters if parameters.get(key, ())}",
            "",
            "        if missing := sorted(required - present):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=f'One or more parameters are missing: {\", \".join(missing)}',",
            "                    error_type=SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR,",
            "                    level=ErrorLevel.WARNING,",
            "                    extra={\"missing\": missing},",
            "                ),",
            "            )",
            "",
            "        host = parameters.get(\"host\", None)",
            "        if not host:",
            "            return errors",
            "        if not is_hostname_valid(host):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The hostname provided can't be resolved.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"host\"]},",
            "                ),",
            "            )",
            "            return errors",
            "",
            "        port = parameters.get(\"port\", None)",
            "        if not port:",
            "            return errors",
            "        try:",
            "            port = int(port)",
            "        except (ValueError, TypeError):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"Port must be a valid integer.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        if not (isinstance(port, int) and 0 <= port < 2**16):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=(",
            "                        \"The port must be an integer between 0 and 65535 \"",
            "                        \"(inclusive).\"",
            "                    ),",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        elif not is_port_open(host, port):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The port is closed.\",",
            "                    error_type=SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "",
            "        return errors",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.2\",",
            "            plugins=[MarshmallowPlugin()],",
            "        )",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "",
            "from __future__ import annotations",
            "",
            "import json",
            "import logging",
            "import re",
            "from datetime import datetime",
            "from re import Match, Pattern",
            "from typing import (",
            "    Any,",
            "    Callable,",
            "    cast,",
            "    ContextManager,",
            "    NamedTuple,",
            "    TYPE_CHECKING,",
            "    TypedDict,",
            "    Union,",
            ")",
            "",
            "import pandas as pd",
            "import sqlparse",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask import current_app",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import gettext as __, lazy_gettext as _",
            "from marshmallow import fields, Schema",
            "from marshmallow.validate import Range",
            "from sqlalchemy import column, select, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.interfaces import Compiled, Dialect",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.ext.compiler import compiles",
            "from sqlalchemy.orm import Session",
            "from sqlalchemy.sql import literal_column, quoted_name, text",
            "from sqlalchemy.sql.expression import ColumnClause, Select, TextAsFrom, TextClause",
            "from sqlalchemy.types import TypeEngine",
            "from sqlparse.tokens import CTE",
            "",
            "from superset import security_manager, sql_parse",
            "from superset.constants import TimeGrain as TimeGrainConstants",
            "from superset.databases.utils import make_url_safe",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.sql_parse import ParsedQuery, Table",
            "from superset.superset_typing import ResultSetColumnType, SQLAColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.core import ColumnSpec, GenericDataType",
            "from superset.utils.hashing import md5_sha_from_str",
            "from superset.utils.network import is_hostname_valid, is_port_open",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.models.core import Database",
            "    from superset.models.sql_lab import Query",
            "",
            "ColumnTypeMapping = tuple[",
            "    Pattern[str],",
            "    Union[TypeEngine, Callable[[Match[str]], TypeEngine]],",
            "    GenericDataType,",
            "]",
            "",
            "logger = logging.getLogger()",
            "",
            "",
            "def convert_inspector_columns(cols: list[SQLAColumnType]) -> list[ResultSetColumnType]:",
            "    result_set_columns: list[ResultSetColumnType] = []",
            "    for col in cols:",
            "        result_set_columns.append({\"column_name\": col.get(\"name\"), **col})  # type: ignore",
            "    return result_set_columns",
            "",
            "",
            "class TimeGrain(NamedTuple):",
            "    name: str  # TODO: redundant field, remove",
            "    label: str",
            "    function: str",
            "    duration: str | None",
            "",
            "",
            "builtin_time_grains: dict[str | None, str] = {",
            "    TimeGrainConstants.SECOND: __(\"Second\"),",
            "    TimeGrainConstants.FIVE_SECONDS: __(\"5 second\"),",
            "    TimeGrainConstants.THIRTY_SECONDS: __(\"30 second\"),",
            "    TimeGrainConstants.MINUTE: __(\"Minute\"),",
            "    TimeGrainConstants.FIVE_MINUTES: __(\"5 minute\"),",
            "    TimeGrainConstants.TEN_MINUTES: __(\"10 minute\"),",
            "    TimeGrainConstants.FIFTEEN_MINUTES: __(\"15 minute\"),",
            "    TimeGrainConstants.THIRTY_MINUTES: __(\"30 minute\"),",
            "    TimeGrainConstants.HOUR: __(\"Hour\"),",
            "    TimeGrainConstants.SIX_HOURS: __(\"6 hour\"),",
            "    TimeGrainConstants.DAY: __(\"Day\"),",
            "    TimeGrainConstants.WEEK: __(\"Week\"),",
            "    TimeGrainConstants.MONTH: __(\"Month\"),",
            "    TimeGrainConstants.QUARTER: __(\"Quarter\"),",
            "    TimeGrainConstants.YEAR: __(\"Year\"),",
            "    TimeGrainConstants.WEEK_STARTING_SUNDAY: __(\"Week starting Sunday\"),",
            "    TimeGrainConstants.WEEK_STARTING_MONDAY: __(\"Week starting Monday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SATURDAY: __(\"Week ending Saturday\"),",
            "    TimeGrainConstants.WEEK_ENDING_SUNDAY: __(\"Week ending Sunday\"),",
            "}",
            "",
            "",
            "class TimestampExpression(",
            "    ColumnClause",
            "):  # pylint: disable=abstract-method, too-many-ancestors",
            "    def __init__(self, expr: str, col: ColumnClause, **kwargs: Any) -> None:",
            "        \"\"\"Sqlalchemy class that can be used to render native column elements respecting",
            "        engine-specific quoting rules as part of a string-based expression.",
            "",
            "        :param expr: Sql expression with '{col}' denoting the locations where the col",
            "        object will be rendered.",
            "        :param col: the target column",
            "        \"\"\"",
            "        super().__init__(expr, **kwargs)",
            "        self.col = col",
            "",
            "    @property",
            "    def _constructor(self) -> ColumnClause:",
            "        # Needed to ensure that the column label is rendered correctly when",
            "        # proxied to the outer query.",
            "        # See https://github.com/sqlalchemy/sqlalchemy/issues/4730",
            "        return ColumnClause",
            "",
            "",
            "@compiles(TimestampExpression)",
            "def compile_timegrain_expression(",
            "    element: TimestampExpression, compiler: Compiled, **kwargs: Any",
            ") -> str:",
            "    return element.name.replace(\"{col}\", compiler.process(element.col, **kwargs))",
            "",
            "",
            "class LimitMethod:  # pylint: disable=too-few-public-methods",
            "    \"\"\"Enum the ways that limits can be applied\"\"\"",
            "",
            "    FETCH_MANY = \"fetch_many\"",
            "    WRAP_SQL = \"wrap_sql\"",
            "    FORCE_LIMIT = \"force_limit\"",
            "",
            "",
            "class MetricType(TypedDict, total=False):",
            "    \"\"\"",
            "    Type for metrics return by `get_metrics`.",
            "    \"\"\"",
            "",
            "    metric_name: str",
            "    expression: str",
            "    verbose_name: str | None",
            "    metric_type: str | None",
            "    description: str | None",
            "    d3format: str | None",
            "    currency: str | None",
            "    warning_text: str | None",
            "    extra: str | None",
            "",
            "",
            "class BaseEngineSpec:  # pylint: disable=too-many-public-methods",
            "    \"\"\"Abstract class for database engine specific configurations",
            "",
            "    Attributes:",
            "        allows_alias_to_source_column: Whether the engine is able to pick the",
            "                                       source column for aggregation clauses",
            "                                       used in ORDER BY when a column in SELECT",
            "                                       has an alias that is the same as a source",
            "                                       column.",
            "        allows_hidden_orderby_agg:     Whether the engine allows ORDER BY to",
            "                                       directly use aggregation clauses, without",
            "                                       having to add the same aggregation in SELECT.",
            "    \"\"\"",
            "",
            "    engine_name: str | None = None  # for user messages, overridden in child classes",
            "",
            "    # These attributes map the DB engine spec to one or more SQLAlchemy dialects/drivers;",
            "    # see the ``supports_url`` and ``supports_backend`` methods below.",
            "    engine = \"base\"  # str as defined in sqlalchemy.engine.engine",
            "    engine_aliases: set[str] = set()",
            "    drivers: dict[str, str] = {}",
            "    default_driver: str | None = None",
            "",
            "    # placeholder with the SQLAlchemy URI template",
            "    sqlalchemy_uri_placeholder = (",
            "        \"engine+driver://user:password@host:port/dbname[?key=value&key=value...]\"",
            "    )",
            "",
            "    disable_ssh_tunneling = False",
            "",
            "    _date_trunc_functions: dict[str, str] = {}",
            "    _time_grain_expressions: dict[str | None, str] = {}",
            "    _default_column_type_mappings: tuple[ColumnTypeMapping, ...] = (",
            "        (",
            "            re.compile(r\"^string\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^n((var)?char|text)\", re.IGNORECASE),",
            "            types.UnicodeText(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(var)?char\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^(tiny|medium|long)?text\", re.IGNORECASE),",
            "            types.String(),",
            "            GenericDataType.STRING,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallint\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^int(eger)?\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigint\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^long\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^decimal\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^numeric\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^float\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^double\", re.IGNORECASE),",
            "            types.Float(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^real\", re.IGNORECASE),",
            "            types.REAL,",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^smallserial\", re.IGNORECASE),",
            "            types.SmallInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^serial\", re.IGNORECASE),",
            "            types.Integer(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^bigserial\", re.IGNORECASE),",
            "            types.BigInteger(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^money\", re.IGNORECASE),",
            "            types.Numeric(),",
            "            GenericDataType.NUMERIC,",
            "        ),",
            "        (",
            "            re.compile(r\"^timestamp\", re.IGNORECASE),",
            "            types.TIMESTAMP(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^datetime\", re.IGNORECASE),",
            "            types.DateTime(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^date\", re.IGNORECASE),",
            "            types.Date(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^time\", re.IGNORECASE),",
            "            types.Time(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^interval\", re.IGNORECASE),",
            "            types.Interval(),",
            "            GenericDataType.TEMPORAL,",
            "        ),",
            "        (",
            "            re.compile(r\"^bool(ean)?\", re.IGNORECASE),",
            "            types.Boolean(),",
            "            GenericDataType.BOOLEAN,",
            "        ),",
            "    )",
            "    # engine-specific type mappings to check prior to the defaults",
            "    column_type_mappings: tuple[ColumnTypeMapping, ...] = ()",
            "",
            "    # type-specific functions to mutate values received from the database.",
            "    # Needed on certain databases that return values in an unexpected format",
            "    column_type_mutators: dict[TypeEngine, Callable[[Any], Any]] = {}",
            "",
            "    # Does database support join-free timeslot grouping",
            "    time_groupby_inline = False",
            "    limit_method = LimitMethod.FORCE_LIMIT",
            "    allows_joins = True",
            "    allows_subqueries = True",
            "    allows_alias_in_select = True",
            "    allows_alias_in_orderby = True",
            "    allows_sql_comments = True",
            "    allows_escaped_colons = True",
            "",
            "    # Whether ORDER BY clause can use aliases created in SELECT",
            "    # that are the same as a source column",
            "    allows_alias_to_source_column = True",
            "",
            "    # Whether ORDER BY clause must appear in SELECT",
            "    # if True, then it doesn't have to.",
            "    allows_hidden_orderby_agg = True",
            "",
            "    # Whether ORDER BY clause can use sql calculated expression",
            "    # if True, use alias of select column for `order by`",
            "    # the True is safely for most database",
            "    # But for backward compatibility, False by default",
            "    allows_hidden_cc_in_orderby = False",
            "",
            "    # Whether allow CTE as subquery or regular CTE",
            "    # If True, then it will allow  in subquery ,",
            "    # if False it will allow as regular CTE",
            "    allows_cte_in_subquery = True",
            "    # Define alias for CTE",
            "    cte_alias = \"__cte\"",
            "    # Whether allow LIMIT clause in the SQL",
            "    # If True, then the database engine is allowed for LIMIT clause",
            "    # If False, then the database engine is allowed for TOP clause",
            "    allow_limit_clause = True",
            "    # This set will give keywords for select statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    select_keywords: set[str] = {\"SELECT\"}",
            "    # This set will give the keywords for data limit statements",
            "    # to consider for the engines with TOP SQL parsing",
            "    top_keywords: set[str] = {\"TOP\"}",
            "    # A set of disallowed connection query parameters by driver name",
            "    disallow_uri_query_params: dict[str, set[str]] = {}",
            "    # A Dict of query parameters that will always be used on every connection",
            "    # by driver name",
            "    enforce_uri_query_params: dict[str, dict[str, Any]] = {}",
            "",
            "    force_column_alias_quotes = False",
            "    arraysize = 0",
            "    max_column_name_length: int | None = None",
            "    try_remove_schema_from_table_name = True  # pylint: disable=invalid-name",
            "    run_multiple_statements_as_one = False",
            "    custom_errors: dict[",
            "        Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]",
            "    ] = {}",
            "",
            "    # Whether the engine supports file uploads",
            "    # if True, database will be listed as option in the upload file form",
            "    supports_file_upload = True",
            "",
            "    # Is the DB engine spec able to change the default schema? This requires implementing",
            "    # a custom `adjust_engine_params` method.",
            "    supports_dynamic_schema = False",
            "",
            "    # Does the DB support catalogs? A catalog here is a group of schemas, and has",
            "    # different names depending on the DB: BigQuery calles it a \"project\", Postgres calls",
            "    # it a \"database\", Trino calls it a \"catalog\", etc.",
            "    supports_catalog = False",
            "",
            "    # Can the catalog be changed on a per-query basis?",
            "    supports_dynamic_catalog = False",
            "",
            "    @classmethod",
            "    def get_allows_alias_in_select(",
            "        cls, database: Database  # pylint: disable=unused-argument",
            "    ) -> bool:",
            "        \"\"\"",
            "        Method for dynamic `allows_alias_in_select`.",
            "",
            "        In Dremio this atribute is version-dependent, so Superset needs to inspect the",
            "        database configuration in order to determine it. This method allows engine-specs",
            "        to define dynamic values for the attribute.",
            "        \"\"\"",
            "        return cls.allows_alias_in_select",
            "",
            "    @classmethod",
            "    def supports_url(cls, url: URL) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy URL.",
            "",
            "        As an example, if a given DB engine spec has:",
            "",
            "            class PostgresDBEngineSpec:",
            "                engine = \"postgresql\"",
            "                engine_aliases = \"postgres\"",
            "                drivers = {",
            "                    \"psycopg2\": \"The default Postgres driver\",",
            "                    \"asyncpg\": \"An asynchronous Postgres driver\",",
            "                }",
            "",
            "        It would be used for all the following SQLAlchemy URIs:",
            "",
            "            - postgres://user:password@host/db",
            "            - postgresql://user:password@host/db",
            "            - postgres+asyncpg://user:password@host/db",
            "            - postgres+psycopg2://user:password@host/db",
            "            - postgresql+asyncpg://user:password@host/db",
            "            - postgresql+psycopg2://user:password@host/db",
            "",
            "        Note that SQLAlchemy has a default driver even if one is not specified:",
            "",
            "            >>> from sqlalchemy.engine.url import make_url",
            "            >>> make_url('postgres://').get_driver_name()",
            "            'psycopg2'",
            "",
            "        \"\"\"",
            "        backend = url.get_backend_name()",
            "        driver = url.get_driver_name()",
            "        return cls.supports_backend(backend, driver)",
            "",
            "    @classmethod",
            "    def supports_backend(cls, backend: str, driver: str | None = None) -> bool:",
            "        \"\"\"",
            "        Returns true if the DB engine spec supports a given SQLAlchemy backend/driver.",
            "        \"\"\"",
            "        # check the backend first",
            "        if backend != cls.engine and backend not in cls.engine_aliases:",
            "            return False",
            "",
            "        # originally DB engine specs didn't declare any drivers and the check was made",
            "        # only on the engine; if that's the case, ignore the driver for backwards",
            "        # compatibility",
            "        if not cls.drivers or driver is None:",
            "            return True",
            "",
            "        return driver in cls.drivers",
            "",
            "    @classmethod",
            "    def get_default_schema(cls, database: Database) -> str | None:",
            "        \"\"\"",
            "        Return the default schema in a given database.",
            "        \"\"\"",
            "        with database.get_inspector_with_context() as inspector:",
            "            return inspector.default_schema_name",
            "",
            "    @classmethod",
            "    def get_schema_from_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        sqlalchemy_uri: URL,",
            "        connect_args: dict[str, Any],",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the schema configured in a SQLALchemy URI and connection arguments, if any.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def get_default_schema_for_query(",
            "        cls,",
            "        database: Database,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Return the default schema for a given query.",
            "",
            "        This is used to determine the schema of tables that aren't fully qualified, eg:",
            "",
            "            SELECT * FROM foo;",
            "",
            "        In the example above, the schema where the `foo` table lives depends on a few",
            "        factors:",
            "",
            "            1. For DB engine specs that allow dynamically changing the schema based on the",
            "               query we should use the query schema.",
            "            2. For DB engine specs that don't support dynamically changing the schema and",
            "               have the schema hardcoded in the SQLAlchemy URI we should use the schema",
            "               from the URI.",
            "            3. For DB engine specs that don't connect to a specific schema and can't",
            "               change it dynamically we need to probe the database for the default schema.",
            "",
            "        Determining the correct schema is crucial for managing access to data, so please",
            "        make sure you understand this logic when working on a new DB engine spec.",
            "        \"\"\"",
            "        # dynamic schema varies on a per-query basis",
            "        if cls.supports_dynamic_schema:",
            "            return query.schema",
            "",
            "        # check if the schema is stored in the SQLAlchemy URI or connection arguments",
            "        try:",
            "            connect_args = database.get_extra()[\"engine_params\"][\"connect_args\"]",
            "        except KeyError:",
            "            connect_args = {}",
            "        sqlalchemy_uri = make_url_safe(database.sqlalchemy_uri)",
            "        if schema := cls.get_schema_from_engine_params(sqlalchemy_uri, connect_args):",
            "            return schema",
            "",
            "        # return the default schema of the database",
            "        return cls.get_default_schema(database)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific exceptions into",
            "        Superset DBAPI exceptions",
            "",
            "        Note: On python 3.9 this method can be changed to a classmethod property",
            "        without the need of implementing a metaclass type",
            "",
            "        :return: A map of driver specific exception to superset custom exceptions",
            "        \"\"\"",
            "        return {}",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Each engine can implement and converge its own specific parser method",
            "",
            "        :return: An Exception with a parsed string off the original exception",
            "        \"\"\"",
            "        return exception",
            "",
            "    @classmethod",
            "    def get_dbapi_mapped_exception(cls, exception: Exception) -> Exception:",
            "        \"\"\"",
            "        Get a superset custom DBAPI exception from the driver specific exception.",
            "",
            "        Override if the engine needs to perform extra changes to the exception, for",
            "        example change the exception message or implement custom more complex logic",
            "",
            "        :param exception: The driver specific exception",
            "        :return: Superset custom DBAPI exception",
            "        \"\"\"",
            "        new_exception = cls.get_dbapi_exception_mapping().get(type(exception))",
            "        if not new_exception:",
            "            return cls.parse_error_exception(exception)",
            "        return new_exception(str(exception))",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(  # pylint: disable=unused-argument",
            "        cls,",
            "        extra: dict[str, Any],",
            "    ) -> bool:",
            "        return False",
            "",
            "    @classmethod",
            "    def get_text_clause(cls, clause: str) -> TextClause:",
            "        \"\"\"",
            "        SQLAlchemy wrapper to ensure text clauses are escaped properly",
            "",
            "        :param clause: string clause with potentially unescaped characters",
            "        :return: text clause with escaped characters",
            "        \"\"\"",
            "        if cls.allows_escaped_colons:",
            "            clause = clause.replace(\":\", \"\\\\:\")",
            "        return text(clause)",
            "",
            "    @classmethod",
            "    def get_engine(",
            "        cls,",
            "        database: Database,",
            "        schema: str | None = None,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> ContextManager[Engine]:",
            "        \"\"\"",
            "        Return an engine context manager.",
            "",
            "            >>> with DBEngineSpec.get_engine(database, schema, source) as engine:",
            "            ...     connection = engine.connect()",
            "            ...     connection.execute(sql)",
            "",
            "        \"\"\"",
            "        return database.get_sqla_engine_with_context(schema=schema, source=source)",
            "",
            "    @classmethod",
            "    def get_timestamp_expr(",
            "        cls,",
            "        col: ColumnClause,",
            "        pdf: str | None,",
            "        time_grain: str | None,",
            "    ) -> TimestampExpression:",
            "        \"\"\"",
            "        Construct a TimestampExpression to be used in a SQLAlchemy query.",
            "",
            "        :param col: Target column for the TimestampExpression",
            "        :param pdf: date format (seconds or milliseconds)",
            "        :param time_grain: time grain, e.g. P1Y for 1 year",
            "        :return: TimestampExpression object",
            "        \"\"\"",
            "        if time_grain:",
            "            type_ = str(getattr(col, \"type\", \"\"))",
            "            time_expr = cls.get_time_grain_expressions().get(time_grain)",
            "            if not time_expr:",
            "                raise NotImplementedError(",
            "                    f\"No grain spec for {time_grain} for database {cls.engine}\"",
            "                )",
            "            if type_ and \"{func}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{func}\", date_trunc_function)",
            "            if type_ and \"{type}\" in time_expr:",
            "                date_trunc_function = cls._date_trunc_functions.get(type_)",
            "                if date_trunc_function:",
            "                    time_expr = time_expr.replace(\"{type}\", type_)",
            "        else:",
            "            time_expr = \"{col}\"",
            "",
            "        # if epoch, translate to DATE using db specific conf",
            "        if pdf == \"epoch_s\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_to_dttm())",
            "        elif pdf == \"epoch_ms\":",
            "            time_expr = time_expr.replace(\"{col}\", cls.epoch_ms_to_dttm())",
            "",
            "        return TimestampExpression(time_expr, col, type_=col.type)",
            "",
            "    @classmethod",
            "    def get_time_grains(cls) -> tuple[TimeGrain, ...]:",
            "        \"\"\"",
            "        Generate a tuple of supported time grains.",
            "",
            "        :return: All time grains supported by the engine",
            "        \"\"\"",
            "",
            "        ret_list = []",
            "        time_grains = builtin_time_grains.copy()",
            "        time_grains.update(current_app.config[\"TIME_GRAIN_ADDONS\"])",
            "        for duration, func in cls.get_time_grain_expressions().items():",
            "            if duration in time_grains:",
            "                name = time_grains[duration]",
            "                ret_list.append(TimeGrain(name, _(name), func, duration))",
            "        return tuple(ret_list)",
            "",
            "    @classmethod",
            "    def _sort_time_grains(",
            "        cls, val: tuple[str | None, str], index: int",
            "    ) -> float | int | str:",
            "        \"\"\"",
            "        Return an ordered time-based value of a portion of a time grain",
            "        for sorting",
            "        Values are expected to be either None or start with P or PT",
            "        Have a numerical value in the middle and end with",
            "        a value for the time interval",
            "        It can also start or end with epoch start time denoting a range",
            "        i.e, week beginning or ending with a day",
            "        \"\"\"",
            "        pos = {",
            "            \"FIRST\": 0,",
            "            \"SECOND\": 1,",
            "            \"THIRD\": 2,",
            "            \"LAST\": 3,",
            "        }",
            "",
            "        if val[0] is None:",
            "            return pos[\"FIRST\"]",
            "",
            "        prog = re.compile(r\"(.*\\/)?(P|PT)([0-9\\.]+)(S|M|H|D|W|M|Y)(\\/.*)?\")",
            "        result = prog.match(val[0])",
            "",
            "        # for any time grains that don't match the format, put them at the end",
            "        if result is None:",
            "            return pos[\"LAST\"]",
            "",
            "        second_minute_hour = [\"S\", \"M\", \"H\"]",
            "        day_week_month_year = [\"D\", \"W\", \"M\", \"Y\"]",
            "        is_less_than_day = result.group(2) == \"PT\"",
            "        interval = result.group(4)",
            "        epoch_time_start_string = result.group(1) or result.group(5)",
            "        has_starting_or_ending = bool(len(epoch_time_start_string or \"\"))",
            "",
            "        def sort_day_week() -> int:",
            "            if has_starting_or_ending:",
            "                return pos[\"LAST\"]",
            "            if is_less_than_day:",
            "                return pos[\"SECOND\"]",
            "            return pos[\"THIRD\"]",
            "",
            "        def sort_interval() -> float:",
            "            if is_less_than_day:",
            "                return second_minute_hour.index(interval)",
            "            return day_week_month_year.index(interval)",
            "",
            "        # 0: all \"PT\" values should come before \"P\" values (i.e, PT10M)",
            "        # 1: order values within the above arrays (\"D\" before \"W\")",
            "        # 2: sort by numeric value (PT10M before PT15M)",
            "        # 3: sort by any week starting/ending values",
            "        plist = {",
            "            0: sort_day_week(),",
            "            1: pos[\"SECOND\"] if is_less_than_day else pos[\"THIRD\"],",
            "            2: sort_interval(),",
            "            3: float(result.group(3)),",
            "        }",
            "",
            "        return plist.get(index, 0)",
            "",
            "    @classmethod",
            "    def get_time_grain_expressions(cls) -> dict[str | None, str]:",
            "        \"\"\"",
            "        Return a dict of all supported time grains including any potential added grains",
            "        but excluding any potentially disabled grains in the config file.",
            "",
            "        :return: All time grain expressions supported by the engine",
            "        \"\"\"",
            "        # TODO: use @memoize decorator or similar to avoid recomputation on every call",
            "        time_grain_expressions = cls._time_grain_expressions.copy()",
            "        grain_addon_expressions = current_app.config[\"TIME_GRAIN_ADDON_EXPRESSIONS\"]",
            "        time_grain_expressions.update(grain_addon_expressions.get(cls.engine, {}))",
            "        denylist: list[str] = current_app.config[\"TIME_GRAIN_DENYLIST\"]",
            "        for key in denylist:",
            "            time_grain_expressions.pop(key, None)",
            "",
            "        return dict(",
            "            sorted(",
            "                time_grain_expressions.items(),",
            "                key=lambda x: (",
            "                    cls._sort_time_grains(x, 0),",
            "                    cls._sort_time_grains(x, 1),",
            "                    cls._sort_time_grains(x, 2),",
            "                    cls._sort_time_grains(x, 3),",
            "                ),",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def fetch_data(cls, cursor: Any, limit: int | None = None) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "",
            "        :param cursor: Cursor instance",
            "        :param limit: Maximum number of rows to be returned by the cursor",
            "        :return: Result of query",
            "        \"\"\"",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            if cls.limit_method == LimitMethod.FETCH_MANY and limit:",
            "                return cursor.fetchmany(limit)",
            "            data = cursor.fetchall()",
            "            description = cursor.description or []",
            "            # Create a mapping between column name and a mutator function to normalize",
            "            # values with. The first two items in the description row are",
            "            # the column name and type.",
            "            column_mutators = {",
            "                row[0]: func",
            "                for row in description",
            "                if (",
            "                    func := cls.column_type_mutators.get(",
            "                        type(cls.get_sqla_column_type(cls.get_datatype(row[1])))",
            "                    )",
            "                )",
            "            }",
            "            if column_mutators:",
            "                indexes = {row[0]: idx for idx, row in enumerate(description)}",
            "                for row_idx, row in enumerate(data):",
            "                    new_row = list(row)",
            "                    for col, func in column_mutators.items():",
            "                        col_idx = indexes[col]",
            "                        new_row[col_idx] = func(row[col_idx])",
            "                    data[row_idx] = tuple(new_row)",
            "",
            "            return data",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def expand_data(",
            "        cls, columns: list[ResultSetColumnType], data: list[dict[Any, Any]]",
            "    ) -> tuple[",
            "        list[ResultSetColumnType], list[dict[Any, Any]], list[ResultSetColumnType]",
            "    ]:",
            "        \"\"\"",
            "        Some engines support expanding nested fields. See implementation in Presto",
            "        spec for details.",
            "",
            "        :param columns: columns selected in the query",
            "        :param data: original data set",
            "        :return: list of all columns(selected columns and their nested fields),",
            "                 expanded data set, listed of nested fields",
            "        \"\"\"",
            "        return columns, data, []",
            "",
            "    @classmethod",
            "    def alter_new_orm_column(cls, orm_col: TableColumn) -> None:",
            "        \"\"\"Allow altering default column attributes when first detected/added",
            "",
            "        For instance special column like `__time` for Druid can be",
            "        set to is_dttm=True. Note that this only gets called when new",
            "        columns are detected/created\"\"\"",
            "        # TODO: Fix circular import caused by importing TableColumn",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (seconds) to datetime that can be used in a",
            "        query. The reference column should be denoted as `{col}` in the return",
            "        expression, e.g. \"FROM_UNIXTIME({col})\"",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        raise NotImplementedError()",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        \"\"\"",
            "        SQL expression that converts epoch (milliseconds) to datetime that can be used",
            "        in a query.",
            "",
            "        :return: SQL Expression",
            "        \"\"\"",
            "        return cls.epoch_to_dttm().replace(\"{col}\", \"({col}/1000)\")",
            "",
            "    @classmethod",
            "    def get_datatype(cls, type_code: Any) -> str | None:",
            "        \"\"\"",
            "        Change column type code from cursor description to string representation.",
            "",
            "        :param type_code: Type code from cursor description",
            "        :return: String representation of type code",
            "        \"\"\"",
            "        if isinstance(type_code, str) and type_code != \"\":",
            "            return type_code.upper()",
            "        return None",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        noop by default",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        return indexes",
            "",
            "    @classmethod",
            "    def extra_table_metadata(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        schema_name: str | None,",
            "    ) -> dict[str, Any]:",
            "        \"\"\"",
            "        Returns engine-specific table metadata",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name",
            "        :param schema_name: Schema name",
            "        :return: Engine-specific table metadata",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        return {}",
            "",
            "    @classmethod",
            "    def apply_limit_to_sql(",
            "        cls, sql: str, limit: int, database: Database, force: bool = False",
            "    ) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a LIMIT clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param database: Database instance",
            "        :return: SQL query with limit clause",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database",
            "        if cls.limit_method == LimitMethod.WRAP_SQL:",
            "            sql = sql.strip(\"\\t\\n ;\")",
            "            qry = (",
            "                select(\"*\")",
            "                .select_from(TextAsFrom(text(sql), [\"*\"]).alias(\"inner_qry\"))",
            "                .limit(limit)",
            "            )",
            "            return database.compile_sqla_query(qry)",
            "",
            "        if cls.limit_method == LimitMethod.FORCE_LIMIT:",
            "            parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "            sql = parsed_query.set_or_update_query_limit(limit, force=force)",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def apply_top_to_sql(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Alters the SQL statement to apply a TOP clause",
            "        :param limit: Maximum number of rows to be returned by the query",
            "        :param sql: SQL query",
            "        :return: SQL query with top clause",
            "        \"\"\"",
            "",
            "        cte = None",
            "        sql_remainder = None",
            "        sql = sql.strip(\" \\t\\n;\")",
            "        sql_statement = sqlparse.format(sql, strip_comments=True)",
            "        query_limit: int | None = sql_parse.extract_top_from_query(",
            "            sql_statement, cls.top_keywords",
            "        )",
            "        if not limit:",
            "            final_limit = query_limit",
            "        elif int(query_limit or 0) < limit and query_limit is not None:",
            "            final_limit = query_limit",
            "        else:",
            "            final_limit = limit",
            "        if not cls.allows_cte_in_subquery:",
            "            cte, sql_remainder = sql_parse.get_cte_remainder_query(sql_statement)",
            "        if cte:",
            "            str_statement = str(sql_remainder)",
            "            cte = cte + \"\\n\"",
            "        else:",
            "            cte = \"\"",
            "            str_statement = str(sql)",
            "        str_statement = str_statement.replace(\"\\n\", \" \").replace(\"\\r\", \"\")",
            "",
            "        tokens = str_statement.rstrip().split(\" \")",
            "        tokens = [token for token in tokens if token]",
            "        if cls.top_not_in_sql(str_statement):",
            "            selects = [",
            "                i",
            "                for i, word in enumerate(tokens)",
            "                if word.upper() in cls.select_keywords",
            "            ]",
            "            first_select = selects[0]",
            "            if tokens[first_select + 1].upper() == \"DISTINCT\":",
            "                first_select += 1",
            "",
            "            tokens.insert(first_select + 1, \"TOP\")",
            "            tokens.insert(first_select + 2, str(final_limit))",
            "",
            "        next_is_limit_token = False",
            "        new_tokens = []",
            "",
            "        for token in tokens:",
            "            if token in cls.top_keywords:",
            "                next_is_limit_token = True",
            "            elif next_is_limit_token:",
            "                if token.isdigit():",
            "                    token = str(final_limit)",
            "                    next_is_limit_token = False",
            "            new_tokens.append(token)",
            "        sql = \" \".join(new_tokens)",
            "        return cte + sql",
            "",
            "    @classmethod",
            "    def top_not_in_sql(cls, sql: str) -> bool:",
            "        for top_word in cls.top_keywords:",
            "            if top_word.upper() in sql.upper():",
            "                return False",
            "        return True",
            "",
            "    @classmethod",
            "    def get_limit_from_sql(cls, sql: str) -> int | None:",
            "        \"\"\"",
            "        Extract limit from SQL query",
            "",
            "        :param sql: SQL query",
            "        :return: Value of limit clause in query",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.limit",
            "",
            "    @classmethod",
            "    def set_or_update_query_limit(cls, sql: str, limit: int) -> str:",
            "        \"\"\"",
            "        Create a query based on original query but with new limit clause",
            "",
            "        :param sql: SQL query",
            "        :param limit: New limit to insert/replace into query",
            "        :return: Query with new limit",
            "        \"\"\"",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        return parsed_query.set_or_update_query_limit(limit)",
            "",
            "    @classmethod",
            "    def get_cte_query(cls, sql: str) -> str | None:",
            "        \"\"\"",
            "        Convert the input CTE based SQL to the SQL for virtual table conversion",
            "",
            "        :param sql: SQL query",
            "        :return: CTE with the main select query aliased as `__cte`",
            "",
            "        \"\"\"",
            "        if not cls.allows_cte_in_subquery:",
            "            stmt = sqlparse.parse(sql)[0]",
            "",
            "            # The first meaningful token for CTE will be with WITH",
            "            idx, token = stmt.token_next(-1, skip_ws=True, skip_cm=True)",
            "            if not (token and token.ttype == CTE):",
            "                return None",
            "            idx, token = stmt.token_next(idx)",
            "            idx = stmt.token_index(token) + 1",
            "",
            "            # extract rest of the SQLs after CTE",
            "            remainder = \"\".join(str(token) for token in stmt.tokens[idx:]).strip()",
            "            return f\"WITH {token.value},\\n{cls.cte_alias} AS (\\n{remainder}\\n)\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: Database,",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        For regular engines this calls the `pandas.DataFrame.to_sql` method. Can be",
            "        overridden for engines that don't work well with this method, e.g. Hive and",
            "        BigQuery.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "",
            "        to_sql_kwargs[\"name\"] = table.table",
            "",
            "        if table.schema:",
            "            # Only add schema when it is preset and non-empty.",
            "            to_sql_kwargs[\"schema\"] = table.schema",
            "",
            "        with cls.get_engine(database) as engine:",
            "            if engine.dialect.supports_multivalues_insert:",
            "                to_sql_kwargs[\"method\"] = \"multi\"",
            "",
            "            df.to_sql(con=engine, **to_sql_kwargs)",
            "",
            "    @classmethod",
            "    def convert_dttm(  # pylint: disable=unused-argument",
            "        cls, target_type: str, dttm: datetime, db_extra: dict[str, Any] | None = None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Convert a Python `datetime` object to a SQL expression.",
            "",
            "        :param target_type: The target type of expression",
            "        :param dttm: The datetime object",
            "        :param db_extra: The database extra object",
            "        :return: The SQL expression",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def handle_cursor(cls, cursor: Any, query: Query, session: Session) -> None:",
            "        \"\"\"Handle a live cursor between the execute and fetchall calls",
            "",
            "        The flow works without this method doing anything, but it allows",
            "        for handling the cursor and updating progress information in the",
            "        query object\"\"\"",
            "        # TODO: Fix circular import error caused by importing sql_lab.Query",
            "",
            "    @classmethod",
            "    def execute_with_cursor(",
            "        cls, cursor: Any, sql: str, query: Query, session: Session",
            "    ) -> None:",
            "        \"\"\"",
            "        Trigger execution of a query and handle the resulting cursor.",
            "",
            "        For most implementations this just makes calls to `execute` and",
            "        `handle_cursor` consecutively, but in some engines (e.g. Trino) we may",
            "        need to handle client limitations such as lack of async support and",
            "        perform a more complicated operation to get information from the cursor",
            "        in a timely manner and facilitate operations such as query stop",
            "        \"\"\"",
            "        logger.debug(\"Query %d: Running query: %s\", query.id, sql)",
            "        cls.execute(cursor, sql, async_=True)",
            "        logger.debug(\"Query %d: Handling cursor\", query.id)",
            "        cls.handle_cursor(cursor, query, session)",
            "",
            "    @classmethod",
            "    def extract_error_message(cls, ex: Exception) -> str:",
            "        return f\"{cls.engine} error: {cls._extract_error_message(ex)}\"",
            "",
            "    @classmethod",
            "    def _extract_error_message(cls, ex: Exception) -> str:",
            "        \"\"\"Extract error message for queries\"\"\"",
            "        return utils.error_msg_from_exception(ex)",
            "",
            "    @classmethod",
            "    def extract_errors(",
            "        cls, ex: Exception, context: dict[str, Any] | None = None",
            "    ) -> list[SupersetError]:",
            "        raw_message = cls._extract_error_message(ex)",
            "",
            "        context = context or {}",
            "        for regex, (message, error_type, extra) in cls.custom_errors.items():",
            "            if match := regex.search(raw_message):",
            "                params = {**context, **match.groupdict()}",
            "                extra[\"engine_name\"] = cls.engine_name",
            "                return [",
            "                    SupersetError(",
            "                        error_type=error_type,",
            "                        message=message % params,",
            "                        level=ErrorLevel.ERROR,",
            "                        extra=extra,",
            "                    )",
            "                ]",
            "",
            "        return [",
            "            SupersetError(",
            "                error_type=SupersetErrorType.GENERIC_DB_ENGINE_ERROR,",
            "                message=cls._extract_error_message(ex),",
            "                level=ErrorLevel.ERROR,",
            "                extra={\"engine_name\": cls.engine_name},",
            "            )",
            "        ]",
            "",
            "    @classmethod",
            "    def adjust_engine_params(  # pylint: disable=unused-argument",
            "        cls,",
            "        uri: URL,",
            "        connect_args: dict[str, Any],",
            "        catalog: str | None = None,",
            "        schema: str | None = None,",
            "    ) -> tuple[URL, dict[str, Any]]:",
            "        \"\"\"",
            "        Return a new URL and ``connect_args`` for a specific catalog/schema.",
            "",
            "        This is used in SQL Lab, allowing users to select a schema from the list of",
            "        schemas available in a given database, and have the query run with that schema as",
            "        the default one.",
            "",
            "        For some databases (like MySQL, Presto, Snowflake) this requires modifying the",
            "        SQLAlchemy URI before creating the connection. For others (like Postgres), it",
            "        requires additional parameters in ``connect_args`` or running pre-session",
            "        queries with ``set`` parameters.",
            "",
            "        When a DB engine spec implements this method or ``get_prequeries`` (see below) it",
            "        should also have the attribute ``supports_dynamic_schema`` set to true, so that",
            "        Superset knows in which schema a given query is running in order to enforce",
            "        permissions (see #23385 and #23401).",
            "",
            "        Currently, changing the catalog is not supported. The method accepts a catalog so",
            "        that when catalog support is added to Superset the interface remains the same.",
            "        This is important because DB engine specs can be installed from 3rd party",
            "        packages, so we want to keep these methods as stable as possible.",
            "        \"\"\"",
            "        return uri, {",
            "            **connect_args,",
            "            **cls.enforce_uri_query_params.get(uri.get_driver_name(), {}),",
            "        }",
            "",
            "    @classmethod",
            "    def get_prequeries(",
            "        cls,",
            "        catalog: str | None = None,  # pylint: disable=unused-argument",
            "        schema: str | None = None,  # pylint: disable=unused-argument",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return pre-session queries.",
            "",
            "        These are currently used as an alternative to ``adjust_engine_params`` for",
            "        databases where the selected schema cannot be specified in the SQLAlchemy URI or",
            "        connection arguments.",
            "",
            "        For example, in order to specify a default schema in RDS we need to run a query",
            "        at the beginning of the session:",
            "",
            "            sql> set search_path = my_schema;",
            "",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def patch(cls) -> None:",
            "        \"\"\"",
            "        TODO: Improve docstring and refactor implementation in Hive",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def get_catalog_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs from database.",
            "",
            "        This needs to be implemented per database, since SQLAlchemy doesn't offer an",
            "        abstraction.",
            "        \"\"\"",
            "        return []",
            "",
            "    @classmethod",
            "    def get_schema_names(cls, inspector: Inspector) -> list[str]:",
            "        \"\"\"",
            "        Get all schemas from database",
            "",
            "        :param inspector: SqlAlchemy inspector",
            "        :return: All schemas in the database",
            "        \"\"\"",
            "        return sorted(inspector.get_schema_names())",
            "",
            "    @classmethod",
            "    def get_table_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the real table names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The physical table names",
            "        \"\"\"",
            "",
            "        try:",
            "            tables = set(inspector.get_table_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            tables = {re.sub(f\"^{schema}\\\\.\", \"\", table) for table in tables}",
            "        return tables",
            "",
            "    @classmethod",
            "    def get_view_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        schema: str | None,",
            "    ) -> set[str]:",
            "        \"\"\"",
            "        Get all the view names within the specified schema.",
            "",
            "        Per the SQLAlchemy definition if the schema is omitted the database\u2019s default",
            "        schema is used, however some dialects infer the request as schema agnostic.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param schema: The schema to inspect",
            "        :returns: The view names",
            "        \"\"\"",
            "",
            "        try:",
            "            views = set(inspector.get_view_names(schema))",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "        if schema and cls.try_remove_schema_from_table_name:",
            "            views = {re.sub(f\"^{schema}\\\\.\", \"\", view) for view in views}",
            "        return views",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: Database,  # pylint: disable=unused-argument",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return inspector.get_indexes(table_name, schema)",
            "",
            "    @classmethod",
            "    def get_table_comment(",
            "        cls, inspector: Inspector, table_name: str, schema: str | None",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Get comment of table from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :return: comment of table",
            "        \"\"\"",
            "        comment = None",
            "        try:",
            "            comment = inspector.get_table_comment(table_name, schema)",
            "            comment = comment.get(\"text\") if isinstance(comment, dict) else None",
            "        except NotImplementedError:",
            "            # It's expected that some dialects don't implement the comment method",
            "            pass",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            logger.error(\"Unexpected error while fetching table comment\", exc_info=True)",
            "            logger.exception(ex)",
            "        return comment",
            "",
            "    @classmethod",
            "    def get_columns(  # pylint: disable=unused-argument",
            "        cls,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "        options: dict[str, Any] | None = None,",
            "    ) -> list[ResultSetColumnType]:",
            "        \"\"\"",
            "        Get all columns from a given schema and table",
            "",
            "        :param inspector: SqlAlchemy Inspector instance",
            "        :param table_name: Table name",
            "        :param schema: Schema name. If omitted, uses default schema for database",
            "        :param options: Extra options to customise the display of columns in",
            "                        some databases",
            "        :return: All columns in table",
            "        \"\"\"",
            "        return convert_inspector_columns(",
            "            cast(list[SQLAColumnType], inspector.get_columns(table_name, schema))",
            "        )",
            "",
            "    @classmethod",
            "    def get_metrics(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: str | None,",
            "    ) -> list[MetricType]:",
            "        \"\"\"",
            "        Get all metrics from a given schema and table.",
            "        \"\"\"",
            "        return [",
            "            {",
            "                \"metric_name\": \"count\",",
            "                \"verbose_name\": \"COUNT(*)\",",
            "                \"metric_type\": \"count\",",
            "                \"expression\": \"COUNT(*)\",",
            "            }",
            "        ]",
            "",
            "    @classmethod",
            "    def where_latest_partition(  # pylint: disable=too-many-arguments,unused-argument",
            "        cls,",
            "        table_name: str,",
            "        schema: str | None,",
            "        database: Database,",
            "        query: Select,",
            "        columns: list[ResultSetColumnType] | None = None,",
            "    ) -> Select | None:",
            "        \"\"\"",
            "        Add a where clause to a query to reference only the most recent partition",
            "",
            "        :param table_name: Table name",
            "        :param schema: Schema name",
            "        :param database: Database instance",
            "        :param query: SqlAlchemy query",
            "        :param columns: List of TableColumns",
            "        :return: SqlAlchemy query with additional where clause referencing the latest",
            "        partition",
            "        \"\"\"",
            "        # TODO: Fix circular import caused by importing Database, TableColumn",
            "        return None",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        return [",
            "            literal_column(query_as)",
            "            if (query_as := c.get(\"query_as\"))",
            "            else column(c[\"column_name\"])",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments,too-many-locals",
            "        cls,",
            "        database: Database,",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: str | None = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: list[ResultSetColumnType] | None = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Generate a \"SELECT * from [schema.]table_name\" query with appropriate limit.",
            "",
            "        WARNING: expects only unquoted table and schema names.",
            "",
            "        :param database: Database instance",
            "        :param table_name: Table name, unquoted",
            "        :param engine: SqlAlchemy Engine instance",
            "        :param schema: Schema, unquoted",
            "        :param limit: limit to impose on query",
            "        :param show_cols: Show columns in query; otherwise use \"*\"",
            "        :param indent: Add indentation to query",
            "        :param latest_partition: Only query the latest partition",
            "        :param cols: Columns to include in query",
            "        :return: SQL query",
            "        \"\"\"",
            "        # pylint: disable=redefined-outer-name",
            "        fields: str | list[Any] = \"*\"",
            "        cols = cols or []",
            "        if (show_cols or latest_partition) and not cols:",
            "            cols = database.get_columns(table_name, schema)",
            "",
            "        if show_cols:",
            "            fields = cls._get_fields(cols)",
            "        quote = engine.dialect.identifier_preparer.quote",
            "        quote_schema = engine.dialect.identifier_preparer.quote_schema",
            "        if schema:",
            "            full_table_name = quote_schema(schema) + \".\" + quote(table_name)",
            "        else:",
            "            full_table_name = quote(table_name)",
            "",
            "        qry = select(fields).select_from(text(full_table_name))",
            "",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "        if latest_partition:",
            "            partition_query = cls.where_latest_partition(",
            "                table_name, schema, database, qry, columns=cols",
            "            )",
            "            if partition_query is not None:",
            "                qry = partition_query",
            "        sql = database.compile_sqla_query(qry)",
            "        if indent:",
            "            sql = sqlparse.format(sql, reindent=True)",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        \"\"\"",
            "        Generate a SQL query that estimates the cost of a given statement.",
            "",
            "        :param statement: A single SQL statement",
            "        :param cursor: Cursor instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        \"\"\"",
            "        Format cost estimate.",
            "",
            "        :param raw_cost: Raw estimate from `estimate_query_cost`",
            "        :return: Human readable cost estimate",
            "        \"\"\"",
            "        raise Exception(  # pylint: disable=broad-exception-raised",
            "            \"Database does not support cost estimation\"",
            "        )",
            "",
            "    @classmethod",
            "    def process_statement(cls, statement: str, database: Database) -> str:",
            "        \"\"\"",
            "        Process a SQL statement by stripping and mutating it.",
            "",
            "        :param statement: A single SQL statement",
            "        :param database: Database instance",
            "        :return: Dictionary with different costs",
            "        \"\"\"",
            "        parsed_query = ParsedQuery(statement, engine=cls.engine)",
            "        sql = parsed_query.stripped()",
            "        sql_query_mutator = current_app.config[\"SQL_QUERY_MUTATOR\"]",
            "        mutate_after_split = current_app.config[\"MUTATE_AFTER_SPLIT\"]",
            "        if sql_query_mutator and not mutate_after_split:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=database,",
            "            )",
            "",
            "        return sql",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: Database,",
            "        schema: str,",
            "        sql: str,",
            "        source: utils.QuerySource | None = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise Exception(  # pylint: disable=broad-exception-raised",
            "                \"Database does not support cost estimation\"",
            "            )",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        statements = parsed_query.get_statements()",
            "",
            "        costs = []",
            "        with database.get_raw_connection(schema=schema, source=source) as conn:",
            "            cursor = conn.cursor()",
            "            for statement in statements:",
            "                processed_statement = cls.process_statement(statement, database)",
            "                costs.append(cls.estimate_statement_cost(processed_statement, cursor))",
            "",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_url_for_impersonation(",
            "        cls, url: URL, impersonate_user: bool, username: str | None",
            "    ) -> URL:",
            "        \"\"\"",
            "        Return a modified URL with the username set.",
            "",
            "        :param url: SQLAlchemy URL object",
            "        :param impersonate_user: Flag indicating if impersonation is enabled",
            "        :param username: Effective username",
            "        \"\"\"",
            "        if impersonate_user and username is not None:",
            "            url = url.set(username=username)",
            "",
            "        return url",
            "",
            "    @classmethod",
            "    def update_impersonation_config(",
            "        cls,",
            "        connect_args: dict[str, Any],",
            "        uri: str,",
            "        username: str | None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Update a configuration dictionary",
            "        that can set the correct properties for impersonating users",
            "",
            "        :param connect_args: config to be updated",
            "        :param uri: URI",
            "        :param username: Effective username",
            "        :return: None",
            "        \"\"\"",
            "",
            "    @classmethod",
            "    def execute(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: str,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        \"\"\"",
            "        Execute a SQL query",
            "",
            "        :param cursor: Cursor instance",
            "        :param query: Query to execute",
            "        :param kwargs: kwargs to be passed to cursor.execute()",
            "        :return:",
            "        \"\"\"",
            "        if not cls.allows_sql_comments:",
            "            query = sql_parse.strip_comments_from_sql(query, engine=cls.engine)",
            "",
            "        if cls.arraysize:",
            "            cursor.arraysize = cls.arraysize",
            "        try:",
            "            cursor.execute(query)",
            "        except Exception as ex:",
            "            raise cls.get_dbapi_mapped_exception(ex) from ex",
            "",
            "    @classmethod",
            "    def make_label_compatible(cls, label: str) -> str | quoted_name:",
            "        \"\"\"",
            "        Conditionally mutate and/or quote a sqlalchemy expression label. If",
            "        force_column_alias_quotes is set to True, return the label as a",
            "        sqlalchemy.sql.elements.quoted_name object to ensure that the select query",
            "        and query results have same case. Otherwise, return the mutated label as a",
            "        regular string. If maximum supported column name length is exceeded,",
            "        generate a truncated label by calling truncate_label().",
            "",
            "        :param label: expected expression label/alias",
            "        :return: conditionally mutated label supported by the db engine",
            "        \"\"\"",
            "        label_mutated = cls._mutate_label(label)",
            "        if (",
            "            cls.max_column_name_length",
            "            and len(label_mutated) > cls.max_column_name_length",
            "        ):",
            "            label_mutated = cls._truncate_label(label)",
            "        if cls.force_column_alias_quotes:",
            "            label_mutated = quoted_name(label_mutated, True)",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def get_column_types(",
            "        cls,",
            "        column_type: str | None,",
            "    ) -> tuple[TypeEngine, GenericDataType] | None:",
            "        \"\"\"",
            "        Return a sqlalchemy native column type and generic data type that corresponds",
            "        to the column type defined in the data source (return None to use default type",
            "        inferred by SQLAlchemy). Override `column_type_mappings` for specific needs",
            "        (see MSSQL for example of NCHAR/NVARCHAR handling).",
            "",
            "        :param column_type: Column type returned by inspector",
            "        :return: SQLAlchemy and generic Superset column types",
            "        \"\"\"",
            "        if not column_type:",
            "            return None",
            "",
            "        for regex, sqla_type, generic_type in (",
            "            cls.column_type_mappings + cls._default_column_type_mappings",
            "        ):",
            "            match = regex.match(column_type)",
            "            if not match:",
            "                continue",
            "            if callable(sqla_type):",
            "                return sqla_type(match), generic_type",
            "            return sqla_type, generic_type",
            "        return None",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        Most engines support mixed case aliases that can include numbers",
            "        and special characters, like commas, parentheses etc. For engines that",
            "        have restrictions on what types of aliases are supported, this method",
            "        can be overridden to ensure that labels conform to the engine's",
            "        limitations. Mutated labels should be deterministic (input label A always",
            "        yields output label X) and unique (input labels A and B don't yield the same",
            "        output label X).",
            "",
            "        :param label: Preferred expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        return label",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"",
            "        In the case that a label exceeds the max length supported by the engine,",
            "        this method is used to construct a deterministic and unique label based on",
            "        the original label. By default, this returns a md5 hash of the original label,",
            "        conditionally truncated if the length of the hash exceeds the max column length",
            "        of the engine.",
            "",
            "        :param label: Expected expression label",
            "        :return: Truncated label",
            "        \"\"\"",
            "        label = md5_sha_from_str(label)",
            "        # truncate hash if it exceeds max length",
            "        if cls.max_column_name_length and len(label) > cls.max_column_name_length:",
            "            label = label[: cls.max_column_name_length]",
            "        return label",
            "",
            "    @classmethod",
            "    def column_datatype_to_string(",
            "        cls, sqla_column_type: TypeEngine, dialect: Dialect",
            "    ) -> str:",
            "        \"\"\"",
            "        Convert sqlalchemy column type to string representation.",
            "        By default, removes collation and character encoding info to avoid",
            "        unnecessarily long datatypes.",
            "",
            "        :param sqla_column_type: SqlAlchemy column type",
            "        :param dialect: Sqlalchemy dialect",
            "        :return: Compiled column type",
            "        \"\"\"",
            "        sqla_column_type = sqla_column_type.copy()",
            "        if hasattr(sqla_column_type, \"collation\"):",
            "            sqla_column_type.collation = None",
            "        if hasattr(sqla_column_type, \"charset\"):",
            "            sqla_column_type.charset = None",
            "        return sqla_column_type.compile(dialect=dialect).upper()",
            "",
            "    @classmethod",
            "    def get_function_names(  # pylint: disable=unused-argument",
            "        cls,",
            "        database: Database,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get a list of function names that are able to be called on the database.",
            "        Used for SQL Lab autocomplete.",
            "",
            "        :param database: The database to get functions for",
            "        :return: A list of function names useable in the database",
            "        \"\"\"",
            "        return []",
            "",
            "    @staticmethod",
            "    def pyodbc_rows_to_tuples(data: list[Any]) -> list[tuple[Any, ...]]:",
            "        \"\"\"",
            "        Convert pyodbc.Row objects from `fetch_data` to tuples.",
            "",
            "        :param data: List of tuples or pyodbc.Row objects",
            "        :return: List of tuples",
            "        \"\"\"",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [tuple(row) for row in data]",
            "        return data",
            "",
            "    @staticmethod",
            "    def mutate_db_for_connection_test(  # pylint: disable=unused-argument",
            "        database: Database,",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require passing additional parameters for validating database",
            "        connections. This method makes it possible to mutate the database instance prior",
            "        to testing if a connection is ok.",
            "",
            "        :param database: instance to be mutated",
            "        \"\"\"",
            "        return None",
            "",
            "    @staticmethod",
            "    def get_extra_params(database: Database) -> dict[str, Any]:",
            "        \"\"\"",
            "        Some databases require adding elements to connection parameters,",
            "        like passing certificates to `extra`. This can be done here.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :raises CertificateException: If certificate is not valid/unparseable",
            "        \"\"\"",
            "        extra: dict[str, Any] = {}",
            "        if database.extra:",
            "            try:",
            "                extra = json.loads(database.extra)",
            "            except json.JSONDecodeError as ex:",
            "                logger.error(ex, exc_info=True)",
            "                raise ex",
            "        return extra",
            "",
            "    @staticmethod",
            "    def update_params_from_encrypted_extra(  # pylint: disable=invalid-name",
            "        database: Database, params: dict[str, Any]",
            "    ) -> None:",
            "        \"\"\"",
            "        Some databases require some sensitive information which do not conform to",
            "        the username:password syntax normally used by SQLAlchemy.",
            "",
            "        :param database: database instance from which to extract extras",
            "        :param params: params to be updated",
            "        \"\"\"",
            "        if not database.encrypted_extra:",
            "            return",
            "        try:",
            "            encrypted_extra = json.loads(database.encrypted_extra)",
            "            params.update(encrypted_extra)",
            "        except json.JSONDecodeError as ex:",
            "            logger.error(ex, exc_info=True)",
            "            raise ex",
            "",
            "    @classmethod",
            "    def is_readonly_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"Pessimistic readonly, 100% sure statement won't mutate anything\"\"\"",
            "        return (",
            "            parsed_query.is_select()",
            "            or parsed_query.is_explain()",
            "            or parsed_query.is_show()",
            "        )",
            "",
            "    @classmethod",
            "    def is_select_query(cls, parsed_query: ParsedQuery) -> bool:",
            "        \"\"\"",
            "        Determine if the statement should be considered as SELECT statement.",
            "        Some query dialects do not contain \"SELECT\" word in queries (eg. Kusto)",
            "        \"\"\"",
            "        return parsed_query.is_select()",
            "",
            "    @classmethod",
            "    def get_column_spec(  # pylint: disable=unused-argument",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> ColumnSpec | None:",
            "        \"\"\"",
            "        Get generic type related specs regarding a native column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        if col_types := cls.get_column_types(native_type):",
            "            column_type, generic_type = col_types",
            "            is_dttm = generic_type == GenericDataType.TEMPORAL",
            "            return ColumnSpec(",
            "                sqla_type=column_type, generic_type=generic_type, is_dttm=is_dttm",
            "            )",
            "        return None",
            "",
            "    @classmethod",
            "    def get_sqla_column_type(",
            "        cls,",
            "        native_type: str | None,",
            "        db_extra: dict[str, Any] | None = None,",
            "        source: utils.ColumnTypeSource = utils.ColumnTypeSource.GET_TABLE,",
            "    ) -> TypeEngine | None:",
            "        \"\"\"",
            "        Converts native database type to sqlalchemy column type.",
            "",
            "        :param native_type: Native database type",
            "        :param db_extra: The database extra object",
            "        :param source: Type coming from the database table or cursor description",
            "        :return: ColumnSpec object",
            "        \"\"\"",
            "        column_spec = cls.get_column_spec(",
            "            native_type=native_type,",
            "            db_extra=db_extra,",
            "            source=source,",
            "        )",
            "        return column_spec.sqla_type if column_spec else None",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def prepare_cancel_query(cls, query: Query, session: Session) -> None:",
            "        \"\"\"",
            "        Some databases may acquire the query cancelation id after the query",
            "        cancelation request has been received. For those cases, the db engine spec",
            "        can record the cancelation intent so that the query can either be stopped",
            "        prior to execution, or canceled once the query id is acquired.",
            "        \"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def has_implicit_cancel(cls) -> bool:",
            "        \"\"\"",
            "        Return True if the live cursor handles the implicit cancelation of the query,",
            "        False otherwise.",
            "",
            "        :return: Whether the live cursor implicitly cancels the query",
            "        :see: handle_cursor",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def get_cancel_query_id(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "    ) -> str | None:",
            "        \"\"\"",
            "        Select identifiers from the database engine that uniquely identifies the",
            "        queries to cancel. The identifier is typically a session id, process id",
            "        or similar.",
            "",
            "        :param cursor: Cursor instance in which the query will be executed",
            "        :param query: Query instance",
            "        :return: Query identifier",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "    @classmethod",
            "    def cancel_query(  # pylint: disable=unused-argument",
            "        cls,",
            "        cursor: Any,",
            "        query: Query,",
            "        cancel_query_id: str,",
            "    ) -> bool:",
            "        \"\"\"",
            "        Cancel query in the underlying database.",
            "",
            "        :param cursor: New cursor instance to the db of the query",
            "        :param query: Query instance",
            "        :param cancel_query_id: Value returned by get_cancel_query_payload or set in",
            "        other life-cycle methods of the query",
            "        :return: True if query cancelled successfully, False otherwise",
            "        \"\"\"",
            "",
            "        return False",
            "",
            "    @classmethod",
            "    def parse_sql(cls, sql: str) -> list[str]:",
            "        return [str(s).strip(\" ;\") for s in sqlparse.parse(sql)]",
            "",
            "    @classmethod",
            "    def get_impersonation_key(cls, user: User | None) -> Any:",
            "        \"\"\"",
            "        Construct an impersonation key, by default it's the given username.",
            "",
            "        :param user: logged-in user",
            "",
            "        :returns: username if given user is not null",
            "        \"\"\"",
            "        return user.username if user else None",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: str | None) -> str | None:",
            "        \"\"\"",
            "        Mask ``encrypted_extra``.",
            "",
            "        This is used to remove any sensitive data in ``encrypted_extra`` when presenting",
            "        it to the user. For example, a private key might be replaced with a masked value",
            "        \"XXXXXXXXXX\". If the masked value is changed the corresponding entry is updated,",
            "        otherwise the old value is used (see ``unmask_encrypted_extra`` below).",
            "        \"\"\"",
            "        return encrypted_extra",
            "",
            "    # pylint: disable=unused-argument",
            "    @classmethod",
            "    def unmask_encrypted_extra(cls, old: str | None, new: str | None) -> str | None:",
            "        \"\"\"",
            "        Remove masks from ``encrypted_extra``.",
            "",
            "        This method allows reusing existing values from the current encrypted extra on",
            "        updates. It's useful for reusing masked passwords, allowing keys to be updated",
            "        without having to provide sensitive data to the client.",
            "        \"\"\"",
            "        return new",
            "",
            "    @classmethod",
            "    def get_public_information(cls) -> dict[str, Any]:",
            "        \"\"\"",
            "        Construct a Dict with properties we want to expose.",
            "",
            "        :returns: Dict with properties of our class like supports_file_upload",
            "        and disable_ssh_tunneling",
            "        \"\"\"",
            "        return {",
            "            \"supports_file_upload\": cls.supports_file_upload,",
            "            \"disable_ssh_tunneling\": cls.disable_ssh_tunneling,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_database_uri(cls, sqlalchemy_uri: URL) -> None:",
            "        \"\"\"",
            "        Validates a database SQLAlchemy URI per engine spec.",
            "        Use this to implement a final validation for unwanted connection configuration",
            "",
            "        :param sqlalchemy_uri:",
            "        \"\"\"",
            "        if existing_disallowed := cls.disallow_uri_query_params.get(",
            "            sqlalchemy_uri.get_driver_name(), set()",
            "        ).intersection(sqlalchemy_uri.query):",
            "            raise ValueError(f\"Forbidden query parameter(s): {existing_disallowed}\")",
            "",
            "    @classmethod",
            "    def denormalize_name(cls, dialect: Dialect, name: str) -> str:",
            "        if (",
            "            hasattr(dialect, \"requires_name_normalize\")",
            "            and dialect.requires_name_normalize",
            "        ):",
            "            return dialect.denormalize_name(name)",
            "",
            "        return name",
            "",
            "",
            "# schema for adding a database by providing parameters instead of the",
            "# full SQLAlchemy URI",
            "class BasicParametersSchema(Schema):",
            "    username = fields.String(",
            "        required=True, allow_none=True, metadata={\"description\": __(\"Username\")}",
            "    )",
            "    password = fields.String(allow_none=True, metadata={\"description\": __(\"Password\")})",
            "    host = fields.String(",
            "        required=True, metadata={\"description\": __(\"Hostname or IP address\")}",
            "    )",
            "    port = fields.Integer(",
            "        required=True,",
            "        metadata={\"description\": __(\"Database port\")},",
            "        validate=Range(min=0, max=2**16, max_inclusive=False),",
            "    )",
            "    database = fields.String(",
            "        required=True, metadata={\"description\": __(\"Database name\")}",
            "    )",
            "    query = fields.Dict(",
            "        keys=fields.Str(),",
            "        values=fields.Raw(),",
            "        metadata={\"description\": __(\"Additional parameters\")},",
            "    )",
            "    encryption = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an encrypted connection to the database\")},",
            "    )",
            "    ssh = fields.Boolean(",
            "        required=False,",
            "        metadata={\"description\": __(\"Use an ssh tunnel connection to the database\")},",
            "    )",
            "",
            "",
            "class BasicParametersType(TypedDict, total=False):",
            "    username: str | None",
            "    password: str | None",
            "    host: str",
            "    port: int",
            "    database: str",
            "    query: dict[str, Any]",
            "    encryption: bool",
            "",
            "",
            "class BasicPropertiesType(TypedDict):",
            "    parameters: BasicParametersType",
            "",
            "",
            "class BasicParametersMixin:",
            "    \"\"\"",
            "    Mixin for configuring DB engine specs via a dictionary.",
            "",
            "    With this mixin the SQLAlchemy engine can be configured through",
            "    individual parameters, instead of the full SQLAlchemy URI. This",
            "    mixin is for the most common pattern of URI:",
            "",
            "        engine+driver://user:password@host:port/dbname[?key=value&key=value...]",
            "",
            "    \"\"\"",
            "",
            "    # schema describing the parameters used to configure the DB",
            "    parameters_schema = BasicParametersSchema()",
            "",
            "    # recommended driver name for the DB engine spec",
            "    default_driver = \"\"",
            "",
            "    # query parameter to enable encryption in the database connection",
            "    # for Postgres this would be `{\"sslmode\": \"verify-ca\"}`, eg.",
            "    encryption_parameters: dict[str, str] = {}",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(  # pylint: disable=unused-argument",
            "        cls,",
            "        parameters: BasicParametersType,",
            "        encrypted_extra: dict[str, str] | None = None,",
            "    ) -> str:",
            "        # make a copy so that we don't update the original",
            "        query = parameters.get(\"query\", {}).copy()",
            "        if parameters.get(\"encryption\"):",
            "            if not cls.encryption_parameters:",
            "                raise Exception(  # pylint: disable=broad-exception-raised",
            "                    \"Unable to build a URL with encryption enabled\"",
            "                )",
            "            query.update(cls.encryption_parameters)",
            "",
            "        return str(",
            "            URL.create(",
            "                f\"{cls.engine}+{cls.default_driver}\".rstrip(\"+\"),  # type: ignore",
            "                username=parameters.get(\"username\"),",
            "                password=parameters.get(\"password\"),",
            "                host=parameters[\"host\"],",
            "                port=parameters[\"port\"],",
            "                database=parameters[\"database\"],",
            "                query=query,",
            "            )",
            "        )",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(  # pylint: disable=unused-argument",
            "        cls, uri: str, encrypted_extra: dict[str, Any] | None = None",
            "    ) -> BasicParametersType:",
            "        url = make_url_safe(uri)",
            "        query = {",
            "            key: value",
            "            for (key, value) in url.query.items()",
            "            if (key, value) not in cls.encryption_parameters.items()",
            "        }",
            "        encryption = all(",
            "            item in url.query.items() for item in cls.encryption_parameters.items()",
            "        )",
            "        return {",
            "            \"username\": url.username,",
            "            \"password\": url.password,",
            "            \"host\": url.host,",
            "            \"port\": url.port,",
            "            \"database\": url.database,",
            "            \"query\": query,",
            "            \"encryption\": encryption,",
            "        }",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls, properties: BasicPropertiesType",
            "    ) -> list[SupersetError]:",
            "        \"\"\"",
            "        Validates any number of parameters, for progressive validation.",
            "",
            "        If only the hostname is present it will check if the name is resolvable. As more",
            "        parameters are present in the request, more validation is done.",
            "        \"\"\"",
            "        errors: list[SupersetError] = []",
            "",
            "        required = {\"host\", \"port\", \"username\", \"database\"}",
            "        parameters = properties.get(\"parameters\", {})",
            "        present = {key for key in parameters if parameters.get(key, ())}",
            "",
            "        if missing := sorted(required - present):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=f'One or more parameters are missing: {\", \".join(missing)}',",
            "                    error_type=SupersetErrorType.CONNECTION_MISSING_PARAMETERS_ERROR,",
            "                    level=ErrorLevel.WARNING,",
            "                    extra={\"missing\": missing},",
            "                ),",
            "            )",
            "",
            "        host = parameters.get(\"host\", None)",
            "        if not host:",
            "            return errors",
            "        if not is_hostname_valid(host):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The hostname provided can't be resolved.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_HOSTNAME_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"host\"]},",
            "                ),",
            "            )",
            "            return errors",
            "",
            "        port = parameters.get(\"port\", None)",
            "        if not port:",
            "            return errors",
            "        try:",
            "            port = int(port)",
            "        except (ValueError, TypeError):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"Port must be a valid integer.\",",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        if not (isinstance(port, int) and 0 <= port < 2**16):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=(",
            "                        \"The port must be an integer between 0 and 65535 \"",
            "                        \"(inclusive).\"",
            "                    ),",
            "                    error_type=SupersetErrorType.CONNECTION_INVALID_PORT_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "        elif not is_port_open(host, port):",
            "            errors.append(",
            "                SupersetError(",
            "                    message=\"The port is closed.\",",
            "                    error_type=SupersetErrorType.CONNECTION_PORT_CLOSED_ERROR,",
            "                    level=ErrorLevel.ERROR,",
            "                    extra={\"invalid\": [\"port\"]},",
            "                ),",
            "            )",
            "",
            "        return errors",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.2\",",
            "            plugins=[MarshmallowPlugin()],",
            "        )",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "903": [
                "BaseEngineSpec",
                "apply_limit_to_sql"
            ],
            "984": [
                "BaseEngineSpec",
                "get_limit_from_sql"
            ],
            "996": [
                "BaseEngineSpec",
                "set_or_update_query_limit"
            ],
            "1493": [
                "BaseEngineSpec",
                "process_statement"
            ],
            "1528": [
                "BaseEngineSpec",
                "estimate_query_cost"
            ],
            "1589": [
                "BaseEngineSpec",
                "execute"
            ]
        },
        "addLocation": []
    },
    "superset/db_engine_specs/bigquery.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 435,
                "afterPatchRowNumber": 435,
                "PatchRowcode": "         if not cls.get_allow_cost_estimate(extra):"
            },
            "1": {
                "beforePatchRowNumber": 436,
                "afterPatchRowNumber": 436,
                "PatchRowcode": "             raise SupersetException(\"Database does not support cost estimation\")"
            },
            "2": {
                "beforePatchRowNumber": 437,
                "afterPatchRowNumber": 437,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 438,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        parsed_query = sql_parse.ParsedQuery(sql)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 438,
                "PatchRowcode": "+        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)"
            },
            "5": {
                "beforePatchRowNumber": 439,
                "afterPatchRowNumber": 439,
                "PatchRowcode": "         statements = parsed_query.get_statements()"
            },
            "6": {
                "beforePatchRowNumber": 440,
                "afterPatchRowNumber": 440,
                "PatchRowcode": "         costs = []"
            },
            "7": {
                "beforePatchRowNumber": 441,
                "afterPatchRowNumber": 441,
                "PatchRowcode": "         for statement in statements:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import contextlib",
            "import json",
            "import re",
            "import urllib",
            "from datetime import datetime",
            "from re import Pattern",
            "from typing import Any, Optional, TYPE_CHECKING, TypedDict",
            "",
            "import pandas as pd",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask_babel import gettext as __",
            "from marshmallow import fields, Schema",
            "from marshmallow.exceptions import ValidationError",
            "from sqlalchemy import column, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.sql import sqltypes",
            "",
            "from superset import sql_parse",
            "from superset.constants import PASSWORD_MASK, TimeGrain",
            "from superset.databases.schemas import encrypted_field_properties, EncryptedString",
            "from superset.databases.utils import make_url_safe",
            "from superset.db_engine_specs.base import BaseEngineSpec, BasicPropertiesType",
            "from superset.db_engine_specs.exceptions import SupersetDBAPIConnectionError",
            "from superset.errors import SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetException",
            "from superset.sql_parse import Table",
            "from superset.superset_typing import ResultSetColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.hashing import md5_sha_from_str",
            "",
            "try:",
            "    from google.cloud import bigquery",
            "    from google.oauth2 import service_account",
            "",
            "    dependencies_installed = True",
            "except ImportError:",
            "    dependencies_installed = False",
            "",
            "try:",
            "    import pandas_gbq",
            "",
            "    can_upload = True",
            "except ModuleNotFoundError:",
            "    can_upload = False",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database  # pragma: no cover",
            "",
            "CONNECTION_DATABASE_PERMISSIONS_REGEX = re.compile(",
            "    \"Access Denied: Project (?P<project_name>.+?): User does not have \"",
            "    + \"bigquery.jobs.create permission in project (?P<project>.+?)\"",
            ")",
            "",
            "TABLE_DOES_NOT_EXIST_REGEX = re.compile(",
            "    'Table name \"(?P<table>.*?)\" missing dataset while no default '",
            "    \"dataset is set in the request\"",
            ")",
            "",
            "COLUMN_DOES_NOT_EXIST_REGEX = re.compile(",
            "    r\"Unrecognized name: (?P<column>.*?) at \\[(?P<location>.+?)\\]\"",
            ")",
            "",
            "SCHEMA_DOES_NOT_EXIST_REGEX = re.compile(",
            "    r\"bigquery error: 404 Not found: Dataset (?P<dataset>.*?):\"",
            "    r\"(?P<schema>.*?) was not found in location\"",
            ")",
            "",
            "SYNTAX_ERROR_REGEX = re.compile(",
            "    'Syntax error: Expected end of input but got identifier \"(?P<syntax_error>.+?)\"'",
            ")",
            "",
            "ma_plugin = MarshmallowPlugin()",
            "",
            "",
            "class BigQueryParametersSchema(Schema):",
            "    credentials_info = EncryptedString(",
            "        required=False,",
            "        metadata={\"description\": \"Contents of BigQuery JSON credentials.\"},",
            "    )",
            "    query = fields.Dict(required=False)",
            "",
            "",
            "class BigQueryParametersType(TypedDict):",
            "    credentials_info: dict[str, Any]",
            "    query: dict[str, Any]",
            "",
            "",
            "class BigQueryEngineSpec(BaseEngineSpec):  # pylint: disable=too-many-public-methods",
            "    \"\"\"Engine spec for Google's BigQuery",
            "",
            "    As contributed by @mxmzdlv on issue #945\"\"\"",
            "",
            "    engine = \"bigquery\"",
            "    engine_name = \"Google BigQuery\"",
            "    max_column_name_length = 128",
            "    disable_ssh_tunneling = True",
            "",
            "    parameters_schema = BigQueryParametersSchema()",
            "    default_driver = \"bigquery\"",
            "    sqlalchemy_uri_placeholder = \"bigquery://{project_id}\"",
            "",
            "    # BigQuery doesn't maintain context when running multiple statements in the",
            "    # same cursor, so we need to run all statements at once",
            "    run_multiple_statements_as_one = True",
            "",
            "    allows_hidden_cc_in_orderby = True",
            "",
            "    supports_catalog = True",
            "",
            "    \"\"\"",
            "    https://www.python.org/dev/peps/pep-0249/#arraysize",
            "    raw_connections bypass the sqlalchemy-bigquery query execution context and deal with",
            "    raw dbapi connection directly.",
            "    If this value is not set, the default value is set to 1, as described here,",
            "    https://googlecloudplatform.github.io/google-cloud-python/latest/_modules/google/cloud/bigquery/dbapi/cursor.html#Cursor",
            "",
            "    The default value of 5000 is derived from the sqlalchemy-bigquery.",
            "    https://github.com/googleapis/python-bigquery-sqlalchemy/blob/4e17259088f89eac155adc19e0985278a29ecf9c/sqlalchemy_bigquery/base.py#L762",
            "    \"\"\"",
            "    arraysize = 5000",
            "",
            "    _date_trunc_functions = {",
            "        \"DATE\": \"DATE_TRUNC\",",
            "        \"DATETIME\": \"DATETIME_TRUNC\",",
            "        \"TIME\": \"TIME_TRUNC\",",
            "        \"TIMESTAMP\": \"TIMESTAMP_TRUNC\",",
            "    }",
            "",
            "    _time_grain_expressions = {",
            "        None: \"{col}\",",
            "        TimeGrain.SECOND: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"UNIX_SECONDS(CAST({col} AS TIMESTAMP))\"",
            "        \") AS {type})\",",
            "        TimeGrain.MINUTE: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.FIVE_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"5*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 5*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.TEN_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"10*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 10*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.FIFTEEN_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"15*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 15*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.THIRTY_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"30*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 30*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.HOUR: \"{func}({col}, HOUR)\",",
            "        TimeGrain.DAY: \"{func}({col}, DAY)\",",
            "        TimeGrain.WEEK: \"{func}({col}, WEEK)\",",
            "        TimeGrain.WEEK_STARTING_MONDAY: \"{func}({col}, ISOWEEK)\",",
            "        TimeGrain.MONTH: \"{func}({col}, MONTH)\",",
            "        TimeGrain.QUARTER: \"{func}({col}, QUARTER)\",",
            "        TimeGrain.YEAR: \"{func}({col}, YEAR)\",",
            "    }",
            "",
            "    custom_errors: dict[Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]] = {",
            "        CONNECTION_DATABASE_PERMISSIONS_REGEX: (",
            "            __(",
            "                \"Unable to connect. Verify that the following roles are set \"",
            "                'on the service account: \"BigQuery Data Viewer\", '",
            "                '\"BigQuery Metadata Viewer\", \"BigQuery Job User\" '",
            "                \"and the following permissions are set \"",
            "                '\"bigquery.readsessions.create\", '",
            "                '\"bigquery.readsessions.getData\"'",
            "            ),",
            "            SupersetErrorType.CONNECTION_DATABASE_PERMISSIONS_ERROR,",
            "            {},",
            "        ),",
            "        TABLE_DOES_NOT_EXIST_REGEX: (",
            "            __(",
            "                'The table \"%(table)s\" does not exist. '",
            "                \"A valid table must be used to run this query.\",",
            "            ),",
            "            SupersetErrorType.TABLE_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        COLUMN_DOES_NOT_EXIST_REGEX: (",
            "            __('We can\\'t seem to resolve column \"%(column)s\" at line %(location)s.'),",
            "            SupersetErrorType.COLUMN_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        SCHEMA_DOES_NOT_EXIST_REGEX: (",
            "            __(",
            "                'The schema \"%(schema)s\" does not exist. '",
            "                \"A valid schema must be used to run this query.\"",
            "            ),",
            "            SupersetErrorType.SCHEMA_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        SYNTAX_ERROR_REGEX: (",
            "            __(",
            "                \"Please check your query for syntax errors at or near \"",
            "                '\"%(syntax_error)s\". Then, try running your query again.'",
            "            ),",
            "            SupersetErrorType.SYNTAX_ERROR,",
            "            {},",
            "        ),",
            "    }",
            "",
            "    @classmethod",
            "    def convert_dttm(",
            "        cls, target_type: str, dttm: datetime, db_extra: Optional[dict[str, Any]] = None",
            "    ) -> Optional[str]:",
            "        sqla_type = cls.get_sqla_column_type(target_type)",
            "        if isinstance(sqla_type, types.Date):",
            "            return f\"CAST('{dttm.date().isoformat()}' AS DATE)\"",
            "        if isinstance(sqla_type, types.TIMESTAMP):",
            "            return f\"\"\"CAST('{dttm.isoformat(timespec=\"microseconds\")}' AS TIMESTAMP)\"\"\"",
            "        if isinstance(sqla_type, types.DateTime):",
            "            return f\"\"\"CAST('{dttm.isoformat(timespec=\"microseconds\")}' AS DATETIME)\"\"\"",
            "        if isinstance(sqla_type, types.Time):",
            "            return f\"\"\"CAST('{dttm.strftime(\"%H:%M:%S.%f\")}' AS TIME)\"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def fetch_data(",
            "        cls, cursor: Any, limit: Optional[int] = None",
            "    ) -> list[tuple[Any, ...]]:",
            "        data = super().fetch_data(cursor, limit)",
            "        # Support type BigQuery Row, introduced here PR #4071",
            "        # google.cloud.bigquery.table.Row",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [r.values() for r in data]  # type: ignore",
            "        return data",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        BigQuery field_name should start with a letter or underscore and contain only",
            "        alphanumeric characters. Labels that start with a number are prefixed with an",
            "        underscore. Any unsupported characters are replaced with underscores and an",
            "        md5 hash is added to the end of the label to avoid possible collisions.",
            "",
            "        :param label: Expected expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        label_hashed = \"_\" + md5_sha_from_str(label)",
            "",
            "        # if label starts with number, add underscore as first character",
            "        label_mutated = \"_\" + label if re.match(r\"^\\d\", label) else label",
            "",
            "        # replace non-alphanumeric characters with underscores",
            "        label_mutated = re.sub(r\"[^\\w]+\", \"_\", label_mutated)",
            "        if label_mutated != label:",
            "            # add first 5 chars from md5 hash to label to avoid possible collisions",
            "            label_mutated += label_hashed[:6]",
            "",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"BigQuery requires column names start with either a letter or",
            "        underscore. To make sure this is always the case, an underscore is prefixed",
            "        to the md5 hash of the original label.",
            "",
            "        :param label: expected expression label",
            "        :return: truncated label",
            "        \"\"\"",
            "        return \"_\" + md5_sha_from_str(label)",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        normalized_idxs = []",
            "        # Fixing a bug/behavior observed in pybigquery==0.4.15 where",
            "        # the index's `column_names` == [None]",
            "        # Here we're returning only non-None indexes",
            "        for ix in indexes:",
            "            column_names = ix.get(\"column_names\") or []",
            "            ix[\"column_names\"] = [col for col in column_names if col is not None]",
            "            if ix[\"column_names\"]:",
            "                normalized_idxs.append(ix)",
            "        return normalized_idxs",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: \"Database\",",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: Optional[str],",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return cls.normalize_indexes(inspector.get_indexes(table_name, schema))",
            "",
            "    @classmethod",
            "    def extra_table_metadata(",
            "        cls, database: \"Database\", table_name: str, schema_name: Optional[str]",
            "    ) -> dict[str, Any]:",
            "        indexes = database.get_indexes(table_name, schema_name)",
            "        if not indexes:",
            "            return {}",
            "        partitions_columns = [",
            "            index.get(\"column_names\", [])",
            "            for index in indexes",
            "            if index.get(\"name\") == \"partition\"",
            "        ]",
            "        cluster_columns = [",
            "            index.get(\"column_names\", [])",
            "            for index in indexes",
            "            if index.get(\"name\") == \"clustering\"",
            "        ]",
            "        return {",
            "            \"partitions\": {\"cols\": partitions_columns},",
            "            \"clustering\": {\"cols\": cluster_columns},",
            "        }",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        return \"TIMESTAMP_SECONDS({col})\"",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        return \"TIMESTAMP_MILLIS({col})\"",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: \"Database\",",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        Calls `pandas_gbq.DataFrame.to_gbq` which requires `pandas_gbq` to be installed.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "        if not can_upload:",
            "            raise SupersetException(",
            "                \"Could not import libraries needed to upload data to BigQuery.\"",
            "            )",
            "",
            "        if not table.schema:",
            "            raise SupersetException(\"The table schema must be defined\")",
            "",
            "        to_gbq_kwargs = {}",
            "        with cls.get_engine(database) as engine:",
            "            to_gbq_kwargs = {",
            "                \"destination_table\": str(table),",
            "                \"project_id\": engine.url.host,",
            "            }",
            "",
            "        # Add credentials if they are set on the SQLAlchemy dialect.",
            "",
            "        if creds := engine.dialect.credentials_info:",
            "            to_gbq_kwargs[",
            "                \"credentials\"",
            "            ] = service_account.Credentials.from_service_account_info(creds)",
            "",
            "        # Only pass through supported kwargs.",
            "        supported_kwarg_keys = {\"if_exists\"}",
            "",
            "        for key in supported_kwarg_keys:",
            "            if key in to_sql_kwargs:",
            "                to_gbq_kwargs[key] = to_sql_kwargs[key]",
            "",
            "        pandas_gbq.to_gbq(df, **to_gbq_kwargs)",
            "",
            "    @classmethod",
            "    def _get_client(cls, engine: Engine) -> Any:",
            "        \"\"\"",
            "        Return the BigQuery client associated with an engine.",
            "        \"\"\"",
            "        if not dependencies_installed:",
            "            raise SupersetException(",
            "                \"Could not import libraries needed to connect to BigQuery.\"",
            "            )",
            "",
            "        credentials = service_account.Credentials.from_service_account_info(",
            "            engine.dialect.credentials_info",
            "        )",
            "        return bigquery.Client(credentials=credentials)",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: \"Database\",",
            "        schema: str,",
            "        sql: str,",
            "        source: Optional[utils.QuerySource] = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise SupersetException(\"Database does not support cost estimation\")",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql)",
            "        statements = parsed_query.get_statements()",
            "        costs = []",
            "        for statement in statements:",
            "            processed_statement = cls.process_statement(statement, database)",
            "",
            "            costs.append(cls.estimate_statement_cost(processed_statement, database))",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_catalog_names(",
            "        cls,",
            "        database: \"Database\",",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs.",
            "",
            "        In BigQuery, a catalog is called a \"project\".",
            "        \"\"\"",
            "        engine: Engine",
            "        with database.get_sqla_engine_with_context() as engine:",
            "            client = cls._get_client(engine)",
            "            projects = client.list_projects()",
            "",
            "        return sorted(project.project_id for project in projects)",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(cls, extra: dict[str, Any]) -> bool:",
            "        return True",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        with cls.get_engine(cursor) as engine:",
            "            client = cls._get_client(engine)",
            "            job_config = bigquery.QueryJobConfig(dry_run=True)",
            "            query_job = client.query(",
            "                statement,",
            "                job_config=job_config,",
            "            )  # Make an API request.",
            "",
            "        # Format Bytes.",
            "        # TODO: Humanize in case more db engine specs need to be added,",
            "        # this should be made a function outside this scope.",
            "        byte_division = 1024",
            "        if hasattr(query_job, \"total_bytes_processed\"):",
            "            query_bytes_processed = query_job.total_bytes_processed",
            "            if query_bytes_processed // byte_division == 0:",
            "                byte_type = \"B\"",
            "                total_bytes_processed = query_bytes_processed",
            "            elif query_bytes_processed // (byte_division**2) == 0:",
            "                byte_type = \"KB\"",
            "                total_bytes_processed = round(query_bytes_processed / byte_division, 2)",
            "            elif query_bytes_processed // (byte_division**3) == 0:",
            "                byte_type = \"MB\"",
            "                total_bytes_processed = round(",
            "                    query_bytes_processed / (byte_division**2), 2",
            "                )",
            "            else:",
            "                byte_type = \"GB\"",
            "                total_bytes_processed = round(",
            "                    query_bytes_processed / (byte_division**3), 2",
            "                )",
            "",
            "            return {f\"{byte_type} Processed\": total_bytes_processed}",
            "        return {}",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        return [{k: str(v) for k, v in row.items()} for row in raw_cost]",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(",
            "        cls,",
            "        parameters: BigQueryParametersType,",
            "        encrypted_extra: Optional[dict[str, Any]] = None,",
            "    ) -> str:",
            "        query = parameters.get(\"query\", {})",
            "        query_params = urllib.parse.urlencode(query)",
            "",
            "        if encrypted_extra:",
            "            credentials_info = encrypted_extra.get(\"credentials_info\")",
            "            if isinstance(credentials_info, str):",
            "                credentials_info = json.loads(credentials_info)",
            "            project_id = credentials_info.get(\"project_id\")",
            "        if not encrypted_extra:",
            "            raise ValidationError(\"Missing service credentials\")",
            "",
            "        if project_id:",
            "            return f\"{cls.default_driver}://{project_id}/?{query_params}\"",
            "",
            "        raise ValidationError(\"Invalid service credentials\")",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(",
            "        cls,",
            "        uri: str,",
            "        encrypted_extra: Optional[dict[str, Any]] = None,",
            "    ) -> Any:",
            "        value = make_url_safe(uri)",
            "",
            "        # Building parameters from encrypted_extra and uri",
            "        if encrypted_extra:",
            "            # ``value.query`` needs to be explicitly converted into a dict (from an",
            "            # ``immutabledict``) so that it can be JSON serialized",
            "            return {**encrypted_extra, \"query\": dict(value.query)}",
            "",
            "        raise ValidationError(\"Invalid service credentials\")",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: Optional[str]) -> Optional[str]:",
            "        if encrypted_extra is None:",
            "            return encrypted_extra",
            "",
            "        try:",
            "            config = json.loads(encrypted_extra)",
            "        except (json.JSONDecodeError, TypeError):",
            "            return encrypted_extra",
            "",
            "        with contextlib.suppress(KeyError):",
            "            config[\"credentials_info\"][\"private_key\"] = PASSWORD_MASK",
            "        return json.dumps(config)",
            "",
            "    @classmethod",
            "    def unmask_encrypted_extra(",
            "        cls, old: Optional[str], new: Optional[str]",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Reuse ``private_key`` if available and unchanged.",
            "        \"\"\"",
            "        if old is None or new is None:",
            "            return new",
            "",
            "        try:",
            "            old_config = json.loads(old)",
            "            new_config = json.loads(new)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return new",
            "",
            "        if \"credentials_info\" not in new_config:",
            "            return new",
            "",
            "        if \"private_key\" not in new_config[\"credentials_info\"]:",
            "            return new",
            "",
            "        if new_config[\"credentials_info\"][\"private_key\"] == PASSWORD_MASK:",
            "            new_config[\"credentials_info\"][\"private_key\"] = old_config[",
            "                \"credentials_info\"",
            "            ][\"private_key\"]",
            "",
            "        return json.dumps(new_config)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from google.auth.exceptions import DefaultCredentialsError",
            "",
            "        return {DefaultCredentialsError: SupersetDBAPIConnectionError}",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls,",
            "        properties: BasicPropertiesType,  # pylint: disable=unused-argument",
            "    ) -> list[SupersetError]:",
            "        return []",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.0\",",
            "            plugins=[ma_plugin],",
            "        )",
            "",
            "        ma_plugin.init_spec(spec)",
            "        ma_plugin.converter.add_attribute_function(encrypted_field_properties)",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments",
            "        cls,",
            "        database: \"Database\",",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: Optional[str] = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: Optional[list[ResultSetColumnType]] = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Remove array structures from `SELECT *`.",
            "",
            "        BigQuery supports structures and arrays of structures, eg:",
            "",
            "            author STRUCT<name STRING, email STRING>",
            "            trailer ARRAY<STRUCT<key STRING, value STRING>>",
            "",
            "        When loading metadata for a table each key in the struct is displayed as a",
            "        separate pseudo-column, eg:",
            "",
            "            - author",
            "            - author.name",
            "            - author.email",
            "            - trailer",
            "            - trailer.key",
            "            - trailer.value",
            "",
            "        When generating the `SELECT *` statement we want to remove any keys from",
            "        structs inside an array, since selecting them results in an error. The correct",
            "        select statement should look like this:",
            "",
            "            SELECT",
            "              `author`,",
            "              `author`.`name`,",
            "              `author`.`email`,",
            "              `trailer`",
            "            FROM",
            "              table",
            "",
            "        Selecting `trailer.key` or `trailer.value` results in an error, as opposed to",
            "        selecting `author.name`, since they are keys in a structure inside an array.",
            "",
            "        This method removes any array pseudo-columns.",
            "        \"\"\"",
            "        if cols:",
            "            # For arrays of structs, remove the child columns, otherwise the query",
            "            # will fail.",
            "            array_prefixes = {",
            "                col[\"column_name\"]",
            "                for col in cols",
            "                if isinstance(col[\"type\"], sqltypes.ARRAY)",
            "            }",
            "            cols = [",
            "                col",
            "                for col in cols",
            "                if \".\" not in col[\"column_name\"]",
            "                or col[\"column_name\"].split(\".\")[0] not in array_prefixes",
            "            ]",
            "",
            "        return super().select_star(",
            "            database,",
            "            table_name,",
            "            engine,",
            "            schema,",
            "            limit,",
            "            show_cols,",
            "            indent,",
            "            latest_partition,",
            "            cols,",
            "        )",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        \"\"\"",
            "        Label columns using their fully qualified name.",
            "",
            "        BigQuery supports columns of type `struct`, which are basically dictionaries.",
            "        When loading metadata for a table with struct columns, each key in the struct",
            "        is displayed as a separate pseudo-column, eg:",
            "",
            "            author STRUCT<name STRING, email STRING>",
            "",
            "        Will be shown as 3 columns:",
            "",
            "            - author",
            "            - author.name",
            "            - author.email",
            "",
            "        If we select those fields:",
            "",
            "            SELECT `author`, `author`.`name`, `author`.`email` FROM table",
            "",
            "        The resulting columns will be called \"author\", \"name\", and \"email\", This may",
            "        result in a clash with other columns. To prevent that, we explicitly label",
            "        the columns using their fully qualified name, so we end up with \"author\",",
            "        \"author__name\" and \"author__email\", respectively.",
            "        \"\"\"",
            "        return [",
            "            column(c[\"column_name\"]).label(c[\"column_name\"].replace(\".\", \"__\"))",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        try:",
            "            return Exception(str(exception).splitlines()[0].strip())",
            "        except Exception:  # pylint: disable=broad-except",
            "            # If for some reason we get an exception, for example, no new line",
            "            # We will return the original exception",
            "            return exception"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import contextlib",
            "import json",
            "import re",
            "import urllib",
            "from datetime import datetime",
            "from re import Pattern",
            "from typing import Any, Optional, TYPE_CHECKING, TypedDict",
            "",
            "import pandas as pd",
            "from apispec import APISpec",
            "from apispec.ext.marshmallow import MarshmallowPlugin",
            "from deprecation import deprecated",
            "from flask_babel import gettext as __",
            "from marshmallow import fields, Schema",
            "from marshmallow.exceptions import ValidationError",
            "from sqlalchemy import column, types",
            "from sqlalchemy.engine.base import Engine",
            "from sqlalchemy.engine.reflection import Inspector",
            "from sqlalchemy.sql import sqltypes",
            "",
            "from superset import sql_parse",
            "from superset.constants import PASSWORD_MASK, TimeGrain",
            "from superset.databases.schemas import encrypted_field_properties, EncryptedString",
            "from superset.databases.utils import make_url_safe",
            "from superset.db_engine_specs.base import BaseEngineSpec, BasicPropertiesType",
            "from superset.db_engine_specs.exceptions import SupersetDBAPIConnectionError",
            "from superset.errors import SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetException",
            "from superset.sql_parse import Table",
            "from superset.superset_typing import ResultSetColumnType",
            "from superset.utils import core as utils",
            "from superset.utils.hashing import md5_sha_from_str",
            "",
            "try:",
            "    from google.cloud import bigquery",
            "    from google.oauth2 import service_account",
            "",
            "    dependencies_installed = True",
            "except ImportError:",
            "    dependencies_installed = False",
            "",
            "try:",
            "    import pandas_gbq",
            "",
            "    can_upload = True",
            "except ModuleNotFoundError:",
            "    can_upload = False",
            "",
            "if TYPE_CHECKING:",
            "    from superset.models.core import Database  # pragma: no cover",
            "",
            "CONNECTION_DATABASE_PERMISSIONS_REGEX = re.compile(",
            "    \"Access Denied: Project (?P<project_name>.+?): User does not have \"",
            "    + \"bigquery.jobs.create permission in project (?P<project>.+?)\"",
            ")",
            "",
            "TABLE_DOES_NOT_EXIST_REGEX = re.compile(",
            "    'Table name \"(?P<table>.*?)\" missing dataset while no default '",
            "    \"dataset is set in the request\"",
            ")",
            "",
            "COLUMN_DOES_NOT_EXIST_REGEX = re.compile(",
            "    r\"Unrecognized name: (?P<column>.*?) at \\[(?P<location>.+?)\\]\"",
            ")",
            "",
            "SCHEMA_DOES_NOT_EXIST_REGEX = re.compile(",
            "    r\"bigquery error: 404 Not found: Dataset (?P<dataset>.*?):\"",
            "    r\"(?P<schema>.*?) was not found in location\"",
            ")",
            "",
            "SYNTAX_ERROR_REGEX = re.compile(",
            "    'Syntax error: Expected end of input but got identifier \"(?P<syntax_error>.+?)\"'",
            ")",
            "",
            "ma_plugin = MarshmallowPlugin()",
            "",
            "",
            "class BigQueryParametersSchema(Schema):",
            "    credentials_info = EncryptedString(",
            "        required=False,",
            "        metadata={\"description\": \"Contents of BigQuery JSON credentials.\"},",
            "    )",
            "    query = fields.Dict(required=False)",
            "",
            "",
            "class BigQueryParametersType(TypedDict):",
            "    credentials_info: dict[str, Any]",
            "    query: dict[str, Any]",
            "",
            "",
            "class BigQueryEngineSpec(BaseEngineSpec):  # pylint: disable=too-many-public-methods",
            "    \"\"\"Engine spec for Google's BigQuery",
            "",
            "    As contributed by @mxmzdlv on issue #945\"\"\"",
            "",
            "    engine = \"bigquery\"",
            "    engine_name = \"Google BigQuery\"",
            "    max_column_name_length = 128",
            "    disable_ssh_tunneling = True",
            "",
            "    parameters_schema = BigQueryParametersSchema()",
            "    default_driver = \"bigquery\"",
            "    sqlalchemy_uri_placeholder = \"bigquery://{project_id}\"",
            "",
            "    # BigQuery doesn't maintain context when running multiple statements in the",
            "    # same cursor, so we need to run all statements at once",
            "    run_multiple_statements_as_one = True",
            "",
            "    allows_hidden_cc_in_orderby = True",
            "",
            "    supports_catalog = True",
            "",
            "    \"\"\"",
            "    https://www.python.org/dev/peps/pep-0249/#arraysize",
            "    raw_connections bypass the sqlalchemy-bigquery query execution context and deal with",
            "    raw dbapi connection directly.",
            "    If this value is not set, the default value is set to 1, as described here,",
            "    https://googlecloudplatform.github.io/google-cloud-python/latest/_modules/google/cloud/bigquery/dbapi/cursor.html#Cursor",
            "",
            "    The default value of 5000 is derived from the sqlalchemy-bigquery.",
            "    https://github.com/googleapis/python-bigquery-sqlalchemy/blob/4e17259088f89eac155adc19e0985278a29ecf9c/sqlalchemy_bigquery/base.py#L762",
            "    \"\"\"",
            "    arraysize = 5000",
            "",
            "    _date_trunc_functions = {",
            "        \"DATE\": \"DATE_TRUNC\",",
            "        \"DATETIME\": \"DATETIME_TRUNC\",",
            "        \"TIME\": \"TIME_TRUNC\",",
            "        \"TIMESTAMP\": \"TIMESTAMP_TRUNC\",",
            "    }",
            "",
            "    _time_grain_expressions = {",
            "        None: \"{col}\",",
            "        TimeGrain.SECOND: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"UNIX_SECONDS(CAST({col} AS TIMESTAMP))\"",
            "        \") AS {type})\",",
            "        TimeGrain.MINUTE: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.FIVE_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"5*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 5*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.TEN_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"10*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 10*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.FIFTEEN_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"15*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 15*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.THIRTY_MINUTES: \"CAST(TIMESTAMP_SECONDS(\"",
            "        \"30*60 * DIV(UNIX_SECONDS(CAST({col} AS TIMESTAMP)), 30*60)\"",
            "        \") AS {type})\",",
            "        TimeGrain.HOUR: \"{func}({col}, HOUR)\",",
            "        TimeGrain.DAY: \"{func}({col}, DAY)\",",
            "        TimeGrain.WEEK: \"{func}({col}, WEEK)\",",
            "        TimeGrain.WEEK_STARTING_MONDAY: \"{func}({col}, ISOWEEK)\",",
            "        TimeGrain.MONTH: \"{func}({col}, MONTH)\",",
            "        TimeGrain.QUARTER: \"{func}({col}, QUARTER)\",",
            "        TimeGrain.YEAR: \"{func}({col}, YEAR)\",",
            "    }",
            "",
            "    custom_errors: dict[Pattern[str], tuple[str, SupersetErrorType, dict[str, Any]]] = {",
            "        CONNECTION_DATABASE_PERMISSIONS_REGEX: (",
            "            __(",
            "                \"Unable to connect. Verify that the following roles are set \"",
            "                'on the service account: \"BigQuery Data Viewer\", '",
            "                '\"BigQuery Metadata Viewer\", \"BigQuery Job User\" '",
            "                \"and the following permissions are set \"",
            "                '\"bigquery.readsessions.create\", '",
            "                '\"bigquery.readsessions.getData\"'",
            "            ),",
            "            SupersetErrorType.CONNECTION_DATABASE_PERMISSIONS_ERROR,",
            "            {},",
            "        ),",
            "        TABLE_DOES_NOT_EXIST_REGEX: (",
            "            __(",
            "                'The table \"%(table)s\" does not exist. '",
            "                \"A valid table must be used to run this query.\",",
            "            ),",
            "            SupersetErrorType.TABLE_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        COLUMN_DOES_NOT_EXIST_REGEX: (",
            "            __('We can\\'t seem to resolve column \"%(column)s\" at line %(location)s.'),",
            "            SupersetErrorType.COLUMN_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        SCHEMA_DOES_NOT_EXIST_REGEX: (",
            "            __(",
            "                'The schema \"%(schema)s\" does not exist. '",
            "                \"A valid schema must be used to run this query.\"",
            "            ),",
            "            SupersetErrorType.SCHEMA_DOES_NOT_EXIST_ERROR,",
            "            {},",
            "        ),",
            "        SYNTAX_ERROR_REGEX: (",
            "            __(",
            "                \"Please check your query for syntax errors at or near \"",
            "                '\"%(syntax_error)s\". Then, try running your query again.'",
            "            ),",
            "            SupersetErrorType.SYNTAX_ERROR,",
            "            {},",
            "        ),",
            "    }",
            "",
            "    @classmethod",
            "    def convert_dttm(",
            "        cls, target_type: str, dttm: datetime, db_extra: Optional[dict[str, Any]] = None",
            "    ) -> Optional[str]:",
            "        sqla_type = cls.get_sqla_column_type(target_type)",
            "        if isinstance(sqla_type, types.Date):",
            "            return f\"CAST('{dttm.date().isoformat()}' AS DATE)\"",
            "        if isinstance(sqla_type, types.TIMESTAMP):",
            "            return f\"\"\"CAST('{dttm.isoformat(timespec=\"microseconds\")}' AS TIMESTAMP)\"\"\"",
            "        if isinstance(sqla_type, types.DateTime):",
            "            return f\"\"\"CAST('{dttm.isoformat(timespec=\"microseconds\")}' AS DATETIME)\"\"\"",
            "        if isinstance(sqla_type, types.Time):",
            "            return f\"\"\"CAST('{dttm.strftime(\"%H:%M:%S.%f\")}' AS TIME)\"\"\"",
            "        return None",
            "",
            "    @classmethod",
            "    def fetch_data(",
            "        cls, cursor: Any, limit: Optional[int] = None",
            "    ) -> list[tuple[Any, ...]]:",
            "        data = super().fetch_data(cursor, limit)",
            "        # Support type BigQuery Row, introduced here PR #4071",
            "        # google.cloud.bigquery.table.Row",
            "        if data and type(data[0]).__name__ == \"Row\":",
            "            data = [r.values() for r in data]  # type: ignore",
            "        return data",
            "",
            "    @staticmethod",
            "    def _mutate_label(label: str) -> str:",
            "        \"\"\"",
            "        BigQuery field_name should start with a letter or underscore and contain only",
            "        alphanumeric characters. Labels that start with a number are prefixed with an",
            "        underscore. Any unsupported characters are replaced with underscores and an",
            "        md5 hash is added to the end of the label to avoid possible collisions.",
            "",
            "        :param label: Expected expression label",
            "        :return: Conditionally mutated label",
            "        \"\"\"",
            "        label_hashed = \"_\" + md5_sha_from_str(label)",
            "",
            "        # if label starts with number, add underscore as first character",
            "        label_mutated = \"_\" + label if re.match(r\"^\\d\", label) else label",
            "",
            "        # replace non-alphanumeric characters with underscores",
            "        label_mutated = re.sub(r\"[^\\w]+\", \"_\", label_mutated)",
            "        if label_mutated != label:",
            "            # add first 5 chars from md5 hash to label to avoid possible collisions",
            "            label_mutated += label_hashed[:6]",
            "",
            "        return label_mutated",
            "",
            "    @classmethod",
            "    def _truncate_label(cls, label: str) -> str:",
            "        \"\"\"BigQuery requires column names start with either a letter or",
            "        underscore. To make sure this is always the case, an underscore is prefixed",
            "        to the md5 hash of the original label.",
            "",
            "        :param label: expected expression label",
            "        :return: truncated label",
            "        \"\"\"",
            "        return \"_\" + md5_sha_from_str(label)",
            "",
            "    @classmethod",
            "    @deprecated(deprecated_in=\"3.0\")",
            "    def normalize_indexes(cls, indexes: list[dict[str, Any]]) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Normalizes indexes for more consistency across db engines",
            "",
            "        :param indexes: Raw indexes as returned by SQLAlchemy",
            "        :return: cleaner, more aligned index definition",
            "        \"\"\"",
            "        normalized_idxs = []",
            "        # Fixing a bug/behavior observed in pybigquery==0.4.15 where",
            "        # the index's `column_names` == [None]",
            "        # Here we're returning only non-None indexes",
            "        for ix in indexes:",
            "            column_names = ix.get(\"column_names\") or []",
            "            ix[\"column_names\"] = [col for col in column_names if col is not None]",
            "            if ix[\"column_names\"]:",
            "                normalized_idxs.append(ix)",
            "        return normalized_idxs",
            "",
            "    @classmethod",
            "    def get_indexes(",
            "        cls,",
            "        database: \"Database\",",
            "        inspector: Inspector,",
            "        table_name: str,",
            "        schema: Optional[str],",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Get the indexes associated with the specified schema/table.",
            "",
            "        :param database: The database to inspect",
            "        :param inspector: The SQLAlchemy inspector",
            "        :param table_name: The table to inspect",
            "        :param schema: The schema to inspect",
            "        :returns: The indexes",
            "        \"\"\"",
            "",
            "        return cls.normalize_indexes(inspector.get_indexes(table_name, schema))",
            "",
            "    @classmethod",
            "    def extra_table_metadata(",
            "        cls, database: \"Database\", table_name: str, schema_name: Optional[str]",
            "    ) -> dict[str, Any]:",
            "        indexes = database.get_indexes(table_name, schema_name)",
            "        if not indexes:",
            "            return {}",
            "        partitions_columns = [",
            "            index.get(\"column_names\", [])",
            "            for index in indexes",
            "            if index.get(\"name\") == \"partition\"",
            "        ]",
            "        cluster_columns = [",
            "            index.get(\"column_names\", [])",
            "            for index in indexes",
            "            if index.get(\"name\") == \"clustering\"",
            "        ]",
            "        return {",
            "            \"partitions\": {\"cols\": partitions_columns},",
            "            \"clustering\": {\"cols\": cluster_columns},",
            "        }",
            "",
            "    @classmethod",
            "    def epoch_to_dttm(cls) -> str:",
            "        return \"TIMESTAMP_SECONDS({col})\"",
            "",
            "    @classmethod",
            "    def epoch_ms_to_dttm(cls) -> str:",
            "        return \"TIMESTAMP_MILLIS({col})\"",
            "",
            "    @classmethod",
            "    def df_to_sql(",
            "        cls,",
            "        database: \"Database\",",
            "        table: Table,",
            "        df: pd.DataFrame,",
            "        to_sql_kwargs: dict[str, Any],",
            "    ) -> None:",
            "        \"\"\"",
            "        Upload data from a Pandas DataFrame to a database.",
            "",
            "        Calls `pandas_gbq.DataFrame.to_gbq` which requires `pandas_gbq` to be installed.",
            "",
            "        Note this method does not create metadata for the table.",
            "",
            "        :param database: The database to upload the data to",
            "        :param table: The table to upload the data to",
            "        :param df: The dataframe with data to be uploaded",
            "        :param to_sql_kwargs: The kwargs to be passed to pandas.DataFrame.to_sql` method",
            "        \"\"\"",
            "        if not can_upload:",
            "            raise SupersetException(",
            "                \"Could not import libraries needed to upload data to BigQuery.\"",
            "            )",
            "",
            "        if not table.schema:",
            "            raise SupersetException(\"The table schema must be defined\")",
            "",
            "        to_gbq_kwargs = {}",
            "        with cls.get_engine(database) as engine:",
            "            to_gbq_kwargs = {",
            "                \"destination_table\": str(table),",
            "                \"project_id\": engine.url.host,",
            "            }",
            "",
            "        # Add credentials if they are set on the SQLAlchemy dialect.",
            "",
            "        if creds := engine.dialect.credentials_info:",
            "            to_gbq_kwargs[",
            "                \"credentials\"",
            "            ] = service_account.Credentials.from_service_account_info(creds)",
            "",
            "        # Only pass through supported kwargs.",
            "        supported_kwarg_keys = {\"if_exists\"}",
            "",
            "        for key in supported_kwarg_keys:",
            "            if key in to_sql_kwargs:",
            "                to_gbq_kwargs[key] = to_sql_kwargs[key]",
            "",
            "        pandas_gbq.to_gbq(df, **to_gbq_kwargs)",
            "",
            "    @classmethod",
            "    def _get_client(cls, engine: Engine) -> Any:",
            "        \"\"\"",
            "        Return the BigQuery client associated with an engine.",
            "        \"\"\"",
            "        if not dependencies_installed:",
            "            raise SupersetException(",
            "                \"Could not import libraries needed to connect to BigQuery.\"",
            "            )",
            "",
            "        credentials = service_account.Credentials.from_service_account_info(",
            "            engine.dialect.credentials_info",
            "        )",
            "        return bigquery.Client(credentials=credentials)",
            "",
            "    @classmethod",
            "    def estimate_query_cost(",
            "        cls,",
            "        database: \"Database\",",
            "        schema: str,",
            "        sql: str,",
            "        source: Optional[utils.QuerySource] = None,",
            "    ) -> list[dict[str, Any]]:",
            "        \"\"\"",
            "        Estimate the cost of a multiple statement SQL query.",
            "",
            "        :param database: Database instance",
            "        :param schema: Database schema",
            "        :param sql: SQL query with possibly multiple statements",
            "        :param source: Source of the query (eg, \"sql_lab\")",
            "        \"\"\"",
            "        extra = database.get_extra() or {}",
            "        if not cls.get_allow_cost_estimate(extra):",
            "            raise SupersetException(\"Database does not support cost estimation\")",
            "",
            "        parsed_query = sql_parse.ParsedQuery(sql, engine=cls.engine)",
            "        statements = parsed_query.get_statements()",
            "        costs = []",
            "        for statement in statements:",
            "            processed_statement = cls.process_statement(statement, database)",
            "",
            "            costs.append(cls.estimate_statement_cost(processed_statement, database))",
            "        return costs",
            "",
            "    @classmethod",
            "    def get_catalog_names(",
            "        cls,",
            "        database: \"Database\",",
            "        inspector: Inspector,",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Get all catalogs.",
            "",
            "        In BigQuery, a catalog is called a \"project\".",
            "        \"\"\"",
            "        engine: Engine",
            "        with database.get_sqla_engine_with_context() as engine:",
            "            client = cls._get_client(engine)",
            "            projects = client.list_projects()",
            "",
            "        return sorted(project.project_id for project in projects)",
            "",
            "    @classmethod",
            "    def get_allow_cost_estimate(cls, extra: dict[str, Any]) -> bool:",
            "        return True",
            "",
            "    @classmethod",
            "    def estimate_statement_cost(cls, statement: str, cursor: Any) -> dict[str, Any]:",
            "        with cls.get_engine(cursor) as engine:",
            "            client = cls._get_client(engine)",
            "            job_config = bigquery.QueryJobConfig(dry_run=True)",
            "            query_job = client.query(",
            "                statement,",
            "                job_config=job_config,",
            "            )  # Make an API request.",
            "",
            "        # Format Bytes.",
            "        # TODO: Humanize in case more db engine specs need to be added,",
            "        # this should be made a function outside this scope.",
            "        byte_division = 1024",
            "        if hasattr(query_job, \"total_bytes_processed\"):",
            "            query_bytes_processed = query_job.total_bytes_processed",
            "            if query_bytes_processed // byte_division == 0:",
            "                byte_type = \"B\"",
            "                total_bytes_processed = query_bytes_processed",
            "            elif query_bytes_processed // (byte_division**2) == 0:",
            "                byte_type = \"KB\"",
            "                total_bytes_processed = round(query_bytes_processed / byte_division, 2)",
            "            elif query_bytes_processed // (byte_division**3) == 0:",
            "                byte_type = \"MB\"",
            "                total_bytes_processed = round(",
            "                    query_bytes_processed / (byte_division**2), 2",
            "                )",
            "            else:",
            "                byte_type = \"GB\"",
            "                total_bytes_processed = round(",
            "                    query_bytes_processed / (byte_division**3), 2",
            "                )",
            "",
            "            return {f\"{byte_type} Processed\": total_bytes_processed}",
            "        return {}",
            "",
            "    @classmethod",
            "    def query_cost_formatter(",
            "        cls, raw_cost: list[dict[str, Any]]",
            "    ) -> list[dict[str, str]]:",
            "        return [{k: str(v) for k, v in row.items()} for row in raw_cost]",
            "",
            "    @classmethod",
            "    def build_sqlalchemy_uri(",
            "        cls,",
            "        parameters: BigQueryParametersType,",
            "        encrypted_extra: Optional[dict[str, Any]] = None,",
            "    ) -> str:",
            "        query = parameters.get(\"query\", {})",
            "        query_params = urllib.parse.urlencode(query)",
            "",
            "        if encrypted_extra:",
            "            credentials_info = encrypted_extra.get(\"credentials_info\")",
            "            if isinstance(credentials_info, str):",
            "                credentials_info = json.loads(credentials_info)",
            "            project_id = credentials_info.get(\"project_id\")",
            "        if not encrypted_extra:",
            "            raise ValidationError(\"Missing service credentials\")",
            "",
            "        if project_id:",
            "            return f\"{cls.default_driver}://{project_id}/?{query_params}\"",
            "",
            "        raise ValidationError(\"Invalid service credentials\")",
            "",
            "    @classmethod",
            "    def get_parameters_from_uri(",
            "        cls,",
            "        uri: str,",
            "        encrypted_extra: Optional[dict[str, Any]] = None,",
            "    ) -> Any:",
            "        value = make_url_safe(uri)",
            "",
            "        # Building parameters from encrypted_extra and uri",
            "        if encrypted_extra:",
            "            # ``value.query`` needs to be explicitly converted into a dict (from an",
            "            # ``immutabledict``) so that it can be JSON serialized",
            "            return {**encrypted_extra, \"query\": dict(value.query)}",
            "",
            "        raise ValidationError(\"Invalid service credentials\")",
            "",
            "    @classmethod",
            "    def mask_encrypted_extra(cls, encrypted_extra: Optional[str]) -> Optional[str]:",
            "        if encrypted_extra is None:",
            "            return encrypted_extra",
            "",
            "        try:",
            "            config = json.loads(encrypted_extra)",
            "        except (json.JSONDecodeError, TypeError):",
            "            return encrypted_extra",
            "",
            "        with contextlib.suppress(KeyError):",
            "            config[\"credentials_info\"][\"private_key\"] = PASSWORD_MASK",
            "        return json.dumps(config)",
            "",
            "    @classmethod",
            "    def unmask_encrypted_extra(",
            "        cls, old: Optional[str], new: Optional[str]",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Reuse ``private_key`` if available and unchanged.",
            "        \"\"\"",
            "        if old is None or new is None:",
            "            return new",
            "",
            "        try:",
            "            old_config = json.loads(old)",
            "            new_config = json.loads(new)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return new",
            "",
            "        if \"credentials_info\" not in new_config:",
            "            return new",
            "",
            "        if \"private_key\" not in new_config[\"credentials_info\"]:",
            "            return new",
            "",
            "        if new_config[\"credentials_info\"][\"private_key\"] == PASSWORD_MASK:",
            "            new_config[\"credentials_info\"][\"private_key\"] = old_config[",
            "                \"credentials_info\"",
            "            ][\"private_key\"]",
            "",
            "        return json.dumps(new_config)",
            "",
            "    @classmethod",
            "    def get_dbapi_exception_mapping(cls) -> dict[type[Exception], type[Exception]]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from google.auth.exceptions import DefaultCredentialsError",
            "",
            "        return {DefaultCredentialsError: SupersetDBAPIConnectionError}",
            "",
            "    @classmethod",
            "    def validate_parameters(",
            "        cls,",
            "        properties: BasicPropertiesType,  # pylint: disable=unused-argument",
            "    ) -> list[SupersetError]:",
            "        return []",
            "",
            "    @classmethod",
            "    def parameters_json_schema(cls) -> Any:",
            "        \"\"\"",
            "        Return configuration parameters as OpenAPI.",
            "        \"\"\"",
            "        if not cls.parameters_schema:",
            "            return None",
            "",
            "        spec = APISpec(",
            "            title=\"Database Parameters\",",
            "            version=\"1.0.0\",",
            "            openapi_version=\"3.0.0\",",
            "            plugins=[ma_plugin],",
            "        )",
            "",
            "        ma_plugin.init_spec(spec)",
            "        ma_plugin.converter.add_attribute_function(encrypted_field_properties)",
            "        spec.components.schema(cls.__name__, schema=cls.parameters_schema)",
            "        return spec.to_dict()[\"components\"][\"schemas\"][cls.__name__]",
            "",
            "    @classmethod",
            "    def select_star(  # pylint: disable=too-many-arguments",
            "        cls,",
            "        database: \"Database\",",
            "        table_name: str,",
            "        engine: Engine,",
            "        schema: Optional[str] = None,",
            "        limit: int = 100,",
            "        show_cols: bool = False,",
            "        indent: bool = True,",
            "        latest_partition: bool = True,",
            "        cols: Optional[list[ResultSetColumnType]] = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        Remove array structures from `SELECT *`.",
            "",
            "        BigQuery supports structures and arrays of structures, eg:",
            "",
            "            author STRUCT<name STRING, email STRING>",
            "            trailer ARRAY<STRUCT<key STRING, value STRING>>",
            "",
            "        When loading metadata for a table each key in the struct is displayed as a",
            "        separate pseudo-column, eg:",
            "",
            "            - author",
            "            - author.name",
            "            - author.email",
            "            - trailer",
            "            - trailer.key",
            "            - trailer.value",
            "",
            "        When generating the `SELECT *` statement we want to remove any keys from",
            "        structs inside an array, since selecting them results in an error. The correct",
            "        select statement should look like this:",
            "",
            "            SELECT",
            "              `author`,",
            "              `author`.`name`,",
            "              `author`.`email`,",
            "              `trailer`",
            "            FROM",
            "              table",
            "",
            "        Selecting `trailer.key` or `trailer.value` results in an error, as opposed to",
            "        selecting `author.name`, since they are keys in a structure inside an array.",
            "",
            "        This method removes any array pseudo-columns.",
            "        \"\"\"",
            "        if cols:",
            "            # For arrays of structs, remove the child columns, otherwise the query",
            "            # will fail.",
            "            array_prefixes = {",
            "                col[\"column_name\"]",
            "                for col in cols",
            "                if isinstance(col[\"type\"], sqltypes.ARRAY)",
            "            }",
            "            cols = [",
            "                col",
            "                for col in cols",
            "                if \".\" not in col[\"column_name\"]",
            "                or col[\"column_name\"].split(\".\")[0] not in array_prefixes",
            "            ]",
            "",
            "        return super().select_star(",
            "            database,",
            "            table_name,",
            "            engine,",
            "            schema,",
            "            limit,",
            "            show_cols,",
            "            indent,",
            "            latest_partition,",
            "            cols,",
            "        )",
            "",
            "    @classmethod",
            "    def _get_fields(cls, cols: list[ResultSetColumnType]) -> list[Any]:",
            "        \"\"\"",
            "        Label columns using their fully qualified name.",
            "",
            "        BigQuery supports columns of type `struct`, which are basically dictionaries.",
            "        When loading metadata for a table with struct columns, each key in the struct",
            "        is displayed as a separate pseudo-column, eg:",
            "",
            "            author STRUCT<name STRING, email STRING>",
            "",
            "        Will be shown as 3 columns:",
            "",
            "            - author",
            "            - author.name",
            "            - author.email",
            "",
            "        If we select those fields:",
            "",
            "            SELECT `author`, `author`.`name`, `author`.`email` FROM table",
            "",
            "        The resulting columns will be called \"author\", \"name\", and \"email\", This may",
            "        result in a clash with other columns. To prevent that, we explicitly label",
            "        the columns using their fully qualified name, so we end up with \"author\",",
            "        \"author__name\" and \"author__email\", respectively.",
            "        \"\"\"",
            "        return [",
            "            column(c[\"column_name\"]).label(c[\"column_name\"].replace(\".\", \"__\"))",
            "            for c in cols",
            "        ]",
            "",
            "    @classmethod",
            "    def parse_error_exception(cls, exception: Exception) -> Exception:",
            "        try:",
            "            return Exception(str(exception).splitlines()[0].strip())",
            "        except Exception:  # pylint: disable=broad-except",
            "            # If for some reason we get an exception, for example, no new line",
            "            # We will return the original exception",
            "            return exception"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "438": [
                "BigQueryEngineSpec",
                "estimate_query_cost"
            ]
        },
        "addLocation": []
    },
    "superset/models/helpers.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1094,
                "afterPatchRowNumber": 1094,
                "PatchRowcode": "         \"\"\""
            },
            "1": {
                "beforePatchRowNumber": 1095,
                "afterPatchRowNumber": 1095,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 1096,
                "afterPatchRowNumber": 1096,
                "PatchRowcode": "         from_sql = self.get_rendered_sql(template_processor)"
            },
            "3": {
                "beforePatchRowNumber": 1097,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        parsed_query = ParsedQuery(from_sql)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1097,
                "PatchRowcode": "+        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)"
            },
            "5": {
                "beforePatchRowNumber": 1098,
                "afterPatchRowNumber": 1098,
                "PatchRowcode": "         if not ("
            },
            "6": {
                "beforePatchRowNumber": 1099,
                "afterPatchRowNumber": 1099,
                "PatchRowcode": "             parsed_query.is_unknown()"
            },
            "7": {
                "beforePatchRowNumber": 1100,
                "afterPatchRowNumber": 1100,
                "PatchRowcode": "             or self.db_engine_spec.is_readonly_query(parsed_query)"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"a collection of model-related helper classes and functions\"\"\"",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "import uuid",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "import dateutil.parser",
            "import humanize",
            "import numpy as np",
            "import pandas as pd",
            "import pytz",
            "import sqlalchemy as sa",
            "import sqlparse",
            "import yaml",
            "from flask import escape, g, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.models.decorators import renders",
            "from flask_appbuilder.models.mixins import AuditMixin",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import and_, Column, or_, UniqueConstraint",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.orm import Mapper, Session, validates",
            "from sqlalchemy.orm.exc import MultipleResultsFound",
            "from sqlalchemy.sql.elements import ColumnElement, literal_column, TextClause",
            "from sqlalchemy.sql.expression import Label, Select, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "from sqlalchemy_utils import UUIDType",
            "",
            "from superset import app, is_feature_enabled, security_manager",
            "from superset.advanced_data_type.types import AdvancedDataTypeResponse",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.common.utils.time_range_utils import get_since_until_from_time_range",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import TimestampExpression",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    AdvancedDataTypeResponseError,",
            "    ColumnNotFoundException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetSecurityException,",
            ")",
            "from superset.extensions import feature_flag_manager",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.sql_parse import (",
            "    has_table_query,",
            "    insert_rls_in_predicate,",
            "    ParsedQuery,",
            "    sanitize_clause,",
            ")",
            "from superset.superset_typing import (",
            "    AdhocMetric,",
            "    Column as ColumnTyping,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    OrderBy,",
            "    QueryObjectDict,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.core import (",
            "    GenericDataType,",
            "    get_column_name,",
            "    get_user_id,",
            "    is_adhoc_column,",
            "    remove_duplicates,",
            ")",
            "from superset.utils.dates import datetime_to_epoch",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import SqlMetric, TableColumn",
            "    from superset.db_engine_specs import BaseEngineSpec",
            "    from superset.models.core import Database",
            "",
            "",
            "config = app.config",
            "logger = logging.getLogger(__name__)",
            "",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "SERIES_LIMIT_SUBQ_ALIAS = \"series_limit\"",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "",
            "",
            "def validate_adhoc_subquery(",
            "    sql: str,",
            "    database_id: int,",
            "    default_schema: str,",
            ") -> str:",
            "    \"\"\"",
            "    Check if adhoc SQL contains sub-queries or nested sub-queries with table.",
            "",
            "    If sub-queries are allowed, the adhoc SQL is modified to insert any applicable RLS",
            "    predicates to it.",
            "",
            "    :param sql: adhoc sql expression",
            "    :raise SupersetSecurityException if sql contains sub-queries or",
            "    nested sub-queries with table",
            "    \"\"\"",
            "    statements = []",
            "    for statement in sqlparse.parse(sql):",
            "        if has_table_query(statement):",
            "            if not is_feature_enabled(\"ALLOW_ADHOC_SUBQUERY\"):",
            "                raise SupersetSecurityException(",
            "                    SupersetError(",
            "                        error_type=SupersetErrorType.ADHOC_SUBQUERY_NOT_ALLOWED_ERROR,",
            "                        message=_(\"Custom SQL fields cannot contain sub-queries.\"),",
            "                        level=ErrorLevel.ERROR,",
            "                    )",
            "                )",
            "            statement = insert_rls_in_predicate(statement, database_id, default_schema)",
            "        statements.append(statement)",
            "",
            "    return \";\\n\".join(str(statement) for statement in statements)",
            "",
            "",
            "def json_to_dict(json_str: str) -> dict[Any, Any]:",
            "    if json_str:",
            "        val = re.sub(\",[ \\t\\r\\n]+}\", \"}\", json_str)",
            "        val = re.sub(\",[ \\t\\r\\n]+\\\\]\", \"]\", val)",
            "        return json.loads(val)",
            "",
            "    return {}",
            "",
            "",
            "def convert_uuids(obj: Any) -> Any:",
            "    \"\"\"",
            "    Convert UUID objects to str so we can use yaml.safe_dump",
            "    \"\"\"",
            "    if isinstance(obj, uuid.UUID):",
            "        return str(obj)",
            "",
            "    if isinstance(obj, list):",
            "        return [convert_uuids(el) for el in obj]",
            "",
            "    if isinstance(obj, dict):",
            "        return {k: convert_uuids(v) for k, v in obj.items()}",
            "",
            "    return obj",
            "",
            "",
            "class ImportExportMixin:",
            "    uuid = sa.Column(",
            "        UUIDType(binary=True), primary_key=False, unique=True, default=uuid.uuid4",
            "    )",
            "",
            "    export_parent: Optional[str] = None",
            "    # The name of the attribute",
            "    # with the SQL Alchemy back reference",
            "",
            "    export_children: list[str] = []",
            "    # List of (str) names of attributes",
            "    # with the SQL Alchemy forward references",
            "",
            "    export_fields: list[str] = []",
            "    # The names of the attributes",
            "    # that are available for import and export",
            "",
            "    extra_import_fields: list[str] = []",
            "    # Additional fields that should be imported,",
            "    # even though they were not exported",
            "",
            "    __mapper__: Mapper",
            "",
            "    @classmethod",
            "    def _unique_constraints(cls) -> list[set[str]]:",
            "        \"\"\"Get all (single column and multi column) unique constraints\"\"\"",
            "        unique = [",
            "            {c.name for c in u.columns}",
            "            for u in cls.__table_args__  # type: ignore",
            "            if isinstance(u, UniqueConstraint)",
            "        ]",
            "        unique.extend(",
            "            {c.name} for c in cls.__table__.columns if c.unique  # type: ignore",
            "        )",
            "        return unique",
            "",
            "    @classmethod",
            "    def parent_foreign_key_mappings(cls) -> dict[str, str]:",
            "        \"\"\"Get a mapping of foreign name to the local name of foreign keys\"\"\"",
            "        parent_rel = cls.__mapper__.relationships.get(cls.export_parent)",
            "        if parent_rel:",
            "            return {l.name: r.name for (l, r) in parent_rel.local_remote_pairs}",
            "        return {}",
            "",
            "    @classmethod",
            "    def export_schema(",
            "        cls, recursive: bool = True, include_parent_ref: bool = False",
            "    ) -> dict[str, Any]:",
            "        \"\"\"Export schema as a dictionary\"\"\"",
            "        parent_excludes = set()",
            "        if not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {column.name for column in parent_ref.local_columns}",
            "",
            "        def formatter(column: sa.Column) -> str:",
            "            return (",
            "                f\"{str(column.type)} Default ({column.default.arg})\"",
            "                if column.default",
            "                else str(column.type)",
            "            )",
            "",
            "        schema: dict[str, Any] = {",
            "            column.name: formatter(column)",
            "            for column in cls.__table__.columns  # type: ignore",
            "            if (column.name in cls.export_fields and column.name not in parent_excludes)",
            "        }",
            "        if recursive:",
            "            for column in cls.export_children:",
            "                child_class = cls.__mapper__.relationships[column].argument.class_",
            "                schema[column] = [",
            "                    child_class.export_schema(",
            "                        recursive=recursive, include_parent_ref=include_parent_ref",
            "                    )",
            "                ]",
            "        return schema",
            "",
            "    @classmethod",
            "    def import_from_dict(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        cls,",
            "        session: Session,",
            "        dict_rep: dict[Any, Any],",
            "        parent: Optional[Any] = None,",
            "        recursive: bool = True,",
            "        sync: Optional[list[str]] = None,",
            "        allow_reparenting: bool = False,",
            "    ) -> Any:",
            "        \"\"\"Import obj from a dictionary\"\"\"",
            "        if sync is None:",
            "            sync = []",
            "        parent_refs = cls.parent_foreign_key_mappings()",
            "        export_fields = (",
            "            set(cls.export_fields)",
            "            | set(cls.extra_import_fields)",
            "            | set(parent_refs.keys())",
            "            | {\"uuid\"}",
            "        )",
            "        new_children = {c: dict_rep[c] for c in cls.export_children if c in dict_rep}",
            "        unique_constraints = cls._unique_constraints()",
            "",
            "        filters = []  # Using these filters to check if obj already exists",
            "",
            "        # Remove fields that should not get imported",
            "        for k in list(dict_rep):",
            "            if k not in export_fields and k not in parent_refs:",
            "                del dict_rep[k]",
            "",
            "        if not parent:",
            "            if cls.export_parent:",
            "                for prnt in parent_refs.keys():",
            "                    if prnt not in dict_rep:",
            "                        raise RuntimeError(f\"{cls.__name__}: Missing field {prnt}\")",
            "        else:",
            "            # Set foreign keys to parent obj",
            "            for k, v in parent_refs.items():",
            "                dict_rep[k] = getattr(parent, v)",
            "",
            "        if not allow_reparenting:",
            "            # Add filter for parent obj",
            "            filters.extend(",
            "                [getattr(cls, k) == dict_rep.get(k) for k in parent_refs.keys()]",
            "            )",
            "",
            "        # Add filter for unique constraints",
            "        ucs = [",
            "            and_(",
            "                *[",
            "                    getattr(cls, k) == dict_rep.get(k)",
            "                    for k in cs",
            "                    if dict_rep.get(k) is not None",
            "                ]",
            "            )",
            "            for cs in unique_constraints",
            "        ]",
            "        filters.append(or_(*ucs))",
            "",
            "        # Check if object already exists in DB, break if more than one is found",
            "        try:",
            "            obj_query = session.query(cls).filter(and_(*filters))",
            "            obj = obj_query.one_or_none()",
            "        except MultipleResultsFound as ex:",
            "            logger.error(",
            "                \"Error importing %s \\n %s \\n %s\",",
            "                cls.__name__,",
            "                str(obj_query),",
            "                yaml.safe_dump(dict_rep),",
            "                exc_info=True,",
            "            )",
            "            raise ex",
            "",
            "        if not obj:",
            "            is_new_obj = True",
            "            # Create new DB object",
            "            obj = cls(**dict_rep)",
            "            logger.info(\"Importing new %s %s\", obj.__tablename__, str(obj))",
            "            if cls.export_parent and parent:",
            "                setattr(obj, cls.export_parent, parent)",
            "            session.add(obj)",
            "        else:",
            "            is_new_obj = False",
            "            logger.info(\"Updating %s %s\", obj.__tablename__, str(obj))",
            "            # Update columns",
            "            for k, v in dict_rep.items():",
            "                setattr(obj, k, v)",
            "",
            "        # Recursively create children",
            "        if recursive:",
            "            for child in cls.export_children:",
            "                argument = cls.__mapper__.relationships[child].argument",
            "                child_class = (",
            "                    argument.class_ if hasattr(argument, \"class_\") else argument",
            "                )",
            "                added = []",
            "                for c_obj in new_children.get(child, []):",
            "                    added.append(",
            "                        child_class.import_from_dict(",
            "                            session=session, dict_rep=c_obj, parent=obj, sync=sync",
            "                        )",
            "                    )",
            "                # If children should get synced, delete the ones that did not",
            "                # get updated.",
            "                if child in sync and not is_new_obj:",
            "                    back_refs = child_class.parent_foreign_key_mappings()",
            "                    delete_filters = [",
            "                        getattr(child_class, k) == getattr(obj, back_refs.get(k))",
            "                        for k in back_refs.keys()",
            "                    ]",
            "                    to_delete = set(",
            "                        session.query(child_class).filter(and_(*delete_filters))",
            "                    ).difference(set(added))",
            "                    for o in to_delete:",
            "                        logger.info(\"Deleting %s %s\", child, str(obj))",
            "                        session.delete(o)",
            "",
            "        return obj",
            "",
            "    def export_to_dict(",
            "        self,",
            "        recursive: bool = True,",
            "        include_parent_ref: bool = False,",
            "        include_defaults: bool = False,",
            "        export_uuids: bool = False,",
            "    ) -> dict[Any, Any]:",
            "        \"\"\"Export obj to dictionary\"\"\"",
            "        export_fields = set(self.export_fields)",
            "        if export_uuids:",
            "            export_fields.add(\"uuid\")",
            "            if \"id\" in export_fields:",
            "                export_fields.remove(\"id\")",
            "",
            "        cls = self.__class__",
            "        parent_excludes = set()",
            "        if recursive and not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {c.name for c in parent_ref.local_columns}",
            "        dict_rep = {",
            "            c.name: getattr(self, c.name)",
            "            for c in cls.__table__.columns  # type: ignore",
            "            if (",
            "                c.name in export_fields",
            "                and c.name not in parent_excludes",
            "                and (",
            "                    include_defaults",
            "                    or (",
            "                        getattr(self, c.name) is not None",
            "                        and (not c.default or getattr(self, c.name) != c.default.arg)",
            "                    )",
            "                )",
            "            )",
            "        }",
            "",
            "        # sort according to export_fields using DSU (decorate, sort, undecorate)",
            "        order = {field: i for i, field in enumerate(self.export_fields)}",
            "        decorated_keys = [(order.get(k, len(order)), k) for k in dict_rep]",
            "        decorated_keys.sort()",
            "        dict_rep = {k: dict_rep[k] for _, k in decorated_keys}",
            "",
            "        if recursive:",
            "            for cld in self.export_children:",
            "                # sorting to make lists of children stable",
            "                dict_rep[cld] = sorted(",
            "                    [",
            "                        child.export_to_dict(",
            "                            recursive=recursive,",
            "                            include_parent_ref=include_parent_ref,",
            "                            include_defaults=include_defaults,",
            "                        )",
            "                        for child in getattr(self, cld)",
            "                    ],",
            "                    key=lambda k: sorted(str(k.items())),",
            "                )",
            "",
            "        return convert_uuids(dict_rep)",
            "",
            "    def override(self, obj: Any) -> None:",
            "        \"\"\"Overrides the plain fields of the dashboard.\"\"\"",
            "        for field in obj.__class__.export_fields:",
            "            setattr(self, field, getattr(obj, field))",
            "",
            "    def copy(self) -> Any:",
            "        \"\"\"Creates a copy of the dashboard without relationships.\"\"\"",
            "        new_obj = self.__class__()",
            "        new_obj.override(self)",
            "        return new_obj",
            "",
            "    def alter_params(self, **kwargs: Any) -> None:",
            "        params = self.params_dict",
            "        params.update(kwargs)",
            "        self.params = json.dumps(params)",
            "",
            "    def remove_params(self, param_to_remove: str) -> None:",
            "        params = self.params_dict",
            "        params.pop(param_to_remove, None)",
            "        self.params = json.dumps(params)",
            "",
            "    def reset_ownership(self) -> None:",
            "        \"\"\"object will belong to the user the current user\"\"\"",
            "        # make sure the object doesn't have relations to a user",
            "        # it will be filled by appbuilder on save",
            "        self.created_by = None",
            "        self.changed_by = None",
            "        # flask global context might not exist (in cli or tests for example)",
            "        self.owners = []",
            "        if g and hasattr(g, \"user\"):",
            "            self.owners = [g.user]",
            "",
            "    @property",
            "    def params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.params)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.template_params)  # type: ignore",
            "",
            "",
            "def _user_link(user: User) -> Union[Markup, str]:",
            "    if not user:",
            "        return \"\"",
            "    url = f\"/superset/profile/{user.username}/\"",
            "    return Markup(f\"<a href=\\\"{url}\\\">{escape(user) or ''}</a>\")",
            "",
            "",
            "class AuditMixinNullable(AuditMixin):",
            "    \"\"\"Altering the AuditMixin to use nullable fields",
            "",
            "    Allows creating objects programmatically outside of CRUD",
            "    \"\"\"",
            "",
            "    created_on = sa.Column(sa.DateTime, default=datetime.now, nullable=True)",
            "    changed_on = sa.Column(",
            "        sa.DateTime, default=datetime.now, onupdate=datetime.now, nullable=True",
            "    )",
            "",
            "    @declared_attr",
            "    def created_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @declared_attr",
            "    def changed_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            onupdate=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @property",
            "    def created_by_name(self) -> str:",
            "        if self.created_by:",
            "            return escape(f\"{self.created_by}\")",
            "        return \"\"",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if self.changed_by:",
            "            return escape(f\"{self.changed_by}\")",
            "        return \"\"",
            "",
            "    @renders(\"created_by\")",
            "    def creator(self) -> Union[Markup, str]:",
            "        return _user_link(self.created_by)",
            "",
            "    @property",
            "    def changed_by_(self) -> Union[Markup, str]:",
            "        return _user_link(self.changed_by)",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on}</span>')",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_delta_humanized(self) -> str:",
            "        return self.changed_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_dttm(self) -> float:",
            "        return datetime_to_epoch(self.changed_on)",
            "",
            "    @renders(\"created_on\")",
            "    def created_on_delta_humanized(self) -> str:",
            "        return self.created_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_utc(self) -> str:",
            "        # Convert naive datetime to UTC",
            "        return self.changed_on.astimezone(pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")",
            "",
            "    @property",
            "    def changed_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @property",
            "    def created_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.created_on)",
            "",
            "    @renders(\"changed_on\")",
            "    def modified(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on_humanized}</span>')",
            "",
            "",
            "class QueryResult:  # pylint: disable=too-few-public-methods",
            "",
            "    \"\"\"Object returned by the query interface\"\"\"",
            "",
            "    def __init__(  # pylint: disable=too-many-arguments",
            "        self,",
            "        df: pd.DataFrame,",
            "        query: str,",
            "        duration: timedelta,",
            "        applied_template_filters: Optional[list[str]] = None,",
            "        applied_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        rejected_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        status: str = QueryStatus.SUCCESS,",
            "        error_message: Optional[str] = None,",
            "        errors: Optional[list[dict[str, Any]]] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        to_dttm: Optional[datetime] = None,",
            "    ) -> None:",
            "        self.df = df",
            "        self.query = query",
            "        self.duration = duration",
            "        self.applied_template_filters = applied_template_filters or []",
            "        self.applied_filter_columns = applied_filter_columns or []",
            "        self.rejected_filter_columns = rejected_filter_columns or []",
            "        self.status = status",
            "        self.error_message = error_message",
            "        self.errors = errors or []",
            "        self.from_dttm = from_dttm",
            "        self.to_dttm = to_dttm",
            "",
            "",
            "class ExtraJSONMixin:",
            "    \"\"\"Mixin to add an `extra` column (JSON) and utility methods\"\"\"",
            "",
            "    extra_json = sa.Column(sa.Text, default=\"{}\")",
            "",
            "    @property",
            "    def extra(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra_json or \"{}\") or {}",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load an extra json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return {}",
            "",
            "    @extra.setter",
            "    def extra(self, extras: dict[str, Any]) -> None:",
            "        self.extra_json = json.dumps(extras)",
            "",
            "    def set_extra_json_key(self, key: str, value: Any) -> None:",
            "        extra = self.extra",
            "        extra[key] = value",
            "        self.extra_json = json.dumps(extra)",
            "",
            "    @validates(\"extra_json\")",
            "    def ensure_extra_json_is_not_none(",
            "        self,",
            "        _: str,",
            "        value: Optional[dict[str, Any]],",
            "    ) -> Any:",
            "        if value is None:",
            "            return \"{}\"",
            "        return value",
            "",
            "",
            "class CertificationMixin:",
            "    \"\"\"Mixin to add extra certification fields\"\"\"",
            "",
            "    extra = sa.Column(sa.Text, default=\"{}\")",
            "",
            "    def get_extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    @property",
            "    def is_certified(self) -> bool:",
            "        return bool(self.get_extra_dict().get(\"certification\"))",
            "",
            "    @property",
            "    def certified_by(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"certified_by\")",
            "",
            "    @property",
            "    def certification_details(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"details\")",
            "",
            "    @property",
            "    def warning_markdown(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"warning_markdown\")",
            "",
            "",
            "def clone_model(",
            "    target: Model,",
            "    ignore: Optional[list[str]] = None,",
            "    keep_relations: Optional[list[str]] = None,",
            "    **kwargs: Any,",
            ") -> Model:",
            "    \"\"\"",
            "    Clone a SQLAlchemy model. By default will only clone naive column attributes.",
            "    To include relationship attributes, use `keep_relations`.",
            "    \"\"\"",
            "    ignore = ignore or []",
            "",
            "    table = target.__table__",
            "    primary_keys = table.primary_key.columns.keys()",
            "    data = {",
            "        attr: getattr(target, attr)",
            "        for attr in list(table.columns.keys()) + (keep_relations or [])",
            "        if attr not in primary_keys and attr not in ignore",
            "    }",
            "    data.update(kwargs)",
            "",
            "    return target.__class__(**data)",
            "",
            "",
            "# todo(hugh): centralize where this code lives",
            "class QueryStringExtended(NamedTuple):",
            "    applied_template_filters: Optional[list[str]]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sql: str",
            "",
            "",
            "class SqlaQuery(NamedTuple):",
            "    applied_template_filters: list[str]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    cte: Optional[str]",
            "    extra_cache_keys: list[Any]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sqla_query: Select",
            "",
            "",
            "class ExploreMixin:  # pylint: disable=too-many-public-methods",
            "    \"\"\"",
            "    Allows any flask_appbuilder.Model (Query, Table, etc.)",
            "    to be used to power a chart inside /explore",
            "    \"\"\"",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "    fetch_values_predicate = None",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_extra(self) -> Optional[dict[str, Any]]:",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database_id(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def owners_data(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def metrics(self) -> list[Any]:",
            "        return []",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def is_rls_supported(self) -> bool:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def cache_timeout(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def offset(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def main_dttm_col(self) -> Optional[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def always_filter_main_dttm(self) -> Optional[bool]:",
            "        return False",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[\"BaseEngineSpec\"]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database(self) -> \"Database\":",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def schema(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def sql(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def columns(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    def get_extra_cache_keys(self, query_obj: dict[str, Any]) -> list[Hashable]:",
            "        raise NotImplementedError()",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        raise NotImplementedError()",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: Optional[  # pylint: disable=unused-argument",
            "            BaseTemplateProcessor",
            "        ] = None,",
            "    ) -> TextClause:",
            "        return self.fetch_values_predicate",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[Union[int, str], list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def _process_sql_expression(",
            "        self,",
            "        expression: Optional[str],",
            "        database_id: int,",
            "        schema: str,",
            "        template_processor: Optional[BaseTemplateProcessor],",
            "    ) -> Optional[str]:",
            "        if template_processor and expression:",
            "            expression = template_processor.process_template(expression)",
            "        if expression:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            try:",
            "                expression = sanitize_clause(expression)",
            "            except QueryClauseValidationException as ex:",
            "                raise QueryObjectValidationError(ex.message) from ex",
            "        return expression",
            "",
            "    def make_sqla_column_compatible(",
            "        self, sqla_col: ColumnElement, label: Optional[str] = None",
            "    ) -> ColumnElement:",
            "        \"\"\"Takes a sqlalchemy column object and adds label info if supported by engine.",
            "        :param sqla_col: sqlalchemy column instance",
            "        :param label: alias/label that column is expected to have",
            "        :return: either a sql alchemy column or label instance if supported by engine",
            "        \"\"\"",
            "        label_expected = label or sqla_col.name",
            "        db_engine_spec = self.db_engine_spec",
            "        # add quotes to tables",
            "        if db_engine_spec.get_allows_alias_in_select(self.database):",
            "            label = db_engine_spec.make_label_compatible(label_expected)",
            "            sqla_col = sqla_col.label(label)",
            "        sqla_col.key = label_expected",
            "        return sqla_col",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        if sql_query_mutator := config[\"SQL_QUERY_MUTATOR\"]:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: Optional[str]) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    def get_query_str_extended(",
            "        self, query_obj: QueryObjectDict, mutate: bool = True",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> Union[str, int, float, bool, str]:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if isinstance(column_, dict):",
            "            if (",
            "                column_.get(\"type\")",
            "                and column_.get(\"is_temporal\")",
            "                and isinstance(value, str)",
            "            ):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.get(\"type\"), dateutil.parser.parse(value), db_extra=None",
            "                )",
            "",
            "                if sql:",
            "                    value = self.db_engine_spec.get_text_clause(sql)",
            "        else:",
            "            if column_.type and column_.is_temporal and isinstance(value, str):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "                )",
            "",
            "                if sql:",
            "                    value = self.text(sql)",
            "        return value",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def exc_query(self, qry: Any) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(qry)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> Optional[pd.DataFrame]:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: Optional[BaseTemplateProcessor] = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def get_from_clause(",
            "        self, template_processor: Optional[BaseTemplateProcessor] = None",
            "    ) -> tuple[Union[TableClause, Alias], Optional[str]]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            sa.table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, \"TableColumn\"],  # pylint: disable=unused-argument",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            sqla_column = sa.column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = self._process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return {}",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: Optional[FilterValues],",
            "        operator: str,",
            "        target_generic_type: utils.GenericDataType,",
            "        target_native_type: Optional[str] = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: Optional[",
            "            builtins.type[\"BaseEngineSpec\"]",
            "        ] = None,  # fix(hughhh): Optional[Type[BaseEngineSpec]]",
            "        db_extra: Optional[dict[str, Any]] = None,",
            "    ) -> Optional[FilterValues]:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: Optional[FilterValue]) -> Optional[FilterValue]:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, \"SqlMetric\"],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def adhoc_column_to_sqla(",
            "        self,",
            "        col: \"AdhocColumn\",  # type: ignore",
            "        force_type_check: bool = False,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        raise NotImplementedError()",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def dttm_sql_literal(self, dttm: datetime, col: \"TableColumn\") -> str:",
            "        \"\"\"Convert datetime object to a SQL expression string\"\"\"",
            "",
            "        sql = (",
            "            self.db_engine_spec.convert_dttm(col.type, dttm, db_extra=self.db_extra)",
            "            if col.type",
            "            else None",
            "        )",
            "",
            "        if sql:",
            "            return sql",
            "",
            "        tf = col.python_date_format",
            "",
            "        # Fallback to the default format (if defined).",
            "        if not tf and self.db_extra:",
            "            tf = self.db_extra.get(\"python_date_format_by_column_name\", {}).get(",
            "                col.column_name",
            "            )",
            "",
            "        if tf:",
            "            if tf in {\"epoch_ms\", \"epoch_s\"}:",
            "                seconds_since_epoch = int(dttm.timestamp())",
            "                if tf == \"epoch_s\":",
            "                    return str(seconds_since_epoch)",
            "                return str(seconds_since_epoch * 1000)",
            "            return f\"'{dttm.strftime(tf)}'\"",
            "",
            "        return f\"\"\"'{dttm.strftime(\"%Y-%m-%d %H:%M:%S.%f\")}'\"\"\"",
            "",
            "    def get_time_filter(  # pylint: disable=too-many-arguments",
            "        self,",
            "        time_col: \"TableColumn\",",
            "        start_dttm: Optional[sa.DateTime],",
            "        end_dttm: Optional[sa.DateTime],",
            "        time_grain: Optional[str] = None,",
            "        label: Optional[str] = \"__time\",",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        col = (",
            "            time_col.get_timestamp_expression(",
            "                time_grain=time_grain,",
            "                label=label,",
            "                template_processor=template_processor,",
            "            )",
            "            if time_grain",
            "            else self.convert_tbl_column_to_sqla_col(",
            "                time_col, label=label, template_processor=template_processor",
            "            )",
            "        )",
            "",
            "        l = []",
            "        if start_dttm:",
            "            l.append(",
            "                col",
            "                >= self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(start_dttm, time_col)",
            "                )",
            "            )",
            "        if end_dttm:",
            "            l.append(",
            "                col",
            "                < self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(end_dttm, time_col)",
            "                )",
            "            )",
            "        return and_(*l)",
            "",
            "    def values_for_column(",
            "        self, column_name: str, limit: int = 10000, denormalize_column: bool = False",
            "    ) -> list[Any]:",
            "        # denormalize column name before querying for values",
            "        # unless disabled in the dataset configuration",
            "        db_dialect = self.database.get_dialect()",
            "        column_name_ = (",
            "            self.database.db_engine_spec.denormalize_name(db_dialect, column_name)",
            "            if denormalize_column",
            "            else column_name",
            "        )",
            "        cols = {col.column_name: col for col in self.columns}",
            "        target_col = cols[column_name_]",
            "        tp = self.get_template_processor()",
            "        tbl, cte = self.get_from_clause(tp)",
            "",
            "        qry = (",
            "            sa.select(",
            "                # The alias (label) here is important because some dialects will",
            "                # automatically add a random alias to the projection because of the",
            "                # call to DISTINCT; others will uppercase the column names. This",
            "                # gives us a deterministic column name in the dataframe.",
            "                [target_col.get_sqla_col(template_processor=tp).label(\"column_values\")]",
            "            )",
            "            .select_from(tbl)",
            "            .distinct()",
            "        )",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "",
            "        if self.fetch_values_predicate:",
            "            qry = qry.where(self.get_fetch_values_predicate(template_processor=tp))",
            "",
            "        with self.database.get_sqla_engine_with_context() as engine:",
            "            sql = qry.compile(engine, compile_kwargs={\"literal_binds\": True})",
            "            sql = self._apply_cte(sql, cte)",
            "            sql = self.mutate_query_from_config(sql)",
            "",
            "            df = pd.read_sql_query(sql=sql, con=engine)",
            "            return df[\"column_values\"].to_list()",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        column: dict[str, Any],",
            "        time_grain: Optional[str],",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Union[TimestampExpression, Label]:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param column: column object",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "        column_spec = self.db_engine_spec.get_column_spec(column.get(\"type\"))",
            "        type_ = column_spec.sqla_type if column_spec else sa.DateTime",
            "        col = sa.column(column.get(\"column_name\"), type_=type_)",
            "",
            "        if template_processor:",
            "            expression = template_processor.process_template(column[\"column_name\"])",
            "            col = sa.literal_column(expression, type_=type_)",
            "",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, None, time_grain)",
            "        return self.make_sqla_column_compatible(time_expr, label)",
            "",
            "    def convert_tbl_column_to_sqla_col(",
            "        self,",
            "        tbl_column: \"TableColumn\",",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        label = label or tbl_column.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := tbl_column.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = sa.column(tbl_column.column_name, type_=type_)",
            "        col = self.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    def get_sqla_query(  # pylint: disable=too-many-arguments,too-many-locals,too-many-branches,too-many-statements",
            "        self,",
            "        apply_fetch_values_predicate: bool = False,",
            "        columns: Optional[list[Column]] = None,",
            "        extras: Optional[dict[str, Any]] = None,",
            "        filter: Optional[  # pylint: disable=redefined-builtin",
            "            list[utils.QueryObjectFilterClause]",
            "        ] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        granularity: Optional[str] = None,",
            "        groupby: Optional[list[Column]] = None,",
            "        inner_from_dttm: Optional[datetime] = None,",
            "        inner_to_dttm: Optional[datetime] = None,",
            "        is_rowcount: bool = False,",
            "        is_timeseries: bool = True,",
            "        metrics: Optional[list[Metric]] = None,",
            "        orderby: Optional[list[OrderBy]] = None,",
            "        order_desc: bool = True,",
            "        to_dttm: Optional[datetime] = None,",
            "        series_columns: Optional[list[Column]] = None,",
            "        series_limit: Optional[int] = None,",
            "        series_limit_metric: Optional[Metric] = None,",
            "        row_limit: Optional[int] = None,",
            "        row_offset: Optional[int] = None,",
            "        timeseries_limit: Optional[int] = None,",
            "        timeseries_limit_metric: Optional[Metric] = None,",
            "        time_shift: Optional[str] = None,",
            "    ) -> SqlaQuery:",
            "        \"\"\"Querying any sqla table from this common interface\"\"\"",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        extras = extras or {}",
            "        time_grain = extras.get(\"time_grain_sqla\")",
            "",
            "        template_kwargs = {",
            "            \"columns\": columns,",
            "            \"from_dttm\": from_dttm.isoformat() if from_dttm else None,",
            "            \"groupby\": groupby,",
            "            \"metrics\": metrics,",
            "            \"row_limit\": row_limit,",
            "            \"row_offset\": row_offset,",
            "            \"time_column\": granularity,",
            "            \"time_grain\": time_grain,",
            "            \"to_dttm\": to_dttm.isoformat() if to_dttm else None,",
            "            \"table_columns\": [col.column_name for col in self.columns],",
            "            \"filter\": filter,",
            "        }",
            "        columns = columns or []",
            "        groupby = groupby or []",
            "        rejected_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        applied_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        db_engine_spec = self.db_engine_spec",
            "        series_column_labels = [",
            "            db_engine_spec.make_label_compatible(column)",
            "            for column in utils.get_column_names(",
            "                columns=series_columns or [],",
            "            )",
            "        ]",
            "        # deprecated, to be removed in 2.0",
            "        if is_timeseries and timeseries_limit:",
            "            series_limit = timeseries_limit",
            "        series_limit_metric = series_limit_metric or timeseries_limit_metric",
            "        template_kwargs.update(self.template_params_dict)",
            "        extra_cache_keys: list[Any] = []",
            "        template_kwargs[\"extra_cache_keys\"] = extra_cache_keys",
            "        removed_filters: list[str] = []",
            "        applied_template_filters: list[str] = []",
            "        template_kwargs[\"removed_filters\"] = removed_filters",
            "        template_kwargs[\"applied_filters\"] = applied_template_filters",
            "        template_processor = self.get_template_processor(**template_kwargs)",
            "        prequeries: list[str] = []",
            "        orderby = orderby or []",
            "        need_groupby = bool(metrics is not None or groupby)",
            "        metrics = metrics or []",
            "",
            "        # For backward compatibility",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        columns_by_name: dict[str, \"TableColumn\"] = {",
            "            col.column_name: col for col in self.columns",
            "        }",
            "",
            "        metrics_by_name: dict[str, \"SqlMetric\"] = {",
            "            m.metric_name: m for m in self.metrics",
            "        }",
            "",
            "        if not granularity and is_timeseries:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Datetime column not provided as part table configuration \"",
            "                    \"and is required by this type of chart\"",
            "                )",
            "            )",
            "        if not metrics and not columns and not groupby:",
            "            raise QueryObjectValidationError(_(\"Empty query?\"))",
            "",
            "        metrics_exprs: list[ColumnElement] = []",
            "        for metric in metrics:",
            "            if utils.is_adhoc_metric(metric):",
            "                assert isinstance(metric, dict)",
            "                metrics_exprs.append(",
            "                    self.adhoc_metric_to_sqla(",
            "                        metric=metric,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "            elif isinstance(metric, str) and metric in metrics_by_name:",
            "                metrics_exprs.append(",
            "                    metrics_by_name[metric].get_sqla_col(",
            "                        template_processor=template_processor",
            "                    )",
            "                )",
            "            else:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Metric '%(metric)s' does not exist\", metric=metric)",
            "                )",
            "",
            "        if metrics_exprs:",
            "            main_metric_expr = metrics_exprs[0]",
            "        else:",
            "            main_metric_expr, label = literal_column(\"COUNT(*)\"), \"ccount\"",
            "            main_metric_expr = self.make_sqla_column_compatible(main_metric_expr, label)",
            "",
            "        # To ensure correct handling of the ORDER BY labeling we need to reference the",
            "        # metric instance if defined in the SELECT clause.",
            "        # use the key of the ColumnClause for the expected label",
            "        metrics_exprs_by_label = {m.key: m for m in metrics_exprs}",
            "        metrics_exprs_by_expr = {str(m): m for m in metrics_exprs}",
            "",
            "        # Since orderby may use adhoc metrics, too; we need to process them first",
            "        orderby_exprs: list[ColumnElement] = []",
            "        for orig_col, ascending in orderby:",
            "            col: Union[AdhocMetric, ColumnElement] = orig_col",
            "            if isinstance(col, dict):",
            "                col = cast(AdhocMetric, col)",
            "                if col.get(\"sqlExpression\"):",
            "                    col[\"sqlExpression\"] = self._process_sql_expression(",
            "                        expression=col[\"sqlExpression\"],",
            "                        database_id=self.database_id,",
            "                        schema=self.schema,",
            "                        template_processor=template_processor,",
            "                    )",
            "                if utils.is_adhoc_metric(col):",
            "                    # add adhoc sort by column to columns_by_name if not exists",
            "                    col = self.adhoc_metric_to_sqla(col, columns_by_name)",
            "                    # if the adhoc metric has been defined before",
            "                    # use the existing instance.",
            "                    col = metrics_exprs_by_expr.get(str(col), col)",
            "                    need_groupby = True",
            "            elif col in columns_by_name:",
            "                col = self.convert_tbl_column_to_sqla_col(",
            "                    columns_by_name[col], template_processor=template_processor",
            "                )",
            "            elif col in metrics_exprs_by_label:",
            "                col = metrics_exprs_by_label[col]",
            "                need_groupby = True",
            "            elif col in metrics_by_name:",
            "                col = metrics_by_name[col].get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "                need_groupby = True",
            "",
            "            if isinstance(col, ColumnElement):",
            "                orderby_exprs.append(col)",
            "            else:",
            "                # Could not convert a column reference to valid ColumnElement",
            "                raise QueryObjectValidationError(",
            "                    _(\"Unknown column used in orderby: %(col)s\", col=orig_col)",
            "                )",
            "",
            "        select_exprs: list[Union[Column, Label]] = []",
            "        groupby_all_columns = {}",
            "        groupby_series_columns = {}",
            "",
            "        # filter out the pseudo column  __timestamp from columns",
            "        columns = [col for col in columns if col != utils.DTTM_ALIAS]",
            "        dttm_col = columns_by_name.get(granularity) if granularity else None",
            "",
            "        if need_groupby:",
            "            # dedup columns while preserving order",
            "            columns = groupby or columns",
            "            for selected in columns:",
            "                if isinstance(selected, str):",
            "                    # if groupby field/expr equals granularity field/expr",
            "                    if selected == granularity:",
            "                        table_col = columns_by_name[selected]",
            "                        outer = table_col.get_timestamp_expression(",
            "                            time_grain=time_grain,",
            "                            label=selected,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    # if groupby field equals a selected column",
            "                    elif selected in columns_by_name:",
            "                        outer = self.convert_tbl_column_to_sqla_col(",
            "                            columns_by_name[selected],",
            "                            template_processor=template_processor,",
            "                        )",
            "                    else:",
            "                        selected = validate_adhoc_subquery(",
            "                            selected,",
            "                            self.database_id,",
            "                            self.schema,",
            "                        )",
            "                        outer = literal_column(f\"({selected})\")",
            "                        outer = self.make_sqla_column_compatible(outer, selected)",
            "                else:",
            "                    outer = self.adhoc_column_to_sqla(",
            "                        col=selected, template_processor=template_processor",
            "                    )",
            "                groupby_all_columns[outer.name] = outer",
            "                if (",
            "                    is_timeseries and not series_column_labels",
            "                ) or outer.name in series_column_labels:",
            "                    groupby_series_columns[outer.name] = outer",
            "                select_exprs.append(outer)",
            "        elif columns:",
            "            for selected in columns:",
            "                if is_adhoc_column(selected):",
            "                    _sql = selected[\"sqlExpression\"]",
            "                    _column_label = selected[\"label\"]",
            "                elif isinstance(selected, str):",
            "                    _sql = selected",
            "                    _column_label = selected",
            "",
            "                selected = validate_adhoc_subquery(",
            "                    _sql,",
            "                    self.database_id,",
            "                    self.schema,",
            "                )",
            "",
            "                select_exprs.append(",
            "                    self.convert_tbl_column_to_sqla_col(",
            "                        columns_by_name[selected], template_processor=template_processor",
            "                    )",
            "                    if isinstance(selected, str) and selected in columns_by_name",
            "                    else self.make_sqla_column_compatible(",
            "                        literal_column(selected), _column_label",
            "                    )",
            "                )",
            "            metrics_exprs = []",
            "",
            "        if granularity:",
            "            if granularity not in columns_by_name or not dttm_col:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        'Time column \"%(col)s\" does not exist in dataset',",
            "                        col=granularity,",
            "                    )",
            "                )",
            "            time_filters = []",
            "",
            "            if is_timeseries:",
            "                timestamp = dttm_col.get_timestamp_expression(",
            "                    time_grain=time_grain, template_processor=template_processor",
            "                )",
            "                # always put timestamp as the first column",
            "                select_exprs.insert(0, timestamp)",
            "                groupby_all_columns[timestamp.name] = timestamp",
            "",
            "            # Use main dttm column to support index with secondary dttm columns.",
            "            if (",
            "                self.always_filter_main_dttm",
            "                and self.main_dttm_col in self.dttm_cols",
            "                and self.main_dttm_col != dttm_col.column_name",
            "            ):",
            "                time_filters.append(",
            "                    self.get_time_filter(",
            "                        time_col=columns_by_name[self.main_dttm_col],",
            "                        start_dttm=from_dttm,",
            "                        end_dttm=to_dttm,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "",
            "            time_filter_column = self.get_time_filter(",
            "                time_col=dttm_col,",
            "                start_dttm=from_dttm,",
            "                end_dttm=to_dttm,",
            "                template_processor=template_processor,",
            "            )",
            "            time_filters.append(time_filter_column)",
            "",
            "        # Always remove duplicates by column name, as sometimes `metrics_exprs`",
            "        # can have the same name as a groupby column (e.g. when users use",
            "        # raw columns as custom SQL adhoc metric).",
            "        select_exprs = remove_duplicates(",
            "            select_exprs + metrics_exprs, key=lambda x: x.name",
            "        )",
            "",
            "        # Expected output columns",
            "        labels_expected = [c.key for c in select_exprs]",
            "",
            "        # Order by columns are \"hidden\" columns, some databases require them",
            "        # always be present in SELECT if an aggregation function is used",
            "        if not db_engine_spec.allows_hidden_orderby_agg:",
            "            select_exprs = remove_duplicates(select_exprs + orderby_exprs)",
            "",
            "        qry = sa.select(select_exprs)",
            "",
            "        tbl, cte = self.get_from_clause(template_processor)",
            "",
            "        if groupby_all_columns:",
            "            qry = qry.group_by(*groupby_all_columns.values())",
            "",
            "        where_clause_and = []",
            "        having_clause_and = []",
            "",
            "        for flt in filter:  # type: ignore",
            "            if not all(flt.get(s) for s in [\"col\", \"op\"]):",
            "                continue",
            "            flt_col = flt[\"col\"]",
            "            val = flt.get(\"val\")",
            "            flt_grain = flt.get(\"grain\")",
            "            op = flt[\"op\"].upper()",
            "            col_obj: Optional[\"TableColumn\"] = None",
            "            sqla_col: Optional[Column] = None",
            "            if flt_col == utils.DTTM_ALIAS and is_timeseries and dttm_col:",
            "                col_obj = dttm_col",
            "            elif is_adhoc_column(flt_col):",
            "                try:",
            "                    sqla_col = self.adhoc_column_to_sqla(flt_col, force_type_check=True)",
            "                    applied_adhoc_filters_columns.append(flt_col)",
            "                except ColumnNotFoundException:",
            "                    rejected_adhoc_filters_columns.append(flt_col)",
            "                    continue",
            "            else:",
            "                col_obj = columns_by_name.get(cast(str, flt_col))",
            "            filter_grain = flt.get(\"grain\")",
            "",
            "            if is_feature_enabled(\"ENABLE_TEMPLATE_REMOVE_FILTERS\"):",
            "                if get_column_name(flt_col) in removed_filters:",
            "                    # Skip generating SQLA filter when the jinja template handles it.",
            "                    continue",
            "",
            "            if col_obj or sqla_col is not None:",
            "                if sqla_col is not None:",
            "                    pass",
            "                elif col_obj and filter_grain:",
            "                    sqla_col = col_obj.get_timestamp_expression(",
            "                        time_grain=filter_grain, template_processor=template_processor",
            "                    )",
            "                elif col_obj:",
            "                    sqla_col = self.convert_tbl_column_to_sqla_col(",
            "                        tbl_column=col_obj, template_processor=template_processor",
            "                    )",
            "                col_type = col_obj.type if col_obj else None",
            "                col_spec = db_engine_spec.get_column_spec(native_type=col_type)",
            "                is_list_target = op in (",
            "                    utils.FilterOperator.IN.value,",
            "                    utils.FilterOperator.NOT_IN.value,",
            "                )",
            "",
            "                col_advanced_data_type = col_obj.advanced_data_type if col_obj else \"\"",
            "",
            "                if col_spec and not col_advanced_data_type:",
            "                    target_generic_type = col_spec.generic_type",
            "                else:",
            "                    target_generic_type = GenericDataType.STRING",
            "                eq = self.filter_values_handler(",
            "                    values=val,",
            "                    operator=op,",
            "                    target_generic_type=target_generic_type,",
            "                    target_native_type=col_type,",
            "                    is_list_target=is_list_target,",
            "                    db_engine_spec=db_engine_spec,",
            "                )",
            "                if (",
            "                    col_advanced_data_type != \"\"",
            "                    and feature_flag_manager.is_feature_enabled(",
            "                        \"ENABLE_ADVANCED_DATA_TYPES\"",
            "                    )",
            "                    and col_advanced_data_type in ADVANCED_DATA_TYPES",
            "                ):",
            "                    values = eq if is_list_target else [eq]  # type: ignore",
            "                    bus_resp: AdvancedDataTypeResponse = ADVANCED_DATA_TYPES[",
            "                        col_advanced_data_type",
            "                    ].translate_type(",
            "                        {",
            "                            \"type\": col_advanced_data_type,",
            "                            \"values\": values,",
            "                        }",
            "                    )",
            "                    if bus_resp[\"error_message\"]:",
            "                        raise AdvancedDataTypeResponseError(",
            "                            _(bus_resp[\"error_message\"])",
            "                        )",
            "",
            "                    where_clause_and.append(",
            "                        ADVANCED_DATA_TYPES[col_advanced_data_type].translate_filter(",
            "                            sqla_col, op, bus_resp[\"values\"]",
            "                        )",
            "                    )",
            "                elif is_list_target:",
            "                    assert isinstance(eq, (tuple, list))",
            "                    if len(eq) == 0:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Filter value list cannot be empty\")",
            "                        )",
            "                    if len(eq) > len(",
            "                        eq_without_none := [x for x in eq if x is not None]",
            "                    ):",
            "                        is_null_cond = sqla_col.is_(None)",
            "                        if eq:",
            "                            cond = or_(is_null_cond, sqla_col.in_(eq_without_none))",
            "                        else:",
            "                            cond = is_null_cond",
            "                    else:",
            "                        cond = sqla_col.in_(eq)",
            "                    if op == utils.FilterOperator.NOT_IN.value:",
            "                        cond = ~cond",
            "                    where_clause_and.append(cond)",
            "                elif op == utils.FilterOperator.IS_NULL.value:",
            "                    where_clause_and.append(sqla_col.is_(None))",
            "                elif op == utils.FilterOperator.IS_NOT_NULL.value:",
            "                    where_clause_and.append(sqla_col.isnot(None))",
            "                elif op == utils.FilterOperator.IS_TRUE.value:",
            "                    where_clause_and.append(sqla_col.is_(True))",
            "                elif op == utils.FilterOperator.IS_FALSE.value:",
            "                    where_clause_and.append(sqla_col.is_(False))",
            "                else:",
            "                    if (",
            "                        op",
            "                        not in {",
            "                            utils.FilterOperator.EQUALS.value,",
            "                            utils.FilterOperator.NOT_EQUALS.value,",
            "                        }",
            "                        and eq is None",
            "                    ):",
            "                        raise QueryObjectValidationError(",
            "                            _(",
            "                                \"Must specify a value for filters \"",
            "                                \"with comparison operators\"",
            "                            )",
            "                        )",
            "                    if op == utils.FilterOperator.EQUALS.value:",
            "                        where_clause_and.append(sqla_col == eq)",
            "                    elif op == utils.FilterOperator.NOT_EQUALS.value:",
            "                        where_clause_and.append(sqla_col != eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN.value:",
            "                        where_clause_and.append(sqla_col > eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN.value:",
            "                        where_clause_and.append(sqla_col < eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col >= eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col <= eq)",
            "                    elif op in {",
            "                        utils.FilterOperator.ILIKE.value,",
            "                        utils.FilterOperator.LIKE.value,",
            "                    }:",
            "                        if target_generic_type != GenericDataType.STRING:",
            "                            sqla_col = sa.cast(sqla_col, sa.String)",
            "",
            "                        if op == utils.FilterOperator.LIKE.value:",
            "                            where_clause_and.append(sqla_col.like(eq))",
            "                        else:",
            "                            where_clause_and.append(sqla_col.ilike(eq))",
            "                    elif (",
            "                        op == utils.FilterOperator.TEMPORAL_RANGE.value",
            "                        and isinstance(eq, str)",
            "                        and col_obj is not None",
            "                    ):",
            "                        _since, _until = get_since_until_from_time_range(",
            "                            time_range=eq,",
            "                            time_shift=time_shift,",
            "                            extras=extras,",
            "                        )",
            "                        where_clause_and.append(",
            "                            self.get_time_filter(",
            "                                time_col=col_obj,",
            "                                start_dttm=_since,",
            "                                end_dttm=_until,",
            "                                time_grain=flt_grain,",
            "                                label=sqla_col.key,",
            "                                template_processor=template_processor,",
            "                            )",
            "                        )",
            "                    else:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Invalid filter operation type: %(op)s\", op=op)",
            "                        )",
            "        where_clause_and += self.get_sqla_row_level_filters(template_processor)",
            "        if extras:",
            "            where = extras.get(\"where\")",
            "            if where:",
            "                try:",
            "                    where = template_processor.process_template(f\"({where})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in WHERE clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                where = self._process_sql_expression(",
            "                    expression=where,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                where_clause_and += [self.text(where)]",
            "            having = extras.get(\"having\")",
            "            if having:",
            "                try:",
            "                    having = template_processor.process_template(f\"({having})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in HAVING clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                having = self._process_sql_expression(",
            "                    expression=having,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                having_clause_and += [self.text(having)]",
            "",
            "        if apply_fetch_values_predicate and self.fetch_values_predicate:",
            "            qry = qry.where(",
            "                self.get_fetch_values_predicate(template_processor=template_processor)",
            "            )",
            "        if granularity:",
            "            qry = qry.where(and_(*(time_filters + where_clause_and)))",
            "        else:",
            "            qry = qry.where(and_(*where_clause_and))",
            "        qry = qry.having(and_(*having_clause_and))",
            "",
            "        self.make_orderby_compatible(select_exprs, orderby_exprs)",
            "",
            "        for col, (orig_col, ascending) in zip(orderby_exprs, orderby):",
            "            if not db_engine_spec.allows_alias_in_orderby and isinstance(col, Label):",
            "                # if engine does not allow using SELECT alias in ORDER BY",
            "                # revert to the underlying column",
            "                col = col.element",
            "",
            "            if (",
            "                db_engine_spec.get_allows_alias_in_select(self.database)",
            "                and db_engine_spec.allows_hidden_cc_in_orderby",
            "                and col.name in [select_col.name for select_col in select_exprs]",
            "            ):",
            "                with self.database.get_sqla_engine_with_context() as engine:",
            "                    quote = engine.dialect.identifier_preparer.quote",
            "                    col = literal_column(quote(col.name))",
            "            direction = sa.asc if ascending else sa.desc",
            "            qry = qry.order_by(direction(col))",
            "",
            "        if row_limit:",
            "            qry = qry.limit(row_limit)",
            "        if row_offset:",
            "            qry = qry.offset(row_offset)",
            "",
            "        if series_limit and groupby_series_columns:",
            "            if db_engine_spec.allows_joins and db_engine_spec.allows_subqueries:",
            "                # some sql dialects require for order by expressions",
            "                # to also be in the select clause -- others, e.g. vertica,",
            "                # require a unique inner alias",
            "                inner_main_metric_expr = self.make_sqla_column_compatible(",
            "                    main_metric_expr, \"mme_inner__\"",
            "                )",
            "                inner_groupby_exprs = []",
            "                inner_select_exprs = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    inner = self.make_sqla_column_compatible(gby_obj, gby_name + \"__\")",
            "                    inner_groupby_exprs.append(inner)",
            "                    inner_select_exprs.append(inner)",
            "",
            "                inner_select_exprs += [inner_main_metric_expr]",
            "                subq = sa.select(inner_select_exprs).select_from(tbl)",
            "                inner_time_filter = []",
            "",
            "                if dttm_col and not db_engine_spec.time_groupby_inline:",
            "                    inner_time_filter = [",
            "                        self.get_time_filter(",
            "                            time_col=dttm_col,",
            "                            start_dttm=inner_from_dttm or from_dttm,",
            "                            end_dttm=inner_to_dttm or to_dttm,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    ]",
            "                subq = subq.where(and_(*(where_clause_and + inner_time_filter)))",
            "                subq = subq.group_by(*inner_groupby_exprs)",
            "",
            "                ob = inner_main_metric_expr",
            "                if series_limit_metric:",
            "                    ob = self._get_series_orderby(",
            "                        series_limit_metric=series_limit_metric,",
            "                        metrics_by_name=metrics_by_name,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                direction = sa.desc if order_desc else sa.asc",
            "                subq = subq.order_by(direction(ob))",
            "                subq = subq.limit(series_limit)",
            "",
            "                on_clause = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    # in this case the column name, not the alias, needs to be",
            "                    # conditionally mutated, as it refers to the column alias in",
            "                    # the inner query",
            "                    col_name = db_engine_spec.make_label_compatible(gby_name + \"__\")",
            "                    on_clause.append(gby_obj == sa.column(col_name))",
            "",
            "                tbl = tbl.join(subq.alias(SERIES_LIMIT_SUBQ_ALIAS), and_(*on_clause))",
            "            else:",
            "                if series_limit_metric:",
            "                    orderby = [",
            "                        (",
            "                            self._get_series_orderby(",
            "                                series_limit_metric=series_limit_metric,",
            "                                metrics_by_name=metrics_by_name,",
            "                                columns_by_name=columns_by_name,",
            "                                template_processor=template_processor,",
            "                            ),",
            "                            not order_desc,",
            "                        )",
            "                    ]",
            "",
            "                # run prequery to get top groups",
            "                prequery_obj = {",
            "                    \"is_timeseries\": False,",
            "                    \"row_limit\": series_limit,",
            "                    \"metrics\": metrics,",
            "                    \"granularity\": granularity,",
            "                    \"groupby\": groupby,",
            "                    \"from_dttm\": inner_from_dttm or from_dttm,",
            "                    \"to_dttm\": inner_to_dttm or to_dttm,",
            "                    \"filter\": filter,",
            "                    \"orderby\": orderby,",
            "                    \"extras\": extras,",
            "                    \"columns\": columns,",
            "                    \"order_desc\": True,",
            "                }",
            "",
            "                result = self.query(prequery_obj)",
            "                prequeries.append(result.query)",
            "                dimensions = [",
            "                    c",
            "                    for c in result.df.columns",
            "                    if c not in metrics and c in groupby_series_columns",
            "                ]",
            "                top_groups = self._get_top_groups(",
            "                    result.df, dimensions, groupby_series_columns, columns_by_name",
            "                )",
            "                qry = qry.where(top_groups)",
            "",
            "        qry = qry.select_from(tbl)",
            "",
            "        if is_rowcount:",
            "            if not db_engine_spec.allows_subqueries:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Database does not support subqueries\")",
            "                )",
            "            label = \"rowcount\"",
            "            col = self.make_sqla_column_compatible(literal_column(\"COUNT(*)\"), label)",
            "            qry = sa.select([col]).select_from(qry.alias(\"rowcount_qry\"))",
            "            labels_expected = [label]",
            "",
            "        filter_columns = [flt.get(\"col\") for flt in filter] if filter else []",
            "        rejected_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and col not in self.column_names",
            "            and col not in applied_template_filters",
            "        ] + rejected_adhoc_filters_columns",
            "",
            "        applied_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and (col in self.column_names or col in applied_template_filters)",
            "        ] + applied_adhoc_filters_columns",
            "",
            "        return SqlaQuery(",
            "            applied_template_filters=applied_template_filters,",
            "            cte=cte,",
            "            applied_filter_columns=applied_filter_columns,",
            "            rejected_filter_columns=rejected_filter_columns,",
            "            extra_cache_keys=extra_cache_keys,",
            "            labels_expected=labels_expected,",
            "            sqla_query=qry,",
            "            prequeries=prequeries,",
            "        )"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"a collection of model-related helper classes and functions\"\"\"",
            "import builtins",
            "import dataclasses",
            "import json",
            "import logging",
            "import re",
            "import uuid",
            "from collections import defaultdict",
            "from collections.abc import Hashable",
            "from datetime import datetime, timedelta",
            "from json.decoder import JSONDecodeError",
            "from typing import Any, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "import dateutil.parser",
            "import humanize",
            "import numpy as np",
            "import pandas as pd",
            "import pytz",
            "import sqlalchemy as sa",
            "import sqlparse",
            "import yaml",
            "from flask import escape, g, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.models.decorators import renders",
            "from flask_appbuilder.models.mixins import AuditMixin",
            "from flask_appbuilder.security.sqla.models import User",
            "from flask_babel import lazy_gettext as _",
            "from jinja2.exceptions import TemplateError",
            "from sqlalchemy import and_, Column, or_, UniqueConstraint",
            "from sqlalchemy.ext.declarative import declared_attr",
            "from sqlalchemy.orm import Mapper, Session, validates",
            "from sqlalchemy.orm.exc import MultipleResultsFound",
            "from sqlalchemy.sql.elements import ColumnElement, literal_column, TextClause",
            "from sqlalchemy.sql.expression import Label, Select, TextAsFrom",
            "from sqlalchemy.sql.selectable import Alias, TableClause",
            "from sqlalchemy_utils import UUIDType",
            "",
            "from superset import app, is_feature_enabled, security_manager",
            "from superset.advanced_data_type.types import AdvancedDataTypeResponse",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.common.utils.time_range_utils import get_since_until_from_time_range",
            "from superset.constants import EMPTY_STRING, NULL_STRING",
            "from superset.db_engine_specs.base import TimestampExpression",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    AdvancedDataTypeResponseError,",
            "    ColumnNotFoundException,",
            "    QueryClauseValidationException,",
            "    QueryObjectValidationError,",
            "    SupersetSecurityException,",
            ")",
            "from superset.extensions import feature_flag_manager",
            "from superset.jinja_context import BaseTemplateProcessor",
            "from superset.sql_parse import (",
            "    has_table_query,",
            "    insert_rls_in_predicate,",
            "    ParsedQuery,",
            "    sanitize_clause,",
            ")",
            "from superset.superset_typing import (",
            "    AdhocMetric,",
            "    Column as ColumnTyping,",
            "    FilterValue,",
            "    FilterValues,",
            "    Metric,",
            "    OrderBy,",
            "    QueryObjectDict,",
            ")",
            "from superset.utils import core as utils",
            "from superset.utils.core import (",
            "    GenericDataType,",
            "    get_column_name,",
            "    get_user_id,",
            "    is_adhoc_column,",
            "    remove_duplicates,",
            ")",
            "from superset.utils.dates import datetime_to_epoch",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import SqlMetric, TableColumn",
            "    from superset.db_engine_specs import BaseEngineSpec",
            "    from superset.models.core import Database",
            "",
            "",
            "config = app.config",
            "logger = logging.getLogger(__name__)",
            "",
            "VIRTUAL_TABLE_ALIAS = \"virtual_table\"",
            "SERIES_LIMIT_SUBQ_ALIAS = \"series_limit\"",
            "ADVANCED_DATA_TYPES = config[\"ADVANCED_DATA_TYPES\"]",
            "",
            "",
            "def validate_adhoc_subquery(",
            "    sql: str,",
            "    database_id: int,",
            "    default_schema: str,",
            ") -> str:",
            "    \"\"\"",
            "    Check if adhoc SQL contains sub-queries or nested sub-queries with table.",
            "",
            "    If sub-queries are allowed, the adhoc SQL is modified to insert any applicable RLS",
            "    predicates to it.",
            "",
            "    :param sql: adhoc sql expression",
            "    :raise SupersetSecurityException if sql contains sub-queries or",
            "    nested sub-queries with table",
            "    \"\"\"",
            "    statements = []",
            "    for statement in sqlparse.parse(sql):",
            "        if has_table_query(statement):",
            "            if not is_feature_enabled(\"ALLOW_ADHOC_SUBQUERY\"):",
            "                raise SupersetSecurityException(",
            "                    SupersetError(",
            "                        error_type=SupersetErrorType.ADHOC_SUBQUERY_NOT_ALLOWED_ERROR,",
            "                        message=_(\"Custom SQL fields cannot contain sub-queries.\"),",
            "                        level=ErrorLevel.ERROR,",
            "                    )",
            "                )",
            "            statement = insert_rls_in_predicate(statement, database_id, default_schema)",
            "        statements.append(statement)",
            "",
            "    return \";\\n\".join(str(statement) for statement in statements)",
            "",
            "",
            "def json_to_dict(json_str: str) -> dict[Any, Any]:",
            "    if json_str:",
            "        val = re.sub(\",[ \\t\\r\\n]+}\", \"}\", json_str)",
            "        val = re.sub(\",[ \\t\\r\\n]+\\\\]\", \"]\", val)",
            "        return json.loads(val)",
            "",
            "    return {}",
            "",
            "",
            "def convert_uuids(obj: Any) -> Any:",
            "    \"\"\"",
            "    Convert UUID objects to str so we can use yaml.safe_dump",
            "    \"\"\"",
            "    if isinstance(obj, uuid.UUID):",
            "        return str(obj)",
            "",
            "    if isinstance(obj, list):",
            "        return [convert_uuids(el) for el in obj]",
            "",
            "    if isinstance(obj, dict):",
            "        return {k: convert_uuids(v) for k, v in obj.items()}",
            "",
            "    return obj",
            "",
            "",
            "class ImportExportMixin:",
            "    uuid = sa.Column(",
            "        UUIDType(binary=True), primary_key=False, unique=True, default=uuid.uuid4",
            "    )",
            "",
            "    export_parent: Optional[str] = None",
            "    # The name of the attribute",
            "    # with the SQL Alchemy back reference",
            "",
            "    export_children: list[str] = []",
            "    # List of (str) names of attributes",
            "    # with the SQL Alchemy forward references",
            "",
            "    export_fields: list[str] = []",
            "    # The names of the attributes",
            "    # that are available for import and export",
            "",
            "    extra_import_fields: list[str] = []",
            "    # Additional fields that should be imported,",
            "    # even though they were not exported",
            "",
            "    __mapper__: Mapper",
            "",
            "    @classmethod",
            "    def _unique_constraints(cls) -> list[set[str]]:",
            "        \"\"\"Get all (single column and multi column) unique constraints\"\"\"",
            "        unique = [",
            "            {c.name for c in u.columns}",
            "            for u in cls.__table_args__  # type: ignore",
            "            if isinstance(u, UniqueConstraint)",
            "        ]",
            "        unique.extend(",
            "            {c.name} for c in cls.__table__.columns if c.unique  # type: ignore",
            "        )",
            "        return unique",
            "",
            "    @classmethod",
            "    def parent_foreign_key_mappings(cls) -> dict[str, str]:",
            "        \"\"\"Get a mapping of foreign name to the local name of foreign keys\"\"\"",
            "        parent_rel = cls.__mapper__.relationships.get(cls.export_parent)",
            "        if parent_rel:",
            "            return {l.name: r.name for (l, r) in parent_rel.local_remote_pairs}",
            "        return {}",
            "",
            "    @classmethod",
            "    def export_schema(",
            "        cls, recursive: bool = True, include_parent_ref: bool = False",
            "    ) -> dict[str, Any]:",
            "        \"\"\"Export schema as a dictionary\"\"\"",
            "        parent_excludes = set()",
            "        if not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {column.name for column in parent_ref.local_columns}",
            "",
            "        def formatter(column: sa.Column) -> str:",
            "            return (",
            "                f\"{str(column.type)} Default ({column.default.arg})\"",
            "                if column.default",
            "                else str(column.type)",
            "            )",
            "",
            "        schema: dict[str, Any] = {",
            "            column.name: formatter(column)",
            "            for column in cls.__table__.columns  # type: ignore",
            "            if (column.name in cls.export_fields and column.name not in parent_excludes)",
            "        }",
            "        if recursive:",
            "            for column in cls.export_children:",
            "                child_class = cls.__mapper__.relationships[column].argument.class_",
            "                schema[column] = [",
            "                    child_class.export_schema(",
            "                        recursive=recursive, include_parent_ref=include_parent_ref",
            "                    )",
            "                ]",
            "        return schema",
            "",
            "    @classmethod",
            "    def import_from_dict(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        cls,",
            "        session: Session,",
            "        dict_rep: dict[Any, Any],",
            "        parent: Optional[Any] = None,",
            "        recursive: bool = True,",
            "        sync: Optional[list[str]] = None,",
            "        allow_reparenting: bool = False,",
            "    ) -> Any:",
            "        \"\"\"Import obj from a dictionary\"\"\"",
            "        if sync is None:",
            "            sync = []",
            "        parent_refs = cls.parent_foreign_key_mappings()",
            "        export_fields = (",
            "            set(cls.export_fields)",
            "            | set(cls.extra_import_fields)",
            "            | set(parent_refs.keys())",
            "            | {\"uuid\"}",
            "        )",
            "        new_children = {c: dict_rep[c] for c in cls.export_children if c in dict_rep}",
            "        unique_constraints = cls._unique_constraints()",
            "",
            "        filters = []  # Using these filters to check if obj already exists",
            "",
            "        # Remove fields that should not get imported",
            "        for k in list(dict_rep):",
            "            if k not in export_fields and k not in parent_refs:",
            "                del dict_rep[k]",
            "",
            "        if not parent:",
            "            if cls.export_parent:",
            "                for prnt in parent_refs.keys():",
            "                    if prnt not in dict_rep:",
            "                        raise RuntimeError(f\"{cls.__name__}: Missing field {prnt}\")",
            "        else:",
            "            # Set foreign keys to parent obj",
            "            for k, v in parent_refs.items():",
            "                dict_rep[k] = getattr(parent, v)",
            "",
            "        if not allow_reparenting:",
            "            # Add filter for parent obj",
            "            filters.extend(",
            "                [getattr(cls, k) == dict_rep.get(k) for k in parent_refs.keys()]",
            "            )",
            "",
            "        # Add filter for unique constraints",
            "        ucs = [",
            "            and_(",
            "                *[",
            "                    getattr(cls, k) == dict_rep.get(k)",
            "                    for k in cs",
            "                    if dict_rep.get(k) is not None",
            "                ]",
            "            )",
            "            for cs in unique_constraints",
            "        ]",
            "        filters.append(or_(*ucs))",
            "",
            "        # Check if object already exists in DB, break if more than one is found",
            "        try:",
            "            obj_query = session.query(cls).filter(and_(*filters))",
            "            obj = obj_query.one_or_none()",
            "        except MultipleResultsFound as ex:",
            "            logger.error(",
            "                \"Error importing %s \\n %s \\n %s\",",
            "                cls.__name__,",
            "                str(obj_query),",
            "                yaml.safe_dump(dict_rep),",
            "                exc_info=True,",
            "            )",
            "            raise ex",
            "",
            "        if not obj:",
            "            is_new_obj = True",
            "            # Create new DB object",
            "            obj = cls(**dict_rep)",
            "            logger.info(\"Importing new %s %s\", obj.__tablename__, str(obj))",
            "            if cls.export_parent and parent:",
            "                setattr(obj, cls.export_parent, parent)",
            "            session.add(obj)",
            "        else:",
            "            is_new_obj = False",
            "            logger.info(\"Updating %s %s\", obj.__tablename__, str(obj))",
            "            # Update columns",
            "            for k, v in dict_rep.items():",
            "                setattr(obj, k, v)",
            "",
            "        # Recursively create children",
            "        if recursive:",
            "            for child in cls.export_children:",
            "                argument = cls.__mapper__.relationships[child].argument",
            "                child_class = (",
            "                    argument.class_ if hasattr(argument, \"class_\") else argument",
            "                )",
            "                added = []",
            "                for c_obj in new_children.get(child, []):",
            "                    added.append(",
            "                        child_class.import_from_dict(",
            "                            session=session, dict_rep=c_obj, parent=obj, sync=sync",
            "                        )",
            "                    )",
            "                # If children should get synced, delete the ones that did not",
            "                # get updated.",
            "                if child in sync and not is_new_obj:",
            "                    back_refs = child_class.parent_foreign_key_mappings()",
            "                    delete_filters = [",
            "                        getattr(child_class, k) == getattr(obj, back_refs.get(k))",
            "                        for k in back_refs.keys()",
            "                    ]",
            "                    to_delete = set(",
            "                        session.query(child_class).filter(and_(*delete_filters))",
            "                    ).difference(set(added))",
            "                    for o in to_delete:",
            "                        logger.info(\"Deleting %s %s\", child, str(obj))",
            "                        session.delete(o)",
            "",
            "        return obj",
            "",
            "    def export_to_dict(",
            "        self,",
            "        recursive: bool = True,",
            "        include_parent_ref: bool = False,",
            "        include_defaults: bool = False,",
            "        export_uuids: bool = False,",
            "    ) -> dict[Any, Any]:",
            "        \"\"\"Export obj to dictionary\"\"\"",
            "        export_fields = set(self.export_fields)",
            "        if export_uuids:",
            "            export_fields.add(\"uuid\")",
            "            if \"id\" in export_fields:",
            "                export_fields.remove(\"id\")",
            "",
            "        cls = self.__class__",
            "        parent_excludes = set()",
            "        if recursive and not include_parent_ref:",
            "            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)",
            "            if parent_ref:",
            "                parent_excludes = {c.name for c in parent_ref.local_columns}",
            "        dict_rep = {",
            "            c.name: getattr(self, c.name)",
            "            for c in cls.__table__.columns  # type: ignore",
            "            if (",
            "                c.name in export_fields",
            "                and c.name not in parent_excludes",
            "                and (",
            "                    include_defaults",
            "                    or (",
            "                        getattr(self, c.name) is not None",
            "                        and (not c.default or getattr(self, c.name) != c.default.arg)",
            "                    )",
            "                )",
            "            )",
            "        }",
            "",
            "        # sort according to export_fields using DSU (decorate, sort, undecorate)",
            "        order = {field: i for i, field in enumerate(self.export_fields)}",
            "        decorated_keys = [(order.get(k, len(order)), k) for k in dict_rep]",
            "        decorated_keys.sort()",
            "        dict_rep = {k: dict_rep[k] for _, k in decorated_keys}",
            "",
            "        if recursive:",
            "            for cld in self.export_children:",
            "                # sorting to make lists of children stable",
            "                dict_rep[cld] = sorted(",
            "                    [",
            "                        child.export_to_dict(",
            "                            recursive=recursive,",
            "                            include_parent_ref=include_parent_ref,",
            "                            include_defaults=include_defaults,",
            "                        )",
            "                        for child in getattr(self, cld)",
            "                    ],",
            "                    key=lambda k: sorted(str(k.items())),",
            "                )",
            "",
            "        return convert_uuids(dict_rep)",
            "",
            "    def override(self, obj: Any) -> None:",
            "        \"\"\"Overrides the plain fields of the dashboard.\"\"\"",
            "        for field in obj.__class__.export_fields:",
            "            setattr(self, field, getattr(obj, field))",
            "",
            "    def copy(self) -> Any:",
            "        \"\"\"Creates a copy of the dashboard without relationships.\"\"\"",
            "        new_obj = self.__class__()",
            "        new_obj.override(self)",
            "        return new_obj",
            "",
            "    def alter_params(self, **kwargs: Any) -> None:",
            "        params = self.params_dict",
            "        params.update(kwargs)",
            "        self.params = json.dumps(params)",
            "",
            "    def remove_params(self, param_to_remove: str) -> None:",
            "        params = self.params_dict",
            "        params.pop(param_to_remove, None)",
            "        self.params = json.dumps(params)",
            "",
            "    def reset_ownership(self) -> None:",
            "        \"\"\"object will belong to the user the current user\"\"\"",
            "        # make sure the object doesn't have relations to a user",
            "        # it will be filled by appbuilder on save",
            "        self.created_by = None",
            "        self.changed_by = None",
            "        # flask global context might not exist (in cli or tests for example)",
            "        self.owners = []",
            "        if g and hasattr(g, \"user\"):",
            "            self.owners = [g.user]",
            "",
            "    @property",
            "    def params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.params)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return json_to_dict(self.template_params)  # type: ignore",
            "",
            "",
            "def _user_link(user: User) -> Union[Markup, str]:",
            "    if not user:",
            "        return \"\"",
            "    url = f\"/superset/profile/{user.username}/\"",
            "    return Markup(f\"<a href=\\\"{url}\\\">{escape(user) or ''}</a>\")",
            "",
            "",
            "class AuditMixinNullable(AuditMixin):",
            "    \"\"\"Altering the AuditMixin to use nullable fields",
            "",
            "    Allows creating objects programmatically outside of CRUD",
            "    \"\"\"",
            "",
            "    created_on = sa.Column(sa.DateTime, default=datetime.now, nullable=True)",
            "    changed_on = sa.Column(",
            "        sa.DateTime, default=datetime.now, onupdate=datetime.now, nullable=True",
            "    )",
            "",
            "    @declared_attr",
            "    def created_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @declared_attr",
            "    def changed_by_fk(self) -> sa.Column:  # pylint: disable=arguments-renamed",
            "        return sa.Column(",
            "            sa.Integer,",
            "            sa.ForeignKey(\"ab_user.id\"),",
            "            default=get_user_id,",
            "            onupdate=get_user_id,",
            "            nullable=True,",
            "        )",
            "",
            "    @property",
            "    def created_by_name(self) -> str:",
            "        if self.created_by:",
            "            return escape(f\"{self.created_by}\")",
            "        return \"\"",
            "",
            "    @property",
            "    def changed_by_name(self) -> str:",
            "        if self.changed_by:",
            "            return escape(f\"{self.changed_by}\")",
            "        return \"\"",
            "",
            "    @renders(\"created_by\")",
            "    def creator(self) -> Union[Markup, str]:",
            "        return _user_link(self.created_by)",
            "",
            "    @property",
            "    def changed_by_(self) -> Union[Markup, str]:",
            "        return _user_link(self.changed_by)",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on}</span>')",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_delta_humanized(self) -> str:",
            "        return self.changed_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_dttm(self) -> float:",
            "        return datetime_to_epoch(self.changed_on)",
            "",
            "    @renders(\"created_on\")",
            "    def created_on_delta_humanized(self) -> str:",
            "        return self.created_on_humanized",
            "",
            "    @renders(\"changed_on\")",
            "    def changed_on_utc(self) -> str:",
            "        # Convert naive datetime to UTC",
            "        return self.changed_on.astimezone(pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")",
            "",
            "    @property",
            "    def changed_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @property",
            "    def created_on_humanized(self) -> str:",
            "        return humanize.naturaltime(datetime.now() - self.created_on)",
            "",
            "    @renders(\"changed_on\")",
            "    def modified(self) -> Markup:",
            "        return Markup(f'<span class=\"no-wrap\">{self.changed_on_humanized}</span>')",
            "",
            "",
            "class QueryResult:  # pylint: disable=too-few-public-methods",
            "",
            "    \"\"\"Object returned by the query interface\"\"\"",
            "",
            "    def __init__(  # pylint: disable=too-many-arguments",
            "        self,",
            "        df: pd.DataFrame,",
            "        query: str,",
            "        duration: timedelta,",
            "        applied_template_filters: Optional[list[str]] = None,",
            "        applied_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        rejected_filter_columns: Optional[list[ColumnTyping]] = None,",
            "        status: str = QueryStatus.SUCCESS,",
            "        error_message: Optional[str] = None,",
            "        errors: Optional[list[dict[str, Any]]] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        to_dttm: Optional[datetime] = None,",
            "    ) -> None:",
            "        self.df = df",
            "        self.query = query",
            "        self.duration = duration",
            "        self.applied_template_filters = applied_template_filters or []",
            "        self.applied_filter_columns = applied_filter_columns or []",
            "        self.rejected_filter_columns = rejected_filter_columns or []",
            "        self.status = status",
            "        self.error_message = error_message",
            "        self.errors = errors or []",
            "        self.from_dttm = from_dttm",
            "        self.to_dttm = to_dttm",
            "",
            "",
            "class ExtraJSONMixin:",
            "    \"\"\"Mixin to add an `extra` column (JSON) and utility methods\"\"\"",
            "",
            "    extra_json = sa.Column(sa.Text, default=\"{}\")",
            "",
            "    @property",
            "    def extra(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra_json or \"{}\") or {}",
            "        except (TypeError, JSONDecodeError) as exc:",
            "            logger.error(",
            "                \"Unable to load an extra json: %r. Leaving empty.\", exc, exc_info=True",
            "            )",
            "            return {}",
            "",
            "    @extra.setter",
            "    def extra(self, extras: dict[str, Any]) -> None:",
            "        self.extra_json = json.dumps(extras)",
            "",
            "    def set_extra_json_key(self, key: str, value: Any) -> None:",
            "        extra = self.extra",
            "        extra[key] = value",
            "        self.extra_json = json.dumps(extra)",
            "",
            "    @validates(\"extra_json\")",
            "    def ensure_extra_json_is_not_none(",
            "        self,",
            "        _: str,",
            "        value: Optional[dict[str, Any]],",
            "    ) -> Any:",
            "        if value is None:",
            "            return \"{}\"",
            "        return value",
            "",
            "",
            "class CertificationMixin:",
            "    \"\"\"Mixin to add extra certification fields\"\"\"",
            "",
            "    extra = sa.Column(sa.Text, default=\"{}\")",
            "",
            "    def get_extra_dict(self) -> dict[str, Any]:",
            "        try:",
            "            return json.loads(self.extra)",
            "        except (TypeError, json.JSONDecodeError):",
            "            return {}",
            "",
            "    @property",
            "    def is_certified(self) -> bool:",
            "        return bool(self.get_extra_dict().get(\"certification\"))",
            "",
            "    @property",
            "    def certified_by(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"certified_by\")",
            "",
            "    @property",
            "    def certification_details(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"certification\", {}).get(\"details\")",
            "",
            "    @property",
            "    def warning_markdown(self) -> Optional[str]:",
            "        return self.get_extra_dict().get(\"warning_markdown\")",
            "",
            "",
            "def clone_model(",
            "    target: Model,",
            "    ignore: Optional[list[str]] = None,",
            "    keep_relations: Optional[list[str]] = None,",
            "    **kwargs: Any,",
            ") -> Model:",
            "    \"\"\"",
            "    Clone a SQLAlchemy model. By default will only clone naive column attributes.",
            "    To include relationship attributes, use `keep_relations`.",
            "    \"\"\"",
            "    ignore = ignore or []",
            "",
            "    table = target.__table__",
            "    primary_keys = table.primary_key.columns.keys()",
            "    data = {",
            "        attr: getattr(target, attr)",
            "        for attr in list(table.columns.keys()) + (keep_relations or [])",
            "        if attr not in primary_keys and attr not in ignore",
            "    }",
            "    data.update(kwargs)",
            "",
            "    return target.__class__(**data)",
            "",
            "",
            "# todo(hugh): centralize where this code lives",
            "class QueryStringExtended(NamedTuple):",
            "    applied_template_filters: Optional[list[str]]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sql: str",
            "",
            "",
            "class SqlaQuery(NamedTuple):",
            "    applied_template_filters: list[str]",
            "    applied_filter_columns: list[ColumnTyping]",
            "    rejected_filter_columns: list[ColumnTyping]",
            "    cte: Optional[str]",
            "    extra_cache_keys: list[Any]",
            "    labels_expected: list[str]",
            "    prequeries: list[str]",
            "    sqla_query: Select",
            "",
            "",
            "class ExploreMixin:  # pylint: disable=too-many-public-methods",
            "    \"\"\"",
            "    Allows any flask_appbuilder.Model (Query, Table, etc.)",
            "    to be used to power a chart inside /explore",
            "    \"\"\"",
            "",
            "    sqla_aggregations = {",
            "        \"COUNT_DISTINCT\": lambda column_name: sa.func.COUNT(sa.distinct(column_name)),",
            "        \"COUNT\": sa.func.COUNT,",
            "        \"SUM\": sa.func.SUM,",
            "        \"AVG\": sa.func.AVG,",
            "        \"MIN\": sa.func.MIN,",
            "        \"MAX\": sa.func.MAX,",
            "    }",
            "    fetch_values_predicate = None",
            "",
            "    @property",
            "    def type(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_extra(self) -> Optional[dict[str, Any]]:",
            "        raise NotImplementedError()",
            "",
            "    def query(self, query_obj: QueryObjectDict) -> QueryResult:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database_id(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def owners_data(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def metrics(self) -> list[Any]:",
            "        return []",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def is_rls_supported(self) -> bool:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def cache_timeout(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def column_names(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def offset(self) -> int:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def main_dttm_col(self) -> Optional[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def always_filter_main_dttm(self) -> Optional[bool]:",
            "        return False",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[str]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def db_engine_spec(self) -> builtins.type[\"BaseEngineSpec\"]:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def database(self) -> \"Database\":",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def schema(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def sql(self) -> str:",
            "        raise NotImplementedError()",
            "",
            "    @property",
            "    def columns(self) -> list[Any]:",
            "        raise NotImplementedError()",
            "",
            "    def get_extra_cache_keys(self, query_obj: dict[str, Any]) -> list[Hashable]:",
            "        raise NotImplementedError()",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        raise NotImplementedError()",
            "",
            "    def get_fetch_values_predicate(",
            "        self,",
            "        template_processor: Optional[  # pylint: disable=unused-argument",
            "            BaseTemplateProcessor",
            "        ] = None,",
            "    ) -> TextClause:",
            "        return self.fetch_values_predicate",
            "",
            "    def get_sqla_row_level_filters(",
            "        self,",
            "        template_processor: BaseTemplateProcessor,",
            "    ) -> list[TextClause]:",
            "        \"\"\"",
            "        Return the appropriate row level security filters for this table and the",
            "        current user. A custom username can be passed when the user is not present in the",
            "        Flask global namespace.",
            "",
            "        :param template_processor: The template processor to apply to the filters.",
            "        :returns: A list of SQL clauses to be ANDed together.",
            "        \"\"\"",
            "        all_filters: list[TextClause] = []",
            "        filter_groups: dict[Union[int, str], list[TextClause]] = defaultdict(list)",
            "        try:",
            "            for filter_ in security_manager.get_rls_filters(self):",
            "                clause = self.text(",
            "                    f\"({template_processor.process_template(filter_.clause)})\"",
            "                )",
            "                if filter_.group_key:",
            "                    filter_groups[filter_.group_key].append(clause)",
            "                else:",
            "                    all_filters.append(clause)",
            "",
            "            if is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "                for rule in security_manager.get_guest_rls_filters(self):",
            "                    clause = self.text(",
            "                        f\"({template_processor.process_template(rule['clause'])})\"",
            "                    )",
            "                    all_filters.append(clause)",
            "",
            "            grouped_filters = [or_(*clauses) for clauses in filter_groups.values()]",
            "            all_filters.extend(grouped_filters)",
            "            return all_filters",
            "        except TemplateError as ex:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Error in jinja expression in RLS filters: %(msg)s\",",
            "                    msg=ex.message,",
            "                )",
            "            ) from ex",
            "",
            "    def _process_sql_expression(",
            "        self,",
            "        expression: Optional[str],",
            "        database_id: int,",
            "        schema: str,",
            "        template_processor: Optional[BaseTemplateProcessor],",
            "    ) -> Optional[str]:",
            "        if template_processor and expression:",
            "            expression = template_processor.process_template(expression)",
            "        if expression:",
            "            expression = validate_adhoc_subquery(",
            "                expression,",
            "                database_id,",
            "                schema,",
            "            )",
            "            try:",
            "                expression = sanitize_clause(expression)",
            "            except QueryClauseValidationException as ex:",
            "                raise QueryObjectValidationError(ex.message) from ex",
            "        return expression",
            "",
            "    def make_sqla_column_compatible(",
            "        self, sqla_col: ColumnElement, label: Optional[str] = None",
            "    ) -> ColumnElement:",
            "        \"\"\"Takes a sqlalchemy column object and adds label info if supported by engine.",
            "        :param sqla_col: sqlalchemy column instance",
            "        :param label: alias/label that column is expected to have",
            "        :return: either a sql alchemy column or label instance if supported by engine",
            "        \"\"\"",
            "        label_expected = label or sqla_col.name",
            "        db_engine_spec = self.db_engine_spec",
            "        # add quotes to tables",
            "        if db_engine_spec.get_allows_alias_in_select(self.database):",
            "            label = db_engine_spec.make_label_compatible(label_expected)",
            "            sqla_col = sqla_col.label(label)",
            "        sqla_col.key = label_expected",
            "        return sqla_col",
            "",
            "    def mutate_query_from_config(self, sql: str) -> str:",
            "        \"\"\"Apply config's SQL_QUERY_MUTATOR",
            "",
            "        Typically adds comments to the query with context\"\"\"",
            "        if sql_query_mutator := config[\"SQL_QUERY_MUTATOR\"]:",
            "            sql = sql_query_mutator(",
            "                sql,",
            "                security_manager=security_manager,",
            "                database=self.database,",
            "            )",
            "        return sql",
            "",
            "    @staticmethod",
            "    def _apply_cte(sql: str, cte: Optional[str]) -> str:",
            "        \"\"\"",
            "        Append a CTE before the SELECT statement if defined",
            "",
            "        :param sql: SELECT statement",
            "        :param cte: CTE statement",
            "        :return:",
            "        \"\"\"",
            "        if cte:",
            "            sql = f\"{cte}\\n{sql}\"",
            "        return sql",
            "",
            "    def get_query_str_extended(",
            "        self, query_obj: QueryObjectDict, mutate: bool = True",
            "    ) -> QueryStringExtended:",
            "        sqlaq = self.get_sqla_query(**query_obj)",
            "        sql = self.database.compile_sqla_query(sqlaq.sqla_query)",
            "        sql = self._apply_cte(sql, sqlaq.cte)",
            "        sql = sqlparse.format(sql, reindent=True)",
            "        if mutate:",
            "            sql = self.mutate_query_from_config(sql)",
            "        return QueryStringExtended(",
            "            applied_template_filters=sqlaq.applied_template_filters,",
            "            applied_filter_columns=sqlaq.applied_filter_columns,",
            "            rejected_filter_columns=sqlaq.rejected_filter_columns,",
            "            labels_expected=sqlaq.labels_expected,",
            "            prequeries=sqlaq.prequeries,",
            "            sql=sql,",
            "        )",
            "",
            "    def _normalize_prequery_result_type(",
            "        self,",
            "        row: pd.Series,",
            "        dimension: str,",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> Union[str, int, float, bool, str]:",
            "        \"\"\"",
            "        Convert a prequery result type to its equivalent Python type.",
            "",
            "        Some databases like Druid will return timestamps as strings, but do not perform",
            "        automatic casting when comparing these strings to a timestamp. For cases like",
            "        this we convert the value via the appropriate SQL transform.",
            "",
            "        :param row: A prequery record",
            "        :param dimension: The dimension name",
            "        :param columns_by_name: The mapping of columns by name",
            "        :return: equivalent primitive python type",
            "        \"\"\"",
            "",
            "        value = row[dimension]",
            "",
            "        if isinstance(value, np.generic):",
            "            value = value.item()",
            "",
            "        column_ = columns_by_name[dimension]",
            "        db_extra: dict[str, Any] = self.database.get_extra()",
            "",
            "        if isinstance(column_, dict):",
            "            if (",
            "                column_.get(\"type\")",
            "                and column_.get(\"is_temporal\")",
            "                and isinstance(value, str)",
            "            ):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.get(\"type\"), dateutil.parser.parse(value), db_extra=None",
            "                )",
            "",
            "                if sql:",
            "                    value = self.db_engine_spec.get_text_clause(sql)",
            "        else:",
            "            if column_.type and column_.is_temporal and isinstance(value, str):",
            "                sql = self.db_engine_spec.convert_dttm(",
            "                    column_.type, dateutil.parser.parse(value), db_extra=db_extra",
            "                )",
            "",
            "                if sql:",
            "                    value = self.text(sql)",
            "        return value",
            "",
            "    def make_orderby_compatible(",
            "        self, select_exprs: list[ColumnElement], orderby_exprs: list[ColumnElement]",
            "    ) -> None:",
            "        \"\"\"",
            "        If needed, make sure aliases for selected columns are not used in",
            "        `ORDER BY`.",
            "",
            "        In some databases (e.g. Presto), `ORDER BY` clause is not able to",
            "        automatically pick the source column if a `SELECT` clause alias is named",
            "        the same as a source column. In this case, we update the SELECT alias to",
            "        another name to avoid the conflict.",
            "        \"\"\"",
            "        if self.db_engine_spec.allows_alias_to_source_column:",
            "            return",
            "",
            "        def is_alias_used_in_orderby(col: ColumnElement) -> bool:",
            "            if not isinstance(col, Label):",
            "                return False",
            "            regexp = re.compile(f\"\\\\(.*\\\\b{re.escape(col.name)}\\\\b.*\\\\)\", re.IGNORECASE)",
            "            return any(regexp.search(str(x)) for x in orderby_exprs)",
            "",
            "        # Iterate through selected columns, if column alias appears in orderby",
            "        # use another `alias`. The final output columns will still use the",
            "        # original names, because they are updated by `labels_expected` after",
            "        # querying.",
            "        for col in select_exprs:",
            "            if is_alias_used_in_orderby(col):",
            "                col.name = f\"{col.name}__\"",
            "",
            "    def exc_query(self, qry: Any) -> QueryResult:",
            "        qry_start_dttm = datetime.now()",
            "        query_str_ext = self.get_query_str_extended(qry)",
            "        sql = query_str_ext.sql",
            "        status = QueryStatus.SUCCESS",
            "        errors = None",
            "        error_message = None",
            "",
            "        def assign_column_label(df: pd.DataFrame) -> Optional[pd.DataFrame]:",
            "            \"\"\"",
            "            Some engines change the case or generate bespoke column names, either by",
            "            default or due to lack of support for aliasing. This function ensures that",
            "            the column names in the DataFrame correspond to what is expected by",
            "            the viz components.",
            "            Sometimes a query may also contain only order by columns that are not used",
            "            as metrics or groupby columns, but need to present in the SQL `select`,",
            "            filtering by `labels_expected` make sure we only return columns users want.",
            "            :param df: Original DataFrame returned by the engine",
            "            :return: Mutated DataFrame",
            "            \"\"\"",
            "            labels_expected = query_str_ext.labels_expected",
            "            if df is not None and not df.empty:",
            "                if len(df.columns) < len(labels_expected):",
            "                    raise QueryObjectValidationError(",
            "                        _(\"Db engine did not return all queried columns\")",
            "                    )",
            "                if len(df.columns) > len(labels_expected):",
            "                    df = df.iloc[:, 0 : len(labels_expected)]",
            "                df.columns = labels_expected",
            "            return df",
            "",
            "        try:",
            "            df = self.database.get_df(sql, self.schema, mutator=assign_column_label)",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            df = pd.DataFrame()",
            "            status = QueryStatus.FAILED",
            "            logger.warning(",
            "                \"Query %s on schema %s failed\", sql, self.schema, exc_info=True",
            "            )",
            "            db_engine_spec = self.db_engine_spec",
            "            errors = [",
            "                dataclasses.asdict(error) for error in db_engine_spec.extract_errors(ex)",
            "            ]",
            "            error_message = utils.error_msg_from_exception(ex)",
            "",
            "        return QueryResult(",
            "            applied_template_filters=query_str_ext.applied_template_filters,",
            "            applied_filter_columns=query_str_ext.applied_filter_columns,",
            "            rejected_filter_columns=query_str_ext.rejected_filter_columns,",
            "            status=status,",
            "            df=df,",
            "            duration=datetime.now() - qry_start_dttm,",
            "            query=sql,",
            "            errors=errors,",
            "            error_message=error_message,",
            "        )",
            "",
            "    def get_rendered_sql(",
            "        self, template_processor: Optional[BaseTemplateProcessor] = None",
            "    ) -> str:",
            "        \"\"\"",
            "        Render sql with template engine (Jinja).",
            "        \"\"\"",
            "",
            "        sql = self.sql",
            "        if template_processor:",
            "            try:",
            "                sql = template_processor.process_template(sql)",
            "            except TemplateError as ex:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        \"Error while rendering virtual dataset query: %(msg)s\",",
            "                        msg=ex.message,",
            "                    )",
            "                ) from ex",
            "        sql = sqlparse.format(sql.strip(\"\\t\\r\\n; \"), strip_comments=True)",
            "        if not sql:",
            "            raise QueryObjectValidationError(_(\"Virtual dataset query cannot be empty\"))",
            "        if len(sqlparse.split(sql)) > 1:",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query cannot consist of multiple statements\")",
            "            )",
            "        return sql",
            "",
            "    def text(self, clause: str) -> TextClause:",
            "        return self.db_engine_spec.get_text_clause(clause)",
            "",
            "    def get_from_clause(",
            "        self, template_processor: Optional[BaseTemplateProcessor] = None",
            "    ) -> tuple[Union[TableClause, Alias], Optional[str]]:",
            "        \"\"\"",
            "        Return where to select the columns and metrics from. Either a physical table",
            "        or a virtual table with it's own subquery. If the FROM is referencing a",
            "        CTE, the CTE is returned as the second value in the return tuple.",
            "        \"\"\"",
            "",
            "        from_sql = self.get_rendered_sql(template_processor)",
            "        parsed_query = ParsedQuery(from_sql, engine=self.db_engine_spec.engine)",
            "        if not (",
            "            parsed_query.is_unknown()",
            "            or self.db_engine_spec.is_readonly_query(parsed_query)",
            "        ):",
            "            raise QueryObjectValidationError(",
            "                _(\"Virtual dataset query must be read-only\")",
            "            )",
            "",
            "        cte = self.db_engine_spec.get_cte_query(from_sql)",
            "        from_clause = (",
            "            sa.table(self.db_engine_spec.cte_alias)",
            "            if cte",
            "            else TextAsFrom(self.text(from_sql), []).alias(VIRTUAL_TABLE_ALIAS)",
            "        )",
            "",
            "        return from_clause, cte",
            "",
            "    def adhoc_metric_to_sqla(",
            "        self,",
            "        metric: AdhocMetric,",
            "        columns_by_name: dict[str, \"TableColumn\"],  # pylint: disable=unused-argument",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc metric into a sqlalchemy column.",
            "",
            "        :param dict metric: Adhoc metric definition",
            "        :param dict columns_by_name: Columns for the current table",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        expression_type = metric.get(\"expressionType\")",
            "        label = utils.get_metric_name(metric)",
            "",
            "        if expression_type == utils.AdhocMetricExpressionType.SIMPLE:",
            "            metric_column = metric.get(\"column\") or {}",
            "            column_name = cast(str, metric_column.get(\"column_name\"))",
            "            sqla_column = sa.column(column_name)",
            "            sqla_metric = self.sqla_aggregations[metric[\"aggregate\"]](sqla_column)",
            "        elif expression_type == utils.AdhocMetricExpressionType.SQL:",
            "            expression = self._process_sql_expression(",
            "                expression=metric[\"sqlExpression\"],",
            "                database_id=self.database_id,",
            "                schema=self.schema,",
            "                template_processor=template_processor,",
            "            )",
            "            sqla_metric = literal_column(expression)",
            "        else:",
            "            raise QueryObjectValidationError(\"Adhoc metric expressionType is invalid\")",
            "",
            "        return self.make_sqla_column_compatible(sqla_metric, label)",
            "",
            "    @property",
            "    def template_params_dict(self) -> dict[Any, Any]:",
            "        return {}",
            "",
            "    @staticmethod",
            "    def filter_values_handler(  # pylint: disable=too-many-arguments",
            "        values: Optional[FilterValues],",
            "        operator: str,",
            "        target_generic_type: utils.GenericDataType,",
            "        target_native_type: Optional[str] = None,",
            "        is_list_target: bool = False,",
            "        db_engine_spec: Optional[",
            "            builtins.type[\"BaseEngineSpec\"]",
            "        ] = None,  # fix(hughhh): Optional[Type[BaseEngineSpec]]",
            "        db_extra: Optional[dict[str, Any]] = None,",
            "    ) -> Optional[FilterValues]:",
            "        if values is None:",
            "            return None",
            "",
            "        def handle_single_value(value: Optional[FilterValue]) -> Optional[FilterValue]:",
            "            if operator == utils.FilterOperator.TEMPORAL_RANGE:",
            "                return value",
            "            if (",
            "                isinstance(value, (float, int))",
            "                and target_generic_type == utils.GenericDataType.TEMPORAL",
            "                and target_native_type is not None",
            "                and db_engine_spec is not None",
            "            ):",
            "                value = db_engine_spec.convert_dttm(",
            "                    target_type=target_native_type,",
            "                    dttm=datetime.utcfromtimestamp(value / 1000),",
            "                    db_extra=db_extra,",
            "                )",
            "                value = literal_column(value)",
            "            if isinstance(value, str):",
            "                value = value.strip(\"\\t\\n\")",
            "",
            "                if (",
            "                    target_generic_type == utils.GenericDataType.NUMERIC",
            "                    and operator",
            "                    not in {",
            "                        utils.FilterOperator.ILIKE,",
            "                        utils.FilterOperator.LIKE,",
            "                    }",
            "                ):",
            "                    # For backwards compatibility and edge cases",
            "                    # where a column data type might have changed",
            "                    return utils.cast_to_num(value)",
            "                if value == NULL_STRING:",
            "                    return None",
            "                if value == EMPTY_STRING:",
            "                    return \"\"",
            "            if target_generic_type == utils.GenericDataType.BOOLEAN:",
            "                return utils.cast_to_boolean(value)",
            "            return value",
            "",
            "        if isinstance(values, (list, tuple)):",
            "            values = [handle_single_value(v) for v in values]  # type: ignore",
            "        else:",
            "            values = handle_single_value(values)",
            "        if is_list_target and not isinstance(values, (tuple, list)):",
            "            values = [values]  # type: ignore",
            "        elif not is_list_target and isinstance(values, (tuple, list)):",
            "            values = values[0] if values else None",
            "        return values",
            "",
            "    def get_query_str(self, query_obj: QueryObjectDict) -> str:",
            "        query_str_ext = self.get_query_str_extended(query_obj)",
            "        all_queries = query_str_ext.prequeries + [query_str_ext.sql]",
            "        return \";\\n\\n\".join(all_queries) + \";\"",
            "",
            "    def _get_series_orderby(",
            "        self,",
            "        series_limit_metric: Metric,",
            "        metrics_by_name: dict[str, \"SqlMetric\"],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        if utils.is_adhoc_metric(series_limit_metric):",
            "            assert isinstance(series_limit_metric, dict)",
            "            ob = self.adhoc_metric_to_sqla(series_limit_metric, columns_by_name)",
            "        elif (",
            "            isinstance(series_limit_metric, str)",
            "            and series_limit_metric in metrics_by_name",
            "        ):",
            "            ob = metrics_by_name[series_limit_metric].get_sqla_col(",
            "                template_processor=template_processor",
            "            )",
            "        else:",
            "            raise QueryObjectValidationError(",
            "                _(\"Metric '%(metric)s' does not exist\", metric=series_limit_metric)",
            "            )",
            "        return ob",
            "",
            "    def adhoc_column_to_sqla(",
            "        self,",
            "        col: \"AdhocColumn\",  # type: ignore",
            "        force_type_check: bool = False,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        raise NotImplementedError()",
            "",
            "    def _get_top_groups(",
            "        self,",
            "        df: pd.DataFrame,",
            "        dimensions: list[str],",
            "        groupby_exprs: dict[str, Any],",
            "        columns_by_name: dict[str, \"TableColumn\"],",
            "    ) -> ColumnElement:",
            "        groups = []",
            "        for _unused, row in df.iterrows():",
            "            group = []",
            "            for dimension in dimensions:",
            "                value = self._normalize_prequery_result_type(",
            "                    row,",
            "                    dimension,",
            "                    columns_by_name,",
            "                )",
            "",
            "                group.append(groupby_exprs[dimension] == value)",
            "            groups.append(and_(*group))",
            "",
            "        return or_(*groups)",
            "",
            "    def dttm_sql_literal(self, dttm: datetime, col: \"TableColumn\") -> str:",
            "        \"\"\"Convert datetime object to a SQL expression string\"\"\"",
            "",
            "        sql = (",
            "            self.db_engine_spec.convert_dttm(col.type, dttm, db_extra=self.db_extra)",
            "            if col.type",
            "            else None",
            "        )",
            "",
            "        if sql:",
            "            return sql",
            "",
            "        tf = col.python_date_format",
            "",
            "        # Fallback to the default format (if defined).",
            "        if not tf and self.db_extra:",
            "            tf = self.db_extra.get(\"python_date_format_by_column_name\", {}).get(",
            "                col.column_name",
            "            )",
            "",
            "        if tf:",
            "            if tf in {\"epoch_ms\", \"epoch_s\"}:",
            "                seconds_since_epoch = int(dttm.timestamp())",
            "                if tf == \"epoch_s\":",
            "                    return str(seconds_since_epoch)",
            "                return str(seconds_since_epoch * 1000)",
            "            return f\"'{dttm.strftime(tf)}'\"",
            "",
            "        return f\"\"\"'{dttm.strftime(\"%Y-%m-%d %H:%M:%S.%f\")}'\"\"\"",
            "",
            "    def get_time_filter(  # pylint: disable=too-many-arguments",
            "        self,",
            "        time_col: \"TableColumn\",",
            "        start_dttm: Optional[sa.DateTime],",
            "        end_dttm: Optional[sa.DateTime],",
            "        time_grain: Optional[str] = None,",
            "        label: Optional[str] = \"__time\",",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        col = (",
            "            time_col.get_timestamp_expression(",
            "                time_grain=time_grain,",
            "                label=label,",
            "                template_processor=template_processor,",
            "            )",
            "            if time_grain",
            "            else self.convert_tbl_column_to_sqla_col(",
            "                time_col, label=label, template_processor=template_processor",
            "            )",
            "        )",
            "",
            "        l = []",
            "        if start_dttm:",
            "            l.append(",
            "                col",
            "                >= self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(start_dttm, time_col)",
            "                )",
            "            )",
            "        if end_dttm:",
            "            l.append(",
            "                col",
            "                < self.db_engine_spec.get_text_clause(",
            "                    self.dttm_sql_literal(end_dttm, time_col)",
            "                )",
            "            )",
            "        return and_(*l)",
            "",
            "    def values_for_column(",
            "        self, column_name: str, limit: int = 10000, denormalize_column: bool = False",
            "    ) -> list[Any]:",
            "        # denormalize column name before querying for values",
            "        # unless disabled in the dataset configuration",
            "        db_dialect = self.database.get_dialect()",
            "        column_name_ = (",
            "            self.database.db_engine_spec.denormalize_name(db_dialect, column_name)",
            "            if denormalize_column",
            "            else column_name",
            "        )",
            "        cols = {col.column_name: col for col in self.columns}",
            "        target_col = cols[column_name_]",
            "        tp = self.get_template_processor()",
            "        tbl, cte = self.get_from_clause(tp)",
            "",
            "        qry = (",
            "            sa.select(",
            "                # The alias (label) here is important because some dialects will",
            "                # automatically add a random alias to the projection because of the",
            "                # call to DISTINCT; others will uppercase the column names. This",
            "                # gives us a deterministic column name in the dataframe.",
            "                [target_col.get_sqla_col(template_processor=tp).label(\"column_values\")]",
            "            )",
            "            .select_from(tbl)",
            "            .distinct()",
            "        )",
            "        if limit:",
            "            qry = qry.limit(limit)",
            "",
            "        if self.fetch_values_predicate:",
            "            qry = qry.where(self.get_fetch_values_predicate(template_processor=tp))",
            "",
            "        with self.database.get_sqla_engine_with_context() as engine:",
            "            sql = qry.compile(engine, compile_kwargs={\"literal_binds\": True})",
            "            sql = self._apply_cte(sql, cte)",
            "            sql = self.mutate_query_from_config(sql)",
            "",
            "            df = pd.read_sql_query(sql=sql, con=engine)",
            "            return df[\"column_values\"].to_list()",
            "",
            "    def get_timestamp_expression(",
            "        self,",
            "        column: dict[str, Any],",
            "        time_grain: Optional[str],",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Union[TimestampExpression, Label]:",
            "        \"\"\"",
            "        Return a SQLAlchemy Core element representation of self to be used in a query.",
            "",
            "        :param column: column object",
            "        :param time_grain: Optional time grain, e.g. P1Y",
            "        :param label: alias/label that column is expected to have",
            "        :param template_processor: template processor",
            "        :return: A TimeExpression object wrapped in a Label if supported by db",
            "        \"\"\"",
            "        label = label or utils.DTTM_ALIAS",
            "        column_spec = self.db_engine_spec.get_column_spec(column.get(\"type\"))",
            "        type_ = column_spec.sqla_type if column_spec else sa.DateTime",
            "        col = sa.column(column.get(\"column_name\"), type_=type_)",
            "",
            "        if template_processor:",
            "            expression = template_processor.process_template(column[\"column_name\"])",
            "            col = sa.literal_column(expression, type_=type_)",
            "",
            "        time_expr = self.db_engine_spec.get_timestamp_expr(col, None, time_grain)",
            "        return self.make_sqla_column_compatible(time_expr, label)",
            "",
            "    def convert_tbl_column_to_sqla_col(",
            "        self,",
            "        tbl_column: \"TableColumn\",",
            "        label: Optional[str] = None,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> Column:",
            "        label = label or tbl_column.column_name",
            "        db_engine_spec = self.db_engine_spec",
            "        column_spec = db_engine_spec.get_column_spec(self.type, db_extra=self.db_extra)",
            "        type_ = column_spec.sqla_type if column_spec else None",
            "        if expression := tbl_column.expression:",
            "            if template_processor:",
            "                expression = template_processor.process_template(expression)",
            "            col = literal_column(expression, type_=type_)",
            "        else:",
            "            col = sa.column(tbl_column.column_name, type_=type_)",
            "        col = self.make_sqla_column_compatible(col, label)",
            "        return col",
            "",
            "    def get_sqla_query(  # pylint: disable=too-many-arguments,too-many-locals,too-many-branches,too-many-statements",
            "        self,",
            "        apply_fetch_values_predicate: bool = False,",
            "        columns: Optional[list[Column]] = None,",
            "        extras: Optional[dict[str, Any]] = None,",
            "        filter: Optional[  # pylint: disable=redefined-builtin",
            "            list[utils.QueryObjectFilterClause]",
            "        ] = None,",
            "        from_dttm: Optional[datetime] = None,",
            "        granularity: Optional[str] = None,",
            "        groupby: Optional[list[Column]] = None,",
            "        inner_from_dttm: Optional[datetime] = None,",
            "        inner_to_dttm: Optional[datetime] = None,",
            "        is_rowcount: bool = False,",
            "        is_timeseries: bool = True,",
            "        metrics: Optional[list[Metric]] = None,",
            "        orderby: Optional[list[OrderBy]] = None,",
            "        order_desc: bool = True,",
            "        to_dttm: Optional[datetime] = None,",
            "        series_columns: Optional[list[Column]] = None,",
            "        series_limit: Optional[int] = None,",
            "        series_limit_metric: Optional[Metric] = None,",
            "        row_limit: Optional[int] = None,",
            "        row_offset: Optional[int] = None,",
            "        timeseries_limit: Optional[int] = None,",
            "        timeseries_limit_metric: Optional[Metric] = None,",
            "        time_shift: Optional[str] = None,",
            "    ) -> SqlaQuery:",
            "        \"\"\"Querying any sqla table from this common interface\"\"\"",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        extras = extras or {}",
            "        time_grain = extras.get(\"time_grain_sqla\")",
            "",
            "        template_kwargs = {",
            "            \"columns\": columns,",
            "            \"from_dttm\": from_dttm.isoformat() if from_dttm else None,",
            "            \"groupby\": groupby,",
            "            \"metrics\": metrics,",
            "            \"row_limit\": row_limit,",
            "            \"row_offset\": row_offset,",
            "            \"time_column\": granularity,",
            "            \"time_grain\": time_grain,",
            "            \"to_dttm\": to_dttm.isoformat() if to_dttm else None,",
            "            \"table_columns\": [col.column_name for col in self.columns],",
            "            \"filter\": filter,",
            "        }",
            "        columns = columns or []",
            "        groupby = groupby or []",
            "        rejected_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        applied_adhoc_filters_columns: list[Union[str, ColumnTyping]] = []",
            "        db_engine_spec = self.db_engine_spec",
            "        series_column_labels = [",
            "            db_engine_spec.make_label_compatible(column)",
            "            for column in utils.get_column_names(",
            "                columns=series_columns or [],",
            "            )",
            "        ]",
            "        # deprecated, to be removed in 2.0",
            "        if is_timeseries and timeseries_limit:",
            "            series_limit = timeseries_limit",
            "        series_limit_metric = series_limit_metric or timeseries_limit_metric",
            "        template_kwargs.update(self.template_params_dict)",
            "        extra_cache_keys: list[Any] = []",
            "        template_kwargs[\"extra_cache_keys\"] = extra_cache_keys",
            "        removed_filters: list[str] = []",
            "        applied_template_filters: list[str] = []",
            "        template_kwargs[\"removed_filters\"] = removed_filters",
            "        template_kwargs[\"applied_filters\"] = applied_template_filters",
            "        template_processor = self.get_template_processor(**template_kwargs)",
            "        prequeries: list[str] = []",
            "        orderby = orderby or []",
            "        need_groupby = bool(metrics is not None or groupby)",
            "        metrics = metrics or []",
            "",
            "        # For backward compatibility",
            "        if granularity not in self.dttm_cols and granularity is not None:",
            "            granularity = self.main_dttm_col",
            "",
            "        columns_by_name: dict[str, \"TableColumn\"] = {",
            "            col.column_name: col for col in self.columns",
            "        }",
            "",
            "        metrics_by_name: dict[str, \"SqlMetric\"] = {",
            "            m.metric_name: m for m in self.metrics",
            "        }",
            "",
            "        if not granularity and is_timeseries:",
            "            raise QueryObjectValidationError(",
            "                _(",
            "                    \"Datetime column not provided as part table configuration \"",
            "                    \"and is required by this type of chart\"",
            "                )",
            "            )",
            "        if not metrics and not columns and not groupby:",
            "            raise QueryObjectValidationError(_(\"Empty query?\"))",
            "",
            "        metrics_exprs: list[ColumnElement] = []",
            "        for metric in metrics:",
            "            if utils.is_adhoc_metric(metric):",
            "                assert isinstance(metric, dict)",
            "                metrics_exprs.append(",
            "                    self.adhoc_metric_to_sqla(",
            "                        metric=metric,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "            elif isinstance(metric, str) and metric in metrics_by_name:",
            "                metrics_exprs.append(",
            "                    metrics_by_name[metric].get_sqla_col(",
            "                        template_processor=template_processor",
            "                    )",
            "                )",
            "            else:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Metric '%(metric)s' does not exist\", metric=metric)",
            "                )",
            "",
            "        if metrics_exprs:",
            "            main_metric_expr = metrics_exprs[0]",
            "        else:",
            "            main_metric_expr, label = literal_column(\"COUNT(*)\"), \"ccount\"",
            "            main_metric_expr = self.make_sqla_column_compatible(main_metric_expr, label)",
            "",
            "        # To ensure correct handling of the ORDER BY labeling we need to reference the",
            "        # metric instance if defined in the SELECT clause.",
            "        # use the key of the ColumnClause for the expected label",
            "        metrics_exprs_by_label = {m.key: m for m in metrics_exprs}",
            "        metrics_exprs_by_expr = {str(m): m for m in metrics_exprs}",
            "",
            "        # Since orderby may use adhoc metrics, too; we need to process them first",
            "        orderby_exprs: list[ColumnElement] = []",
            "        for orig_col, ascending in orderby:",
            "            col: Union[AdhocMetric, ColumnElement] = orig_col",
            "            if isinstance(col, dict):",
            "                col = cast(AdhocMetric, col)",
            "                if col.get(\"sqlExpression\"):",
            "                    col[\"sqlExpression\"] = self._process_sql_expression(",
            "                        expression=col[\"sqlExpression\"],",
            "                        database_id=self.database_id,",
            "                        schema=self.schema,",
            "                        template_processor=template_processor,",
            "                    )",
            "                if utils.is_adhoc_metric(col):",
            "                    # add adhoc sort by column to columns_by_name if not exists",
            "                    col = self.adhoc_metric_to_sqla(col, columns_by_name)",
            "                    # if the adhoc metric has been defined before",
            "                    # use the existing instance.",
            "                    col = metrics_exprs_by_expr.get(str(col), col)",
            "                    need_groupby = True",
            "            elif col in columns_by_name:",
            "                col = self.convert_tbl_column_to_sqla_col(",
            "                    columns_by_name[col], template_processor=template_processor",
            "                )",
            "            elif col in metrics_exprs_by_label:",
            "                col = metrics_exprs_by_label[col]",
            "                need_groupby = True",
            "            elif col in metrics_by_name:",
            "                col = metrics_by_name[col].get_sqla_col(",
            "                    template_processor=template_processor",
            "                )",
            "                need_groupby = True",
            "",
            "            if isinstance(col, ColumnElement):",
            "                orderby_exprs.append(col)",
            "            else:",
            "                # Could not convert a column reference to valid ColumnElement",
            "                raise QueryObjectValidationError(",
            "                    _(\"Unknown column used in orderby: %(col)s\", col=orig_col)",
            "                )",
            "",
            "        select_exprs: list[Union[Column, Label]] = []",
            "        groupby_all_columns = {}",
            "        groupby_series_columns = {}",
            "",
            "        # filter out the pseudo column  __timestamp from columns",
            "        columns = [col for col in columns if col != utils.DTTM_ALIAS]",
            "        dttm_col = columns_by_name.get(granularity) if granularity else None",
            "",
            "        if need_groupby:",
            "            # dedup columns while preserving order",
            "            columns = groupby or columns",
            "            for selected in columns:",
            "                if isinstance(selected, str):",
            "                    # if groupby field/expr equals granularity field/expr",
            "                    if selected == granularity:",
            "                        table_col = columns_by_name[selected]",
            "                        outer = table_col.get_timestamp_expression(",
            "                            time_grain=time_grain,",
            "                            label=selected,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    # if groupby field equals a selected column",
            "                    elif selected in columns_by_name:",
            "                        outer = self.convert_tbl_column_to_sqla_col(",
            "                            columns_by_name[selected],",
            "                            template_processor=template_processor,",
            "                        )",
            "                    else:",
            "                        selected = validate_adhoc_subquery(",
            "                            selected,",
            "                            self.database_id,",
            "                            self.schema,",
            "                        )",
            "                        outer = literal_column(f\"({selected})\")",
            "                        outer = self.make_sqla_column_compatible(outer, selected)",
            "                else:",
            "                    outer = self.adhoc_column_to_sqla(",
            "                        col=selected, template_processor=template_processor",
            "                    )",
            "                groupby_all_columns[outer.name] = outer",
            "                if (",
            "                    is_timeseries and not series_column_labels",
            "                ) or outer.name in series_column_labels:",
            "                    groupby_series_columns[outer.name] = outer",
            "                select_exprs.append(outer)",
            "        elif columns:",
            "            for selected in columns:",
            "                if is_adhoc_column(selected):",
            "                    _sql = selected[\"sqlExpression\"]",
            "                    _column_label = selected[\"label\"]",
            "                elif isinstance(selected, str):",
            "                    _sql = selected",
            "                    _column_label = selected",
            "",
            "                selected = validate_adhoc_subquery(",
            "                    _sql,",
            "                    self.database_id,",
            "                    self.schema,",
            "                )",
            "",
            "                select_exprs.append(",
            "                    self.convert_tbl_column_to_sqla_col(",
            "                        columns_by_name[selected], template_processor=template_processor",
            "                    )",
            "                    if isinstance(selected, str) and selected in columns_by_name",
            "                    else self.make_sqla_column_compatible(",
            "                        literal_column(selected), _column_label",
            "                    )",
            "                )",
            "            metrics_exprs = []",
            "",
            "        if granularity:",
            "            if granularity not in columns_by_name or not dttm_col:",
            "                raise QueryObjectValidationError(",
            "                    _(",
            "                        'Time column \"%(col)s\" does not exist in dataset',",
            "                        col=granularity,",
            "                    )",
            "                )",
            "            time_filters = []",
            "",
            "            if is_timeseries:",
            "                timestamp = dttm_col.get_timestamp_expression(",
            "                    time_grain=time_grain, template_processor=template_processor",
            "                )",
            "                # always put timestamp as the first column",
            "                select_exprs.insert(0, timestamp)",
            "                groupby_all_columns[timestamp.name] = timestamp",
            "",
            "            # Use main dttm column to support index with secondary dttm columns.",
            "            if (",
            "                self.always_filter_main_dttm",
            "                and self.main_dttm_col in self.dttm_cols",
            "                and self.main_dttm_col != dttm_col.column_name",
            "            ):",
            "                time_filters.append(",
            "                    self.get_time_filter(",
            "                        time_col=columns_by_name[self.main_dttm_col],",
            "                        start_dttm=from_dttm,",
            "                        end_dttm=to_dttm,",
            "                        template_processor=template_processor,",
            "                    )",
            "                )",
            "",
            "            time_filter_column = self.get_time_filter(",
            "                time_col=dttm_col,",
            "                start_dttm=from_dttm,",
            "                end_dttm=to_dttm,",
            "                template_processor=template_processor,",
            "            )",
            "            time_filters.append(time_filter_column)",
            "",
            "        # Always remove duplicates by column name, as sometimes `metrics_exprs`",
            "        # can have the same name as a groupby column (e.g. when users use",
            "        # raw columns as custom SQL adhoc metric).",
            "        select_exprs = remove_duplicates(",
            "            select_exprs + metrics_exprs, key=lambda x: x.name",
            "        )",
            "",
            "        # Expected output columns",
            "        labels_expected = [c.key for c in select_exprs]",
            "",
            "        # Order by columns are \"hidden\" columns, some databases require them",
            "        # always be present in SELECT if an aggregation function is used",
            "        if not db_engine_spec.allows_hidden_orderby_agg:",
            "            select_exprs = remove_duplicates(select_exprs + orderby_exprs)",
            "",
            "        qry = sa.select(select_exprs)",
            "",
            "        tbl, cte = self.get_from_clause(template_processor)",
            "",
            "        if groupby_all_columns:",
            "            qry = qry.group_by(*groupby_all_columns.values())",
            "",
            "        where_clause_and = []",
            "        having_clause_and = []",
            "",
            "        for flt in filter:  # type: ignore",
            "            if not all(flt.get(s) for s in [\"col\", \"op\"]):",
            "                continue",
            "            flt_col = flt[\"col\"]",
            "            val = flt.get(\"val\")",
            "            flt_grain = flt.get(\"grain\")",
            "            op = flt[\"op\"].upper()",
            "            col_obj: Optional[\"TableColumn\"] = None",
            "            sqla_col: Optional[Column] = None",
            "            if flt_col == utils.DTTM_ALIAS and is_timeseries and dttm_col:",
            "                col_obj = dttm_col",
            "            elif is_adhoc_column(flt_col):",
            "                try:",
            "                    sqla_col = self.adhoc_column_to_sqla(flt_col, force_type_check=True)",
            "                    applied_adhoc_filters_columns.append(flt_col)",
            "                except ColumnNotFoundException:",
            "                    rejected_adhoc_filters_columns.append(flt_col)",
            "                    continue",
            "            else:",
            "                col_obj = columns_by_name.get(cast(str, flt_col))",
            "            filter_grain = flt.get(\"grain\")",
            "",
            "            if is_feature_enabled(\"ENABLE_TEMPLATE_REMOVE_FILTERS\"):",
            "                if get_column_name(flt_col) in removed_filters:",
            "                    # Skip generating SQLA filter when the jinja template handles it.",
            "                    continue",
            "",
            "            if col_obj or sqla_col is not None:",
            "                if sqla_col is not None:",
            "                    pass",
            "                elif col_obj and filter_grain:",
            "                    sqla_col = col_obj.get_timestamp_expression(",
            "                        time_grain=filter_grain, template_processor=template_processor",
            "                    )",
            "                elif col_obj:",
            "                    sqla_col = self.convert_tbl_column_to_sqla_col(",
            "                        tbl_column=col_obj, template_processor=template_processor",
            "                    )",
            "                col_type = col_obj.type if col_obj else None",
            "                col_spec = db_engine_spec.get_column_spec(native_type=col_type)",
            "                is_list_target = op in (",
            "                    utils.FilterOperator.IN.value,",
            "                    utils.FilterOperator.NOT_IN.value,",
            "                )",
            "",
            "                col_advanced_data_type = col_obj.advanced_data_type if col_obj else \"\"",
            "",
            "                if col_spec and not col_advanced_data_type:",
            "                    target_generic_type = col_spec.generic_type",
            "                else:",
            "                    target_generic_type = GenericDataType.STRING",
            "                eq = self.filter_values_handler(",
            "                    values=val,",
            "                    operator=op,",
            "                    target_generic_type=target_generic_type,",
            "                    target_native_type=col_type,",
            "                    is_list_target=is_list_target,",
            "                    db_engine_spec=db_engine_spec,",
            "                )",
            "                if (",
            "                    col_advanced_data_type != \"\"",
            "                    and feature_flag_manager.is_feature_enabled(",
            "                        \"ENABLE_ADVANCED_DATA_TYPES\"",
            "                    )",
            "                    and col_advanced_data_type in ADVANCED_DATA_TYPES",
            "                ):",
            "                    values = eq if is_list_target else [eq]  # type: ignore",
            "                    bus_resp: AdvancedDataTypeResponse = ADVANCED_DATA_TYPES[",
            "                        col_advanced_data_type",
            "                    ].translate_type(",
            "                        {",
            "                            \"type\": col_advanced_data_type,",
            "                            \"values\": values,",
            "                        }",
            "                    )",
            "                    if bus_resp[\"error_message\"]:",
            "                        raise AdvancedDataTypeResponseError(",
            "                            _(bus_resp[\"error_message\"])",
            "                        )",
            "",
            "                    where_clause_and.append(",
            "                        ADVANCED_DATA_TYPES[col_advanced_data_type].translate_filter(",
            "                            sqla_col, op, bus_resp[\"values\"]",
            "                        )",
            "                    )",
            "                elif is_list_target:",
            "                    assert isinstance(eq, (tuple, list))",
            "                    if len(eq) == 0:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Filter value list cannot be empty\")",
            "                        )",
            "                    if len(eq) > len(",
            "                        eq_without_none := [x for x in eq if x is not None]",
            "                    ):",
            "                        is_null_cond = sqla_col.is_(None)",
            "                        if eq:",
            "                            cond = or_(is_null_cond, sqla_col.in_(eq_without_none))",
            "                        else:",
            "                            cond = is_null_cond",
            "                    else:",
            "                        cond = sqla_col.in_(eq)",
            "                    if op == utils.FilterOperator.NOT_IN.value:",
            "                        cond = ~cond",
            "                    where_clause_and.append(cond)",
            "                elif op == utils.FilterOperator.IS_NULL.value:",
            "                    where_clause_and.append(sqla_col.is_(None))",
            "                elif op == utils.FilterOperator.IS_NOT_NULL.value:",
            "                    where_clause_and.append(sqla_col.isnot(None))",
            "                elif op == utils.FilterOperator.IS_TRUE.value:",
            "                    where_clause_and.append(sqla_col.is_(True))",
            "                elif op == utils.FilterOperator.IS_FALSE.value:",
            "                    where_clause_and.append(sqla_col.is_(False))",
            "                else:",
            "                    if (",
            "                        op",
            "                        not in {",
            "                            utils.FilterOperator.EQUALS.value,",
            "                            utils.FilterOperator.NOT_EQUALS.value,",
            "                        }",
            "                        and eq is None",
            "                    ):",
            "                        raise QueryObjectValidationError(",
            "                            _(",
            "                                \"Must specify a value for filters \"",
            "                                \"with comparison operators\"",
            "                            )",
            "                        )",
            "                    if op == utils.FilterOperator.EQUALS.value:",
            "                        where_clause_and.append(sqla_col == eq)",
            "                    elif op == utils.FilterOperator.NOT_EQUALS.value:",
            "                        where_clause_and.append(sqla_col != eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN.value:",
            "                        where_clause_and.append(sqla_col > eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN.value:",
            "                        where_clause_and.append(sqla_col < eq)",
            "                    elif op == utils.FilterOperator.GREATER_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col >= eq)",
            "                    elif op == utils.FilterOperator.LESS_THAN_OR_EQUALS.value:",
            "                        where_clause_and.append(sqla_col <= eq)",
            "                    elif op in {",
            "                        utils.FilterOperator.ILIKE.value,",
            "                        utils.FilterOperator.LIKE.value,",
            "                    }:",
            "                        if target_generic_type != GenericDataType.STRING:",
            "                            sqla_col = sa.cast(sqla_col, sa.String)",
            "",
            "                        if op == utils.FilterOperator.LIKE.value:",
            "                            where_clause_and.append(sqla_col.like(eq))",
            "                        else:",
            "                            where_clause_and.append(sqla_col.ilike(eq))",
            "                    elif (",
            "                        op == utils.FilterOperator.TEMPORAL_RANGE.value",
            "                        and isinstance(eq, str)",
            "                        and col_obj is not None",
            "                    ):",
            "                        _since, _until = get_since_until_from_time_range(",
            "                            time_range=eq,",
            "                            time_shift=time_shift,",
            "                            extras=extras,",
            "                        )",
            "                        where_clause_and.append(",
            "                            self.get_time_filter(",
            "                                time_col=col_obj,",
            "                                start_dttm=_since,",
            "                                end_dttm=_until,",
            "                                time_grain=flt_grain,",
            "                                label=sqla_col.key,",
            "                                template_processor=template_processor,",
            "                            )",
            "                        )",
            "                    else:",
            "                        raise QueryObjectValidationError(",
            "                            _(\"Invalid filter operation type: %(op)s\", op=op)",
            "                        )",
            "        where_clause_and += self.get_sqla_row_level_filters(template_processor)",
            "        if extras:",
            "            where = extras.get(\"where\")",
            "            if where:",
            "                try:",
            "                    where = template_processor.process_template(f\"({where})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in WHERE clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                where = self._process_sql_expression(",
            "                    expression=where,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                where_clause_and += [self.text(where)]",
            "            having = extras.get(\"having\")",
            "            if having:",
            "                try:",
            "                    having = template_processor.process_template(f\"({having})\")",
            "                except TemplateError as ex:",
            "                    raise QueryObjectValidationError(",
            "                        _(",
            "                            \"Error in jinja expression in HAVING clause: %(msg)s\",",
            "                            msg=ex.message,",
            "                        )",
            "                    ) from ex",
            "                having = self._process_sql_expression(",
            "                    expression=having,",
            "                    database_id=self.database_id,",
            "                    schema=self.schema,",
            "                    template_processor=template_processor,",
            "                )",
            "                having_clause_and += [self.text(having)]",
            "",
            "        if apply_fetch_values_predicate and self.fetch_values_predicate:",
            "            qry = qry.where(",
            "                self.get_fetch_values_predicate(template_processor=template_processor)",
            "            )",
            "        if granularity:",
            "            qry = qry.where(and_(*(time_filters + where_clause_and)))",
            "        else:",
            "            qry = qry.where(and_(*where_clause_and))",
            "        qry = qry.having(and_(*having_clause_and))",
            "",
            "        self.make_orderby_compatible(select_exprs, orderby_exprs)",
            "",
            "        for col, (orig_col, ascending) in zip(orderby_exprs, orderby):",
            "            if not db_engine_spec.allows_alias_in_orderby and isinstance(col, Label):",
            "                # if engine does not allow using SELECT alias in ORDER BY",
            "                # revert to the underlying column",
            "                col = col.element",
            "",
            "            if (",
            "                db_engine_spec.get_allows_alias_in_select(self.database)",
            "                and db_engine_spec.allows_hidden_cc_in_orderby",
            "                and col.name in [select_col.name for select_col in select_exprs]",
            "            ):",
            "                with self.database.get_sqla_engine_with_context() as engine:",
            "                    quote = engine.dialect.identifier_preparer.quote",
            "                    col = literal_column(quote(col.name))",
            "            direction = sa.asc if ascending else sa.desc",
            "            qry = qry.order_by(direction(col))",
            "",
            "        if row_limit:",
            "            qry = qry.limit(row_limit)",
            "        if row_offset:",
            "            qry = qry.offset(row_offset)",
            "",
            "        if series_limit and groupby_series_columns:",
            "            if db_engine_spec.allows_joins and db_engine_spec.allows_subqueries:",
            "                # some sql dialects require for order by expressions",
            "                # to also be in the select clause -- others, e.g. vertica,",
            "                # require a unique inner alias",
            "                inner_main_metric_expr = self.make_sqla_column_compatible(",
            "                    main_metric_expr, \"mme_inner__\"",
            "                )",
            "                inner_groupby_exprs = []",
            "                inner_select_exprs = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    inner = self.make_sqla_column_compatible(gby_obj, gby_name + \"__\")",
            "                    inner_groupby_exprs.append(inner)",
            "                    inner_select_exprs.append(inner)",
            "",
            "                inner_select_exprs += [inner_main_metric_expr]",
            "                subq = sa.select(inner_select_exprs).select_from(tbl)",
            "                inner_time_filter = []",
            "",
            "                if dttm_col and not db_engine_spec.time_groupby_inline:",
            "                    inner_time_filter = [",
            "                        self.get_time_filter(",
            "                            time_col=dttm_col,",
            "                            start_dttm=inner_from_dttm or from_dttm,",
            "                            end_dttm=inner_to_dttm or to_dttm,",
            "                            template_processor=template_processor,",
            "                        )",
            "                    ]",
            "                subq = subq.where(and_(*(where_clause_and + inner_time_filter)))",
            "                subq = subq.group_by(*inner_groupby_exprs)",
            "",
            "                ob = inner_main_metric_expr",
            "                if series_limit_metric:",
            "                    ob = self._get_series_orderby(",
            "                        series_limit_metric=series_limit_metric,",
            "                        metrics_by_name=metrics_by_name,",
            "                        columns_by_name=columns_by_name,",
            "                        template_processor=template_processor,",
            "                    )",
            "                direction = sa.desc if order_desc else sa.asc",
            "                subq = subq.order_by(direction(ob))",
            "                subq = subq.limit(series_limit)",
            "",
            "                on_clause = []",
            "                for gby_name, gby_obj in groupby_series_columns.items():",
            "                    # in this case the column name, not the alias, needs to be",
            "                    # conditionally mutated, as it refers to the column alias in",
            "                    # the inner query",
            "                    col_name = db_engine_spec.make_label_compatible(gby_name + \"__\")",
            "                    on_clause.append(gby_obj == sa.column(col_name))",
            "",
            "                tbl = tbl.join(subq.alias(SERIES_LIMIT_SUBQ_ALIAS), and_(*on_clause))",
            "            else:",
            "                if series_limit_metric:",
            "                    orderby = [",
            "                        (",
            "                            self._get_series_orderby(",
            "                                series_limit_metric=series_limit_metric,",
            "                                metrics_by_name=metrics_by_name,",
            "                                columns_by_name=columns_by_name,",
            "                                template_processor=template_processor,",
            "                            ),",
            "                            not order_desc,",
            "                        )",
            "                    ]",
            "",
            "                # run prequery to get top groups",
            "                prequery_obj = {",
            "                    \"is_timeseries\": False,",
            "                    \"row_limit\": series_limit,",
            "                    \"metrics\": metrics,",
            "                    \"granularity\": granularity,",
            "                    \"groupby\": groupby,",
            "                    \"from_dttm\": inner_from_dttm or from_dttm,",
            "                    \"to_dttm\": inner_to_dttm or to_dttm,",
            "                    \"filter\": filter,",
            "                    \"orderby\": orderby,",
            "                    \"extras\": extras,",
            "                    \"columns\": columns,",
            "                    \"order_desc\": True,",
            "                }",
            "",
            "                result = self.query(prequery_obj)",
            "                prequeries.append(result.query)",
            "                dimensions = [",
            "                    c",
            "                    for c in result.df.columns",
            "                    if c not in metrics and c in groupby_series_columns",
            "                ]",
            "                top_groups = self._get_top_groups(",
            "                    result.df, dimensions, groupby_series_columns, columns_by_name",
            "                )",
            "                qry = qry.where(top_groups)",
            "",
            "        qry = qry.select_from(tbl)",
            "",
            "        if is_rowcount:",
            "            if not db_engine_spec.allows_subqueries:",
            "                raise QueryObjectValidationError(",
            "                    _(\"Database does not support subqueries\")",
            "                )",
            "            label = \"rowcount\"",
            "            col = self.make_sqla_column_compatible(literal_column(\"COUNT(*)\"), label)",
            "            qry = sa.select([col]).select_from(qry.alias(\"rowcount_qry\"))",
            "            labels_expected = [label]",
            "",
            "        filter_columns = [flt.get(\"col\") for flt in filter] if filter else []",
            "        rejected_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and col not in self.column_names",
            "            and col not in applied_template_filters",
            "        ] + rejected_adhoc_filters_columns",
            "",
            "        applied_filter_columns = [",
            "            col",
            "            for col in filter_columns",
            "            if col",
            "            and not is_adhoc_column(col)",
            "            and (col in self.column_names or col in applied_template_filters)",
            "        ] + applied_adhoc_filters_columns",
            "",
            "        return SqlaQuery(",
            "            applied_template_filters=applied_template_filters,",
            "            cte=cte,",
            "            applied_filter_columns=applied_filter_columns,",
            "            rejected_filter_columns=rejected_filter_columns,",
            "            extra_cache_keys=extra_cache_keys,",
            "            labels_expected=labels_expected,",
            "            sqla_query=qry,",
            "            prequeries=prequeries,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1097": [
                "ExploreMixin",
                "get_from_clause"
            ]
        },
        "addLocation": []
    },
    "superset/models/sql_lab.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 183,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 184,
                "PatchRowcode": "     @property"
            },
            "2": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "     def sql_tables(self) -> list[Table]:"
            },
            "3": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return list(ParsedQuery(self.sql).tables)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+        return list(ParsedQuery(self.sql, engine=self.db_engine_spec.engine).tables)"
            },
            "5": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 187,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "     @property"
            },
            "7": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "     def columns(self) -> list[\"TableColumn\"]:"
            },
            "8": {
                "beforePatchRowNumber": 427,
                "afterPatchRowNumber": 427,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 428,
                "afterPatchRowNumber": 428,
                "PatchRowcode": "     @property"
            },
            "10": {
                "beforePatchRowNumber": 429,
                "afterPatchRowNumber": 429,
                "PatchRowcode": "     def sql_tables(self) -> list[Table]:"
            },
            "11": {
                "beforePatchRowNumber": 430,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return list(ParsedQuery(self.sql).tables)"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 430,
                "PatchRowcode": "+        return list("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 431,
                "PatchRowcode": "+            ParsedQuery(self.sql, engine=self.database.db_engine_spec.engine).tables"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 432,
                "PatchRowcode": "+        )"
            },
            "15": {
                "beforePatchRowNumber": 431,
                "afterPatchRowNumber": 433,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 432,
                "afterPatchRowNumber": 434,
                "PatchRowcode": "     @property"
            },
            "17": {
                "beforePatchRowNumber": 433,
                "afterPatchRowNumber": 435,
                "PatchRowcode": "     def last_run_humanized(self) -> str:"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"A collection of ORM sqlalchemy models for SQL Lab\"\"\"",
            "import builtins",
            "import inspect",
            "import logging",
            "import re",
            "from collections.abc import Hashable",
            "from datetime import datetime",
            "from typing import Any, Optional, TYPE_CHECKING",
            "",
            "import simplejson as json",
            "import sqlalchemy as sqla",
            "from flask import current_app, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.models.decorators import renders",
            "from flask_babel import gettext as __",
            "from humanize import naturaltime",
            "from sqlalchemy import (",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    Integer,",
            "    Numeric,",
            "    String,",
            "    Text,",
            ")",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.orm import backref, relationship",
            "from sqlalchemy.sql.elements import ColumnElement, literal_column",
            "",
            "from superset import security_manager",
            "from superset.jinja_context import BaseTemplateProcessor, get_template_processor",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    ExploreMixin,",
            "    ExtraJSONMixin,",
            "    ImportExportMixin,",
            ")",
            "from superset.sql_parse import CtasMethod, ParsedQuery, Table",
            "from superset.sqllab.limiting_factor import LimitingFactor",
            "from superset.utils.core import get_column_name, QueryStatus, user_label",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.db_engine_specs import BaseEngineSpec",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class Query(",
            "    Model, ExtraJSONMixin, ExploreMixin",
            "):  # pylint: disable=abstract-method,too-many-public-methods",
            "    \"\"\"ORM model for SQL query",
            "",
            "    Now that SQL Lab support multi-statement execution, an entry in this",
            "    table may represent multiple SQL statements executed sequentially\"\"\"",
            "",
            "    __tablename__ = \"query\"",
            "    type = \"query\"",
            "    id = Column(Integer, primary_key=True)",
            "    client_id = Column(String(11), unique=True, nullable=False)",
            "    query_language = \"sql\"",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "",
            "    # Store the tmp table into the DB only if the user asks for it.",
            "    tmp_table_name = Column(String(256))",
            "    tmp_schema_name = Column(String(256))",
            "    user_id = Column(Integer, ForeignKey(\"ab_user.id\"), nullable=True)",
            "    status = Column(String(16), default=QueryStatus.PENDING)",
            "    tab_name = Column(String(256))",
            "    sql_editor_id = Column(String(256))",
            "    schema = Column(String(256))",
            "    sql = Column(Text)",
            "    # Query to retrieve the results,",
            "    # used only in case of select_as_cta_used is true.",
            "    select_sql = Column(Text)",
            "    executed_sql = Column(Text)",
            "    # Could be configured in the superset config.",
            "    limit = Column(Integer)",
            "    limiting_factor = Column(",
            "        Enum(LimitingFactor), server_default=LimitingFactor.UNKNOWN",
            "    )",
            "    select_as_cta = Column(Boolean)",
            "    select_as_cta_used = Column(Boolean, default=False)",
            "    ctas_method = Column(String(16), default=CtasMethod.TABLE)",
            "",
            "    progress = Column(Integer, default=0)  # 1..100",
            "    # # of rows in the result set or rows modified.",
            "    rows = Column(Integer)",
            "    error_message = Column(Text)",
            "    # key used to store the results in the results backend",
            "    results_key = Column(String(64), index=True)",
            "",
            "    # Using Numeric in place of DateTime for sub-second precision",
            "    # stored as seconds since epoch, allowing for milliseconds",
            "    start_time = Column(Numeric(precision=20, scale=6))",
            "    start_running_time = Column(Numeric(precision=20, scale=6))",
            "    end_time = Column(Numeric(precision=20, scale=6))",
            "    end_result_backend_time = Column(Numeric(precision=20, scale=6))",
            "    tracking_url_raw = Column(Text, name=\"tracking_url\")",
            "",
            "    changed_on = Column(",
            "        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=True",
            "    )",
            "",
            "    database = relationship(",
            "        \"Database\",",
            "        foreign_keys=[database_id],",
            "        backref=backref(\"queries\", cascade=\"all, delete-orphan\"),",
            "    )",
            "    user = relationship(security_manager.user_model, foreign_keys=[user_id])",
            "",
            "    __table_args__ = (sqla.Index(\"ti_user_id_changed_on\", user_id, changed_on),)",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(query=self, database=self.database, **kwargs)",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return {",
            "            \"changed_on\": self.changed_on.isoformat(),",
            "            \"dbId\": self.database_id,",
            "            \"db\": self.database.database_name if self.database else None,",
            "            \"endDttm\": self.end_time,",
            "            \"errorMessage\": self.error_message,",
            "            \"executedSql\": self.executed_sql,",
            "            \"id\": self.client_id,",
            "            \"queryId\": self.id,",
            "            \"limit\": self.limit,",
            "            \"limitingFactor\": self.limiting_factor,",
            "            \"progress\": self.progress,",
            "            \"rows\": self.rows,",
            "            \"schema\": self.schema,",
            "            \"ctas\": self.select_as_cta,",
            "            \"serverId\": self.id,",
            "            \"sql\": self.sql,",
            "            \"sqlEditorId\": self.sql_editor_id,",
            "            \"startDttm\": self.start_time,",
            "            \"state\": self.status.lower(),",
            "            \"tab\": self.tab_name,",
            "            \"tempSchema\": self.tmp_schema_name,",
            "            \"tempTable\": self.tmp_table_name,",
            "            \"userId\": self.user_id,",
            "            \"user\": user_label(self.user),",
            "            \"resultsKey\": self.results_key,",
            "            \"trackingUrl\": self.tracking_url,",
            "            \"extra\": self.extra,",
            "        }",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        \"\"\"Name property\"\"\"",
            "        ts = datetime.now().isoformat()",
            "        ts = ts.replace(\"-\", \"\").replace(\":\", \"\").split(\".\")[0]",
            "        tab = self.tab_name.replace(\" \", \"_\").lower() if self.tab_name else \"notab\"",
            "        tab = re.sub(r\"\\W+\", \"\", tab)",
            "        return f\"sqllab_{tab}_{ts}\"",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @property",
            "    def username(self) -> str:",
            "        return self.user.username",
            "",
            "    @property",
            "    def sql_tables(self) -> list[Table]:",
            "        return list(ParsedQuery(self.sql).tables)",
            "",
            "    @property",
            "    def columns(self) -> list[\"TableColumn\"]:",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            TableColumn,",
            "        )",
            "",
            "        return [",
            "            TableColumn(",
            "                column_name=col[\"column_name\"],",
            "                database=self.database,",
            "                is_dttm=col[\"is_dttm\"],",
            "                filterable=True,",
            "                groupby=True,",
            "                type=col[\"type\"],",
            "            )",
            "            for col in self.extra.get(\"columns\", [])",
            "        ]",
            "",
            "    @property",
            "    def db_extra(self) -> Optional[dict[str, Any]]:",
            "        return None",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        order_by_choices = []",
            "        for col in self.columns:",
            "            column_name = str(col.column_name or \"\")",
            "            order_by_choices.append(",
            "                (json.dumps([column_name, True]), f\"{column_name} \" + __(\"[asc]\"))",
            "            )",
            "            order_by_choices.append(",
            "                (json.dumps([column_name, False]), f\"{column_name} \" + __(\"[desc]\"))",
            "            )",
            "",
            "        return {",
            "            \"time_grain_sqla\": [",
            "                (g.duration, g.name) for g in self.database.grains() or []",
            "            ],",
            "            \"filter_select\": True,",
            "            \"name\": self.tab_name,",
            "            \"columns\": [o.data for o in self.columns],",
            "            \"metrics\": [],",
            "            \"id\": self.id,",
            "            \"type\": self.type,",
            "            \"sql\": self.sql,",
            "            \"owners\": self.owners_data,",
            "            \"database\": {\"id\": self.database_id, \"backend\": self.database.backend},",
            "            \"order_by_choices\": order_by_choices,",
            "            \"schema\": self.schema,",
            "            \"verbose_map\": {},",
            "        }",
            "",
            "    def raise_for_access(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        security_manager.raise_for_access(query=self)",
            "",
            "    @property",
            "    def db_engine_spec(",
            "        self,",
            "    ) -> builtins.type[\"BaseEngineSpec\"]:  # pylint: disable=unsubscriptable-object",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def owners_data(self) -> list[dict[str, Any]]:",
            "        return []",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        return f\"{self.id}__{self.type}\"",
            "",
            "    @property",
            "    def is_rls_supported(self) -> bool:",
            "        return False",
            "",
            "    @property",
            "    def cache_timeout(self) -> int:",
            "        return 0",
            "",
            "    @property",
            "    def column_names(self) -> list[Any]:",
            "        return [col.column_name for col in self.columns]",
            "",
            "    @property",
            "    def offset(self) -> int:",
            "        return 0",
            "",
            "    @property",
            "    def main_dttm_col(self) -> Optional[str]:",
            "        for col in self.columns:",
            "            if col.get(\"is_dttm\"):",
            "                return col.get(\"column_name\")",
            "        return None",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[Any]:",
            "        return [col.column_name for col in self.columns if col.is_dttm]",
            "",
            "    @property",
            "    def schema_perm(self) -> str:",
            "        return f\"{self.database.database_name}.{self.schema}\"",
            "",
            "    @property",
            "    def perm(self) -> str:",
            "        return f\"[{self.database.database_name}].[{self.tab_name}](id:{self.id})\"",
            "",
            "    @property",
            "    def default_endpoint(self) -> str:",
            "        return \"\"",
            "",
            "    def get_extra_cache_keys(self, query_obj: dict[str, Any]) -> list[Hashable]:",
            "        return []",
            "",
            "    @property",
            "    def tracking_url(self) -> Optional[str]:",
            "        \"\"\"",
            "        Transform tracking url at run time because the exact URL may depend",
            "        on query properties such as execution and finish time.",
            "        \"\"\"",
            "        transform = current_app.config.get(\"TRACKING_URL_TRANSFORMER\")",
            "        url = self.tracking_url_raw",
            "        if url and transform:",
            "            sig = inspect.signature(transform)",
            "            # for backward compatibility, users may define a transformer function",
            "            # with only one parameter (`url`).",
            "            args = [url, self][: len(sig.parameters)]",
            "            url = transform(*args)",
            "            logger.debug(\"Transformed tracking url: %s\", url)",
            "        return url",
            "",
            "    @tracking_url.setter",
            "    def tracking_url(self, value: str) -> None:",
            "        self.tracking_url_raw = value",
            "",
            "    def get_column(self, column_name: Optional[str]) -> Optional[dict[str, Any]]:",
            "        if not column_name:",
            "            return None",
            "        for col in self.columns:",
            "            if col.column_name == column_name:",
            "                return col",
            "        return None",
            "",
            "    def adhoc_column_to_sqla(",
            "        self,",
            "        col: \"AdhocColumn\",  # type: ignore",
            "        force_type_check: bool = False,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "        :param col: Adhoc column definition",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = get_column_name(col)",
            "        expression = self._process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        sqla_column = literal_column(expression)",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "",
            "class SavedQuery(Model, AuditMixinNullable, ExtraJSONMixin, ImportExportMixin):",
            "    \"\"\"ORM model for SQL query\"\"\"",
            "",
            "    __tablename__ = \"saved_query\"",
            "    id = Column(Integer, primary_key=True)",
            "    user_id = Column(Integer, ForeignKey(\"ab_user.id\"), nullable=True)",
            "    db_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=True)",
            "    schema = Column(String(128))",
            "    label = Column(String(256))",
            "    description = Column(Text)",
            "    sql = Column(Text)",
            "    template_parameters = Column(Text)",
            "    user = relationship(",
            "        security_manager.user_model,",
            "        backref=backref(\"saved_queries\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[user_id],",
            "    )",
            "    database = relationship(",
            "        \"Database\",",
            "        foreign_keys=[db_id],",
            "        backref=backref(\"saved_queries\", cascade=\"all, delete-orphan\"),",
            "    )",
            "    rows = Column(Integer, nullable=True)",
            "    last_run = Column(DateTime, nullable=True)",
            "    tags = relationship(",
            "        \"Tag\",",
            "        secondary=\"tagged_object\",",
            "        overlaps=\"tags\",",
            "        primaryjoin=\"and_(SavedQuery.id == TaggedObject.object_id)\",",
            "        secondaryjoin=\"and_(TaggedObject.tag_id == Tag.id, \"",
            "        \"TaggedObject.object_type == 'query')\",",
            "    )",
            "",
            "    export_parent = \"database\"",
            "    export_fields = [",
            "        \"schema\",",
            "        \"label\",",
            "        \"description\",",
            "        \"sql\",",
            "    ]",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.label)",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return {",
            "            \"id\": self.id,",
            "        }",
            "",
            "    @property",
            "    def pop_tab_link(self) -> Markup:",
            "        return Markup(",
            "            f\"\"\"",
            "            <a href=\"/sqllab?savedQueryId={self.id}\">",
            "                <i class=\"fa fa-link\"></i>",
            "            </a>",
            "        \"\"\"",
            "        )",
            "",
            "    @property",
            "    def user_email(self) -> str:",
            "        return self.user.email",
            "",
            "    @property",
            "    def sqlalchemy_uri(self) -> URL:",
            "        return self.database.sqlalchemy_uri",
            "",
            "    def url(self) -> str:",
            "        return f\"/sqllab?savedQueryId={self.id}\"",
            "",
            "    @property",
            "    def sql_tables(self) -> list[Table]:",
            "        return list(ParsedQuery(self.sql).tables)",
            "",
            "    @property",
            "    def last_run_humanized(self) -> str:",
            "        return naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @property",
            "    def _last_run_delta_humanized(self) -> str:",
            "        return naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @renders(\"changed_on\")",
            "    def last_run_delta_humanized(self) -> str:",
            "        return self._last_run_delta_humanized",
            "",
            "",
            "class TabState(Model, AuditMixinNullable, ExtraJSONMixin):",
            "    __tablename__ = \"tab_state\"",
            "",
            "    # basic info",
            "    id = Column(Integer, primary_key=True, autoincrement=True)",
            "    user_id = Column(Integer, ForeignKey(\"ab_user.id\"))",
            "    label = Column(String(256))",
            "    active = Column(Boolean, default=False)",
            "",
            "    # selected DB and schema",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\", ondelete=\"CASCADE\"))",
            "    database = relationship(\"Database\", foreign_keys=[database_id])",
            "    schema = Column(String(256))",
            "",
            "    # tables that are open in the schema browser and their data previews",
            "    table_schemas = relationship(",
            "        \"TableSchema\",",
            "        cascade=\"all, delete-orphan\",",
            "        backref=\"tab_state\",",
            "        passive_deletes=True,",
            "    )",
            "",
            "    # the query in the textarea, and results (if any)",
            "    sql = Column(Text)",
            "    query_limit = Column(Integer)",
            "",
            "    # latest query that was run",
            "    latest_query_id = Column(",
            "        Integer, ForeignKey(\"query.client_id\", ondelete=\"SET NULL\")",
            "    )",
            "    latest_query = relationship(\"Query\")",
            "",
            "    # other properties",
            "    autorun = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    hide_left_bar = Column(Boolean, default=False)",
            "",
            "    # any saved queries that are associated with the Tab State",
            "    saved_query_id = Column(",
            "        Integer, ForeignKey(\"saved_query.id\", ondelete=\"SET NULL\"), nullable=True",
            "    )",
            "    saved_query = relationship(\"SavedQuery\", foreign_keys=[saved_query_id])",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return {",
            "            \"id\": self.id,",
            "            \"user_id\": self.user_id,",
            "            \"label\": self.label,",
            "            \"active\": self.active,",
            "            \"database_id\": self.database_id,",
            "            \"schema\": self.schema,",
            "            \"table_schemas\": [ts.to_dict() for ts in self.table_schemas],",
            "            \"sql\": self.sql,",
            "            \"query_limit\": self.query_limit,",
            "            \"latest_query\": self.latest_query.to_dict() if self.latest_query else None,",
            "            \"autorun\": self.autorun,",
            "            \"template_params\": self.template_params,",
            "            \"hide_left_bar\": self.hide_left_bar,",
            "            \"saved_query\": self.saved_query.to_dict() if self.saved_query else None,",
            "            \"extra_json\": self.extra,",
            "        }",
            "",
            "",
            "class TableSchema(Model, AuditMixinNullable, ExtraJSONMixin):",
            "    __tablename__ = \"table_schema\"",
            "",
            "    id = Column(Integer, primary_key=True, autoincrement=True)",
            "    tab_state_id = Column(Integer, ForeignKey(\"tab_state.id\", ondelete=\"CASCADE\"))",
            "",
            "    database_id = Column(",
            "        Integer, ForeignKey(\"dbs.id\", ondelete=\"CASCADE\"), nullable=False",
            "    )",
            "    database = relationship(\"Database\", foreign_keys=[database_id])",
            "    schema = Column(String(256))",
            "    table = Column(String(256))",
            "",
            "    # JSON describing the schema, partitions, latest partition, etc.",
            "    description = Column(Text)",
            "",
            "    expanded = Column(Boolean, default=False)",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        try:",
            "            description = json.loads(self.description)",
            "        except json.JSONDecodeError:",
            "            description = None",
            "",
            "        return {",
            "            \"id\": self.id,",
            "            \"tab_state_id\": self.tab_state_id,",
            "            \"database_id\": self.database_id,",
            "            \"schema\": self.schema,",
            "            \"table\": self.table,",
            "            \"description\": description,",
            "            \"expanded\": self.expanded,",
            "        }"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"A collection of ORM sqlalchemy models for SQL Lab\"\"\"",
            "import builtins",
            "import inspect",
            "import logging",
            "import re",
            "from collections.abc import Hashable",
            "from datetime import datetime",
            "from typing import Any, Optional, TYPE_CHECKING",
            "",
            "import simplejson as json",
            "import sqlalchemy as sqla",
            "from flask import current_app, Markup",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.models.decorators import renders",
            "from flask_babel import gettext as __",
            "from humanize import naturaltime",
            "from sqlalchemy import (",
            "    Boolean,",
            "    Column,",
            "    DateTime,",
            "    Enum,",
            "    ForeignKey,",
            "    Integer,",
            "    Numeric,",
            "    String,",
            "    Text,",
            ")",
            "from sqlalchemy.engine.url import URL",
            "from sqlalchemy.orm import backref, relationship",
            "from sqlalchemy.sql.elements import ColumnElement, literal_column",
            "",
            "from superset import security_manager",
            "from superset.jinja_context import BaseTemplateProcessor, get_template_processor",
            "from superset.models.helpers import (",
            "    AuditMixinNullable,",
            "    ExploreMixin,",
            "    ExtraJSONMixin,",
            "    ImportExportMixin,",
            ")",
            "from superset.sql_parse import CtasMethod, ParsedQuery, Table",
            "from superset.sqllab.limiting_factor import LimitingFactor",
            "from superset.utils.core import get_column_name, QueryStatus, user_label",
            "",
            "if TYPE_CHECKING:",
            "    from superset.connectors.sqla.models import TableColumn",
            "    from superset.db_engine_specs import BaseEngineSpec",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class Query(",
            "    Model, ExtraJSONMixin, ExploreMixin",
            "):  # pylint: disable=abstract-method,too-many-public-methods",
            "    \"\"\"ORM model for SQL query",
            "",
            "    Now that SQL Lab support multi-statement execution, an entry in this",
            "    table may represent multiple SQL statements executed sequentially\"\"\"",
            "",
            "    __tablename__ = \"query\"",
            "    type = \"query\"",
            "    id = Column(Integer, primary_key=True)",
            "    client_id = Column(String(11), unique=True, nullable=False)",
            "    query_language = \"sql\"",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=False)",
            "",
            "    # Store the tmp table into the DB only if the user asks for it.",
            "    tmp_table_name = Column(String(256))",
            "    tmp_schema_name = Column(String(256))",
            "    user_id = Column(Integer, ForeignKey(\"ab_user.id\"), nullable=True)",
            "    status = Column(String(16), default=QueryStatus.PENDING)",
            "    tab_name = Column(String(256))",
            "    sql_editor_id = Column(String(256))",
            "    schema = Column(String(256))",
            "    sql = Column(Text)",
            "    # Query to retrieve the results,",
            "    # used only in case of select_as_cta_used is true.",
            "    select_sql = Column(Text)",
            "    executed_sql = Column(Text)",
            "    # Could be configured in the superset config.",
            "    limit = Column(Integer)",
            "    limiting_factor = Column(",
            "        Enum(LimitingFactor), server_default=LimitingFactor.UNKNOWN",
            "    )",
            "    select_as_cta = Column(Boolean)",
            "    select_as_cta_used = Column(Boolean, default=False)",
            "    ctas_method = Column(String(16), default=CtasMethod.TABLE)",
            "",
            "    progress = Column(Integer, default=0)  # 1..100",
            "    # # of rows in the result set or rows modified.",
            "    rows = Column(Integer)",
            "    error_message = Column(Text)",
            "    # key used to store the results in the results backend",
            "    results_key = Column(String(64), index=True)",
            "",
            "    # Using Numeric in place of DateTime for sub-second precision",
            "    # stored as seconds since epoch, allowing for milliseconds",
            "    start_time = Column(Numeric(precision=20, scale=6))",
            "    start_running_time = Column(Numeric(precision=20, scale=6))",
            "    end_time = Column(Numeric(precision=20, scale=6))",
            "    end_result_backend_time = Column(Numeric(precision=20, scale=6))",
            "    tracking_url_raw = Column(Text, name=\"tracking_url\")",
            "",
            "    changed_on = Column(",
            "        DateTime, default=datetime.utcnow, onupdate=datetime.utcnow, nullable=True",
            "    )",
            "",
            "    database = relationship(",
            "        \"Database\",",
            "        foreign_keys=[database_id],",
            "        backref=backref(\"queries\", cascade=\"all, delete-orphan\"),",
            "    )",
            "    user = relationship(security_manager.user_model, foreign_keys=[user_id])",
            "",
            "    __table_args__ = (sqla.Index(\"ti_user_id_changed_on\", user_id, changed_on),)",
            "",
            "    def get_template_processor(self, **kwargs: Any) -> BaseTemplateProcessor:",
            "        return get_template_processor(query=self, database=self.database, **kwargs)",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return {",
            "            \"changed_on\": self.changed_on.isoformat(),",
            "            \"dbId\": self.database_id,",
            "            \"db\": self.database.database_name if self.database else None,",
            "            \"endDttm\": self.end_time,",
            "            \"errorMessage\": self.error_message,",
            "            \"executedSql\": self.executed_sql,",
            "            \"id\": self.client_id,",
            "            \"queryId\": self.id,",
            "            \"limit\": self.limit,",
            "            \"limitingFactor\": self.limiting_factor,",
            "            \"progress\": self.progress,",
            "            \"rows\": self.rows,",
            "            \"schema\": self.schema,",
            "            \"ctas\": self.select_as_cta,",
            "            \"serverId\": self.id,",
            "            \"sql\": self.sql,",
            "            \"sqlEditorId\": self.sql_editor_id,",
            "            \"startDttm\": self.start_time,",
            "            \"state\": self.status.lower(),",
            "            \"tab\": self.tab_name,",
            "            \"tempSchema\": self.tmp_schema_name,",
            "            \"tempTable\": self.tmp_table_name,",
            "            \"userId\": self.user_id,",
            "            \"user\": user_label(self.user),",
            "            \"resultsKey\": self.results_key,",
            "            \"trackingUrl\": self.tracking_url,",
            "            \"extra\": self.extra,",
            "        }",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        \"\"\"Name property\"\"\"",
            "        ts = datetime.now().isoformat()",
            "        ts = ts.replace(\"-\", \"\").replace(\":\", \"\").split(\".\")[0]",
            "        tab = self.tab_name.replace(\" \", \"_\").lower() if self.tab_name else \"notab\"",
            "        tab = re.sub(r\"\\W+\", \"\", tab)",
            "        return f\"sqllab_{tab}_{ts}\"",
            "",
            "    @property",
            "    def database_name(self) -> str:",
            "        return self.database.name",
            "",
            "    @property",
            "    def username(self) -> str:",
            "        return self.user.username",
            "",
            "    @property",
            "    def sql_tables(self) -> list[Table]:",
            "        return list(ParsedQuery(self.sql, engine=self.db_engine_spec.engine).tables)",
            "",
            "    @property",
            "    def columns(self) -> list[\"TableColumn\"]:",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            TableColumn,",
            "        )",
            "",
            "        return [",
            "            TableColumn(",
            "                column_name=col[\"column_name\"],",
            "                database=self.database,",
            "                is_dttm=col[\"is_dttm\"],",
            "                filterable=True,",
            "                groupby=True,",
            "                type=col[\"type\"],",
            "            )",
            "            for col in self.extra.get(\"columns\", [])",
            "        ]",
            "",
            "    @property",
            "    def db_extra(self) -> Optional[dict[str, Any]]:",
            "        return None",
            "",
            "    @property",
            "    def data(self) -> dict[str, Any]:",
            "        order_by_choices = []",
            "        for col in self.columns:",
            "            column_name = str(col.column_name or \"\")",
            "            order_by_choices.append(",
            "                (json.dumps([column_name, True]), f\"{column_name} \" + __(\"[asc]\"))",
            "            )",
            "            order_by_choices.append(",
            "                (json.dumps([column_name, False]), f\"{column_name} \" + __(\"[desc]\"))",
            "            )",
            "",
            "        return {",
            "            \"time_grain_sqla\": [",
            "                (g.duration, g.name) for g in self.database.grains() or []",
            "            ],",
            "            \"filter_select\": True,",
            "            \"name\": self.tab_name,",
            "            \"columns\": [o.data for o in self.columns],",
            "            \"metrics\": [],",
            "            \"id\": self.id,",
            "            \"type\": self.type,",
            "            \"sql\": self.sql,",
            "            \"owners\": self.owners_data,",
            "            \"database\": {\"id\": self.database_id, \"backend\": self.database.backend},",
            "            \"order_by_choices\": order_by_choices,",
            "            \"schema\": self.schema,",
            "            \"verbose_map\": {},",
            "        }",
            "",
            "    def raise_for_access(self) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        security_manager.raise_for_access(query=self)",
            "",
            "    @property",
            "    def db_engine_spec(",
            "        self,",
            "    ) -> builtins.type[\"BaseEngineSpec\"]:  # pylint: disable=unsubscriptable-object",
            "        return self.database.db_engine_spec",
            "",
            "    @property",
            "    def owners_data(self) -> list[dict[str, Any]]:",
            "        return []",
            "",
            "    @property",
            "    def uid(self) -> str:",
            "        return f\"{self.id}__{self.type}\"",
            "",
            "    @property",
            "    def is_rls_supported(self) -> bool:",
            "        return False",
            "",
            "    @property",
            "    def cache_timeout(self) -> int:",
            "        return 0",
            "",
            "    @property",
            "    def column_names(self) -> list[Any]:",
            "        return [col.column_name for col in self.columns]",
            "",
            "    @property",
            "    def offset(self) -> int:",
            "        return 0",
            "",
            "    @property",
            "    def main_dttm_col(self) -> Optional[str]:",
            "        for col in self.columns:",
            "            if col.get(\"is_dttm\"):",
            "                return col.get(\"column_name\")",
            "        return None",
            "",
            "    @property",
            "    def dttm_cols(self) -> list[Any]:",
            "        return [col.column_name for col in self.columns if col.is_dttm]",
            "",
            "    @property",
            "    def schema_perm(self) -> str:",
            "        return f\"{self.database.database_name}.{self.schema}\"",
            "",
            "    @property",
            "    def perm(self) -> str:",
            "        return f\"[{self.database.database_name}].[{self.tab_name}](id:{self.id})\"",
            "",
            "    @property",
            "    def default_endpoint(self) -> str:",
            "        return \"\"",
            "",
            "    def get_extra_cache_keys(self, query_obj: dict[str, Any]) -> list[Hashable]:",
            "        return []",
            "",
            "    @property",
            "    def tracking_url(self) -> Optional[str]:",
            "        \"\"\"",
            "        Transform tracking url at run time because the exact URL may depend",
            "        on query properties such as execution and finish time.",
            "        \"\"\"",
            "        transform = current_app.config.get(\"TRACKING_URL_TRANSFORMER\")",
            "        url = self.tracking_url_raw",
            "        if url and transform:",
            "            sig = inspect.signature(transform)",
            "            # for backward compatibility, users may define a transformer function",
            "            # with only one parameter (`url`).",
            "            args = [url, self][: len(sig.parameters)]",
            "            url = transform(*args)",
            "            logger.debug(\"Transformed tracking url: %s\", url)",
            "        return url",
            "",
            "    @tracking_url.setter",
            "    def tracking_url(self, value: str) -> None:",
            "        self.tracking_url_raw = value",
            "",
            "    def get_column(self, column_name: Optional[str]) -> Optional[dict[str, Any]]:",
            "        if not column_name:",
            "            return None",
            "        for col in self.columns:",
            "            if col.column_name == column_name:",
            "                return col",
            "        return None",
            "",
            "    def adhoc_column_to_sqla(",
            "        self,",
            "        col: \"AdhocColumn\",  # type: ignore",
            "        force_type_check: bool = False,",
            "        template_processor: Optional[BaseTemplateProcessor] = None,",
            "    ) -> ColumnElement:",
            "        \"\"\"",
            "        Turn an adhoc column into a sqlalchemy column.",
            "        :param col: Adhoc column definition",
            "        :param template_processor: template_processor instance",
            "        :returns: The metric defined as a sqlalchemy column",
            "        :rtype: sqlalchemy.sql.column",
            "        \"\"\"",
            "        label = get_column_name(col)",
            "        expression = self._process_sql_expression(",
            "            expression=col[\"sqlExpression\"],",
            "            database_id=self.database_id,",
            "            schema=self.schema,",
            "            template_processor=template_processor,",
            "        )",
            "        sqla_column = literal_column(expression)",
            "        return self.make_sqla_column_compatible(sqla_column, label)",
            "",
            "",
            "class SavedQuery(Model, AuditMixinNullable, ExtraJSONMixin, ImportExportMixin):",
            "    \"\"\"ORM model for SQL query\"\"\"",
            "",
            "    __tablename__ = \"saved_query\"",
            "    id = Column(Integer, primary_key=True)",
            "    user_id = Column(Integer, ForeignKey(\"ab_user.id\"), nullable=True)",
            "    db_id = Column(Integer, ForeignKey(\"dbs.id\"), nullable=True)",
            "    schema = Column(String(128))",
            "    label = Column(String(256))",
            "    description = Column(Text)",
            "    sql = Column(Text)",
            "    template_parameters = Column(Text)",
            "    user = relationship(",
            "        security_manager.user_model,",
            "        backref=backref(\"saved_queries\", cascade=\"all, delete-orphan\"),",
            "        foreign_keys=[user_id],",
            "    )",
            "    database = relationship(",
            "        \"Database\",",
            "        foreign_keys=[db_id],",
            "        backref=backref(\"saved_queries\", cascade=\"all, delete-orphan\"),",
            "    )",
            "    rows = Column(Integer, nullable=True)",
            "    last_run = Column(DateTime, nullable=True)",
            "    tags = relationship(",
            "        \"Tag\",",
            "        secondary=\"tagged_object\",",
            "        overlaps=\"tags\",",
            "        primaryjoin=\"and_(SavedQuery.id == TaggedObject.object_id)\",",
            "        secondaryjoin=\"and_(TaggedObject.tag_id == Tag.id, \"",
            "        \"TaggedObject.object_type == 'query')\",",
            "    )",
            "",
            "    export_parent = \"database\"",
            "    export_fields = [",
            "        \"schema\",",
            "        \"label\",",
            "        \"description\",",
            "        \"sql\",",
            "    ]",
            "",
            "    def __repr__(self) -> str:",
            "        return str(self.label)",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return {",
            "            \"id\": self.id,",
            "        }",
            "",
            "    @property",
            "    def pop_tab_link(self) -> Markup:",
            "        return Markup(",
            "            f\"\"\"",
            "            <a href=\"/sqllab?savedQueryId={self.id}\">",
            "                <i class=\"fa fa-link\"></i>",
            "            </a>",
            "        \"\"\"",
            "        )",
            "",
            "    @property",
            "    def user_email(self) -> str:",
            "        return self.user.email",
            "",
            "    @property",
            "    def sqlalchemy_uri(self) -> URL:",
            "        return self.database.sqlalchemy_uri",
            "",
            "    def url(self) -> str:",
            "        return f\"/sqllab?savedQueryId={self.id}\"",
            "",
            "    @property",
            "    def sql_tables(self) -> list[Table]:",
            "        return list(",
            "            ParsedQuery(self.sql, engine=self.database.db_engine_spec.engine).tables",
            "        )",
            "",
            "    @property",
            "    def last_run_humanized(self) -> str:",
            "        return naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @property",
            "    def _last_run_delta_humanized(self) -> str:",
            "        return naturaltime(datetime.now() - self.changed_on)",
            "",
            "    @renders(\"changed_on\")",
            "    def last_run_delta_humanized(self) -> str:",
            "        return self._last_run_delta_humanized",
            "",
            "",
            "class TabState(Model, AuditMixinNullable, ExtraJSONMixin):",
            "    __tablename__ = \"tab_state\"",
            "",
            "    # basic info",
            "    id = Column(Integer, primary_key=True, autoincrement=True)",
            "    user_id = Column(Integer, ForeignKey(\"ab_user.id\"))",
            "    label = Column(String(256))",
            "    active = Column(Boolean, default=False)",
            "",
            "    # selected DB and schema",
            "    database_id = Column(Integer, ForeignKey(\"dbs.id\", ondelete=\"CASCADE\"))",
            "    database = relationship(\"Database\", foreign_keys=[database_id])",
            "    schema = Column(String(256))",
            "",
            "    # tables that are open in the schema browser and their data previews",
            "    table_schemas = relationship(",
            "        \"TableSchema\",",
            "        cascade=\"all, delete-orphan\",",
            "        backref=\"tab_state\",",
            "        passive_deletes=True,",
            "    )",
            "",
            "    # the query in the textarea, and results (if any)",
            "    sql = Column(Text)",
            "    query_limit = Column(Integer)",
            "",
            "    # latest query that was run",
            "    latest_query_id = Column(",
            "        Integer, ForeignKey(\"query.client_id\", ondelete=\"SET NULL\")",
            "    )",
            "    latest_query = relationship(\"Query\")",
            "",
            "    # other properties",
            "    autorun = Column(Boolean, default=False)",
            "    template_params = Column(Text)",
            "    hide_left_bar = Column(Boolean, default=False)",
            "",
            "    # any saved queries that are associated with the Tab State",
            "    saved_query_id = Column(",
            "        Integer, ForeignKey(\"saved_query.id\", ondelete=\"SET NULL\"), nullable=True",
            "    )",
            "    saved_query = relationship(\"SavedQuery\", foreign_keys=[saved_query_id])",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        return {",
            "            \"id\": self.id,",
            "            \"user_id\": self.user_id,",
            "            \"label\": self.label,",
            "            \"active\": self.active,",
            "            \"database_id\": self.database_id,",
            "            \"schema\": self.schema,",
            "            \"table_schemas\": [ts.to_dict() for ts in self.table_schemas],",
            "            \"sql\": self.sql,",
            "            \"query_limit\": self.query_limit,",
            "            \"latest_query\": self.latest_query.to_dict() if self.latest_query else None,",
            "            \"autorun\": self.autorun,",
            "            \"template_params\": self.template_params,",
            "            \"hide_left_bar\": self.hide_left_bar,",
            "            \"saved_query\": self.saved_query.to_dict() if self.saved_query else None,",
            "            \"extra_json\": self.extra,",
            "        }",
            "",
            "",
            "class TableSchema(Model, AuditMixinNullable, ExtraJSONMixin):",
            "    __tablename__ = \"table_schema\"",
            "",
            "    id = Column(Integer, primary_key=True, autoincrement=True)",
            "    tab_state_id = Column(Integer, ForeignKey(\"tab_state.id\", ondelete=\"CASCADE\"))",
            "",
            "    database_id = Column(",
            "        Integer, ForeignKey(\"dbs.id\", ondelete=\"CASCADE\"), nullable=False",
            "    )",
            "    database = relationship(\"Database\", foreign_keys=[database_id])",
            "    schema = Column(String(256))",
            "    table = Column(String(256))",
            "",
            "    # JSON describing the schema, partitions, latest partition, etc.",
            "    description = Column(Text)",
            "",
            "    expanded = Column(Boolean, default=False)",
            "",
            "    def to_dict(self) -> dict[str, Any]:",
            "        try:",
            "            description = json.loads(self.description)",
            "        except json.JSONDecodeError:",
            "            description = None",
            "",
            "        return {",
            "            \"id\": self.id,",
            "            \"tab_state_id\": self.tab_state_id,",
            "            \"database_id\": self.database_id,",
            "            \"schema\": self.schema,",
            "            \"table\": self.table,",
            "            \"description\": description,",
            "            \"expanded\": self.expanded,",
            "        }"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "186": [
                "Query",
                "sql_tables"
            ],
            "430": [
                "SavedQuery",
                "sql_tables"
            ]
        },
        "addLocation": []
    },
    "superset/security/manager.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1877,
                "afterPatchRowNumber": 1877,
                "PatchRowcode": "                 default_schema = database.get_default_schema_for_query(query)"
            },
            "1": {
                "beforePatchRowNumber": 1878,
                "afterPatchRowNumber": 1878,
                "PatchRowcode": "                 tables = {"
            },
            "2": {
                "beforePatchRowNumber": 1879,
                "afterPatchRowNumber": 1879,
                "PatchRowcode": "                     Table(table_.table, table_.schema or default_schema)"
            },
            "3": {
                "beforePatchRowNumber": 1880,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    for table_ in sql_parse.ParsedQuery(query.sql).tables"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1880,
                "PatchRowcode": "+                    for table_ in sql_parse.ParsedQuery("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1881,
                "PatchRowcode": "+                        query.sql,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1882,
                "PatchRowcode": "+                        engine=database.db_engine_spec.engine,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1883,
                "PatchRowcode": "+                    ).tables"
            },
            "8": {
                "beforePatchRowNumber": 1881,
                "afterPatchRowNumber": 1884,
                "PatchRowcode": "                 }"
            },
            "9": {
                "beforePatchRowNumber": 1882,
                "afterPatchRowNumber": 1885,
                "PatchRowcode": "             elif table:"
            },
            "10": {
                "beforePatchRowNumber": 1883,
                "afterPatchRowNumber": 1886,
                "PatchRowcode": "                 tables = {table}"
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"A set of constants and methods to manage permissions and security\"\"\"",
            "import json",
            "import logging",
            "import re",
            "import time",
            "from collections import defaultdict",
            "from typing import Any, Callable, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "from flask import current_app, Flask, g, Request",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.manager import SecurityManager",
            "from flask_appbuilder.security.sqla.models import (",
            "    assoc_permissionview_role,",
            "    assoc_user_role,",
            "    Permission,",
            "    PermissionView,",
            "    Role,",
            "    User,",
            "    ViewMenu,",
            ")",
            "from flask_appbuilder.security.views import (",
            "    PermissionModelView,",
            "    PermissionViewModelView,",
            "    RoleModelView,",
            "    UserModelView,",
            "    ViewMenuModelView,",
            ")",
            "from flask_appbuilder.widgets import ListWidget",
            "from flask_babel import lazy_gettext as _",
            "from flask_login import AnonymousUserMixin, LoginManager",
            "from jwt.api_jwt import _jwt_global_obj",
            "from sqlalchemy import and_, inspect, or_",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.orm import eagerload, Session",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.orm.query import Query as SqlaQuery",
            "",
            "from superset import sql_parse",
            "from superset.constants import RouteMethod",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    DatasetInvalidPermissionEvaluationException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.security.guest_token import (",
            "    GuestToken,",
            "    GuestTokenResources,",
            "    GuestTokenResourceType,",
            "    GuestTokenRlsRule,",
            "    GuestTokenUser,",
            "    GuestUser,",
            ")",
            "from superset.utils.core import (",
            "    DatasourceName,",
            "    DatasourceType,",
            "    get_user_id,",
            "    RowLevelSecurityFilterType,",
            ")",
            "from superset.utils.filters import get_dataset_access_filters",
            "from superset.utils.urls import get_url_host",
            "",
            "if TYPE_CHECKING:",
            "    from superset.common.query_context import QueryContext",
            "    from superset.connectors.sqla.models import (",
            "        BaseDatasource,",
            "        RowLevelSecurityFilter,",
            "        SqlaTable,",
            "    )",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.sql_lab import Query",
            "    from superset.sql_parse import Table",
            "    from superset.viz import BaseViz",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "DATABASE_PERM_REGEX = re.compile(r\"^\\[.+\\]\\.\\(id\\:(?P<id>\\d+)\\)$\")",
            "",
            "",
            "class DatabaseAndSchema(NamedTuple):",
            "    database: str",
            "    schema: str",
            "",
            "",
            "class SupersetSecurityListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Redeclaring to avoid circular imports",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list.html\"",
            "",
            "",
            "class SupersetRoleListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Role model view from FAB already uses a custom list widget override",
            "    So we override the override",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list_role.html\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        kwargs[\"appbuilder\"] = current_app.appbuilder",
            "        super().__init__(**kwargs)",
            "",
            "",
            "UserModelView.list_widget = SupersetSecurityListWidget",
            "RoleModelView.list_widget = SupersetRoleListWidget",
            "PermissionViewModelView.list_widget = SupersetSecurityListWidget",
            "PermissionModelView.list_widget = SupersetSecurityListWidget",
            "",
            "# Limiting routes on FAB model views",
            "UserModelView.include_route_methods = RouteMethod.CRUD_SET | {",
            "    RouteMethod.ACTION,",
            "    RouteMethod.API_READ,",
            "    RouteMethod.ACTION_POST,",
            "    \"userinfo\",",
            "}",
            "RoleModelView.include_route_methods = RouteMethod.CRUD_SET",
            "PermissionViewModelView.include_route_methods = {RouteMethod.LIST}",
            "PermissionModelView.include_route_methods = {RouteMethod.LIST}",
            "ViewMenuModelView.include_route_methods = {RouteMethod.LIST}",
            "",
            "RoleModelView.list_columns = [\"name\"]",
            "RoleModelView.edit_columns = [\"name\", \"permissions\", \"user\"]",
            "RoleModelView.related_views = []",
            "",
            "",
            "class SupersetSecurityManager(  # pylint: disable=too-many-public-methods",
            "    SecurityManager",
            "):",
            "    userstatschartview = None",
            "    READ_ONLY_MODEL_VIEWS = {\"Database\", \"DynamicPlugin\"}",
            "",
            "    USER_MODEL_VIEWS = {",
            "        \"RegisterUserModelView\",",
            "        \"UserDBModelView\",",
            "        \"UserLDAPModelView\",",
            "        \"UserInfoEditView\",",
            "        \"UserOAuthModelView\",",
            "        \"UserOIDModelView\",",
            "        \"UserRemoteUserModelView\",",
            "    }",
            "",
            "    GAMMA_READ_ONLY_MODEL_VIEWS = {",
            "        \"Dataset\",",
            "        \"Datasource\",",
            "    } | READ_ONLY_MODEL_VIEWS",
            "",
            "    ADMIN_ONLY_VIEW_MENUS = {",
            "        \"Access Requests\",",
            "        \"Action Log\",",
            "        \"Log\",",
            "        \"List Users\",",
            "        \"List Roles\",",
            "        \"ResetPasswordView\",",
            "        \"RoleModelView\",",
            "        \"Row Level Security\",",
            "        \"Row Level Security Filters\",",
            "        \"RowLevelSecurityFiltersModelView\",",
            "        \"Security\",",
            "        \"SQL Lab\",",
            "        \"User Registrations\",",
            "        \"User's Statistics\",",
            "    } | USER_MODEL_VIEWS",
            "",
            "    ALPHA_ONLY_VIEW_MENUS = {",
            "        \"Alerts & Report\",",
            "        \"Annotation Layers\",",
            "        \"Annotation\",",
            "        \"CSS Templates\",",
            "        \"ColumnarToDatabaseView\",",
            "        \"CssTemplate\",",
            "        \"CsvToDatabaseView\",",
            "        \"ExcelToDatabaseView\",",
            "        \"Import dashboards\",",
            "        \"ImportExportRestApi\",",
            "        \"Manage\",",
            "        \"Queries\",",
            "        \"ReportSchedule\",",
            "        \"TableSchemaView\",",
            "        \"Upload a CSV\",",
            "    }",
            "",
            "    ADMIN_ONLY_PERMISSIONS = {",
            "        \"can_update_role\",",
            "        \"all_query_access\",",
            "        \"can_grant_guest_token\",",
            "        \"can_set_embedded\",",
            "        \"can_warm_up_cache\",",
            "    }",
            "",
            "    READ_ONLY_PERMISSION = {",
            "        \"can_show\",",
            "        \"can_list\",",
            "        \"can_get\",",
            "        \"can_external_metadata\",",
            "        \"can_external_metadata_by_name\",",
            "        \"can_read\",",
            "    }",
            "",
            "    ALPHA_ONLY_PERMISSIONS = {",
            "        \"muldelete\",",
            "        \"all_database_access\",",
            "        \"all_datasource_access\",",
            "    }",
            "",
            "    OBJECT_SPEC_PERMISSIONS = {",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "    }",
            "",
            "    ACCESSIBLE_PERMS = {\"can_userinfo\", \"resetmypassword\", \"can_recent_activity\"}",
            "",
            "    SQLLAB_ONLY_PERMISSIONS = {",
            "        (\"can_my_queries\", \"SqlLab\"),",
            "        (\"can_read\", \"SavedQuery\"),",
            "        (\"can_write\", \"SavedQuery\"),",
            "        (\"can_export\", \"SavedQuery\"),",
            "        (\"can_read\", \"Query\"),",
            "        (\"can_export_csv\", \"Query\"),",
            "        (\"can_get_results\", \"SQLLab\"),",
            "        (\"can_execute_sql_query\", \"SQLLab\"),",
            "        (\"can_estimate_query_cost\", \"SQL Lab\"),",
            "        (\"can_export_csv\", \"SQLLab\"),",
            "        (\"can_read\", \"SQLLab\"),",
            "        (\"can_sqllab_history\", \"Superset\"),",
            "        (\"can_sqllab\", \"Superset\"),",
            "        (\"can_test_conn\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_activate\", \"TabStateView\"),",
            "        (\"can_get\", \"TabStateView\"),",
            "        (\"can_delete_query\", \"TabStateView\"),",
            "        (\"can_post\", \"TabStateView\"),",
            "        (\"can_delete\", \"TabStateView\"),",
            "        (\"can_put\", \"TabStateView\"),",
            "        (\"can_migrate_query\", \"TabStateView\"),",
            "        (\"menu_access\", \"SQL Lab\"),",
            "        (\"menu_access\", \"SQL Editor\"),",
            "        (\"menu_access\", \"Saved Queries\"),",
            "        (\"menu_access\", \"Query Search\"),",
            "    }",
            "",
            "    SQLLAB_EXTRA_PERMISSION_VIEWS = {",
            "        (\"can_csv\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_read\", \"Superset\"),",
            "        (\"can_read\", \"Database\"),",
            "    }",
            "",
            "    data_access_permissions = (",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "        \"all_datasource_access\",",
            "        \"all_database_access\",",
            "        \"all_query_access\",",
            "    )",
            "",
            "    guest_user_cls = GuestUser",
            "    pyjwt_for_guest_token = _jwt_global_obj",
            "",
            "    def create_login_manager(self, app: Flask) -> LoginManager:",
            "        lm = super().create_login_manager(app)",
            "        lm.request_loader(self.request_loader)",
            "        return lm",
            "",
            "    def request_loader(self, request: Request) -> Optional[User]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.extensions import feature_flag_manager",
            "",
            "        if feature_flag_manager.is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return self.get_guest_user_from_request(request)",
            "        return None",
            "",
            "    def get_schema_perm(",
            "        self, database: Union[\"Database\", str], schema: Optional[str] = None",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the database specific schema permission.",
            "",
            "        :param database: The Superset database or database name",
            "        :param schema: The Superset schema name",
            "        :return: The database specific schema permission",
            "        \"\"\"",
            "        return f\"[{database}].[{schema}]\" if schema else None",
            "",
            "    @staticmethod",
            "    def get_database_perm(database_id: int, database_name: str) -> Optional[str]:",
            "        return f\"[{database_name}].(id:{database_id})\"",
            "",
            "    @staticmethod",
            "    def get_dataset_perm(",
            "        dataset_id: int,",
            "        dataset_name: str,",
            "        database_name: str,",
            "    ) -> Optional[str]:",
            "        return f\"[{database_name}].[{dataset_name}](id:{dataset_id})\"",
            "",
            "    def unpack_database_and_schema(self, schema_permission: str) -> DatabaseAndSchema:",
            "        # [database_name].[schema|table]",
            "",
            "        schema_name = schema_permission.split(\".\")[1][1:-1]",
            "        database_name = schema_permission.split(\".\")[0][1:-1]",
            "        return DatabaseAndSchema(database_name, schema_name)",
            "",
            "    def can_access(self, permission_name: str, view_name: str) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the FAB permission/view, False otherwise.",
            "",
            "        Note this method adds protection from has_access failing from missing",
            "        permission/view entries.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_name: The FAB view-menu name",
            "        :returns: Whether the user can access the FAB permission/view",
            "        \"\"\"",
            "",
            "        user = g.user",
            "        if user.is_anonymous:",
            "            return self.is_item_public(permission_name, view_name)",
            "        return self._has_view_access(user, permission_name, view_name)",
            "",
            "    def can_access_all_queries(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all SQL Lab queries, False otherwise.",
            "",
            "        :returns: Whether the user can access all queries",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_query_access\", \"all_query_access\")",
            "",
            "    def can_access_all_datasources(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the datasources, False otherwise.",
            "",
            "        :returns: Whether the user can access all the datasources",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_datasource_access\", \"all_datasource_access\")",
            "",
            "    def can_access_all_databases(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the databases, False otherwise.",
            "",
            "        :returns: Whether the user can access all the databases",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_database_access\", \"all_database_access\")",
            "",
            "    def can_access_database(self, database: \"Database\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified database, False otherwise.",
            "",
            "        :param database: The database",
            "        :returns: Whether the user can access the database",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_all_databases()",
            "            or self.can_access(\"database_access\", database.perm)  # type: ignore",
            "        )",
            "",
            "    def can_access_schema(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the schema associated with specified",
            "        datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource's schema",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_database(datasource.database)",
            "            or self.can_access(\"schema_access\", datasource.schema_perm or \"\")",
            "        )",
            "",
            "    def can_access_datasource(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(datasource=datasource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def can_access_dashboard(self, dashboard: \"Dashboard\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified dashboard, False otherwise.",
            "",
            "        :param dashboard: The dashboard",
            "        :returns: Whether the user can access the dashboard",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(dashboard=dashboard)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def get_dashboard_access_error_object(  # pylint: disable=invalid-name",
            "        self,",
            "        dashboard: \"Dashboard\",  # pylint: disable=unused-argument",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset dashboard.",
            "",
            "        :param dashboard: The denied Superset dashboard",
            "        :returns: The error object",
            "        \"\"\"",
            "",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DASHBOARD_SECURITY_ACCESS_ERROR,",
            "            message=\"You don't have access to this dashboard.\",",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_error_msg(datasource: \"BaseDatasource\") -> str:",
            "        \"\"\"",
            "        Return the error message for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        return (",
            "            f\"This endpoint requires the datasource {datasource.name}, \"",
            "            \"database or `all_datasource_access` permission\"",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_link(  # pylint: disable=unused-argument",
            "        datasource: \"BaseDatasource\",",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the link for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_datasource_access_error_object(  # pylint: disable=invalid-name",
            "        self, datasource: \"BaseDatasource\"",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_datasource_access_error_msg(datasource),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_datasource_access_link(datasource),",
            "                \"datasource\": datasource.name,",
            "            },",
            "        )",
            "",
            "    def get_table_access_error_msg(self, tables: set[\"Table\"]) -> str:",
            "        \"\"\"",
            "        Return the error message for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        quoted_tables = [f\"`{table}`\" for table in tables]",
            "        return f\"\"\"You need access to the following tables: {\", \".join(quoted_tables)},",
            "            `all_database_access` or `all_datasource_access` permission\"\"\"",
            "",
            "    def get_table_access_error_object(self, tables: set[\"Table\"]) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.TABLE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_table_access_error_msg(tables),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_table_access_link(tables),",
            "                \"tables\": [str(table) for table in tables],",
            "            },",
            "        )",
            "",
            "    def get_table_access_link(  # pylint: disable=unused-argument",
            "        self, tables: set[\"Table\"]",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the access link for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_user_datasources(self) -> list[\"BaseDatasource\"]:",
            "        \"\"\"",
            "        Collect datasources which the user has explicit permissions to.",
            "",
            "        :returns: The list of datasources",
            "        \"\"\"",
            "",
            "        user_datasources = set()",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        user_datasources.update(",
            "            self.get_session.query(SqlaTable)",
            "            .filter(get_dataset_access_filters(SqlaTable))",
            "            .all()",
            "        )",
            "",
            "        # group all datasources by database",
            "        session = self.get_session",
            "        all_datasources = SqlaTable.get_all_datasources(session)",
            "        datasources_by_database: dict[\"Database\", set[\"SqlaTable\"]] = defaultdict(set)",
            "        for datasource in all_datasources:",
            "            datasources_by_database[datasource.database].add(datasource)",
            "",
            "        # add datasources with implicit permission (eg, database access)",
            "        for database, datasources in datasources_by_database.items():",
            "            if self.can_access_database(database):",
            "                user_datasources.update(datasources)",
            "",
            "        return list(user_datasources)",
            "",
            "    def can_access_table(self, database: \"Database\", table: \"Table\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the SQL table, False otherwise.",
            "",
            "        :param database: The SQL database",
            "        :param table: The SQL table",
            "        :returns: Whether the user can access the SQL table",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(database=database, table=table)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def user_view_menu_names(self, permission_name: str) -> set[str]:",
            "        base_query = (",
            "            self.get_session.query(self.viewmenu_model.name)",
            "            .join(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(assoc_permissionview_role)",
            "            .join(self.role_model)",
            "        )",
            "",
            "        if not g.user.is_anonymous:",
            "            # filter by user id",
            "            view_menu_names = (",
            "                base_query.join(assoc_user_role)",
            "                .join(self.user_model)",
            "                .filter(self.user_model.id == get_user_id())",
            "                .filter(self.permission_model.name == permission_name)",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "",
            "        # Properly treat anonymous user",
            "        if public_role := self.get_public_role():",
            "            # filter by public role",
            "            view_menu_names = (",
            "                base_query.filter(self.role_model.id == public_role.id).filter(",
            "                    self.permission_model.name == permission_name",
            "                )",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "        return set()",
            "",
            "    def get_accessible_databases(self) -> list[int]:",
            "        \"\"\"",
            "        Return the list of databases accessible by the user.",
            "",
            "        :return: The list of accessible Databases",
            "        \"\"\"",
            "        perms = self.user_view_menu_names(\"database_access\")",
            "        return [",
            "            int(match.group(\"id\"))",
            "            for perm in perms",
            "            if (match := DATABASE_PERM_REGEX.match(perm))",
            "        ]",
            "",
            "    def get_schemas_accessible_by_user(",
            "        self, database: \"Database\", schemas: list[str], hierarchical: bool = True",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return the list of SQL schemas accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param schemas: The list of eligible SQL schemas",
            "        :param hierarchical: Whether to check using the hierarchical permission logic",
            "        :returns: The list of accessible SQL schemas",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if hierarchical and self.can_access_database(database):",
            "            return schemas",
            "",
            "        # schema_access",
            "        accessible_schemas = {",
            "            self.unpack_database_and_schema(s).schema",
            "            for s in self.user_view_menu_names(\"schema_access\")",
            "            if s.startswith(f\"[{database}].\")",
            "        }",
            "",
            "        # datasource_access",
            "        if perms := self.user_view_menu_names(\"datasource_access\"):",
            "            tables = (",
            "                self.get_session.query(SqlaTable.schema)",
            "                .filter(SqlaTable.database_id == database.id)",
            "                .filter(SqlaTable.schema.isnot(None))",
            "                .filter(SqlaTable.schema != \"\")",
            "                .filter(or_(SqlaTable.perm.in_(perms)))",
            "                .distinct()",
            "            )",
            "            accessible_schemas.update([table.schema for table in tables])",
            "",
            "        return [s for s in schemas if s in accessible_schemas]",
            "",
            "    def get_datasources_accessible_by_user(  # pylint: disable=invalid-name",
            "        self,",
            "        database: \"Database\",",
            "        datasource_names: list[DatasourceName],",
            "        schema: Optional[str] = None,",
            "    ) -> list[DatasourceName]:",
            "        \"\"\"",
            "        Return the list of SQL tables accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param datasource_names: The list of eligible SQL tables w/ schema",
            "        :param schema: The fallback SQL schema if not present in the table name",
            "        :returns: The list of accessible SQL tables w/ schema",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if self.can_access_database(database):",
            "            return datasource_names",
            "",
            "        if schema:",
            "            schema_perm = self.get_schema_perm(database, schema)",
            "            if schema_perm and self.can_access(\"schema_access\", schema_perm):",
            "                return datasource_names",
            "",
            "        user_perms = self.user_view_menu_names(\"datasource_access\")",
            "        schema_perms = self.user_view_menu_names(\"schema_access\")",
            "        user_datasources = SqlaTable.query_datasources_by_permissions(",
            "            self.get_session, database, user_perms, schema_perms",
            "        )",
            "        if schema:",
            "            names = {d.table_name for d in user_datasources if d.schema == schema}",
            "            return [d for d in datasource_names if d.table in names]",
            "",
            "        full_names = {d.full_name for d in user_datasources}",
            "        return [d for d in datasource_names if f\"[{database}].[{d}]\" in full_names]",
            "",
            "    def merge_perm(self, permission_name: str, view_menu_name: str) -> None:",
            "        \"\"\"",
            "        Add the FAB permission/view-menu.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_menu_name: The FAB view-menu name",
            "        :see: SecurityManager.add_permission_view_menu",
            "        \"\"\"",
            "",
            "        logger.warning(",
            "            \"This method 'merge_perm' is deprecated use add_permission_view_menu\"",
            "        )",
            "        self.add_permission_view_menu(permission_name, view_menu_name)",
            "",
            "    def _is_user_defined_permission(self, perm: Model) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission is user defined, False otherwise.",
            "",
            "        :param perm: The FAB permission",
            "        :returns: Whether the FAB permission is user defined",
            "        \"\"\"",
            "",
            "        return perm.permission.name in self.OBJECT_SPEC_PERMISSIONS",
            "",
            "    def create_custom_permissions(self) -> None:",
            "        \"\"\"",
            "        Create custom FAB permissions.",
            "        \"\"\"",
            "        self.add_permission_view_menu(\"all_datasource_access\", \"all_datasource_access\")",
            "        self.add_permission_view_menu(\"all_database_access\", \"all_database_access\")",
            "        self.add_permission_view_menu(\"all_query_access\", \"all_query_access\")",
            "        self.add_permission_view_menu(\"can_csv\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_dashboard\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_chart\", \"Superset\")",
            "",
            "    def create_missing_perms(self) -> None:",
            "        \"\"\"",
            "        Creates missing FAB permissions for datasources, schemas and metrics.",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models import core as models",
            "",
            "        logger.info(\"Fetching a set of all perms to lookup which ones are missing\")",
            "        all_pvs = set()",
            "        for pv in self._get_all_pvms():",
            "            if pv.permission and pv.view_menu:",
            "                all_pvs.add((pv.permission.name, pv.view_menu.name))",
            "",
            "        def merge_pv(view_menu: str, perm: Optional[str]) -> None:",
            "            \"\"\"Create permission view menu only if it doesn't exist\"\"\"",
            "            if view_menu and perm and (view_menu, perm) not in all_pvs:",
            "                self.add_permission_view_menu(view_menu, perm)",
            "",
            "        logger.info(\"Creating missing datasource permissions.\")",
            "        datasources = SqlaTable.get_all_datasources(self.get_session)",
            "        for datasource in datasources:",
            "            merge_pv(\"datasource_access\", datasource.get_perm())",
            "            merge_pv(\"schema_access\", datasource.get_schema_perm())",
            "",
            "        logger.info(\"Creating missing database permissions.\")",
            "        databases = self.get_session.query(models.Database).all()",
            "        for database in databases:",
            "            merge_pv(\"database_access\", database.perm)",
            "",
            "    def clean_perms(self) -> None:",
            "        \"\"\"",
            "        Clean up the FAB faulty permissions.",
            "        \"\"\"",
            "",
            "        logger.info(\"Cleaning faulty perms\")",
            "        sesh = self.get_session",
            "        pvms = sesh.query(PermissionView).filter(",
            "            or_(",
            "                PermissionView.permission  # pylint: disable=singleton-comparison",
            "                == None,",
            "                PermissionView.view_menu  # pylint: disable=singleton-comparison",
            "                == None,",
            "            )",
            "        )",
            "        sesh.commit()",
            "        if deleted_count := pvms.delete():",
            "            logger.info(\"Deleted %i faulty permissions\", deleted_count)",
            "",
            "    def sync_role_definitions(self) -> None:",
            "        \"\"\"",
            "        Initialize the Superset application with security roles and such.",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing role definition\")",
            "",
            "        self.create_custom_permissions()",
            "",
            "        pvms = self._get_all_pvms()",
            "",
            "        # Creating default roles",
            "        self.set_role(\"Admin\", self._is_admin_pvm, pvms)",
            "        self.set_role(\"Alpha\", self._is_alpha_pvm, pvms)",
            "        self.set_role(\"Gamma\", self._is_gamma_pvm, pvms)",
            "        self.set_role(\"sql_lab\", self._is_sql_lab_pvm, pvms)",
            "",
            "        # Configure public role",
            "        if current_app.config[\"PUBLIC_ROLE_LIKE\"]:",
            "            self.copy_role(",
            "                current_app.config[\"PUBLIC_ROLE_LIKE\"],",
            "                self.auth_role_public,",
            "                merge=True,",
            "            )",
            "",
            "        self.create_missing_perms()",
            "",
            "        # commit role and view menu updates",
            "        self.get_session.commit()",
            "        self.clean_perms()",
            "",
            "    def _get_all_pvms(self) -> list[PermissionView]:",
            "        \"\"\"",
            "        Gets list of all PVM",
            "        \"\"\"",
            "        pvms = (",
            "            self.get_session.query(self.permissionview_model)",
            "            .options(",
            "                eagerload(self.permissionview_model.permission),",
            "                eagerload(self.permissionview_model.view_menu),",
            "            )",
            "            .all()",
            "        )",
            "        return [p for p in pvms if p.permission and p.view_menu]",
            "",
            "    def _get_pvms_from_builtin_role(self, role_name: str) -> list[PermissionView]:",
            "        \"\"\"",
            "        Gets a list of model PermissionView permissions inferred from a builtin role",
            "        definition",
            "        \"\"\"",
            "        role_from_permissions_names = self.builtin_roles.get(role_name, [])",
            "        all_pvms = self.get_session.query(PermissionView).all()",
            "        role_from_permissions = []",
            "        for pvm_regex in role_from_permissions_names:",
            "            view_name_regex = pvm_regex[0]",
            "            permission_name_regex = pvm_regex[1]",
            "            for pvm in all_pvms:",
            "                if re.match(view_name_regex, pvm.view_menu.name) and re.match(",
            "                    permission_name_regex, pvm.permission.name",
            "                ):",
            "                    if pvm not in role_from_permissions:",
            "                        role_from_permissions.append(pvm)",
            "        return role_from_permissions",
            "",
            "    def find_roles_by_id(self, role_ids: list[int]) -> list[Role]:",
            "        \"\"\"",
            "        Find a List of models by a list of ids, if defined applies `base_filter`",
            "        \"\"\"",
            "        query = self.get_session.query(Role).filter(Role.id.in_(role_ids))",
            "        return query.all()",
            "",
            "    def copy_role(",
            "        self, role_from_name: str, role_to_name: str, merge: bool = True",
            "    ) -> None:",
            "        \"\"\"",
            "        Copies permissions from a role to another.",
            "",
            "        Note: Supports regex defined builtin roles",
            "",
            "        :param role_from_name: The FAB role name from where the permissions are taken",
            "        :param role_to_name: The FAB role name from where the permissions are copied to",
            "        :param merge: If merge is true, keep data access permissions",
            "            if they already exist on the target role",
            "        \"\"\"",
            "",
            "        logger.info(\"Copy/Merge %s to %s\", role_from_name, role_to_name)",
            "        # If it's a builtin role extract permissions from it",
            "        if role_from_name in self.builtin_roles:",
            "            role_from_permissions = self._get_pvms_from_builtin_role(role_from_name)",
            "        else:",
            "            role_from_permissions = list(self.find_role(role_from_name).permissions)",
            "        role_to = self.add_role(role_to_name)",
            "        # If merge, recover existing data access permissions",
            "        if merge:",
            "            for permission_view in role_to.permissions:",
            "                if (",
            "                    permission_view not in role_from_permissions",
            "                    and permission_view.permission.name in self.data_access_permissions",
            "                ):",
            "                    role_from_permissions.append(permission_view)",
            "        role_to.permissions = role_from_permissions",
            "        self.get_session.commit()",
            "",
            "    def set_role(",
            "        self,",
            "        role_name: str,",
            "        pvm_check: Callable[[PermissionView], bool],",
            "        pvms: list[PermissionView],",
            "    ) -> None:",
            "        \"\"\"",
            "        Set the FAB permission/views for the role.",
            "",
            "        :param role_name: The FAB role name",
            "        :param pvm_check: The FAB permission/view check",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing %s perms\", role_name)",
            "        role = self.add_role(role_name)",
            "        role_pvms = [",
            "            permission_view for permission_view in pvms if pvm_check(permission_view)",
            "        ]",
            "        role.permissions = role_pvms",
            "        self.get_session.commit()",
            "",
            "    def _is_admin_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Admin users,",
            "        False otherwise.",
            "",
            "        Note readonly operations on read only model views are allowed only for admins.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Admin users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ADMIN_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ADMIN_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_alpha_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Alpha users,",
            "        False otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Alpha users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.GAMMA_READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ALPHA_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ALPHA_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_accessible_to_all(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to all, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to all users",
            "        \"\"\"",
            "",
            "        return pvm.permission.name in self.ACCESSIBLE_PERMS",
            "",
            "    def _is_admin_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Admin user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Admin related",
            "        \"\"\"",
            "",
            "        return not self._is_user_defined_permission(pvm)",
            "",
            "    def _is_alpha_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Alpha user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Alpha related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_gamma_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Gamma user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Gamma related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_alpha_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_sql_lab_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is only SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (pvm.permission.name, pvm.view_menu.name) in self.SQLLAB_ONLY_PERMISSIONS",
            "",
            "    def _is_sql_lab_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (",
            "            self._is_sql_lab_only(pvm)",
            "            or (pvm.permission.name, pvm.view_menu.name)",
            "            in self.SQLLAB_EXTRA_PERMISSION_VIEWS",
            "        )",
            "",
            "    def database_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions when a database is created.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", target.get_perm()",
            "        )",
            "",
            "    def database_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a database is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._delete_vm_database_access(",
            "            mapper, connection, target.id, target.database_name",
            "        )",
            "",
            "    def database_after_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a database is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The database PVM",
            "         - All datasets PVMs that reference the db, and it's local perm name",
            "         - All datasets local schema perm that reference the db.",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        # Check if database name has changed",
            "        state = inspect(target)",
            "        history = state.get_history(\"database_name\", True)",
            "        if not history.has_changes() or not history.deleted:",
            "            return",
            "",
            "        old_database_name = history.deleted[0]",
            "        # update database access permission",
            "        self._update_vm_database_access(mapper, connection, old_database_name, target)",
            "        # update datasource access",
            "        self._update_vm_datasources_access(",
            "            mapper, connection, old_database_name, target",
            "        )",
            "        # Note schema permissions are updated at the API level",
            "        # (database.commands.update). Since we need to fetch all existing schemas from",
            "        # the db",
            "",
            "    def _delete_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        database_id: int,",
            "        database_name: str,",
            "    ) -> None:",
            "        view_menu_name = self.get_database_perm(database_id, database_name)",
            "        # Clean database access permission",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", view_menu_name",
            "        )",
            "        # Clean database schema permissions",
            "        schema_pvms = (",
            "            self.get_session.query(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(self.viewmenu_model)",
            "            .filter(self.permission_model.name == \"schema_access\")",
            "            .filter(self.viewmenu_model.name.like(f\"[{database_name}].[%]\"))",
            "            .all()",
            "        )",
            "        for schema_pvm in schema_pvms:",
            "            self._delete_pvm_on_sqla_event(mapper, connection, pvm=schema_pvm)",
            "",
            "    def _update_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> Optional[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all database access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        old_view_menu_name = self.get_database_perm(target.id, old_database_name)",
            "        new_view_menu_name = self.get_database_perm(target.id, new_database_name)",
            "        db_pvm = self.find_permission_view_menu(\"database_access\", old_view_menu_name)",
            "        if not db_pvm:",
            "            logger.warning(",
            "                \"Could not find previous database permission %s\",",
            "                old_view_menu_name,",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"database_access\", new_view_menu_name",
            "            )",
            "            return None",
            "        new_updated_pvm = self.find_permission_view_menu(",
            "            \"database_access\", new_view_menu_name",
            "        )",
            "        if new_updated_pvm:",
            "            logger.info(",
            "                \"New permission [%s] already exists, deleting the previous\",",
            "                new_view_menu_name,",
            "            )",
            "            self._delete_vm_database_access(",
            "                mapper, connection, target.id, old_database_name",
            "            )",
            "            return None",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.id == db_pvm.view_menu_id)",
            "            .values(name=new_view_menu_name)",
            "        )",
            "        if not new_view_menu_name:",
            "            return None",
            "        new_db_view_menu = self._find_view_menu_on_sqla_event(",
            "            connection, new_view_menu_name",
            "        )",
            "",
            "        self.on_view_menu_after_update(mapper, connection, new_db_view_menu)",
            "        return new_db_view_menu",
            "",
            "    def _update_vm_datasources_access(  # pylint: disable=too-many-locals",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> list[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all datasource access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        datasets = (",
            "            self.get_session.query(SqlaTable)",
            "            .filter(SqlaTable.database_id == target.id)",
            "            .all()",
            "        )",
            "        updated_view_menus: list[ViewMenu] = []",
            "        for dataset in datasets:",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, old_database_name",
            "            )",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, new_database_name",
            "            )",
            "            new_dataset_view_menu = self.find_view_menu(new_dataset_vm_name)",
            "            if new_dataset_view_menu:",
            "                continue",
            "            connection.execute(",
            "                view_menu_table.update()",
            "                .where(view_menu_table.c.name == old_dataset_vm_name)",
            "                .values(name=new_dataset_vm_name)",
            "            )",
            "",
            "            # Update dataset (SqlaTable perm field)",
            "            connection.execute(",
            "                sqlatable_table.update()",
            "                .where(",
            "                    sqlatable_table.c.id == dataset.id,",
            "                    sqlatable_table.c.perm == old_dataset_vm_name,",
            "                )",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            # Update charts (Slice perm field)",
            "            connection.execute(",
            "                chart_table.update()",
            "                .where(chart_table.c.perm == old_dataset_vm_name)",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            if new_dataset_vm_name:",
            "                # After update refresh",
            "                new_dataset_view_menu = self._find_view_menu_on_sqla_event(",
            "                    connection,",
            "                    new_dataset_vm_name,",
            "                )",
            "                self.on_view_menu_after_update(",
            "                    mapper,",
            "                    connection,",
            "                    new_dataset_view_menu,",
            "                )",
            "                updated_view_menus.append(new_dataset_view_menu)",
            "        return updated_view_menus",
            "",
            "    def dataset_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permission creation when a dataset is inserted.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The dataset PVM and set local and schema perm",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        from superset.models.core import (  # pylint: disable=import-outside-toplevel",
            "            Database,",
            "        )",
            "",
            "        try:",
            "            dataset_perm: Optional[str] = target.get_perm()",
            "            database = target.database",
            "        except DatasetInvalidPermissionEvaluationException:",
            "            logger.warning(",
            "                \"Dataset has no database will retry with database_id to set permission\"",
            "            )",
            "            database = self.get_session.query(Database).get(target.database_id)",
            "            dataset_perm = self.get_dataset_perm(",
            "                target.id, target.table_name, database.database_name",
            "            )",
            "        dataset_table = target.__table__",
            "",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_perm",
            "        )",
            "        if target.perm != dataset_perm:",
            "            target.perm = dataset_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(perm=dataset_perm)",
            "            )",
            "",
            "        if target.schema:",
            "            dataset_schema_perm = self.get_schema_perm(",
            "                database.database_name, target.schema",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"schema_access\", dataset_schema_perm",
            "            )",
            "            target.schema_perm = dataset_schema_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(schema_perm=dataset_schema_perm)",
            "            )",
            "",
            "    def dataset_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a dataset is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The dataset PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        dataset_vm_name = self.get_dataset_perm(",
            "            target.id, target.table_name, target.database.database_name",
            "        )",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_vm_name",
            "        )",
            "",
            "    def dataset_before_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a dataset is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The dataset PVM and local perm",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        # Check if watched fields have changed",
            "        table = SqlaTable.__table__  # pylint: disable=no-member",
            "        current_dataset = connection.execute(",
            "            table.select().where(table.c.id == target.id)",
            "        ).one()",
            "        current_db_id = current_dataset.database_id",
            "        current_schema = current_dataset.schema",
            "        current_table_name = current_dataset.table_name",
            "",
            "        # When database name changes",
            "        if current_db_id != target.database_id:",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, target.perm, new_dataset_vm_name, target",
            "            )",
            "",
            "            # Updates schema permissions",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "        # When table name changes",
            "        if current_table_name != target.table_name:",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, current_table_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, old_dataset_vm_name, new_dataset_vm_name, target",
            "            )",
            "",
            "        # When schema changes",
            "        if current_schema != target.schema:",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "    def _update_dataset_schema_perm(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        new_schema_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a new schema permission name, propagates the name change to datasets and charts.",
            "",
            "        If the schema permission name does not exist already has a PVM,",
            "        creates a new one.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param new_schema_permission_name: The new schema permission name that changed",
            "        :param target: Dataset that was updated",
            "        :return:",
            "        \"\"\"",
            "        logger.info(\"Updating schema perm, new: %s\", new_schema_permission_name)",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        # insert new schema PVM if it does not exist",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"schema_access\", new_schema_permission_name",
            "        )",
            "",
            "        # Update dataset (SqlaTable schema_perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "        # Update charts (Slice schema_perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_id == target.id,",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "    def _update_dataset_perm(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_permission_name: Optional[str],",
            "        new_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a permission name change, propagates the name change to VM, datasets and charts.",
            "",
            "        :param mapper:",
            "        :param connection:",
            "        :param old_permission_name",
            "        :param new_permission_name:",
            "        :param target:",
            "        :return:",
            "        \"\"\"",
            "        logger.info(",
            "            \"Updating dataset perm, old: %s, new: %s\",",
            "            old_permission_name,",
            "            new_permission_name,",
            "        )",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        if new_dataset_view_menu:",
            "            return",
            "        old_dataset_view_menu = self.find_view_menu(old_permission_name)",
            "        if not old_dataset_view_menu:",
            "            logger.warning(",
            "                \"Could not find previous dataset permission %s\", old_permission_name",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"datasource_access\", new_permission_name",
            "            )",
            "            return",
            "        # Update VM",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.name == old_permission_name)",
            "            .values(name=new_permission_name)",
            "        )",
            "        # VM changed, so call hook",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        self.on_view_menu_after_update(mapper, connection, new_dataset_view_menu)",
            "        # Update dataset (SqlaTable perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "        # Update charts (Slice perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "                chart_table.c.datasource_id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "",
            "    def _delete_pvm_on_sqla_event(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: Optional[str] = None,",
            "        view_menu_name: Optional[str] = None,",
            "        pvm: Optional[PermissionView] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Deletes a PVM.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :param pvm: Can be called with the actual PVM already",
            "        :return:",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_menu_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "",
            "        if not pvm:",
            "            pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if not pvm:",
            "            return",
            "        # Delete Any Role to PVM association",
            "        connection.execute(",
            "            assoc_permissionview_role.delete().where(",
            "                assoc_permissionview_role.c.permission_view_id == pvm.id",
            "            )",
            "        )",
            "        # Delete the database access PVM",
            "        connection.execute(",
            "            permission_view_menu_table.delete().where(",
            "                permission_view_menu_table.c.id == pvm.id",
            "            )",
            "        )",
            "        self.on_permission_view_after_delete(mapper, connection, pvm)",
            "        connection.execute(",
            "            view_menu_table.delete().where(view_menu_table.c.id == pvm.view_menu_id)",
            "        )",
            "",
            "    def _find_permission_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> Permission:",
            "        \"\"\"",
            "        Find a FAB Permission using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The permission name (it's unique)",
            "        :return: Permission",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "",
            "        permission_ = connection.execute(",
            "            permission_table.select().where(permission_table.c.name == name)",
            "        ).fetchone()",
            "        permission = Permission()",
            "        # ensures this object is never persisted",
            "        permission.metadata = None",
            "        permission.id = permission_.id",
            "        permission.name = permission_.name",
            "        return permission",
            "",
            "    def _find_view_menu_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> ViewMenu:",
            "        \"\"\"",
            "        Find a FAB ViewMenu using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The ViewMenu name (it's unique)",
            "        :return: ViewMenu",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "",
            "        view_menu_ = connection.execute(",
            "            view_menu_table.select().where(view_menu_table.c.name == name)",
            "        ).fetchone()",
            "        view_menu = ViewMenu()",
            "        # ensures this object is never persisted",
            "        view_menu.metadata = None",
            "        view_menu.id = view_menu_.id",
            "        view_menu.name = view_menu_.name",
            "        return view_menu",
            "",
            "    def _insert_pvm_on_sqla_event(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: str,",
            "        view_menu_name: Optional[str],",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Inserts a new PVM (if it does not exist already)",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :return:",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "        if not view_menu_name:",
            "            return",
            "        pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if pvm:",
            "            return",
            "        permission = self.find_permission(permission_name)",
            "        view_menu = self.find_view_menu(view_menu_name)",
            "        if not permission:",
            "            _ = connection.execute(",
            "                permission_table.insert().values(name=permission_name)",
            "            )",
            "            permission = self._find_permission_on_sqla_event(",
            "                connection, permission_name",
            "            )",
            "            self.on_permission_after_insert(mapper, connection, permission)",
            "        if not view_menu:",
            "            _ = connection.execute(view_menu_table.insert().values(name=view_menu_name))",
            "            view_menu = self._find_view_menu_on_sqla_event(connection, view_menu_name)",
            "            self.on_view_menu_after_insert(mapper, connection, view_menu)",
            "        connection.execute(",
            "            permission_view_table.insert().values(",
            "                permission_id=permission.id, view_menu_id=view_menu.id",
            "            )",
            "        )",
            "        permission_view = connection.execute(",
            "            permission_view_table.select().where(",
            "                permission_view_table.c.permission_id == permission.id,",
            "                permission_view_table.c.view_menu_id == view_menu.id,",
            "            )",
            "        ).fetchone()",
            "        permission_view_model = PermissionView()",
            "        permission_view_model.metadata = None",
            "        permission_view_model.id = permission_view.id",
            "        permission_view_model.permission_id = permission.id",
            "        permission_view_model.view_menu_id = view_menu.id",
            "        permission_view_model.permission = permission",
            "        permission_view_model.view_menu = view_menu",
            "        self.on_permission_view_after_insert(mapper, connection, permission_view_model)",
            "",
            "    def on_role_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: Role",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a Role update",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being changed",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is created by set_perm.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is updated",
            "",
            "        Since the update may be performed on after_update event. We cannot",
            "        update ViewMenus using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_update.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: Permission",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new permission",
            "        is created by set_perm.",
            "",
            "        Since set_perm is executed by SQLAlchemy after_insert events, we cannot",
            "        create new permissions using a session, so any SQLAlchemy events hooked to",
            "        `Permission` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_delete(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is delete by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_delete events, we cannot",
            "        delete pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_delete.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    @staticmethod",
            "    def get_exclude_users_from_lists() -> list[str]:",
            "        \"\"\"",
            "        Override to dynamically identify a list of usernames to exclude from",
            "        all UI dropdown lists, owners, created_by filters etc...",
            "",
            "        It will exclude all users from the all endpoints of the form",
            "        ``/api/v1/<modelview>/related/<column>``",
            "",
            "        Optionally you can also exclude them using the `EXCLUDE_USERS_FROM_LISTS`",
            "        config setting.",
            "",
            "        :return: A list of usernames",
            "        \"\"\"",
            "        return []",
            "",
            "    def raise_for_access(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        self,",
            "        dashboard: Optional[\"Dashboard\"] = None,",
            "        database: Optional[\"Database\"] = None,",
            "        datasource: Optional[\"BaseDatasource\"] = None,",
            "        query: Optional[\"Query\"] = None,",
            "        query_context: Optional[\"QueryContext\"] = None,",
            "        table: Optional[\"Table\"] = None,",
            "        viz: Optional[\"BaseViz\"] = None,",
            "        sql: Optional[str] = None,",
            "        schema: Optional[str] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :param database: The Superset database",
            "        :param datasource: The Superset datasource",
            "        :param query: The SQL Lab query",
            "        :param query_context: The query context",
            "        :param table: The Superset table (requires database)",
            "        :param viz: The visualization",
            "        :param sql: The SQL string (requires database)",
            "        :param schema: Optional schema name",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models.dashboard import Dashboard",
            "        from superset.models.slice import Slice",
            "        from superset.models.sql_lab import Query",
            "        from superset.sql_parse import Table",
            "        from superset.utils.core import shortid",
            "",
            "        if sql and database:",
            "            query = Query(",
            "                database=database,",
            "                sql=sql,",
            "                schema=schema,",
            "                client_id=shortid()[:10],",
            "                user_id=get_user_id(),",
            "            )",
            "            self.get_session.expunge(query)",
            "",
            "        if database and table or query:",
            "            if query:",
            "                database = query.database",
            "",
            "            database = cast(\"Database\", database)",
            "",
            "            if self.can_access_database(database):",
            "                return",
            "",
            "            if query:",
            "                default_schema = database.get_default_schema_for_query(query)",
            "                tables = {",
            "                    Table(table_.table, table_.schema or default_schema)",
            "                    for table_ in sql_parse.ParsedQuery(query.sql).tables",
            "                }",
            "            elif table:",
            "                tables = {table}",
            "",
            "            denied = set()",
            "",
            "            for table_ in tables:",
            "                schema_perm = self.get_schema_perm(database, schema=table_.schema)",
            "",
            "                if not (schema_perm and self.can_access(\"schema_access\", schema_perm)):",
            "                    datasources = SqlaTable.query_datasources_by_name(",
            "                        self.get_session, database, table_.table, schema=table_.schema",
            "                    )",
            "",
            "                    # Access to any datasource is suffice.",
            "                    for datasource_ in datasources:",
            "                        if self.can_access(",
            "                            \"datasource_access\", datasource_.perm",
            "                        ) or self.is_owner(datasource_):",
            "                            break",
            "                    else:",
            "                        denied.add(table_)",
            "",
            "            if denied:",
            "                raise SupersetSecurityException(",
            "                    self.get_table_access_error_object(denied)",
            "                )",
            "",
            "        if datasource or query_context or viz:",
            "            form_data = None",
            "",
            "            if query_context:",
            "                datasource = query_context.datasource",
            "                form_data = query_context.form_data",
            "            elif viz:",
            "                datasource = viz.datasource",
            "                form_data = viz.form_data",
            "",
            "            assert datasource",
            "",
            "            if not (",
            "                self.can_access_schema(datasource)",
            "                or self.can_access(\"datasource_access\", datasource.perm or \"\")",
            "                or self.is_owner(datasource)",
            "                or (",
            "                    # Grant access to the datasource only if dashboard RBAC is enabled",
            "                    # or the user is an embedded guest user with access to the dashboard",
            "                    # and said datasource is associated with the dashboard chart in",
            "                    # question.",
            "                    form_data",
            "                    and (dashboard_id := form_data.get(\"dashboardId\"))",
            "                    and (",
            "                        dashboard_ := self.get_session.query(Dashboard)",
            "                        .filter(Dashboard.id == dashboard_id)",
            "                        .one_or_none()",
            "                    )",
            "                    and (",
            "                        (is_feature_enabled(\"DASHBOARD_RBAC\") and dashboard_.roles)",
            "                        or (",
            "                            is_feature_enabled(\"EMBEDDED_SUPERSET\")",
            "                            and self.is_guest_user()",
            "                        )",
            "                    )",
            "                    and (",
            "                        (",
            "                            # Native filter.",
            "                            form_data.get(\"type\") == \"NATIVE_FILTER\"",
            "                            and (native_filter_id := form_data.get(\"native_filter_id\"))",
            "                            and dashboard_.json_metadata",
            "                            and (json_metadata := json.loads(dashboard_.json_metadata))",
            "                            and any(",
            "                                target.get(\"datasetId\") == datasource.id",
            "                                for fltr in json_metadata.get(",
            "                                    \"native_filter_configuration\",",
            "                                    [],",
            "                                )",
            "                                for target in fltr.get(\"targets\", [])",
            "                                if native_filter_id == fltr.get(\"id\")",
            "                            )",
            "                        )",
            "                        or (",
            "                            # Chart.",
            "                            form_data.get(\"type\") != \"NATIVE_FILTER\"",
            "                            and (slice_id := form_data.get(\"slice_id\"))",
            "                            and (",
            "                                slc := self.get_session.query(Slice)",
            "                                .filter(Slice.id == slice_id)",
            "                                .one_or_none()",
            "                            )",
            "                            and slc in dashboard_.slices",
            "                            and slc.datasource == datasource",
            "                        )",
            "                    )",
            "                    and self.can_access_dashboard(dashboard_)",
            "                )",
            "            ):",
            "                raise SupersetSecurityException(",
            "                    self.get_datasource_access_error_object(datasource)",
            "                )",
            "",
            "        if dashboard:",
            "            if self.is_guest_user():",
            "                # Guest user is currently used for embedded dashboards only. If the guest",
            "                # user doesn't have access to the dashboard, ignore all other checks.",
            "                if self.has_guest_access(dashboard):",
            "                    return",
            "                raise SupersetSecurityException(",
            "                    self.get_dashboard_access_error_object(dashboard)",
            "                )",
            "",
            "            if self.is_admin() or self.is_owner(dashboard):",
            "                return",
            "",
            "            # RBAC and legacy (datasource inferred) access controls.",
            "            if is_feature_enabled(\"DASHBOARD_RBAC\") and dashboard.roles:",
            "                if dashboard.published and {role.id for role in dashboard.roles} & {",
            "                    role.id for role in self.get_user_roles()",
            "                }:",
            "                    return",
            "            elif (",
            "                # To understand why we rely on status and give access to draft dashboards",
            "                # without roles take a look at:",
            "                #",
            "                #   - https://github.com/apache/superset/pull/24350#discussion_r1225061550",
            "                #   - https://github.com/apache/superset/pull/17511#issuecomment-975870169",
            "                #",
            "                not dashboard.published",
            "                or not dashboard.datasources",
            "                or any(",
            "                    self.can_access_datasource(datasource)",
            "                    for datasource in dashboard.datasources",
            "                )",
            "            ):",
            "                return",
            "",
            "            raise SupersetSecurityException(",
            "                self.get_dashboard_access_error_object(dashboard)",
            "            )",
            "",
            "    def get_user_by_username(",
            "        self, username: str, session: Session = None",
            "    ) -> Optional[User]:",
            "        \"\"\"",
            "        Retrieves a user by it's username case sensitive. Optional session parameter",
            "        utility method normally useful for celery tasks where the session",
            "        need to be scoped",
            "        \"\"\"",
            "        session = session or self.get_session",
            "        return (",
            "            session.query(self.user_model)",
            "            .filter(self.user_model.username == username)",
            "            .one_or_none()",
            "        )",
            "",
            "    def get_anonymous_user(self) -> User:",
            "        return AnonymousUserMixin()",
            "",
            "    def get_user_roles(self, user: Optional[User] = None) -> list[Role]:",
            "        if not user:",
            "            user = g.user",
            "        if user.is_anonymous:",
            "            public_role = current_app.config.get(\"AUTH_ROLE_PUBLIC\")",
            "            return [self.get_public_role()] if public_role else []",
            "        return user.roles",
            "",
            "    def get_guest_rls_filters(",
            "        self, dataset: \"BaseDatasource\"",
            "    ) -> list[GuestTokenRlsRule]:",
            "        \"\"\"",
            "        Retrieves the row level security filters for the current user and the dataset,",
            "        if the user is authenticated with a guest token.",
            "        :param dataset: The dataset to check against",
            "        :return: A list of filters",
            "        \"\"\"",
            "        if guest_user := self.get_current_guest_user_if_guest():",
            "            return [",
            "                rule",
            "                for rule in guest_user.rls",
            "                if not rule.get(\"dataset\")",
            "                or str(rule.get(\"dataset\")) == str(dataset.id)",
            "            ]",
            "        return []",
            "",
            "    def get_rls_filters(self, table: \"BaseDatasource\") -> list[SqlaQuery]:",
            "        \"\"\"",
            "        Retrieves the appropriate row level security filters for the current user and",
            "        the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list of filters",
            "        \"\"\"",
            "",
            "        if not (hasattr(g, \"user\") and g.user is not None):",
            "            return []",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import (",
            "            RLSFilterRoles,",
            "            RLSFilterTables,",
            "            RowLevelSecurityFilter,",
            "        )",
            "",
            "        user_roles = [role.id for role in self.get_user_roles(g.user)]",
            "        regular_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.REGULAR",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        base_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.BASE",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        filter_tables = (",
            "            self.get_session()",
            "            .query(RLSFilterTables.c.rls_filter_id)",
            "            .filter(RLSFilterTables.c.table_id == table.id)",
            "        )",
            "        query = (",
            "            self.get_session()",
            "            .query(",
            "                RowLevelSecurityFilter.id,",
            "                RowLevelSecurityFilter.group_key,",
            "                RowLevelSecurityFilter.clause,",
            "            )",
            "            .filter(RowLevelSecurityFilter.id.in_(filter_tables))",
            "            .filter(",
            "                or_(",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.REGULAR,",
            "                        RowLevelSecurityFilter.id.in_(regular_filter_roles),",
            "                    ),",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.BASE,",
            "                        RowLevelSecurityFilter.id.notin_(base_filter_roles),",
            "                    ),",
            "                )",
            "            )",
            "        )",
            "        return query.all()",
            "",
            "    def get_rls_sorted(self, table: \"BaseDatasource\") -> list[\"RowLevelSecurityFilter\"]:",
            "        \"\"\"",
            "        Retrieves a list RLS filters sorted by ID for",
            "        the current user and the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list RLS filters",
            "        \"\"\"",
            "        filters = self.get_rls_filters(table)",
            "        filters.sort(key=lambda f: f.id)",
            "        return filters",
            "",
            "    def get_guest_rls_filters_str(self, table: \"BaseDatasource\") -> list[str]:",
            "        return [f.get(\"clause\", \"\") for f in self.get_guest_rls_filters(table)]",
            "",
            "    def get_rls_cache_key(self, datasource: \"BaseDatasource\") -> list[str]:",
            "        rls_clauses_with_group_key = []",
            "        if datasource.is_rls_supported:",
            "            rls_clauses_with_group_key = [",
            "                f\"{f.clause}-{f.group_key or ''}\"",
            "                for f in self.get_rls_sorted(datasource)",
            "            ]",
            "        guest_rls = self.get_guest_rls_filters_str(datasource)",
            "        return guest_rls + rls_clauses_with_group_key",
            "",
            "    @staticmethod",
            "    def _get_current_epoch_time() -> float:",
            "        \"\"\"This is used so the tests can mock time\"\"\"",
            "        return time.time()",
            "",
            "    @staticmethod",
            "    def _get_guest_token_jwt_audience() -> str:",
            "        audience = current_app.config[\"GUEST_TOKEN_JWT_AUDIENCE\"] or get_url_host()",
            "        if callable(audience):",
            "            audience = audience()",
            "        return audience",
            "",
            "    @staticmethod",
            "    def validate_guest_token_resources(resources: GuestTokenResources) -> None:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.commands.dashboard.embedded.exceptions import (",
            "            EmbeddedDashboardNotFoundError,",
            "        )",
            "        from superset.daos.dashboard import EmbeddedDashboardDAO",
            "        from superset.models.dashboard import Dashboard",
            "",
            "        for resource in resources:",
            "            if resource[\"type\"] == GuestTokenResourceType.DASHBOARD.value:",
            "                # TODO (embedded): remove this check once uuids are rolled out",
            "                dashboard = Dashboard.get(str(resource[\"id\"]))",
            "                if not dashboard:",
            "                    embedded = EmbeddedDashboardDAO.find_by_id(str(resource[\"id\"]))",
            "                    if not embedded:",
            "                        raise EmbeddedDashboardNotFoundError()",
            "",
            "    def create_guest_access_token(",
            "        self,",
            "        user: GuestTokenUser,",
            "        resources: GuestTokenResources,",
            "        rls: list[GuestTokenRlsRule],",
            "    ) -> bytes:",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        exp_seconds = current_app.config[\"GUEST_TOKEN_JWT_EXP_SECONDS\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        # calculate expiration time",
            "        now = self._get_current_epoch_time()",
            "        exp = now + exp_seconds",
            "        claims = {",
            "            \"user\": user,",
            "            \"resources\": resources,",
            "            \"rls_rules\": rls,",
            "            # standard jwt claims:",
            "            \"iat\": now,  # issued at",
            "            \"exp\": exp,  # expiration time",
            "            \"aud\": audience,",
            "            \"type\": \"guest\",",
            "        }",
            "        return self.pyjwt_for_guest_token.encode(claims, secret, algorithm=algo)",
            "",
            "    def get_guest_user_from_request(self, req: Request) -> Optional[GuestUser]:",
            "        \"\"\"",
            "        If there is a guest token in the request (used for embedded),",
            "        parses the token and returns the guest user.",
            "        This is meant to be used as a request loader for the LoginManager.",
            "        The LoginManager will only call this if an active session cannot be found.",
            "",
            "        :return: A guest user object",
            "        \"\"\"",
            "        raw_token = req.headers.get(",
            "            current_app.config[\"GUEST_TOKEN_HEADER_NAME\"]",
            "        ) or req.form.get(\"guest_token\")",
            "        if raw_token is None:",
            "            return None",
            "",
            "        try:",
            "            token = self.parse_jwt_guest_token(raw_token)",
            "            if token.get(\"user\") is None:",
            "                raise ValueError(\"Guest token does not contain a user claim\")",
            "            if token.get(\"resources\") is None:",
            "                raise ValueError(\"Guest token does not contain a resources claim\")",
            "            if token.get(\"rls_rules\") is None:",
            "                raise ValueError(\"Guest token does not contain an rls_rules claim\")",
            "            if token.get(\"type\") != \"guest\":",
            "                raise ValueError(\"This is not a guest token.\")",
            "        except Exception:  # pylint: disable=broad-except",
            "            # The login manager will handle sending 401s.",
            "            # We don't need to send a special error message.",
            "            logger.warning(\"Invalid guest token\", exc_info=True)",
            "            return None",
            "",
            "        return self.get_guest_user_from_token(cast(GuestToken, token))",
            "",
            "    def get_guest_user_from_token(self, token: GuestToken) -> GuestUser:",
            "        return self.guest_user_cls(",
            "            token=token,",
            "            roles=[self.find_role(current_app.config[\"GUEST_ROLE_NAME\"])],",
            "        )",
            "",
            "    def parse_jwt_guest_token(self, raw_token: str) -> dict[str, Any]:",
            "        \"\"\"",
            "        Parses a guest token. Raises an error if the jwt fails standard claims checks.",
            "        :param raw_token: the token gotten from the request",
            "        :return: the same token that was passed in, tested but unchanged",
            "        \"\"\"",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        return self.pyjwt_for_guest_token.decode(",
            "            raw_token, secret, algorithms=[algo], audience=audience",
            "        )",
            "",
            "    @staticmethod",
            "    def is_guest_user(user: Optional[Any] = None) -> bool:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "",
            "        if not is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return False",
            "        if not user:",
            "            user = g.user",
            "        return hasattr(user, \"is_guest_user\") and user.is_guest_user",
            "",
            "    def get_current_guest_user_if_guest(self) -> Optional[GuestUser]:",
            "        return g.user if self.is_guest_user() else None",
            "",
            "    def has_guest_access(self, dashboard: \"Dashboard\") -> bool:",
            "        user = self.get_current_guest_user_if_guest()",
            "        if not user:",
            "            return False",
            "",
            "        dashboards = [",
            "            r",
            "            for r in user.resources",
            "            if r[\"type\"] == GuestTokenResourceType.DASHBOARD.value",
            "        ]",
            "",
            "        # TODO (embedded): remove this check once uuids are rolled out",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.id):",
            "                return True",
            "",
            "        if not dashboard.embedded:",
            "            return False",
            "",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.embedded[0].uuid):",
            "                return True",
            "        return False",
            "",
            "    def raise_for_ownership(self, resource: Model) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user does not own the resource.",
            "",
            "        Note admins are deemed owners of all resources.",
            "",
            "        :param resource: The dashboard, dataset, chart, etc. resource",
            "        :raises SupersetSecurityException: If the current user is not an owner",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import db",
            "",
            "        if self.is_admin():",
            "            return",
            "",
            "        orig_resource = db.session.query(resource.__class__).get(resource.id)",
            "        owners = orig_resource.owners if hasattr(orig_resource, \"owners\") else []",
            "",
            "        if g.user.is_anonymous or g.user not in owners:",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.MISSING_OWNERSHIP_ERROR,",
            "                    message=_(",
            "                        \"You don't have the rights to alter %(resource)s\",",
            "                        resource=resource,",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            )",
            "",
            "    def is_owner(self, resource: Model) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an owner of the resource, False otherwise.",
            "",
            "        :param resource: The dashboard, dataset, chart, etc. resource",
            "        :returns: Whether the current user is an owner of the resource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_ownership(resource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def is_admin(self) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an admin user, False otherwise.",
            "",
            "        :returns: Whether the current user is an admin user",
            "        \"\"\"",
            "",
            "        return current_app.config[\"AUTH_ROLE_ADMIN\"] in [",
            "            role.name for role in self.get_user_roles()",
            "        ]"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "# pylint: disable=too-many-lines",
            "\"\"\"A set of constants and methods to manage permissions and security\"\"\"",
            "import json",
            "import logging",
            "import re",
            "import time",
            "from collections import defaultdict",
            "from typing import Any, Callable, cast, NamedTuple, Optional, TYPE_CHECKING, Union",
            "",
            "from flask import current_app, Flask, g, Request",
            "from flask_appbuilder import Model",
            "from flask_appbuilder.security.sqla.manager import SecurityManager",
            "from flask_appbuilder.security.sqla.models import (",
            "    assoc_permissionview_role,",
            "    assoc_user_role,",
            "    Permission,",
            "    PermissionView,",
            "    Role,",
            "    User,",
            "    ViewMenu,",
            ")",
            "from flask_appbuilder.security.views import (",
            "    PermissionModelView,",
            "    PermissionViewModelView,",
            "    RoleModelView,",
            "    UserModelView,",
            "    ViewMenuModelView,",
            ")",
            "from flask_appbuilder.widgets import ListWidget",
            "from flask_babel import lazy_gettext as _",
            "from flask_login import AnonymousUserMixin, LoginManager",
            "from jwt.api_jwt import _jwt_global_obj",
            "from sqlalchemy import and_, inspect, or_",
            "from sqlalchemy.engine.base import Connection",
            "from sqlalchemy.orm import eagerload, Session",
            "from sqlalchemy.orm.mapper import Mapper",
            "from sqlalchemy.orm.query import Query as SqlaQuery",
            "",
            "from superset import sql_parse",
            "from superset.constants import RouteMethod",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import (",
            "    DatasetInvalidPermissionEvaluationException,",
            "    SupersetSecurityException,",
            ")",
            "from superset.security.guest_token import (",
            "    GuestToken,",
            "    GuestTokenResources,",
            "    GuestTokenResourceType,",
            "    GuestTokenRlsRule,",
            "    GuestTokenUser,",
            "    GuestUser,",
            ")",
            "from superset.utils.core import (",
            "    DatasourceName,",
            "    DatasourceType,",
            "    get_user_id,",
            "    RowLevelSecurityFilterType,",
            ")",
            "from superset.utils.filters import get_dataset_access_filters",
            "from superset.utils.urls import get_url_host",
            "",
            "if TYPE_CHECKING:",
            "    from superset.common.query_context import QueryContext",
            "    from superset.connectors.sqla.models import (",
            "        BaseDatasource,",
            "        RowLevelSecurityFilter,",
            "        SqlaTable,",
            "    )",
            "    from superset.models.core import Database",
            "    from superset.models.dashboard import Dashboard",
            "    from superset.models.sql_lab import Query",
            "    from superset.sql_parse import Table",
            "    from superset.viz import BaseViz",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "DATABASE_PERM_REGEX = re.compile(r\"^\\[.+\\]\\.\\(id\\:(?P<id>\\d+)\\)$\")",
            "",
            "",
            "class DatabaseAndSchema(NamedTuple):",
            "    database: str",
            "    schema: str",
            "",
            "",
            "class SupersetSecurityListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Redeclaring to avoid circular imports",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list.html\"",
            "",
            "",
            "class SupersetRoleListWidget(ListWidget):  # pylint: disable=too-few-public-methods",
            "    \"\"\"",
            "    Role model view from FAB already uses a custom list widget override",
            "    So we override the override",
            "    \"\"\"",
            "",
            "    template = \"superset/fab_overrides/list_role.html\"",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        kwargs[\"appbuilder\"] = current_app.appbuilder",
            "        super().__init__(**kwargs)",
            "",
            "",
            "UserModelView.list_widget = SupersetSecurityListWidget",
            "RoleModelView.list_widget = SupersetRoleListWidget",
            "PermissionViewModelView.list_widget = SupersetSecurityListWidget",
            "PermissionModelView.list_widget = SupersetSecurityListWidget",
            "",
            "# Limiting routes on FAB model views",
            "UserModelView.include_route_methods = RouteMethod.CRUD_SET | {",
            "    RouteMethod.ACTION,",
            "    RouteMethod.API_READ,",
            "    RouteMethod.ACTION_POST,",
            "    \"userinfo\",",
            "}",
            "RoleModelView.include_route_methods = RouteMethod.CRUD_SET",
            "PermissionViewModelView.include_route_methods = {RouteMethod.LIST}",
            "PermissionModelView.include_route_methods = {RouteMethod.LIST}",
            "ViewMenuModelView.include_route_methods = {RouteMethod.LIST}",
            "",
            "RoleModelView.list_columns = [\"name\"]",
            "RoleModelView.edit_columns = [\"name\", \"permissions\", \"user\"]",
            "RoleModelView.related_views = []",
            "",
            "",
            "class SupersetSecurityManager(  # pylint: disable=too-many-public-methods",
            "    SecurityManager",
            "):",
            "    userstatschartview = None",
            "    READ_ONLY_MODEL_VIEWS = {\"Database\", \"DynamicPlugin\"}",
            "",
            "    USER_MODEL_VIEWS = {",
            "        \"RegisterUserModelView\",",
            "        \"UserDBModelView\",",
            "        \"UserLDAPModelView\",",
            "        \"UserInfoEditView\",",
            "        \"UserOAuthModelView\",",
            "        \"UserOIDModelView\",",
            "        \"UserRemoteUserModelView\",",
            "    }",
            "",
            "    GAMMA_READ_ONLY_MODEL_VIEWS = {",
            "        \"Dataset\",",
            "        \"Datasource\",",
            "    } | READ_ONLY_MODEL_VIEWS",
            "",
            "    ADMIN_ONLY_VIEW_MENUS = {",
            "        \"Access Requests\",",
            "        \"Action Log\",",
            "        \"Log\",",
            "        \"List Users\",",
            "        \"List Roles\",",
            "        \"ResetPasswordView\",",
            "        \"RoleModelView\",",
            "        \"Row Level Security\",",
            "        \"Row Level Security Filters\",",
            "        \"RowLevelSecurityFiltersModelView\",",
            "        \"Security\",",
            "        \"SQL Lab\",",
            "        \"User Registrations\",",
            "        \"User's Statistics\",",
            "    } | USER_MODEL_VIEWS",
            "",
            "    ALPHA_ONLY_VIEW_MENUS = {",
            "        \"Alerts & Report\",",
            "        \"Annotation Layers\",",
            "        \"Annotation\",",
            "        \"CSS Templates\",",
            "        \"ColumnarToDatabaseView\",",
            "        \"CssTemplate\",",
            "        \"CsvToDatabaseView\",",
            "        \"ExcelToDatabaseView\",",
            "        \"Import dashboards\",",
            "        \"ImportExportRestApi\",",
            "        \"Manage\",",
            "        \"Queries\",",
            "        \"ReportSchedule\",",
            "        \"TableSchemaView\",",
            "        \"Upload a CSV\",",
            "    }",
            "",
            "    ADMIN_ONLY_PERMISSIONS = {",
            "        \"can_update_role\",",
            "        \"all_query_access\",",
            "        \"can_grant_guest_token\",",
            "        \"can_set_embedded\",",
            "        \"can_warm_up_cache\",",
            "    }",
            "",
            "    READ_ONLY_PERMISSION = {",
            "        \"can_show\",",
            "        \"can_list\",",
            "        \"can_get\",",
            "        \"can_external_metadata\",",
            "        \"can_external_metadata_by_name\",",
            "        \"can_read\",",
            "    }",
            "",
            "    ALPHA_ONLY_PERMISSIONS = {",
            "        \"muldelete\",",
            "        \"all_database_access\",",
            "        \"all_datasource_access\",",
            "    }",
            "",
            "    OBJECT_SPEC_PERMISSIONS = {",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "    }",
            "",
            "    ACCESSIBLE_PERMS = {\"can_userinfo\", \"resetmypassword\", \"can_recent_activity\"}",
            "",
            "    SQLLAB_ONLY_PERMISSIONS = {",
            "        (\"can_my_queries\", \"SqlLab\"),",
            "        (\"can_read\", \"SavedQuery\"),",
            "        (\"can_write\", \"SavedQuery\"),",
            "        (\"can_export\", \"SavedQuery\"),",
            "        (\"can_read\", \"Query\"),",
            "        (\"can_export_csv\", \"Query\"),",
            "        (\"can_get_results\", \"SQLLab\"),",
            "        (\"can_execute_sql_query\", \"SQLLab\"),",
            "        (\"can_estimate_query_cost\", \"SQL Lab\"),",
            "        (\"can_export_csv\", \"SQLLab\"),",
            "        (\"can_read\", \"SQLLab\"),",
            "        (\"can_sqllab_history\", \"Superset\"),",
            "        (\"can_sqllab\", \"Superset\"),",
            "        (\"can_test_conn\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_activate\", \"TabStateView\"),",
            "        (\"can_get\", \"TabStateView\"),",
            "        (\"can_delete_query\", \"TabStateView\"),",
            "        (\"can_post\", \"TabStateView\"),",
            "        (\"can_delete\", \"TabStateView\"),",
            "        (\"can_put\", \"TabStateView\"),",
            "        (\"can_migrate_query\", \"TabStateView\"),",
            "        (\"menu_access\", \"SQL Lab\"),",
            "        (\"menu_access\", \"SQL Editor\"),",
            "        (\"menu_access\", \"Saved Queries\"),",
            "        (\"menu_access\", \"Query Search\"),",
            "    }",
            "",
            "    SQLLAB_EXTRA_PERMISSION_VIEWS = {",
            "        (\"can_csv\", \"Superset\"),  # Deprecated permission remove on 3.0.0",
            "        (\"can_read\", \"Superset\"),",
            "        (\"can_read\", \"Database\"),",
            "    }",
            "",
            "    data_access_permissions = (",
            "        \"database_access\",",
            "        \"schema_access\",",
            "        \"datasource_access\",",
            "        \"all_datasource_access\",",
            "        \"all_database_access\",",
            "        \"all_query_access\",",
            "    )",
            "",
            "    guest_user_cls = GuestUser",
            "    pyjwt_for_guest_token = _jwt_global_obj",
            "",
            "    def create_login_manager(self, app: Flask) -> LoginManager:",
            "        lm = super().create_login_manager(app)",
            "        lm.request_loader(self.request_loader)",
            "        return lm",
            "",
            "    def request_loader(self, request: Request) -> Optional[User]:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.extensions import feature_flag_manager",
            "",
            "        if feature_flag_manager.is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return self.get_guest_user_from_request(request)",
            "        return None",
            "",
            "    def get_schema_perm(",
            "        self, database: Union[\"Database\", str], schema: Optional[str] = None",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the database specific schema permission.",
            "",
            "        :param database: The Superset database or database name",
            "        :param schema: The Superset schema name",
            "        :return: The database specific schema permission",
            "        \"\"\"",
            "        return f\"[{database}].[{schema}]\" if schema else None",
            "",
            "    @staticmethod",
            "    def get_database_perm(database_id: int, database_name: str) -> Optional[str]:",
            "        return f\"[{database_name}].(id:{database_id})\"",
            "",
            "    @staticmethod",
            "    def get_dataset_perm(",
            "        dataset_id: int,",
            "        dataset_name: str,",
            "        database_name: str,",
            "    ) -> Optional[str]:",
            "        return f\"[{database_name}].[{dataset_name}](id:{dataset_id})\"",
            "",
            "    def unpack_database_and_schema(self, schema_permission: str) -> DatabaseAndSchema:",
            "        # [database_name].[schema|table]",
            "",
            "        schema_name = schema_permission.split(\".\")[1][1:-1]",
            "        database_name = schema_permission.split(\".\")[0][1:-1]",
            "        return DatabaseAndSchema(database_name, schema_name)",
            "",
            "    def can_access(self, permission_name: str, view_name: str) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the FAB permission/view, False otherwise.",
            "",
            "        Note this method adds protection from has_access failing from missing",
            "        permission/view entries.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_name: The FAB view-menu name",
            "        :returns: Whether the user can access the FAB permission/view",
            "        \"\"\"",
            "",
            "        user = g.user",
            "        if user.is_anonymous:",
            "            return self.is_item_public(permission_name, view_name)",
            "        return self._has_view_access(user, permission_name, view_name)",
            "",
            "    def can_access_all_queries(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all SQL Lab queries, False otherwise.",
            "",
            "        :returns: Whether the user can access all queries",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_query_access\", \"all_query_access\")",
            "",
            "    def can_access_all_datasources(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the datasources, False otherwise.",
            "",
            "        :returns: Whether the user can access all the datasources",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_datasource_access\", \"all_datasource_access\")",
            "",
            "    def can_access_all_databases(self) -> bool:",
            "        \"\"\"",
            "        Return True if the user can access all the databases, False otherwise.",
            "",
            "        :returns: Whether the user can access all the databases",
            "        \"\"\"",
            "",
            "        return self.can_access(\"all_database_access\", \"all_database_access\")",
            "",
            "    def can_access_database(self, database: \"Database\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified database, False otherwise.",
            "",
            "        :param database: The database",
            "        :returns: Whether the user can access the database",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_all_databases()",
            "            or self.can_access(\"database_access\", database.perm)  # type: ignore",
            "        )",
            "",
            "    def can_access_schema(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the schema associated with specified",
            "        datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource's schema",
            "        \"\"\"",
            "",
            "        return (",
            "            self.can_access_all_datasources()",
            "            or self.can_access_database(datasource.database)",
            "            or self.can_access(\"schema_access\", datasource.schema_perm or \"\")",
            "        )",
            "",
            "    def can_access_datasource(self, datasource: \"BaseDatasource\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified datasource, False otherwise.",
            "",
            "        :param datasource: The datasource",
            "        :returns: Whether the user can access the datasource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(datasource=datasource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def can_access_dashboard(self, dashboard: \"Dashboard\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the specified dashboard, False otherwise.",
            "",
            "        :param dashboard: The dashboard",
            "        :returns: Whether the user can access the dashboard",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(dashboard=dashboard)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def get_dashboard_access_error_object(  # pylint: disable=invalid-name",
            "        self,",
            "        dashboard: \"Dashboard\",  # pylint: disable=unused-argument",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset dashboard.",
            "",
            "        :param dashboard: The denied Superset dashboard",
            "        :returns: The error object",
            "        \"\"\"",
            "",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DASHBOARD_SECURITY_ACCESS_ERROR,",
            "            message=\"You don't have access to this dashboard.\",",
            "            level=ErrorLevel.ERROR,",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_error_msg(datasource: \"BaseDatasource\") -> str:",
            "        \"\"\"",
            "        Return the error message for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        return (",
            "            f\"This endpoint requires the datasource {datasource.name}, \"",
            "            \"database or `all_datasource_access` permission\"",
            "        )",
            "",
            "    @staticmethod",
            "    def get_datasource_access_link(  # pylint: disable=unused-argument",
            "        datasource: \"BaseDatasource\",",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the link for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_datasource_access_error_object(  # pylint: disable=invalid-name",
            "        self, datasource: \"BaseDatasource\"",
            "    ) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied Superset datasource.",
            "",
            "        :param datasource: The denied Superset datasource",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.DATASOURCE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_datasource_access_error_msg(datasource),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_datasource_access_link(datasource),",
            "                \"datasource\": datasource.name,",
            "            },",
            "        )",
            "",
            "    def get_table_access_error_msg(self, tables: set[\"Table\"]) -> str:",
            "        \"\"\"",
            "        Return the error message for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error message",
            "        \"\"\"",
            "",
            "        quoted_tables = [f\"`{table}`\" for table in tables]",
            "        return f\"\"\"You need access to the following tables: {\", \".join(quoted_tables)},",
            "            `all_database_access` or `all_datasource_access` permission\"\"\"",
            "",
            "    def get_table_access_error_object(self, tables: set[\"Table\"]) -> SupersetError:",
            "        \"\"\"",
            "        Return the error object for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The error object",
            "        \"\"\"",
            "        return SupersetError(",
            "            error_type=SupersetErrorType.TABLE_SECURITY_ACCESS_ERROR,",
            "            message=self.get_table_access_error_msg(tables),",
            "            level=ErrorLevel.ERROR,",
            "            extra={",
            "                \"link\": self.get_table_access_link(tables),",
            "                \"tables\": [str(table) for table in tables],",
            "            },",
            "        )",
            "",
            "    def get_table_access_link(  # pylint: disable=unused-argument",
            "        self, tables: set[\"Table\"]",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Return the access link for the denied SQL tables.",
            "",
            "        :param tables: The set of denied SQL tables",
            "        :returns: The access URL",
            "        \"\"\"",
            "",
            "        return current_app.config.get(\"PERMISSION_INSTRUCTIONS_LINK\")",
            "",
            "    def get_user_datasources(self) -> list[\"BaseDatasource\"]:",
            "        \"\"\"",
            "        Collect datasources which the user has explicit permissions to.",
            "",
            "        :returns: The list of datasources",
            "        \"\"\"",
            "",
            "        user_datasources = set()",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        user_datasources.update(",
            "            self.get_session.query(SqlaTable)",
            "            .filter(get_dataset_access_filters(SqlaTable))",
            "            .all()",
            "        )",
            "",
            "        # group all datasources by database",
            "        session = self.get_session",
            "        all_datasources = SqlaTable.get_all_datasources(session)",
            "        datasources_by_database: dict[\"Database\", set[\"SqlaTable\"]] = defaultdict(set)",
            "        for datasource in all_datasources:",
            "            datasources_by_database[datasource.database].add(datasource)",
            "",
            "        # add datasources with implicit permission (eg, database access)",
            "        for database, datasources in datasources_by_database.items():",
            "            if self.can_access_database(database):",
            "                user_datasources.update(datasources)",
            "",
            "        return list(user_datasources)",
            "",
            "    def can_access_table(self, database: \"Database\", table: \"Table\") -> bool:",
            "        \"\"\"",
            "        Return True if the user can access the SQL table, False otherwise.",
            "",
            "        :param database: The SQL database",
            "        :param table: The SQL table",
            "        :returns: Whether the user can access the SQL table",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_access(database=database, table=table)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def user_view_menu_names(self, permission_name: str) -> set[str]:",
            "        base_query = (",
            "            self.get_session.query(self.viewmenu_model.name)",
            "            .join(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(assoc_permissionview_role)",
            "            .join(self.role_model)",
            "        )",
            "",
            "        if not g.user.is_anonymous:",
            "            # filter by user id",
            "            view_menu_names = (",
            "                base_query.join(assoc_user_role)",
            "                .join(self.user_model)",
            "                .filter(self.user_model.id == get_user_id())",
            "                .filter(self.permission_model.name == permission_name)",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "",
            "        # Properly treat anonymous user",
            "        if public_role := self.get_public_role():",
            "            # filter by public role",
            "            view_menu_names = (",
            "                base_query.filter(self.role_model.id == public_role.id).filter(",
            "                    self.permission_model.name == permission_name",
            "                )",
            "            ).all()",
            "            return {s.name for s in view_menu_names}",
            "        return set()",
            "",
            "    def get_accessible_databases(self) -> list[int]:",
            "        \"\"\"",
            "        Return the list of databases accessible by the user.",
            "",
            "        :return: The list of accessible Databases",
            "        \"\"\"",
            "        perms = self.user_view_menu_names(\"database_access\")",
            "        return [",
            "            int(match.group(\"id\"))",
            "            for perm in perms",
            "            if (match := DATABASE_PERM_REGEX.match(perm))",
            "        ]",
            "",
            "    def get_schemas_accessible_by_user(",
            "        self, database: \"Database\", schemas: list[str], hierarchical: bool = True",
            "    ) -> list[str]:",
            "        \"\"\"",
            "        Return the list of SQL schemas accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param schemas: The list of eligible SQL schemas",
            "        :param hierarchical: Whether to check using the hierarchical permission logic",
            "        :returns: The list of accessible SQL schemas",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if hierarchical and self.can_access_database(database):",
            "            return schemas",
            "",
            "        # schema_access",
            "        accessible_schemas = {",
            "            self.unpack_database_and_schema(s).schema",
            "            for s in self.user_view_menu_names(\"schema_access\")",
            "            if s.startswith(f\"[{database}].\")",
            "        }",
            "",
            "        # datasource_access",
            "        if perms := self.user_view_menu_names(\"datasource_access\"):",
            "            tables = (",
            "                self.get_session.query(SqlaTable.schema)",
            "                .filter(SqlaTable.database_id == database.id)",
            "                .filter(SqlaTable.schema.isnot(None))",
            "                .filter(SqlaTable.schema != \"\")",
            "                .filter(or_(SqlaTable.perm.in_(perms)))",
            "                .distinct()",
            "            )",
            "            accessible_schemas.update([table.schema for table in tables])",
            "",
            "        return [s for s in schemas if s in accessible_schemas]",
            "",
            "    def get_datasources_accessible_by_user(  # pylint: disable=invalid-name",
            "        self,",
            "        database: \"Database\",",
            "        datasource_names: list[DatasourceName],",
            "        schema: Optional[str] = None,",
            "    ) -> list[DatasourceName]:",
            "        \"\"\"",
            "        Return the list of SQL tables accessible by the user.",
            "",
            "        :param database: The SQL database",
            "        :param datasource_names: The list of eligible SQL tables w/ schema",
            "        :param schema: The fallback SQL schema if not present in the table name",
            "        :returns: The list of accessible SQL tables w/ schema",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        if self.can_access_database(database):",
            "            return datasource_names",
            "",
            "        if schema:",
            "            schema_perm = self.get_schema_perm(database, schema)",
            "            if schema_perm and self.can_access(\"schema_access\", schema_perm):",
            "                return datasource_names",
            "",
            "        user_perms = self.user_view_menu_names(\"datasource_access\")",
            "        schema_perms = self.user_view_menu_names(\"schema_access\")",
            "        user_datasources = SqlaTable.query_datasources_by_permissions(",
            "            self.get_session, database, user_perms, schema_perms",
            "        )",
            "        if schema:",
            "            names = {d.table_name for d in user_datasources if d.schema == schema}",
            "            return [d for d in datasource_names if d.table in names]",
            "",
            "        full_names = {d.full_name for d in user_datasources}",
            "        return [d for d in datasource_names if f\"[{database}].[{d}]\" in full_names]",
            "",
            "    def merge_perm(self, permission_name: str, view_menu_name: str) -> None:",
            "        \"\"\"",
            "        Add the FAB permission/view-menu.",
            "",
            "        :param permission_name: The FAB permission name",
            "        :param view_menu_name: The FAB view-menu name",
            "        :see: SecurityManager.add_permission_view_menu",
            "        \"\"\"",
            "",
            "        logger.warning(",
            "            \"This method 'merge_perm' is deprecated use add_permission_view_menu\"",
            "        )",
            "        self.add_permission_view_menu(permission_name, view_menu_name)",
            "",
            "    def _is_user_defined_permission(self, perm: Model) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission is user defined, False otherwise.",
            "",
            "        :param perm: The FAB permission",
            "        :returns: Whether the FAB permission is user defined",
            "        \"\"\"",
            "",
            "        return perm.permission.name in self.OBJECT_SPEC_PERMISSIONS",
            "",
            "    def create_custom_permissions(self) -> None:",
            "        \"\"\"",
            "        Create custom FAB permissions.",
            "        \"\"\"",
            "        self.add_permission_view_menu(\"all_datasource_access\", \"all_datasource_access\")",
            "        self.add_permission_view_menu(\"all_database_access\", \"all_database_access\")",
            "        self.add_permission_view_menu(\"all_query_access\", \"all_query_access\")",
            "        self.add_permission_view_menu(\"can_csv\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_dashboard\", \"Superset\")",
            "        self.add_permission_view_menu(\"can_share_chart\", \"Superset\")",
            "",
            "    def create_missing_perms(self) -> None:",
            "        \"\"\"",
            "        Creates missing FAB permissions for datasources, schemas and metrics.",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models import core as models",
            "",
            "        logger.info(\"Fetching a set of all perms to lookup which ones are missing\")",
            "        all_pvs = set()",
            "        for pv in self._get_all_pvms():",
            "            if pv.permission and pv.view_menu:",
            "                all_pvs.add((pv.permission.name, pv.view_menu.name))",
            "",
            "        def merge_pv(view_menu: str, perm: Optional[str]) -> None:",
            "            \"\"\"Create permission view menu only if it doesn't exist\"\"\"",
            "            if view_menu and perm and (view_menu, perm) not in all_pvs:",
            "                self.add_permission_view_menu(view_menu, perm)",
            "",
            "        logger.info(\"Creating missing datasource permissions.\")",
            "        datasources = SqlaTable.get_all_datasources(self.get_session)",
            "        for datasource in datasources:",
            "            merge_pv(\"datasource_access\", datasource.get_perm())",
            "            merge_pv(\"schema_access\", datasource.get_schema_perm())",
            "",
            "        logger.info(\"Creating missing database permissions.\")",
            "        databases = self.get_session.query(models.Database).all()",
            "        for database in databases:",
            "            merge_pv(\"database_access\", database.perm)",
            "",
            "    def clean_perms(self) -> None:",
            "        \"\"\"",
            "        Clean up the FAB faulty permissions.",
            "        \"\"\"",
            "",
            "        logger.info(\"Cleaning faulty perms\")",
            "        sesh = self.get_session",
            "        pvms = sesh.query(PermissionView).filter(",
            "            or_(",
            "                PermissionView.permission  # pylint: disable=singleton-comparison",
            "                == None,",
            "                PermissionView.view_menu  # pylint: disable=singleton-comparison",
            "                == None,",
            "            )",
            "        )",
            "        sesh.commit()",
            "        if deleted_count := pvms.delete():",
            "            logger.info(\"Deleted %i faulty permissions\", deleted_count)",
            "",
            "    def sync_role_definitions(self) -> None:",
            "        \"\"\"",
            "        Initialize the Superset application with security roles and such.",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing role definition\")",
            "",
            "        self.create_custom_permissions()",
            "",
            "        pvms = self._get_all_pvms()",
            "",
            "        # Creating default roles",
            "        self.set_role(\"Admin\", self._is_admin_pvm, pvms)",
            "        self.set_role(\"Alpha\", self._is_alpha_pvm, pvms)",
            "        self.set_role(\"Gamma\", self._is_gamma_pvm, pvms)",
            "        self.set_role(\"sql_lab\", self._is_sql_lab_pvm, pvms)",
            "",
            "        # Configure public role",
            "        if current_app.config[\"PUBLIC_ROLE_LIKE\"]:",
            "            self.copy_role(",
            "                current_app.config[\"PUBLIC_ROLE_LIKE\"],",
            "                self.auth_role_public,",
            "                merge=True,",
            "            )",
            "",
            "        self.create_missing_perms()",
            "",
            "        # commit role and view menu updates",
            "        self.get_session.commit()",
            "        self.clean_perms()",
            "",
            "    def _get_all_pvms(self) -> list[PermissionView]:",
            "        \"\"\"",
            "        Gets list of all PVM",
            "        \"\"\"",
            "        pvms = (",
            "            self.get_session.query(self.permissionview_model)",
            "            .options(",
            "                eagerload(self.permissionview_model.permission),",
            "                eagerload(self.permissionview_model.view_menu),",
            "            )",
            "            .all()",
            "        )",
            "        return [p for p in pvms if p.permission and p.view_menu]",
            "",
            "    def _get_pvms_from_builtin_role(self, role_name: str) -> list[PermissionView]:",
            "        \"\"\"",
            "        Gets a list of model PermissionView permissions inferred from a builtin role",
            "        definition",
            "        \"\"\"",
            "        role_from_permissions_names = self.builtin_roles.get(role_name, [])",
            "        all_pvms = self.get_session.query(PermissionView).all()",
            "        role_from_permissions = []",
            "        for pvm_regex in role_from_permissions_names:",
            "            view_name_regex = pvm_regex[0]",
            "            permission_name_regex = pvm_regex[1]",
            "            for pvm in all_pvms:",
            "                if re.match(view_name_regex, pvm.view_menu.name) and re.match(",
            "                    permission_name_regex, pvm.permission.name",
            "                ):",
            "                    if pvm not in role_from_permissions:",
            "                        role_from_permissions.append(pvm)",
            "        return role_from_permissions",
            "",
            "    def find_roles_by_id(self, role_ids: list[int]) -> list[Role]:",
            "        \"\"\"",
            "        Find a List of models by a list of ids, if defined applies `base_filter`",
            "        \"\"\"",
            "        query = self.get_session.query(Role).filter(Role.id.in_(role_ids))",
            "        return query.all()",
            "",
            "    def copy_role(",
            "        self, role_from_name: str, role_to_name: str, merge: bool = True",
            "    ) -> None:",
            "        \"\"\"",
            "        Copies permissions from a role to another.",
            "",
            "        Note: Supports regex defined builtin roles",
            "",
            "        :param role_from_name: The FAB role name from where the permissions are taken",
            "        :param role_to_name: The FAB role name from where the permissions are copied to",
            "        :param merge: If merge is true, keep data access permissions",
            "            if they already exist on the target role",
            "        \"\"\"",
            "",
            "        logger.info(\"Copy/Merge %s to %s\", role_from_name, role_to_name)",
            "        # If it's a builtin role extract permissions from it",
            "        if role_from_name in self.builtin_roles:",
            "            role_from_permissions = self._get_pvms_from_builtin_role(role_from_name)",
            "        else:",
            "            role_from_permissions = list(self.find_role(role_from_name).permissions)",
            "        role_to = self.add_role(role_to_name)",
            "        # If merge, recover existing data access permissions",
            "        if merge:",
            "            for permission_view in role_to.permissions:",
            "                if (",
            "                    permission_view not in role_from_permissions",
            "                    and permission_view.permission.name in self.data_access_permissions",
            "                ):",
            "                    role_from_permissions.append(permission_view)",
            "        role_to.permissions = role_from_permissions",
            "        self.get_session.commit()",
            "",
            "    def set_role(",
            "        self,",
            "        role_name: str,",
            "        pvm_check: Callable[[PermissionView], bool],",
            "        pvms: list[PermissionView],",
            "    ) -> None:",
            "        \"\"\"",
            "        Set the FAB permission/views for the role.",
            "",
            "        :param role_name: The FAB role name",
            "        :param pvm_check: The FAB permission/view check",
            "        \"\"\"",
            "",
            "        logger.info(\"Syncing %s perms\", role_name)",
            "        role = self.add_role(role_name)",
            "        role_pvms = [",
            "            permission_view for permission_view in pvms if pvm_check(permission_view)",
            "        ]",
            "        role.permissions = role_pvms",
            "        self.get_session.commit()",
            "",
            "    def _is_admin_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Admin users,",
            "        False otherwise.",
            "",
            "        Note readonly operations on read only model views are allowed only for admins.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Admin users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ADMIN_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ADMIN_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_alpha_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to only Alpha users,",
            "        False otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to only Alpha users",
            "        \"\"\"",
            "",
            "        if (",
            "            pvm.view_menu.name in self.GAMMA_READ_ONLY_MODEL_VIEWS",
            "            and pvm.permission.name not in self.READ_ONLY_PERMISSION",
            "        ):",
            "            return True",
            "        return (",
            "            pvm.view_menu.name in self.ALPHA_ONLY_VIEW_MENUS",
            "            or pvm.permission.name in self.ALPHA_ONLY_PERMISSIONS",
            "        )",
            "",
            "    def _is_accessible_to_all(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is accessible to all, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is accessible to all users",
            "        \"\"\"",
            "",
            "        return pvm.permission.name in self.ACCESSIBLE_PERMS",
            "",
            "    def _is_admin_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Admin user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Admin related",
            "        \"\"\"",
            "",
            "        return not self._is_user_defined_permission(pvm)",
            "",
            "    def _is_alpha_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Alpha user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Alpha related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_gamma_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is Gamma user related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is Gamma related",
            "        \"\"\"",
            "",
            "        return not (",
            "            self._is_user_defined_permission(pvm)",
            "            or self._is_admin_only(pvm)",
            "            or self._is_alpha_only(pvm)",
            "            or self._is_sql_lab_only(pvm)",
            "        ) or self._is_accessible_to_all(pvm)",
            "",
            "    def _is_sql_lab_only(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is only SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (pvm.permission.name, pvm.view_menu.name) in self.SQLLAB_ONLY_PERMISSIONS",
            "",
            "    def _is_sql_lab_pvm(self, pvm: PermissionView) -> bool:",
            "        \"\"\"",
            "        Return True if the FAB permission/view is SQL Lab related, False",
            "        otherwise.",
            "",
            "        :param pvm: The FAB permission/view",
            "        :returns: Whether the FAB object is SQL Lab related",
            "        \"\"\"",
            "        return (",
            "            self._is_sql_lab_only(pvm)",
            "            or (pvm.permission.name, pvm.view_menu.name)",
            "            in self.SQLLAB_EXTRA_PERMISSION_VIEWS",
            "        )",
            "",
            "    def database_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions when a database is created.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", target.get_perm()",
            "        )",
            "",
            "    def database_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a database is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The database PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        self._delete_vm_database_access(",
            "            mapper, connection, target.id, target.database_name",
            "        )",
            "",
            "    def database_after_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"Database\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a database is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The database PVM",
            "         - All datasets PVMs that reference the db, and it's local perm name",
            "         - All datasets local schema perm that reference the db.",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed database object",
            "        :return:",
            "        \"\"\"",
            "        # Check if database name has changed",
            "        state = inspect(target)",
            "        history = state.get_history(\"database_name\", True)",
            "        if not history.has_changes() or not history.deleted:",
            "            return",
            "",
            "        old_database_name = history.deleted[0]",
            "        # update database access permission",
            "        self._update_vm_database_access(mapper, connection, old_database_name, target)",
            "        # update datasource access",
            "        self._update_vm_datasources_access(",
            "            mapper, connection, old_database_name, target",
            "        )",
            "        # Note schema permissions are updated at the API level",
            "        # (database.commands.update). Since we need to fetch all existing schemas from",
            "        # the db",
            "",
            "    def _delete_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        database_id: int,",
            "        database_name: str,",
            "    ) -> None:",
            "        view_menu_name = self.get_database_perm(database_id, database_name)",
            "        # Clean database access permission",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"database_access\", view_menu_name",
            "        )",
            "        # Clean database schema permissions",
            "        schema_pvms = (",
            "            self.get_session.query(self.permissionview_model)",
            "            .join(self.permission_model)",
            "            .join(self.viewmenu_model)",
            "            .filter(self.permission_model.name == \"schema_access\")",
            "            .filter(self.viewmenu_model.name.like(f\"[{database_name}].[%]\"))",
            "            .all()",
            "        )",
            "        for schema_pvm in schema_pvms:",
            "            self._delete_pvm_on_sqla_event(mapper, connection, pvm=schema_pvm)",
            "",
            "    def _update_vm_database_access(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> Optional[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all database access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        old_view_menu_name = self.get_database_perm(target.id, old_database_name)",
            "        new_view_menu_name = self.get_database_perm(target.id, new_database_name)",
            "        db_pvm = self.find_permission_view_menu(\"database_access\", old_view_menu_name)",
            "        if not db_pvm:",
            "            logger.warning(",
            "                \"Could not find previous database permission %s\",",
            "                old_view_menu_name,",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"database_access\", new_view_menu_name",
            "            )",
            "            return None",
            "        new_updated_pvm = self.find_permission_view_menu(",
            "            \"database_access\", new_view_menu_name",
            "        )",
            "        if new_updated_pvm:",
            "            logger.info(",
            "                \"New permission [%s] already exists, deleting the previous\",",
            "                new_view_menu_name,",
            "            )",
            "            self._delete_vm_database_access(",
            "                mapper, connection, target.id, old_database_name",
            "            )",
            "            return None",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.id == db_pvm.view_menu_id)",
            "            .values(name=new_view_menu_name)",
            "        )",
            "        if not new_view_menu_name:",
            "            return None",
            "        new_db_view_menu = self._find_view_menu_on_sqla_event(",
            "            connection, new_view_menu_name",
            "        )",
            "",
            "        self.on_view_menu_after_update(mapper, connection, new_db_view_menu)",
            "        return new_db_view_menu",
            "",
            "    def _update_vm_datasources_access(  # pylint: disable=too-many-locals",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_database_name: str,",
            "        target: \"Database\",",
            "    ) -> list[ViewMenu]:",
            "        \"\"\"",
            "        Helper method that Updates all datasource access permission",
            "        when a database name changes.",
            "",
            "        :param connection: Current connection (called on SQLAlchemy event listener scope)",
            "        :param old_database_name: the old database name",
            "        :param target: The database object",
            "        :return: A list of changed view menus (permission resource names)",
            "        \"\"\"",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "        new_database_name = target.database_name",
            "        datasets = (",
            "            self.get_session.query(SqlaTable)",
            "            .filter(SqlaTable.database_id == target.id)",
            "            .all()",
            "        )",
            "        updated_view_menus: list[ViewMenu] = []",
            "        for dataset in datasets:",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, old_database_name",
            "            )",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                dataset.id, dataset.table_name, new_database_name",
            "            )",
            "            new_dataset_view_menu = self.find_view_menu(new_dataset_vm_name)",
            "            if new_dataset_view_menu:",
            "                continue",
            "            connection.execute(",
            "                view_menu_table.update()",
            "                .where(view_menu_table.c.name == old_dataset_vm_name)",
            "                .values(name=new_dataset_vm_name)",
            "            )",
            "",
            "            # Update dataset (SqlaTable perm field)",
            "            connection.execute(",
            "                sqlatable_table.update()",
            "                .where(",
            "                    sqlatable_table.c.id == dataset.id,",
            "                    sqlatable_table.c.perm == old_dataset_vm_name,",
            "                )",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            # Update charts (Slice perm field)",
            "            connection.execute(",
            "                chart_table.update()",
            "                .where(chart_table.c.perm == old_dataset_vm_name)",
            "                .values(perm=new_dataset_vm_name)",
            "            )",
            "            if new_dataset_vm_name:",
            "                # After update refresh",
            "                new_dataset_view_menu = self._find_view_menu_on_sqla_event(",
            "                    connection,",
            "                    new_dataset_vm_name,",
            "                )",
            "                self.on_view_menu_after_update(",
            "                    mapper,",
            "                    connection,",
            "                    new_dataset_view_menu,",
            "                )",
            "                updated_view_menus.append(new_dataset_view_menu)",
            "        return updated_view_menus",
            "",
            "    def dataset_after_insert(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permission creation when a dataset is inserted.",
            "        Triggered by a SQLAlchemy after_insert event.",
            "",
            "        We need to create:",
            "         - The dataset PVM and set local and schema perm",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        from superset.models.core import (  # pylint: disable=import-outside-toplevel",
            "            Database,",
            "        )",
            "",
            "        try:",
            "            dataset_perm: Optional[str] = target.get_perm()",
            "            database = target.database",
            "        except DatasetInvalidPermissionEvaluationException:",
            "            logger.warning(",
            "                \"Dataset has no database will retry with database_id to set permission\"",
            "            )",
            "            database = self.get_session.query(Database).get(target.database_id)",
            "            dataset_perm = self.get_dataset_perm(",
            "                target.id, target.table_name, database.database_name",
            "            )",
            "        dataset_table = target.__table__",
            "",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_perm",
            "        )",
            "        if target.perm != dataset_perm:",
            "            target.perm = dataset_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(perm=dataset_perm)",
            "            )",
            "",
            "        if target.schema:",
            "            dataset_schema_perm = self.get_schema_perm(",
            "                database.database_name, target.schema",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"schema_access\", dataset_schema_perm",
            "            )",
            "            target.schema_perm = dataset_schema_perm",
            "            connection.execute(",
            "                dataset_table.update()",
            "                .where(dataset_table.c.id == target.id)",
            "                .values(schema_perm=dataset_schema_perm)",
            "            )",
            "",
            "    def dataset_after_delete(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles permissions update when a dataset is deleted.",
            "        Triggered by a SQLAlchemy after_delete event.",
            "",
            "        We need to delete:",
            "         - The dataset PVM",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        dataset_vm_name = self.get_dataset_perm(",
            "            target.id, target.table_name, target.database.database_name",
            "        )",
            "        self._delete_pvm_on_sqla_event(",
            "            mapper, connection, \"datasource_access\", dataset_vm_name",
            "        )",
            "",
            "    def dataset_before_update(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Handles all permissions update when a dataset is changed.",
            "        Triggered by a SQLAlchemy after_update event.",
            "",
            "        We need to update:",
            "         - The dataset PVM and local perm",
            "         - All charts local perm related with said datasets",
            "         - All charts local schema perm related with said datasets",
            "",
            "        :param mapper: The SQLA mapper",
            "        :param connection: The SQLA connection",
            "        :param target: The changed dataset object",
            "        :return:",
            "        \"\"\"",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import SqlaTable",
            "",
            "        # Check if watched fields have changed",
            "        table = SqlaTable.__table__  # pylint: disable=no-member",
            "        current_dataset = connection.execute(",
            "            table.select().where(table.c.id == target.id)",
            "        ).one()",
            "        current_db_id = current_dataset.database_id",
            "        current_schema = current_dataset.schema",
            "        current_table_name = current_dataset.table_name",
            "",
            "        # When database name changes",
            "        if current_db_id != target.database_id:",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, target.perm, new_dataset_vm_name, target",
            "            )",
            "",
            "            # Updates schema permissions",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "        # When table name changes",
            "        if current_table_name != target.table_name:",
            "            new_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, target.table_name, target.database.database_name",
            "            )",
            "            old_dataset_vm_name = self.get_dataset_perm(",
            "                target.id, current_table_name, target.database.database_name",
            "            )",
            "            self._update_dataset_perm(",
            "                mapper, connection, old_dataset_vm_name, new_dataset_vm_name, target",
            "            )",
            "",
            "        # When schema changes",
            "        if current_schema != target.schema:",
            "            new_dataset_schema_name = self.get_schema_perm(",
            "                target.database.database_name, target.schema",
            "            )",
            "            self._update_dataset_schema_perm(",
            "                mapper,",
            "                connection,",
            "                new_dataset_schema_name,",
            "                target,",
            "            )",
            "",
            "    def _update_dataset_schema_perm(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        new_schema_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a new schema permission name, propagates the name change to datasets and charts.",
            "",
            "        If the schema permission name does not exist already has a PVM,",
            "        creates a new one.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param new_schema_permission_name: The new schema permission name that changed",
            "        :param target: Dataset that was updated",
            "        :return:",
            "        \"\"\"",
            "        logger.info(\"Updating schema perm, new: %s\", new_schema_permission_name)",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        # insert new schema PVM if it does not exist",
            "        self._insert_pvm_on_sqla_event(",
            "            mapper, connection, \"schema_access\", new_schema_permission_name",
            "        )",
            "",
            "        # Update dataset (SqlaTable schema_perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "        # Update charts (Slice schema_perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_id == target.id,",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "            )",
            "            .values(schema_perm=new_schema_permission_name)",
            "        )",
            "",
            "    def _update_dataset_perm(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        old_permission_name: Optional[str],",
            "        new_permission_name: Optional[str],",
            "        target: \"SqlaTable\",",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events on datasets to update",
            "        a permission name change, propagates the name change to VM, datasets and charts.",
            "",
            "        :param mapper:",
            "        :param connection:",
            "        :param old_permission_name",
            "        :param new_permission_name:",
            "        :param target:",
            "        :return:",
            "        \"\"\"",
            "        logger.info(",
            "            \"Updating dataset perm, old: %s, new: %s\",",
            "            old_permission_name,",
            "            new_permission_name,",
            "        )",
            "        from superset.connectors.sqla.models import (  # pylint: disable=import-outside-toplevel",
            "            SqlaTable,",
            "        )",
            "        from superset.models.slice import (  # pylint: disable=import-outside-toplevel",
            "            Slice,",
            "        )",
            "",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        sqlatable_table = SqlaTable.__table__  # pylint: disable=no-member",
            "        chart_table = Slice.__table__  # pylint: disable=no-member",
            "",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        if new_dataset_view_menu:",
            "            return",
            "        old_dataset_view_menu = self.find_view_menu(old_permission_name)",
            "        if not old_dataset_view_menu:",
            "            logger.warning(",
            "                \"Could not find previous dataset permission %s\", old_permission_name",
            "            )",
            "            self._insert_pvm_on_sqla_event(",
            "                mapper, connection, \"datasource_access\", new_permission_name",
            "            )",
            "            return",
            "        # Update VM",
            "        connection.execute(",
            "            view_menu_table.update()",
            "            .where(view_menu_table.c.name == old_permission_name)",
            "            .values(name=new_permission_name)",
            "        )",
            "        # VM changed, so call hook",
            "        new_dataset_view_menu = self.find_view_menu(new_permission_name)",
            "        self.on_view_menu_after_update(mapper, connection, new_dataset_view_menu)",
            "        # Update dataset (SqlaTable perm field)",
            "        connection.execute(",
            "            sqlatable_table.update()",
            "            .where(",
            "                sqlatable_table.c.id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "        # Update charts (Slice perm field)",
            "        connection.execute(",
            "            chart_table.update()",
            "            .where(",
            "                chart_table.c.datasource_type == DatasourceType.TABLE,",
            "                chart_table.c.datasource_id == target.id,",
            "            )",
            "            .values(perm=new_permission_name)",
            "        )",
            "",
            "    def _delete_pvm_on_sqla_event(  # pylint: disable=too-many-arguments",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: Optional[str] = None,",
            "        view_menu_name: Optional[str] = None,",
            "        pvm: Optional[PermissionView] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Deletes a PVM.",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :param pvm: Can be called with the actual PVM already",
            "        :return:",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_menu_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "",
            "        if not pvm:",
            "            pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if not pvm:",
            "            return",
            "        # Delete Any Role to PVM association",
            "        connection.execute(",
            "            assoc_permissionview_role.delete().where(",
            "                assoc_permissionview_role.c.permission_view_id == pvm.id",
            "            )",
            "        )",
            "        # Delete the database access PVM",
            "        connection.execute(",
            "            permission_view_menu_table.delete().where(",
            "                permission_view_menu_table.c.id == pvm.id",
            "            )",
            "        )",
            "        self.on_permission_view_after_delete(mapper, connection, pvm)",
            "        connection.execute(",
            "            view_menu_table.delete().where(view_menu_table.c.id == pvm.view_menu_id)",
            "        )",
            "",
            "    def _find_permission_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> Permission:",
            "        \"\"\"",
            "        Find a FAB Permission using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The permission name (it's unique)",
            "        :return: Permission",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "",
            "        permission_ = connection.execute(",
            "            permission_table.select().where(permission_table.c.name == name)",
            "        ).fetchone()",
            "        permission = Permission()",
            "        # ensures this object is never persisted",
            "        permission.metadata = None",
            "        permission.id = permission_.id",
            "        permission.name = permission_.name",
            "        return permission",
            "",
            "    def _find_view_menu_on_sqla_event(",
            "        self, connection: Connection, name: str",
            "    ) -> ViewMenu:",
            "        \"\"\"",
            "        Find a FAB ViewMenu using a SQLA connection.",
            "",
            "        A session.query may not return the latest results on newly created/updated",
            "        objects/rows using connection. On this case we should use a connection also",
            "",
            "        :param connection: SQLAlchemy connection",
            "        :param name: The ViewMenu name (it's unique)",
            "        :return: ViewMenu",
            "        \"\"\"",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "",
            "        view_menu_ = connection.execute(",
            "            view_menu_table.select().where(view_menu_table.c.name == name)",
            "        ).fetchone()",
            "        view_menu = ViewMenu()",
            "        # ensures this object is never persisted",
            "        view_menu.metadata = None",
            "        view_menu.id = view_menu_.id",
            "        view_menu.name = view_menu_.name",
            "        return view_menu",
            "",
            "    def _insert_pvm_on_sqla_event(",
            "        self,",
            "        mapper: Mapper,",
            "        connection: Connection,",
            "        permission_name: str,",
            "        view_menu_name: Optional[str],",
            "    ) -> None:",
            "        \"\"\"",
            "        Helper method that is called by SQLAlchemy events.",
            "        Inserts a new PVM (if it does not exist already)",
            "",
            "        :param mapper: The SQLA event mapper",
            "        :param connection: The SQLA connection",
            "        :param permission_name: e.g.: datasource_access, schema_access",
            "        :param view_menu_name: e.g. [db1].[public]",
            "        :return:",
            "        \"\"\"",
            "        permission_table = self.permission_model.__table__  # pylint: disable=no-member",
            "        view_menu_table = self.viewmenu_model.__table__  # pylint: disable=no-member",
            "        permission_view_table = (",
            "            self.permissionview_model.__table__  # pylint: disable=no-member",
            "        )",
            "        if not view_menu_name:",
            "            return",
            "        pvm = self.find_permission_view_menu(permission_name, view_menu_name)",
            "        if pvm:",
            "            return",
            "        permission = self.find_permission(permission_name)",
            "        view_menu = self.find_view_menu(view_menu_name)",
            "        if not permission:",
            "            _ = connection.execute(",
            "                permission_table.insert().values(name=permission_name)",
            "            )",
            "            permission = self._find_permission_on_sqla_event(",
            "                connection, permission_name",
            "            )",
            "            self.on_permission_after_insert(mapper, connection, permission)",
            "        if not view_menu:",
            "            _ = connection.execute(view_menu_table.insert().values(name=view_menu_name))",
            "            view_menu = self._find_view_menu_on_sqla_event(connection, view_menu_name)",
            "            self.on_view_menu_after_insert(mapper, connection, view_menu)",
            "        connection.execute(",
            "            permission_view_table.insert().values(",
            "                permission_id=permission.id, view_menu_id=view_menu.id",
            "            )",
            "        )",
            "        permission_view = connection.execute(",
            "            permission_view_table.select().where(",
            "                permission_view_table.c.permission_id == permission.id,",
            "                permission_view_table.c.view_menu_id == view_menu.id,",
            "            )",
            "        ).fetchone()",
            "        permission_view_model = PermissionView()",
            "        permission_view_model.metadata = None",
            "        permission_view_model.id = permission_view.id",
            "        permission_view_model.permission_id = permission.id",
            "        permission_view_model.view_menu_id = view_menu.id",
            "        permission_view_model.permission = permission",
            "        permission_view_model.view_menu = view_menu",
            "        self.on_permission_view_after_insert(mapper, connection, permission_view_model)",
            "",
            "    def on_role_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: Role",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a Role update",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being changed",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is created by set_perm.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new view_menu's using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_view_menu_after_update(",
            "        self, mapper: Mapper, connection: Connection, target: ViewMenu",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new ViewMenu",
            "        is updated",
            "",
            "        Since the update may be performed on after_update event. We cannot",
            "        update ViewMenus using a session, so any SQLAlchemy events hooked to",
            "        `ViewMenu` will not trigger an after_update.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: Permission",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new permission",
            "        is created by set_perm.",
            "",
            "        Since set_perm is executed by SQLAlchemy after_insert events, we cannot",
            "        create new permissions using a session, so any SQLAlchemy events hooked to",
            "        `Permission` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_insert(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is created by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_insert events, we cannot",
            "        create new pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_insert.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    def on_permission_view_after_delete(",
            "        self, mapper: Mapper, connection: Connection, target: PermissionView",
            "    ) -> None:",
            "        \"\"\"",
            "        Hook that allows for further custom operations when a new PermissionView",
            "        is delete by SQLAlchemy events.",
            "",
            "        On SQLAlchemy after_delete events, we cannot",
            "        delete pvms using a session, so any SQLAlchemy events hooked to",
            "        `PermissionView` will not trigger an after_delete.",
            "",
            "        :param mapper: The table mapper",
            "        :param connection: The DB-API connection",
            "        :param target: The mapped instance being persisted",
            "        \"\"\"",
            "",
            "    @staticmethod",
            "    def get_exclude_users_from_lists() -> list[str]:",
            "        \"\"\"",
            "        Override to dynamically identify a list of usernames to exclude from",
            "        all UI dropdown lists, owners, created_by filters etc...",
            "",
            "        It will exclude all users from the all endpoints of the form",
            "        ``/api/v1/<modelview>/related/<column>``",
            "",
            "        Optionally you can also exclude them using the `EXCLUDE_USERS_FROM_LISTS`",
            "        config setting.",
            "",
            "        :return: A list of usernames",
            "        \"\"\"",
            "        return []",
            "",
            "    def raise_for_access(",
            "        # pylint: disable=too-many-arguments,too-many-branches,too-many-locals",
            "        self,",
            "        dashboard: Optional[\"Dashboard\"] = None,",
            "        database: Optional[\"Database\"] = None,",
            "        datasource: Optional[\"BaseDatasource\"] = None,",
            "        query: Optional[\"Query\"] = None,",
            "        query_context: Optional[\"QueryContext\"] = None,",
            "        table: Optional[\"Table\"] = None,",
            "        viz: Optional[\"BaseViz\"] = None,",
            "        sql: Optional[str] = None,",
            "        schema: Optional[str] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user cannot access the resource.",
            "",
            "        :param database: The Superset database",
            "        :param datasource: The Superset datasource",
            "        :param query: The SQL Lab query",
            "        :param query_context: The query context",
            "        :param table: The Superset table (requires database)",
            "        :param viz: The visualization",
            "        :param sql: The SQL string (requires database)",
            "        :param schema: Optional schema name",
            "        :raises SupersetSecurityException: If the user cannot access the resource",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "        from superset.connectors.sqla.models import SqlaTable",
            "        from superset.models.dashboard import Dashboard",
            "        from superset.models.slice import Slice",
            "        from superset.models.sql_lab import Query",
            "        from superset.sql_parse import Table",
            "        from superset.utils.core import shortid",
            "",
            "        if sql and database:",
            "            query = Query(",
            "                database=database,",
            "                sql=sql,",
            "                schema=schema,",
            "                client_id=shortid()[:10],",
            "                user_id=get_user_id(),",
            "            )",
            "            self.get_session.expunge(query)",
            "",
            "        if database and table or query:",
            "            if query:",
            "                database = query.database",
            "",
            "            database = cast(\"Database\", database)",
            "",
            "            if self.can_access_database(database):",
            "                return",
            "",
            "            if query:",
            "                default_schema = database.get_default_schema_for_query(query)",
            "                tables = {",
            "                    Table(table_.table, table_.schema or default_schema)",
            "                    for table_ in sql_parse.ParsedQuery(",
            "                        query.sql,",
            "                        engine=database.db_engine_spec.engine,",
            "                    ).tables",
            "                }",
            "            elif table:",
            "                tables = {table}",
            "",
            "            denied = set()",
            "",
            "            for table_ in tables:",
            "                schema_perm = self.get_schema_perm(database, schema=table_.schema)",
            "",
            "                if not (schema_perm and self.can_access(\"schema_access\", schema_perm)):",
            "                    datasources = SqlaTable.query_datasources_by_name(",
            "                        self.get_session, database, table_.table, schema=table_.schema",
            "                    )",
            "",
            "                    # Access to any datasource is suffice.",
            "                    for datasource_ in datasources:",
            "                        if self.can_access(",
            "                            \"datasource_access\", datasource_.perm",
            "                        ) or self.is_owner(datasource_):",
            "                            break",
            "                    else:",
            "                        denied.add(table_)",
            "",
            "            if denied:",
            "                raise SupersetSecurityException(",
            "                    self.get_table_access_error_object(denied)",
            "                )",
            "",
            "        if datasource or query_context or viz:",
            "            form_data = None",
            "",
            "            if query_context:",
            "                datasource = query_context.datasource",
            "                form_data = query_context.form_data",
            "            elif viz:",
            "                datasource = viz.datasource",
            "                form_data = viz.form_data",
            "",
            "            assert datasource",
            "",
            "            if not (",
            "                self.can_access_schema(datasource)",
            "                or self.can_access(\"datasource_access\", datasource.perm or \"\")",
            "                or self.is_owner(datasource)",
            "                or (",
            "                    # Grant access to the datasource only if dashboard RBAC is enabled",
            "                    # or the user is an embedded guest user with access to the dashboard",
            "                    # and said datasource is associated with the dashboard chart in",
            "                    # question.",
            "                    form_data",
            "                    and (dashboard_id := form_data.get(\"dashboardId\"))",
            "                    and (",
            "                        dashboard_ := self.get_session.query(Dashboard)",
            "                        .filter(Dashboard.id == dashboard_id)",
            "                        .one_or_none()",
            "                    )",
            "                    and (",
            "                        (is_feature_enabled(\"DASHBOARD_RBAC\") and dashboard_.roles)",
            "                        or (",
            "                            is_feature_enabled(\"EMBEDDED_SUPERSET\")",
            "                            and self.is_guest_user()",
            "                        )",
            "                    )",
            "                    and (",
            "                        (",
            "                            # Native filter.",
            "                            form_data.get(\"type\") == \"NATIVE_FILTER\"",
            "                            and (native_filter_id := form_data.get(\"native_filter_id\"))",
            "                            and dashboard_.json_metadata",
            "                            and (json_metadata := json.loads(dashboard_.json_metadata))",
            "                            and any(",
            "                                target.get(\"datasetId\") == datasource.id",
            "                                for fltr in json_metadata.get(",
            "                                    \"native_filter_configuration\",",
            "                                    [],",
            "                                )",
            "                                for target in fltr.get(\"targets\", [])",
            "                                if native_filter_id == fltr.get(\"id\")",
            "                            )",
            "                        )",
            "                        or (",
            "                            # Chart.",
            "                            form_data.get(\"type\") != \"NATIVE_FILTER\"",
            "                            and (slice_id := form_data.get(\"slice_id\"))",
            "                            and (",
            "                                slc := self.get_session.query(Slice)",
            "                                .filter(Slice.id == slice_id)",
            "                                .one_or_none()",
            "                            )",
            "                            and slc in dashboard_.slices",
            "                            and slc.datasource == datasource",
            "                        )",
            "                    )",
            "                    and self.can_access_dashboard(dashboard_)",
            "                )",
            "            ):",
            "                raise SupersetSecurityException(",
            "                    self.get_datasource_access_error_object(datasource)",
            "                )",
            "",
            "        if dashboard:",
            "            if self.is_guest_user():",
            "                # Guest user is currently used for embedded dashboards only. If the guest",
            "                # user doesn't have access to the dashboard, ignore all other checks.",
            "                if self.has_guest_access(dashboard):",
            "                    return",
            "                raise SupersetSecurityException(",
            "                    self.get_dashboard_access_error_object(dashboard)",
            "                )",
            "",
            "            if self.is_admin() or self.is_owner(dashboard):",
            "                return",
            "",
            "            # RBAC and legacy (datasource inferred) access controls.",
            "            if is_feature_enabled(\"DASHBOARD_RBAC\") and dashboard.roles:",
            "                if dashboard.published and {role.id for role in dashboard.roles} & {",
            "                    role.id for role in self.get_user_roles()",
            "                }:",
            "                    return",
            "            elif (",
            "                # To understand why we rely on status and give access to draft dashboards",
            "                # without roles take a look at:",
            "                #",
            "                #   - https://github.com/apache/superset/pull/24350#discussion_r1225061550",
            "                #   - https://github.com/apache/superset/pull/17511#issuecomment-975870169",
            "                #",
            "                not dashboard.published",
            "                or not dashboard.datasources",
            "                or any(",
            "                    self.can_access_datasource(datasource)",
            "                    for datasource in dashboard.datasources",
            "                )",
            "            ):",
            "                return",
            "",
            "            raise SupersetSecurityException(",
            "                self.get_dashboard_access_error_object(dashboard)",
            "            )",
            "",
            "    def get_user_by_username(",
            "        self, username: str, session: Session = None",
            "    ) -> Optional[User]:",
            "        \"\"\"",
            "        Retrieves a user by it's username case sensitive. Optional session parameter",
            "        utility method normally useful for celery tasks where the session",
            "        need to be scoped",
            "        \"\"\"",
            "        session = session or self.get_session",
            "        return (",
            "            session.query(self.user_model)",
            "            .filter(self.user_model.username == username)",
            "            .one_or_none()",
            "        )",
            "",
            "    def get_anonymous_user(self) -> User:",
            "        return AnonymousUserMixin()",
            "",
            "    def get_user_roles(self, user: Optional[User] = None) -> list[Role]:",
            "        if not user:",
            "            user = g.user",
            "        if user.is_anonymous:",
            "            public_role = current_app.config.get(\"AUTH_ROLE_PUBLIC\")",
            "            return [self.get_public_role()] if public_role else []",
            "        return user.roles",
            "",
            "    def get_guest_rls_filters(",
            "        self, dataset: \"BaseDatasource\"",
            "    ) -> list[GuestTokenRlsRule]:",
            "        \"\"\"",
            "        Retrieves the row level security filters for the current user and the dataset,",
            "        if the user is authenticated with a guest token.",
            "        :param dataset: The dataset to check against",
            "        :return: A list of filters",
            "        \"\"\"",
            "        if guest_user := self.get_current_guest_user_if_guest():",
            "            return [",
            "                rule",
            "                for rule in guest_user.rls",
            "                if not rule.get(\"dataset\")",
            "                or str(rule.get(\"dataset\")) == str(dataset.id)",
            "            ]",
            "        return []",
            "",
            "    def get_rls_filters(self, table: \"BaseDatasource\") -> list[SqlaQuery]:",
            "        \"\"\"",
            "        Retrieves the appropriate row level security filters for the current user and",
            "        the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list of filters",
            "        \"\"\"",
            "",
            "        if not (hasattr(g, \"user\") and g.user is not None):",
            "            return []",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.connectors.sqla.models import (",
            "            RLSFilterRoles,",
            "            RLSFilterTables,",
            "            RowLevelSecurityFilter,",
            "        )",
            "",
            "        user_roles = [role.id for role in self.get_user_roles(g.user)]",
            "        regular_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.REGULAR",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        base_filter_roles = (",
            "            self.get_session()",
            "            .query(RLSFilterRoles.c.rls_filter_id)",
            "            .join(RowLevelSecurityFilter)",
            "            .filter(",
            "                RowLevelSecurityFilter.filter_type == RowLevelSecurityFilterType.BASE",
            "            )",
            "            .filter(RLSFilterRoles.c.role_id.in_(user_roles))",
            "        )",
            "        filter_tables = (",
            "            self.get_session()",
            "            .query(RLSFilterTables.c.rls_filter_id)",
            "            .filter(RLSFilterTables.c.table_id == table.id)",
            "        )",
            "        query = (",
            "            self.get_session()",
            "            .query(",
            "                RowLevelSecurityFilter.id,",
            "                RowLevelSecurityFilter.group_key,",
            "                RowLevelSecurityFilter.clause,",
            "            )",
            "            .filter(RowLevelSecurityFilter.id.in_(filter_tables))",
            "            .filter(",
            "                or_(",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.REGULAR,",
            "                        RowLevelSecurityFilter.id.in_(regular_filter_roles),",
            "                    ),",
            "                    and_(",
            "                        RowLevelSecurityFilter.filter_type",
            "                        == RowLevelSecurityFilterType.BASE,",
            "                        RowLevelSecurityFilter.id.notin_(base_filter_roles),",
            "                    ),",
            "                )",
            "            )",
            "        )",
            "        return query.all()",
            "",
            "    def get_rls_sorted(self, table: \"BaseDatasource\") -> list[\"RowLevelSecurityFilter\"]:",
            "        \"\"\"",
            "        Retrieves a list RLS filters sorted by ID for",
            "        the current user and the passed table.",
            "",
            "        :param table: The table to check against",
            "        :returns: A list RLS filters",
            "        \"\"\"",
            "        filters = self.get_rls_filters(table)",
            "        filters.sort(key=lambda f: f.id)",
            "        return filters",
            "",
            "    def get_guest_rls_filters_str(self, table: \"BaseDatasource\") -> list[str]:",
            "        return [f.get(\"clause\", \"\") for f in self.get_guest_rls_filters(table)]",
            "",
            "    def get_rls_cache_key(self, datasource: \"BaseDatasource\") -> list[str]:",
            "        rls_clauses_with_group_key = []",
            "        if datasource.is_rls_supported:",
            "            rls_clauses_with_group_key = [",
            "                f\"{f.clause}-{f.group_key or ''}\"",
            "                for f in self.get_rls_sorted(datasource)",
            "            ]",
            "        guest_rls = self.get_guest_rls_filters_str(datasource)",
            "        return guest_rls + rls_clauses_with_group_key",
            "",
            "    @staticmethod",
            "    def _get_current_epoch_time() -> float:",
            "        \"\"\"This is used so the tests can mock time\"\"\"",
            "        return time.time()",
            "",
            "    @staticmethod",
            "    def _get_guest_token_jwt_audience() -> str:",
            "        audience = current_app.config[\"GUEST_TOKEN_JWT_AUDIENCE\"] or get_url_host()",
            "        if callable(audience):",
            "            audience = audience()",
            "        return audience",
            "",
            "    @staticmethod",
            "    def validate_guest_token_resources(resources: GuestTokenResources) -> None:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset.commands.dashboard.embedded.exceptions import (",
            "            EmbeddedDashboardNotFoundError,",
            "        )",
            "        from superset.daos.dashboard import EmbeddedDashboardDAO",
            "        from superset.models.dashboard import Dashboard",
            "",
            "        for resource in resources:",
            "            if resource[\"type\"] == GuestTokenResourceType.DASHBOARD.value:",
            "                # TODO (embedded): remove this check once uuids are rolled out",
            "                dashboard = Dashboard.get(str(resource[\"id\"]))",
            "                if not dashboard:",
            "                    embedded = EmbeddedDashboardDAO.find_by_id(str(resource[\"id\"]))",
            "                    if not embedded:",
            "                        raise EmbeddedDashboardNotFoundError()",
            "",
            "    def create_guest_access_token(",
            "        self,",
            "        user: GuestTokenUser,",
            "        resources: GuestTokenResources,",
            "        rls: list[GuestTokenRlsRule],",
            "    ) -> bytes:",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        exp_seconds = current_app.config[\"GUEST_TOKEN_JWT_EXP_SECONDS\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        # calculate expiration time",
            "        now = self._get_current_epoch_time()",
            "        exp = now + exp_seconds",
            "        claims = {",
            "            \"user\": user,",
            "            \"resources\": resources,",
            "            \"rls_rules\": rls,",
            "            # standard jwt claims:",
            "            \"iat\": now,  # issued at",
            "            \"exp\": exp,  # expiration time",
            "            \"aud\": audience,",
            "            \"type\": \"guest\",",
            "        }",
            "        return self.pyjwt_for_guest_token.encode(claims, secret, algorithm=algo)",
            "",
            "    def get_guest_user_from_request(self, req: Request) -> Optional[GuestUser]:",
            "        \"\"\"",
            "        If there is a guest token in the request (used for embedded),",
            "        parses the token and returns the guest user.",
            "        This is meant to be used as a request loader for the LoginManager.",
            "        The LoginManager will only call this if an active session cannot be found.",
            "",
            "        :return: A guest user object",
            "        \"\"\"",
            "        raw_token = req.headers.get(",
            "            current_app.config[\"GUEST_TOKEN_HEADER_NAME\"]",
            "        ) or req.form.get(\"guest_token\")",
            "        if raw_token is None:",
            "            return None",
            "",
            "        try:",
            "            token = self.parse_jwt_guest_token(raw_token)",
            "            if token.get(\"user\") is None:",
            "                raise ValueError(\"Guest token does not contain a user claim\")",
            "            if token.get(\"resources\") is None:",
            "                raise ValueError(\"Guest token does not contain a resources claim\")",
            "            if token.get(\"rls_rules\") is None:",
            "                raise ValueError(\"Guest token does not contain an rls_rules claim\")",
            "            if token.get(\"type\") != \"guest\":",
            "                raise ValueError(\"This is not a guest token.\")",
            "        except Exception:  # pylint: disable=broad-except",
            "            # The login manager will handle sending 401s.",
            "            # We don't need to send a special error message.",
            "            logger.warning(\"Invalid guest token\", exc_info=True)",
            "            return None",
            "",
            "        return self.get_guest_user_from_token(cast(GuestToken, token))",
            "",
            "    def get_guest_user_from_token(self, token: GuestToken) -> GuestUser:",
            "        return self.guest_user_cls(",
            "            token=token,",
            "            roles=[self.find_role(current_app.config[\"GUEST_ROLE_NAME\"])],",
            "        )",
            "",
            "    def parse_jwt_guest_token(self, raw_token: str) -> dict[str, Any]:",
            "        \"\"\"",
            "        Parses a guest token. Raises an error if the jwt fails standard claims checks.",
            "        :param raw_token: the token gotten from the request",
            "        :return: the same token that was passed in, tested but unchanged",
            "        \"\"\"",
            "        secret = current_app.config[\"GUEST_TOKEN_JWT_SECRET\"]",
            "        algo = current_app.config[\"GUEST_TOKEN_JWT_ALGO\"]",
            "        audience = self._get_guest_token_jwt_audience()",
            "        return self.pyjwt_for_guest_token.decode(",
            "            raw_token, secret, algorithms=[algo], audience=audience",
            "        )",
            "",
            "    @staticmethod",
            "    def is_guest_user(user: Optional[Any] = None) -> bool:",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import is_feature_enabled",
            "",
            "        if not is_feature_enabled(\"EMBEDDED_SUPERSET\"):",
            "            return False",
            "        if not user:",
            "            user = g.user",
            "        return hasattr(user, \"is_guest_user\") and user.is_guest_user",
            "",
            "    def get_current_guest_user_if_guest(self) -> Optional[GuestUser]:",
            "        return g.user if self.is_guest_user() else None",
            "",
            "    def has_guest_access(self, dashboard: \"Dashboard\") -> bool:",
            "        user = self.get_current_guest_user_if_guest()",
            "        if not user:",
            "            return False",
            "",
            "        dashboards = [",
            "            r",
            "            for r in user.resources",
            "            if r[\"type\"] == GuestTokenResourceType.DASHBOARD.value",
            "        ]",
            "",
            "        # TODO (embedded): remove this check once uuids are rolled out",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.id):",
            "                return True",
            "",
            "        if not dashboard.embedded:",
            "            return False",
            "",
            "        for resource in dashboards:",
            "            if str(resource[\"id\"]) == str(dashboard.embedded[0].uuid):",
            "                return True",
            "        return False",
            "",
            "    def raise_for_ownership(self, resource: Model) -> None:",
            "        \"\"\"",
            "        Raise an exception if the user does not own the resource.",
            "",
            "        Note admins are deemed owners of all resources.",
            "",
            "        :param resource: The dashboard, dataset, chart, etc. resource",
            "        :raises SupersetSecurityException: If the current user is not an owner",
            "        \"\"\"",
            "",
            "        # pylint: disable=import-outside-toplevel",
            "        from superset import db",
            "",
            "        if self.is_admin():",
            "            return",
            "",
            "        orig_resource = db.session.query(resource.__class__).get(resource.id)",
            "        owners = orig_resource.owners if hasattr(orig_resource, \"owners\") else []",
            "",
            "        if g.user.is_anonymous or g.user not in owners:",
            "            raise SupersetSecurityException(",
            "                SupersetError(",
            "                    error_type=SupersetErrorType.MISSING_OWNERSHIP_ERROR,",
            "                    message=_(",
            "                        \"You don't have the rights to alter %(resource)s\",",
            "                        resource=resource,",
            "                    ),",
            "                    level=ErrorLevel.ERROR,",
            "                )",
            "            )",
            "",
            "    def is_owner(self, resource: Model) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an owner of the resource, False otherwise.",
            "",
            "        :param resource: The dashboard, dataset, chart, etc. resource",
            "        :returns: Whether the current user is an owner of the resource",
            "        \"\"\"",
            "",
            "        try:",
            "            self.raise_for_ownership(resource)",
            "        except SupersetSecurityException:",
            "            return False",
            "",
            "        return True",
            "",
            "    def is_admin(self) -> bool:",
            "        \"\"\"",
            "        Returns True if the current user is an admin user, False otherwise.",
            "",
            "        :returns: Whether the current user is an admin user",
            "        \"\"\"",
            "",
            "        return current_app.config[\"AUTH_ROLE_ADMIN\"] in [",
            "            role.name for role in self.get_user_roles()",
            "        ]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1880": [
                "SupersetSecurityManager",
                "raise_for_access"
            ]
        },
        "addLocation": []
    },
    "superset/sql_lab.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 208,
                "PatchRowcode": "     database: Database = query.database"
            },
            "1": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 209,
                "PatchRowcode": "     db_engine_spec = database.db_engine_spec"
            },
            "2": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 210,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    parsed_query = ParsedQuery(sql_statement)"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 211,
                "PatchRowcode": "+    parsed_query = ParsedQuery(sql_statement, engine=db_engine_spec.engine)"
            },
            "5": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": 212,
                "PatchRowcode": "     if is_feature_enabled(\"RLS_IN_SQLLAB\"):"
            },
            "6": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "         # There are two ways to insert RLS: either replacing the table with a subquery"
            },
            "7": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "         # that has the RLS, or appending the RLS to the ``WHERE`` clause. The former is"
            },
            "8": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 228,
                "PatchRowcode": "                     database.id,"
            },
            "9": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "                     query.schema,"
            },
            "10": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "                 )"
            },
            "11": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 231,
                "PatchRowcode": "+            ),"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 232,
                "PatchRowcode": "+            engine=db_engine_spec.engine,"
            },
            "14": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "         )"
            },
            "15": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": 234,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "     sql = parsed_query.stripped()"
            },
            "17": {
                "beforePatchRowNumber": 419,
                "afterPatchRowNumber": 420,
                "PatchRowcode": "         )"
            },
            "18": {
                "beforePatchRowNumber": 420,
                "afterPatchRowNumber": 421,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 422,
                "PatchRowcode": "     # Breaking down into multiple statements"
            },
            "20": {
                "beforePatchRowNumber": 422,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    parsed_query = ParsedQuery(rendered_query, strip_comments=True)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 423,
                "PatchRowcode": "+    parsed_query = ParsedQuery("
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 424,
                "PatchRowcode": "+        rendered_query,"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 425,
                "PatchRowcode": "+        strip_comments=True,"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 426,
                "PatchRowcode": "+        engine=db_engine_spec.engine,"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 427,
                "PatchRowcode": "+    )"
            },
            "26": {
                "beforePatchRowNumber": 423,
                "afterPatchRowNumber": 428,
                "PatchRowcode": "     if not db_engine_spec.run_multiple_statements_as_one:"
            },
            "27": {
                "beforePatchRowNumber": 424,
                "afterPatchRowNumber": 429,
                "PatchRowcode": "         statements = parsed_query.get_statements()"
            },
            "28": {
                "beforePatchRowNumber": 425,
                "afterPatchRowNumber": 430,
                "PatchRowcode": "         logger.info("
            }
        },
        "frontPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import dataclasses",
            "import logging",
            "import uuid",
            "from contextlib import closing",
            "from datetime import datetime",
            "from sys import getsizeof",
            "from typing import Any, cast, Optional, Union",
            "",
            "import backoff",
            "import msgpack",
            "import simplejson as json",
            "from celery import Task",
            "from celery.exceptions import SoftTimeLimitExceeded",
            "from flask_babel import gettext as __",
            "from sqlalchemy.orm import Session",
            "",
            "from superset import (",
            "    app,",
            "    db,",
            "    is_feature_enabled,",
            "    results_backend,",
            "    results_backend_use_msgpack,",
            "    security_manager,",
            ")",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.constants import QUERY_CANCEL_KEY, QUERY_EARLY_CANCEL_KEY",
            "from superset.dataframe import df_to_records",
            "from superset.db_engine_specs import BaseEngineSpec",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorException, SupersetErrorsException",
            "from superset.extensions import celery_app",
            "from superset.models.core import Database",
            "from superset.models.sql_lab import Query",
            "from superset.result_set import SupersetResultSet",
            "from superset.sql_parse import (",
            "    CtasMethod,",
            "    insert_rls_as_subquery,",
            "    insert_rls_in_predicate,",
            "    ParsedQuery,",
            ")",
            "from superset.sqllab.limiting_factor import LimitingFactor",
            "from superset.sqllab.utils import write_ipc_buffer",
            "from superset.utils.celery import session_scope",
            "from superset.utils.core import (",
            "    json_iso_dttm_ser,",
            "    override_user,",
            "    QuerySource,",
            "    zlib_compress,",
            ")",
            "from superset.utils.dates import now_as_float",
            "from superset.utils.decorators import stats_timing",
            "",
            "config = app.config",
            "stats_logger = config[\"STATS_LOGGER\"]",
            "SQLLAB_TIMEOUT = config[\"SQLLAB_ASYNC_TIME_LIMIT_SEC\"]",
            "SQLLAB_HARD_TIMEOUT = SQLLAB_TIMEOUT + 60",
            "SQL_MAX_ROW = config[\"SQL_MAX_ROW\"]",
            "SQLLAB_CTAS_NO_LIMIT = config[\"SQLLAB_CTAS_NO_LIMIT\"]",
            "SQL_QUERY_MUTATOR = config[\"SQL_QUERY_MUTATOR\"]",
            "log_query = config[\"QUERY_LOGGER\"]",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SqlLabException(Exception):",
            "    pass",
            "",
            "",
            "class SqlLabSecurityException(SqlLabException):",
            "    pass",
            "",
            "",
            "class SqlLabQueryStoppedException(SqlLabException):",
            "    pass",
            "",
            "",
            "def handle_query_error(",
            "    ex: Exception,",
            "    query: Query,",
            "    session: Session,",
            "    payload: Optional[dict[str, Any]] = None,",
            "    prefix_message: str = \"\",",
            ") -> dict[str, Any]:",
            "    \"\"\"Local method handling error while processing the SQL\"\"\"",
            "    payload = payload or {}",
            "    msg = f\"{prefix_message} {str(ex)}\".strip()",
            "    query.error_message = msg",
            "    query.tmp_table_name = None",
            "    query.status = QueryStatus.FAILED",
            "    # TODO: re-enable this after updating the frontend to properly display timeout status",
            "    # if query.status != QueryStatus.TIMED_OUT:",
            "    #   query.status = QueryStatus.FAILED",
            "    if not query.end_time:",
            "        query.end_time = now_as_float()",
            "",
            "    # extract DB-specific errors (invalid column, eg)",
            "    if isinstance(ex, SupersetErrorException):",
            "        errors = [ex.error]",
            "    elif isinstance(ex, SupersetErrorsException):",
            "        errors = ex.errors",
            "    else:",
            "        errors = query.database.db_engine_spec.extract_errors(str(ex))",
            "",
            "    errors_payload = [dataclasses.asdict(error) for error in errors]",
            "    if errors:",
            "        query.set_extra_json_key(\"errors\", errors_payload)",
            "",
            "    session.commit()",
            "    payload.update({\"status\": query.status, \"error\": msg, \"errors\": errors_payload})",
            "    if troubleshooting_link := config[\"TROUBLESHOOTING_LINK\"]:",
            "        payload[\"link\"] = troubleshooting_link",
            "    return payload",
            "",
            "",
            "def get_query_backoff_handler(details: dict[Any, Any]) -> None:",
            "    query_id = details[\"kwargs\"][\"query_id\"]",
            "    logger.error(",
            "        \"Query with id `%s` could not be retrieved\", str(query_id), exc_info=True",
            "    )",
            "    stats_logger.incr(f\"error_attempting_orm_query_{details['tries'] - 1}\")",
            "    logger.error(",
            "        \"Query %s: Sleeping for a sec before retrying...\", str(query_id), exc_info=True",
            "    )",
            "",
            "",
            "def get_query_giveup_handler(_: Any) -> None:",
            "    stats_logger.incr(\"error_failed_at_getting_orm_query\")",
            "",
            "",
            "@backoff.on_exception(",
            "    backoff.constant,",
            "    SqlLabException,",
            "    interval=1,",
            "    on_backoff=get_query_backoff_handler,",
            "    on_giveup=get_query_giveup_handler,",
            "    max_tries=5,",
            ")",
            "def get_query(query_id: int, session: Session) -> Query:",
            "    \"\"\"attempts to get the query and retry if it cannot\"\"\"",
            "    try:",
            "        return session.query(Query).filter_by(id=query_id).one()",
            "    except Exception as ex:",
            "        raise SqlLabException(\"Failed at getting query\") from ex",
            "",
            "",
            "@celery_app.task(",
            "    name=\"sql_lab.get_sql_results\",",
            "    bind=True,",
            "    time_limit=SQLLAB_HARD_TIMEOUT,",
            "    soft_time_limit=SQLLAB_TIMEOUT,",
            ")",
            "def get_sql_results(  # pylint: disable=too-many-arguments",
            "    ctask: Task,",
            "    query_id: int,",
            "    rendered_query: str,",
            "    return_results: bool = True,",
            "    store_results: bool = False,",
            "    username: Optional[str] = None,",
            "    start_time: Optional[float] = None,",
            "    expand_data: bool = False,",
            "    log_params: Optional[dict[str, Any]] = None,",
            ") -> Optional[dict[str, Any]]:",
            "    \"\"\"Executes the sql query returns the results.\"\"\"",
            "    with session_scope(not ctask.request.called_directly) as session:",
            "        with override_user(security_manager.find_user(username)):",
            "            try:",
            "                return execute_sql_statements(",
            "                    query_id,",
            "                    rendered_query,",
            "                    return_results,",
            "                    store_results,",
            "                    session=session,",
            "                    start_time=start_time,",
            "                    expand_data=expand_data,",
            "                    log_params=log_params,",
            "                )",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                logger.debug(\"Query %d: %s\", query_id, ex)",
            "                stats_logger.incr(\"error_sqllab_unhandled\")",
            "                query = get_query(query_id, session)",
            "                return handle_query_error(ex, query, session)",
            "",
            "",
            "def execute_sql_statement(  # pylint: disable=too-many-arguments, too-many-locals",
            "    sql_statement: str,",
            "    query: Query,",
            "    session: Session,",
            "    cursor: Any,",
            "    log_params: Optional[dict[str, Any]],",
            "    apply_ctas: bool = False,",
            ") -> SupersetResultSet:",
            "    \"\"\"Executes a single SQL statement\"\"\"",
            "    database: Database = query.database",
            "    db_engine_spec = database.db_engine_spec",
            "",
            "    parsed_query = ParsedQuery(sql_statement)",
            "    if is_feature_enabled(\"RLS_IN_SQLLAB\"):",
            "        # There are two ways to insert RLS: either replacing the table with a subquery",
            "        # that has the RLS, or appending the RLS to the ``WHERE`` clause. The former is",
            "        # safer, but not supported in all databases.",
            "        insert_rls = (",
            "            insert_rls_as_subquery",
            "            if database.db_engine_spec.allows_subqueries",
            "            and database.db_engine_spec.allows_alias_in_select",
            "            else insert_rls_in_predicate",
            "        )",
            "",
            "        # Insert any applicable RLS predicates",
            "        parsed_query = ParsedQuery(",
            "            str(",
            "                insert_rls(",
            "                    parsed_query._parsed[0],  # pylint: disable=protected-access",
            "                    database.id,",
            "                    query.schema,",
            "                )",
            "            )",
            "        )",
            "",
            "    sql = parsed_query.stripped()",
            "",
            "    # This is a test to see if the query is being",
            "    # limited by either the dropdown or the sql.",
            "    # We are testing to see if more rows exist than the limit.",
            "    increased_limit = None if query.limit is None else query.limit + 1",
            "",
            "    if not db_engine_spec.is_readonly_query(parsed_query) and not database.allow_dml:",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(\"Only SELECT statements are allowed against this database.\"),",
            "                error_type=SupersetErrorType.DML_NOT_ALLOWED_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    if apply_ctas:",
            "        if not query.tmp_table_name:",
            "            start_dttm = datetime.fromtimestamp(query.start_time)",
            "            query.tmp_table_name = (",
            "                f'tmp_{query.user_id}_table_{start_dttm.strftime(\"%Y_%m_%d_%H_%M_%S\")}'",
            "            )",
            "        sql = parsed_query.as_create_table(",
            "            query.tmp_table_name,",
            "            schema_name=query.tmp_schema_name,",
            "            method=query.ctas_method,",
            "        )",
            "        query.select_as_cta_used = True",
            "",
            "    # Do not apply limit to the CTA queries when SQLLAB_CTAS_NO_LIMIT is set to true",
            "    if db_engine_spec.is_select_query(parsed_query) and not (",
            "        query.select_as_cta_used and SQLLAB_CTAS_NO_LIMIT",
            "    ):",
            "        if SQL_MAX_ROW and (not query.limit or query.limit > SQL_MAX_ROW):",
            "            query.limit = SQL_MAX_ROW",
            "        sql = apply_limit_if_exists(database, increased_limit, query, sql)",
            "",
            "    # Hook to allow environment-specific mutation (usually comments) to the SQL",
            "    sql = SQL_QUERY_MUTATOR(",
            "        sql,",
            "        security_manager=security_manager,",
            "        database=database,",
            "    )",
            "    try:",
            "        query.executed_sql = sql",
            "        if log_query:",
            "            log_query(",
            "                query.database.sqlalchemy_uri,",
            "                query.executed_sql,",
            "                query.schema,",
            "                __name__,",
            "                security_manager,",
            "                log_params,",
            "            )",
            "        session.commit()",
            "        with stats_timing(\"sqllab.query.time_executing_query\", stats_logger):",
            "            db_engine_spec.execute_with_cursor(cursor, sql, query, session)",
            "",
            "        with stats_timing(\"sqllab.query.time_fetching_results\", stats_logger):",
            "            logger.debug(",
            "                \"Query %d: Fetching data for query object: %s\",",
            "                query.id,",
            "                str(query.to_dict()),",
            "            )",
            "            data = db_engine_spec.fetch_data(cursor, increased_limit)",
            "            if query.limit is None or len(data) <= query.limit:",
            "                query.limiting_factor = LimitingFactor.NOT_LIMITED",
            "            else:",
            "                # return 1 row less than increased_query",
            "                data = data[:-1]",
            "    except SoftTimeLimitExceeded as ex:",
            "        query.status = QueryStatus.TIMED_OUT",
            "",
            "        logger.warning(\"Query %d: Time limit exceeded\", query.id)",
            "        logger.debug(\"Query %d: %s\", query.id, ex)",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(",
            "                    \"The query was killed after %(sqllab_timeout)s seconds. It might \"",
            "                    \"be too complex, or the database might be under heavy load.\",",
            "                    sqllab_timeout=SQLLAB_TIMEOUT,",
            "                ),",
            "                error_type=SupersetErrorType.SQLLAB_TIMEOUT_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        ) from ex",
            "    except Exception as ex:",
            "        # query is stopped in another thread/worker",
            "        # stopping raises expected exceptions which we should skip",
            "        session.refresh(query)",
            "        if query.status == QueryStatus.STOPPED:",
            "            raise SqlLabQueryStoppedException() from ex",
            "",
            "        logger.debug(\"Query %d: %s\", query.id, ex)",
            "        raise SqlLabException(db_engine_spec.extract_error_message(ex)) from ex",
            "",
            "    logger.debug(\"Query %d: Fetching cursor description\", query.id)",
            "    cursor_description = cursor.description",
            "    return SupersetResultSet(data, cursor_description, db_engine_spec)",
            "",
            "",
            "def apply_limit_if_exists(",
            "    database: Database, increased_limit: Optional[int], query: Query, sql: str",
            ") -> str:",
            "    if query.limit and increased_limit:",
            "        # We are fetching one more than the requested limit in order",
            "        # to test whether there are more rows than the limit. According to the DB",
            "        # Engine support it will choose top or limit parse",
            "        # Later, the extra row will be dropped before sending",
            "        # the results back to the user.",
            "        sql = database.apply_limit_to_sql(sql, increased_limit, force=True)",
            "    return sql",
            "",
            "",
            "def _serialize_payload(",
            "    payload: dict[Any, Any], use_msgpack: Optional[bool] = False",
            ") -> Union[bytes, str]:",
            "    logger.debug(\"Serializing to msgpack: %r\", use_msgpack)",
            "    if use_msgpack:",
            "        return msgpack.dumps(payload, default=json_iso_dttm_ser, use_bin_type=True)",
            "",
            "    return json.dumps(payload, default=json_iso_dttm_ser, ignore_nan=True)",
            "",
            "",
            "def _serialize_and_expand_data(",
            "    result_set: SupersetResultSet,",
            "    db_engine_spec: BaseEngineSpec,",
            "    use_msgpack: Optional[bool] = False,",
            "    expand_data: bool = False,",
            ") -> tuple[Union[bytes, str], list[Any], list[Any], list[Any]]:",
            "    selected_columns = result_set.columns",
            "    all_columns: list[Any]",
            "    expanded_columns: list[Any]",
            "",
            "    if use_msgpack:",
            "        with stats_timing(",
            "            \"sqllab.query.results_backend_pa_serialization\", stats_logger",
            "        ):",
            "            data = write_ipc_buffer(result_set.pa_table).to_pybytes()",
            "",
            "        # expand when loading data from results backend",
            "        all_columns, expanded_columns = (selected_columns, [])",
            "    else:",
            "        df = result_set.to_pandas_df()",
            "        data = df_to_records(df) or []",
            "",
            "        if expand_data:",
            "            all_columns, data, expanded_columns = db_engine_spec.expand_data(",
            "                selected_columns, data",
            "            )",
            "        else:",
            "            all_columns = selected_columns",
            "            expanded_columns = []",
            "",
            "    return (data, selected_columns, all_columns, expanded_columns)",
            "",
            "",
            "def execute_sql_statements(",
            "    # pylint: disable=too-many-arguments, too-many-locals, too-many-statements, too-many-branches",
            "    query_id: int,",
            "    rendered_query: str,",
            "    return_results: bool,",
            "    store_results: bool,",
            "    session: Session,",
            "    start_time: Optional[float],",
            "    expand_data: bool,",
            "    log_params: Optional[dict[str, Any]],",
            ") -> Optional[dict[str, Any]]:",
            "    \"\"\"Executes the sql query returns the results.\"\"\"",
            "    if store_results and start_time:",
            "        # only asynchronous queries",
            "        stats_logger.timing(\"sqllab.query.time_pending\", now_as_float() - start_time)",
            "",
            "    query = get_query(query_id, session)",
            "    payload: dict[str, Any] = {\"query_id\": query_id}",
            "    database = query.database",
            "    db_engine_spec = database.db_engine_spec",
            "    db_engine_spec.patch()",
            "",
            "    if database.allow_run_async and not results_backend:",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(\"Results backend is not configured.\"),",
            "                error_type=SupersetErrorType.RESULTS_BACKEND_NOT_CONFIGURED_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "",
            "    # Breaking down into multiple statements",
            "    parsed_query = ParsedQuery(rendered_query, strip_comments=True)",
            "    if not db_engine_spec.run_multiple_statements_as_one:",
            "        statements = parsed_query.get_statements()",
            "        logger.info(",
            "            \"Query %s: Executing %i statement(s)\", str(query_id), len(statements)",
            "        )",
            "    else:",
            "        statements = [rendered_query]",
            "        logger.info(\"Query %s: Executing query as a single statement\", str(query_id))",
            "",
            "    logger.info(\"Query %s: Set query to 'running'\", str(query_id))",
            "    query.status = QueryStatus.RUNNING",
            "    query.start_running_time = now_as_float()",
            "    session.commit()",
            "",
            "    # Should we create a table or view from the select?",
            "    if (",
            "        query.select_as_cta",
            "        and query.ctas_method == CtasMethod.TABLE",
            "        and not parsed_query.is_valid_ctas()",
            "    ):",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(",
            "                    \"CTAS (create table as select) can only be run with a query where \"",
            "                    \"the last statement is a SELECT. Please make sure your query has \"",
            "                    \"a SELECT as its last statement. Then, try running your query \"",
            "                    \"again.\"",
            "                ),",
            "                error_type=SupersetErrorType.INVALID_CTAS_QUERY_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    if (",
            "        query.select_as_cta",
            "        and query.ctas_method == CtasMethod.VIEW",
            "        and not parsed_query.is_valid_cvas()",
            "    ):",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(",
            "                    \"CVAS (create view as select) can only be run with a query with \"",
            "                    \"a single SELECT statement. Please make sure your query has only \"",
            "                    \"a SELECT statement. Then, try running your query again.\"",
            "                ),",
            "                error_type=SupersetErrorType.INVALID_CVAS_QUERY_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "",
            "    with database.get_raw_connection(query.schema, source=QuerySource.SQL_LAB) as conn:",
            "        # Sharing a single connection and cursor across the",
            "        # execution of all statements (if many)",
            "        cursor = conn.cursor()",
            "        cancel_query_id = db_engine_spec.get_cancel_query_id(cursor, query)",
            "        if cancel_query_id is not None:",
            "            query.set_extra_json_key(QUERY_CANCEL_KEY, cancel_query_id)",
            "            session.commit()",
            "        statement_count = len(statements)",
            "        for i, statement in enumerate(statements):",
            "            # Check if stopped",
            "            session.refresh(query)",
            "            if query.status == QueryStatus.STOPPED:",
            "                payload.update({\"status\": query.status})",
            "                return payload",
            "            # For CTAS we create the table only on the last statement",
            "            apply_ctas = query.select_as_cta and (",
            "                query.ctas_method == CtasMethod.VIEW",
            "                or (query.ctas_method == CtasMethod.TABLE and i == len(statements) - 1)",
            "            )",
            "            # Run statement",
            "            msg = __(",
            "                \"Running statement %(statement_num)s out of %(statement_count)s\",",
            "                statement_num=i + 1,",
            "                statement_count=statement_count,",
            "            )",
            "            logger.info(\"Query %s: %s\", str(query_id), msg)",
            "            query.set_extra_json_key(\"progress\", msg)",
            "            session.commit()",
            "            try:",
            "                result_set = execute_sql_statement(",
            "                    statement,",
            "                    query,",
            "                    session,",
            "                    cursor,",
            "                    log_params,",
            "                    apply_ctas,",
            "                )",
            "            except SqlLabQueryStoppedException:",
            "                payload.update({\"status\": QueryStatus.STOPPED})",
            "                return payload",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                msg = str(ex)",
            "                prefix_message = (",
            "                    __(",
            "                        \"Statement %(statement_num)s out of %(statement_count)s\",",
            "                        statement_num=i + 1,",
            "                        statement_count=statement_count,",
            "                    )",
            "                    if statement_count > 1",
            "                    else \"\"",
            "                )",
            "                payload = handle_query_error(",
            "                    ex, query, session, payload, prefix_message",
            "                )",
            "                return payload",
            "",
            "        # Commit the connection so CTA queries will create the table and any DML.",
            "        should_commit = (",
            "            not db_engine_spec.is_select_query(parsed_query)  # check if query is DML",
            "            or apply_ctas",
            "        )",
            "        if should_commit:",
            "            conn.commit()",
            "",
            "    # Success, updating the query entry in database",
            "    query.rows = result_set.size",
            "    query.progress = 100",
            "    query.set_extra_json_key(\"progress\", None)",
            "    query.set_extra_json_key(\"columns\", result_set.columns)",
            "    if query.select_as_cta:",
            "        query.select_sql = database.select_star(",
            "            query.tmp_table_name,",
            "            schema=query.tmp_schema_name,",
            "            limit=query.limit,",
            "            show_cols=False,",
            "            latest_partition=False,",
            "        )",
            "    query.end_time = now_as_float()",
            "",
            "    use_arrow_data = store_results and cast(bool, results_backend_use_msgpack)",
            "    data, selected_columns, all_columns, expanded_columns = _serialize_and_expand_data(",
            "        result_set, db_engine_spec, use_arrow_data, expand_data",
            "    )",
            "",
            "    # TODO: data should be saved separately from metadata (likely in Parquet)",
            "    payload.update(",
            "        {",
            "            \"status\": QueryStatus.SUCCESS,",
            "            \"data\": data,",
            "            \"columns\": all_columns,",
            "            \"selected_columns\": selected_columns,",
            "            \"expanded_columns\": expanded_columns,",
            "            \"query\": query.to_dict(),",
            "        }",
            "    )",
            "    payload[\"query\"][\"state\"] = QueryStatus.SUCCESS",
            "",
            "    if store_results and results_backend:",
            "        key = str(uuid.uuid4())",
            "        payload[\"query\"][\"resultsKey\"] = key",
            "        logger.info(",
            "            \"Query %s: Storing results in results backend, key: %s\", str(query_id), key",
            "        )",
            "        with stats_timing(\"sqllab.query.results_backend_write\", stats_logger):",
            "            with stats_timing(",
            "                \"sqllab.query.results_backend_write_serialization\", stats_logger",
            "            ):",
            "                serialized_payload = _serialize_payload(",
            "                    payload, cast(bool, results_backend_use_msgpack)",
            "                )",
            "            cache_timeout = database.cache_timeout",
            "            if cache_timeout is None:",
            "                cache_timeout = config[\"CACHE_DEFAULT_TIMEOUT\"]",
            "",
            "            compressed = zlib_compress(serialized_payload)",
            "            logger.debug(",
            "                \"*** serialized payload size: %i\", getsizeof(serialized_payload)",
            "            )",
            "            logger.debug(\"*** compressed payload size: %i\", getsizeof(compressed))",
            "            results_backend.set(key, compressed, cache_timeout)",
            "        query.results_key = key",
            "",
            "    query.status = QueryStatus.SUCCESS",
            "    session.commit()",
            "",
            "    if return_results:",
            "        # since we're returning results we need to create non-arrow data",
            "        if use_arrow_data:",
            "            (",
            "                data,",
            "                selected_columns,",
            "                all_columns,",
            "                expanded_columns,",
            "            ) = _serialize_and_expand_data(",
            "                result_set, db_engine_spec, False, expand_data",
            "            )",
            "            payload.update(",
            "                {",
            "                    \"data\": data,",
            "                    \"columns\": all_columns,",
            "                    \"selected_columns\": selected_columns,",
            "                    \"expanded_columns\": expanded_columns,",
            "                }",
            "            )",
            "        return payload",
            "",
            "    return None",
            "",
            "",
            "def cancel_query(query: Query) -> bool:",
            "    \"\"\"",
            "    Cancel a running query.",
            "",
            "    Note some engines implicitly handle the cancelation of a query and thus no explicit",
            "    action is required.",
            "",
            "    :param query: Query to cancel",
            "    :return: True if query cancelled successfully, False otherwise",
            "    \"\"\"",
            "",
            "    if query.database.db_engine_spec.has_implicit_cancel():",
            "        return True",
            "",
            "    # Some databases may need to make preparations for query cancellation",
            "    query.database.db_engine_spec.prepare_cancel_query(query, db.session)",
            "",
            "    if query.extra.get(QUERY_EARLY_CANCEL_KEY):",
            "        # Query has been cancelled prior to being able to set the cancel key.",
            "        # This can happen if the query cancellation key can only be acquired after the",
            "        # query has been executed",
            "        return True",
            "",
            "    cancel_query_id = query.extra.get(QUERY_CANCEL_KEY)",
            "    if cancel_query_id is None:",
            "        return False",
            "",
            "    with query.database.get_sqla_engine_with_context(",
            "        query.schema, source=QuerySource.SQL_LAB",
            "    ) as engine:",
            "        with closing(engine.raw_connection()) as conn:",
            "            with closing(conn.cursor()) as cursor:",
            "                return query.database.db_engine_spec.cancel_query(",
            "                    cursor, query, cancel_query_id",
            "                )"
        ],
        "afterPatchFile": [
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "import dataclasses",
            "import logging",
            "import uuid",
            "from contextlib import closing",
            "from datetime import datetime",
            "from sys import getsizeof",
            "from typing import Any, cast, Optional, Union",
            "",
            "import backoff",
            "import msgpack",
            "import simplejson as json",
            "from celery import Task",
            "from celery.exceptions import SoftTimeLimitExceeded",
            "from flask_babel import gettext as __",
            "from sqlalchemy.orm import Session",
            "",
            "from superset import (",
            "    app,",
            "    db,",
            "    is_feature_enabled,",
            "    results_backend,",
            "    results_backend_use_msgpack,",
            "    security_manager,",
            ")",
            "from superset.common.db_query_status import QueryStatus",
            "from superset.constants import QUERY_CANCEL_KEY, QUERY_EARLY_CANCEL_KEY",
            "from superset.dataframe import df_to_records",
            "from superset.db_engine_specs import BaseEngineSpec",
            "from superset.errors import ErrorLevel, SupersetError, SupersetErrorType",
            "from superset.exceptions import SupersetErrorException, SupersetErrorsException",
            "from superset.extensions import celery_app",
            "from superset.models.core import Database",
            "from superset.models.sql_lab import Query",
            "from superset.result_set import SupersetResultSet",
            "from superset.sql_parse import (",
            "    CtasMethod,",
            "    insert_rls_as_subquery,",
            "    insert_rls_in_predicate,",
            "    ParsedQuery,",
            ")",
            "from superset.sqllab.limiting_factor import LimitingFactor",
            "from superset.sqllab.utils import write_ipc_buffer",
            "from superset.utils.celery import session_scope",
            "from superset.utils.core import (",
            "    json_iso_dttm_ser,",
            "    override_user,",
            "    QuerySource,",
            "    zlib_compress,",
            ")",
            "from superset.utils.dates import now_as_float",
            "from superset.utils.decorators import stats_timing",
            "",
            "config = app.config",
            "stats_logger = config[\"STATS_LOGGER\"]",
            "SQLLAB_TIMEOUT = config[\"SQLLAB_ASYNC_TIME_LIMIT_SEC\"]",
            "SQLLAB_HARD_TIMEOUT = SQLLAB_TIMEOUT + 60",
            "SQL_MAX_ROW = config[\"SQL_MAX_ROW\"]",
            "SQLLAB_CTAS_NO_LIMIT = config[\"SQLLAB_CTAS_NO_LIMIT\"]",
            "SQL_QUERY_MUTATOR = config[\"SQL_QUERY_MUTATOR\"]",
            "log_query = config[\"QUERY_LOGGER\"]",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SqlLabException(Exception):",
            "    pass",
            "",
            "",
            "class SqlLabSecurityException(SqlLabException):",
            "    pass",
            "",
            "",
            "class SqlLabQueryStoppedException(SqlLabException):",
            "    pass",
            "",
            "",
            "def handle_query_error(",
            "    ex: Exception,",
            "    query: Query,",
            "    session: Session,",
            "    payload: Optional[dict[str, Any]] = None,",
            "    prefix_message: str = \"\",",
            ") -> dict[str, Any]:",
            "    \"\"\"Local method handling error while processing the SQL\"\"\"",
            "    payload = payload or {}",
            "    msg = f\"{prefix_message} {str(ex)}\".strip()",
            "    query.error_message = msg",
            "    query.tmp_table_name = None",
            "    query.status = QueryStatus.FAILED",
            "    # TODO: re-enable this after updating the frontend to properly display timeout status",
            "    # if query.status != QueryStatus.TIMED_OUT:",
            "    #   query.status = QueryStatus.FAILED",
            "    if not query.end_time:",
            "        query.end_time = now_as_float()",
            "",
            "    # extract DB-specific errors (invalid column, eg)",
            "    if isinstance(ex, SupersetErrorException):",
            "        errors = [ex.error]",
            "    elif isinstance(ex, SupersetErrorsException):",
            "        errors = ex.errors",
            "    else:",
            "        errors = query.database.db_engine_spec.extract_errors(str(ex))",
            "",
            "    errors_payload = [dataclasses.asdict(error) for error in errors]",
            "    if errors:",
            "        query.set_extra_json_key(\"errors\", errors_payload)",
            "",
            "    session.commit()",
            "    payload.update({\"status\": query.status, \"error\": msg, \"errors\": errors_payload})",
            "    if troubleshooting_link := config[\"TROUBLESHOOTING_LINK\"]:",
            "        payload[\"link\"] = troubleshooting_link",
            "    return payload",
            "",
            "",
            "def get_query_backoff_handler(details: dict[Any, Any]) -> None:",
            "    query_id = details[\"kwargs\"][\"query_id\"]",
            "    logger.error(",
            "        \"Query with id `%s` could not be retrieved\", str(query_id), exc_info=True",
            "    )",
            "    stats_logger.incr(f\"error_attempting_orm_query_{details['tries'] - 1}\")",
            "    logger.error(",
            "        \"Query %s: Sleeping for a sec before retrying...\", str(query_id), exc_info=True",
            "    )",
            "",
            "",
            "def get_query_giveup_handler(_: Any) -> None:",
            "    stats_logger.incr(\"error_failed_at_getting_orm_query\")",
            "",
            "",
            "@backoff.on_exception(",
            "    backoff.constant,",
            "    SqlLabException,",
            "    interval=1,",
            "    on_backoff=get_query_backoff_handler,",
            "    on_giveup=get_query_giveup_handler,",
            "    max_tries=5,",
            ")",
            "def get_query(query_id: int, session: Session) -> Query:",
            "    \"\"\"attempts to get the query and retry if it cannot\"\"\"",
            "    try:",
            "        return session.query(Query).filter_by(id=query_id).one()",
            "    except Exception as ex:",
            "        raise SqlLabException(\"Failed at getting query\") from ex",
            "",
            "",
            "@celery_app.task(",
            "    name=\"sql_lab.get_sql_results\",",
            "    bind=True,",
            "    time_limit=SQLLAB_HARD_TIMEOUT,",
            "    soft_time_limit=SQLLAB_TIMEOUT,",
            ")",
            "def get_sql_results(  # pylint: disable=too-many-arguments",
            "    ctask: Task,",
            "    query_id: int,",
            "    rendered_query: str,",
            "    return_results: bool = True,",
            "    store_results: bool = False,",
            "    username: Optional[str] = None,",
            "    start_time: Optional[float] = None,",
            "    expand_data: bool = False,",
            "    log_params: Optional[dict[str, Any]] = None,",
            ") -> Optional[dict[str, Any]]:",
            "    \"\"\"Executes the sql query returns the results.\"\"\"",
            "    with session_scope(not ctask.request.called_directly) as session:",
            "        with override_user(security_manager.find_user(username)):",
            "            try:",
            "                return execute_sql_statements(",
            "                    query_id,",
            "                    rendered_query,",
            "                    return_results,",
            "                    store_results,",
            "                    session=session,",
            "                    start_time=start_time,",
            "                    expand_data=expand_data,",
            "                    log_params=log_params,",
            "                )",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                logger.debug(\"Query %d: %s\", query_id, ex)",
            "                stats_logger.incr(\"error_sqllab_unhandled\")",
            "                query = get_query(query_id, session)",
            "                return handle_query_error(ex, query, session)",
            "",
            "",
            "def execute_sql_statement(  # pylint: disable=too-many-arguments, too-many-locals",
            "    sql_statement: str,",
            "    query: Query,",
            "    session: Session,",
            "    cursor: Any,",
            "    log_params: Optional[dict[str, Any]],",
            "    apply_ctas: bool = False,",
            ") -> SupersetResultSet:",
            "    \"\"\"Executes a single SQL statement\"\"\"",
            "    database: Database = query.database",
            "    db_engine_spec = database.db_engine_spec",
            "",
            "    parsed_query = ParsedQuery(sql_statement, engine=db_engine_spec.engine)",
            "    if is_feature_enabled(\"RLS_IN_SQLLAB\"):",
            "        # There are two ways to insert RLS: either replacing the table with a subquery",
            "        # that has the RLS, or appending the RLS to the ``WHERE`` clause. The former is",
            "        # safer, but not supported in all databases.",
            "        insert_rls = (",
            "            insert_rls_as_subquery",
            "            if database.db_engine_spec.allows_subqueries",
            "            and database.db_engine_spec.allows_alias_in_select",
            "            else insert_rls_in_predicate",
            "        )",
            "",
            "        # Insert any applicable RLS predicates",
            "        parsed_query = ParsedQuery(",
            "            str(",
            "                insert_rls(",
            "                    parsed_query._parsed[0],  # pylint: disable=protected-access",
            "                    database.id,",
            "                    query.schema,",
            "                )",
            "            ),",
            "            engine=db_engine_spec.engine,",
            "        )",
            "",
            "    sql = parsed_query.stripped()",
            "",
            "    # This is a test to see if the query is being",
            "    # limited by either the dropdown or the sql.",
            "    # We are testing to see if more rows exist than the limit.",
            "    increased_limit = None if query.limit is None else query.limit + 1",
            "",
            "    if not db_engine_spec.is_readonly_query(parsed_query) and not database.allow_dml:",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(\"Only SELECT statements are allowed against this database.\"),",
            "                error_type=SupersetErrorType.DML_NOT_ALLOWED_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    if apply_ctas:",
            "        if not query.tmp_table_name:",
            "            start_dttm = datetime.fromtimestamp(query.start_time)",
            "            query.tmp_table_name = (",
            "                f'tmp_{query.user_id}_table_{start_dttm.strftime(\"%Y_%m_%d_%H_%M_%S\")}'",
            "            )",
            "        sql = parsed_query.as_create_table(",
            "            query.tmp_table_name,",
            "            schema_name=query.tmp_schema_name,",
            "            method=query.ctas_method,",
            "        )",
            "        query.select_as_cta_used = True",
            "",
            "    # Do not apply limit to the CTA queries when SQLLAB_CTAS_NO_LIMIT is set to true",
            "    if db_engine_spec.is_select_query(parsed_query) and not (",
            "        query.select_as_cta_used and SQLLAB_CTAS_NO_LIMIT",
            "    ):",
            "        if SQL_MAX_ROW and (not query.limit or query.limit > SQL_MAX_ROW):",
            "            query.limit = SQL_MAX_ROW",
            "        sql = apply_limit_if_exists(database, increased_limit, query, sql)",
            "",
            "    # Hook to allow environment-specific mutation (usually comments) to the SQL",
            "    sql = SQL_QUERY_MUTATOR(",
            "        sql,",
            "        security_manager=security_manager,",
            "        database=database,",
            "    )",
            "    try:",
            "        query.executed_sql = sql",
            "        if log_query:",
            "            log_query(",
            "                query.database.sqlalchemy_uri,",
            "                query.executed_sql,",
            "                query.schema,",
            "                __name__,",
            "                security_manager,",
            "                log_params,",
            "            )",
            "        session.commit()",
            "        with stats_timing(\"sqllab.query.time_executing_query\", stats_logger):",
            "            db_engine_spec.execute_with_cursor(cursor, sql, query, session)",
            "",
            "        with stats_timing(\"sqllab.query.time_fetching_results\", stats_logger):",
            "            logger.debug(",
            "                \"Query %d: Fetching data for query object: %s\",",
            "                query.id,",
            "                str(query.to_dict()),",
            "            )",
            "            data = db_engine_spec.fetch_data(cursor, increased_limit)",
            "            if query.limit is None or len(data) <= query.limit:",
            "                query.limiting_factor = LimitingFactor.NOT_LIMITED",
            "            else:",
            "                # return 1 row less than increased_query",
            "                data = data[:-1]",
            "    except SoftTimeLimitExceeded as ex:",
            "        query.status = QueryStatus.TIMED_OUT",
            "",
            "        logger.warning(\"Query %d: Time limit exceeded\", query.id)",
            "        logger.debug(\"Query %d: %s\", query.id, ex)",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(",
            "                    \"The query was killed after %(sqllab_timeout)s seconds. It might \"",
            "                    \"be too complex, or the database might be under heavy load.\",",
            "                    sqllab_timeout=SQLLAB_TIMEOUT,",
            "                ),",
            "                error_type=SupersetErrorType.SQLLAB_TIMEOUT_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        ) from ex",
            "    except Exception as ex:",
            "        # query is stopped in another thread/worker",
            "        # stopping raises expected exceptions which we should skip",
            "        session.refresh(query)",
            "        if query.status == QueryStatus.STOPPED:",
            "            raise SqlLabQueryStoppedException() from ex",
            "",
            "        logger.debug(\"Query %d: %s\", query.id, ex)",
            "        raise SqlLabException(db_engine_spec.extract_error_message(ex)) from ex",
            "",
            "    logger.debug(\"Query %d: Fetching cursor description\", query.id)",
            "    cursor_description = cursor.description",
            "    return SupersetResultSet(data, cursor_description, db_engine_spec)",
            "",
            "",
            "def apply_limit_if_exists(",
            "    database: Database, increased_limit: Optional[int], query: Query, sql: str",
            ") -> str:",
            "    if query.limit and increased_limit:",
            "        # We are fetching one more than the requested limit in order",
            "        # to test whether there are more rows than the limit. According to the DB",
            "        # Engine support it will choose top or limit parse",
            "        # Later, the extra row will be dropped before sending",
            "        # the results back to the user.",
            "        sql = database.apply_limit_to_sql(sql, increased_limit, force=True)",
            "    return sql",
            "",
            "",
            "def _serialize_payload(",
            "    payload: dict[Any, Any], use_msgpack: Optional[bool] = False",
            ") -> Union[bytes, str]:",
            "    logger.debug(\"Serializing to msgpack: %r\", use_msgpack)",
            "    if use_msgpack:",
            "        return msgpack.dumps(payload, default=json_iso_dttm_ser, use_bin_type=True)",
            "",
            "    return json.dumps(payload, default=json_iso_dttm_ser, ignore_nan=True)",
            "",
            "",
            "def _serialize_and_expand_data(",
            "    result_set: SupersetResultSet,",
            "    db_engine_spec: BaseEngineSpec,",
            "    use_msgpack: Optional[bool] = False,",
            "    expand_data: bool = False,",
            ") -> tuple[Union[bytes, str], list[Any], list[Any], list[Any]]:",
            "    selected_columns = result_set.columns",
            "    all_columns: list[Any]",
            "    expanded_columns: list[Any]",
            "",
            "    if use_msgpack:",
            "        with stats_timing(",
            "            \"sqllab.query.results_backend_pa_serialization\", stats_logger",
            "        ):",
            "            data = write_ipc_buffer(result_set.pa_table).to_pybytes()",
            "",
            "        # expand when loading data from results backend",
            "        all_columns, expanded_columns = (selected_columns, [])",
            "    else:",
            "        df = result_set.to_pandas_df()",
            "        data = df_to_records(df) or []",
            "",
            "        if expand_data:",
            "            all_columns, data, expanded_columns = db_engine_spec.expand_data(",
            "                selected_columns, data",
            "            )",
            "        else:",
            "            all_columns = selected_columns",
            "            expanded_columns = []",
            "",
            "    return (data, selected_columns, all_columns, expanded_columns)",
            "",
            "",
            "def execute_sql_statements(",
            "    # pylint: disable=too-many-arguments, too-many-locals, too-many-statements, too-many-branches",
            "    query_id: int,",
            "    rendered_query: str,",
            "    return_results: bool,",
            "    store_results: bool,",
            "    session: Session,",
            "    start_time: Optional[float],",
            "    expand_data: bool,",
            "    log_params: Optional[dict[str, Any]],",
            ") -> Optional[dict[str, Any]]:",
            "    \"\"\"Executes the sql query returns the results.\"\"\"",
            "    if store_results and start_time:",
            "        # only asynchronous queries",
            "        stats_logger.timing(\"sqllab.query.time_pending\", now_as_float() - start_time)",
            "",
            "    query = get_query(query_id, session)",
            "    payload: dict[str, Any] = {\"query_id\": query_id}",
            "    database = query.database",
            "    db_engine_spec = database.db_engine_spec",
            "    db_engine_spec.patch()",
            "",
            "    if database.allow_run_async and not results_backend:",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(\"Results backend is not configured.\"),",
            "                error_type=SupersetErrorType.RESULTS_BACKEND_NOT_CONFIGURED_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "",
            "    # Breaking down into multiple statements",
            "    parsed_query = ParsedQuery(",
            "        rendered_query,",
            "        strip_comments=True,",
            "        engine=db_engine_spec.engine,",
            "    )",
            "    if not db_engine_spec.run_multiple_statements_as_one:",
            "        statements = parsed_query.get_statements()",
            "        logger.info(",
            "            \"Query %s: Executing %i statement(s)\", str(query_id), len(statements)",
            "        )",
            "    else:",
            "        statements = [rendered_query]",
            "        logger.info(\"Query %s: Executing query as a single statement\", str(query_id))",
            "",
            "    logger.info(\"Query %s: Set query to 'running'\", str(query_id))",
            "    query.status = QueryStatus.RUNNING",
            "    query.start_running_time = now_as_float()",
            "    session.commit()",
            "",
            "    # Should we create a table or view from the select?",
            "    if (",
            "        query.select_as_cta",
            "        and query.ctas_method == CtasMethod.TABLE",
            "        and not parsed_query.is_valid_ctas()",
            "    ):",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(",
            "                    \"CTAS (create table as select) can only be run with a query where \"",
            "                    \"the last statement is a SELECT. Please make sure your query has \"",
            "                    \"a SELECT as its last statement. Then, try running your query \"",
            "                    \"again.\"",
            "                ),",
            "                error_type=SupersetErrorType.INVALID_CTAS_QUERY_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "    if (",
            "        query.select_as_cta",
            "        and query.ctas_method == CtasMethod.VIEW",
            "        and not parsed_query.is_valid_cvas()",
            "    ):",
            "        raise SupersetErrorException(",
            "            SupersetError(",
            "                message=__(",
            "                    \"CVAS (create view as select) can only be run with a query with \"",
            "                    \"a single SELECT statement. Please make sure your query has only \"",
            "                    \"a SELECT statement. Then, try running your query again.\"",
            "                ),",
            "                error_type=SupersetErrorType.INVALID_CVAS_QUERY_ERROR,",
            "                level=ErrorLevel.ERROR,",
            "            )",
            "        )",
            "",
            "    with database.get_raw_connection(query.schema, source=QuerySource.SQL_LAB) as conn:",
            "        # Sharing a single connection and cursor across the",
            "        # execution of all statements (if many)",
            "        cursor = conn.cursor()",
            "        cancel_query_id = db_engine_spec.get_cancel_query_id(cursor, query)",
            "        if cancel_query_id is not None:",
            "            query.set_extra_json_key(QUERY_CANCEL_KEY, cancel_query_id)",
            "            session.commit()",
            "        statement_count = len(statements)",
            "        for i, statement in enumerate(statements):",
            "            # Check if stopped",
            "            session.refresh(query)",
            "            if query.status == QueryStatus.STOPPED:",
            "                payload.update({\"status\": query.status})",
            "                return payload",
            "            # For CTAS we create the table only on the last statement",
            "            apply_ctas = query.select_as_cta and (",
            "                query.ctas_method == CtasMethod.VIEW",
            "                or (query.ctas_method == CtasMethod.TABLE and i == len(statements) - 1)",
            "            )",
            "            # Run statement",
            "            msg = __(",
            "                \"Running statement %(statement_num)s out of %(statement_count)s\",",
            "                statement_num=i + 1,",
            "                statement_count=statement_count,",
            "            )",
            "            logger.info(\"Query %s: %s\", str(query_id), msg)",
            "            query.set_extra_json_key(\"progress\", msg)",
            "            session.commit()",
            "            try:",
            "                result_set = execute_sql_statement(",
            "                    statement,",
            "                    query,",
            "                    session,",
            "                    cursor,",
            "                    log_params,",
            "                    apply_ctas,",
            "                )",
            "            except SqlLabQueryStoppedException:",
            "                payload.update({\"status\": QueryStatus.STOPPED})",
            "                return payload",
            "            except Exception as ex:  # pylint: disable=broad-except",
            "                msg = str(ex)",
            "                prefix_message = (",
            "                    __(",
            "                        \"Statement %(statement_num)s out of %(statement_count)s\",",
            "                        statement_num=i + 1,",
            "                        statement_count=statement_count,",
            "                    )",
            "                    if statement_count > 1",
            "                    else \"\"",
            "                )",
            "                payload = handle_query_error(",
            "                    ex, query, session, payload, prefix_message",
            "                )",
            "                return payload",
            "",
            "        # Commit the connection so CTA queries will create the table and any DML.",
            "        should_commit = (",
            "            not db_engine_spec.is_select_query(parsed_query)  # check if query is DML",
            "            or apply_ctas",
            "        )",
            "        if should_commit:",
            "            conn.commit()",
            "",
            "    # Success, updating the query entry in database",
            "    query.rows = result_set.size",
            "    query.progress = 100",
            "    query.set_extra_json_key(\"progress\", None)",
            "    query.set_extra_json_key(\"columns\", result_set.columns)",
            "    if query.select_as_cta:",
            "        query.select_sql = database.select_star(",
            "            query.tmp_table_name,",
            "            schema=query.tmp_schema_name,",
            "            limit=query.limit,",
            "            show_cols=False,",
            "            latest_partition=False,",
            "        )",
            "    query.end_time = now_as_float()",
            "",
            "    use_arrow_data = store_results and cast(bool, results_backend_use_msgpack)",
            "    data, selected_columns, all_columns, expanded_columns = _serialize_and_expand_data(",
            "        result_set, db_engine_spec, use_arrow_data, expand_data",
            "    )",
            "",
            "    # TODO: data should be saved separately from metadata (likely in Parquet)",
            "    payload.update(",
            "        {",
            "            \"status\": QueryStatus.SUCCESS,",
            "            \"data\": data,",
            "            \"columns\": all_columns,",
            "            \"selected_columns\": selected_columns,",
            "            \"expanded_columns\": expanded_columns,",
            "            \"query\": query.to_dict(),",
            "        }",
            "    )",
            "    payload[\"query\"][\"state\"] = QueryStatus.SUCCESS",
            "",
            "    if store_results and results_backend:",
            "        key = str(uuid.uuid4())",
            "        payload[\"query\"][\"resultsKey\"] = key",
            "        logger.info(",
            "            \"Query %s: Storing results in results backend, key: %s\", str(query_id), key",
            "        )",
            "        with stats_timing(\"sqllab.query.results_backend_write\", stats_logger):",
            "            with stats_timing(",
            "                \"sqllab.query.results_backend_write_serialization\", stats_logger",
            "            ):",
            "                serialized_payload = _serialize_payload(",
            "                    payload, cast(bool, results_backend_use_msgpack)",
            "                )",
            "            cache_timeout = database.cache_timeout",
            "            if cache_timeout is None:",
            "                cache_timeout = config[\"CACHE_DEFAULT_TIMEOUT\"]",
            "",
            "            compressed = zlib_compress(serialized_payload)",
            "            logger.debug(",
            "                \"*** serialized payload size: %i\", getsizeof(serialized_payload)",
            "            )",
            "            logger.debug(\"*** compressed payload size: %i\", getsizeof(compressed))",
            "            results_backend.set(key, compressed, cache_timeout)",
            "        query.results_key = key",
            "",
            "    query.status = QueryStatus.SUCCESS",
            "    session.commit()",
            "",
            "    if return_results:",
            "        # since we're returning results we need to create non-arrow data",
            "        if use_arrow_data:",
            "            (",
            "                data,",
            "                selected_columns,",
            "                all_columns,",
            "                expanded_columns,",
            "            ) = _serialize_and_expand_data(",
            "                result_set, db_engine_spec, False, expand_data",
            "            )",
            "            payload.update(",
            "                {",
            "                    \"data\": data,",
            "                    \"columns\": all_columns,",
            "                    \"selected_columns\": selected_columns,",
            "                    \"expanded_columns\": expanded_columns,",
            "                }",
            "            )",
            "        return payload",
            "",
            "    return None",
            "",
            "",
            "def cancel_query(query: Query) -> bool:",
            "    \"\"\"",
            "    Cancel a running query.",
            "",
            "    Note some engines implicitly handle the cancelation of a query and thus no explicit",
            "    action is required.",
            "",
            "    :param query: Query to cancel",
            "    :return: True if query cancelled successfully, False otherwise",
            "    \"\"\"",
            "",
            "    if query.database.db_engine_spec.has_implicit_cancel():",
            "        return True",
            "",
            "    # Some databases may need to make preparations for query cancellation",
            "    query.database.db_engine_spec.prepare_cancel_query(query, db.session)",
            "",
            "    if query.extra.get(QUERY_EARLY_CANCEL_KEY):",
            "        # Query has been cancelled prior to being able to set the cancel key.",
            "        # This can happen if the query cancellation key can only be acquired after the",
            "        # query has been executed",
            "        return True",
            "",
            "    cancel_query_id = query.extra.get(QUERY_CANCEL_KEY)",
            "    if cancel_query_id is None:",
            "        return False",
            "",
            "    with query.database.get_sqla_engine_with_context(",
            "        query.schema, source=QuerySource.SQL_LAB",
            "    ) as engine:",
            "        with closing(engine.raw_connection()) as conn:",
            "            with closing(conn.cursor()) as cursor:",
            "                return query.database.db_engine_spec.cancel_query(",
            "                    cursor, query, cancel_query_id",
            "                )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "211": [
                "execute_sql_statement"
            ],
            "231": [
                "execute_sql_statement"
            ],
            "422": [
                "execute_sql_statements"
            ]
        },
        "addLocation": []
    }
}