{
    "python/pyarrow/tests/test_cffi.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " # specific language governing permissions and limitations"
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " # under the License."
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+import contextlib"
            },
            "4": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " import ctypes"
            },
            "5": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " import gc"
            },
            "6": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1"
            },
            "8": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-class ParamExtType(pa.PyExtensionType):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+@contextlib.contextmanager"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+def registered_extension_type(ext_type):"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+    pa.register_extension_type(ext_type)"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+    try:"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+        yield"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+    finally:"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+        pa.unregister_extension_type(ext_type.extension_name)"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+class ParamExtType(pa.ExtensionType):"
            },
            "21": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 65,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "     def __init__(self, width):"
            },
            "23": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "         self._width = width"
            },
            "24": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        pa.PyExtensionType.__init__(self, pa.binary(width))"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+        super().__init__(pa.binary(width),"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+                         \"pyarrow.tests.test_cffi.ParamExtType\")"
            },
            "27": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            },
            "28": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "     @property"
            },
            "29": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 72,
                "PatchRowcode": "     def width(self):"
            },
            "30": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "         return self._width"
            },
            "31": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " "
            },
            "32": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __reduce__(self):"
            },
            "33": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return ParamExtType, (self.width,)"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+    def __arrow_ext_serialize__(self):"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+        return str(self.width).encode()"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 78,
                "PatchRowcode": "+    @classmethod"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 79,
                "PatchRowcode": "+    def __arrow_ext_deserialize__(cls, storage_type, serialized):"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+        width = int(serialized.decode())"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+        return cls(width)"
            },
            "41": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 82,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 83,
                "PatchRowcode": " "
            },
            "43": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " def make_schema():"
            },
            "44": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "                      metadata={b'key1': b'value1'})"
            },
            "45": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 93,
                "PatchRowcode": " "
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+def make_extension_storage_schema():"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+    # Should be kept in sync with make_extension_schema"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+    return pa.schema([('ext', ParamExtType(3).storage_type)],"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+                     metadata={b'key1': b'value1'})"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+"
            },
            "53": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 100,
                "PatchRowcode": " def make_batch():"
            },
            "54": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "     return pa.record_batch([[[1], [2, 42]]], make_schema())"
            },
            "55": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 102,
                "PatchRowcode": " "
            },
            "56": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": 226,
                "PatchRowcode": "         pa.Array._import_from_c(ptr_array, ptr_schema)"
            },
            "57": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": 227,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": 228,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def check_export_import_schema(schema_factory):"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 229,
                "PatchRowcode": "+def check_export_import_schema(schema_factory, expected_schema_factory=None):"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 230,
                "PatchRowcode": "+    if expected_schema_factory is None:"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 231,
                "PatchRowcode": "+        expected_schema_factory = schema_factory"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 232,
                "PatchRowcode": "+"
            },
            "64": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": 233,
                "PatchRowcode": "     c_schema = ffi.new(\"struct ArrowSchema*\")"
            },
            "65": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": 234,
                "PatchRowcode": "     ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))"
            },
            "66": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": 235,
                "PatchRowcode": " "
            },
            "67": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 240,
                "PatchRowcode": "     assert pa.total_allocated_bytes() > old_allocated"
            },
            "68": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "     # Delete and recreate C++ object from exported pointer"
            },
            "69": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "     schema_new = pa.Schema._import_from_c(ptr_schema)"
            },
            "70": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    assert schema_new == schema_factory()"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 243,
                "PatchRowcode": "+    assert schema_new == expected_schema_factory()"
            },
            "72": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 244,
                "PatchRowcode": "     assert pa.total_allocated_bytes() == old_allocated"
            },
            "73": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 245,
                "PatchRowcode": "     del schema_new"
            },
            "74": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": 246,
                "PatchRowcode": "     assert pa.total_allocated_bytes() == old_allocated"
            },
            "75": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 265,
                "PatchRowcode": " "
            },
            "76": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 266,
                "PatchRowcode": " @needs_cffi"
            },
            "77": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": 267,
                "PatchRowcode": " def test_export_import_schema_with_extension():"
            },
            "78": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    check_export_import_schema(make_extension_schema)"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 268,
                "PatchRowcode": "+    # Extension type is unregistered => the storage type is imported"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 269,
                "PatchRowcode": "+    check_export_import_schema(make_extension_schema,"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 270,
                "PatchRowcode": "+                               make_extension_storage_schema)"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 271,
                "PatchRowcode": "+"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 272,
                "PatchRowcode": "+    # Extension type is registered => the extension type is imported"
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 273,
                "PatchRowcode": "+    with registered_extension_type(ParamExtType(1)):"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 274,
                "PatchRowcode": "+        check_export_import_schema(make_extension_schema)"
            },
            "86": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 275,
                "PatchRowcode": " "
            },
            "87": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 276,
                "PatchRowcode": " "
            },
            "88": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 277,
                "PatchRowcode": " @needs_cffi"
            },
            "89": {
                "beforePatchRowNumber": 319,
                "afterPatchRowNumber": 350,
                "PatchRowcode": " "
            },
            "90": {
                "beforePatchRowNumber": 320,
                "afterPatchRowNumber": 351,
                "PatchRowcode": " @needs_cffi"
            },
            "91": {
                "beforePatchRowNumber": 321,
                "afterPatchRowNumber": 352,
                "PatchRowcode": " def test_export_import_batch_with_extension():"
            },
            "92": {
                "beforePatchRowNumber": 322,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    check_export_import_batch(make_extension_batch)"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 353,
                "PatchRowcode": "+    with registered_extension_type(ParamExtType(1)):"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 354,
                "PatchRowcode": "+        check_export_import_batch(make_extension_batch)"
            },
            "95": {
                "beforePatchRowNumber": 323,
                "afterPatchRowNumber": 355,
                "PatchRowcode": " "
            },
            "96": {
                "beforePatchRowNumber": 324,
                "afterPatchRowNumber": 356,
                "PatchRowcode": " "
            },
            "97": {
                "beforePatchRowNumber": 325,
                "afterPatchRowNumber": 357,
                "PatchRowcode": " def _export_import_batch_reader(ptr_stream, reader_factory):"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import ctypes",
            "import gc",
            "",
            "import pyarrow as pa",
            "try:",
            "    from pyarrow.cffi import ffi",
            "except ImportError:",
            "    ffi = None",
            "",
            "import pytest",
            "",
            "try:",
            "    import pandas as pd",
            "    import pandas.testing as tm",
            "except ImportError:",
            "    pd = tm = None",
            "",
            "",
            "needs_cffi = pytest.mark.skipif(ffi is None,",
            "                                reason=\"test needs cffi package installed\")",
            "",
            "assert_schema_released = pytest.raises(",
            "    ValueError, match=\"Cannot import released ArrowSchema\")",
            "",
            "assert_array_released = pytest.raises(",
            "    ValueError, match=\"Cannot import released ArrowArray\")",
            "",
            "assert_stream_released = pytest.raises(",
            "    ValueError, match=\"Cannot import released ArrowArrayStream\")",
            "",
            "",
            "def PyCapsule_IsValid(capsule, name):",
            "    return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1",
            "",
            "",
            "class ParamExtType(pa.PyExtensionType):",
            "",
            "    def __init__(self, width):",
            "        self._width = width",
            "        pa.PyExtensionType.__init__(self, pa.binary(width))",
            "",
            "    @property",
            "    def width(self):",
            "        return self._width",
            "",
            "    def __reduce__(self):",
            "        return ParamExtType, (self.width,)",
            "",
            "",
            "def make_schema():",
            "    return pa.schema([('ints', pa.list_(pa.int32()))],",
            "                     metadata={b'key1': b'value1'})",
            "",
            "",
            "def make_extension_schema():",
            "    return pa.schema([('ext', ParamExtType(3))],",
            "                     metadata={b'key1': b'value1'})",
            "",
            "",
            "def make_batch():",
            "    return pa.record_batch([[[1], [2, 42]]], make_schema())",
            "",
            "",
            "def make_extension_batch():",
            "    schema = make_extension_schema()",
            "    ext_col = schema[0].type.wrap_array(pa.array([b\"foo\", b\"bar\"],",
            "                                                 type=pa.binary(3)))",
            "    return pa.record_batch([ext_col], schema)",
            "",
            "",
            "def make_batches():",
            "    schema = make_schema()",
            "    return [",
            "        pa.record_batch([[[1], [2, 42]]], schema),",
            "        pa.record_batch([[None, [], [5, 6]]], schema),",
            "    ]",
            "",
            "",
            "def make_serialized(schema, batches):",
            "    with pa.BufferOutputStream() as sink:",
            "        with pa.ipc.new_stream(sink, schema) as out:",
            "            for batch in batches:",
            "                out.write(batch)",
            "        return sink.getvalue()",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_type():",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    typ = pa.list_(pa.int32())",
            "    typ._export_to_c(ptr_schema)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    del typ",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    typ_new = pa.DataType._import_from_c(ptr_schema)",
            "    assert typ_new == pa.list_(pa.int32())",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.DataType._import_from_c(ptr_schema)",
            "",
            "    # Invalid format string",
            "    pa.int32()._export_to_c(ptr_schema)",
            "    bad_format = ffi.new(\"char[]\", b\"zzz\")",
            "    c_schema.format = bad_format",
            "    with pytest.raises(ValueError,",
            "                       match=\"Invalid or unsupported format string\"):",
            "        pa.DataType._import_from_c(ptr_schema)",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.DataType._import_from_c(ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_field():",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    field = pa.field(\"test\", pa.list_(pa.int32()), nullable=True)",
            "    field._export_to_c(ptr_schema)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    del field",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "",
            "    field_new = pa.Field._import_from_c(ptr_schema)",
            "    assert field_new == pa.field(\"test\", pa.list_(pa.int32()), nullable=True)",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Field._import_from_c(ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_array():",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "    c_array = ffi.new(\"struct ArrowArray*\")",
            "    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    # Type is known up front",
            "    typ = pa.list_(pa.int32())",
            "    arr = pa.array([[1], [2, 42]], type=typ)",
            "    py_value = arr.to_pylist()",
            "    arr._export_to_c(ptr_array)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete recreate C++ object from exported pointer",
            "    del arr",
            "    arr_new = pa.Array._import_from_c(ptr_array, typ)",
            "    assert arr_new.to_pylist() == py_value",
            "    assert arr_new.type == pa.list_(pa.int32())",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del arr_new, typ",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_array_released:",
            "        pa.Array._import_from_c(ptr_array, pa.list_(pa.int32()))",
            "",
            "    # Type is exported and imported at the same time",
            "    arr = pa.array([[1], [2, 42]], type=pa.list_(pa.int32()))",
            "    py_value = arr.to_pylist()",
            "    arr._export_to_c(ptr_array, ptr_schema)",
            "    # Delete and recreate C++ objects from exported pointers",
            "    del arr",
            "    arr_new = pa.Array._import_from_c(ptr_array, ptr_schema)",
            "    assert arr_new.to_pylist() == py_value",
            "    assert arr_new.type == pa.list_(pa.int32())",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del arr_new",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Array._import_from_c(ptr_array, ptr_schema)",
            "",
            "",
            "def check_export_import_schema(schema_factory):",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    schema_factory()._export_to_c(ptr_schema)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    schema_new = pa.Schema._import_from_c(ptr_schema)",
            "    assert schema_new == schema_factory()",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    del schema_new",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Schema._import_from_c(ptr_schema)",
            "",
            "    # Not a struct type",
            "    pa.int32()._export_to_c(ptr_schema)",
            "    with pytest.raises(ValueError,",
            "                       match=\"ArrowSchema describes non-struct type\"):",
            "        pa.Schema._import_from_c(ptr_schema)",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Schema._import_from_c(ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_schema():",
            "    check_export_import_schema(make_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_schema_with_extension():",
            "    check_export_import_schema(make_extension_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_schema_float_pointer():",
            "    # Previous versions of the R Arrow library used to pass pointer",
            "    # values as a double.",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    match = \"Passing a pointer value as a float is unsafe\"",
            "    with pytest.warns(UserWarning, match=match):",
            "        make_schema()._export_to_c(float(ptr_schema))",
            "    with pytest.warns(UserWarning, match=match):",
            "        schema_new = pa.Schema._import_from_c(float(ptr_schema))",
            "    assert schema_new == make_schema()",
            "",
            "",
            "def check_export_import_batch(batch_factory):",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "    c_array = ffi.new(\"struct ArrowArray*\")",
            "    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    # Schema is known up front",
            "    batch = batch_factory()",
            "    schema = batch.schema",
            "    py_value = batch.to_pydict()",
            "    batch._export_to_c(ptr_array)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    del batch",
            "    batch_new = pa.RecordBatch._import_from_c(ptr_array, schema)",
            "    assert batch_new.to_pydict() == py_value",
            "    assert batch_new.schema == schema",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del batch_new, schema",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_array_released:",
            "        pa.RecordBatch._import_from_c(ptr_array, make_schema())",
            "",
            "    # Type is exported and imported at the same time",
            "    batch = batch_factory()",
            "    py_value = batch.to_pydict()",
            "    batch._export_to_c(ptr_array, ptr_schema)",
            "    # Delete and recreate C++ objects from exported pointers",
            "    del batch",
            "    batch_new = pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "    assert batch_new.to_pydict() == py_value",
            "    assert batch_new.schema == batch_factory().schema",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del batch_new",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "",
            "    # Not a struct type",
            "    pa.int32()._export_to_c(ptr_schema)",
            "    batch_factory()._export_to_c(ptr_array)",
            "    with pytest.raises(ValueError,",
            "                       match=\"ArrowSchema describes non-struct type\"):",
            "        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_batch():",
            "    check_export_import_batch(make_batch)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_batch_with_extension():",
            "    check_export_import_batch(make_extension_batch)",
            "",
            "",
            "def _export_import_batch_reader(ptr_stream, reader_factory):",
            "    # Prepare input",
            "    batches = make_batches()",
            "    schema = batches[0].schema",
            "",
            "    reader = reader_factory(schema, batches)",
            "    reader._export_to_c(ptr_stream)",
            "    # Delete and recreate C++ object from exported pointer",
            "    del reader, batches",
            "",
            "    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "    assert reader_new.schema == schema",
            "    got_batches = list(reader_new)",
            "    del reader_new",
            "    assert got_batches == make_batches()",
            "",
            "    # Test read_pandas()",
            "    if pd is not None:",
            "        batches = make_batches()",
            "        schema = batches[0].schema",
            "        expected_df = pa.Table.from_batches(batches).to_pandas()",
            "",
            "        reader = reader_factory(schema, batches)",
            "        reader._export_to_c(ptr_stream)",
            "        del reader, batches",
            "",
            "        reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "        got_df = reader_new.read_pandas()",
            "        del reader_new",
            "        tm.assert_frame_equal(expected_df, got_df)",
            "",
            "",
            "def make_ipc_stream_reader(schema, batches):",
            "    return pa.ipc.open_stream(make_serialized(schema, batches))",
            "",
            "",
            "def make_py_record_batch_reader(schema, batches):",
            "    return pa.RecordBatchReader.from_batches(schema, batches)",
            "",
            "",
            "@needs_cffi",
            "@pytest.mark.parametrize('reader_factory',",
            "                         [make_ipc_stream_reader,",
            "                          make_py_record_batch_reader])",
            "def test_export_import_batch_reader(reader_factory):",
            "    c_stream = ffi.new(\"struct ArrowArrayStream*\")",
            "    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    _export_import_batch_reader(ptr_stream, reader_factory)",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    # Now released",
            "    with assert_stream_released:",
            "        pa.RecordBatchReader._import_from_c(ptr_stream)",
            "",
            "",
            "@needs_cffi",
            "def test_imported_batch_reader_error():",
            "    c_stream = ffi.new(\"struct ArrowArrayStream*\")",
            "    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))",
            "",
            "    schema = pa.schema([('foo', pa.int32())])",
            "    batches = [pa.record_batch([[1, 2, 3]], schema=schema),",
            "               pa.record_batch([[4, 5, 6]], schema=schema)]",
            "    buf = make_serialized(schema, batches)",
            "",
            "    # Open a corrupt/incomplete stream and export it",
            "    reader = pa.ipc.open_stream(buf[:-16])",
            "    reader._export_to_c(ptr_stream)",
            "    del reader",
            "",
            "    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "    batch = reader_new.read_next_batch()",
            "    assert batch == batches[0]",
            "    with pytest.raises(OSError,",
            "                       match=\"Expected to be able to read 16 bytes \"",
            "                             \"for message body, got 8\"):",
            "        reader_new.read_next_batch()",
            "",
            "    # Again, but call read_all()",
            "    reader = pa.ipc.open_stream(buf[:-16])",
            "    reader._export_to_c(ptr_stream)",
            "    del reader",
            "",
            "    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "    with pytest.raises(OSError,",
            "                       match=\"Expected to be able to read 16 bytes \"",
            "                             \"for message body, got 8\"):",
            "        reader_new.read_all()",
            "",
            "",
            "@pytest.mark.parametrize('obj', [pa.int32(), pa.field('foo', pa.int32()),",
            "                                 pa.schema({'foo': pa.int32()})],",
            "                         ids=['type', 'field', 'schema'])",
            "def test_roundtrip_schema_capsule(obj):",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    capsule = obj.__arrow_c_schema__()",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_schema\") == 1",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    obj_out = type(obj)._import_from_c_capsule(capsule)",
            "    assert obj_out == obj",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    capsule = obj.__arrow_c_schema__()",
            "",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del capsule",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "",
            "@pytest.mark.parametrize('arr,schema_accessor,bad_type,good_type', [",
            "    (pa.array(['a', 'b', 'c']), lambda x: x.type, pa.int32(), pa.string()),",
            "    (",
            "        pa.record_batch([pa.array(['a', 'b', 'c'])], names=['x']),",
            "        lambda x: x.schema,",
            "        pa.schema({'x': pa.int32()}),",
            "        pa.schema({'x': pa.string()})",
            "    ),",
            "], ids=['array', 'record_batch'])",
            "def test_roundtrip_array_capsule(arr, schema_accessor, bad_type, good_type):",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    import_array = type(arr)._import_from_c_capsule",
            "",
            "    schema_capsule, capsule = arr.__arrow_c_array__()",
            "    assert PyCapsule_IsValid(schema_capsule, b\"arrow_schema\") == 1",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_array\") == 1",
            "    arr_out = import_array(schema_capsule, capsule)",
            "    assert arr_out.equals(arr)",
            "",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del arr_out",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    capsule = arr.__arrow_c_array__()",
            "",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del capsule",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    with pytest.raises(ValueError,",
            "                       match=r\"Could not cast.* string to requested .* int32\"):",
            "        arr.__arrow_c_array__(bad_type.__arrow_c_schema__())",
            "",
            "    schema_capsule, array_capsule = arr.__arrow_c_array__(",
            "        good_type.__arrow_c_schema__())",
            "    arr_out = import_array(schema_capsule, array_capsule)",
            "    assert schema_accessor(arr_out) == good_type",
            "",
            "",
            "# TODO: implement requested_schema for stream",
            "@pytest.mark.parametrize('constructor', [",
            "    pa.RecordBatchReader.from_batches,",
            "    # Use a lambda because we need to re-order the parameters",
            "    lambda schema, batches: pa.Table.from_batches(batches, schema),",
            "], ids=['recordbatchreader', 'table'])",
            "def test_roundtrip_reader_capsule(constructor):",
            "    batches = make_batches()",
            "    schema = batches[0].schema",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    obj = constructor(schema, batches)",
            "",
            "    capsule = obj.__arrow_c_stream__()",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1",
            "    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
            "    assert imported_reader.schema == schema",
            "    imported_batches = list(imported_reader)",
            "    assert len(imported_batches) == len(batches)",
            "    for batch, expected in zip(imported_batches, batches):",
            "        assert batch.equals(expected)",
            "",
            "    del obj, imported_reader, batch, expected, imported_batches",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    obj = constructor(schema, batches)",
            "",
            "    # TODO: turn this to ValueError once we implement validation.",
            "    bad_schema = pa.schema({'ints': pa.int32()})",
            "    with pytest.raises(NotImplementedError):",
            "        obj.__arrow_c_stream__(bad_schema.__arrow_c_schema__())",
            "",
            "    # Can work with matching schema",
            "    matching_schema = pa.schema({'ints': pa.list_(pa.int32())})",
            "    capsule = obj.__arrow_c_stream__(matching_schema.__arrow_c_schema__())",
            "    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
            "    assert imported_reader.schema == matching_schema",
            "    for batch, expected in zip(imported_reader, batches):",
            "        assert batch.equals(expected)",
            "",
            "",
            "def test_roundtrip_batch_reader_capsule():",
            "    batch = make_batch()",
            "",
            "    capsule = batch.__arrow_c_stream__()",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1",
            "    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
            "    assert imported_reader.schema == batch.schema",
            "    assert imported_reader.read_next_batch().equals(batch)",
            "    with pytest.raises(StopIteration):",
            "        imported_reader.read_next_batch()"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "",
            "import contextlib",
            "import ctypes",
            "import gc",
            "",
            "import pyarrow as pa",
            "try:",
            "    from pyarrow.cffi import ffi",
            "except ImportError:",
            "    ffi = None",
            "",
            "import pytest",
            "",
            "try:",
            "    import pandas as pd",
            "    import pandas.testing as tm",
            "except ImportError:",
            "    pd = tm = None",
            "",
            "",
            "needs_cffi = pytest.mark.skipif(ffi is None,",
            "                                reason=\"test needs cffi package installed\")",
            "",
            "assert_schema_released = pytest.raises(",
            "    ValueError, match=\"Cannot import released ArrowSchema\")",
            "",
            "assert_array_released = pytest.raises(",
            "    ValueError, match=\"Cannot import released ArrowArray\")",
            "",
            "assert_stream_released = pytest.raises(",
            "    ValueError, match=\"Cannot import released ArrowArrayStream\")",
            "",
            "",
            "def PyCapsule_IsValid(capsule, name):",
            "    return ctypes.pythonapi.PyCapsule_IsValid(ctypes.py_object(capsule), name) == 1",
            "",
            "",
            "@contextlib.contextmanager",
            "def registered_extension_type(ext_type):",
            "    pa.register_extension_type(ext_type)",
            "    try:",
            "        yield",
            "    finally:",
            "        pa.unregister_extension_type(ext_type.extension_name)",
            "",
            "",
            "class ParamExtType(pa.ExtensionType):",
            "",
            "    def __init__(self, width):",
            "        self._width = width",
            "        super().__init__(pa.binary(width),",
            "                         \"pyarrow.tests.test_cffi.ParamExtType\")",
            "",
            "    @property",
            "    def width(self):",
            "        return self._width",
            "",
            "    def __arrow_ext_serialize__(self):",
            "        return str(self.width).encode()",
            "",
            "    @classmethod",
            "    def __arrow_ext_deserialize__(cls, storage_type, serialized):",
            "        width = int(serialized.decode())",
            "        return cls(width)",
            "",
            "",
            "def make_schema():",
            "    return pa.schema([('ints', pa.list_(pa.int32()))],",
            "                     metadata={b'key1': b'value1'})",
            "",
            "",
            "def make_extension_schema():",
            "    return pa.schema([('ext', ParamExtType(3))],",
            "                     metadata={b'key1': b'value1'})",
            "",
            "",
            "def make_extension_storage_schema():",
            "    # Should be kept in sync with make_extension_schema",
            "    return pa.schema([('ext', ParamExtType(3).storage_type)],",
            "                     metadata={b'key1': b'value1'})",
            "",
            "",
            "def make_batch():",
            "    return pa.record_batch([[[1], [2, 42]]], make_schema())",
            "",
            "",
            "def make_extension_batch():",
            "    schema = make_extension_schema()",
            "    ext_col = schema[0].type.wrap_array(pa.array([b\"foo\", b\"bar\"],",
            "                                                 type=pa.binary(3)))",
            "    return pa.record_batch([ext_col], schema)",
            "",
            "",
            "def make_batches():",
            "    schema = make_schema()",
            "    return [",
            "        pa.record_batch([[[1], [2, 42]]], schema),",
            "        pa.record_batch([[None, [], [5, 6]]], schema),",
            "    ]",
            "",
            "",
            "def make_serialized(schema, batches):",
            "    with pa.BufferOutputStream() as sink:",
            "        with pa.ipc.new_stream(sink, schema) as out:",
            "            for batch in batches:",
            "                out.write(batch)",
            "        return sink.getvalue()",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_type():",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    typ = pa.list_(pa.int32())",
            "    typ._export_to_c(ptr_schema)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    del typ",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    typ_new = pa.DataType._import_from_c(ptr_schema)",
            "    assert typ_new == pa.list_(pa.int32())",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.DataType._import_from_c(ptr_schema)",
            "",
            "    # Invalid format string",
            "    pa.int32()._export_to_c(ptr_schema)",
            "    bad_format = ffi.new(\"char[]\", b\"zzz\")",
            "    c_schema.format = bad_format",
            "    with pytest.raises(ValueError,",
            "                       match=\"Invalid or unsupported format string\"):",
            "        pa.DataType._import_from_c(ptr_schema)",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.DataType._import_from_c(ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_field():",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    field = pa.field(\"test\", pa.list_(pa.int32()), nullable=True)",
            "    field._export_to_c(ptr_schema)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    del field",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "",
            "    field_new = pa.Field._import_from_c(ptr_schema)",
            "    assert field_new == pa.field(\"test\", pa.list_(pa.int32()), nullable=True)",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Field._import_from_c(ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_array():",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "    c_array = ffi.new(\"struct ArrowArray*\")",
            "    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    # Type is known up front",
            "    typ = pa.list_(pa.int32())",
            "    arr = pa.array([[1], [2, 42]], type=typ)",
            "    py_value = arr.to_pylist()",
            "    arr._export_to_c(ptr_array)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete recreate C++ object from exported pointer",
            "    del arr",
            "    arr_new = pa.Array._import_from_c(ptr_array, typ)",
            "    assert arr_new.to_pylist() == py_value",
            "    assert arr_new.type == pa.list_(pa.int32())",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del arr_new, typ",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_array_released:",
            "        pa.Array._import_from_c(ptr_array, pa.list_(pa.int32()))",
            "",
            "    # Type is exported and imported at the same time",
            "    arr = pa.array([[1], [2, 42]], type=pa.list_(pa.int32()))",
            "    py_value = arr.to_pylist()",
            "    arr._export_to_c(ptr_array, ptr_schema)",
            "    # Delete and recreate C++ objects from exported pointers",
            "    del arr",
            "    arr_new = pa.Array._import_from_c(ptr_array, ptr_schema)",
            "    assert arr_new.to_pylist() == py_value",
            "    assert arr_new.type == pa.list_(pa.int32())",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del arr_new",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Array._import_from_c(ptr_array, ptr_schema)",
            "",
            "",
            "def check_export_import_schema(schema_factory, expected_schema_factory=None):",
            "    if expected_schema_factory is None:",
            "        expected_schema_factory = schema_factory",
            "",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    schema_factory()._export_to_c(ptr_schema)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    schema_new = pa.Schema._import_from_c(ptr_schema)",
            "    assert schema_new == expected_schema_factory()",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    del schema_new",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Schema._import_from_c(ptr_schema)",
            "",
            "    # Not a struct type",
            "    pa.int32()._export_to_c(ptr_schema)",
            "    with pytest.raises(ValueError,",
            "                       match=\"ArrowSchema describes non-struct type\"):",
            "        pa.Schema._import_from_c(ptr_schema)",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.Schema._import_from_c(ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_schema():",
            "    check_export_import_schema(make_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_schema_with_extension():",
            "    # Extension type is unregistered => the storage type is imported",
            "    check_export_import_schema(make_extension_schema,",
            "                               make_extension_storage_schema)",
            "",
            "    # Extension type is registered => the extension type is imported",
            "    with registered_extension_type(ParamExtType(1)):",
            "        check_export_import_schema(make_extension_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_schema_float_pointer():",
            "    # Previous versions of the R Arrow library used to pass pointer",
            "    # values as a double.",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "",
            "    match = \"Passing a pointer value as a float is unsafe\"",
            "    with pytest.warns(UserWarning, match=match):",
            "        make_schema()._export_to_c(float(ptr_schema))",
            "    with pytest.warns(UserWarning, match=match):",
            "        schema_new = pa.Schema._import_from_c(float(ptr_schema))",
            "    assert schema_new == make_schema()",
            "",
            "",
            "def check_export_import_batch(batch_factory):",
            "    c_schema = ffi.new(\"struct ArrowSchema*\")",
            "    ptr_schema = int(ffi.cast(\"uintptr_t\", c_schema))",
            "    c_array = ffi.new(\"struct ArrowArray*\")",
            "    ptr_array = int(ffi.cast(\"uintptr_t\", c_array))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    # Schema is known up front",
            "    batch = batch_factory()",
            "    schema = batch.schema",
            "    py_value = batch.to_pydict()",
            "    batch._export_to_c(ptr_array)",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    # Delete and recreate C++ object from exported pointer",
            "    del batch",
            "    batch_new = pa.RecordBatch._import_from_c(ptr_array, schema)",
            "    assert batch_new.to_pydict() == py_value",
            "    assert batch_new.schema == schema",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del batch_new, schema",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_array_released:",
            "        pa.RecordBatch._import_from_c(ptr_array, make_schema())",
            "",
            "    # Type is exported and imported at the same time",
            "    batch = batch_factory()",
            "    py_value = batch.to_pydict()",
            "    batch._export_to_c(ptr_array, ptr_schema)",
            "    # Delete and recreate C++ objects from exported pointers",
            "    del batch",
            "    batch_new = pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "    assert batch_new.to_pydict() == py_value",
            "    assert batch_new.schema == batch_factory().schema",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del batch_new",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "",
            "    # Not a struct type",
            "    pa.int32()._export_to_c(ptr_schema)",
            "    batch_factory()._export_to_c(ptr_array)",
            "    with pytest.raises(ValueError,",
            "                       match=\"ArrowSchema describes non-struct type\"):",
            "        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "    # Now released",
            "    with assert_schema_released:",
            "        pa.RecordBatch._import_from_c(ptr_array, ptr_schema)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_batch():",
            "    check_export_import_batch(make_batch)",
            "",
            "",
            "@needs_cffi",
            "def test_export_import_batch_with_extension():",
            "    with registered_extension_type(ParamExtType(1)):",
            "        check_export_import_batch(make_extension_batch)",
            "",
            "",
            "def _export_import_batch_reader(ptr_stream, reader_factory):",
            "    # Prepare input",
            "    batches = make_batches()",
            "    schema = batches[0].schema",
            "",
            "    reader = reader_factory(schema, batches)",
            "    reader._export_to_c(ptr_stream)",
            "    # Delete and recreate C++ object from exported pointer",
            "    del reader, batches",
            "",
            "    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "    assert reader_new.schema == schema",
            "    got_batches = list(reader_new)",
            "    del reader_new",
            "    assert got_batches == make_batches()",
            "",
            "    # Test read_pandas()",
            "    if pd is not None:",
            "        batches = make_batches()",
            "        schema = batches[0].schema",
            "        expected_df = pa.Table.from_batches(batches).to_pandas()",
            "",
            "        reader = reader_factory(schema, batches)",
            "        reader._export_to_c(ptr_stream)",
            "        del reader, batches",
            "",
            "        reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "        got_df = reader_new.read_pandas()",
            "        del reader_new",
            "        tm.assert_frame_equal(expected_df, got_df)",
            "",
            "",
            "def make_ipc_stream_reader(schema, batches):",
            "    return pa.ipc.open_stream(make_serialized(schema, batches))",
            "",
            "",
            "def make_py_record_batch_reader(schema, batches):",
            "    return pa.RecordBatchReader.from_batches(schema, batches)",
            "",
            "",
            "@needs_cffi",
            "@pytest.mark.parametrize('reader_factory',",
            "                         [make_ipc_stream_reader,",
            "                          make_py_record_batch_reader])",
            "def test_export_import_batch_reader(reader_factory):",
            "    c_stream = ffi.new(\"struct ArrowArrayStream*\")",
            "    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    _export_import_batch_reader(ptr_stream, reader_factory)",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    # Now released",
            "    with assert_stream_released:",
            "        pa.RecordBatchReader._import_from_c(ptr_stream)",
            "",
            "",
            "@needs_cffi",
            "def test_imported_batch_reader_error():",
            "    c_stream = ffi.new(\"struct ArrowArrayStream*\")",
            "    ptr_stream = int(ffi.cast(\"uintptr_t\", c_stream))",
            "",
            "    schema = pa.schema([('foo', pa.int32())])",
            "    batches = [pa.record_batch([[1, 2, 3]], schema=schema),",
            "               pa.record_batch([[4, 5, 6]], schema=schema)]",
            "    buf = make_serialized(schema, batches)",
            "",
            "    # Open a corrupt/incomplete stream and export it",
            "    reader = pa.ipc.open_stream(buf[:-16])",
            "    reader._export_to_c(ptr_stream)",
            "    del reader",
            "",
            "    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "    batch = reader_new.read_next_batch()",
            "    assert batch == batches[0]",
            "    with pytest.raises(OSError,",
            "                       match=\"Expected to be able to read 16 bytes \"",
            "                             \"for message body, got 8\"):",
            "        reader_new.read_next_batch()",
            "",
            "    # Again, but call read_all()",
            "    reader = pa.ipc.open_stream(buf[:-16])",
            "    reader._export_to_c(ptr_stream)",
            "    del reader",
            "",
            "    reader_new = pa.RecordBatchReader._import_from_c(ptr_stream)",
            "    with pytest.raises(OSError,",
            "                       match=\"Expected to be able to read 16 bytes \"",
            "                             \"for message body, got 8\"):",
            "        reader_new.read_all()",
            "",
            "",
            "@pytest.mark.parametrize('obj', [pa.int32(), pa.field('foo', pa.int32()),",
            "                                 pa.schema({'foo': pa.int32()})],",
            "                         ids=['type', 'field', 'schema'])",
            "def test_roundtrip_schema_capsule(obj):",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    capsule = obj.__arrow_c_schema__()",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_schema\") == 1",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    obj_out = type(obj)._import_from_c_capsule(capsule)",
            "    assert obj_out == obj",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    capsule = obj.__arrow_c_schema__()",
            "",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del capsule",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "",
            "@pytest.mark.parametrize('arr,schema_accessor,bad_type,good_type', [",
            "    (pa.array(['a', 'b', 'c']), lambda x: x.type, pa.int32(), pa.string()),",
            "    (",
            "        pa.record_batch([pa.array(['a', 'b', 'c'])], names=['x']),",
            "        lambda x: x.schema,",
            "        pa.schema({'x': pa.int32()}),",
            "        pa.schema({'x': pa.string()})",
            "    ),",
            "], ids=['array', 'record_batch'])",
            "def test_roundtrip_array_capsule(arr, schema_accessor, bad_type, good_type):",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    import_array = type(arr)._import_from_c_capsule",
            "",
            "    schema_capsule, capsule = arr.__arrow_c_array__()",
            "    assert PyCapsule_IsValid(schema_capsule, b\"arrow_schema\") == 1",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_array\") == 1",
            "    arr_out = import_array(schema_capsule, capsule)",
            "    assert arr_out.equals(arr)",
            "",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del arr_out",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    capsule = arr.__arrow_c_array__()",
            "",
            "    assert pa.total_allocated_bytes() > old_allocated",
            "    del capsule",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    with pytest.raises(ValueError,",
            "                       match=r\"Could not cast.* string to requested .* int32\"):",
            "        arr.__arrow_c_array__(bad_type.__arrow_c_schema__())",
            "",
            "    schema_capsule, array_capsule = arr.__arrow_c_array__(",
            "        good_type.__arrow_c_schema__())",
            "    arr_out = import_array(schema_capsule, array_capsule)",
            "    assert schema_accessor(arr_out) == good_type",
            "",
            "",
            "# TODO: implement requested_schema for stream",
            "@pytest.mark.parametrize('constructor', [",
            "    pa.RecordBatchReader.from_batches,",
            "    # Use a lambda because we need to re-order the parameters",
            "    lambda schema, batches: pa.Table.from_batches(batches, schema),",
            "], ids=['recordbatchreader', 'table'])",
            "def test_roundtrip_reader_capsule(constructor):",
            "    batches = make_batches()",
            "    schema = batches[0].schema",
            "",
            "    gc.collect()  # Make sure no Arrow data dangles in a ref cycle",
            "    old_allocated = pa.total_allocated_bytes()",
            "",
            "    obj = constructor(schema, batches)",
            "",
            "    capsule = obj.__arrow_c_stream__()",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1",
            "    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
            "    assert imported_reader.schema == schema",
            "    imported_batches = list(imported_reader)",
            "    assert len(imported_batches) == len(batches)",
            "    for batch, expected in zip(imported_batches, batches):",
            "        assert batch.equals(expected)",
            "",
            "    del obj, imported_reader, batch, expected, imported_batches",
            "",
            "    assert pa.total_allocated_bytes() == old_allocated",
            "",
            "    obj = constructor(schema, batches)",
            "",
            "    # TODO: turn this to ValueError once we implement validation.",
            "    bad_schema = pa.schema({'ints': pa.int32()})",
            "    with pytest.raises(NotImplementedError):",
            "        obj.__arrow_c_stream__(bad_schema.__arrow_c_schema__())",
            "",
            "    # Can work with matching schema",
            "    matching_schema = pa.schema({'ints': pa.list_(pa.int32())})",
            "    capsule = obj.__arrow_c_stream__(matching_schema.__arrow_c_schema__())",
            "    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
            "    assert imported_reader.schema == matching_schema",
            "    for batch, expected in zip(imported_reader, batches):",
            "        assert batch.equals(expected)",
            "",
            "",
            "def test_roundtrip_batch_reader_capsule():",
            "    batch = make_batch()",
            "",
            "    capsule = batch.__arrow_c_stream__()",
            "    assert PyCapsule_IsValid(capsule, b\"arrow_array_stream\") == 1",
            "    imported_reader = pa.RecordBatchReader._import_from_c_capsule(capsule)",
            "    assert imported_reader.schema == batch.schema",
            "    assert imported_reader.read_next_batch().equals(batch)",
            "    with pytest.raises(StopIteration):",
            "        imported_reader.read_next_batch()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "54": [
                "ParamExtType"
            ],
            "58": [
                "ParamExtType",
                "__init__"
            ],
            "64": [
                "ParamExtType",
                "__reduce__"
            ],
            "65": [
                "ParamExtType",
                "__reduce__"
            ],
            "207": [
                "check_export_import_schema"
            ],
            "218": [
                "check_export_import_schema"
            ],
            "243": [
                "test_export_import_schema_with_extension"
            ],
            "322": [
                "test_export_import_batch_with_extension"
            ]
        },
        "addLocation": []
    }
}