{
    "litellm/integrations/slack_alerting.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 698,
                "afterPatchRowNumber": 698,
                "PatchRowcode": "             return"
            },
            "1": {
                "beforePatchRowNumber": 699,
                "afterPatchRowNumber": 699,
                "PatchRowcode": "         if \"budget_alerts\" not in self.alert_types:"
            },
            "2": {
                "beforePatchRowNumber": 700,
                "afterPatchRowNumber": 700,
                "PatchRowcode": "             return"
            },
            "3": {
                "beforePatchRowNumber": 701,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        _id: str = \"default_id\"  # used for caching"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 701,
                "PatchRowcode": "+        _id: Optional[str] = \"default_id\"  # used for caching"
            },
            "5": {
                "beforePatchRowNumber": 702,
                "afterPatchRowNumber": 702,
                "PatchRowcode": "         user_info_json = user_info.model_dump(exclude_none=True)"
            },
            "6": {
                "beforePatchRowNumber": 703,
                "afterPatchRowNumber": 703,
                "PatchRowcode": "         for k, v in user_info_json.items():"
            },
            "7": {
                "beforePatchRowNumber": 704,
                "afterPatchRowNumber": 704,
                "PatchRowcode": "             user_info_str = \"\\n{}: {}\\n\".format(k, v)"
            }
        },
        "frontPatchFile": [
            "#### What this does ####",
            "#    Class for sending Slack Alerts #",
            "import dotenv, os, traceback",
            "from litellm.proxy._types import UserAPIKeyAuth, CallInfo, AlertType",
            "from litellm._logging import verbose_logger, verbose_proxy_logger",
            "import litellm, threading",
            "from typing import List, Literal, Any, Union, Optional, Dict, Set",
            "from litellm.caching import DualCache",
            "import asyncio, time",
            "import aiohttp",
            "from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler",
            "import datetime",
            "from pydantic import BaseModel, Field",
            "from enum import Enum",
            "from datetime import datetime as dt, timedelta, timezone",
            "from litellm.integrations.custom_logger import CustomLogger",
            "from litellm.proxy._types import WebhookEvent",
            "import random",
            "from typing import TypedDict",
            "from openai import APIError",
            "from .email_templates.templates import *",
            "",
            "import litellm.types",
            "from litellm.types.router import LiteLLM_Params",
            "",
            "",
            "class BaseOutageModel(TypedDict):",
            "    alerts: List[int]",
            "    minor_alert_sent: bool",
            "    major_alert_sent: bool",
            "    last_updated_at: float",
            "",
            "",
            "class OutageModel(BaseOutageModel):",
            "    model_id: str",
            "",
            "",
            "class ProviderRegionOutageModel(BaseOutageModel):",
            "    provider_region_id: str",
            "    deployment_ids: Set[str]",
            "",
            "",
            "# we use this for the email header, please send a test email if you change this. verify it looks good on email",
            "LITELLM_LOGO_URL = \"https://litellm-listing.s3.amazonaws.com/litellm_logo.png\"",
            "LITELLM_SUPPORT_CONTACT = \"support@berri.ai\"",
            "",
            "",
            "class LiteLLMBase(BaseModel):",
            "    \"\"\"",
            "    Implements default functions, all pydantic objects should have.",
            "    \"\"\"",
            "",
            "    def json(self, **kwargs):",
            "        try:",
            "            return self.model_dump()  # noqa",
            "        except:",
            "            # if using pydantic v1",
            "            return self.dict()",
            "",
            "",
            "class SlackAlertingArgsEnum(Enum):",
            "    daily_report_frequency: int = 12 * 60 * 60",
            "    report_check_interval: int = 5 * 60",
            "    budget_alert_ttl: int = 24 * 60 * 60",
            "    outage_alert_ttl: int = 1 * 60",
            "    region_outage_alert_ttl: int = 1 * 60",
            "    minor_outage_alert_threshold: int = 1 * 5",
            "    major_outage_alert_threshold: int = 1 * 10",
            "    max_outage_alert_list_size: int = 1 * 10",
            "",
            "",
            "class SlackAlertingArgs(LiteLLMBase):",
            "    daily_report_frequency: int = Field(",
            "        default=int(",
            "            os.getenv(",
            "                \"SLACK_DAILY_REPORT_FREQUENCY\",",
            "                SlackAlertingArgsEnum.daily_report_frequency.value,",
            "            )",
            "        ),",
            "        description=\"Frequency of receiving deployment latency/failure reports. Default is 12hours. Value is in seconds.\",",
            "    )",
            "    report_check_interval: int = Field(",
            "        default=SlackAlertingArgsEnum.report_check_interval.value,",
            "        description=\"Frequency of checking cache if report should be sent. Background process. Default is once per hour. Value is in seconds.\",",
            "    )  # 5 minutes",
            "    budget_alert_ttl: int = Field(",
            "        default=SlackAlertingArgsEnum.budget_alert_ttl.value,",
            "        description=\"Cache ttl for budgets alerts. Prevents spamming same alert, each time budget is crossed. Value is in seconds.\",",
            "    )  # 24 hours",
            "    outage_alert_ttl: int = Field(",
            "        default=SlackAlertingArgsEnum.outage_alert_ttl.value,",
            "        description=\"Cache ttl for model outage alerts. Sets time-window for errors. Default is 1 minute. Value is in seconds.\",",
            "    )  # 1 minute ttl",
            "    region_outage_alert_ttl: int = Field(",
            "        default=SlackAlertingArgsEnum.region_outage_alert_ttl.value,",
            "        description=\"Cache ttl for provider-region based outage alerts. Alert sent if 2+ models in same region report errors. Sets time-window for errors. Default is 1 minute. Value is in seconds.\",",
            "    )  # 1 minute ttl",
            "    minor_outage_alert_threshold: int = Field(",
            "        default=SlackAlertingArgsEnum.minor_outage_alert_threshold.value,",
            "        description=\"The number of errors that count as a model/region minor outage. ('400' error code is not counted).\",",
            "    )",
            "    major_outage_alert_threshold: int = Field(",
            "        default=SlackAlertingArgsEnum.major_outage_alert_threshold.value,",
            "        description=\"The number of errors that countas a model/region major outage. ('400' error code is not counted).\",",
            "    )",
            "    max_outage_alert_list_size: int = Field(",
            "        default=SlackAlertingArgsEnum.max_outage_alert_list_size.value,",
            "        description=\"Maximum number of errors to store in cache. For a given model/region. Prevents memory leaks.\",",
            "    )  # prevent memory leak",
            "",
            "",
            "class DeploymentMetrics(LiteLLMBase):",
            "    \"\"\"",
            "    Metrics per deployment, stored in cache",
            "",
            "    Used for daily reporting",
            "    \"\"\"",
            "",
            "    id: str",
            "    \"\"\"id of deployment in router model list\"\"\"",
            "",
            "    failed_request: bool",
            "    \"\"\"did it fail the request?\"\"\"",
            "",
            "    latency_per_output_token: Optional[float]",
            "    \"\"\"latency/output token of deployment\"\"\"",
            "",
            "    updated_at: dt",
            "    \"\"\"Current time of deployment being updated\"\"\"",
            "",
            "",
            "class SlackAlertingCacheKeys(Enum):",
            "    \"\"\"",
            "    Enum for deployment daily metrics keys - {deployment_id}:{enum}",
            "    \"\"\"",
            "",
            "    failed_requests_key = \"failed_requests_daily_metrics\"",
            "    latency_key = \"latency_daily_metrics\"",
            "    report_sent_key = \"daily_metrics_report_sent\"",
            "",
            "",
            "class SlackAlerting(CustomLogger):",
            "    \"\"\"",
            "    Class for sending Slack Alerts",
            "    \"\"\"",
            "",
            "    # Class variables or attributes",
            "    def __init__(",
            "        self,",
            "        internal_usage_cache: Optional[DualCache] = None,",
            "        alerting_threshold: float = 300,  # threshold for slow / hanging llm responses (in seconds)",
            "        alerting: Optional[List] = [],",
            "        alert_types: List[AlertType] = [",
            "            \"llm_exceptions\",",
            "            \"llm_too_slow\",",
            "            \"llm_requests_hanging\",",
            "            \"budget_alerts\",",
            "            \"db_exceptions\",",
            "            \"daily_reports\",",
            "            \"spend_reports\",",
            "            \"cooldown_deployment\",",
            "            \"new_model_added\",",
            "            \"outage_alerts\",",
            "        ],",
            "        alert_to_webhook_url: Optional[",
            "            Dict",
            "        ] = None,  # if user wants to separate alerts to diff channels",
            "        alerting_args={},",
            "        default_webhook_url: Optional[str] = None,",
            "    ):",
            "        self.alerting_threshold = alerting_threshold",
            "        self.alerting = alerting",
            "        self.alert_types = alert_types",
            "        self.internal_usage_cache = internal_usage_cache or DualCache()",
            "        self.async_http_handler = AsyncHTTPHandler()",
            "        self.alert_to_webhook_url = alert_to_webhook_url",
            "        self.is_running = False",
            "        self.alerting_args = SlackAlertingArgs(**alerting_args)",
            "        self.default_webhook_url = default_webhook_url",
            "        self.llm_router: Optional[litellm.Router] = None",
            "",
            "    def update_values(",
            "        self,",
            "        alerting: Optional[List] = None,",
            "        alerting_threshold: Optional[float] = None,",
            "        alert_types: Optional[List] = None,",
            "        alert_to_webhook_url: Optional[Dict] = None,",
            "        alerting_args: Optional[Dict] = None,",
            "        llm_router: Optional[litellm.Router] = None,",
            "    ):",
            "        if alerting is not None:",
            "            self.alerting = alerting",
            "        if alerting_threshold is not None:",
            "            self.alerting_threshold = alerting_threshold",
            "        if alert_types is not None:",
            "            self.alert_types = alert_types",
            "        if alerting_args is not None:",
            "            self.alerting_args = SlackAlertingArgs(**alerting_args)",
            "        if alert_to_webhook_url is not None:",
            "            # update the dict",
            "            if self.alert_to_webhook_url is None:",
            "                self.alert_to_webhook_url = alert_to_webhook_url",
            "            else:",
            "                self.alert_to_webhook_url.update(alert_to_webhook_url)",
            "        if llm_router is not None:",
            "            self.llm_router = llm_router",
            "",
            "    async def deployment_in_cooldown(self):",
            "        pass",
            "",
            "    async def deployment_removed_from_cooldown(self):",
            "        pass",
            "",
            "    def _all_possible_alert_types(self):",
            "        # used by the UI to show all supported alert types",
            "        # Note: This is not the alerts the user has configured, instead it's all possible alert types a user can select",
            "        return [",
            "            \"llm_exceptions\",",
            "            \"llm_too_slow\",",
            "            \"llm_requests_hanging\",",
            "            \"budget_alerts\",",
            "            \"db_exceptions\",",
            "        ]",
            "",
            "    def _add_langfuse_trace_id_to_alert(",
            "        self,",
            "        request_data: Optional[dict] = None,",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Returns langfuse trace url",
            "",
            "        - check:",
            "        -> existing_trace_id",
            "        -> trace_id",
            "        -> litellm_call_id",
            "        \"\"\"",
            "        # do nothing for now",
            "        if request_data is not None:",
            "            trace_id = None",
            "            if (",
            "                request_data.get(\"metadata\", {}).get(\"existing_trace_id\", None)",
            "                is not None",
            "            ):",
            "                trace_id = request_data[\"metadata\"][\"existing_trace_id\"]",
            "            elif request_data.get(\"metadata\", {}).get(\"trace_id\", None) is not None:",
            "                trace_id = request_data[\"metadata\"][\"trace_id\"]",
            "            elif request_data.get(\"litellm_logging_obj\", None) is not None and hasattr(",
            "                request_data[\"litellm_logging_obj\"], \"model_call_details\"",
            "            ):",
            "                trace_id = request_data[\"litellm_logging_obj\"].model_call_details[",
            "                    \"litellm_call_id\"",
            "                ]",
            "            if litellm.utils.langFuseLogger is not None:",
            "                base_url = litellm.utils.langFuseLogger.Langfuse.base_url",
            "                return f\"{base_url}/trace/{trace_id}\"",
            "        return None",
            "",
            "    def _response_taking_too_long_callback_helper(",
            "        self,",
            "        kwargs,  # kwargs to completion",
            "        start_time,",
            "        end_time,  # start/end time",
            "    ):",
            "        try:",
            "            time_difference = end_time - start_time",
            "            # Convert the timedelta to float (in seconds)",
            "            time_difference_float = time_difference.total_seconds()",
            "            litellm_params = kwargs.get(\"litellm_params\", {})",
            "            model = kwargs.get(\"model\", \"\")",
            "            api_base = litellm.get_api_base(model=model, optional_params=litellm_params)",
            "            messages = kwargs.get(\"messages\", None)",
            "            # if messages does not exist fallback to \"input\"",
            "            if messages is None:",
            "                messages = kwargs.get(\"input\", None)",
            "",
            "            # only use first 100 chars for alerting",
            "            _messages = str(messages)[:100]",
            "",
            "            return time_difference_float, model, api_base, _messages",
            "        except Exception as e:",
            "            raise e",
            "",
            "    def _get_deployment_latencies_to_alert(self, metadata=None):",
            "        if metadata is None:",
            "            return None",
            "",
            "        if \"_latency_per_deployment\" in metadata:",
            "            # Translate model_id to -> api_base",
            "            # _latency_per_deployment is a dictionary that looks like this:",
            "            \"\"\"",
            "            _latency_per_deployment: {",
            "                api_base: 0.01336697916666667",
            "            }",
            "            \"\"\"",
            "            _message_to_send = \"\"",
            "            _deployment_latencies = metadata[\"_latency_per_deployment\"]",
            "            if len(_deployment_latencies) == 0:",
            "                return None",
            "            try:",
            "                # try sorting deployments by latency",
            "                _deployment_latencies = sorted(",
            "                    _deployment_latencies.items(), key=lambda x: x[1]",
            "                )",
            "                _deployment_latencies = dict(_deployment_latencies)",
            "            except:",
            "                pass",
            "            for api_base, latency in _deployment_latencies.items():",
            "                _message_to_send += f\"\\n{api_base}: {round(latency,2)}s\"",
            "            _message_to_send = \"```\" + _message_to_send + \"```\"",
            "            return _message_to_send",
            "",
            "    async def response_taking_too_long_callback(",
            "        self,",
            "        kwargs,  # kwargs to completion",
            "        completion_response,  # response from completion",
            "        start_time,",
            "        end_time,  # start/end time",
            "    ):",
            "        if self.alerting is None or self.alert_types is None:",
            "            return",
            "",
            "        time_difference_float, model, api_base, messages = (",
            "            self._response_taking_too_long_callback_helper(",
            "                kwargs=kwargs,",
            "                start_time=start_time,",
            "                end_time=end_time,",
            "            )",
            "        )",
            "        if litellm.turn_off_message_logging or litellm.redact_messages_in_exceptions:",
            "            messages = \"Message not logged. litellm.redact_messages_in_exceptions=True\"",
            "        request_info = f\"\\nRequest Model: `{model}`\\nAPI Base: `{api_base}`\\nMessages: `{messages}`\"",
            "        slow_message = f\"`Responses are slow - {round(time_difference_float,2)}s response time > Alerting threshold: {self.alerting_threshold}s`\"",
            "        alerting_metadata: dict = {}",
            "        if time_difference_float > self.alerting_threshold:",
            "            # add deployment latencies to alert",
            "            if (",
            "                kwargs is not None",
            "                and \"litellm_params\" in kwargs",
            "                and \"metadata\" in kwargs[\"litellm_params\"]",
            "            ):",
            "                _metadata: dict = kwargs[\"litellm_params\"][\"metadata\"]",
            "                request_info = litellm.utils._add_key_name_and_team_to_alert(",
            "                    request_info=request_info, metadata=_metadata",
            "                )",
            "",
            "                _deployment_latency_map = self._get_deployment_latencies_to_alert(",
            "                    metadata=_metadata",
            "                )",
            "                if _deployment_latency_map is not None:",
            "                    request_info += (",
            "                        f\"\\nAvailable Deployment Latencies\\n{_deployment_latency_map}\"",
            "                    )",
            "",
            "                if \"alerting_metadata\" in _metadata:",
            "                    alerting_metadata = _metadata[\"alerting_metadata\"]",
            "            await self.send_alert(",
            "                message=slow_message + request_info,",
            "                level=\"Low\",",
            "                alert_type=\"llm_too_slow\",",
            "                alerting_metadata=alerting_metadata,",
            "            )",
            "",
            "    async def async_update_daily_reports(",
            "        self, deployment_metrics: DeploymentMetrics",
            "    ) -> int:",
            "        \"\"\"",
            "        Store the perf by deployment in cache",
            "        - Number of failed requests per deployment",
            "        - Latency / output tokens per deployment",
            "",
            "        'deployment_id:daily_metrics:failed_requests'",
            "        'deployment_id:daily_metrics:latency_per_output_token'",
            "",
            "        Returns",
            "            int - count of metrics set (1 - if just latency, 2 - if failed + latency)",
            "        \"\"\"",
            "",
            "        return_val = 0",
            "        try:",
            "            ## FAILED REQUESTS ##",
            "            if deployment_metrics.failed_request:",
            "                await self.internal_usage_cache.async_increment_cache(",
            "                    key=\"{}:{}\".format(",
            "                        deployment_metrics.id,",
            "                        SlackAlertingCacheKeys.failed_requests_key.value,",
            "                    ),",
            "                    value=1,",
            "                )",
            "",
            "                return_val += 1",
            "",
            "            ## LATENCY ##",
            "            if deployment_metrics.latency_per_output_token is not None:",
            "                await self.internal_usage_cache.async_increment_cache(",
            "                    key=\"{}:{}\".format(",
            "                        deployment_metrics.id, SlackAlertingCacheKeys.latency_key.value",
            "                    ),",
            "                    value=deployment_metrics.latency_per_output_token,",
            "                )",
            "",
            "                return_val += 1",
            "",
            "            return return_val",
            "        except Exception as e:",
            "            return 0",
            "",
            "    async def send_daily_reports(self, router) -> bool:",
            "        \"\"\"",
            "        Send a daily report on:",
            "        - Top 5 deployments with most failed requests",
            "        - Top 5 slowest deployments (normalized by latency/output tokens)",
            "",
            "        Get the value from redis cache (if available) or in-memory and send it",
            "",
            "        Cleanup:",
            "        - reset values in cache -> prevent memory leak",
            "",
            "        Returns:",
            "            True -> if successfuly sent",
            "            False -> if not sent",
            "        \"\"\"",
            "",
            "        ids = router.get_model_ids()",
            "",
            "        # get keys",
            "        failed_request_keys = [",
            "            \"{}:{}\".format(id, SlackAlertingCacheKeys.failed_requests_key.value)",
            "            for id in ids",
            "        ]",
            "        latency_keys = [",
            "            \"{}:{}\".format(id, SlackAlertingCacheKeys.latency_key.value) for id in ids",
            "        ]",
            "",
            "        combined_metrics_keys = failed_request_keys + latency_keys  # reduce cache calls",
            "",
            "        combined_metrics_values = await self.internal_usage_cache.async_batch_get_cache(",
            "            keys=combined_metrics_keys",
            "        )  # [1, 2, None, ..]",
            "",
            "        if combined_metrics_values is None:",
            "            return False",
            "",
            "        all_none = True",
            "        for val in combined_metrics_values:",
            "            if val is not None and val > 0:",
            "                all_none = False",
            "                break",
            "",
            "        if all_none:",
            "            return False",
            "",
            "        failed_request_values = combined_metrics_values[",
            "            : len(failed_request_keys)",
            "        ]  # # [1, 2, None, ..]",
            "        latency_values = combined_metrics_values[len(failed_request_keys) :]",
            "",
            "        # find top 5 failed",
            "        ## Replace None values with a placeholder value (-1 in this case)",
            "        placeholder_value = 0",
            "        replaced_failed_values = [",
            "            value if value is not None else placeholder_value",
            "            for value in failed_request_values",
            "        ]",
            "",
            "        ## Get the indices of top 5 keys with the highest numerical values (ignoring None and 0 values)",
            "        top_5_failed = sorted(",
            "            range(len(replaced_failed_values)),",
            "            key=lambda i: replaced_failed_values[i],",
            "            reverse=True,",
            "        )[:5]",
            "        top_5_failed = [",
            "            index for index in top_5_failed if replaced_failed_values[index] > 0",
            "        ]",
            "",
            "        # find top 5 slowest",
            "        # Replace None values with a placeholder value (-1 in this case)",
            "        placeholder_value = 0",
            "        replaced_slowest_values = [",
            "            value if value is not None else placeholder_value",
            "            for value in latency_values",
            "        ]",
            "",
            "        # Get the indices of top 5 values with the highest numerical values (ignoring None and 0 values)",
            "        top_5_slowest = sorted(",
            "            range(len(replaced_slowest_values)),",
            "            key=lambda i: replaced_slowest_values[i],",
            "            reverse=True,",
            "        )[:5]",
            "        top_5_slowest = [",
            "            index for index in top_5_slowest if replaced_slowest_values[index] > 0",
            "        ]",
            "",
            "        # format alert -> return the litellm model name + api base",
            "        message = f\"\\n\\nTime: `{time.time()}`s\\nHere are today's key metrics \ud83d\udcc8: \\n\\n\"",
            "",
            "        message += \"\\n\\n*\u2757\ufe0f Top Deployments with Most Failed Requests:*\\n\\n\"",
            "        if not top_5_failed:",
            "            message += \"\\tNone\\n\"",
            "        for i in range(len(top_5_failed)):",
            "            key = failed_request_keys[top_5_failed[i]].split(\":\")[0]",
            "            _deployment = router.get_model_info(key)",
            "            if isinstance(_deployment, dict):",
            "                deployment_name = _deployment[\"litellm_params\"].get(\"model\", \"\")",
            "            else:",
            "                return False",
            "",
            "            api_base = litellm.get_api_base(",
            "                model=deployment_name,",
            "                optional_params=(",
            "                    _deployment[\"litellm_params\"] if _deployment is not None else {}",
            "                ),",
            "            )",
            "            if api_base is None:",
            "                api_base = \"\"",
            "            value = replaced_failed_values[top_5_failed[i]]",
            "            message += f\"\\t{i+1}. Deployment: `{deployment_name}`, Failed Requests: `{value}`,  API Base: `{api_base}`\\n\"",
            "",
            "        message += \"\\n\\n*\ud83d\ude05 Top Slowest Deployments:*\\n\\n\"",
            "        if not top_5_slowest:",
            "            message += \"\\tNone\\n\"",
            "        for i in range(len(top_5_slowest)):",
            "            key = latency_keys[top_5_slowest[i]].split(\":\")[0]",
            "            _deployment = router.get_model_info(key)",
            "            if _deployment is not None:",
            "                deployment_name = _deployment[\"litellm_params\"].get(\"model\", \"\")",
            "            else:",
            "                deployment_name = \"\"",
            "            api_base = litellm.get_api_base(",
            "                model=deployment_name,",
            "                optional_params=(",
            "                    _deployment[\"litellm_params\"] if _deployment is not None else {}",
            "                ),",
            "            )",
            "            value = round(replaced_slowest_values[top_5_slowest[i]], 3)",
            "            message += f\"\\t{i+1}. Deployment: `{deployment_name}`, Latency per output token: `{value}s/token`,  API Base: `{api_base}`\\n\\n\"",
            "",
            "        # cache cleanup -> reset values to 0",
            "        latency_cache_keys = [(key, 0) for key in latency_keys]",
            "        failed_request_cache_keys = [(key, 0) for key in failed_request_keys]",
            "        combined_metrics_cache_keys = latency_cache_keys + failed_request_cache_keys",
            "        await self.internal_usage_cache.async_batch_set_cache(",
            "            cache_list=combined_metrics_cache_keys",
            "        )",
            "",
            "        message += f\"\\n\\nNext Run is at: `{time.time() + self.alerting_args.daily_report_frequency}`s\"",
            "",
            "        # send alert",
            "        await self.send_alert(",
            "            message=message,",
            "            level=\"Low\",",
            "            alert_type=\"daily_reports\",",
            "            alerting_metadata={},",
            "        )",
            "",
            "        return True",
            "",
            "    async def response_taking_too_long(",
            "        self,",
            "        start_time: Optional[datetime.datetime] = None,",
            "        end_time: Optional[datetime.datetime] = None,",
            "        type: Literal[\"hanging_request\", \"slow_response\"] = \"hanging_request\",",
            "        request_data: Optional[dict] = None,",
            "    ):",
            "        if self.alerting is None or self.alert_types is None:",
            "            return",
            "        if request_data is not None:",
            "            model = request_data.get(\"model\", \"\")",
            "            messages = request_data.get(\"messages\", None)",
            "            if messages is None:",
            "                # if messages does not exist fallback to \"input\"",
            "                messages = request_data.get(\"input\", None)",
            "",
            "            # try casting messages to str and get the first 100 characters, else mark as None",
            "            try:",
            "                messages = str(messages)",
            "                messages = messages[:100]",
            "            except:",
            "                messages = \"\"",
            "",
            "            if (",
            "                litellm.turn_off_message_logging",
            "                or litellm.redact_messages_in_exceptions",
            "            ):",
            "                messages = (",
            "                    \"Message not logged. litellm.redact_messages_in_exceptions=True\"",
            "                )",
            "            request_info = f\"\\nRequest Model: `{model}`\\nMessages: `{messages}`\"",
            "        else:",
            "            request_info = \"\"",
            "",
            "        if type == \"hanging_request\":",
            "            await asyncio.sleep(",
            "                self.alerting_threshold",
            "            )  # Set it to 5 minutes - i'd imagine this might be different for streaming, non-streaming, non-completion (embedding + img) requests",
            "            alerting_metadata: dict = {}",
            "            if (",
            "                request_data is not None",
            "                and request_data.get(\"litellm_status\", \"\") != \"success\"",
            "                and request_data.get(\"litellm_status\", \"\") != \"fail\"",
            "            ):",
            "                if request_data.get(\"deployment\", None) is not None and isinstance(",
            "                    request_data[\"deployment\"], dict",
            "                ):",
            "                    _api_base = litellm.get_api_base(",
            "                        model=model,",
            "                        optional_params=request_data[\"deployment\"].get(",
            "                            \"litellm_params\", {}",
            "                        ),",
            "                    )",
            "",
            "                    if _api_base is None:",
            "                        _api_base = \"\"",
            "",
            "                    request_info += f\"\\nAPI Base: {_api_base}\"",
            "                elif request_data.get(\"metadata\", None) is not None and isinstance(",
            "                    request_data[\"metadata\"], dict",
            "                ):",
            "                    # In hanging requests sometime it has not made it to the point where the deployment is passed to the `request_data``",
            "                    # in that case we fallback to the api base set in the request metadata",
            "                    _metadata: dict = request_data[\"metadata\"]",
            "                    _api_base = _metadata.get(\"api_base\", \"\")",
            "",
            "                    request_info = litellm.utils._add_key_name_and_team_to_alert(",
            "                        request_info=request_info, metadata=_metadata",
            "                    )",
            "",
            "                    if _api_base is None:",
            "                        _api_base = \"\"",
            "",
            "                    if \"alerting_metadata\" in _metadata:",
            "                        alerting_metadata = _metadata[\"alerting_metadata\"]",
            "                    request_info += f\"\\nAPI Base: `{_api_base}`\"",
            "                # only alert hanging responses if they have not been marked as success",
            "                alerting_message = (",
            "                    f\"`Requests are hanging - {self.alerting_threshold}s+ request time`\"",
            "                )",
            "",
            "                if \"langfuse\" in litellm.success_callback:",
            "                    langfuse_url = self._add_langfuse_trace_id_to_alert(",
            "                        request_data=request_data,",
            "                    )",
            "",
            "                    if langfuse_url is not None:",
            "                        request_info += \"\\n\ud83e\udea2 Langfuse Trace: {}\".format(langfuse_url)",
            "",
            "                # add deployment latencies to alert",
            "                _deployment_latency_map = self._get_deployment_latencies_to_alert(",
            "                    metadata=request_data.get(\"metadata\", {})",
            "                )",
            "                if _deployment_latency_map is not None:",
            "                    request_info += f\"\\nDeployment Latencies\\n{_deployment_latency_map}\"",
            "",
            "                await self.send_alert(",
            "                    message=alerting_message + request_info,",
            "                    level=\"Medium\",",
            "                    alert_type=\"llm_requests_hanging\",",
            "                    alerting_metadata=alerting_metadata,",
            "                )",
            "",
            "    async def failed_tracking_alert(self, error_message: str):",
            "        \"\"\"Raise alert when tracking failed for specific model\"\"\"",
            "        _cache: DualCache = self.internal_usage_cache",
            "        message = \"Failed Tracking Cost for\" + error_message",
            "        _cache_key = \"budget_alerts:failed_tracking:{}\".format(message)",
            "        result = await _cache.async_get_cache(key=_cache_key)",
            "        if result is None:",
            "            await self.send_alert(",
            "                message=message,",
            "                level=\"High\",",
            "                alert_type=\"budget_alerts\",",
            "                alerting_metadata={},",
            "            )",
            "            await _cache.async_set_cache(",
            "                key=_cache_key,",
            "                value=\"SENT\",",
            "                ttl=self.alerting_args.budget_alert_ttl,",
            "            )",
            "",
            "    async def budget_alerts(",
            "        self,",
            "        type: Literal[",
            "            \"token_budget\",",
            "            \"user_budget\",",
            "            \"team_budget\",",
            "            \"proxy_budget\",",
            "            \"projected_limit_exceeded\",",
            "        ],",
            "        user_info: CallInfo,",
            "    ):",
            "        ## PREVENTITIVE ALERTING ## - https://github.com/BerriAI/litellm/issues/2727",
            "        # - Alert once within 24hr period",
            "        # - Cache this information",
            "        # - Don't re-alert, if alert already sent",
            "        _cache: DualCache = self.internal_usage_cache",
            "",
            "        if self.alerting is None or self.alert_types is None:",
            "            # do nothing if alerting is not switched on",
            "            return",
            "        if \"budget_alerts\" not in self.alert_types:",
            "            return",
            "        _id: str = \"default_id\"  # used for caching",
            "        user_info_json = user_info.model_dump(exclude_none=True)",
            "        for k, v in user_info_json.items():",
            "            user_info_str = \"\\n{}: {}\\n\".format(k, v)",
            "",
            "        event: Optional[",
            "            Literal[\"budget_crossed\", \"threshold_crossed\", \"projected_limit_exceeded\"]",
            "        ] = None",
            "        event_group: Optional[",
            "            Literal[\"internal_user\", \"team\", \"key\", \"proxy\", \"customer\"]",
            "        ] = None",
            "        event_message: str = \"\"",
            "        webhook_event: Optional[WebhookEvent] = None",
            "        if type == \"proxy_budget\":",
            "            event_group = \"proxy\"",
            "            event_message += \"Proxy Budget: \"",
            "        elif type == \"user_budget\":",
            "            event_group = \"internal_user\"",
            "            event_message += \"User Budget: \"",
            "            _id = user_info.user_id or _id",
            "        elif type == \"team_budget\":",
            "            event_group = \"team\"",
            "            event_message += \"Team Budget: \"",
            "            _id = user_info.team_id or _id",
            "        elif type == \"token_budget\":",
            "            event_group = \"key\"",
            "            event_message += \"Key Budget: \"",
            "            _id = user_info.token",
            "        elif type == \"projected_limit_exceeded\":",
            "            event_group = \"key\"",
            "            event_message += \"Key Budget: Projected Limit Exceeded\"",
            "            event = \"projected_limit_exceeded\"",
            "            _id = user_info.token",
            "",
            "        # percent of max_budget left to spend",
            "        if user_info.max_budget is None:",
            "            return",
            "",
            "        if user_info.max_budget > 0:",
            "            percent_left = (",
            "                user_info.max_budget - user_info.spend",
            "            ) / user_info.max_budget",
            "        else:",
            "            percent_left = 0",
            "",
            "        # check if crossed budget",
            "        if user_info.spend >= user_info.max_budget:",
            "            event = \"budget_crossed\"",
            "            event_message += f\"Budget Crossed\\n Total Budget:`{user_info.max_budget}`\"",
            "        elif percent_left <= 0.05:",
            "            event = \"threshold_crossed\"",
            "            event_message += \"5% Threshold Crossed \"",
            "        elif percent_left <= 0.15:",
            "            event = \"threshold_crossed\"",
            "            event_message += \"15% Threshold Crossed\"",
            "",
            "        if event is not None and event_group is not None:",
            "            _cache_key = \"budget_alerts:{}:{}\".format(event, _id)",
            "            result = await _cache.async_get_cache(key=_cache_key)",
            "            if result is None:",
            "                webhook_event = WebhookEvent(",
            "                    event=event,",
            "                    event_group=event_group,",
            "                    event_message=event_message,",
            "                    **user_info_json,",
            "                )",
            "                await self.send_alert(",
            "                    message=event_message + \"\\n\\n\" + user_info_str,",
            "                    level=\"High\",",
            "                    alert_type=\"budget_alerts\",",
            "                    user_info=webhook_event,",
            "                    alerting_metadata={},",
            "                )",
            "                await _cache.async_set_cache(",
            "                    key=_cache_key,",
            "                    value=\"SENT\",",
            "                    ttl=self.alerting_args.budget_alert_ttl,",
            "                )",
            "",
            "            return",
            "        return",
            "",
            "    async def customer_spend_alert(",
            "        self,",
            "        token: Optional[str],",
            "        key_alias: Optional[str],",
            "        end_user_id: Optional[str],",
            "        response_cost: Optional[float],",
            "        max_budget: Optional[float],",
            "    ):",
            "        if (",
            "            self.alerting is not None",
            "            and \"webhook\" in self.alerting",
            "            and end_user_id is not None",
            "            and token is not None",
            "            and response_cost is not None",
            "        ):",
            "            # log customer spend",
            "            event = WebhookEvent(",
            "                spend=response_cost,",
            "                max_budget=max_budget,",
            "                token=token,",
            "                customer_id=end_user_id,",
            "                user_id=None,",
            "                team_id=None,",
            "                user_email=None,",
            "                key_alias=key_alias,",
            "                projected_exceeded_date=None,",
            "                projected_spend=None,",
            "                event=\"spend_tracked\",",
            "                event_group=\"customer\",",
            "                event_message=\"Customer spend tracked. Customer={}, spend={}\".format(",
            "                    end_user_id, response_cost",
            "                ),",
            "            )",
            "",
            "            await self.send_webhook_alert(webhook_event=event)",
            "",
            "    def _count_outage_alerts(self, alerts: List[int]) -> str:",
            "        \"\"\"",
            "        Parameters:",
            "        - alerts: List[int] -> list of error codes (either 408 or 500+)",
            "",
            "        Returns:",
            "        - str -> formatted string. This is an alert message, giving a human-friendly description of the errors.",
            "        \"\"\"",
            "        error_breakdown = {\"Timeout Errors\": 0, \"API Errors\": 0, \"Unknown Errors\": 0}",
            "        for alert in alerts:",
            "            if alert == 408:",
            "                error_breakdown[\"Timeout Errors\"] += 1",
            "            elif alert >= 500:",
            "                error_breakdown[\"API Errors\"] += 1",
            "            else:",
            "                error_breakdown[\"Unknown Errors\"] += 1",
            "",
            "        error_msg = \"\"",
            "        for key, value in error_breakdown.items():",
            "            if value > 0:",
            "                error_msg += \"\\n{}: {}\\n\".format(key, value)",
            "",
            "        return error_msg",
            "",
            "    def _outage_alert_msg_factory(",
            "        self,",
            "        alert_type: Literal[\"Major\", \"Minor\"],",
            "        key: Literal[\"Model\", \"Region\"],",
            "        key_val: str,",
            "        provider: str,",
            "        api_base: Optional[str],",
            "        outage_value: BaseOutageModel,",
            "    ) -> str:",
            "        \"\"\"Format an alert message for slack\"\"\"",
            "        headers = {f\"{key} Name\": key_val, \"Provider\": provider}",
            "        if api_base is not None:",
            "            headers[\"API Base\"] = api_base  # type: ignore",
            "",
            "        headers_str = \"\\n\"",
            "        for k, v in headers.items():",
            "            headers_str += f\"*{k}:* `{v}`\\n\"",
            "        return f\"\"\"\\n\\n",
            "*\u26a0\ufe0f {alert_type} Service Outage*",
            "",
            "{headers_str}",
            "",
            "*Errors:*",
            "{self._count_outage_alerts(alerts=outage_value[\"alerts\"])}",
            "",
            "*Last Check:* `{round(time.time() - outage_value[\"last_updated_at\"], 4)}s ago`\\n\\n",
            "\"\"\"",
            "",
            "    async def region_outage_alerts(",
            "        self,",
            "        exception: APIError,",
            "        deployment_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Send slack alert if specific provider region is having an outage.",
            "",
            "        Track for 408 (Timeout) and >=500 Error codes",
            "        \"\"\"",
            "        ## CREATE (PROVIDER+REGION) ID ##",
            "        if self.llm_router is None:",
            "            return",
            "",
            "        deployment = self.llm_router.get_deployment(model_id=deployment_id)",
            "",
            "        if deployment is None:",
            "            return",
            "",
            "        model = deployment.litellm_params.model",
            "        ### GET PROVIDER ###",
            "        provider = deployment.litellm_params.custom_llm_provider",
            "        if provider is None:",
            "            model, provider, _, _ = litellm.get_llm_provider(model=model)",
            "",
            "        ### GET REGION ###",
            "        region_name = deployment.litellm_params.region_name",
            "        if region_name is None:",
            "            region_name = litellm.utils._get_model_region(",
            "                custom_llm_provider=provider, litellm_params=deployment.litellm_params",
            "            )",
            "",
            "        if region_name is None:",
            "            return",
            "",
            "        ### UNIQUE CACHE KEY ###",
            "        cache_key = provider + region_name",
            "",
            "        outage_value: Optional[ProviderRegionOutageModel] = (",
            "            await self.internal_usage_cache.async_get_cache(key=cache_key)",
            "        )",
            "",
            "        if (",
            "            getattr(exception, \"status_code\", None) is None",
            "            or (",
            "                exception.status_code != 408  # type: ignore",
            "                and exception.status_code < 500  # type: ignore",
            "            )",
            "            or self.llm_router is None",
            "        ):",
            "            return",
            "",
            "        if outage_value is None:",
            "            _deployment_set = set()",
            "            _deployment_set.add(deployment_id)",
            "            outage_value = ProviderRegionOutageModel(",
            "                provider_region_id=cache_key,",
            "                alerts=[exception.status_code],  # type: ignore",
            "                minor_alert_sent=False,",
            "                major_alert_sent=False,",
            "                last_updated_at=time.time(),",
            "                deployment_ids=_deployment_set,",
            "            )",
            "",
            "            ## add to cache ##",
            "            await self.internal_usage_cache.async_set_cache(",
            "                key=cache_key,",
            "                value=outage_value,",
            "                ttl=self.alerting_args.region_outage_alert_ttl,",
            "            )",
            "            return",
            "",
            "        if len(outage_value[\"alerts\"]) < self.alerting_args.max_outage_alert_list_size:",
            "            outage_value[\"alerts\"].append(exception.status_code)  # type: ignore",
            "        else:  # prevent memory leaks",
            "            pass",
            "        _deployment_set = outage_value[\"deployment_ids\"]",
            "        _deployment_set.add(deployment_id)",
            "        outage_value[\"deployment_ids\"] = _deployment_set",
            "        outage_value[\"last_updated_at\"] = time.time()",
            "",
            "        ## MINOR OUTAGE ALERT SENT ##",
            "        if (",
            "            outage_value[\"minor_alert_sent\"] == False",
            "            and len(outage_value[\"alerts\"])",
            "            >= self.alerting_args.minor_outage_alert_threshold",
            "            and len(_deployment_set) > 1  # make sure it's not just 1 bad deployment",
            "        ):",
            "            msg = self._outage_alert_msg_factory(",
            "                alert_type=\"Minor\",",
            "                key=\"Region\",",
            "                key_val=region_name,",
            "                api_base=None,",
            "                outage_value=outage_value,",
            "                provider=provider,",
            "            )",
            "            # send minor alert",
            "            await self.send_alert(",
            "                message=msg,",
            "                level=\"Medium\",",
            "                alert_type=\"outage_alerts\",",
            "                alerting_metadata={},",
            "            )",
            "            # set to true",
            "            outage_value[\"minor_alert_sent\"] = True",
            "",
            "        ## MAJOR OUTAGE ALERT SENT ##",
            "        elif (",
            "            outage_value[\"major_alert_sent\"] == False",
            "            and len(outage_value[\"alerts\"])",
            "            >= self.alerting_args.major_outage_alert_threshold",
            "            and len(_deployment_set) > 1  # make sure it's not just 1 bad deployment",
            "        ):",
            "            msg = self._outage_alert_msg_factory(",
            "                alert_type=\"Major\",",
            "                key=\"Region\",",
            "                key_val=region_name,",
            "                api_base=None,",
            "                outage_value=outage_value,",
            "                provider=provider,",
            "            )",
            "",
            "            # send minor alert",
            "            await self.send_alert(",
            "                message=msg,",
            "                level=\"High\",",
            "                alert_type=\"outage_alerts\",",
            "                alerting_metadata={},",
            "            )",
            "            # set to true",
            "            outage_value[\"major_alert_sent\"] = True",
            "",
            "        ## update cache ##",
            "        await self.internal_usage_cache.async_set_cache(",
            "            key=cache_key, value=outage_value",
            "        )",
            "",
            "    async def outage_alerts(",
            "        self,",
            "        exception: APIError,",
            "        deployment_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Send slack alert if model is badly configured / having an outage (408, 401, 429, >=500).",
            "",
            "        key = model_id",
            "",
            "        value = {",
            "        - model_id",
            "        - threshold",
            "        - alerts []",
            "        }",
            "",
            "        ttl = 1hr",
            "        max_alerts_size = 10",
            "        \"\"\"",
            "        try:",
            "            outage_value: Optional[OutageModel] = await self.internal_usage_cache.async_get_cache(key=deployment_id)  # type: ignore",
            "            if (",
            "                getattr(exception, \"status_code\", None) is None",
            "                or (",
            "                    exception.status_code != 408  # type: ignore",
            "                    and exception.status_code < 500  # type: ignore",
            "                )",
            "                or self.llm_router is None",
            "            ):",
            "                return",
            "",
            "            ### EXTRACT MODEL DETAILS ###",
            "            deployment = self.llm_router.get_deployment(model_id=deployment_id)",
            "            if deployment is None:",
            "                return",
            "",
            "            model = deployment.litellm_params.model",
            "            provider = deployment.litellm_params.custom_llm_provider",
            "            if provider is None:",
            "                try:",
            "                    model, provider, _, _ = litellm.get_llm_provider(model=model)",
            "                except Exception as e:",
            "                    provider = \"\"",
            "            api_base = litellm.get_api_base(",
            "                model=model, optional_params=deployment.litellm_params",
            "            )",
            "",
            "            if outage_value is None:",
            "                outage_value = OutageModel(",
            "                    model_id=deployment_id,",
            "                    alerts=[exception.status_code],  # type: ignore",
            "                    minor_alert_sent=False,",
            "                    major_alert_sent=False,",
            "                    last_updated_at=time.time(),",
            "                )",
            "",
            "                ## add to cache ##",
            "                await self.internal_usage_cache.async_set_cache(",
            "                    key=deployment_id,",
            "                    value=outage_value,",
            "                    ttl=self.alerting_args.outage_alert_ttl,",
            "                )",
            "                return",
            "",
            "            if (",
            "                len(outage_value[\"alerts\"])",
            "                < self.alerting_args.max_outage_alert_list_size",
            "            ):",
            "                outage_value[\"alerts\"].append(exception.status_code)  # type: ignore",
            "            else:  # prevent memory leaks",
            "                pass",
            "",
            "            outage_value[\"last_updated_at\"] = time.time()",
            "",
            "            ## MINOR OUTAGE ALERT SENT ##",
            "            if (",
            "                outage_value[\"minor_alert_sent\"] == False",
            "                and len(outage_value[\"alerts\"])",
            "                >= self.alerting_args.minor_outage_alert_threshold",
            "            ):",
            "                msg = self._outage_alert_msg_factory(",
            "                    alert_type=\"Minor\",",
            "                    key=\"Model\",",
            "                    key_val=model,",
            "                    api_base=api_base,",
            "                    outage_value=outage_value,",
            "                    provider=provider,",
            "                )",
            "                # send minor alert",
            "                await self.send_alert(",
            "                    message=msg,",
            "                    level=\"Medium\",",
            "                    alert_type=\"outage_alerts\",",
            "                    alerting_metadata={},",
            "                )",
            "                # set to true",
            "                outage_value[\"minor_alert_sent\"] = True",
            "            elif (",
            "                outage_value[\"major_alert_sent\"] == False",
            "                and len(outage_value[\"alerts\"])",
            "                >= self.alerting_args.major_outage_alert_threshold",
            "            ):",
            "                msg = self._outage_alert_msg_factory(",
            "                    alert_type=\"Major\",",
            "                    key=\"Model\",",
            "                    key_val=model,",
            "                    api_base=api_base,",
            "                    outage_value=outage_value,",
            "                    provider=provider,",
            "                )",
            "                # send minor alert",
            "                await self.send_alert(",
            "                    message=msg,",
            "                    level=\"High\",",
            "                    alert_type=\"outage_alerts\",",
            "                    alerting_metadata={},",
            "                )",
            "                # set to true",
            "                outage_value[\"major_alert_sent\"] = True",
            "",
            "            ## update cache ##",
            "            await self.internal_usage_cache.async_set_cache(",
            "                key=deployment_id, value=outage_value",
            "            )",
            "        except Exception as e:",
            "            pass",
            "",
            "    async def model_added_alert(",
            "        self, model_name: str, litellm_model_name: str, passed_model_info: Any",
            "    ):",
            "        base_model_from_user = getattr(passed_model_info, \"base_model\", None)",
            "        model_info = {}",
            "        base_model = \"\"",
            "        if base_model_from_user is not None:",
            "            model_info = litellm.model_cost.get(base_model_from_user, {})",
            "            base_model = f\"Base Model: `{base_model_from_user}`\\n\"",
            "        else:",
            "            model_info = litellm.model_cost.get(litellm_model_name, {})",
            "        model_info_str = \"\"",
            "        for k, v in model_info.items():",
            "            if k == \"input_cost_per_token\" or k == \"output_cost_per_token\":",
            "                # when converting to string it should not be 1.63e-06",
            "                v = \"{:.8f}\".format(v)",
            "",
            "            model_info_str += f\"{k}: {v}\\n\"",
            "",
            "        message = f\"\"\"",
            "*\ud83d\ude85 New Model Added*",
            "Model Name: `{model_name}`",
            "{base_model}",
            "",
            "Usage OpenAI Python SDK:",
            "```",
            "import openai",
            "client = openai.OpenAI(",
            "    api_key=\"your_api_key\",",
            "    base_url={os.getenv(\"PROXY_BASE_URL\", \"http://0.0.0.0:4000\")}",
            ")",
            "",
            "response = client.chat.completions.create(",
            "    model=\"{model_name}\", # model to send to the proxy",
            "    messages = [",
            "        {{",
            "            \"role\": \"user\",",
            "            \"content\": \"this is a test request, write a short poem\"",
            "        }}",
            "    ]",
            ")",
            "```",
            "",
            "Model Info: ",
            "```",
            "{model_info_str}",
            "```",
            "\"\"\"",
            "",
            "        alert_val = self.send_alert(",
            "            message=message,",
            "            level=\"Low\",",
            "            alert_type=\"new_model_added\",",
            "            alerting_metadata={},",
            "        )",
            "",
            "        if alert_val is not None and asyncio.iscoroutine(alert_val):",
            "            await alert_val",
            "",
            "    async def model_removed_alert(self, model_name: str):",
            "        pass",
            "",
            "    async def send_webhook_alert(self, webhook_event: WebhookEvent) -> bool:",
            "        \"\"\"",
            "        Sends structured alert to webhook, if set.",
            "",
            "        Currently only implemented for budget alerts",
            "",
            "        Returns -> True if sent, False if not.",
            "",
            "        Raises Exception",
            "            - if WEBHOOK_URL is not set",
            "        \"\"\"",
            "",
            "        webhook_url = os.getenv(\"WEBHOOK_URL\", None)",
            "        if webhook_url is None:",
            "            raise Exception(\"Missing webhook_url from environment\")",
            "",
            "        payload = webhook_event.model_dump_json()",
            "        headers = {\"Content-type\": \"application/json\"}",
            "",
            "        response = await self.async_http_handler.post(",
            "            url=webhook_url,",
            "            headers=headers,",
            "            data=payload,",
            "        )",
            "        if response.status_code == 200:",
            "            return True",
            "        else:",
            "            print(\"Error sending webhook alert. Error=\", response.text)  # noqa",
            "",
            "        return False",
            "",
            "    async def _check_if_using_premium_email_feature(",
            "        self,",
            "        premium_user: bool,",
            "        email_logo_url: Optional[str] = None,",
            "        email_support_contact: Optional[str] = None,",
            "    ):",
            "        from litellm.proxy.proxy_server import premium_user",
            "        from litellm.proxy.proxy_server import CommonProxyErrors",
            "",
            "        if premium_user is not True:",
            "            if email_logo_url is not None or email_support_contact is not None:",
            "                raise ValueError(",
            "                    f\"Trying to Customize Email Alerting\\n {CommonProxyErrors.not_premium_user.value}\"",
            "                )",
            "        return",
            "",
            "    async def send_key_created_or_user_invited_email(",
            "        self, webhook_event: WebhookEvent",
            "    ) -> bool:",
            "        try:",
            "            from litellm.proxy.utils import send_email",
            "",
            "            if self.alerting is None or \"email\" not in self.alerting:",
            "                # do nothing if user does not want email alerts",
            "                return False",
            "            from litellm.proxy.proxy_server import premium_user, prisma_client",
            "",
            "            email_logo_url = os.getenv(",
            "                \"SMTP_SENDER_LOGO\", os.getenv(\"EMAIL_LOGO_URL\", None)",
            "            )",
            "            email_support_contact = os.getenv(\"EMAIL_SUPPORT_CONTACT\", None)",
            "            await self._check_if_using_premium_email_feature(",
            "                premium_user, email_logo_url, email_support_contact",
            "            )",
            "            if email_logo_url is None:",
            "                email_logo_url = LITELLM_LOGO_URL",
            "            if email_support_contact is None:",
            "                email_support_contact = LITELLM_SUPPORT_CONTACT",
            "",
            "            event_name = webhook_event.event_message",
            "            recipient_email = webhook_event.user_email",
            "            recipient_user_id = webhook_event.user_id",
            "            if (",
            "                recipient_email is None",
            "                and recipient_user_id is not None",
            "                and prisma_client is not None",
            "            ):",
            "                user_row = await prisma_client.db.litellm_usertable.find_unique(",
            "                    where={\"user_id\": recipient_user_id}",
            "                )",
            "",
            "                if user_row is not None:",
            "                    recipient_email = user_row.user_email",
            "",
            "            key_name = webhook_event.key_alias",
            "            key_token = webhook_event.token",
            "            key_budget = webhook_event.max_budget",
            "            base_url = os.getenv(\"PROXY_BASE_URL\", \"http://0.0.0.0:4000\")",
            "",
            "            email_html_content = \"Alert from LiteLLM Server\"",
            "            if recipient_email is None:",
            "                verbose_proxy_logger.error(",
            "                    \"Trying to send email alert to no recipient\",",
            "                    extra=webhook_event.dict(),",
            "                )",
            "",
            "            if webhook_event.event == \"key_created\":",
            "                email_html_content = KEY_CREATED_EMAIL_TEMPLATE.format(",
            "                    email_logo_url=email_logo_url,",
            "                    recipient_email=recipient_email,",
            "                    key_budget=key_budget,",
            "                    key_token=key_token,",
            "                    base_url=base_url,",
            "                    email_support_contact=email_support_contact,",
            "                )",
            "            elif webhook_event.event == \"internal_user_created\":",
            "                # GET TEAM NAME",
            "                team_id = webhook_event.team_id",
            "                team_name = \"Default Team\"",
            "                if team_id is not None and prisma_client is not None:",
            "                    team_row = await prisma_client.db.litellm_teamtable.find_unique(",
            "                        where={\"team_id\": team_id}",
            "                    )",
            "                    if team_row is not None:",
            "                        team_name = team_row.team_alias or \"-\"",
            "                email_html_content = USER_INVITED_EMAIL_TEMPLATE.format(",
            "                    email_logo_url=email_logo_url,",
            "                    recipient_email=recipient_email,",
            "                    team_name=team_name,",
            "                    base_url=base_url,",
            "                    email_support_contact=email_support_contact,",
            "                )",
            "            else:",
            "                verbose_proxy_logger.error(",
            "                    \"Trying to send email alert on unknown webhook event\",",
            "                    extra=webhook_event.model_dump(),",
            "                )",
            "",
            "            payload = webhook_event.model_dump_json()",
            "            email_event = {",
            "                \"to\": recipient_email,",
            "                \"subject\": f\"LiteLLM: {event_name}\",",
            "                \"html\": email_html_content,",
            "            }",
            "",
            "            response = await send_email(",
            "                receiver_email=email_event[\"to\"],",
            "                subject=email_event[\"subject\"],",
            "                html=email_event[\"html\"],",
            "            )",
            "",
            "            return True",
            "",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(\"Error sending email alert %s\", str(e))",
            "            return False",
            "",
            "    async def send_email_alert_using_smtp(",
            "        self, webhook_event: WebhookEvent, alert_type: str",
            "    ) -> bool:",
            "        \"\"\"",
            "        Sends structured Email alert to an SMTP server",
            "",
            "        Currently only implemented for budget alerts",
            "",
            "        Returns -> True if sent, False if not.",
            "        \"\"\"",
            "        from litellm.proxy.utils import send_email",
            "        from litellm.proxy.proxy_server import premium_user, prisma_client",
            "",
            "        email_logo_url = os.getenv(",
            "            \"SMTP_SENDER_LOGO\", os.getenv(\"EMAIL_LOGO_URL\", None)",
            "        )",
            "        email_support_contact = os.getenv(\"EMAIL_SUPPORT_CONTACT\", None)",
            "        await self._check_if_using_premium_email_feature(",
            "            premium_user, email_logo_url, email_support_contact",
            "        )",
            "",
            "        if email_logo_url is None:",
            "            email_logo_url = LITELLM_LOGO_URL",
            "        if email_support_contact is None:",
            "            email_support_contact = LITELLM_SUPPORT_CONTACT",
            "",
            "        event_name = webhook_event.event_message",
            "        recipient_email = webhook_event.user_email",
            "        user_name = webhook_event.user_id",
            "        max_budget = webhook_event.max_budget",
            "        email_html_content = \"Alert from LiteLLM Server\"",
            "        if recipient_email is None:",
            "            verbose_proxy_logger.error(",
            "                \"Trying to send email alert to no recipient\", extra=webhook_event.dict()",
            "            )",
            "",
            "        if webhook_event.event == \"budget_crossed\":",
            "            email_html_content = f\"\"\"",
            "            <img src=\"{email_logo_url}\" alt=\"LiteLLM Logo\" width=\"150\" height=\"50\" />",
            "",
            "            <p> Hi {user_name}, <br/>",
            "",
            "            Your LLM API usage this month has reached your account's <b> monthly budget of ${max_budget} </b> <br /> <br />",
            "",
            "            API requests will be rejected until either (a) you increase your monthly budget or (b) your monthly usage resets at the beginning of the next calendar month. <br /> <br />",
            "",
            "            If you have any questions, please send an email to {email_support_contact} <br /> <br />",
            "",
            "            Best, <br />",
            "            The LiteLLM team <br />",
            "            \"\"\"",
            "",
            "        payload = webhook_event.model_dump_json()",
            "        email_event = {",
            "            \"to\": recipient_email,",
            "            \"subject\": f\"LiteLLM: {event_name}\",",
            "            \"html\": email_html_content,",
            "        }",
            "",
            "        response = await send_email(",
            "            receiver_email=email_event[\"to\"],",
            "            subject=email_event[\"subject\"],",
            "            html=email_event[\"html\"],",
            "        )",
            "        if webhook_event.event_group == \"team\":",
            "            from litellm.integrations.email_alerting import send_team_budget_alert",
            "",
            "            await send_team_budget_alert(webhook_event=webhook_event)",
            "",
            "        return False",
            "",
            "    async def send_alert(",
            "        self,",
            "        message: str,",
            "        level: Literal[\"Low\", \"Medium\", \"High\"],",
            "        alert_type: Literal[AlertType],",
            "        alerting_metadata: dict,",
            "        user_info: Optional[WebhookEvent] = None,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Alerting based on thresholds: - https://github.com/BerriAI/litellm/issues/1298",
            "",
            "        - Responses taking too long",
            "        - Requests are hanging",
            "        - Calls are failing",
            "        - DB Read/Writes are failing",
            "        - Proxy Close to max budget",
            "        - Key Close to max budget",
            "",
            "        Parameters:",
            "            level: str - Low|Medium|High - if calls might fail (Medium) or are failing (High); Currently, no alerts would be 'Low'.",
            "            message: str - what is the alert about",
            "        \"\"\"",
            "        if self.alerting is None:",
            "            return",
            "",
            "        if (",
            "            \"webhook\" in self.alerting",
            "            and alert_type == \"budget_alerts\"",
            "            and user_info is not None",
            "        ):",
            "            await self.send_webhook_alert(webhook_event=user_info)",
            "",
            "        if (",
            "            \"email\" in self.alerting",
            "            and alert_type == \"budget_alerts\"",
            "            and user_info is not None",
            "        ):",
            "            # only send budget alerts over Email",
            "            await self.send_email_alert_using_smtp(",
            "                webhook_event=user_info, alert_type=alert_type",
            "            )",
            "",
            "        if \"slack\" not in self.alerting:",
            "            return",
            "",
            "        if alert_type not in self.alert_types:",
            "            return",
            "",
            "        from datetime import datetime",
            "        import json",
            "",
            "        # Get the current timestamp",
            "        current_time = datetime.now().strftime(\"%H:%M:%S\")",
            "        _proxy_base_url = os.getenv(\"PROXY_BASE_URL\", None)",
            "        if alert_type == \"daily_reports\" or alert_type == \"new_model_added\":",
            "            formatted_message = message",
            "        else:",
            "            formatted_message = (",
            "                f\"Level: `{level}`\\nTimestamp: `{current_time}`\\n\\nMessage: {message}\"",
            "            )",
            "",
            "        if kwargs:",
            "            for key, value in kwargs.items():",
            "                formatted_message += f\"\\n\\n{key}: `{value}`\\n\\n\"",
            "        if alerting_metadata:",
            "            for key, value in alerting_metadata.items():",
            "                formatted_message += f\"\\n\\n*Alerting Metadata*: \\n{key}: `{value}`\\n\\n\"",
            "        if _proxy_base_url is not None:",
            "            formatted_message += f\"\\n\\nProxy URL: `{_proxy_base_url}`\"",
            "",
            "        # check if we find the slack webhook url in self.alert_to_webhook_url",
            "        if (",
            "            self.alert_to_webhook_url is not None",
            "            and alert_type in self.alert_to_webhook_url",
            "        ):",
            "            slack_webhook_url = self.alert_to_webhook_url[alert_type]",
            "        elif self.default_webhook_url is not None:",
            "            slack_webhook_url = self.default_webhook_url",
            "        else:",
            "            slack_webhook_url = os.getenv(\"SLACK_WEBHOOK_URL\", None)",
            "",
            "        if slack_webhook_url is None:",
            "            raise ValueError(\"Missing SLACK_WEBHOOK_URL from environment\")",
            "        payload = {\"text\": formatted_message}",
            "        headers = {\"Content-type\": \"application/json\"}",
            "",
            "        response = await self.async_http_handler.post(",
            "            url=slack_webhook_url,",
            "            headers=headers,",
            "            data=json.dumps(payload),",
            "        )",
            "        if response.status_code == 200:",
            "            pass",
            "        else:",
            "            verbose_proxy_logger.debug(",
            "                \"Error sending slack alert. Error={}\".format(response.text)",
            "            )",
            "",
            "    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):",
            "        \"\"\"Log deployment latency\"\"\"",
            "        try:",
            "            if \"daily_reports\" in self.alert_types:",
            "                model_id = (",
            "                    kwargs.get(\"litellm_params\", {}).get(\"model_info\", {}).get(\"id\", \"\")",
            "                )",
            "                response_s: timedelta = end_time - start_time",
            "",
            "                final_value = response_s",
            "                total_tokens = 0",
            "",
            "                if isinstance(response_obj, litellm.ModelResponse):",
            "                    completion_tokens = response_obj.usage.completion_tokens",
            "                    if completion_tokens is not None and completion_tokens > 0:",
            "                        final_value = float(",
            "                            response_s.total_seconds() / completion_tokens",
            "                        )",
            "                if isinstance(final_value, timedelta):",
            "                    final_value = final_value.total_seconds()",
            "",
            "                await self.async_update_daily_reports(",
            "                    DeploymentMetrics(",
            "                        id=model_id,",
            "                        failed_request=False,",
            "                        latency_per_output_token=final_value,",
            "                        updated_at=litellm.utils.get_utc_datetime(),",
            "                    )",
            "                )",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(",
            "                \"[Non-Blocking Error] Slack Alerting: Got error in logging LLM deployment latency: \",",
            "                e,",
            "            )",
            "            pass",
            "",
            "    async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):",
            "        \"\"\"Log failure + deployment latency\"\"\"",
            "        _litellm_params = kwargs.get(\"litellm_params\", {})",
            "        _model_info = _litellm_params.get(\"model_info\", {}) or {}",
            "        model_id = _model_info.get(\"id\", \"\")",
            "        try:",
            "            if \"daily_reports\" in self.alert_types:",
            "                try:",
            "                    await self.async_update_daily_reports(",
            "                        DeploymentMetrics(",
            "                            id=model_id,",
            "                            failed_request=True,",
            "                            latency_per_output_token=None,",
            "                            updated_at=litellm.utils.get_utc_datetime(),",
            "                        )",
            "                    )",
            "                except Exception as e:",
            "                    verbose_logger.debug(f\"Exception raises -{str(e)}\")",
            "",
            "            if isinstance(kwargs.get(\"exception\", \"\"), APIError):",
            "                if \"outage_alerts\" in self.alert_types:",
            "                    await self.outage_alerts(",
            "                        exception=kwargs[\"exception\"],",
            "                        deployment_id=model_id,",
            "                    )",
            "",
            "                if \"region_outage_alerts\" in self.alert_types:",
            "                    await self.region_outage_alerts(",
            "                        exception=kwargs[\"exception\"], deployment_id=model_id",
            "                    )",
            "        except Exception as e:",
            "            pass",
            "",
            "    async def _run_scheduler_helper(self, llm_router) -> bool:",
            "        \"\"\"",
            "        Returns:",
            "        - True -> report sent",
            "        - False -> report not sent",
            "        \"\"\"",
            "        report_sent_bool = False",
            "",
            "        report_sent = await self.internal_usage_cache.async_get_cache(",
            "            key=SlackAlertingCacheKeys.report_sent_key.value",
            "        )  # None | float",
            "",
            "        current_time = time.time()",
            "",
            "        if report_sent is None:",
            "            await self.internal_usage_cache.async_set_cache(",
            "                key=SlackAlertingCacheKeys.report_sent_key.value,",
            "                value=current_time,",
            "            )",
            "        elif isinstance(report_sent, float):",
            "            # Check if current time - interval >= time last sent",
            "            interval_seconds = self.alerting_args.daily_report_frequency",
            "",
            "            if current_time - report_sent >= interval_seconds:",
            "                # Sneak in the reporting logic here",
            "                await self.send_daily_reports(router=llm_router)",
            "                # Also, don't forget to update the report_sent time after sending the report!",
            "                await self.internal_usage_cache.async_set_cache(",
            "                    key=SlackAlertingCacheKeys.report_sent_key.value,",
            "                    value=current_time,",
            "                )",
            "                report_sent_bool = True",
            "",
            "        return report_sent_bool",
            "",
            "    async def _run_scheduled_daily_report(self, llm_router: Optional[Any] = None):",
            "        \"\"\"",
            "        If 'daily_reports' enabled",
            "",
            "        Ping redis cache every 5 minutes to check if we should send the report",
            "",
            "        If yes -> call send_daily_report()",
            "        \"\"\"",
            "        if llm_router is None or self.alert_types is None:",
            "            return",
            "",
            "        if \"daily_reports\" in self.alert_types:",
            "            while True:",
            "                await self._run_scheduler_helper(llm_router=llm_router)",
            "                interval = random.randint(",
            "                    self.alerting_args.report_check_interval - 3,",
            "                    self.alerting_args.report_check_interval + 3,",
            "                )  # shuffle to prevent collisions",
            "                await asyncio.sleep(interval)",
            "        return",
            "",
            "    async def send_weekly_spend_report(self):",
            "        \"\"\" \"\"\"",
            "        try:",
            "            from litellm.proxy.proxy_server import _get_spend_report_for_time_range",
            "",
            "            todays_date = datetime.datetime.now().date()",
            "            week_before = todays_date - datetime.timedelta(days=7)",
            "",
            "            weekly_spend_per_team, weekly_spend_per_tag = (",
            "                await _get_spend_report_for_time_range(",
            "                    start_date=week_before.strftime(\"%Y-%m-%d\"),",
            "                    end_date=todays_date.strftime(\"%Y-%m-%d\"),",
            "                )",
            "            )",
            "",
            "            _weekly_spend_message = f\"*\ud83d\udcb8 Weekly Spend Report for `{week_before.strftime('%m-%d-%Y')} - {todays_date.strftime('%m-%d-%Y')}` *\\n\"",
            "",
            "            if weekly_spend_per_team is not None:",
            "                _weekly_spend_message += \"\\n*Team Spend Report:*\\n\"",
            "                for spend in weekly_spend_per_team:",
            "                    _team_spend = spend[\"total_spend\"]",
            "                    _team_spend = float(_team_spend)",
            "                    # round to 4 decimal places",
            "                    _team_spend = round(_team_spend, 4)",
            "                    _weekly_spend_message += (",
            "                        f\"Team: `{spend['team_alias']}` | Spend: `${_team_spend}`\\n\"",
            "                    )",
            "",
            "            if weekly_spend_per_tag is not None:",
            "                _weekly_spend_message += \"\\n*Tag Spend Report:*\\n\"",
            "                for spend in weekly_spend_per_tag:",
            "                    _tag_spend = spend[\"total_spend\"]",
            "                    _tag_spend = float(_tag_spend)",
            "                    # round to 4 decimal places",
            "                    _tag_spend = round(_tag_spend, 4)",
            "                    _weekly_spend_message += f\"Tag: `{spend['individual_request_tag']}` | Spend: `${_tag_spend}`\\n\"",
            "",
            "            await self.send_alert(",
            "                message=_weekly_spend_message,",
            "                level=\"Low\",",
            "                alert_type=\"spend_reports\",",
            "                alerting_metadata={},",
            "            )",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(\"Error sending weekly spend report\", e)",
            "",
            "    async def send_monthly_spend_report(self):",
            "        \"\"\" \"\"\"",
            "        try:",
            "            from calendar import monthrange",
            "",
            "            from litellm.proxy.proxy_server import _get_spend_report_for_time_range",
            "",
            "            todays_date = datetime.datetime.now().date()",
            "            first_day_of_month = todays_date.replace(day=1)",
            "            _, last_day_of_month = monthrange(todays_date.year, todays_date.month)",
            "            last_day_of_month = first_day_of_month + datetime.timedelta(",
            "                days=last_day_of_month - 1",
            "            )",
            "",
            "            monthly_spend_per_team, monthly_spend_per_tag = (",
            "                await _get_spend_report_for_time_range(",
            "                    start_date=first_day_of_month.strftime(\"%Y-%m-%d\"),",
            "                    end_date=last_day_of_month.strftime(\"%Y-%m-%d\"),",
            "                )",
            "            )",
            "",
            "            _spend_message = f\"*\ud83d\udcb8 Monthly Spend Report for `{first_day_of_month.strftime('%m-%d-%Y')} - {last_day_of_month.strftime('%m-%d-%Y')}` *\\n\"",
            "",
            "            if monthly_spend_per_team is not None:",
            "                _spend_message += \"\\n*Team Spend Report:*\\n\"",
            "                for spend in monthly_spend_per_team:",
            "                    _team_spend = spend[\"total_spend\"]",
            "                    _team_spend = float(_team_spend)",
            "                    # round to 4 decimal places",
            "                    _team_spend = round(_team_spend, 4)",
            "                    _spend_message += (",
            "                        f\"Team: `{spend['team_alias']}` | Spend: `${_team_spend}`\\n\"",
            "                    )",
            "",
            "            if monthly_spend_per_tag is not None:",
            "                _spend_message += \"\\n*Tag Spend Report:*\\n\"",
            "                for spend in monthly_spend_per_tag:",
            "                    _tag_spend = spend[\"total_spend\"]",
            "                    _tag_spend = float(_tag_spend)",
            "                    # round to 4 decimal places",
            "                    _tag_spend = round(_tag_spend, 4)",
            "                    _spend_message += f\"Tag: `{spend['individual_request_tag']}` | Spend: `${_tag_spend}`\\n\"",
            "",
            "            await self.send_alert(",
            "                message=_spend_message,",
            "                level=\"Low\",",
            "                alert_type=\"spend_reports\",",
            "                alerting_metadata={},",
            "            )",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(\"Error sending weekly spend report\", e)"
        ],
        "afterPatchFile": [
            "#### What this does ####",
            "#    Class for sending Slack Alerts #",
            "import dotenv, os, traceback",
            "from litellm.proxy._types import UserAPIKeyAuth, CallInfo, AlertType",
            "from litellm._logging import verbose_logger, verbose_proxy_logger",
            "import litellm, threading",
            "from typing import List, Literal, Any, Union, Optional, Dict, Set",
            "from litellm.caching import DualCache",
            "import asyncio, time",
            "import aiohttp",
            "from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler",
            "import datetime",
            "from pydantic import BaseModel, Field",
            "from enum import Enum",
            "from datetime import datetime as dt, timedelta, timezone",
            "from litellm.integrations.custom_logger import CustomLogger",
            "from litellm.proxy._types import WebhookEvent",
            "import random",
            "from typing import TypedDict",
            "from openai import APIError",
            "from .email_templates.templates import *",
            "",
            "import litellm.types",
            "from litellm.types.router import LiteLLM_Params",
            "",
            "",
            "class BaseOutageModel(TypedDict):",
            "    alerts: List[int]",
            "    minor_alert_sent: bool",
            "    major_alert_sent: bool",
            "    last_updated_at: float",
            "",
            "",
            "class OutageModel(BaseOutageModel):",
            "    model_id: str",
            "",
            "",
            "class ProviderRegionOutageModel(BaseOutageModel):",
            "    provider_region_id: str",
            "    deployment_ids: Set[str]",
            "",
            "",
            "# we use this for the email header, please send a test email if you change this. verify it looks good on email",
            "LITELLM_LOGO_URL = \"https://litellm-listing.s3.amazonaws.com/litellm_logo.png\"",
            "LITELLM_SUPPORT_CONTACT = \"support@berri.ai\"",
            "",
            "",
            "class LiteLLMBase(BaseModel):",
            "    \"\"\"",
            "    Implements default functions, all pydantic objects should have.",
            "    \"\"\"",
            "",
            "    def json(self, **kwargs):",
            "        try:",
            "            return self.model_dump()  # noqa",
            "        except:",
            "            # if using pydantic v1",
            "            return self.dict()",
            "",
            "",
            "class SlackAlertingArgsEnum(Enum):",
            "    daily_report_frequency: int = 12 * 60 * 60",
            "    report_check_interval: int = 5 * 60",
            "    budget_alert_ttl: int = 24 * 60 * 60",
            "    outage_alert_ttl: int = 1 * 60",
            "    region_outage_alert_ttl: int = 1 * 60",
            "    minor_outage_alert_threshold: int = 1 * 5",
            "    major_outage_alert_threshold: int = 1 * 10",
            "    max_outage_alert_list_size: int = 1 * 10",
            "",
            "",
            "class SlackAlertingArgs(LiteLLMBase):",
            "    daily_report_frequency: int = Field(",
            "        default=int(",
            "            os.getenv(",
            "                \"SLACK_DAILY_REPORT_FREQUENCY\",",
            "                SlackAlertingArgsEnum.daily_report_frequency.value,",
            "            )",
            "        ),",
            "        description=\"Frequency of receiving deployment latency/failure reports. Default is 12hours. Value is in seconds.\",",
            "    )",
            "    report_check_interval: int = Field(",
            "        default=SlackAlertingArgsEnum.report_check_interval.value,",
            "        description=\"Frequency of checking cache if report should be sent. Background process. Default is once per hour. Value is in seconds.\",",
            "    )  # 5 minutes",
            "    budget_alert_ttl: int = Field(",
            "        default=SlackAlertingArgsEnum.budget_alert_ttl.value,",
            "        description=\"Cache ttl for budgets alerts. Prevents spamming same alert, each time budget is crossed. Value is in seconds.\",",
            "    )  # 24 hours",
            "    outage_alert_ttl: int = Field(",
            "        default=SlackAlertingArgsEnum.outage_alert_ttl.value,",
            "        description=\"Cache ttl for model outage alerts. Sets time-window for errors. Default is 1 minute. Value is in seconds.\",",
            "    )  # 1 minute ttl",
            "    region_outage_alert_ttl: int = Field(",
            "        default=SlackAlertingArgsEnum.region_outage_alert_ttl.value,",
            "        description=\"Cache ttl for provider-region based outage alerts. Alert sent if 2+ models in same region report errors. Sets time-window for errors. Default is 1 minute. Value is in seconds.\",",
            "    )  # 1 minute ttl",
            "    minor_outage_alert_threshold: int = Field(",
            "        default=SlackAlertingArgsEnum.minor_outage_alert_threshold.value,",
            "        description=\"The number of errors that count as a model/region minor outage. ('400' error code is not counted).\",",
            "    )",
            "    major_outage_alert_threshold: int = Field(",
            "        default=SlackAlertingArgsEnum.major_outage_alert_threshold.value,",
            "        description=\"The number of errors that countas a model/region major outage. ('400' error code is not counted).\",",
            "    )",
            "    max_outage_alert_list_size: int = Field(",
            "        default=SlackAlertingArgsEnum.max_outage_alert_list_size.value,",
            "        description=\"Maximum number of errors to store in cache. For a given model/region. Prevents memory leaks.\",",
            "    )  # prevent memory leak",
            "",
            "",
            "class DeploymentMetrics(LiteLLMBase):",
            "    \"\"\"",
            "    Metrics per deployment, stored in cache",
            "",
            "    Used for daily reporting",
            "    \"\"\"",
            "",
            "    id: str",
            "    \"\"\"id of deployment in router model list\"\"\"",
            "",
            "    failed_request: bool",
            "    \"\"\"did it fail the request?\"\"\"",
            "",
            "    latency_per_output_token: Optional[float]",
            "    \"\"\"latency/output token of deployment\"\"\"",
            "",
            "    updated_at: dt",
            "    \"\"\"Current time of deployment being updated\"\"\"",
            "",
            "",
            "class SlackAlertingCacheKeys(Enum):",
            "    \"\"\"",
            "    Enum for deployment daily metrics keys - {deployment_id}:{enum}",
            "    \"\"\"",
            "",
            "    failed_requests_key = \"failed_requests_daily_metrics\"",
            "    latency_key = \"latency_daily_metrics\"",
            "    report_sent_key = \"daily_metrics_report_sent\"",
            "",
            "",
            "class SlackAlerting(CustomLogger):",
            "    \"\"\"",
            "    Class for sending Slack Alerts",
            "    \"\"\"",
            "",
            "    # Class variables or attributes",
            "    def __init__(",
            "        self,",
            "        internal_usage_cache: Optional[DualCache] = None,",
            "        alerting_threshold: float = 300,  # threshold for slow / hanging llm responses (in seconds)",
            "        alerting: Optional[List] = [],",
            "        alert_types: List[AlertType] = [",
            "            \"llm_exceptions\",",
            "            \"llm_too_slow\",",
            "            \"llm_requests_hanging\",",
            "            \"budget_alerts\",",
            "            \"db_exceptions\",",
            "            \"daily_reports\",",
            "            \"spend_reports\",",
            "            \"cooldown_deployment\",",
            "            \"new_model_added\",",
            "            \"outage_alerts\",",
            "        ],",
            "        alert_to_webhook_url: Optional[",
            "            Dict",
            "        ] = None,  # if user wants to separate alerts to diff channels",
            "        alerting_args={},",
            "        default_webhook_url: Optional[str] = None,",
            "    ):",
            "        self.alerting_threshold = alerting_threshold",
            "        self.alerting = alerting",
            "        self.alert_types = alert_types",
            "        self.internal_usage_cache = internal_usage_cache or DualCache()",
            "        self.async_http_handler = AsyncHTTPHandler()",
            "        self.alert_to_webhook_url = alert_to_webhook_url",
            "        self.is_running = False",
            "        self.alerting_args = SlackAlertingArgs(**alerting_args)",
            "        self.default_webhook_url = default_webhook_url",
            "        self.llm_router: Optional[litellm.Router] = None",
            "",
            "    def update_values(",
            "        self,",
            "        alerting: Optional[List] = None,",
            "        alerting_threshold: Optional[float] = None,",
            "        alert_types: Optional[List] = None,",
            "        alert_to_webhook_url: Optional[Dict] = None,",
            "        alerting_args: Optional[Dict] = None,",
            "        llm_router: Optional[litellm.Router] = None,",
            "    ):",
            "        if alerting is not None:",
            "            self.alerting = alerting",
            "        if alerting_threshold is not None:",
            "            self.alerting_threshold = alerting_threshold",
            "        if alert_types is not None:",
            "            self.alert_types = alert_types",
            "        if alerting_args is not None:",
            "            self.alerting_args = SlackAlertingArgs(**alerting_args)",
            "        if alert_to_webhook_url is not None:",
            "            # update the dict",
            "            if self.alert_to_webhook_url is None:",
            "                self.alert_to_webhook_url = alert_to_webhook_url",
            "            else:",
            "                self.alert_to_webhook_url.update(alert_to_webhook_url)",
            "        if llm_router is not None:",
            "            self.llm_router = llm_router",
            "",
            "    async def deployment_in_cooldown(self):",
            "        pass",
            "",
            "    async def deployment_removed_from_cooldown(self):",
            "        pass",
            "",
            "    def _all_possible_alert_types(self):",
            "        # used by the UI to show all supported alert types",
            "        # Note: This is not the alerts the user has configured, instead it's all possible alert types a user can select",
            "        return [",
            "            \"llm_exceptions\",",
            "            \"llm_too_slow\",",
            "            \"llm_requests_hanging\",",
            "            \"budget_alerts\",",
            "            \"db_exceptions\",",
            "        ]",
            "",
            "    def _add_langfuse_trace_id_to_alert(",
            "        self,",
            "        request_data: Optional[dict] = None,",
            "    ) -> Optional[str]:",
            "        \"\"\"",
            "        Returns langfuse trace url",
            "",
            "        - check:",
            "        -> existing_trace_id",
            "        -> trace_id",
            "        -> litellm_call_id",
            "        \"\"\"",
            "        # do nothing for now",
            "        if request_data is not None:",
            "            trace_id = None",
            "            if (",
            "                request_data.get(\"metadata\", {}).get(\"existing_trace_id\", None)",
            "                is not None",
            "            ):",
            "                trace_id = request_data[\"metadata\"][\"existing_trace_id\"]",
            "            elif request_data.get(\"metadata\", {}).get(\"trace_id\", None) is not None:",
            "                trace_id = request_data[\"metadata\"][\"trace_id\"]",
            "            elif request_data.get(\"litellm_logging_obj\", None) is not None and hasattr(",
            "                request_data[\"litellm_logging_obj\"], \"model_call_details\"",
            "            ):",
            "                trace_id = request_data[\"litellm_logging_obj\"].model_call_details[",
            "                    \"litellm_call_id\"",
            "                ]",
            "            if litellm.utils.langFuseLogger is not None:",
            "                base_url = litellm.utils.langFuseLogger.Langfuse.base_url",
            "                return f\"{base_url}/trace/{trace_id}\"",
            "        return None",
            "",
            "    def _response_taking_too_long_callback_helper(",
            "        self,",
            "        kwargs,  # kwargs to completion",
            "        start_time,",
            "        end_time,  # start/end time",
            "    ):",
            "        try:",
            "            time_difference = end_time - start_time",
            "            # Convert the timedelta to float (in seconds)",
            "            time_difference_float = time_difference.total_seconds()",
            "            litellm_params = kwargs.get(\"litellm_params\", {})",
            "            model = kwargs.get(\"model\", \"\")",
            "            api_base = litellm.get_api_base(model=model, optional_params=litellm_params)",
            "            messages = kwargs.get(\"messages\", None)",
            "            # if messages does not exist fallback to \"input\"",
            "            if messages is None:",
            "                messages = kwargs.get(\"input\", None)",
            "",
            "            # only use first 100 chars for alerting",
            "            _messages = str(messages)[:100]",
            "",
            "            return time_difference_float, model, api_base, _messages",
            "        except Exception as e:",
            "            raise e",
            "",
            "    def _get_deployment_latencies_to_alert(self, metadata=None):",
            "        if metadata is None:",
            "            return None",
            "",
            "        if \"_latency_per_deployment\" in metadata:",
            "            # Translate model_id to -> api_base",
            "            # _latency_per_deployment is a dictionary that looks like this:",
            "            \"\"\"",
            "            _latency_per_deployment: {",
            "                api_base: 0.01336697916666667",
            "            }",
            "            \"\"\"",
            "            _message_to_send = \"\"",
            "            _deployment_latencies = metadata[\"_latency_per_deployment\"]",
            "            if len(_deployment_latencies) == 0:",
            "                return None",
            "            try:",
            "                # try sorting deployments by latency",
            "                _deployment_latencies = sorted(",
            "                    _deployment_latencies.items(), key=lambda x: x[1]",
            "                )",
            "                _deployment_latencies = dict(_deployment_latencies)",
            "            except:",
            "                pass",
            "            for api_base, latency in _deployment_latencies.items():",
            "                _message_to_send += f\"\\n{api_base}: {round(latency,2)}s\"",
            "            _message_to_send = \"```\" + _message_to_send + \"```\"",
            "            return _message_to_send",
            "",
            "    async def response_taking_too_long_callback(",
            "        self,",
            "        kwargs,  # kwargs to completion",
            "        completion_response,  # response from completion",
            "        start_time,",
            "        end_time,  # start/end time",
            "    ):",
            "        if self.alerting is None or self.alert_types is None:",
            "            return",
            "",
            "        time_difference_float, model, api_base, messages = (",
            "            self._response_taking_too_long_callback_helper(",
            "                kwargs=kwargs,",
            "                start_time=start_time,",
            "                end_time=end_time,",
            "            )",
            "        )",
            "        if litellm.turn_off_message_logging or litellm.redact_messages_in_exceptions:",
            "            messages = \"Message not logged. litellm.redact_messages_in_exceptions=True\"",
            "        request_info = f\"\\nRequest Model: `{model}`\\nAPI Base: `{api_base}`\\nMessages: `{messages}`\"",
            "        slow_message = f\"`Responses are slow - {round(time_difference_float,2)}s response time > Alerting threshold: {self.alerting_threshold}s`\"",
            "        alerting_metadata: dict = {}",
            "        if time_difference_float > self.alerting_threshold:",
            "            # add deployment latencies to alert",
            "            if (",
            "                kwargs is not None",
            "                and \"litellm_params\" in kwargs",
            "                and \"metadata\" in kwargs[\"litellm_params\"]",
            "            ):",
            "                _metadata: dict = kwargs[\"litellm_params\"][\"metadata\"]",
            "                request_info = litellm.utils._add_key_name_and_team_to_alert(",
            "                    request_info=request_info, metadata=_metadata",
            "                )",
            "",
            "                _deployment_latency_map = self._get_deployment_latencies_to_alert(",
            "                    metadata=_metadata",
            "                )",
            "                if _deployment_latency_map is not None:",
            "                    request_info += (",
            "                        f\"\\nAvailable Deployment Latencies\\n{_deployment_latency_map}\"",
            "                    )",
            "",
            "                if \"alerting_metadata\" in _metadata:",
            "                    alerting_metadata = _metadata[\"alerting_metadata\"]",
            "            await self.send_alert(",
            "                message=slow_message + request_info,",
            "                level=\"Low\",",
            "                alert_type=\"llm_too_slow\",",
            "                alerting_metadata=alerting_metadata,",
            "            )",
            "",
            "    async def async_update_daily_reports(",
            "        self, deployment_metrics: DeploymentMetrics",
            "    ) -> int:",
            "        \"\"\"",
            "        Store the perf by deployment in cache",
            "        - Number of failed requests per deployment",
            "        - Latency / output tokens per deployment",
            "",
            "        'deployment_id:daily_metrics:failed_requests'",
            "        'deployment_id:daily_metrics:latency_per_output_token'",
            "",
            "        Returns",
            "            int - count of metrics set (1 - if just latency, 2 - if failed + latency)",
            "        \"\"\"",
            "",
            "        return_val = 0",
            "        try:",
            "            ## FAILED REQUESTS ##",
            "            if deployment_metrics.failed_request:",
            "                await self.internal_usage_cache.async_increment_cache(",
            "                    key=\"{}:{}\".format(",
            "                        deployment_metrics.id,",
            "                        SlackAlertingCacheKeys.failed_requests_key.value,",
            "                    ),",
            "                    value=1,",
            "                )",
            "",
            "                return_val += 1",
            "",
            "            ## LATENCY ##",
            "            if deployment_metrics.latency_per_output_token is not None:",
            "                await self.internal_usage_cache.async_increment_cache(",
            "                    key=\"{}:{}\".format(",
            "                        deployment_metrics.id, SlackAlertingCacheKeys.latency_key.value",
            "                    ),",
            "                    value=deployment_metrics.latency_per_output_token,",
            "                )",
            "",
            "                return_val += 1",
            "",
            "            return return_val",
            "        except Exception as e:",
            "            return 0",
            "",
            "    async def send_daily_reports(self, router) -> bool:",
            "        \"\"\"",
            "        Send a daily report on:",
            "        - Top 5 deployments with most failed requests",
            "        - Top 5 slowest deployments (normalized by latency/output tokens)",
            "",
            "        Get the value from redis cache (if available) or in-memory and send it",
            "",
            "        Cleanup:",
            "        - reset values in cache -> prevent memory leak",
            "",
            "        Returns:",
            "            True -> if successfuly sent",
            "            False -> if not sent",
            "        \"\"\"",
            "",
            "        ids = router.get_model_ids()",
            "",
            "        # get keys",
            "        failed_request_keys = [",
            "            \"{}:{}\".format(id, SlackAlertingCacheKeys.failed_requests_key.value)",
            "            for id in ids",
            "        ]",
            "        latency_keys = [",
            "            \"{}:{}\".format(id, SlackAlertingCacheKeys.latency_key.value) for id in ids",
            "        ]",
            "",
            "        combined_metrics_keys = failed_request_keys + latency_keys  # reduce cache calls",
            "",
            "        combined_metrics_values = await self.internal_usage_cache.async_batch_get_cache(",
            "            keys=combined_metrics_keys",
            "        )  # [1, 2, None, ..]",
            "",
            "        if combined_metrics_values is None:",
            "            return False",
            "",
            "        all_none = True",
            "        for val in combined_metrics_values:",
            "            if val is not None and val > 0:",
            "                all_none = False",
            "                break",
            "",
            "        if all_none:",
            "            return False",
            "",
            "        failed_request_values = combined_metrics_values[",
            "            : len(failed_request_keys)",
            "        ]  # # [1, 2, None, ..]",
            "        latency_values = combined_metrics_values[len(failed_request_keys) :]",
            "",
            "        # find top 5 failed",
            "        ## Replace None values with a placeholder value (-1 in this case)",
            "        placeholder_value = 0",
            "        replaced_failed_values = [",
            "            value if value is not None else placeholder_value",
            "            for value in failed_request_values",
            "        ]",
            "",
            "        ## Get the indices of top 5 keys with the highest numerical values (ignoring None and 0 values)",
            "        top_5_failed = sorted(",
            "            range(len(replaced_failed_values)),",
            "            key=lambda i: replaced_failed_values[i],",
            "            reverse=True,",
            "        )[:5]",
            "        top_5_failed = [",
            "            index for index in top_5_failed if replaced_failed_values[index] > 0",
            "        ]",
            "",
            "        # find top 5 slowest",
            "        # Replace None values with a placeholder value (-1 in this case)",
            "        placeholder_value = 0",
            "        replaced_slowest_values = [",
            "            value if value is not None else placeholder_value",
            "            for value in latency_values",
            "        ]",
            "",
            "        # Get the indices of top 5 values with the highest numerical values (ignoring None and 0 values)",
            "        top_5_slowest = sorted(",
            "            range(len(replaced_slowest_values)),",
            "            key=lambda i: replaced_slowest_values[i],",
            "            reverse=True,",
            "        )[:5]",
            "        top_5_slowest = [",
            "            index for index in top_5_slowest if replaced_slowest_values[index] > 0",
            "        ]",
            "",
            "        # format alert -> return the litellm model name + api base",
            "        message = f\"\\n\\nTime: `{time.time()}`s\\nHere are today's key metrics \ud83d\udcc8: \\n\\n\"",
            "",
            "        message += \"\\n\\n*\u2757\ufe0f Top Deployments with Most Failed Requests:*\\n\\n\"",
            "        if not top_5_failed:",
            "            message += \"\\tNone\\n\"",
            "        for i in range(len(top_5_failed)):",
            "            key = failed_request_keys[top_5_failed[i]].split(\":\")[0]",
            "            _deployment = router.get_model_info(key)",
            "            if isinstance(_deployment, dict):",
            "                deployment_name = _deployment[\"litellm_params\"].get(\"model\", \"\")",
            "            else:",
            "                return False",
            "",
            "            api_base = litellm.get_api_base(",
            "                model=deployment_name,",
            "                optional_params=(",
            "                    _deployment[\"litellm_params\"] if _deployment is not None else {}",
            "                ),",
            "            )",
            "            if api_base is None:",
            "                api_base = \"\"",
            "            value = replaced_failed_values[top_5_failed[i]]",
            "            message += f\"\\t{i+1}. Deployment: `{deployment_name}`, Failed Requests: `{value}`,  API Base: `{api_base}`\\n\"",
            "",
            "        message += \"\\n\\n*\ud83d\ude05 Top Slowest Deployments:*\\n\\n\"",
            "        if not top_5_slowest:",
            "            message += \"\\tNone\\n\"",
            "        for i in range(len(top_5_slowest)):",
            "            key = latency_keys[top_5_slowest[i]].split(\":\")[0]",
            "            _deployment = router.get_model_info(key)",
            "            if _deployment is not None:",
            "                deployment_name = _deployment[\"litellm_params\"].get(\"model\", \"\")",
            "            else:",
            "                deployment_name = \"\"",
            "            api_base = litellm.get_api_base(",
            "                model=deployment_name,",
            "                optional_params=(",
            "                    _deployment[\"litellm_params\"] if _deployment is not None else {}",
            "                ),",
            "            )",
            "            value = round(replaced_slowest_values[top_5_slowest[i]], 3)",
            "            message += f\"\\t{i+1}. Deployment: `{deployment_name}`, Latency per output token: `{value}s/token`,  API Base: `{api_base}`\\n\\n\"",
            "",
            "        # cache cleanup -> reset values to 0",
            "        latency_cache_keys = [(key, 0) for key in latency_keys]",
            "        failed_request_cache_keys = [(key, 0) for key in failed_request_keys]",
            "        combined_metrics_cache_keys = latency_cache_keys + failed_request_cache_keys",
            "        await self.internal_usage_cache.async_batch_set_cache(",
            "            cache_list=combined_metrics_cache_keys",
            "        )",
            "",
            "        message += f\"\\n\\nNext Run is at: `{time.time() + self.alerting_args.daily_report_frequency}`s\"",
            "",
            "        # send alert",
            "        await self.send_alert(",
            "            message=message,",
            "            level=\"Low\",",
            "            alert_type=\"daily_reports\",",
            "            alerting_metadata={},",
            "        )",
            "",
            "        return True",
            "",
            "    async def response_taking_too_long(",
            "        self,",
            "        start_time: Optional[datetime.datetime] = None,",
            "        end_time: Optional[datetime.datetime] = None,",
            "        type: Literal[\"hanging_request\", \"slow_response\"] = \"hanging_request\",",
            "        request_data: Optional[dict] = None,",
            "    ):",
            "        if self.alerting is None or self.alert_types is None:",
            "            return",
            "        if request_data is not None:",
            "            model = request_data.get(\"model\", \"\")",
            "            messages = request_data.get(\"messages\", None)",
            "            if messages is None:",
            "                # if messages does not exist fallback to \"input\"",
            "                messages = request_data.get(\"input\", None)",
            "",
            "            # try casting messages to str and get the first 100 characters, else mark as None",
            "            try:",
            "                messages = str(messages)",
            "                messages = messages[:100]",
            "            except:",
            "                messages = \"\"",
            "",
            "            if (",
            "                litellm.turn_off_message_logging",
            "                or litellm.redact_messages_in_exceptions",
            "            ):",
            "                messages = (",
            "                    \"Message not logged. litellm.redact_messages_in_exceptions=True\"",
            "                )",
            "            request_info = f\"\\nRequest Model: `{model}`\\nMessages: `{messages}`\"",
            "        else:",
            "            request_info = \"\"",
            "",
            "        if type == \"hanging_request\":",
            "            await asyncio.sleep(",
            "                self.alerting_threshold",
            "            )  # Set it to 5 minutes - i'd imagine this might be different for streaming, non-streaming, non-completion (embedding + img) requests",
            "            alerting_metadata: dict = {}",
            "            if (",
            "                request_data is not None",
            "                and request_data.get(\"litellm_status\", \"\") != \"success\"",
            "                and request_data.get(\"litellm_status\", \"\") != \"fail\"",
            "            ):",
            "                if request_data.get(\"deployment\", None) is not None and isinstance(",
            "                    request_data[\"deployment\"], dict",
            "                ):",
            "                    _api_base = litellm.get_api_base(",
            "                        model=model,",
            "                        optional_params=request_data[\"deployment\"].get(",
            "                            \"litellm_params\", {}",
            "                        ),",
            "                    )",
            "",
            "                    if _api_base is None:",
            "                        _api_base = \"\"",
            "",
            "                    request_info += f\"\\nAPI Base: {_api_base}\"",
            "                elif request_data.get(\"metadata\", None) is not None and isinstance(",
            "                    request_data[\"metadata\"], dict",
            "                ):",
            "                    # In hanging requests sometime it has not made it to the point where the deployment is passed to the `request_data``",
            "                    # in that case we fallback to the api base set in the request metadata",
            "                    _metadata: dict = request_data[\"metadata\"]",
            "                    _api_base = _metadata.get(\"api_base\", \"\")",
            "",
            "                    request_info = litellm.utils._add_key_name_and_team_to_alert(",
            "                        request_info=request_info, metadata=_metadata",
            "                    )",
            "",
            "                    if _api_base is None:",
            "                        _api_base = \"\"",
            "",
            "                    if \"alerting_metadata\" in _metadata:",
            "                        alerting_metadata = _metadata[\"alerting_metadata\"]",
            "                    request_info += f\"\\nAPI Base: `{_api_base}`\"",
            "                # only alert hanging responses if they have not been marked as success",
            "                alerting_message = (",
            "                    f\"`Requests are hanging - {self.alerting_threshold}s+ request time`\"",
            "                )",
            "",
            "                if \"langfuse\" in litellm.success_callback:",
            "                    langfuse_url = self._add_langfuse_trace_id_to_alert(",
            "                        request_data=request_data,",
            "                    )",
            "",
            "                    if langfuse_url is not None:",
            "                        request_info += \"\\n\ud83e\udea2 Langfuse Trace: {}\".format(langfuse_url)",
            "",
            "                # add deployment latencies to alert",
            "                _deployment_latency_map = self._get_deployment_latencies_to_alert(",
            "                    metadata=request_data.get(\"metadata\", {})",
            "                )",
            "                if _deployment_latency_map is not None:",
            "                    request_info += f\"\\nDeployment Latencies\\n{_deployment_latency_map}\"",
            "",
            "                await self.send_alert(",
            "                    message=alerting_message + request_info,",
            "                    level=\"Medium\",",
            "                    alert_type=\"llm_requests_hanging\",",
            "                    alerting_metadata=alerting_metadata,",
            "                )",
            "",
            "    async def failed_tracking_alert(self, error_message: str):",
            "        \"\"\"Raise alert when tracking failed for specific model\"\"\"",
            "        _cache: DualCache = self.internal_usage_cache",
            "        message = \"Failed Tracking Cost for\" + error_message",
            "        _cache_key = \"budget_alerts:failed_tracking:{}\".format(message)",
            "        result = await _cache.async_get_cache(key=_cache_key)",
            "        if result is None:",
            "            await self.send_alert(",
            "                message=message,",
            "                level=\"High\",",
            "                alert_type=\"budget_alerts\",",
            "                alerting_metadata={},",
            "            )",
            "            await _cache.async_set_cache(",
            "                key=_cache_key,",
            "                value=\"SENT\",",
            "                ttl=self.alerting_args.budget_alert_ttl,",
            "            )",
            "",
            "    async def budget_alerts(",
            "        self,",
            "        type: Literal[",
            "            \"token_budget\",",
            "            \"user_budget\",",
            "            \"team_budget\",",
            "            \"proxy_budget\",",
            "            \"projected_limit_exceeded\",",
            "        ],",
            "        user_info: CallInfo,",
            "    ):",
            "        ## PREVENTITIVE ALERTING ## - https://github.com/BerriAI/litellm/issues/2727",
            "        # - Alert once within 24hr period",
            "        # - Cache this information",
            "        # - Don't re-alert, if alert already sent",
            "        _cache: DualCache = self.internal_usage_cache",
            "",
            "        if self.alerting is None or self.alert_types is None:",
            "            # do nothing if alerting is not switched on",
            "            return",
            "        if \"budget_alerts\" not in self.alert_types:",
            "            return",
            "        _id: Optional[str] = \"default_id\"  # used for caching",
            "        user_info_json = user_info.model_dump(exclude_none=True)",
            "        for k, v in user_info_json.items():",
            "            user_info_str = \"\\n{}: {}\\n\".format(k, v)",
            "",
            "        event: Optional[",
            "            Literal[\"budget_crossed\", \"threshold_crossed\", \"projected_limit_exceeded\"]",
            "        ] = None",
            "        event_group: Optional[",
            "            Literal[\"internal_user\", \"team\", \"key\", \"proxy\", \"customer\"]",
            "        ] = None",
            "        event_message: str = \"\"",
            "        webhook_event: Optional[WebhookEvent] = None",
            "        if type == \"proxy_budget\":",
            "            event_group = \"proxy\"",
            "            event_message += \"Proxy Budget: \"",
            "        elif type == \"user_budget\":",
            "            event_group = \"internal_user\"",
            "            event_message += \"User Budget: \"",
            "            _id = user_info.user_id or _id",
            "        elif type == \"team_budget\":",
            "            event_group = \"team\"",
            "            event_message += \"Team Budget: \"",
            "            _id = user_info.team_id or _id",
            "        elif type == \"token_budget\":",
            "            event_group = \"key\"",
            "            event_message += \"Key Budget: \"",
            "            _id = user_info.token",
            "        elif type == \"projected_limit_exceeded\":",
            "            event_group = \"key\"",
            "            event_message += \"Key Budget: Projected Limit Exceeded\"",
            "            event = \"projected_limit_exceeded\"",
            "            _id = user_info.token",
            "",
            "        # percent of max_budget left to spend",
            "        if user_info.max_budget is None:",
            "            return",
            "",
            "        if user_info.max_budget > 0:",
            "            percent_left = (",
            "                user_info.max_budget - user_info.spend",
            "            ) / user_info.max_budget",
            "        else:",
            "            percent_left = 0",
            "",
            "        # check if crossed budget",
            "        if user_info.spend >= user_info.max_budget:",
            "            event = \"budget_crossed\"",
            "            event_message += f\"Budget Crossed\\n Total Budget:`{user_info.max_budget}`\"",
            "        elif percent_left <= 0.05:",
            "            event = \"threshold_crossed\"",
            "            event_message += \"5% Threshold Crossed \"",
            "        elif percent_left <= 0.15:",
            "            event = \"threshold_crossed\"",
            "            event_message += \"15% Threshold Crossed\"",
            "",
            "        if event is not None and event_group is not None:",
            "            _cache_key = \"budget_alerts:{}:{}\".format(event, _id)",
            "            result = await _cache.async_get_cache(key=_cache_key)",
            "            if result is None:",
            "                webhook_event = WebhookEvent(",
            "                    event=event,",
            "                    event_group=event_group,",
            "                    event_message=event_message,",
            "                    **user_info_json,",
            "                )",
            "                await self.send_alert(",
            "                    message=event_message + \"\\n\\n\" + user_info_str,",
            "                    level=\"High\",",
            "                    alert_type=\"budget_alerts\",",
            "                    user_info=webhook_event,",
            "                    alerting_metadata={},",
            "                )",
            "                await _cache.async_set_cache(",
            "                    key=_cache_key,",
            "                    value=\"SENT\",",
            "                    ttl=self.alerting_args.budget_alert_ttl,",
            "                )",
            "",
            "            return",
            "        return",
            "",
            "    async def customer_spend_alert(",
            "        self,",
            "        token: Optional[str],",
            "        key_alias: Optional[str],",
            "        end_user_id: Optional[str],",
            "        response_cost: Optional[float],",
            "        max_budget: Optional[float],",
            "    ):",
            "        if (",
            "            self.alerting is not None",
            "            and \"webhook\" in self.alerting",
            "            and end_user_id is not None",
            "            and token is not None",
            "            and response_cost is not None",
            "        ):",
            "            # log customer spend",
            "            event = WebhookEvent(",
            "                spend=response_cost,",
            "                max_budget=max_budget,",
            "                token=token,",
            "                customer_id=end_user_id,",
            "                user_id=None,",
            "                team_id=None,",
            "                user_email=None,",
            "                key_alias=key_alias,",
            "                projected_exceeded_date=None,",
            "                projected_spend=None,",
            "                event=\"spend_tracked\",",
            "                event_group=\"customer\",",
            "                event_message=\"Customer spend tracked. Customer={}, spend={}\".format(",
            "                    end_user_id, response_cost",
            "                ),",
            "            )",
            "",
            "            await self.send_webhook_alert(webhook_event=event)",
            "",
            "    def _count_outage_alerts(self, alerts: List[int]) -> str:",
            "        \"\"\"",
            "        Parameters:",
            "        - alerts: List[int] -> list of error codes (either 408 or 500+)",
            "",
            "        Returns:",
            "        - str -> formatted string. This is an alert message, giving a human-friendly description of the errors.",
            "        \"\"\"",
            "        error_breakdown = {\"Timeout Errors\": 0, \"API Errors\": 0, \"Unknown Errors\": 0}",
            "        for alert in alerts:",
            "            if alert == 408:",
            "                error_breakdown[\"Timeout Errors\"] += 1",
            "            elif alert >= 500:",
            "                error_breakdown[\"API Errors\"] += 1",
            "            else:",
            "                error_breakdown[\"Unknown Errors\"] += 1",
            "",
            "        error_msg = \"\"",
            "        for key, value in error_breakdown.items():",
            "            if value > 0:",
            "                error_msg += \"\\n{}: {}\\n\".format(key, value)",
            "",
            "        return error_msg",
            "",
            "    def _outage_alert_msg_factory(",
            "        self,",
            "        alert_type: Literal[\"Major\", \"Minor\"],",
            "        key: Literal[\"Model\", \"Region\"],",
            "        key_val: str,",
            "        provider: str,",
            "        api_base: Optional[str],",
            "        outage_value: BaseOutageModel,",
            "    ) -> str:",
            "        \"\"\"Format an alert message for slack\"\"\"",
            "        headers = {f\"{key} Name\": key_val, \"Provider\": provider}",
            "        if api_base is not None:",
            "            headers[\"API Base\"] = api_base  # type: ignore",
            "",
            "        headers_str = \"\\n\"",
            "        for k, v in headers.items():",
            "            headers_str += f\"*{k}:* `{v}`\\n\"",
            "        return f\"\"\"\\n\\n",
            "*\u26a0\ufe0f {alert_type} Service Outage*",
            "",
            "{headers_str}",
            "",
            "*Errors:*",
            "{self._count_outage_alerts(alerts=outage_value[\"alerts\"])}",
            "",
            "*Last Check:* `{round(time.time() - outage_value[\"last_updated_at\"], 4)}s ago`\\n\\n",
            "\"\"\"",
            "",
            "    async def region_outage_alerts(",
            "        self,",
            "        exception: APIError,",
            "        deployment_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Send slack alert if specific provider region is having an outage.",
            "",
            "        Track for 408 (Timeout) and >=500 Error codes",
            "        \"\"\"",
            "        ## CREATE (PROVIDER+REGION) ID ##",
            "        if self.llm_router is None:",
            "            return",
            "",
            "        deployment = self.llm_router.get_deployment(model_id=deployment_id)",
            "",
            "        if deployment is None:",
            "            return",
            "",
            "        model = deployment.litellm_params.model",
            "        ### GET PROVIDER ###",
            "        provider = deployment.litellm_params.custom_llm_provider",
            "        if provider is None:",
            "            model, provider, _, _ = litellm.get_llm_provider(model=model)",
            "",
            "        ### GET REGION ###",
            "        region_name = deployment.litellm_params.region_name",
            "        if region_name is None:",
            "            region_name = litellm.utils._get_model_region(",
            "                custom_llm_provider=provider, litellm_params=deployment.litellm_params",
            "            )",
            "",
            "        if region_name is None:",
            "            return",
            "",
            "        ### UNIQUE CACHE KEY ###",
            "        cache_key = provider + region_name",
            "",
            "        outage_value: Optional[ProviderRegionOutageModel] = (",
            "            await self.internal_usage_cache.async_get_cache(key=cache_key)",
            "        )",
            "",
            "        if (",
            "            getattr(exception, \"status_code\", None) is None",
            "            or (",
            "                exception.status_code != 408  # type: ignore",
            "                and exception.status_code < 500  # type: ignore",
            "            )",
            "            or self.llm_router is None",
            "        ):",
            "            return",
            "",
            "        if outage_value is None:",
            "            _deployment_set = set()",
            "            _deployment_set.add(deployment_id)",
            "            outage_value = ProviderRegionOutageModel(",
            "                provider_region_id=cache_key,",
            "                alerts=[exception.status_code],  # type: ignore",
            "                minor_alert_sent=False,",
            "                major_alert_sent=False,",
            "                last_updated_at=time.time(),",
            "                deployment_ids=_deployment_set,",
            "            )",
            "",
            "            ## add to cache ##",
            "            await self.internal_usage_cache.async_set_cache(",
            "                key=cache_key,",
            "                value=outage_value,",
            "                ttl=self.alerting_args.region_outage_alert_ttl,",
            "            )",
            "            return",
            "",
            "        if len(outage_value[\"alerts\"]) < self.alerting_args.max_outage_alert_list_size:",
            "            outage_value[\"alerts\"].append(exception.status_code)  # type: ignore",
            "        else:  # prevent memory leaks",
            "            pass",
            "        _deployment_set = outage_value[\"deployment_ids\"]",
            "        _deployment_set.add(deployment_id)",
            "        outage_value[\"deployment_ids\"] = _deployment_set",
            "        outage_value[\"last_updated_at\"] = time.time()",
            "",
            "        ## MINOR OUTAGE ALERT SENT ##",
            "        if (",
            "            outage_value[\"minor_alert_sent\"] == False",
            "            and len(outage_value[\"alerts\"])",
            "            >= self.alerting_args.minor_outage_alert_threshold",
            "            and len(_deployment_set) > 1  # make sure it's not just 1 bad deployment",
            "        ):",
            "            msg = self._outage_alert_msg_factory(",
            "                alert_type=\"Minor\",",
            "                key=\"Region\",",
            "                key_val=region_name,",
            "                api_base=None,",
            "                outage_value=outage_value,",
            "                provider=provider,",
            "            )",
            "            # send minor alert",
            "            await self.send_alert(",
            "                message=msg,",
            "                level=\"Medium\",",
            "                alert_type=\"outage_alerts\",",
            "                alerting_metadata={},",
            "            )",
            "            # set to true",
            "            outage_value[\"minor_alert_sent\"] = True",
            "",
            "        ## MAJOR OUTAGE ALERT SENT ##",
            "        elif (",
            "            outage_value[\"major_alert_sent\"] == False",
            "            and len(outage_value[\"alerts\"])",
            "            >= self.alerting_args.major_outage_alert_threshold",
            "            and len(_deployment_set) > 1  # make sure it's not just 1 bad deployment",
            "        ):",
            "            msg = self._outage_alert_msg_factory(",
            "                alert_type=\"Major\",",
            "                key=\"Region\",",
            "                key_val=region_name,",
            "                api_base=None,",
            "                outage_value=outage_value,",
            "                provider=provider,",
            "            )",
            "",
            "            # send minor alert",
            "            await self.send_alert(",
            "                message=msg,",
            "                level=\"High\",",
            "                alert_type=\"outage_alerts\",",
            "                alerting_metadata={},",
            "            )",
            "            # set to true",
            "            outage_value[\"major_alert_sent\"] = True",
            "",
            "        ## update cache ##",
            "        await self.internal_usage_cache.async_set_cache(",
            "            key=cache_key, value=outage_value",
            "        )",
            "",
            "    async def outage_alerts(",
            "        self,",
            "        exception: APIError,",
            "        deployment_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Send slack alert if model is badly configured / having an outage (408, 401, 429, >=500).",
            "",
            "        key = model_id",
            "",
            "        value = {",
            "        - model_id",
            "        - threshold",
            "        - alerts []",
            "        }",
            "",
            "        ttl = 1hr",
            "        max_alerts_size = 10",
            "        \"\"\"",
            "        try:",
            "            outage_value: Optional[OutageModel] = await self.internal_usage_cache.async_get_cache(key=deployment_id)  # type: ignore",
            "            if (",
            "                getattr(exception, \"status_code\", None) is None",
            "                or (",
            "                    exception.status_code != 408  # type: ignore",
            "                    and exception.status_code < 500  # type: ignore",
            "                )",
            "                or self.llm_router is None",
            "            ):",
            "                return",
            "",
            "            ### EXTRACT MODEL DETAILS ###",
            "            deployment = self.llm_router.get_deployment(model_id=deployment_id)",
            "            if deployment is None:",
            "                return",
            "",
            "            model = deployment.litellm_params.model",
            "            provider = deployment.litellm_params.custom_llm_provider",
            "            if provider is None:",
            "                try:",
            "                    model, provider, _, _ = litellm.get_llm_provider(model=model)",
            "                except Exception as e:",
            "                    provider = \"\"",
            "            api_base = litellm.get_api_base(",
            "                model=model, optional_params=deployment.litellm_params",
            "            )",
            "",
            "            if outage_value is None:",
            "                outage_value = OutageModel(",
            "                    model_id=deployment_id,",
            "                    alerts=[exception.status_code],  # type: ignore",
            "                    minor_alert_sent=False,",
            "                    major_alert_sent=False,",
            "                    last_updated_at=time.time(),",
            "                )",
            "",
            "                ## add to cache ##",
            "                await self.internal_usage_cache.async_set_cache(",
            "                    key=deployment_id,",
            "                    value=outage_value,",
            "                    ttl=self.alerting_args.outage_alert_ttl,",
            "                )",
            "                return",
            "",
            "            if (",
            "                len(outage_value[\"alerts\"])",
            "                < self.alerting_args.max_outage_alert_list_size",
            "            ):",
            "                outage_value[\"alerts\"].append(exception.status_code)  # type: ignore",
            "            else:  # prevent memory leaks",
            "                pass",
            "",
            "            outage_value[\"last_updated_at\"] = time.time()",
            "",
            "            ## MINOR OUTAGE ALERT SENT ##",
            "            if (",
            "                outage_value[\"minor_alert_sent\"] == False",
            "                and len(outage_value[\"alerts\"])",
            "                >= self.alerting_args.minor_outage_alert_threshold",
            "            ):",
            "                msg = self._outage_alert_msg_factory(",
            "                    alert_type=\"Minor\",",
            "                    key=\"Model\",",
            "                    key_val=model,",
            "                    api_base=api_base,",
            "                    outage_value=outage_value,",
            "                    provider=provider,",
            "                )",
            "                # send minor alert",
            "                await self.send_alert(",
            "                    message=msg,",
            "                    level=\"Medium\",",
            "                    alert_type=\"outage_alerts\",",
            "                    alerting_metadata={},",
            "                )",
            "                # set to true",
            "                outage_value[\"minor_alert_sent\"] = True",
            "            elif (",
            "                outage_value[\"major_alert_sent\"] == False",
            "                and len(outage_value[\"alerts\"])",
            "                >= self.alerting_args.major_outage_alert_threshold",
            "            ):",
            "                msg = self._outage_alert_msg_factory(",
            "                    alert_type=\"Major\",",
            "                    key=\"Model\",",
            "                    key_val=model,",
            "                    api_base=api_base,",
            "                    outage_value=outage_value,",
            "                    provider=provider,",
            "                )",
            "                # send minor alert",
            "                await self.send_alert(",
            "                    message=msg,",
            "                    level=\"High\",",
            "                    alert_type=\"outage_alerts\",",
            "                    alerting_metadata={},",
            "                )",
            "                # set to true",
            "                outage_value[\"major_alert_sent\"] = True",
            "",
            "            ## update cache ##",
            "            await self.internal_usage_cache.async_set_cache(",
            "                key=deployment_id, value=outage_value",
            "            )",
            "        except Exception as e:",
            "            pass",
            "",
            "    async def model_added_alert(",
            "        self, model_name: str, litellm_model_name: str, passed_model_info: Any",
            "    ):",
            "        base_model_from_user = getattr(passed_model_info, \"base_model\", None)",
            "        model_info = {}",
            "        base_model = \"\"",
            "        if base_model_from_user is not None:",
            "            model_info = litellm.model_cost.get(base_model_from_user, {})",
            "            base_model = f\"Base Model: `{base_model_from_user}`\\n\"",
            "        else:",
            "            model_info = litellm.model_cost.get(litellm_model_name, {})",
            "        model_info_str = \"\"",
            "        for k, v in model_info.items():",
            "            if k == \"input_cost_per_token\" or k == \"output_cost_per_token\":",
            "                # when converting to string it should not be 1.63e-06",
            "                v = \"{:.8f}\".format(v)",
            "",
            "            model_info_str += f\"{k}: {v}\\n\"",
            "",
            "        message = f\"\"\"",
            "*\ud83d\ude85 New Model Added*",
            "Model Name: `{model_name}`",
            "{base_model}",
            "",
            "Usage OpenAI Python SDK:",
            "```",
            "import openai",
            "client = openai.OpenAI(",
            "    api_key=\"your_api_key\",",
            "    base_url={os.getenv(\"PROXY_BASE_URL\", \"http://0.0.0.0:4000\")}",
            ")",
            "",
            "response = client.chat.completions.create(",
            "    model=\"{model_name}\", # model to send to the proxy",
            "    messages = [",
            "        {{",
            "            \"role\": \"user\",",
            "            \"content\": \"this is a test request, write a short poem\"",
            "        }}",
            "    ]",
            ")",
            "```",
            "",
            "Model Info: ",
            "```",
            "{model_info_str}",
            "```",
            "\"\"\"",
            "",
            "        alert_val = self.send_alert(",
            "            message=message,",
            "            level=\"Low\",",
            "            alert_type=\"new_model_added\",",
            "            alerting_metadata={},",
            "        )",
            "",
            "        if alert_val is not None and asyncio.iscoroutine(alert_val):",
            "            await alert_val",
            "",
            "    async def model_removed_alert(self, model_name: str):",
            "        pass",
            "",
            "    async def send_webhook_alert(self, webhook_event: WebhookEvent) -> bool:",
            "        \"\"\"",
            "        Sends structured alert to webhook, if set.",
            "",
            "        Currently only implemented for budget alerts",
            "",
            "        Returns -> True if sent, False if not.",
            "",
            "        Raises Exception",
            "            - if WEBHOOK_URL is not set",
            "        \"\"\"",
            "",
            "        webhook_url = os.getenv(\"WEBHOOK_URL\", None)",
            "        if webhook_url is None:",
            "            raise Exception(\"Missing webhook_url from environment\")",
            "",
            "        payload = webhook_event.model_dump_json()",
            "        headers = {\"Content-type\": \"application/json\"}",
            "",
            "        response = await self.async_http_handler.post(",
            "            url=webhook_url,",
            "            headers=headers,",
            "            data=payload,",
            "        )",
            "        if response.status_code == 200:",
            "            return True",
            "        else:",
            "            print(\"Error sending webhook alert. Error=\", response.text)  # noqa",
            "",
            "        return False",
            "",
            "    async def _check_if_using_premium_email_feature(",
            "        self,",
            "        premium_user: bool,",
            "        email_logo_url: Optional[str] = None,",
            "        email_support_contact: Optional[str] = None,",
            "    ):",
            "        from litellm.proxy.proxy_server import premium_user",
            "        from litellm.proxy.proxy_server import CommonProxyErrors",
            "",
            "        if premium_user is not True:",
            "            if email_logo_url is not None or email_support_contact is not None:",
            "                raise ValueError(",
            "                    f\"Trying to Customize Email Alerting\\n {CommonProxyErrors.not_premium_user.value}\"",
            "                )",
            "        return",
            "",
            "    async def send_key_created_or_user_invited_email(",
            "        self, webhook_event: WebhookEvent",
            "    ) -> bool:",
            "        try:",
            "            from litellm.proxy.utils import send_email",
            "",
            "            if self.alerting is None or \"email\" not in self.alerting:",
            "                # do nothing if user does not want email alerts",
            "                return False",
            "            from litellm.proxy.proxy_server import premium_user, prisma_client",
            "",
            "            email_logo_url = os.getenv(",
            "                \"SMTP_SENDER_LOGO\", os.getenv(\"EMAIL_LOGO_URL\", None)",
            "            )",
            "            email_support_contact = os.getenv(\"EMAIL_SUPPORT_CONTACT\", None)",
            "            await self._check_if_using_premium_email_feature(",
            "                premium_user, email_logo_url, email_support_contact",
            "            )",
            "            if email_logo_url is None:",
            "                email_logo_url = LITELLM_LOGO_URL",
            "            if email_support_contact is None:",
            "                email_support_contact = LITELLM_SUPPORT_CONTACT",
            "",
            "            event_name = webhook_event.event_message",
            "            recipient_email = webhook_event.user_email",
            "            recipient_user_id = webhook_event.user_id",
            "            if (",
            "                recipient_email is None",
            "                and recipient_user_id is not None",
            "                and prisma_client is not None",
            "            ):",
            "                user_row = await prisma_client.db.litellm_usertable.find_unique(",
            "                    where={\"user_id\": recipient_user_id}",
            "                )",
            "",
            "                if user_row is not None:",
            "                    recipient_email = user_row.user_email",
            "",
            "            key_name = webhook_event.key_alias",
            "            key_token = webhook_event.token",
            "            key_budget = webhook_event.max_budget",
            "            base_url = os.getenv(\"PROXY_BASE_URL\", \"http://0.0.0.0:4000\")",
            "",
            "            email_html_content = \"Alert from LiteLLM Server\"",
            "            if recipient_email is None:",
            "                verbose_proxy_logger.error(",
            "                    \"Trying to send email alert to no recipient\",",
            "                    extra=webhook_event.dict(),",
            "                )",
            "",
            "            if webhook_event.event == \"key_created\":",
            "                email_html_content = KEY_CREATED_EMAIL_TEMPLATE.format(",
            "                    email_logo_url=email_logo_url,",
            "                    recipient_email=recipient_email,",
            "                    key_budget=key_budget,",
            "                    key_token=key_token,",
            "                    base_url=base_url,",
            "                    email_support_contact=email_support_contact,",
            "                )",
            "            elif webhook_event.event == \"internal_user_created\":",
            "                # GET TEAM NAME",
            "                team_id = webhook_event.team_id",
            "                team_name = \"Default Team\"",
            "                if team_id is not None and prisma_client is not None:",
            "                    team_row = await prisma_client.db.litellm_teamtable.find_unique(",
            "                        where={\"team_id\": team_id}",
            "                    )",
            "                    if team_row is not None:",
            "                        team_name = team_row.team_alias or \"-\"",
            "                email_html_content = USER_INVITED_EMAIL_TEMPLATE.format(",
            "                    email_logo_url=email_logo_url,",
            "                    recipient_email=recipient_email,",
            "                    team_name=team_name,",
            "                    base_url=base_url,",
            "                    email_support_contact=email_support_contact,",
            "                )",
            "            else:",
            "                verbose_proxy_logger.error(",
            "                    \"Trying to send email alert on unknown webhook event\",",
            "                    extra=webhook_event.model_dump(),",
            "                )",
            "",
            "            payload = webhook_event.model_dump_json()",
            "            email_event = {",
            "                \"to\": recipient_email,",
            "                \"subject\": f\"LiteLLM: {event_name}\",",
            "                \"html\": email_html_content,",
            "            }",
            "",
            "            response = await send_email(",
            "                receiver_email=email_event[\"to\"],",
            "                subject=email_event[\"subject\"],",
            "                html=email_event[\"html\"],",
            "            )",
            "",
            "            return True",
            "",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(\"Error sending email alert %s\", str(e))",
            "            return False",
            "",
            "    async def send_email_alert_using_smtp(",
            "        self, webhook_event: WebhookEvent, alert_type: str",
            "    ) -> bool:",
            "        \"\"\"",
            "        Sends structured Email alert to an SMTP server",
            "",
            "        Currently only implemented for budget alerts",
            "",
            "        Returns -> True if sent, False if not.",
            "        \"\"\"",
            "        from litellm.proxy.utils import send_email",
            "        from litellm.proxy.proxy_server import premium_user, prisma_client",
            "",
            "        email_logo_url = os.getenv(",
            "            \"SMTP_SENDER_LOGO\", os.getenv(\"EMAIL_LOGO_URL\", None)",
            "        )",
            "        email_support_contact = os.getenv(\"EMAIL_SUPPORT_CONTACT\", None)",
            "        await self._check_if_using_premium_email_feature(",
            "            premium_user, email_logo_url, email_support_contact",
            "        )",
            "",
            "        if email_logo_url is None:",
            "            email_logo_url = LITELLM_LOGO_URL",
            "        if email_support_contact is None:",
            "            email_support_contact = LITELLM_SUPPORT_CONTACT",
            "",
            "        event_name = webhook_event.event_message",
            "        recipient_email = webhook_event.user_email",
            "        user_name = webhook_event.user_id",
            "        max_budget = webhook_event.max_budget",
            "        email_html_content = \"Alert from LiteLLM Server\"",
            "        if recipient_email is None:",
            "            verbose_proxy_logger.error(",
            "                \"Trying to send email alert to no recipient\", extra=webhook_event.dict()",
            "            )",
            "",
            "        if webhook_event.event == \"budget_crossed\":",
            "            email_html_content = f\"\"\"",
            "            <img src=\"{email_logo_url}\" alt=\"LiteLLM Logo\" width=\"150\" height=\"50\" />",
            "",
            "            <p> Hi {user_name}, <br/>",
            "",
            "            Your LLM API usage this month has reached your account's <b> monthly budget of ${max_budget} </b> <br /> <br />",
            "",
            "            API requests will be rejected until either (a) you increase your monthly budget or (b) your monthly usage resets at the beginning of the next calendar month. <br /> <br />",
            "",
            "            If you have any questions, please send an email to {email_support_contact} <br /> <br />",
            "",
            "            Best, <br />",
            "            The LiteLLM team <br />",
            "            \"\"\"",
            "",
            "        payload = webhook_event.model_dump_json()",
            "        email_event = {",
            "            \"to\": recipient_email,",
            "            \"subject\": f\"LiteLLM: {event_name}\",",
            "            \"html\": email_html_content,",
            "        }",
            "",
            "        response = await send_email(",
            "            receiver_email=email_event[\"to\"],",
            "            subject=email_event[\"subject\"],",
            "            html=email_event[\"html\"],",
            "        )",
            "        if webhook_event.event_group == \"team\":",
            "            from litellm.integrations.email_alerting import send_team_budget_alert",
            "",
            "            await send_team_budget_alert(webhook_event=webhook_event)",
            "",
            "        return False",
            "",
            "    async def send_alert(",
            "        self,",
            "        message: str,",
            "        level: Literal[\"Low\", \"Medium\", \"High\"],",
            "        alert_type: Literal[AlertType],",
            "        alerting_metadata: dict,",
            "        user_info: Optional[WebhookEvent] = None,",
            "        **kwargs,",
            "    ):",
            "        \"\"\"",
            "        Alerting based on thresholds: - https://github.com/BerriAI/litellm/issues/1298",
            "",
            "        - Responses taking too long",
            "        - Requests are hanging",
            "        - Calls are failing",
            "        - DB Read/Writes are failing",
            "        - Proxy Close to max budget",
            "        - Key Close to max budget",
            "",
            "        Parameters:",
            "            level: str - Low|Medium|High - if calls might fail (Medium) or are failing (High); Currently, no alerts would be 'Low'.",
            "            message: str - what is the alert about",
            "        \"\"\"",
            "        if self.alerting is None:",
            "            return",
            "",
            "        if (",
            "            \"webhook\" in self.alerting",
            "            and alert_type == \"budget_alerts\"",
            "            and user_info is not None",
            "        ):",
            "            await self.send_webhook_alert(webhook_event=user_info)",
            "",
            "        if (",
            "            \"email\" in self.alerting",
            "            and alert_type == \"budget_alerts\"",
            "            and user_info is not None",
            "        ):",
            "            # only send budget alerts over Email",
            "            await self.send_email_alert_using_smtp(",
            "                webhook_event=user_info, alert_type=alert_type",
            "            )",
            "",
            "        if \"slack\" not in self.alerting:",
            "            return",
            "",
            "        if alert_type not in self.alert_types:",
            "            return",
            "",
            "        from datetime import datetime",
            "        import json",
            "",
            "        # Get the current timestamp",
            "        current_time = datetime.now().strftime(\"%H:%M:%S\")",
            "        _proxy_base_url = os.getenv(\"PROXY_BASE_URL\", None)",
            "        if alert_type == \"daily_reports\" or alert_type == \"new_model_added\":",
            "            formatted_message = message",
            "        else:",
            "            formatted_message = (",
            "                f\"Level: `{level}`\\nTimestamp: `{current_time}`\\n\\nMessage: {message}\"",
            "            )",
            "",
            "        if kwargs:",
            "            for key, value in kwargs.items():",
            "                formatted_message += f\"\\n\\n{key}: `{value}`\\n\\n\"",
            "        if alerting_metadata:",
            "            for key, value in alerting_metadata.items():",
            "                formatted_message += f\"\\n\\n*Alerting Metadata*: \\n{key}: `{value}`\\n\\n\"",
            "        if _proxy_base_url is not None:",
            "            formatted_message += f\"\\n\\nProxy URL: `{_proxy_base_url}`\"",
            "",
            "        # check if we find the slack webhook url in self.alert_to_webhook_url",
            "        if (",
            "            self.alert_to_webhook_url is not None",
            "            and alert_type in self.alert_to_webhook_url",
            "        ):",
            "            slack_webhook_url = self.alert_to_webhook_url[alert_type]",
            "        elif self.default_webhook_url is not None:",
            "            slack_webhook_url = self.default_webhook_url",
            "        else:",
            "            slack_webhook_url = os.getenv(\"SLACK_WEBHOOK_URL\", None)",
            "",
            "        if slack_webhook_url is None:",
            "            raise ValueError(\"Missing SLACK_WEBHOOK_URL from environment\")",
            "        payload = {\"text\": formatted_message}",
            "        headers = {\"Content-type\": \"application/json\"}",
            "",
            "        response = await self.async_http_handler.post(",
            "            url=slack_webhook_url,",
            "            headers=headers,",
            "            data=json.dumps(payload),",
            "        )",
            "        if response.status_code == 200:",
            "            pass",
            "        else:",
            "            verbose_proxy_logger.debug(",
            "                \"Error sending slack alert. Error={}\".format(response.text)",
            "            )",
            "",
            "    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time):",
            "        \"\"\"Log deployment latency\"\"\"",
            "        try:",
            "            if \"daily_reports\" in self.alert_types:",
            "                model_id = (",
            "                    kwargs.get(\"litellm_params\", {}).get(\"model_info\", {}).get(\"id\", \"\")",
            "                )",
            "                response_s: timedelta = end_time - start_time",
            "",
            "                final_value = response_s",
            "                total_tokens = 0",
            "",
            "                if isinstance(response_obj, litellm.ModelResponse):",
            "                    completion_tokens = response_obj.usage.completion_tokens",
            "                    if completion_tokens is not None and completion_tokens > 0:",
            "                        final_value = float(",
            "                            response_s.total_seconds() / completion_tokens",
            "                        )",
            "                if isinstance(final_value, timedelta):",
            "                    final_value = final_value.total_seconds()",
            "",
            "                await self.async_update_daily_reports(",
            "                    DeploymentMetrics(",
            "                        id=model_id,",
            "                        failed_request=False,",
            "                        latency_per_output_token=final_value,",
            "                        updated_at=litellm.utils.get_utc_datetime(),",
            "                    )",
            "                )",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(",
            "                \"[Non-Blocking Error] Slack Alerting: Got error in logging LLM deployment latency: \",",
            "                e,",
            "            )",
            "            pass",
            "",
            "    async def async_log_failure_event(self, kwargs, response_obj, start_time, end_time):",
            "        \"\"\"Log failure + deployment latency\"\"\"",
            "        _litellm_params = kwargs.get(\"litellm_params\", {})",
            "        _model_info = _litellm_params.get(\"model_info\", {}) or {}",
            "        model_id = _model_info.get(\"id\", \"\")",
            "        try:",
            "            if \"daily_reports\" in self.alert_types:",
            "                try:",
            "                    await self.async_update_daily_reports(",
            "                        DeploymentMetrics(",
            "                            id=model_id,",
            "                            failed_request=True,",
            "                            latency_per_output_token=None,",
            "                            updated_at=litellm.utils.get_utc_datetime(),",
            "                        )",
            "                    )",
            "                except Exception as e:",
            "                    verbose_logger.debug(f\"Exception raises -{str(e)}\")",
            "",
            "            if isinstance(kwargs.get(\"exception\", \"\"), APIError):",
            "                if \"outage_alerts\" in self.alert_types:",
            "                    await self.outage_alerts(",
            "                        exception=kwargs[\"exception\"],",
            "                        deployment_id=model_id,",
            "                    )",
            "",
            "                if \"region_outage_alerts\" in self.alert_types:",
            "                    await self.region_outage_alerts(",
            "                        exception=kwargs[\"exception\"], deployment_id=model_id",
            "                    )",
            "        except Exception as e:",
            "            pass",
            "",
            "    async def _run_scheduler_helper(self, llm_router) -> bool:",
            "        \"\"\"",
            "        Returns:",
            "        - True -> report sent",
            "        - False -> report not sent",
            "        \"\"\"",
            "        report_sent_bool = False",
            "",
            "        report_sent = await self.internal_usage_cache.async_get_cache(",
            "            key=SlackAlertingCacheKeys.report_sent_key.value",
            "        )  # None | float",
            "",
            "        current_time = time.time()",
            "",
            "        if report_sent is None:",
            "            await self.internal_usage_cache.async_set_cache(",
            "                key=SlackAlertingCacheKeys.report_sent_key.value,",
            "                value=current_time,",
            "            )",
            "        elif isinstance(report_sent, float):",
            "            # Check if current time - interval >= time last sent",
            "            interval_seconds = self.alerting_args.daily_report_frequency",
            "",
            "            if current_time - report_sent >= interval_seconds:",
            "                # Sneak in the reporting logic here",
            "                await self.send_daily_reports(router=llm_router)",
            "                # Also, don't forget to update the report_sent time after sending the report!",
            "                await self.internal_usage_cache.async_set_cache(",
            "                    key=SlackAlertingCacheKeys.report_sent_key.value,",
            "                    value=current_time,",
            "                )",
            "                report_sent_bool = True",
            "",
            "        return report_sent_bool",
            "",
            "    async def _run_scheduled_daily_report(self, llm_router: Optional[Any] = None):",
            "        \"\"\"",
            "        If 'daily_reports' enabled",
            "",
            "        Ping redis cache every 5 minutes to check if we should send the report",
            "",
            "        If yes -> call send_daily_report()",
            "        \"\"\"",
            "        if llm_router is None or self.alert_types is None:",
            "            return",
            "",
            "        if \"daily_reports\" in self.alert_types:",
            "            while True:",
            "                await self._run_scheduler_helper(llm_router=llm_router)",
            "                interval = random.randint(",
            "                    self.alerting_args.report_check_interval - 3,",
            "                    self.alerting_args.report_check_interval + 3,",
            "                )  # shuffle to prevent collisions",
            "                await asyncio.sleep(interval)",
            "        return",
            "",
            "    async def send_weekly_spend_report(self):",
            "        \"\"\" \"\"\"",
            "        try:",
            "            from litellm.proxy.proxy_server import _get_spend_report_for_time_range",
            "",
            "            todays_date = datetime.datetime.now().date()",
            "            week_before = todays_date - datetime.timedelta(days=7)",
            "",
            "            weekly_spend_per_team, weekly_spend_per_tag = (",
            "                await _get_spend_report_for_time_range(",
            "                    start_date=week_before.strftime(\"%Y-%m-%d\"),",
            "                    end_date=todays_date.strftime(\"%Y-%m-%d\"),",
            "                )",
            "            )",
            "",
            "            _weekly_spend_message = f\"*\ud83d\udcb8 Weekly Spend Report for `{week_before.strftime('%m-%d-%Y')} - {todays_date.strftime('%m-%d-%Y')}` *\\n\"",
            "",
            "            if weekly_spend_per_team is not None:",
            "                _weekly_spend_message += \"\\n*Team Spend Report:*\\n\"",
            "                for spend in weekly_spend_per_team:",
            "                    _team_spend = spend[\"total_spend\"]",
            "                    _team_spend = float(_team_spend)",
            "                    # round to 4 decimal places",
            "                    _team_spend = round(_team_spend, 4)",
            "                    _weekly_spend_message += (",
            "                        f\"Team: `{spend['team_alias']}` | Spend: `${_team_spend}`\\n\"",
            "                    )",
            "",
            "            if weekly_spend_per_tag is not None:",
            "                _weekly_spend_message += \"\\n*Tag Spend Report:*\\n\"",
            "                for spend in weekly_spend_per_tag:",
            "                    _tag_spend = spend[\"total_spend\"]",
            "                    _tag_spend = float(_tag_spend)",
            "                    # round to 4 decimal places",
            "                    _tag_spend = round(_tag_spend, 4)",
            "                    _weekly_spend_message += f\"Tag: `{spend['individual_request_tag']}` | Spend: `${_tag_spend}`\\n\"",
            "",
            "            await self.send_alert(",
            "                message=_weekly_spend_message,",
            "                level=\"Low\",",
            "                alert_type=\"spend_reports\",",
            "                alerting_metadata={},",
            "            )",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(\"Error sending weekly spend report\", e)",
            "",
            "    async def send_monthly_spend_report(self):",
            "        \"\"\" \"\"\"",
            "        try:",
            "            from calendar import monthrange",
            "",
            "            from litellm.proxy.proxy_server import _get_spend_report_for_time_range",
            "",
            "            todays_date = datetime.datetime.now().date()",
            "            first_day_of_month = todays_date.replace(day=1)",
            "            _, last_day_of_month = monthrange(todays_date.year, todays_date.month)",
            "            last_day_of_month = first_day_of_month + datetime.timedelta(",
            "                days=last_day_of_month - 1",
            "            )",
            "",
            "            monthly_spend_per_team, monthly_spend_per_tag = (",
            "                await _get_spend_report_for_time_range(",
            "                    start_date=first_day_of_month.strftime(\"%Y-%m-%d\"),",
            "                    end_date=last_day_of_month.strftime(\"%Y-%m-%d\"),",
            "                )",
            "            )",
            "",
            "            _spend_message = f\"*\ud83d\udcb8 Monthly Spend Report for `{first_day_of_month.strftime('%m-%d-%Y')} - {last_day_of_month.strftime('%m-%d-%Y')}` *\\n\"",
            "",
            "            if monthly_spend_per_team is not None:",
            "                _spend_message += \"\\n*Team Spend Report:*\\n\"",
            "                for spend in monthly_spend_per_team:",
            "                    _team_spend = spend[\"total_spend\"]",
            "                    _team_spend = float(_team_spend)",
            "                    # round to 4 decimal places",
            "                    _team_spend = round(_team_spend, 4)",
            "                    _spend_message += (",
            "                        f\"Team: `{spend['team_alias']}` | Spend: `${_team_spend}`\\n\"",
            "                    )",
            "",
            "            if monthly_spend_per_tag is not None:",
            "                _spend_message += \"\\n*Tag Spend Report:*\\n\"",
            "                for spend in monthly_spend_per_tag:",
            "                    _tag_spend = spend[\"total_spend\"]",
            "                    _tag_spend = float(_tag_spend)",
            "                    # round to 4 decimal places",
            "                    _tag_spend = round(_tag_spend, 4)",
            "                    _spend_message += f\"Tag: `{spend['individual_request_tag']}` | Spend: `${_tag_spend}`\\n\"",
            "",
            "            await self.send_alert(",
            "                message=_spend_message,",
            "                level=\"Low\",",
            "                alert_type=\"spend_reports\",",
            "                alerting_metadata={},",
            "            )",
            "        except Exception as e:",
            "            verbose_proxy_logger.error(\"Error sending weekly spend report\", e)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "701": [
                "SlackAlerting"
            ]
        },
        "addLocation": []
    },
    "litellm/proxy/_types.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1358,
                "afterPatchRowNumber": 1358,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 1359,
                "afterPatchRowNumber": 1359,
                "PatchRowcode": "     spend: float"
            },
            "2": {
                "beforePatchRowNumber": 1360,
                "afterPatchRowNumber": 1360,
                "PatchRowcode": "     max_budget: Optional[float] = None"
            },
            "3": {
                "beforePatchRowNumber": 1361,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    token: str = Field(description=\"Hashed value of that key\")"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1361,
                "PatchRowcode": "+    token: Optional[str] = Field(default=None, description=\"Hashed value of that key\")"
            },
            "5": {
                "beforePatchRowNumber": 1362,
                "afterPatchRowNumber": 1362,
                "PatchRowcode": "     customer_id: Optional[str] = None"
            },
            "6": {
                "beforePatchRowNumber": 1363,
                "afterPatchRowNumber": 1363,
                "PatchRowcode": "     user_id: Optional[str] = None"
            },
            "7": {
                "beforePatchRowNumber": 1364,
                "afterPatchRowNumber": 1364,
                "PatchRowcode": "     team_id: Optional[str] = None"
            },
            "8": {
                "beforePatchRowNumber": 1575,
                "afterPatchRowNumber": 1575,
                "PatchRowcode": "     exception: Optional[Any] = None"
            },
            "9": {
                "beforePatchRowNumber": 1576,
                "afterPatchRowNumber": 1576,
                "PatchRowcode": "     start_time: Optional[datetime] = None"
            },
            "10": {
                "beforePatchRowNumber": 1577,
                "afterPatchRowNumber": 1577,
                "PatchRowcode": "     end_time: Optional[datetime] = None"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1578,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1579,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1580,
                "PatchRowcode": "+class ProxyException(Exception):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1581,
                "PatchRowcode": "+    # NOTE: DO NOT MODIFY THIS"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1582,
                "PatchRowcode": "+    # This is used to map exactly to OPENAI Exceptions"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1583,
                "PatchRowcode": "+    def __init__("
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1584,
                "PatchRowcode": "+        self,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1585,
                "PatchRowcode": "+        message: str,"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1586,
                "PatchRowcode": "+        type: str,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1587,
                "PatchRowcode": "+        param: Optional[str],"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1588,
                "PatchRowcode": "+        code: Optional[int],"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1589,
                "PatchRowcode": "+    ):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1590,
                "PatchRowcode": "+        self.message = message"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1591,
                "PatchRowcode": "+        self.type = type"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1592,
                "PatchRowcode": "+        self.param = param"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1593,
                "PatchRowcode": "+        self.code = code"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1594,
                "PatchRowcode": "+"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1595,
                "PatchRowcode": "+        # rules for proxyExceptions"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1596,
                "PatchRowcode": "+        # Litellm router.py returns \"No healthy deployment available\" when there are no deployments available"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1597,
                "PatchRowcode": "+        # Should map to 429 errors https://github.com/BerriAI/litellm/issues/2487"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1598,
                "PatchRowcode": "+        if ("
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1599,
                "PatchRowcode": "+            \"No healthy deployment available\" in self.message"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1600,
                "PatchRowcode": "+            or \"No deployments available\" in self.message"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1601,
                "PatchRowcode": "+        ):"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1602,
                "PatchRowcode": "+            self.code = 429"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1603,
                "PatchRowcode": "+"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1604,
                "PatchRowcode": "+    def to_dict(self) -> dict:"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1605,
                "PatchRowcode": "+        \"\"\"Converts the ProxyException instance to a dictionary.\"\"\""
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1606,
                "PatchRowcode": "+        return {"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1607,
                "PatchRowcode": "+            \"message\": self.message,"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1608,
                "PatchRowcode": "+            \"type\": self.type,"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1609,
                "PatchRowcode": "+            \"param\": self.param,"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1610,
                "PatchRowcode": "+            \"code\": self.code,"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1611,
                "PatchRowcode": "+        }"
            }
        },
        "frontPatchFile": [
            "from pydantic import BaseModel, Extra, Field, model_validator, Json, ConfigDict",
            "from dataclasses import fields",
            "import enum",
            "from typing import Optional, List, Union, Dict, Literal, Any, TypedDict, TYPE_CHECKING",
            "from datetime import datetime",
            "import uuid, json, sys, os",
            "from litellm.types.router import UpdateRouterConfig",
            "from litellm.types.utils import ProviderField",
            "from typing_extensions import Annotated",
            "",
            "",
            "if TYPE_CHECKING:",
            "    from opentelemetry.trace import Span as _Span",
            "",
            "    Span = _Span",
            "else:",
            "    Span = Any",
            "",
            "",
            "class LitellmUserRoles(str, enum.Enum):",
            "    \"\"\"",
            "    Admin Roles:",
            "    PROXY_ADMIN: admin over the platform",
            "    PROXY_ADMIN_VIEW_ONLY: can login, view all own keys, view all spend",
            "",
            "    Internal User Roles:",
            "    INTERNAL_USER: can login, view/create/delete their own keys, view their spend",
            "    INTERNAL_USER_VIEW_ONLY: can login, view their own keys, view their own spend",
            "",
            "",
            "    Team Roles:",
            "    TEAM: used for JWT auth",
            "",
            "",
            "    Customer Roles:",
            "    CUSTOMER: External users -> these are customers",
            "",
            "    \"\"\"",
            "",
            "    # Admin Roles",
            "    PROXY_ADMIN = \"proxy_admin\"",
            "    PROXY_ADMIN_VIEW_ONLY = \"proxy_admin_viewer\"",
            "",
            "    # Internal User Roles",
            "    INTERNAL_USER = \"internal_user\"",
            "    INTERNAL_USER_VIEW_ONLY = \"internal_user_viewer\"",
            "",
            "    # Team Roles",
            "    TEAM = \"team\"",
            "",
            "    # Customer Roles - External users of proxy",
            "    CUSTOMER = \"customer\"",
            "",
            "    def __str__(self):",
            "        return str(self.value)",
            "",
            "    @property",
            "    def description(self):",
            "        \"\"\"",
            "        Descriptions for the enum values",
            "        \"\"\"",
            "        descriptions = {",
            "            \"proxy_admin\": \"admin over litellm proxy, has all permissions\",",
            "            \"proxy_admin_viewer\": \"view all keys, view all spend\",",
            "            \"internal_user\": \"view/create/delete their own keys, view their own spend\",",
            "            \"internal_user_viewer\": \"view their own keys, view their own spend\",",
            "            \"team\": \"team scope used for JWT auth\",",
            "            \"customer\": \"customer\",",
            "        }",
            "        return descriptions.get(self.value, \"\")",
            "",
            "    @property",
            "    def ui_label(self):",
            "        \"\"\"",
            "        UI labels for the enum values",
            "        \"\"\"",
            "        ui_labels = {",
            "            \"proxy_admin\": \"Admin (All Permissions)\",",
            "            \"proxy_admin_viewer\": \"Admin (View Only)\",",
            "            \"internal_user\": \"Internal User (Create/Delete/View)\",",
            "            \"internal_user_viewer\": \"Internal User (View Only)\",",
            "            \"team\": \"Team\",",
            "            \"customer\": \"Customer\",",
            "        }",
            "        return ui_labels.get(self.value, \"\")",
            "",
            "",
            "class LitellmTableNames(str, enum.Enum):",
            "    \"\"\"",
            "    Enum for Table Names used by LiteLLM",
            "    \"\"\"",
            "",
            "    TEAM_TABLE_NAME: str = \"LiteLLM_TeamTable\"",
            "    USER_TABLE_NAME: str = \"LiteLLM_UserTable\"",
            "    KEY_TABLE_NAME: str = \"LiteLLM_VerificationToken\"",
            "    PROXY_MODEL_TABLE_NAME: str = \"LiteLLM_ModelTable\"",
            "",
            "",
            "AlertType = Literal[",
            "    \"llm_exceptions\",",
            "    \"llm_too_slow\",",
            "    \"llm_requests_hanging\",",
            "    \"budget_alerts\",",
            "    \"db_exceptions\",",
            "    \"daily_reports\",",
            "    \"spend_reports\",",
            "    \"cooldown_deployment\",",
            "    \"new_model_added\",",
            "    \"outage_alerts\",",
            "    \"region_outage_alerts\",",
            "]",
            "",
            "",
            "def hash_token(token: str):",
            "    import hashlib",
            "",
            "    # Hash the string using SHA-256",
            "    hashed_token = hashlib.sha256(token.encode()).hexdigest()",
            "",
            "    return hashed_token",
            "",
            "",
            "class LiteLLMBase(BaseModel):",
            "    \"\"\"",
            "    Implements default functions, all pydantic objects should have.",
            "    \"\"\"",
            "",
            "    def json(self, **kwargs):",
            "        try:",
            "            return self.model_dump(**kwargs)  # noqa",
            "        except Exception as e:",
            "            # if using pydantic v1",
            "            return self.dict(**kwargs)",
            "",
            "    def fields_set(self):",
            "        try:",
            "            return self.model_fields_set  # noqa",
            "        except:",
            "            # if using pydantic v1",
            "            return self.__fields_set__",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_UpperboundKeyGenerateParams(LiteLLMBase):",
            "    \"\"\"",
            "    Set default upperbound to max budget a key called via `/key/generate` can be.",
            "    \"\"\"",
            "",
            "    max_budget: Optional[float] = None",
            "    budget_duration: Optional[str] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "",
            "",
            "class LiteLLMRoutes(enum.Enum):",
            "    openai_route_names: List = [",
            "        \"chat_completion\",",
            "        \"completion\",",
            "        \"embeddings\",",
            "        \"image_generation\",",
            "        \"audio_transcriptions\",",
            "        \"moderations\",",
            "        \"model_list\",  # OpenAI /v1/models route",
            "    ]",
            "    openai_routes: List = [",
            "        # chat completions",
            "        \"/engines/{model}/chat/completions\",",
            "        \"/openai/deployments/{model}/chat/completions\",",
            "        \"/chat/completions\",",
            "        \"/v1/chat/completions\",",
            "        # completions",
            "        \"/openai/deployments/{model}/completions\",",
            "        \"/completions\",",
            "        \"/v1/completions\",",
            "        # embeddings",
            "        \"/openai/deployments/{model}/embeddings\",",
            "        \"/embeddings\",",
            "        \"/v1/embeddings\",",
            "        # image generation",
            "        \"/images/generations\",",
            "        \"/v1/images/generations\",",
            "        # audio transcription",
            "        \"/audio/transcriptions\",",
            "        \"/v1/audio/transcriptions\",",
            "        # moderations",
            "        \"/moderations\",",
            "        \"/v1/moderations\",",
            "        # batches",
            "        \"/v1/batches\",",
            "        \"/batches\",",
            "        \"/v1/batches{batch_id}\",",
            "        \"/batches{batch_id}\",",
            "        # files",
            "        \"/v1/files\",",
            "        \"/files\",",
            "        # models",
            "        \"/models\",",
            "        \"/v1/models\",",
            "        # token counter",
            "        \"/utils/token_counter\",",
            "    ]",
            "",
            "    info_routes: List = [",
            "        \"/key/info\",",
            "        \"/team/info\",",
            "        \"/team/list\",",
            "        \"/user/info\",",
            "        \"/model/info\",",
            "        \"/v2/model/info\",",
            "        \"/v2/key/info\",",
            "        \"/model_group/info\",",
            "    ]",
            "",
            "    # NOTE: ROUTES ONLY FOR MASTER KEY - only the Master Key should be able to Reset Spend",
            "    master_key_only_routes: List = [",
            "        \"/global/spend/reset\",",
            "    ]",
            "",
            "    sso_only_routes: List = [",
            "        \"/key/generate\",",
            "        \"/key/update\",",
            "        \"/key/delete\",",
            "        \"/global/spend/logs\",",
            "        \"/global/predict/spend/logs\",",
            "        \"/sso/get/logout_url\",",
            "    ]",
            "",
            "    management_routes: List = [  # key",
            "        \"/key/generate\",",
            "        \"/key/update\",",
            "        \"/key/delete\",",
            "        \"/key/info\",",
            "        # user",
            "        \"/user/new\",",
            "        \"/user/update\",",
            "        \"/user/delete\",",
            "        \"/user/info\",",
            "        # team",
            "        \"/team/new\",",
            "        \"/team/update\",",
            "        \"/team/delete\",",
            "        \"/team/list\",",
            "        \"/team/info\",",
            "        \"/team/block\",",
            "        \"/team/unblock\",",
            "        # model",
            "        \"/model/new\",",
            "        \"/model/update\",",
            "        \"/model/delete\",",
            "        \"/model/info\",",
            "    ]",
            "",
            "    spend_tracking_routes: List = [",
            "        # spend",
            "        \"/spend/keys\",",
            "        \"/spend/users\",",
            "        \"/spend/tags\",",
            "        \"/spend/calculate\",",
            "        \"/spend/logs\",",
            "    ]",
            "",
            "    global_spend_tracking_routes: List = [",
            "        # global spend",
            "        \"/global/spend/logs\",",
            "        \"/global/spend\",",
            "        \"/global/spend/keys\",",
            "        \"/global/spend/teams\",",
            "        \"/global/spend/end_users\",",
            "        \"/global/spend/models\",",
            "        \"/global/predict/spend/logs\",",
            "        \"/global/spend/report\",",
            "    ]",
            "",
            "    public_routes: List = [",
            "        \"/routes\",",
            "        \"/\",",
            "        \"/health/liveliness\",",
            "        \"/health/readiness\",",
            "        \"/test\",",
            "        \"/config/yaml\",",
            "        \"/metrics\",",
            "    ]",
            "",
            "    internal_user_routes: List = [",
            "        \"/key/generate\",",
            "        \"/key/update\",",
            "        \"/key/delete\",",
            "        \"/key/info\",",
            "    ] + spend_tracking_routes",
            "",
            "",
            "# class LiteLLMAllowedRoutes(LiteLLMBase):",
            "#     \"\"\"",
            "#     Defines allowed routes based on key type.",
            "",
            "#     Types = [\"admin\", \"team\", \"user\", \"unmapped\"]",
            "#     \"\"\"",
            "",
            "#     admin_allowed_routes: List[",
            "#         Literal[\"openai_routes\", \"info_routes\", \"management_routes\", \"spend_tracking_routes\", \"global_spend_tracking_routes\"]",
            "#     ] = [\"management_routes\"]",
            "",
            "",
            "class LiteLLM_JWTAuth(LiteLLMBase):",
            "    \"\"\"",
            "    A class to define the roles and permissions for a LiteLLM Proxy w/ JWT Auth.",
            "",
            "    Attributes:",
            "    - admin_jwt_scope: The JWT scope required for proxy admin roles.",
            "    - admin_allowed_routes: list of allowed routes for proxy admin roles.",
            "    - team_jwt_scope: The JWT scope required for proxy team roles.",
            "    - team_id_jwt_field: The field in the JWT token that stores the team ID. Default - `client_id`.",
            "    - team_allowed_routes: list of allowed routes for proxy team roles.",
            "    - user_id_jwt_field: The field in the JWT token that stores the user id (maps to `LiteLLMUserTable`). Use this for internal employees.",
            "    - end_user_id_jwt_field: The field in the JWT token that stores the end-user ID (maps to `LiteLLMEndUserTable`). Turn this off by setting to `None`. Enables end-user cost tracking. Use this for external customers.",
            "    - public_key_ttl: Default - 600s. TTL for caching public JWT keys.",
            "",
            "    See `auth_checks.py` for the specific routes",
            "    \"\"\"",
            "",
            "    admin_jwt_scope: str = \"litellm_proxy_admin\"",
            "    admin_allowed_routes: List[",
            "        Literal[",
            "            \"openai_routes\",",
            "            \"info_routes\",",
            "            \"management_routes\",",
            "            \"spend_tracking_routes\",",
            "            \"global_spend_tracking_routes\",",
            "        ]",
            "    ] = [",
            "        \"management_routes\",",
            "        \"spend_tracking_routes\",",
            "        \"global_spend_tracking_routes\",",
            "        \"info_routes\",",
            "    ]",
            "    team_id_jwt_field: Optional[str] = None",
            "    team_allowed_routes: List[",
            "        Literal[\"openai_routes\", \"info_routes\", \"management_routes\"]",
            "    ] = [\"openai_routes\", \"info_routes\"]",
            "    team_id_default: Optional[str] = Field(",
            "        default=None,",
            "        description=\"If no team_id given, default permissions/spend-tracking to this team.s\",",
            "    )",
            "    org_id_jwt_field: Optional[str] = None",
            "    user_id_jwt_field: Optional[str] = None",
            "    user_id_upsert: bool = Field(",
            "        default=False, description=\"If user doesn't exist, upsert them into the db.\"",
            "    )",
            "    end_user_id_jwt_field: Optional[str] = None",
            "    public_key_ttl: float = 600",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        # get the attribute names for this Pydantic model",
            "        allowed_keys = self.__annotations__.keys()",
            "",
            "        invalid_keys = set(kwargs.keys()) - allowed_keys",
            "",
            "        if invalid_keys:",
            "            raise ValueError(",
            "                f\"Invalid arguments provided: {', '.join(invalid_keys)}. Allowed arguments are: {', '.join(allowed_keys)}.\"",
            "            )",
            "",
            "        super().__init__(**kwargs)",
            "",
            "",
            "class LiteLLMPromptInjectionParams(LiteLLMBase):",
            "    heuristics_check: bool = False",
            "    vector_db_check: bool = False",
            "    llm_api_check: bool = False",
            "    llm_api_name: Optional[str] = None",
            "    llm_api_system_prompt: Optional[str] = None",
            "    llm_api_fail_call_string: Optional[str] = None",
            "    reject_as_response: Optional[bool] = Field(",
            "        default=False,",
            "        description=\"Return rejected request error message as a string to the user. Default behaviour is to raise an exception.\",",
            "    )",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_llm_api_params(cls, values):",
            "        llm_api_check = values.get(\"llm_api_check\")",
            "        if llm_api_check is True:",
            "            if \"llm_api_name\" not in values or not values[\"llm_api_name\"]:",
            "                raise ValueError(",
            "                    \"If llm_api_check is set to True, llm_api_name must be provided\"",
            "                )",
            "            if (",
            "                \"llm_api_system_prompt\" not in values",
            "                or not values[\"llm_api_system_prompt\"]",
            "            ):",
            "                raise ValueError(",
            "                    \"If llm_api_check is set to True, llm_api_system_prompt must be provided\"",
            "                )",
            "            if (",
            "                \"llm_api_fail_call_string\" not in values",
            "                or not values[\"llm_api_fail_call_string\"]",
            "            ):",
            "                raise ValueError(",
            "                    \"If llm_api_check is set to True, llm_api_fail_call_string must be provided\"",
            "                )",
            "        return values",
            "",
            "",
            "######### Request Class Definition ######",
            "class ProxyChatCompletionRequest(LiteLLMBase):",
            "    model: str",
            "    messages: List[Dict[str, str]]",
            "    temperature: Optional[float] = None",
            "    top_p: Optional[float] = None",
            "    n: Optional[int] = None",
            "    stream: Optional[bool] = None",
            "    stop: Optional[List[str]] = None",
            "    max_tokens: Optional[int] = None",
            "    presence_penalty: Optional[float] = None",
            "    frequency_penalty: Optional[float] = None",
            "    logit_bias: Optional[Dict[str, float]] = None",
            "    user: Optional[str] = None",
            "    response_format: Optional[Dict[str, str]] = None",
            "    seed: Optional[int] = None",
            "    tools: Optional[List[str]] = None",
            "    tool_choice: Optional[str] = None",
            "    functions: Optional[List[str]] = None  # soon to be deprecated",
            "    function_call: Optional[str] = None  # soon to be deprecated",
            "",
            "    # Optional LiteLLM params",
            "    caching: Optional[bool] = None",
            "    api_base: Optional[str] = None",
            "    api_version: Optional[str] = None",
            "    api_key: Optional[str] = None",
            "    num_retries: Optional[int] = None",
            "    context_window_fallback_dict: Optional[Dict[str, str]] = None",
            "    fallbacks: Optional[List[str]] = None",
            "    metadata: Optional[Dict[str, str]] = {}",
            "    deployment_id: Optional[str] = None",
            "    request_timeout: Optional[int] = None",
            "",
            "    model_config = ConfigDict(",
            "        extra=\"allow\"",
            "    )  # allow params not defined here, these fall in litellm.completion(**kwargs)",
            "",
            "",
            "class ModelInfoDelete(LiteLLMBase):",
            "    id: str",
            "",
            "",
            "class ModelInfo(LiteLLMBase):",
            "    id: Optional[str]",
            "    mode: Optional[Literal[\"embedding\", \"chat\", \"completion\"]]",
            "    input_cost_per_token: Optional[float] = 0.0",
            "    output_cost_per_token: Optional[float] = 0.0",
            "    max_tokens: Optional[int] = 2048  # assume 2048 if not set",
            "",
            "    # for azure models we need users to specify the base model, one azure you can call deployments - azure/my-random-model",
            "    # we look up the base model in model_prices_and_context_window.json",
            "    base_model: Optional[",
            "        Literal[",
            "            \"gpt-4-1106-preview\",",
            "            \"gpt-4-32k\",",
            "            \"gpt-4\",",
            "            \"gpt-3.5-turbo-16k\",",
            "            \"gpt-3.5-turbo\",",
            "            \"text-embedding-ada-002\",",
            "        ]",
            "    ]",
            "",
            "    model_config = ConfigDict(protected_namespaces=(), extra=\"allow\")",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"id\") is None:",
            "            values.update({\"id\": str(uuid.uuid4())})",
            "        if values.get(\"mode\") is None:",
            "            values.update({\"mode\": None})",
            "        if values.get(\"input_cost_per_token\") is None:",
            "            values.update({\"input_cost_per_token\": None})",
            "        if values.get(\"output_cost_per_token\") is None:",
            "            values.update({\"output_cost_per_token\": None})",
            "        if values.get(\"max_tokens\") is None:",
            "            values.update({\"max_tokens\": None})",
            "        if values.get(\"base_model\") is None:",
            "            values.update({\"base_model\": None})",
            "        return values",
            "",
            "",
            "class ProviderInfo(LiteLLMBase):",
            "    name: str",
            "    fields: List[ProviderField]",
            "",
            "",
            "class BlockUsers(LiteLLMBase):",
            "    user_ids: List[str]  # required",
            "",
            "",
            "class ModelParams(LiteLLMBase):",
            "    model_name: str",
            "    litellm_params: dict",
            "    model_info: ModelInfo",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"model_info\") is None:",
            "            values.update({\"model_info\": ModelInfo()})",
            "        return values",
            "",
            "",
            "class GenerateRequestBase(LiteLLMBase):",
            "    \"\"\"",
            "    Overlapping schema between key and user generate/update requests",
            "    \"\"\"",
            "",
            "    models: Optional[list] = []",
            "    spend: Optional[float] = 0",
            "    max_budget: Optional[float] = None",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    metadata: Optional[dict] = {}",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    budget_duration: Optional[str] = None",
            "    allowed_cache_controls: Optional[list] = []",
            "    soft_budget: Optional[float] = None",
            "",
            "",
            "class GenerateKeyRequest(GenerateRequestBase):",
            "    key_alias: Optional[str] = None",
            "    duration: Optional[str] = None",
            "    aliases: Optional[dict] = {}",
            "    config: Optional[dict] = {}",
            "    permissions: Optional[dict] = {}",
            "    model_max_budget: Optional[dict] = (",
            "        {}",
            "    )  # {\"gpt-4\": 5.0, \"gpt-3.5-turbo\": 5.0}, defaults to {}",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "    send_invite_email: Optional[bool] = None",
            "",
            "",
            "class GenerateKeyResponse(GenerateKeyRequest):",
            "    key: str",
            "    key_name: Optional[str] = None",
            "    expires: Optional[datetime]",
            "    user_id: Optional[str] = None",
            "    token_id: Optional[str] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"token\") is not None:",
            "            values.update({\"key\": values.get(\"token\")})",
            "        dict_fields = [",
            "            \"metadata\",",
            "            \"aliases\",",
            "            \"config\",",
            "            \"permissions\",",
            "            \"model_max_budget\",",
            "        ]",
            "        for field in dict_fields:",
            "            value = values.get(field)",
            "            if value is not None and isinstance(value, str):",
            "                try:",
            "                    values[field] = json.loads(value)",
            "                except json.JSONDecodeError:",
            "                    raise ValueError(f\"Field {field} should be a valid dictionary\")",
            "",
            "        return values",
            "",
            "",
            "class UpdateKeyRequest(GenerateKeyRequest):",
            "    # Note: the defaults of all Params here MUST BE NONE",
            "    # else they will get overwritten",
            "    key: str",
            "    duration: Optional[str] = None",
            "    spend: Optional[float] = None",
            "    metadata: Optional[dict] = None",
            "",
            "",
            "class KeyRequest(LiteLLMBase):",
            "    keys: List[str]",
            "",
            "",
            "class LiteLLM_ModelTable(LiteLLMBase):",
            "    model_aliases: Optional[str] = None  # json dump the dict",
            "    created_by: str",
            "    updated_by: str",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class NewUserRequest(GenerateKeyRequest):",
            "    max_budget: Optional[float] = None",
            "    user_email: Optional[str] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    teams: Optional[list] = None",
            "    organization_id: Optional[str] = None",
            "    auto_create_key: bool = (",
            "        True  # flag used for returning a key as part of the /user/new response",
            "    )",
            "    send_invite_email: Optional[bool] = None",
            "",
            "",
            "class NewUserResponse(GenerateKeyResponse):",
            "    max_budget: Optional[float] = None",
            "    user_email: Optional[str] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    teams: Optional[list] = None",
            "    organization_id: Optional[str] = None",
            "",
            "",
            "class UpdateUserRequest(GenerateRequestBase):",
            "    # Note: the defaults of all Params here MUST BE NONE",
            "    # else they will get overwritten",
            "    user_id: Optional[str] = None",
            "    password: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "    spend: Optional[float] = None",
            "    metadata: Optional[dict] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    max_budget: Optional[float] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:",
            "            raise ValueError(\"Either user id or user email must be provided\")",
            "        return values",
            "",
            "",
            "class NewCustomerRequest(LiteLLMBase):",
            "    \"\"\"",
            "    Create a new customer, allocate a budget to them",
            "    \"\"\"",
            "",
            "    user_id: str",
            "    alias: Optional[str] = None  # human-friendly alias",
            "    blocked: bool = False  # allow/disallow requests for this end-user",
            "    max_budget: Optional[float] = None",
            "    budget_id: Optional[str] = None  # give either a budget_id or max_budget",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = (",
            "        None  # require all user requests to use models in this specific region",
            "    )",
            "    default_model: Optional[str] = (",
            "        None  # if no equivalent model in allowed region - default all requests to this model",
            "    )",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if values.get(\"max_budget\") is not None and values.get(\"budget_id\") is not None:",
            "            raise ValueError(\"Set either 'max_budget' or 'budget_id', not both.\")",
            "",
            "        return values",
            "",
            "",
            "class UpdateCustomerRequest(LiteLLMBase):",
            "    \"\"\"",
            "    Update a Customer, use this to update customer budgets etc",
            "",
            "    \"\"\"",
            "",
            "    user_id: str",
            "    alias: Optional[str] = None  # human-friendly alias",
            "    blocked: bool = False  # allow/disallow requests for this end-user",
            "    max_budget: Optional[float] = None",
            "    budget_id: Optional[str] = None  # give either a budget_id or max_budget",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = (",
            "        None  # require all user requests to use models in this specific region",
            "    )",
            "    default_model: Optional[str] = (",
            "        None  # if no equivalent model in allowed region - default all requests to this model",
            "    )",
            "",
            "",
            "class DeleteCustomerRequest(LiteLLMBase):",
            "    \"\"\"",
            "    Delete multiple Customers",
            "    \"\"\"",
            "",
            "    user_ids: List[str]",
            "",
            "",
            "class Member(LiteLLMBase):",
            "    role: Literal[\"admin\", \"user\"]",
            "    user_id: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if not isinstance(values, dict):",
            "            raise ValueError(\"input needs to be a dictionary\")",
            "        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:",
            "            raise ValueError(\"Either user id or user email must be provided\")",
            "        return values",
            "",
            "",
            "class TeamBase(LiteLLMBase):",
            "    team_alias: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    organization_id: Optional[str] = None",
            "    admins: list = []",
            "    members: list = []",
            "    members_with_roles: List[Member] = []",
            "    metadata: Optional[dict] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "",
            "    # Budget fields",
            "    max_budget: Optional[float] = None",
            "    budget_duration: Optional[str] = None",
            "",
            "    models: list = []",
            "    blocked: bool = False",
            "",
            "",
            "class NewTeamRequest(TeamBase):",
            "    model_aliases: Optional[dict] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class GlobalEndUsersSpend(LiteLLMBase):",
            "    api_key: Optional[str] = None",
            "    startTime: Optional[datetime] = None",
            "    endTime: Optional[datetime] = None",
            "",
            "",
            "class TeamMemberAddRequest(LiteLLMBase):",
            "    team_id: str",
            "    member: Union[List[Member], Member]",
            "    max_budget_in_team: Optional[float] = None  # Users max budget within the team",
            "",
            "    def __init__(self, **data):",
            "        member_data = data.get(\"member\")",
            "        if isinstance(member_data, list):",
            "            # If member is a list of dictionaries, convert each dictionary to a Member object",
            "            members = [Member(**item) for item in member_data]",
            "            # Replace member_data with the list of Member objects",
            "            data[\"member\"] = members",
            "        elif isinstance(member_data, dict):",
            "            # If member is a dictionary, convert it to a single Member object",
            "            member = Member(**member_data)",
            "            # Replace member_data with the single Member object",
            "            data[\"member\"] = member",
            "        # Call the superclass __init__ method to initialize the object",
            "        super().__init__(**data)",
            "",
            "",
            "class TeamMemberDeleteRequest(LiteLLMBase):",
            "    team_id: str",
            "    user_id: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:",
            "            raise ValueError(\"Either user id or user email must be provided\")",
            "        return values",
            "",
            "",
            "class UpdateTeamRequest(LiteLLMBase):",
            "    \"\"\"",
            "    UpdateTeamRequest, used by /team/update when you need to update a team",
            "",
            "    team_id: str",
            "    team_alias: Optional[str] = None",
            "    organization_id: Optional[str] = None",
            "    metadata: Optional[dict] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    max_budget: Optional[float] = None",
            "    models: Optional[list] = None",
            "    blocked: Optional[bool] = None",
            "    budget_duration: Optional[str] = None",
            "    \"\"\"",
            "",
            "    team_id: str  # required",
            "    team_alias: Optional[str] = None",
            "    organization_id: Optional[str] = None",
            "    metadata: Optional[dict] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    max_budget: Optional[float] = None",
            "    models: Optional[list] = None",
            "    blocked: Optional[bool] = None",
            "    budget_duration: Optional[str] = None",
            "",
            "",
            "class ResetTeamBudgetRequest(LiteLLMBase):",
            "    \"\"\"",
            "    internal type used to reset the budget on a team",
            "    used by reset_budget()",
            "",
            "    team_id: str",
            "    spend: float",
            "    budget_reset_at: datetime",
            "    \"\"\"",
            "",
            "    team_id: str",
            "    spend: float",
            "    budget_reset_at: datetime",
            "    updated_at: datetime",
            "",
            "",
            "class DeleteTeamRequest(LiteLLMBase):",
            "    team_ids: List[str]  # required",
            "",
            "",
            "class BlockTeamRequest(LiteLLMBase):",
            "    team_id: str  # required",
            "",
            "",
            "class LiteLLM_TeamTable(TeamBase):",
            "    spend: Optional[float] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    budget_duration: Optional[str] = None",
            "    budget_reset_at: Optional[datetime] = None",
            "    model_id: Optional[int] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        dict_fields = [",
            "            \"metadata\",",
            "            \"aliases\",",
            "            \"config\",",
            "            \"permissions\",",
            "            \"model_max_budget\",",
            "            \"model_aliases\",",
            "        ]",
            "        for field in dict_fields:",
            "            value = values.get(field)",
            "            if value is not None and isinstance(value, str):",
            "                try:",
            "                    values[field] = json.loads(value)",
            "                except json.JSONDecodeError:",
            "                    raise ValueError(f\"Field {field} should be a valid dictionary\")",
            "",
            "        return values",
            "",
            "",
            "class TeamRequest(LiteLLMBase):",
            "    teams: List[str]",
            "",
            "",
            "class LiteLLM_BudgetTable(LiteLLMBase):",
            "    \"\"\"Represents user-controllable params for a LiteLLM_BudgetTable record\"\"\"",
            "",
            "    soft_budget: Optional[float] = None",
            "    max_budget: Optional[float] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    model_max_budget: Optional[dict] = None",
            "    budget_duration: Optional[str] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_TeamMemberTable(LiteLLM_BudgetTable):",
            "    \"\"\"",
            "    Used to track spend of a user_id within a team_id",
            "    \"\"\"",
            "",
            "    spend: Optional[float] = None",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    budget_id: Optional[str] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class NewOrganizationRequest(LiteLLM_BudgetTable):",
            "    organization_id: Optional[str] = None",
            "    organization_alias: str",
            "    models: List = []",
            "    budget_id: Optional[str] = None",
            "",
            "",
            "class LiteLLM_OrganizationTable(LiteLLMBase):",
            "    \"\"\"Represents user-controllable params for a LiteLLM_OrganizationTable record\"\"\"",
            "",
            "    organization_id: Optional[str] = None",
            "    organization_alias: Optional[str] = None",
            "    budget_id: str",
            "    metadata: Optional[dict] = None",
            "    models: List[str]",
            "    created_by: str",
            "    updated_by: str",
            "",
            "",
            "class NewOrganizationResponse(LiteLLM_OrganizationTable):",
            "    organization_id: str",
            "    created_at: datetime",
            "    updated_at: datetime",
            "",
            "",
            "class OrganizationRequest(LiteLLMBase):",
            "    organizations: List[str]",
            "",
            "",
            "class BudgetNew(LiteLLMBase):",
            "    budget_id: str = Field(default=None, description=\"The unique budget id.\")",
            "    max_budget: Optional[float] = Field(",
            "        default=None,",
            "        description=\"Requests will fail if this budget (in USD) is exceeded.\",",
            "    )",
            "    soft_budget: Optional[float] = Field(",
            "        default=None,",
            "        description=\"Requests will NOT fail if this is exceeded. Will fire alerting though.\",",
            "    )",
            "    max_parallel_requests: Optional[int] = Field(",
            "        default=None, description=\"Max concurrent requests allowed for this budget id.\"",
            "    )",
            "    tpm_limit: Optional[int] = Field(",
            "        default=None, description=\"Max tokens per minute, allowed for this budget id.\"",
            "    )",
            "    rpm_limit: Optional[int] = Field(",
            "        default=None, description=\"Max requests per minute, allowed for this budget id.\"",
            "    )",
            "    budget_duration: Optional[str] = Field(",
            "        default=None,",
            "        description=\"Max duration budget should be set for (e.g. '1hr', '1d', '28d')\",",
            "    )",
            "",
            "",
            "class BudgetRequest(LiteLLMBase):",
            "    budgets: List[str]",
            "",
            "",
            "class BudgetDeleteRequest(LiteLLMBase):",
            "    id: str",
            "",
            "",
            "class KeyManagementSystem(enum.Enum):",
            "    GOOGLE_KMS = \"google_kms\"",
            "    AZURE_KEY_VAULT = \"azure_key_vault\"",
            "    AWS_SECRET_MANAGER = \"aws_secret_manager\"",
            "    LOCAL = \"local\"",
            "    AWS_KMS = \"aws_kms\"",
            "",
            "",
            "class KeyManagementSettings(LiteLLMBase):",
            "    hosted_keys: List",
            "",
            "",
            "class TeamDefaultSettings(LiteLLMBase):",
            "    team_id: str",
            "",
            "    model_config = ConfigDict(",
            "        extra=\"allow\"",
            "    )  # allow params not defined here, these fall in litellm.completion(**kwargs)",
            "",
            "",
            "class DynamoDBArgs(LiteLLMBase):",
            "    billing_mode: Literal[\"PROVISIONED_THROUGHPUT\", \"PAY_PER_REQUEST\"]",
            "    read_capacity_units: Optional[int] = None",
            "    write_capacity_units: Optional[int] = None",
            "    ssl_verify: Optional[bool] = None",
            "    region_name: str",
            "    user_table_name: str = \"LiteLLM_UserTable\"",
            "    key_table_name: str = \"LiteLLM_VerificationToken\"",
            "    config_table_name: str = \"LiteLLM_Config\"",
            "    spend_table_name: str = \"LiteLLM_SpendLogs\"",
            "    aws_role_name: Optional[str] = None",
            "    aws_session_name: Optional[str] = None",
            "    aws_web_identity_token: Optional[str] = None",
            "    aws_provider_id: Optional[str] = None",
            "    aws_policy_arns: Optional[List[str]] = None",
            "    aws_policy: Optional[str] = None",
            "    aws_duration_seconds: Optional[int] = None",
            "    assume_role_aws_role_name: Optional[str] = None",
            "    assume_role_aws_session_name: Optional[str] = None",
            "",
            "",
            "class ConfigFieldUpdate(LiteLLMBase):",
            "    field_name: str",
            "    field_value: Any",
            "    config_type: Literal[\"general_settings\"]",
            "",
            "",
            "class ConfigFieldDelete(LiteLLMBase):",
            "    config_type: Literal[\"general_settings\"]",
            "    field_name: str",
            "",
            "",
            "class ConfigList(LiteLLMBase):",
            "    field_name: str",
            "    field_type: str",
            "    field_description: str",
            "    field_value: Any",
            "    stored_in_db: Optional[bool]",
            "    field_default_value: Any",
            "    premium_field: bool = False",
            "",
            "",
            "class ConfigGeneralSettings(LiteLLMBase):",
            "    \"\"\"",
            "    Documents all the fields supported by `general_settings` in config.yaml",
            "    \"\"\"",
            "",
            "    completion_model: Optional[str] = Field(",
            "        None, description=\"proxy level default model for all chat completion calls\"",
            "    )",
            "    key_management_system: Optional[KeyManagementSystem] = Field(",
            "        None, description=\"key manager to load keys from / decrypt keys with\"",
            "    )",
            "    use_google_kms: Optional[bool] = Field(",
            "        None, description=\"decrypt keys with google kms\"",
            "    )",
            "    use_azure_key_vault: Optional[bool] = Field(",
            "        None, description=\"load keys from azure key vault\"",
            "    )",
            "    master_key: Optional[str] = Field(",
            "        None, description=\"require a key for all calls to proxy\"",
            "    )",
            "    database_url: Optional[str] = Field(",
            "        None,",
            "        description=\"connect to a postgres db - needed for generating temporary keys + tracking spend / key\",",
            "    )",
            "    database_connection_pool_limit: Optional[int] = Field(",
            "        100,",
            "        description=\"default connection pool for prisma client connecting to postgres db\",",
            "    )",
            "    database_connection_timeout: Optional[float] = Field(",
            "        60, description=\"default timeout for a connection to the database\"",
            "    )",
            "    database_type: Optional[Literal[\"dynamo_db\"]] = Field(",
            "        None, description=\"to use dynamodb instead of postgres db\"",
            "    )",
            "    database_args: Optional[DynamoDBArgs] = Field(",
            "        None,",
            "        description=\"custom args for instantiating dynamodb client - e.g. billing provision\",",
            "    )",
            "    otel: Optional[bool] = Field(",
            "        None,",
            "        description=\"[BETA] OpenTelemetry support - this might change, use with caution.\",",
            "    )",
            "    custom_auth: Optional[str] = Field(",
            "        None,",
            "        description=\"override user_api_key_auth with your own auth script - https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth\",",
            "    )",
            "    max_parallel_requests: Optional[int] = Field(",
            "        None,",
            "        description=\"maximum parallel requests for each api key\",",
            "    )",
            "    global_max_parallel_requests: Optional[int] = Field(",
            "        None, description=\"global max parallel requests to allow for a proxy instance.\"",
            "    )",
            "    infer_model_from_keys: Optional[bool] = Field(",
            "        None,",
            "        description=\"for `/models` endpoint, infers available model based on environment keys (e.g. OPENAI_API_KEY)\",",
            "    )",
            "    background_health_checks: Optional[bool] = Field(",
            "        None, description=\"run health checks in background\"",
            "    )",
            "    health_check_interval: int = Field(",
            "        300, description=\"background health check interval in seconds\"",
            "    )",
            "    alerting: Optional[List] = Field(",
            "        None,",
            "        description=\"List of alerting integrations. Today, just slack - `alerting: ['slack']`\",",
            "    )",
            "    alert_types: Optional[List[AlertType]] = Field(",
            "        None,",
            "        description=\"List of alerting types. By default it is all alerts\",",
            "    )",
            "    alert_to_webhook_url: Optional[Dict] = Field(",
            "        None,",
            "        description=\"Mapping of alert type to webhook url. e.g. `alert_to_webhook_url: {'budget_alerts': 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX'}`\",",
            "    )",
            "    alerting_args: Optional[Dict] = Field(",
            "        None, description=\"Controllable params for slack alerting - e.g. ttl in cache.\"",
            "    )",
            "    alerting_threshold: Optional[int] = Field(",
            "        None,",
            "        description=\"sends alerts if requests hang for 5min+\",",
            "    )",
            "    ui_access_mode: Optional[Literal[\"admin_only\", \"all\"]] = Field(",
            "        \"all\", description=\"Control access to the Proxy UI\"",
            "    )",
            "    allowed_routes: Optional[List] = Field(",
            "        None, description=\"Proxy API Endpoints you want users to be able to access\"",
            "    )",
            "    enable_public_model_hub: bool = Field(",
            "        default=False,",
            "        description=\"Public model hub for users to see what models they have access to, supported openai params, etc.\",",
            "    )",
            "",
            "",
            "class ConfigYAML(LiteLLMBase):",
            "    \"\"\"",
            "    Documents all the fields supported by the config.yaml",
            "    \"\"\"",
            "",
            "    environment_variables: Optional[dict] = Field(",
            "        None,",
            "        description=\"Object to pass in additional environment variables via POST request\",",
            "    )",
            "    model_list: Optional[List[ModelParams]] = Field(",
            "        None,",
            "        description=\"List of supported models on the server, with model-specific configs\",",
            "    )",
            "    litellm_settings: Optional[dict] = Field(",
            "        None,",
            "        description=\"litellm Module settings. See __init__.py for all, example litellm.drop_params=True, litellm.set_verbose=True, litellm.api_base, litellm.cache\",",
            "    )",
            "    general_settings: Optional[ConfigGeneralSettings] = None",
            "    router_settings: Optional[UpdateRouterConfig] = Field(",
            "        None,",
            "        description=\"litellm router object settings. See router.py __init__ for all, example router.num_retries=5, router.timeout=5, router.max_retries=5, router.retry_after=5\",",
            "    )",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_VerificationToken(LiteLLMBase):",
            "    token: Optional[str] = None",
            "    key_name: Optional[str] = None",
            "    key_alias: Optional[str] = None",
            "    spend: float = 0.0",
            "    max_budget: Optional[float] = None",
            "    expires: Optional[str] = None",
            "    models: List = []",
            "    aliases: Dict = {}",
            "    config: Dict = {}",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    metadata: Dict = {}",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    budget_duration: Optional[str] = None",
            "    budget_reset_at: Optional[datetime] = None",
            "    allowed_cache_controls: Optional[list] = []",
            "    permissions: Dict = {}",
            "    model_spend: Dict = {}",
            "    model_max_budget: Dict = {}",
            "    soft_budget_cooldown: bool = False",
            "    litellm_budget_table: Optional[dict] = None",
            "",
            "    org_id: Optional[str] = None  # org id for a given key",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_VerificationTokenView(LiteLLM_VerificationToken):",
            "    \"\"\"",
            "    Combined view of litellm verification token + litellm team table (select values)",
            "    \"\"\"",
            "",
            "    team_spend: Optional[float] = None",
            "    team_alias: Optional[str] = None",
            "    team_tpm_limit: Optional[int] = None",
            "    team_rpm_limit: Optional[int] = None",
            "    team_max_budget: Optional[float] = None",
            "    team_models: List = []",
            "    team_blocked: bool = False",
            "    soft_budget: Optional[float] = None",
            "    team_model_aliases: Optional[Dict] = None",
            "    team_member_spend: Optional[float] = None",
            "",
            "    # End User Params",
            "    end_user_id: Optional[str] = None",
            "    end_user_tpm_limit: Optional[int] = None",
            "    end_user_rpm_limit: Optional[int] = None",
            "    end_user_max_budget: Optional[float] = None",
            "",
            "",
            "class UserAPIKeyAuth(",
            "    LiteLLM_VerificationTokenView",
            "):  # the expected response object for user api key auth",
            "    \"\"\"",
            "    Return the row in the db",
            "    \"\"\"",
            "",
            "    api_key: Optional[str] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = None",
            "    parent_otel_span: Optional[Span] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_api_key(cls, values):",
            "        if values.get(\"api_key\") is not None:",
            "            values.update({\"token\": hash_token(values.get(\"api_key\"))})",
            "            if isinstance(values.get(\"api_key\"), str) and values.get(",
            "                \"api_key\"",
            "            ).startswith(\"sk-\"):",
            "                values.update({\"api_key\": hash_token(values.get(\"api_key\"))})",
            "        return values",
            "",
            "    class Config:",
            "        arbitrary_types_allowed = True",
            "",
            "",
            "class LiteLLM_Config(LiteLLMBase):",
            "    param_name: str",
            "    param_value: Dict",
            "",
            "",
            "class LiteLLM_UserTable(LiteLLMBase):",
            "    user_id: str",
            "    max_budget: Optional[float]",
            "    spend: float = 0.0",
            "    model_max_budget: Optional[Dict] = {}",
            "    model_spend: Optional[Dict] = {}",
            "    user_email: Optional[str]",
            "    models: list = []",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"spend\") is None:",
            "            values.update({\"spend\": 0.0})",
            "        if values.get(\"models\") is None:",
            "            values.update({\"models\": []})",
            "        return values",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_EndUserTable(LiteLLMBase):",
            "    user_id: str",
            "    blocked: bool",
            "    alias: Optional[str] = None",
            "    spend: float = 0.0",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = None",
            "    default_model: Optional[str] = None",
            "    litellm_budget_table: Optional[LiteLLM_BudgetTable] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"spend\") is None:",
            "            values.update({\"spend\": 0.0})",
            "        return values",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_SpendLogs(LiteLLMBase):",
            "    request_id: str",
            "    api_key: str",
            "    model: Optional[str] = \"\"",
            "    api_base: Optional[str] = \"\"",
            "    call_type: str",
            "    spend: Optional[float] = 0.0",
            "    total_tokens: Optional[int] = 0",
            "    prompt_tokens: Optional[int] = 0",
            "    completion_tokens: Optional[int] = 0",
            "    startTime: Union[str, datetime, None]",
            "    endTime: Union[str, datetime, None]",
            "    user: Optional[str] = \"\"",
            "    metadata: Optional[Json] = {}",
            "    cache_hit: Optional[str] = \"False\"",
            "    cache_key: Optional[str] = None",
            "    request_tags: Optional[Json] = None",
            "",
            "",
            "class LiteLLM_ErrorLogs(LiteLLMBase):",
            "    request_id: Optional[str] = str(uuid.uuid4())",
            "    api_base: Optional[str] = \"\"",
            "    model_group: Optional[str] = \"\"",
            "    litellm_model_name: Optional[str] = \"\"",
            "    model_id: Optional[str] = \"\"",
            "    request_kwargs: Optional[dict] = {}",
            "    exception_type: Optional[str] = \"\"",
            "    status_code: Optional[str] = \"\"",
            "    exception_string: Optional[str] = \"\"",
            "    startTime: Union[str, datetime, None]",
            "    endTime: Union[str, datetime, None]",
            "",
            "",
            "class LiteLLM_AuditLogs(LiteLLMBase):",
            "    id: str",
            "    updated_at: datetime",
            "    changed_by: str",
            "    changed_by_api_key: Optional[str] = None",
            "    action: Literal[\"created\", \"updated\", \"deleted\"]",
            "    table_name: Literal[",
            "        LitellmTableNames.TEAM_TABLE_NAME,",
            "        LitellmTableNames.USER_TABLE_NAME,",
            "        LitellmTableNames.KEY_TABLE_NAME,",
            "        LitellmTableNames.PROXY_MODEL_TABLE_NAME,",
            "    ]",
            "    object_id: str",
            "    before_value: Optional[Json] = None",
            "    updated_values: Optional[Json] = None",
            "",
            "",
            "class LiteLLM_SpendLogs_ResponseObject(LiteLLMBase):",
            "    response: Optional[List[Union[LiteLLM_SpendLogs, Any]]] = None",
            "",
            "",
            "class TokenCountRequest(LiteLLMBase):",
            "    model: str",
            "    prompt: Optional[str] = None",
            "    messages: Optional[List[dict]] = None",
            "",
            "",
            "class TokenCountResponse(LiteLLMBase):",
            "    total_tokens: int",
            "    request_model: str",
            "    model_used: str",
            "    tokenizer_type: str",
            "",
            "",
            "class CallInfo(LiteLLMBase):",
            "    \"\"\"Used for slack budget alerting\"\"\"",
            "",
            "    spend: float",
            "    max_budget: Optional[float] = None",
            "    token: str = Field(description=\"Hashed value of that key\")",
            "    customer_id: Optional[str] = None",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    team_alias: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "    key_alias: Optional[str] = None",
            "    projected_exceeded_date: Optional[str] = None",
            "    projected_spend: Optional[float] = None",
            "",
            "",
            "class WebhookEvent(CallInfo):",
            "    event: Literal[",
            "        \"budget_crossed\",",
            "        \"threshold_crossed\",",
            "        \"projected_limit_exceeded\",",
            "        \"key_created\",",
            "        \"internal_user_created\",",
            "        \"spend_tracked\",",
            "    ]",
            "    event_group: Literal[\"internal_user\", \"key\", \"team\", \"proxy\", \"customer\"]",
            "    event_message: str  # human-readable description of event",
            "",
            "",
            "class SpecialModelNames(enum.Enum):",
            "    all_team_models = \"all-team-models\"",
            "    all_proxy_models = \"all-proxy-models\"",
            "",
            "",
            "class InvitationNew(LiteLLMBase):",
            "    user_id: str",
            "",
            "",
            "class InvitationUpdate(LiteLLMBase):",
            "    invitation_id: str",
            "    is_accepted: bool",
            "",
            "",
            "class InvitationDelete(LiteLLMBase):",
            "    invitation_id: str",
            "",
            "",
            "class InvitationModel(LiteLLMBase):",
            "    id: str",
            "    user_id: str",
            "    is_accepted: bool",
            "    accepted_at: Optional[datetime]",
            "    expires_at: datetime",
            "    created_at: datetime",
            "    created_by: str",
            "    updated_at: datetime",
            "    updated_by: str",
            "",
            "",
            "class InvitationClaim(LiteLLMBase):",
            "    invitation_link: str",
            "    user_id: str",
            "    password: str",
            "",
            "",
            "class ConfigFieldInfo(LiteLLMBase):",
            "    field_name: str",
            "    field_value: Any",
            "",
            "",
            "class CallbackOnUI(LiteLLMBase):",
            "    litellm_callback_name: str",
            "    litellm_callback_params: Optional[list]",
            "    ui_callback_name: str",
            "",
            "",
            "class AllCallbacks(LiteLLMBase):",
            "    langfuse: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"langfuse\",",
            "        ui_callback_name=\"Langfuse\",",
            "        litellm_callback_params=[",
            "            \"LANGFUSE_PUBLIC_KEY\",",
            "            \"LANGFUSE_SECRET_KEY\",",
            "        ],",
            "    )",
            "",
            "    otel: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"otel\",",
            "        ui_callback_name=\"OpenTelemetry\",",
            "        litellm_callback_params=[",
            "            \"OTEL_EXPORTER\",",
            "            \"OTEL_ENDPOINT\",",
            "            \"OTEL_HEADERS\",",
            "        ],",
            "    )",
            "",
            "    s3: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"s3\",",
            "        ui_callback_name=\"s3 Bucket (AWS)\",",
            "        litellm_callback_params=[",
            "            \"AWS_ACCESS_KEY_ID\",",
            "            \"AWS_SECRET_ACCESS_KEY\",",
            "            \"AWS_REGION_NAME\",",
            "        ],",
            "    )",
            "",
            "    openmeter: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"openmeter\",",
            "        ui_callback_name=\"OpenMeter\",",
            "        litellm_callback_params=[",
            "            \"OPENMETER_API_ENDPOINT\",",
            "            \"OPENMETER_API_KEY\",",
            "        ],",
            "    )",
            "",
            "    custom_callback_api: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"custom_callback_api\",",
            "        litellm_callback_params=[\"GENERIC_LOGGER_ENDPOINT\"],",
            "        ui_callback_name=\"Custom Callback API\",",
            "    )",
            "",
            "    datadog: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"datadog\",",
            "        litellm_callback_params=[\"DD_API_KEY\", \"DD_SITE\"],",
            "        ui_callback_name=\"Datadog\",",
            "    )",
            "",
            "",
            "class SpendLogsMetadata(TypedDict):",
            "    \"\"\"",
            "    Specific metadata k,v pairs logged to spendlogs for easier cost tracking",
            "    \"\"\"",
            "",
            "    user_api_key: Optional[str]",
            "    user_api_key_alias: Optional[str]",
            "    user_api_key_team_id: Optional[str]",
            "    user_api_key_user_id: Optional[str]",
            "    user_api_key_team_alias: Optional[str]",
            "    spend_logs_metadata: Optional[",
            "        dict",
            "    ]  # special param to log k,v pairs to spendlogs for a call",
            "",
            "",
            "class SpendLogsPayload(TypedDict):",
            "    request_id: str",
            "    call_type: str",
            "    api_key: str",
            "    spend: float",
            "    total_tokens: int",
            "    prompt_tokens: int",
            "    completion_tokens: int",
            "    startTime: datetime",
            "    endTime: datetime",
            "    completionStartTime: Optional[datetime]",
            "    model: str",
            "    model_id: Optional[str]",
            "    model_group: Optional[str]",
            "    api_base: str",
            "    user: str",
            "    metadata: str  # json str",
            "    cache_hit: str",
            "    cache_key: str",
            "    request_tags: str  # json str",
            "    team_id: Optional[str]",
            "    end_user: Optional[str]",
            "",
            "",
            "class SpanAttributes(str, enum.Enum):",
            "    # Note: We've taken this from opentelemetry-semantic-conventions-ai",
            "    # I chose to not add a new dependency to litellm for this",
            "",
            "    # Semantic Conventions for LLM requests, this needs to be removed after",
            "    # OpenTelemetry Semantic Conventions support Gen AI.",
            "    # Issue at https://github.com/open-telemetry/opentelemetry-python/issues/3868",
            "    # Refer to https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/llm-spans.md",
            "",
            "    LLM_SYSTEM = \"gen_ai.system\"",
            "    LLM_REQUEST_MODEL = \"gen_ai.request.model\"",
            "    LLM_REQUEST_MAX_TOKENS = \"gen_ai.request.max_tokens\"",
            "    LLM_REQUEST_TEMPERATURE = \"gen_ai.request.temperature\"",
            "    LLM_REQUEST_TOP_P = \"gen_ai.request.top_p\"",
            "    LLM_PROMPTS = \"gen_ai.prompt\"",
            "    LLM_COMPLETIONS = \"gen_ai.completion\"",
            "    LLM_RESPONSE_MODEL = \"gen_ai.response.model\"",
            "    LLM_USAGE_COMPLETION_TOKENS = \"gen_ai.usage.completion_tokens\"",
            "    LLM_USAGE_PROMPT_TOKENS = \"gen_ai.usage.prompt_tokens\"",
            "    LLM_TOKEN_TYPE = \"gen_ai.token.type\"",
            "    # To be added",
            "    # LLM_RESPONSE_FINISH_REASON = \"gen_ai.response.finish_reasons\"",
            "    # LLM_RESPONSE_ID = \"gen_ai.response.id\"",
            "",
            "    # LLM",
            "    LLM_REQUEST_TYPE = \"llm.request.type\"",
            "    LLM_USAGE_TOTAL_TOKENS = \"llm.usage.total_tokens\"",
            "    LLM_USAGE_TOKEN_TYPE = \"llm.usage.token_type\"",
            "    LLM_USER = \"llm.user\"",
            "    LLM_HEADERS = \"llm.headers\"",
            "    LLM_TOP_K = \"llm.top_k\"",
            "    LLM_IS_STREAMING = \"llm.is_streaming\"",
            "    LLM_FREQUENCY_PENALTY = \"llm.frequency_penalty\"",
            "    LLM_PRESENCE_PENALTY = \"llm.presence_penalty\"",
            "    LLM_CHAT_STOP_SEQUENCES = \"llm.chat.stop_sequences\"",
            "    LLM_REQUEST_FUNCTIONS = \"llm.request.functions\"",
            "    LLM_REQUEST_REPETITION_PENALTY = \"llm.request.repetition_penalty\"",
            "    LLM_RESPONSE_FINISH_REASON = \"llm.response.finish_reason\"",
            "    LLM_RESPONSE_STOP_REASON = \"llm.response.stop_reason\"",
            "    LLM_CONTENT_COMPLETION_CHUNK = \"llm.content.completion.chunk\"",
            "",
            "    # OpenAI",
            "    LLM_OPENAI_RESPONSE_SYSTEM_FINGERPRINT = \"gen_ai.openai.system_fingerprint\"",
            "    LLM_OPENAI_API_BASE = \"gen_ai.openai.api_base\"",
            "    LLM_OPENAI_API_VERSION = \"gen_ai.openai.api_version\"",
            "    LLM_OPENAI_API_TYPE = \"gen_ai.openai.api_type\"",
            "",
            "",
            "class ManagementEndpointLoggingPayload(LiteLLMBase):",
            "    route: str",
            "    request_data: dict",
            "    response: Optional[dict] = None",
            "    exception: Optional[Any] = None",
            "    start_time: Optional[datetime] = None",
            "    end_time: Optional[datetime] = None"
        ],
        "afterPatchFile": [
            "from pydantic import BaseModel, Extra, Field, model_validator, Json, ConfigDict",
            "from dataclasses import fields",
            "import enum",
            "from typing import Optional, List, Union, Dict, Literal, Any, TypedDict, TYPE_CHECKING",
            "from datetime import datetime",
            "import uuid, json, sys, os",
            "from litellm.types.router import UpdateRouterConfig",
            "from litellm.types.utils import ProviderField",
            "from typing_extensions import Annotated",
            "",
            "",
            "if TYPE_CHECKING:",
            "    from opentelemetry.trace import Span as _Span",
            "",
            "    Span = _Span",
            "else:",
            "    Span = Any",
            "",
            "",
            "class LitellmUserRoles(str, enum.Enum):",
            "    \"\"\"",
            "    Admin Roles:",
            "    PROXY_ADMIN: admin over the platform",
            "    PROXY_ADMIN_VIEW_ONLY: can login, view all own keys, view all spend",
            "",
            "    Internal User Roles:",
            "    INTERNAL_USER: can login, view/create/delete their own keys, view their spend",
            "    INTERNAL_USER_VIEW_ONLY: can login, view their own keys, view their own spend",
            "",
            "",
            "    Team Roles:",
            "    TEAM: used for JWT auth",
            "",
            "",
            "    Customer Roles:",
            "    CUSTOMER: External users -> these are customers",
            "",
            "    \"\"\"",
            "",
            "    # Admin Roles",
            "    PROXY_ADMIN = \"proxy_admin\"",
            "    PROXY_ADMIN_VIEW_ONLY = \"proxy_admin_viewer\"",
            "",
            "    # Internal User Roles",
            "    INTERNAL_USER = \"internal_user\"",
            "    INTERNAL_USER_VIEW_ONLY = \"internal_user_viewer\"",
            "",
            "    # Team Roles",
            "    TEAM = \"team\"",
            "",
            "    # Customer Roles - External users of proxy",
            "    CUSTOMER = \"customer\"",
            "",
            "    def __str__(self):",
            "        return str(self.value)",
            "",
            "    @property",
            "    def description(self):",
            "        \"\"\"",
            "        Descriptions for the enum values",
            "        \"\"\"",
            "        descriptions = {",
            "            \"proxy_admin\": \"admin over litellm proxy, has all permissions\",",
            "            \"proxy_admin_viewer\": \"view all keys, view all spend\",",
            "            \"internal_user\": \"view/create/delete their own keys, view their own spend\",",
            "            \"internal_user_viewer\": \"view their own keys, view their own spend\",",
            "            \"team\": \"team scope used for JWT auth\",",
            "            \"customer\": \"customer\",",
            "        }",
            "        return descriptions.get(self.value, \"\")",
            "",
            "    @property",
            "    def ui_label(self):",
            "        \"\"\"",
            "        UI labels for the enum values",
            "        \"\"\"",
            "        ui_labels = {",
            "            \"proxy_admin\": \"Admin (All Permissions)\",",
            "            \"proxy_admin_viewer\": \"Admin (View Only)\",",
            "            \"internal_user\": \"Internal User (Create/Delete/View)\",",
            "            \"internal_user_viewer\": \"Internal User (View Only)\",",
            "            \"team\": \"Team\",",
            "            \"customer\": \"Customer\",",
            "        }",
            "        return ui_labels.get(self.value, \"\")",
            "",
            "",
            "class LitellmTableNames(str, enum.Enum):",
            "    \"\"\"",
            "    Enum for Table Names used by LiteLLM",
            "    \"\"\"",
            "",
            "    TEAM_TABLE_NAME: str = \"LiteLLM_TeamTable\"",
            "    USER_TABLE_NAME: str = \"LiteLLM_UserTable\"",
            "    KEY_TABLE_NAME: str = \"LiteLLM_VerificationToken\"",
            "    PROXY_MODEL_TABLE_NAME: str = \"LiteLLM_ModelTable\"",
            "",
            "",
            "AlertType = Literal[",
            "    \"llm_exceptions\",",
            "    \"llm_too_slow\",",
            "    \"llm_requests_hanging\",",
            "    \"budget_alerts\",",
            "    \"db_exceptions\",",
            "    \"daily_reports\",",
            "    \"spend_reports\",",
            "    \"cooldown_deployment\",",
            "    \"new_model_added\",",
            "    \"outage_alerts\",",
            "    \"region_outage_alerts\",",
            "]",
            "",
            "",
            "def hash_token(token: str):",
            "    import hashlib",
            "",
            "    # Hash the string using SHA-256",
            "    hashed_token = hashlib.sha256(token.encode()).hexdigest()",
            "",
            "    return hashed_token",
            "",
            "",
            "class LiteLLMBase(BaseModel):",
            "    \"\"\"",
            "    Implements default functions, all pydantic objects should have.",
            "    \"\"\"",
            "",
            "    def json(self, **kwargs):",
            "        try:",
            "            return self.model_dump(**kwargs)  # noqa",
            "        except Exception as e:",
            "            # if using pydantic v1",
            "            return self.dict(**kwargs)",
            "",
            "    def fields_set(self):",
            "        try:",
            "            return self.model_fields_set  # noqa",
            "        except:",
            "            # if using pydantic v1",
            "            return self.__fields_set__",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_UpperboundKeyGenerateParams(LiteLLMBase):",
            "    \"\"\"",
            "    Set default upperbound to max budget a key called via `/key/generate` can be.",
            "    \"\"\"",
            "",
            "    max_budget: Optional[float] = None",
            "    budget_duration: Optional[str] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "",
            "",
            "class LiteLLMRoutes(enum.Enum):",
            "    openai_route_names: List = [",
            "        \"chat_completion\",",
            "        \"completion\",",
            "        \"embeddings\",",
            "        \"image_generation\",",
            "        \"audio_transcriptions\",",
            "        \"moderations\",",
            "        \"model_list\",  # OpenAI /v1/models route",
            "    ]",
            "    openai_routes: List = [",
            "        # chat completions",
            "        \"/engines/{model}/chat/completions\",",
            "        \"/openai/deployments/{model}/chat/completions\",",
            "        \"/chat/completions\",",
            "        \"/v1/chat/completions\",",
            "        # completions",
            "        \"/openai/deployments/{model}/completions\",",
            "        \"/completions\",",
            "        \"/v1/completions\",",
            "        # embeddings",
            "        \"/openai/deployments/{model}/embeddings\",",
            "        \"/embeddings\",",
            "        \"/v1/embeddings\",",
            "        # image generation",
            "        \"/images/generations\",",
            "        \"/v1/images/generations\",",
            "        # audio transcription",
            "        \"/audio/transcriptions\",",
            "        \"/v1/audio/transcriptions\",",
            "        # moderations",
            "        \"/moderations\",",
            "        \"/v1/moderations\",",
            "        # batches",
            "        \"/v1/batches\",",
            "        \"/batches\",",
            "        \"/v1/batches{batch_id}\",",
            "        \"/batches{batch_id}\",",
            "        # files",
            "        \"/v1/files\",",
            "        \"/files\",",
            "        # models",
            "        \"/models\",",
            "        \"/v1/models\",",
            "        # token counter",
            "        \"/utils/token_counter\",",
            "    ]",
            "",
            "    info_routes: List = [",
            "        \"/key/info\",",
            "        \"/team/info\",",
            "        \"/team/list\",",
            "        \"/user/info\",",
            "        \"/model/info\",",
            "        \"/v2/model/info\",",
            "        \"/v2/key/info\",",
            "        \"/model_group/info\",",
            "    ]",
            "",
            "    # NOTE: ROUTES ONLY FOR MASTER KEY - only the Master Key should be able to Reset Spend",
            "    master_key_only_routes: List = [",
            "        \"/global/spend/reset\",",
            "    ]",
            "",
            "    sso_only_routes: List = [",
            "        \"/key/generate\",",
            "        \"/key/update\",",
            "        \"/key/delete\",",
            "        \"/global/spend/logs\",",
            "        \"/global/predict/spend/logs\",",
            "        \"/sso/get/logout_url\",",
            "    ]",
            "",
            "    management_routes: List = [  # key",
            "        \"/key/generate\",",
            "        \"/key/update\",",
            "        \"/key/delete\",",
            "        \"/key/info\",",
            "        # user",
            "        \"/user/new\",",
            "        \"/user/update\",",
            "        \"/user/delete\",",
            "        \"/user/info\",",
            "        # team",
            "        \"/team/new\",",
            "        \"/team/update\",",
            "        \"/team/delete\",",
            "        \"/team/list\",",
            "        \"/team/info\",",
            "        \"/team/block\",",
            "        \"/team/unblock\",",
            "        # model",
            "        \"/model/new\",",
            "        \"/model/update\",",
            "        \"/model/delete\",",
            "        \"/model/info\",",
            "    ]",
            "",
            "    spend_tracking_routes: List = [",
            "        # spend",
            "        \"/spend/keys\",",
            "        \"/spend/users\",",
            "        \"/spend/tags\",",
            "        \"/spend/calculate\",",
            "        \"/spend/logs\",",
            "    ]",
            "",
            "    global_spend_tracking_routes: List = [",
            "        # global spend",
            "        \"/global/spend/logs\",",
            "        \"/global/spend\",",
            "        \"/global/spend/keys\",",
            "        \"/global/spend/teams\",",
            "        \"/global/spend/end_users\",",
            "        \"/global/spend/models\",",
            "        \"/global/predict/spend/logs\",",
            "        \"/global/spend/report\",",
            "    ]",
            "",
            "    public_routes: List = [",
            "        \"/routes\",",
            "        \"/\",",
            "        \"/health/liveliness\",",
            "        \"/health/readiness\",",
            "        \"/test\",",
            "        \"/config/yaml\",",
            "        \"/metrics\",",
            "    ]",
            "",
            "    internal_user_routes: List = [",
            "        \"/key/generate\",",
            "        \"/key/update\",",
            "        \"/key/delete\",",
            "        \"/key/info\",",
            "    ] + spend_tracking_routes",
            "",
            "",
            "# class LiteLLMAllowedRoutes(LiteLLMBase):",
            "#     \"\"\"",
            "#     Defines allowed routes based on key type.",
            "",
            "#     Types = [\"admin\", \"team\", \"user\", \"unmapped\"]",
            "#     \"\"\"",
            "",
            "#     admin_allowed_routes: List[",
            "#         Literal[\"openai_routes\", \"info_routes\", \"management_routes\", \"spend_tracking_routes\", \"global_spend_tracking_routes\"]",
            "#     ] = [\"management_routes\"]",
            "",
            "",
            "class LiteLLM_JWTAuth(LiteLLMBase):",
            "    \"\"\"",
            "    A class to define the roles and permissions for a LiteLLM Proxy w/ JWT Auth.",
            "",
            "    Attributes:",
            "    - admin_jwt_scope: The JWT scope required for proxy admin roles.",
            "    - admin_allowed_routes: list of allowed routes for proxy admin roles.",
            "    - team_jwt_scope: The JWT scope required for proxy team roles.",
            "    - team_id_jwt_field: The field in the JWT token that stores the team ID. Default - `client_id`.",
            "    - team_allowed_routes: list of allowed routes for proxy team roles.",
            "    - user_id_jwt_field: The field in the JWT token that stores the user id (maps to `LiteLLMUserTable`). Use this for internal employees.",
            "    - end_user_id_jwt_field: The field in the JWT token that stores the end-user ID (maps to `LiteLLMEndUserTable`). Turn this off by setting to `None`. Enables end-user cost tracking. Use this for external customers.",
            "    - public_key_ttl: Default - 600s. TTL for caching public JWT keys.",
            "",
            "    See `auth_checks.py` for the specific routes",
            "    \"\"\"",
            "",
            "    admin_jwt_scope: str = \"litellm_proxy_admin\"",
            "    admin_allowed_routes: List[",
            "        Literal[",
            "            \"openai_routes\",",
            "            \"info_routes\",",
            "            \"management_routes\",",
            "            \"spend_tracking_routes\",",
            "            \"global_spend_tracking_routes\",",
            "        ]",
            "    ] = [",
            "        \"management_routes\",",
            "        \"spend_tracking_routes\",",
            "        \"global_spend_tracking_routes\",",
            "        \"info_routes\",",
            "    ]",
            "    team_id_jwt_field: Optional[str] = None",
            "    team_allowed_routes: List[",
            "        Literal[\"openai_routes\", \"info_routes\", \"management_routes\"]",
            "    ] = [\"openai_routes\", \"info_routes\"]",
            "    team_id_default: Optional[str] = Field(",
            "        default=None,",
            "        description=\"If no team_id given, default permissions/spend-tracking to this team.s\",",
            "    )",
            "    org_id_jwt_field: Optional[str] = None",
            "    user_id_jwt_field: Optional[str] = None",
            "    user_id_upsert: bool = Field(",
            "        default=False, description=\"If user doesn't exist, upsert them into the db.\"",
            "    )",
            "    end_user_id_jwt_field: Optional[str] = None",
            "    public_key_ttl: float = 600",
            "",
            "    def __init__(self, **kwargs: Any) -> None:",
            "        # get the attribute names for this Pydantic model",
            "        allowed_keys = self.__annotations__.keys()",
            "",
            "        invalid_keys = set(kwargs.keys()) - allowed_keys",
            "",
            "        if invalid_keys:",
            "            raise ValueError(",
            "                f\"Invalid arguments provided: {', '.join(invalid_keys)}. Allowed arguments are: {', '.join(allowed_keys)}.\"",
            "            )",
            "",
            "        super().__init__(**kwargs)",
            "",
            "",
            "class LiteLLMPromptInjectionParams(LiteLLMBase):",
            "    heuristics_check: bool = False",
            "    vector_db_check: bool = False",
            "    llm_api_check: bool = False",
            "    llm_api_name: Optional[str] = None",
            "    llm_api_system_prompt: Optional[str] = None",
            "    llm_api_fail_call_string: Optional[str] = None",
            "    reject_as_response: Optional[bool] = Field(",
            "        default=False,",
            "        description=\"Return rejected request error message as a string to the user. Default behaviour is to raise an exception.\",",
            "    )",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_llm_api_params(cls, values):",
            "        llm_api_check = values.get(\"llm_api_check\")",
            "        if llm_api_check is True:",
            "            if \"llm_api_name\" not in values or not values[\"llm_api_name\"]:",
            "                raise ValueError(",
            "                    \"If llm_api_check is set to True, llm_api_name must be provided\"",
            "                )",
            "            if (",
            "                \"llm_api_system_prompt\" not in values",
            "                or not values[\"llm_api_system_prompt\"]",
            "            ):",
            "                raise ValueError(",
            "                    \"If llm_api_check is set to True, llm_api_system_prompt must be provided\"",
            "                )",
            "            if (",
            "                \"llm_api_fail_call_string\" not in values",
            "                or not values[\"llm_api_fail_call_string\"]",
            "            ):",
            "                raise ValueError(",
            "                    \"If llm_api_check is set to True, llm_api_fail_call_string must be provided\"",
            "                )",
            "        return values",
            "",
            "",
            "######### Request Class Definition ######",
            "class ProxyChatCompletionRequest(LiteLLMBase):",
            "    model: str",
            "    messages: List[Dict[str, str]]",
            "    temperature: Optional[float] = None",
            "    top_p: Optional[float] = None",
            "    n: Optional[int] = None",
            "    stream: Optional[bool] = None",
            "    stop: Optional[List[str]] = None",
            "    max_tokens: Optional[int] = None",
            "    presence_penalty: Optional[float] = None",
            "    frequency_penalty: Optional[float] = None",
            "    logit_bias: Optional[Dict[str, float]] = None",
            "    user: Optional[str] = None",
            "    response_format: Optional[Dict[str, str]] = None",
            "    seed: Optional[int] = None",
            "    tools: Optional[List[str]] = None",
            "    tool_choice: Optional[str] = None",
            "    functions: Optional[List[str]] = None  # soon to be deprecated",
            "    function_call: Optional[str] = None  # soon to be deprecated",
            "",
            "    # Optional LiteLLM params",
            "    caching: Optional[bool] = None",
            "    api_base: Optional[str] = None",
            "    api_version: Optional[str] = None",
            "    api_key: Optional[str] = None",
            "    num_retries: Optional[int] = None",
            "    context_window_fallback_dict: Optional[Dict[str, str]] = None",
            "    fallbacks: Optional[List[str]] = None",
            "    metadata: Optional[Dict[str, str]] = {}",
            "    deployment_id: Optional[str] = None",
            "    request_timeout: Optional[int] = None",
            "",
            "    model_config = ConfigDict(",
            "        extra=\"allow\"",
            "    )  # allow params not defined here, these fall in litellm.completion(**kwargs)",
            "",
            "",
            "class ModelInfoDelete(LiteLLMBase):",
            "    id: str",
            "",
            "",
            "class ModelInfo(LiteLLMBase):",
            "    id: Optional[str]",
            "    mode: Optional[Literal[\"embedding\", \"chat\", \"completion\"]]",
            "    input_cost_per_token: Optional[float] = 0.0",
            "    output_cost_per_token: Optional[float] = 0.0",
            "    max_tokens: Optional[int] = 2048  # assume 2048 if not set",
            "",
            "    # for azure models we need users to specify the base model, one azure you can call deployments - azure/my-random-model",
            "    # we look up the base model in model_prices_and_context_window.json",
            "    base_model: Optional[",
            "        Literal[",
            "            \"gpt-4-1106-preview\",",
            "            \"gpt-4-32k\",",
            "            \"gpt-4\",",
            "            \"gpt-3.5-turbo-16k\",",
            "            \"gpt-3.5-turbo\",",
            "            \"text-embedding-ada-002\",",
            "        ]",
            "    ]",
            "",
            "    model_config = ConfigDict(protected_namespaces=(), extra=\"allow\")",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"id\") is None:",
            "            values.update({\"id\": str(uuid.uuid4())})",
            "        if values.get(\"mode\") is None:",
            "            values.update({\"mode\": None})",
            "        if values.get(\"input_cost_per_token\") is None:",
            "            values.update({\"input_cost_per_token\": None})",
            "        if values.get(\"output_cost_per_token\") is None:",
            "            values.update({\"output_cost_per_token\": None})",
            "        if values.get(\"max_tokens\") is None:",
            "            values.update({\"max_tokens\": None})",
            "        if values.get(\"base_model\") is None:",
            "            values.update({\"base_model\": None})",
            "        return values",
            "",
            "",
            "class ProviderInfo(LiteLLMBase):",
            "    name: str",
            "    fields: List[ProviderField]",
            "",
            "",
            "class BlockUsers(LiteLLMBase):",
            "    user_ids: List[str]  # required",
            "",
            "",
            "class ModelParams(LiteLLMBase):",
            "    model_name: str",
            "    litellm_params: dict",
            "    model_info: ModelInfo",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"model_info\") is None:",
            "            values.update({\"model_info\": ModelInfo()})",
            "        return values",
            "",
            "",
            "class GenerateRequestBase(LiteLLMBase):",
            "    \"\"\"",
            "    Overlapping schema between key and user generate/update requests",
            "    \"\"\"",
            "",
            "    models: Optional[list] = []",
            "    spend: Optional[float] = 0",
            "    max_budget: Optional[float] = None",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    metadata: Optional[dict] = {}",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    budget_duration: Optional[str] = None",
            "    allowed_cache_controls: Optional[list] = []",
            "    soft_budget: Optional[float] = None",
            "",
            "",
            "class GenerateKeyRequest(GenerateRequestBase):",
            "    key_alias: Optional[str] = None",
            "    duration: Optional[str] = None",
            "    aliases: Optional[dict] = {}",
            "    config: Optional[dict] = {}",
            "    permissions: Optional[dict] = {}",
            "    model_max_budget: Optional[dict] = (",
            "        {}",
            "    )  # {\"gpt-4\": 5.0, \"gpt-3.5-turbo\": 5.0}, defaults to {}",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "    send_invite_email: Optional[bool] = None",
            "",
            "",
            "class GenerateKeyResponse(GenerateKeyRequest):",
            "    key: str",
            "    key_name: Optional[str] = None",
            "    expires: Optional[datetime]",
            "    user_id: Optional[str] = None",
            "    token_id: Optional[str] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"token\") is not None:",
            "            values.update({\"key\": values.get(\"token\")})",
            "        dict_fields = [",
            "            \"metadata\",",
            "            \"aliases\",",
            "            \"config\",",
            "            \"permissions\",",
            "            \"model_max_budget\",",
            "        ]",
            "        for field in dict_fields:",
            "            value = values.get(field)",
            "            if value is not None and isinstance(value, str):",
            "                try:",
            "                    values[field] = json.loads(value)",
            "                except json.JSONDecodeError:",
            "                    raise ValueError(f\"Field {field} should be a valid dictionary\")",
            "",
            "        return values",
            "",
            "",
            "class UpdateKeyRequest(GenerateKeyRequest):",
            "    # Note: the defaults of all Params here MUST BE NONE",
            "    # else they will get overwritten",
            "    key: str",
            "    duration: Optional[str] = None",
            "    spend: Optional[float] = None",
            "    metadata: Optional[dict] = None",
            "",
            "",
            "class KeyRequest(LiteLLMBase):",
            "    keys: List[str]",
            "",
            "",
            "class LiteLLM_ModelTable(LiteLLMBase):",
            "    model_aliases: Optional[str] = None  # json dump the dict",
            "    created_by: str",
            "    updated_by: str",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class NewUserRequest(GenerateKeyRequest):",
            "    max_budget: Optional[float] = None",
            "    user_email: Optional[str] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    teams: Optional[list] = None",
            "    organization_id: Optional[str] = None",
            "    auto_create_key: bool = (",
            "        True  # flag used for returning a key as part of the /user/new response",
            "    )",
            "    send_invite_email: Optional[bool] = None",
            "",
            "",
            "class NewUserResponse(GenerateKeyResponse):",
            "    max_budget: Optional[float] = None",
            "    user_email: Optional[str] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    teams: Optional[list] = None",
            "    organization_id: Optional[str] = None",
            "",
            "",
            "class UpdateUserRequest(GenerateRequestBase):",
            "    # Note: the defaults of all Params here MUST BE NONE",
            "    # else they will get overwritten",
            "    user_id: Optional[str] = None",
            "    password: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "    spend: Optional[float] = None",
            "    metadata: Optional[dict] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    max_budget: Optional[float] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:",
            "            raise ValueError(\"Either user id or user email must be provided\")",
            "        return values",
            "",
            "",
            "class NewCustomerRequest(LiteLLMBase):",
            "    \"\"\"",
            "    Create a new customer, allocate a budget to them",
            "    \"\"\"",
            "",
            "    user_id: str",
            "    alias: Optional[str] = None  # human-friendly alias",
            "    blocked: bool = False  # allow/disallow requests for this end-user",
            "    max_budget: Optional[float] = None",
            "    budget_id: Optional[str] = None  # give either a budget_id or max_budget",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = (",
            "        None  # require all user requests to use models in this specific region",
            "    )",
            "    default_model: Optional[str] = (",
            "        None  # if no equivalent model in allowed region - default all requests to this model",
            "    )",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if values.get(\"max_budget\") is not None and values.get(\"budget_id\") is not None:",
            "            raise ValueError(\"Set either 'max_budget' or 'budget_id', not both.\")",
            "",
            "        return values",
            "",
            "",
            "class UpdateCustomerRequest(LiteLLMBase):",
            "    \"\"\"",
            "    Update a Customer, use this to update customer budgets etc",
            "",
            "    \"\"\"",
            "",
            "    user_id: str",
            "    alias: Optional[str] = None  # human-friendly alias",
            "    blocked: bool = False  # allow/disallow requests for this end-user",
            "    max_budget: Optional[float] = None",
            "    budget_id: Optional[str] = None  # give either a budget_id or max_budget",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = (",
            "        None  # require all user requests to use models in this specific region",
            "    )",
            "    default_model: Optional[str] = (",
            "        None  # if no equivalent model in allowed region - default all requests to this model",
            "    )",
            "",
            "",
            "class DeleteCustomerRequest(LiteLLMBase):",
            "    \"\"\"",
            "    Delete multiple Customers",
            "    \"\"\"",
            "",
            "    user_ids: List[str]",
            "",
            "",
            "class Member(LiteLLMBase):",
            "    role: Literal[\"admin\", \"user\"]",
            "    user_id: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if not isinstance(values, dict):",
            "            raise ValueError(\"input needs to be a dictionary\")",
            "        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:",
            "            raise ValueError(\"Either user id or user email must be provided\")",
            "        return values",
            "",
            "",
            "class TeamBase(LiteLLMBase):",
            "    team_alias: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    organization_id: Optional[str] = None",
            "    admins: list = []",
            "    members: list = []",
            "    members_with_roles: List[Member] = []",
            "    metadata: Optional[dict] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "",
            "    # Budget fields",
            "    max_budget: Optional[float] = None",
            "    budget_duration: Optional[str] = None",
            "",
            "    models: list = []",
            "    blocked: bool = False",
            "",
            "",
            "class NewTeamRequest(TeamBase):",
            "    model_aliases: Optional[dict] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class GlobalEndUsersSpend(LiteLLMBase):",
            "    api_key: Optional[str] = None",
            "    startTime: Optional[datetime] = None",
            "    endTime: Optional[datetime] = None",
            "",
            "",
            "class TeamMemberAddRequest(LiteLLMBase):",
            "    team_id: str",
            "    member: Union[List[Member], Member]",
            "    max_budget_in_team: Optional[float] = None  # Users max budget within the team",
            "",
            "    def __init__(self, **data):",
            "        member_data = data.get(\"member\")",
            "        if isinstance(member_data, list):",
            "            # If member is a list of dictionaries, convert each dictionary to a Member object",
            "            members = [Member(**item) for item in member_data]",
            "            # Replace member_data with the list of Member objects",
            "            data[\"member\"] = members",
            "        elif isinstance(member_data, dict):",
            "            # If member is a dictionary, convert it to a single Member object",
            "            member = Member(**member_data)",
            "            # Replace member_data with the single Member object",
            "            data[\"member\"] = member",
            "        # Call the superclass __init__ method to initialize the object",
            "        super().__init__(**data)",
            "",
            "",
            "class TeamMemberDeleteRequest(LiteLLMBase):",
            "    team_id: str",
            "    user_id: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_user_info(cls, values):",
            "        if values.get(\"user_id\") is None and values.get(\"user_email\") is None:",
            "            raise ValueError(\"Either user id or user email must be provided\")",
            "        return values",
            "",
            "",
            "class UpdateTeamRequest(LiteLLMBase):",
            "    \"\"\"",
            "    UpdateTeamRequest, used by /team/update when you need to update a team",
            "",
            "    team_id: str",
            "    team_alias: Optional[str] = None",
            "    organization_id: Optional[str] = None",
            "    metadata: Optional[dict] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    max_budget: Optional[float] = None",
            "    models: Optional[list] = None",
            "    blocked: Optional[bool] = None",
            "    budget_duration: Optional[str] = None",
            "    \"\"\"",
            "",
            "    team_id: str  # required",
            "    team_alias: Optional[str] = None",
            "    organization_id: Optional[str] = None",
            "    metadata: Optional[dict] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    max_budget: Optional[float] = None",
            "    models: Optional[list] = None",
            "    blocked: Optional[bool] = None",
            "    budget_duration: Optional[str] = None",
            "",
            "",
            "class ResetTeamBudgetRequest(LiteLLMBase):",
            "    \"\"\"",
            "    internal type used to reset the budget on a team",
            "    used by reset_budget()",
            "",
            "    team_id: str",
            "    spend: float",
            "    budget_reset_at: datetime",
            "    \"\"\"",
            "",
            "    team_id: str",
            "    spend: float",
            "    budget_reset_at: datetime",
            "    updated_at: datetime",
            "",
            "",
            "class DeleteTeamRequest(LiteLLMBase):",
            "    team_ids: List[str]  # required",
            "",
            "",
            "class BlockTeamRequest(LiteLLMBase):",
            "    team_id: str  # required",
            "",
            "",
            "class LiteLLM_TeamTable(TeamBase):",
            "    spend: Optional[float] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    budget_duration: Optional[str] = None",
            "    budget_reset_at: Optional[datetime] = None",
            "    model_id: Optional[int] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        dict_fields = [",
            "            \"metadata\",",
            "            \"aliases\",",
            "            \"config\",",
            "            \"permissions\",",
            "            \"model_max_budget\",",
            "            \"model_aliases\",",
            "        ]",
            "        for field in dict_fields:",
            "            value = values.get(field)",
            "            if value is not None and isinstance(value, str):",
            "                try:",
            "                    values[field] = json.loads(value)",
            "                except json.JSONDecodeError:",
            "                    raise ValueError(f\"Field {field} should be a valid dictionary\")",
            "",
            "        return values",
            "",
            "",
            "class TeamRequest(LiteLLMBase):",
            "    teams: List[str]",
            "",
            "",
            "class LiteLLM_BudgetTable(LiteLLMBase):",
            "    \"\"\"Represents user-controllable params for a LiteLLM_BudgetTable record\"\"\"",
            "",
            "    soft_budget: Optional[float] = None",
            "    max_budget: Optional[float] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    model_max_budget: Optional[dict] = None",
            "    budget_duration: Optional[str] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_TeamMemberTable(LiteLLM_BudgetTable):",
            "    \"\"\"",
            "    Used to track spend of a user_id within a team_id",
            "    \"\"\"",
            "",
            "    spend: Optional[float] = None",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    budget_id: Optional[str] = None",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class NewOrganizationRequest(LiteLLM_BudgetTable):",
            "    organization_id: Optional[str] = None",
            "    organization_alias: str",
            "    models: List = []",
            "    budget_id: Optional[str] = None",
            "",
            "",
            "class LiteLLM_OrganizationTable(LiteLLMBase):",
            "    \"\"\"Represents user-controllable params for a LiteLLM_OrganizationTable record\"\"\"",
            "",
            "    organization_id: Optional[str] = None",
            "    organization_alias: Optional[str] = None",
            "    budget_id: str",
            "    metadata: Optional[dict] = None",
            "    models: List[str]",
            "    created_by: str",
            "    updated_by: str",
            "",
            "",
            "class NewOrganizationResponse(LiteLLM_OrganizationTable):",
            "    organization_id: str",
            "    created_at: datetime",
            "    updated_at: datetime",
            "",
            "",
            "class OrganizationRequest(LiteLLMBase):",
            "    organizations: List[str]",
            "",
            "",
            "class BudgetNew(LiteLLMBase):",
            "    budget_id: str = Field(default=None, description=\"The unique budget id.\")",
            "    max_budget: Optional[float] = Field(",
            "        default=None,",
            "        description=\"Requests will fail if this budget (in USD) is exceeded.\",",
            "    )",
            "    soft_budget: Optional[float] = Field(",
            "        default=None,",
            "        description=\"Requests will NOT fail if this is exceeded. Will fire alerting though.\",",
            "    )",
            "    max_parallel_requests: Optional[int] = Field(",
            "        default=None, description=\"Max concurrent requests allowed for this budget id.\"",
            "    )",
            "    tpm_limit: Optional[int] = Field(",
            "        default=None, description=\"Max tokens per minute, allowed for this budget id.\"",
            "    )",
            "    rpm_limit: Optional[int] = Field(",
            "        default=None, description=\"Max requests per minute, allowed for this budget id.\"",
            "    )",
            "    budget_duration: Optional[str] = Field(",
            "        default=None,",
            "        description=\"Max duration budget should be set for (e.g. '1hr', '1d', '28d')\",",
            "    )",
            "",
            "",
            "class BudgetRequest(LiteLLMBase):",
            "    budgets: List[str]",
            "",
            "",
            "class BudgetDeleteRequest(LiteLLMBase):",
            "    id: str",
            "",
            "",
            "class KeyManagementSystem(enum.Enum):",
            "    GOOGLE_KMS = \"google_kms\"",
            "    AZURE_KEY_VAULT = \"azure_key_vault\"",
            "    AWS_SECRET_MANAGER = \"aws_secret_manager\"",
            "    LOCAL = \"local\"",
            "    AWS_KMS = \"aws_kms\"",
            "",
            "",
            "class KeyManagementSettings(LiteLLMBase):",
            "    hosted_keys: List",
            "",
            "",
            "class TeamDefaultSettings(LiteLLMBase):",
            "    team_id: str",
            "",
            "    model_config = ConfigDict(",
            "        extra=\"allow\"",
            "    )  # allow params not defined here, these fall in litellm.completion(**kwargs)",
            "",
            "",
            "class DynamoDBArgs(LiteLLMBase):",
            "    billing_mode: Literal[\"PROVISIONED_THROUGHPUT\", \"PAY_PER_REQUEST\"]",
            "    read_capacity_units: Optional[int] = None",
            "    write_capacity_units: Optional[int] = None",
            "    ssl_verify: Optional[bool] = None",
            "    region_name: str",
            "    user_table_name: str = \"LiteLLM_UserTable\"",
            "    key_table_name: str = \"LiteLLM_VerificationToken\"",
            "    config_table_name: str = \"LiteLLM_Config\"",
            "    spend_table_name: str = \"LiteLLM_SpendLogs\"",
            "    aws_role_name: Optional[str] = None",
            "    aws_session_name: Optional[str] = None",
            "    aws_web_identity_token: Optional[str] = None",
            "    aws_provider_id: Optional[str] = None",
            "    aws_policy_arns: Optional[List[str]] = None",
            "    aws_policy: Optional[str] = None",
            "    aws_duration_seconds: Optional[int] = None",
            "    assume_role_aws_role_name: Optional[str] = None",
            "    assume_role_aws_session_name: Optional[str] = None",
            "",
            "",
            "class ConfigFieldUpdate(LiteLLMBase):",
            "    field_name: str",
            "    field_value: Any",
            "    config_type: Literal[\"general_settings\"]",
            "",
            "",
            "class ConfigFieldDelete(LiteLLMBase):",
            "    config_type: Literal[\"general_settings\"]",
            "    field_name: str",
            "",
            "",
            "class ConfigList(LiteLLMBase):",
            "    field_name: str",
            "    field_type: str",
            "    field_description: str",
            "    field_value: Any",
            "    stored_in_db: Optional[bool]",
            "    field_default_value: Any",
            "    premium_field: bool = False",
            "",
            "",
            "class ConfigGeneralSettings(LiteLLMBase):",
            "    \"\"\"",
            "    Documents all the fields supported by `general_settings` in config.yaml",
            "    \"\"\"",
            "",
            "    completion_model: Optional[str] = Field(",
            "        None, description=\"proxy level default model for all chat completion calls\"",
            "    )",
            "    key_management_system: Optional[KeyManagementSystem] = Field(",
            "        None, description=\"key manager to load keys from / decrypt keys with\"",
            "    )",
            "    use_google_kms: Optional[bool] = Field(",
            "        None, description=\"decrypt keys with google kms\"",
            "    )",
            "    use_azure_key_vault: Optional[bool] = Field(",
            "        None, description=\"load keys from azure key vault\"",
            "    )",
            "    master_key: Optional[str] = Field(",
            "        None, description=\"require a key for all calls to proxy\"",
            "    )",
            "    database_url: Optional[str] = Field(",
            "        None,",
            "        description=\"connect to a postgres db - needed for generating temporary keys + tracking spend / key\",",
            "    )",
            "    database_connection_pool_limit: Optional[int] = Field(",
            "        100,",
            "        description=\"default connection pool for prisma client connecting to postgres db\",",
            "    )",
            "    database_connection_timeout: Optional[float] = Field(",
            "        60, description=\"default timeout for a connection to the database\"",
            "    )",
            "    database_type: Optional[Literal[\"dynamo_db\"]] = Field(",
            "        None, description=\"to use dynamodb instead of postgres db\"",
            "    )",
            "    database_args: Optional[DynamoDBArgs] = Field(",
            "        None,",
            "        description=\"custom args for instantiating dynamodb client - e.g. billing provision\",",
            "    )",
            "    otel: Optional[bool] = Field(",
            "        None,",
            "        description=\"[BETA] OpenTelemetry support - this might change, use with caution.\",",
            "    )",
            "    custom_auth: Optional[str] = Field(",
            "        None,",
            "        description=\"override user_api_key_auth with your own auth script - https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth\",",
            "    )",
            "    max_parallel_requests: Optional[int] = Field(",
            "        None,",
            "        description=\"maximum parallel requests for each api key\",",
            "    )",
            "    global_max_parallel_requests: Optional[int] = Field(",
            "        None, description=\"global max parallel requests to allow for a proxy instance.\"",
            "    )",
            "    infer_model_from_keys: Optional[bool] = Field(",
            "        None,",
            "        description=\"for `/models` endpoint, infers available model based on environment keys (e.g. OPENAI_API_KEY)\",",
            "    )",
            "    background_health_checks: Optional[bool] = Field(",
            "        None, description=\"run health checks in background\"",
            "    )",
            "    health_check_interval: int = Field(",
            "        300, description=\"background health check interval in seconds\"",
            "    )",
            "    alerting: Optional[List] = Field(",
            "        None,",
            "        description=\"List of alerting integrations. Today, just slack - `alerting: ['slack']`\",",
            "    )",
            "    alert_types: Optional[List[AlertType]] = Field(",
            "        None,",
            "        description=\"List of alerting types. By default it is all alerts\",",
            "    )",
            "    alert_to_webhook_url: Optional[Dict] = Field(",
            "        None,",
            "        description=\"Mapping of alert type to webhook url. e.g. `alert_to_webhook_url: {'budget_alerts': 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX'}`\",",
            "    )",
            "    alerting_args: Optional[Dict] = Field(",
            "        None, description=\"Controllable params for slack alerting - e.g. ttl in cache.\"",
            "    )",
            "    alerting_threshold: Optional[int] = Field(",
            "        None,",
            "        description=\"sends alerts if requests hang for 5min+\",",
            "    )",
            "    ui_access_mode: Optional[Literal[\"admin_only\", \"all\"]] = Field(",
            "        \"all\", description=\"Control access to the Proxy UI\"",
            "    )",
            "    allowed_routes: Optional[List] = Field(",
            "        None, description=\"Proxy API Endpoints you want users to be able to access\"",
            "    )",
            "    enable_public_model_hub: bool = Field(",
            "        default=False,",
            "        description=\"Public model hub for users to see what models they have access to, supported openai params, etc.\",",
            "    )",
            "",
            "",
            "class ConfigYAML(LiteLLMBase):",
            "    \"\"\"",
            "    Documents all the fields supported by the config.yaml",
            "    \"\"\"",
            "",
            "    environment_variables: Optional[dict] = Field(",
            "        None,",
            "        description=\"Object to pass in additional environment variables via POST request\",",
            "    )",
            "    model_list: Optional[List[ModelParams]] = Field(",
            "        None,",
            "        description=\"List of supported models on the server, with model-specific configs\",",
            "    )",
            "    litellm_settings: Optional[dict] = Field(",
            "        None,",
            "        description=\"litellm Module settings. See __init__.py for all, example litellm.drop_params=True, litellm.set_verbose=True, litellm.api_base, litellm.cache\",",
            "    )",
            "    general_settings: Optional[ConfigGeneralSettings] = None",
            "    router_settings: Optional[UpdateRouterConfig] = Field(",
            "        None,",
            "        description=\"litellm router object settings. See router.py __init__ for all, example router.num_retries=5, router.timeout=5, router.max_retries=5, router.retry_after=5\",",
            "    )",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_VerificationToken(LiteLLMBase):",
            "    token: Optional[str] = None",
            "    key_name: Optional[str] = None",
            "    key_alias: Optional[str] = None",
            "    spend: float = 0.0",
            "    max_budget: Optional[float] = None",
            "    expires: Optional[str] = None",
            "    models: List = []",
            "    aliases: Dict = {}",
            "    config: Dict = {}",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    max_parallel_requests: Optional[int] = None",
            "    metadata: Dict = {}",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "    budget_duration: Optional[str] = None",
            "    budget_reset_at: Optional[datetime] = None",
            "    allowed_cache_controls: Optional[list] = []",
            "    permissions: Dict = {}",
            "    model_spend: Dict = {}",
            "    model_max_budget: Dict = {}",
            "    soft_budget_cooldown: bool = False",
            "    litellm_budget_table: Optional[dict] = None",
            "",
            "    org_id: Optional[str] = None  # org id for a given key",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_VerificationTokenView(LiteLLM_VerificationToken):",
            "    \"\"\"",
            "    Combined view of litellm verification token + litellm team table (select values)",
            "    \"\"\"",
            "",
            "    team_spend: Optional[float] = None",
            "    team_alias: Optional[str] = None",
            "    team_tpm_limit: Optional[int] = None",
            "    team_rpm_limit: Optional[int] = None",
            "    team_max_budget: Optional[float] = None",
            "    team_models: List = []",
            "    team_blocked: bool = False",
            "    soft_budget: Optional[float] = None",
            "    team_model_aliases: Optional[Dict] = None",
            "    team_member_spend: Optional[float] = None",
            "",
            "    # End User Params",
            "    end_user_id: Optional[str] = None",
            "    end_user_tpm_limit: Optional[int] = None",
            "    end_user_rpm_limit: Optional[int] = None",
            "    end_user_max_budget: Optional[float] = None",
            "",
            "",
            "class UserAPIKeyAuth(",
            "    LiteLLM_VerificationTokenView",
            "):  # the expected response object for user api key auth",
            "    \"\"\"",
            "    Return the row in the db",
            "    \"\"\"",
            "",
            "    api_key: Optional[str] = None",
            "    user_role: Optional[",
            "        Literal[",
            "            LitellmUserRoles.PROXY_ADMIN,",
            "            LitellmUserRoles.PROXY_ADMIN_VIEW_ONLY,",
            "            LitellmUserRoles.INTERNAL_USER,",
            "            LitellmUserRoles.INTERNAL_USER_VIEW_ONLY,",
            "            LitellmUserRoles.TEAM,",
            "            LitellmUserRoles.CUSTOMER,",
            "        ]",
            "    ] = None",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = None",
            "    parent_otel_span: Optional[Span] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def check_api_key(cls, values):",
            "        if values.get(\"api_key\") is not None:",
            "            values.update({\"token\": hash_token(values.get(\"api_key\"))})",
            "            if isinstance(values.get(\"api_key\"), str) and values.get(",
            "                \"api_key\"",
            "            ).startswith(\"sk-\"):",
            "                values.update({\"api_key\": hash_token(values.get(\"api_key\"))})",
            "        return values",
            "",
            "    class Config:",
            "        arbitrary_types_allowed = True",
            "",
            "",
            "class LiteLLM_Config(LiteLLMBase):",
            "    param_name: str",
            "    param_value: Dict",
            "",
            "",
            "class LiteLLM_UserTable(LiteLLMBase):",
            "    user_id: str",
            "    max_budget: Optional[float]",
            "    spend: float = 0.0",
            "    model_max_budget: Optional[Dict] = {}",
            "    model_spend: Optional[Dict] = {}",
            "    user_email: Optional[str]",
            "    models: list = []",
            "    tpm_limit: Optional[int] = None",
            "    rpm_limit: Optional[int] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"spend\") is None:",
            "            values.update({\"spend\": 0.0})",
            "        if values.get(\"models\") is None:",
            "            values.update({\"models\": []})",
            "        return values",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_EndUserTable(LiteLLMBase):",
            "    user_id: str",
            "    blocked: bool",
            "    alias: Optional[str] = None",
            "    spend: float = 0.0",
            "    allowed_model_region: Optional[Literal[\"eu\"]] = None",
            "    default_model: Optional[str] = None",
            "    litellm_budget_table: Optional[LiteLLM_BudgetTable] = None",
            "",
            "    @model_validator(mode=\"before\")",
            "    @classmethod",
            "    def set_model_info(cls, values):",
            "        if values.get(\"spend\") is None:",
            "            values.update({\"spend\": 0.0})",
            "        return values",
            "",
            "    model_config = ConfigDict(protected_namespaces=())",
            "",
            "",
            "class LiteLLM_SpendLogs(LiteLLMBase):",
            "    request_id: str",
            "    api_key: str",
            "    model: Optional[str] = \"\"",
            "    api_base: Optional[str] = \"\"",
            "    call_type: str",
            "    spend: Optional[float] = 0.0",
            "    total_tokens: Optional[int] = 0",
            "    prompt_tokens: Optional[int] = 0",
            "    completion_tokens: Optional[int] = 0",
            "    startTime: Union[str, datetime, None]",
            "    endTime: Union[str, datetime, None]",
            "    user: Optional[str] = \"\"",
            "    metadata: Optional[Json] = {}",
            "    cache_hit: Optional[str] = \"False\"",
            "    cache_key: Optional[str] = None",
            "    request_tags: Optional[Json] = None",
            "",
            "",
            "class LiteLLM_ErrorLogs(LiteLLMBase):",
            "    request_id: Optional[str] = str(uuid.uuid4())",
            "    api_base: Optional[str] = \"\"",
            "    model_group: Optional[str] = \"\"",
            "    litellm_model_name: Optional[str] = \"\"",
            "    model_id: Optional[str] = \"\"",
            "    request_kwargs: Optional[dict] = {}",
            "    exception_type: Optional[str] = \"\"",
            "    status_code: Optional[str] = \"\"",
            "    exception_string: Optional[str] = \"\"",
            "    startTime: Union[str, datetime, None]",
            "    endTime: Union[str, datetime, None]",
            "",
            "",
            "class LiteLLM_AuditLogs(LiteLLMBase):",
            "    id: str",
            "    updated_at: datetime",
            "    changed_by: str",
            "    changed_by_api_key: Optional[str] = None",
            "    action: Literal[\"created\", \"updated\", \"deleted\"]",
            "    table_name: Literal[",
            "        LitellmTableNames.TEAM_TABLE_NAME,",
            "        LitellmTableNames.USER_TABLE_NAME,",
            "        LitellmTableNames.KEY_TABLE_NAME,",
            "        LitellmTableNames.PROXY_MODEL_TABLE_NAME,",
            "    ]",
            "    object_id: str",
            "    before_value: Optional[Json] = None",
            "    updated_values: Optional[Json] = None",
            "",
            "",
            "class LiteLLM_SpendLogs_ResponseObject(LiteLLMBase):",
            "    response: Optional[List[Union[LiteLLM_SpendLogs, Any]]] = None",
            "",
            "",
            "class TokenCountRequest(LiteLLMBase):",
            "    model: str",
            "    prompt: Optional[str] = None",
            "    messages: Optional[List[dict]] = None",
            "",
            "",
            "class TokenCountResponse(LiteLLMBase):",
            "    total_tokens: int",
            "    request_model: str",
            "    model_used: str",
            "    tokenizer_type: str",
            "",
            "",
            "class CallInfo(LiteLLMBase):",
            "    \"\"\"Used for slack budget alerting\"\"\"",
            "",
            "    spend: float",
            "    max_budget: Optional[float] = None",
            "    token: Optional[str] = Field(default=None, description=\"Hashed value of that key\")",
            "    customer_id: Optional[str] = None",
            "    user_id: Optional[str] = None",
            "    team_id: Optional[str] = None",
            "    team_alias: Optional[str] = None",
            "    user_email: Optional[str] = None",
            "    key_alias: Optional[str] = None",
            "    projected_exceeded_date: Optional[str] = None",
            "    projected_spend: Optional[float] = None",
            "",
            "",
            "class WebhookEvent(CallInfo):",
            "    event: Literal[",
            "        \"budget_crossed\",",
            "        \"threshold_crossed\",",
            "        \"projected_limit_exceeded\",",
            "        \"key_created\",",
            "        \"internal_user_created\",",
            "        \"spend_tracked\",",
            "    ]",
            "    event_group: Literal[\"internal_user\", \"key\", \"team\", \"proxy\", \"customer\"]",
            "    event_message: str  # human-readable description of event",
            "",
            "",
            "class SpecialModelNames(enum.Enum):",
            "    all_team_models = \"all-team-models\"",
            "    all_proxy_models = \"all-proxy-models\"",
            "",
            "",
            "class InvitationNew(LiteLLMBase):",
            "    user_id: str",
            "",
            "",
            "class InvitationUpdate(LiteLLMBase):",
            "    invitation_id: str",
            "    is_accepted: bool",
            "",
            "",
            "class InvitationDelete(LiteLLMBase):",
            "    invitation_id: str",
            "",
            "",
            "class InvitationModel(LiteLLMBase):",
            "    id: str",
            "    user_id: str",
            "    is_accepted: bool",
            "    accepted_at: Optional[datetime]",
            "    expires_at: datetime",
            "    created_at: datetime",
            "    created_by: str",
            "    updated_at: datetime",
            "    updated_by: str",
            "",
            "",
            "class InvitationClaim(LiteLLMBase):",
            "    invitation_link: str",
            "    user_id: str",
            "    password: str",
            "",
            "",
            "class ConfigFieldInfo(LiteLLMBase):",
            "    field_name: str",
            "    field_value: Any",
            "",
            "",
            "class CallbackOnUI(LiteLLMBase):",
            "    litellm_callback_name: str",
            "    litellm_callback_params: Optional[list]",
            "    ui_callback_name: str",
            "",
            "",
            "class AllCallbacks(LiteLLMBase):",
            "    langfuse: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"langfuse\",",
            "        ui_callback_name=\"Langfuse\",",
            "        litellm_callback_params=[",
            "            \"LANGFUSE_PUBLIC_KEY\",",
            "            \"LANGFUSE_SECRET_KEY\",",
            "        ],",
            "    )",
            "",
            "    otel: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"otel\",",
            "        ui_callback_name=\"OpenTelemetry\",",
            "        litellm_callback_params=[",
            "            \"OTEL_EXPORTER\",",
            "            \"OTEL_ENDPOINT\",",
            "            \"OTEL_HEADERS\",",
            "        ],",
            "    )",
            "",
            "    s3: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"s3\",",
            "        ui_callback_name=\"s3 Bucket (AWS)\",",
            "        litellm_callback_params=[",
            "            \"AWS_ACCESS_KEY_ID\",",
            "            \"AWS_SECRET_ACCESS_KEY\",",
            "            \"AWS_REGION_NAME\",",
            "        ],",
            "    )",
            "",
            "    openmeter: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"openmeter\",",
            "        ui_callback_name=\"OpenMeter\",",
            "        litellm_callback_params=[",
            "            \"OPENMETER_API_ENDPOINT\",",
            "            \"OPENMETER_API_KEY\",",
            "        ],",
            "    )",
            "",
            "    custom_callback_api: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"custom_callback_api\",",
            "        litellm_callback_params=[\"GENERIC_LOGGER_ENDPOINT\"],",
            "        ui_callback_name=\"Custom Callback API\",",
            "    )",
            "",
            "    datadog: CallbackOnUI = CallbackOnUI(",
            "        litellm_callback_name=\"datadog\",",
            "        litellm_callback_params=[\"DD_API_KEY\", \"DD_SITE\"],",
            "        ui_callback_name=\"Datadog\",",
            "    )",
            "",
            "",
            "class SpendLogsMetadata(TypedDict):",
            "    \"\"\"",
            "    Specific metadata k,v pairs logged to spendlogs for easier cost tracking",
            "    \"\"\"",
            "",
            "    user_api_key: Optional[str]",
            "    user_api_key_alias: Optional[str]",
            "    user_api_key_team_id: Optional[str]",
            "    user_api_key_user_id: Optional[str]",
            "    user_api_key_team_alias: Optional[str]",
            "    spend_logs_metadata: Optional[",
            "        dict",
            "    ]  # special param to log k,v pairs to spendlogs for a call",
            "",
            "",
            "class SpendLogsPayload(TypedDict):",
            "    request_id: str",
            "    call_type: str",
            "    api_key: str",
            "    spend: float",
            "    total_tokens: int",
            "    prompt_tokens: int",
            "    completion_tokens: int",
            "    startTime: datetime",
            "    endTime: datetime",
            "    completionStartTime: Optional[datetime]",
            "    model: str",
            "    model_id: Optional[str]",
            "    model_group: Optional[str]",
            "    api_base: str",
            "    user: str",
            "    metadata: str  # json str",
            "    cache_hit: str",
            "    cache_key: str",
            "    request_tags: str  # json str",
            "    team_id: Optional[str]",
            "    end_user: Optional[str]",
            "",
            "",
            "class SpanAttributes(str, enum.Enum):",
            "    # Note: We've taken this from opentelemetry-semantic-conventions-ai",
            "    # I chose to not add a new dependency to litellm for this",
            "",
            "    # Semantic Conventions for LLM requests, this needs to be removed after",
            "    # OpenTelemetry Semantic Conventions support Gen AI.",
            "    # Issue at https://github.com/open-telemetry/opentelemetry-python/issues/3868",
            "    # Refer to https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/llm-spans.md",
            "",
            "    LLM_SYSTEM = \"gen_ai.system\"",
            "    LLM_REQUEST_MODEL = \"gen_ai.request.model\"",
            "    LLM_REQUEST_MAX_TOKENS = \"gen_ai.request.max_tokens\"",
            "    LLM_REQUEST_TEMPERATURE = \"gen_ai.request.temperature\"",
            "    LLM_REQUEST_TOP_P = \"gen_ai.request.top_p\"",
            "    LLM_PROMPTS = \"gen_ai.prompt\"",
            "    LLM_COMPLETIONS = \"gen_ai.completion\"",
            "    LLM_RESPONSE_MODEL = \"gen_ai.response.model\"",
            "    LLM_USAGE_COMPLETION_TOKENS = \"gen_ai.usage.completion_tokens\"",
            "    LLM_USAGE_PROMPT_TOKENS = \"gen_ai.usage.prompt_tokens\"",
            "    LLM_TOKEN_TYPE = \"gen_ai.token.type\"",
            "    # To be added",
            "    # LLM_RESPONSE_FINISH_REASON = \"gen_ai.response.finish_reasons\"",
            "    # LLM_RESPONSE_ID = \"gen_ai.response.id\"",
            "",
            "    # LLM",
            "    LLM_REQUEST_TYPE = \"llm.request.type\"",
            "    LLM_USAGE_TOTAL_TOKENS = \"llm.usage.total_tokens\"",
            "    LLM_USAGE_TOKEN_TYPE = \"llm.usage.token_type\"",
            "    LLM_USER = \"llm.user\"",
            "    LLM_HEADERS = \"llm.headers\"",
            "    LLM_TOP_K = \"llm.top_k\"",
            "    LLM_IS_STREAMING = \"llm.is_streaming\"",
            "    LLM_FREQUENCY_PENALTY = \"llm.frequency_penalty\"",
            "    LLM_PRESENCE_PENALTY = \"llm.presence_penalty\"",
            "    LLM_CHAT_STOP_SEQUENCES = \"llm.chat.stop_sequences\"",
            "    LLM_REQUEST_FUNCTIONS = \"llm.request.functions\"",
            "    LLM_REQUEST_REPETITION_PENALTY = \"llm.request.repetition_penalty\"",
            "    LLM_RESPONSE_FINISH_REASON = \"llm.response.finish_reason\"",
            "    LLM_RESPONSE_STOP_REASON = \"llm.response.stop_reason\"",
            "    LLM_CONTENT_COMPLETION_CHUNK = \"llm.content.completion.chunk\"",
            "",
            "    # OpenAI",
            "    LLM_OPENAI_RESPONSE_SYSTEM_FINGERPRINT = \"gen_ai.openai.system_fingerprint\"",
            "    LLM_OPENAI_API_BASE = \"gen_ai.openai.api_base\"",
            "    LLM_OPENAI_API_VERSION = \"gen_ai.openai.api_version\"",
            "    LLM_OPENAI_API_TYPE = \"gen_ai.openai.api_type\"",
            "",
            "",
            "class ManagementEndpointLoggingPayload(LiteLLMBase):",
            "    route: str",
            "    request_data: dict",
            "    response: Optional[dict] = None",
            "    exception: Optional[Any] = None",
            "    start_time: Optional[datetime] = None",
            "    end_time: Optional[datetime] = None",
            "",
            "",
            "class ProxyException(Exception):",
            "    # NOTE: DO NOT MODIFY THIS",
            "    # This is used to map exactly to OPENAI Exceptions",
            "    def __init__(",
            "        self,",
            "        message: str,",
            "        type: str,",
            "        param: Optional[str],",
            "        code: Optional[int],",
            "    ):",
            "        self.message = message",
            "        self.type = type",
            "        self.param = param",
            "        self.code = code",
            "",
            "        # rules for proxyExceptions",
            "        # Litellm router.py returns \"No healthy deployment available\" when there are no deployments available",
            "        # Should map to 429 errors https://github.com/BerriAI/litellm/issues/2487",
            "        if (",
            "            \"No healthy deployment available\" in self.message",
            "            or \"No deployments available\" in self.message",
            "        ):",
            "            self.code = 429",
            "",
            "    def to_dict(self) -> dict:",
            "        \"\"\"Converts the ProxyException instance to a dictionary.\"\"\"",
            "        return {",
            "            \"message\": self.message,",
            "            \"type\": self.type,",
            "            \"param\": self.param,",
            "            \"code\": self.code,",
            "        }"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "1361": [
                "CallInfo"
            ]
        },
        "addLocation": []
    }
}