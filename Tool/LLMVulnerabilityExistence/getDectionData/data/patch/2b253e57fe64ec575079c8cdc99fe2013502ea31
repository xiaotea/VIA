{
    "example/scrashtest/settings.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " # SPLASH_URL = 'http://192.168.59.103:8050/'"
            },
            "1": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'"
            },
            "2": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+ROBOTSTXT_OBEY = True"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "",
            "BOT_NAME = 'scrashtest'",
            "",
            "SPIDER_MODULES = ['scrashtest.spiders']",
            "NEWSPIDER_MODULE = 'scrashtest.spiders'",
            "",
            "DOWNLOADER_MIDDLEWARES = {",
            "    # Engine side",
            "    'scrapy_splash.SplashCookiesMiddleware': 723,",
            "    'scrapy_splash.SplashMiddleware': 725,",
            "    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,",
            "    # Downloader side",
            "}",
            "",
            "SPIDER_MIDDLEWARES = {",
            "    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,",
            "}",
            "SPLASH_URL = 'http://127.0.0.1:8050/'",
            "# SPLASH_URL = 'http://192.168.59.103:8050/'",
            "DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'",
            "HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "",
            "BOT_NAME = 'scrashtest'",
            "",
            "SPIDER_MODULES = ['scrashtest.spiders']",
            "NEWSPIDER_MODULE = 'scrashtest.spiders'",
            "",
            "DOWNLOADER_MIDDLEWARES = {",
            "    # Engine side",
            "    'scrapy_splash.SplashCookiesMiddleware': 723,",
            "    'scrapy_splash.SplashMiddleware': 725,",
            "    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,",
            "    # Downloader side",
            "}",
            "",
            "SPIDER_MIDDLEWARES = {",
            "    'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,",
            "}",
            "SPLASH_URL = 'http://127.0.0.1:8050/'",
            "# SPLASH_URL = 'http://192.168.59.103:8050/'",
            "DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'",
            "HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'",
            "ROBOTSTXT_OBEY = True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "scrapy_splash/middleware.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from six.moves.urllib.parse import urljoin"
            },
            "1": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from six.moves.http_cookiejar import CookieJar"
            },
            "2": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 13,
                "PatchRowcode": "+from w3lib.http import basic_auth_header"
            },
            "4": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " import scrapy"
            },
            "5": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from scrapy.exceptions import NotConfigured"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 15,
                "PatchRowcode": "+from scrapy.exceptions import NotConfigured, IgnoreRequest"
            },
            "7": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from scrapy.http.headers import Headers"
            },
            "8": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from scrapy.http.response.text import TextResponse"
            },
            "9": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from scrapy import signals"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware"
            },
            "11": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from scrapy_splash.responsetypes import responsetypes"
            },
            "13": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " from scrapy_splash.cookies import jar_to_har, har_to_jar"
            },
            "14": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": 224,
                "PatchRowcode": "     retry_498_priority_adjust = +50"
            },
            "15": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": 225,
                "PatchRowcode": "     remote_keys_key = '_splash_remote_keys'"
            },
            "16": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": 226,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def __init__(self, crawler, splash_base_url, slot_policy, log_400):"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 227,
                "PatchRowcode": "+    def __init__(self, crawler, splash_base_url, slot_policy, log_400, auth):"
            },
            "19": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": 228,
                "PatchRowcode": "         self.crawler = crawler"
            },
            "20": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": 229,
                "PatchRowcode": "         self.splash_base_url = splash_base_url"
            },
            "21": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": 230,
                "PatchRowcode": "         self.slot_policy = slot_policy"
            },
            "22": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": 231,
                "PatchRowcode": "         self.log_400 = log_400"
            },
            "23": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": 232,
                "PatchRowcode": "         self.crawler.signals.connect(self.spider_opened, signals.spider_opened)"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 233,
                "PatchRowcode": "+        self.auth = auth"
            },
            "25": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": 234,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": 235,
                "PatchRowcode": "     @classmethod"
            },
            "27": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": 236,
                "PatchRowcode": "     def from_crawler(cls, crawler):"
            },
            "28": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        splash_base_url = crawler.settings.get('SPLASH_URL',"
            },
            "29": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                               cls.default_splash_url)"
            },
            "30": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        log_400 = crawler.settings.getbool('SPLASH_LOG_400', True)"
            },
            "31": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        slot_policy = crawler.settings.get('SPLASH_SLOT_POLICY',"
            },
            "32": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                           cls.default_policy)"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 237,
                "PatchRowcode": "+        s = crawler.settings"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 238,
                "PatchRowcode": "+        splash_base_url = s.get('SPLASH_URL', cls.default_splash_url)"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 239,
                "PatchRowcode": "+        log_400 = s.getbool('SPLASH_LOG_400', True)"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 240,
                "PatchRowcode": "+        slot_policy = s.get('SPLASH_SLOT_POLICY', cls.default_policy)"
            },
            "37": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 241,
                "PatchRowcode": "         if slot_policy not in SlotPolicy._known:"
            },
            "38": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "             raise NotConfigured(\"Incorrect slot policy: %r\" % slot_policy)"
            },
            "39": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 243,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 242,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return cls(crawler, splash_base_url, slot_policy, log_400)"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 244,
                "PatchRowcode": "+        splash_user = s.get('SPLASH_USER', '')"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 245,
                "PatchRowcode": "+        splash_pass = s.get('SPLASH_PASS', '')"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 246,
                "PatchRowcode": "+        auth = None"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 247,
                "PatchRowcode": "+        if splash_user or splash_pass:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 248,
                "PatchRowcode": "+            auth = basic_auth_header(splash_user, splash_pass)"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 249,
                "PatchRowcode": "+        return cls(crawler, splash_base_url, slot_policy, log_400, auth)"
            },
            "47": {
                "beforePatchRowNumber": 243,
                "afterPatchRowNumber": 250,
                "PatchRowcode": " "
            },
            "48": {
                "beforePatchRowNumber": 244,
                "afterPatchRowNumber": 251,
                "PatchRowcode": "     def spider_opened(self, spider):"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 252,
                "PatchRowcode": "+        if _http_auth_enabled(spider):"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 253,
                "PatchRowcode": "+            replace_downloader_middleware(self.crawler, RobotsTxtMiddleware,"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 254,
                "PatchRowcode": "+                                          SafeRobotsTxtMiddleware)"
            },
            "52": {
                "beforePatchRowNumber": 245,
                "afterPatchRowNumber": 255,
                "PatchRowcode": "         if not hasattr(spider, 'state'):"
            },
            "53": {
                "beforePatchRowNumber": 246,
                "afterPatchRowNumber": 256,
                "PatchRowcode": "             spider.state = {}"
            },
            "54": {
                "beforePatchRowNumber": 247,
                "afterPatchRowNumber": 257,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 260,
                "afterPatchRowNumber": 270,
                "PatchRowcode": "     def process_request(self, request, spider):"
            },
            "56": {
                "beforePatchRowNumber": 261,
                "afterPatchRowNumber": 271,
                "PatchRowcode": "         if 'splash' not in request.meta:"
            },
            "57": {
                "beforePatchRowNumber": 262,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "             return"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 273,
                "PatchRowcode": "+        splash_options = request.meta['splash']"
            },
            "59": {
                "beforePatchRowNumber": 263,
                "afterPatchRowNumber": 274,
                "PatchRowcode": " "
            },
            "60": {
                "beforePatchRowNumber": 264,
                "afterPatchRowNumber": 275,
                "PatchRowcode": "         if request.method not in {'GET', 'POST'}:"
            },
            "61": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            logger.warning("
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 276,
                "PatchRowcode": "+            logger.error("
            },
            "63": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 277,
                "PatchRowcode": "                 \"Currently only GET and POST requests are supported by \""
            },
            "64": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                \"SplashMiddleware; %(request)s will be handled without Splash\","
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 278,
                "PatchRowcode": "+                \"SplashMiddleware; %(request)s is dropped\","
            },
            "66": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 279,
                "PatchRowcode": "                 {'request': request},"
            },
            "67": {
                "beforePatchRowNumber": 269,
                "afterPatchRowNumber": 280,
                "PatchRowcode": "                 extra={'spider': spider}"
            },
            "68": {
                "beforePatchRowNumber": 270,
                "afterPatchRowNumber": 281,
                "PatchRowcode": "             )"
            },
            "69": {
                "beforePatchRowNumber": 271,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return request"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 282,
                "PatchRowcode": "+            self.crawler.stats.inc_value('splash/dropped/method/{}'.format("
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 283,
                "PatchRowcode": "+                request.method))"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 284,
                "PatchRowcode": "+            raise IgnoreRequest(\"SplashRequest doesn't support \""
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 285,
                "PatchRowcode": "+                                \"HTTP {} method\".format(request.method))"
            },
            "74": {
                "beforePatchRowNumber": 272,
                "afterPatchRowNumber": 286,
                "PatchRowcode": " "
            },
            "75": {
                "beforePatchRowNumber": 273,
                "afterPatchRowNumber": 287,
                "PatchRowcode": "         if request.meta.get(\"_splash_processed\"):"
            },
            "76": {
                "beforePatchRowNumber": 274,
                "afterPatchRowNumber": 288,
                "PatchRowcode": "             # don't process the same request more than once"
            },
            "77": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 289,
                "PatchRowcode": "             return"
            },
            "78": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 290,
                "PatchRowcode": " "
            },
            "79": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        splash_options = request.meta['splash']"
            },
            "80": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": 291,
                "PatchRowcode": "         request.meta['_splash_processed'] = True"
            },
            "81": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 292,
                "PatchRowcode": " "
            },
            "82": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 293,
                "PatchRowcode": "         slot_policy = splash_options.get('slot_policy', self.slot_policy)"
            },
            "83": {
                "beforePatchRowNumber": 319,
                "afterPatchRowNumber": 332,
                "PatchRowcode": "         if not splash_options.get('dont_send_headers'):"
            },
            "84": {
                "beforePatchRowNumber": 320,
                "afterPatchRowNumber": 333,
                "PatchRowcode": "             headers = scrapy_headers_to_unicode_dict(request.headers)"
            },
            "85": {
                "beforePatchRowNumber": 321,
                "afterPatchRowNumber": 334,
                "PatchRowcode": "             if headers:"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 335,
                "PatchRowcode": "+                # Headers set by HttpAuthMiddleware should be used for Splash,"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 336,
                "PatchRowcode": "+                # not for the remote website (backwards compatibility)."
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 337,
                "PatchRowcode": "+                if _http_auth_enabled(spider):"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 338,
                "PatchRowcode": "+                    headers.pop('Authorization', None)"
            },
            "90": {
                "beforePatchRowNumber": 322,
                "afterPatchRowNumber": 339,
                "PatchRowcode": "                 args.setdefault('headers', headers)"
            },
            "91": {
                "beforePatchRowNumber": 323,
                "afterPatchRowNumber": 340,
                "PatchRowcode": " "
            },
            "92": {
                "beforePatchRowNumber": 324,
                "afterPatchRowNumber": 341,
                "PatchRowcode": "         body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)"
            },
            "93": {
                "beforePatchRowNumber": 353,
                "afterPatchRowNumber": 370,
                "PatchRowcode": "         splash_url = urljoin(splash_base_url, endpoint)"
            },
            "94": {
                "beforePatchRowNumber": 354,
                "afterPatchRowNumber": 371,
                "PatchRowcode": " "
            },
            "95": {
                "beforePatchRowNumber": 355,
                "afterPatchRowNumber": 372,
                "PatchRowcode": "         headers = Headers({'Content-Type': 'application/json'})"
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 373,
                "PatchRowcode": "+        if self.auth is not None:"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 374,
                "PatchRowcode": "+            headers['Authorization'] = self.auth"
            },
            "98": {
                "beforePatchRowNumber": 356,
                "afterPatchRowNumber": 375,
                "PatchRowcode": "         headers.update(splash_options.get('splash_headers', {}))"
            },
            "99": {
                "beforePatchRowNumber": 357,
                "afterPatchRowNumber": 376,
                "PatchRowcode": "         new_request = request.replace("
            },
            "100": {
                "beforePatchRowNumber": 358,
                "afterPatchRowNumber": 377,
                "PatchRowcode": "             url=splash_url,"
            },
            "101": {
                "beforePatchRowNumber": 361,
                "afterPatchRowNumber": 380,
                "PatchRowcode": "             headers=headers,"
            },
            "102": {
                "beforePatchRowNumber": 362,
                "afterPatchRowNumber": 381,
                "PatchRowcode": "             priority=request.priority + self.rescheduling_priority_adjust"
            },
            "103": {
                "beforePatchRowNumber": 363,
                "afterPatchRowNumber": 382,
                "PatchRowcode": "         )"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 383,
                "PatchRowcode": "+        new_request.meta['dont_obey_robotstxt'] = True"
            },
            "105": {
                "beforePatchRowNumber": 364,
                "afterPatchRowNumber": 384,
                "PatchRowcode": "         self.crawler.stats.inc_value('splash/%s/request_count' % endpoint)"
            },
            "106": {
                "beforePatchRowNumber": 365,
                "afterPatchRowNumber": 385,
                "PatchRowcode": "         return new_request"
            },
            "107": {
                "beforePatchRowNumber": 366,
                "afterPatchRowNumber": 386,
                "PatchRowcode": " "
            },
            "108": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": 498,
                "PatchRowcode": "         return self.crawler.engine.downloader._get_slot_key("
            },
            "109": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": 499,
                "PatchRowcode": "             request_or_response, None"
            },
            "110": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": 500,
                "PatchRowcode": "         )"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 501,
                "PatchRowcode": "+"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 502,
                "PatchRowcode": "+"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 503,
                "PatchRowcode": "+class SafeRobotsTxtMiddleware(RobotsTxtMiddleware):"
            },
            "114": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 504,
                "PatchRowcode": "+    def process_request(self, request, spider):"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 505,
                "PatchRowcode": "+        # disable robots.txt for Splash requests"
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 506,
                "PatchRowcode": "+        if _http_auth_enabled(spider) and 'splash' in request.meta:"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 507,
                "PatchRowcode": "+            return"
            },
            "118": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 508,
                "PatchRowcode": "+        return super(SafeRobotsTxtMiddleware, self).process_request("
            },
            "119": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 509,
                "PatchRowcode": "+            request, spider)"
            },
            "120": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 510,
                "PatchRowcode": "+"
            },
            "121": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 511,
                "PatchRowcode": "+"
            },
            "122": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 512,
                "PatchRowcode": "+def _http_auth_enabled(spider):"
            },
            "123": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 513,
                "PatchRowcode": "+    # FIXME: this function should always return False if HttpAuthMiddleware is"
            },
            "124": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 514,
                "PatchRowcode": "+    # not in a middleware list."
            },
            "125": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 515,
                "PatchRowcode": "+    return getattr(spider, 'http_user', '') or getattr(spider, 'http_pass', '')"
            },
            "126": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 516,
                "PatchRowcode": "+"
            },
            "127": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 517,
                "PatchRowcode": "+"
            },
            "128": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 518,
                "PatchRowcode": "+def replace_downloader_middleware(crawler, old_cls, new_cls):"
            },
            "129": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 519,
                "PatchRowcode": "+    \"\"\" Replace downloader middleware with another one \"\"\""
            },
            "130": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 520,
                "PatchRowcode": "+    try:"
            },
            "131": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 521,
                "PatchRowcode": "+        new_mw = new_cls.from_crawler(crawler)"
            },
            "132": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 522,
                "PatchRowcode": "+    except NotConfigured:"
            },
            "133": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 523,
                "PatchRowcode": "+        return"
            },
            "134": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 524,
                "PatchRowcode": "+"
            },
            "135": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 525,
                "PatchRowcode": "+    mw_manager = crawler.engine.downloader.middleware"
            },
            "136": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 526,
                "PatchRowcode": "+    mw_manager.middlewares = tuple(["
            },
            "137": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 527,
                "PatchRowcode": "+        mw if mw.__class__ is not old_cls else new_mw"
            },
            "138": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 528,
                "PatchRowcode": "+        for mw in mw_manager.middlewares"
            },
            "139": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 529,
                "PatchRowcode": "+    ])"
            },
            "140": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 530,
                "PatchRowcode": "+    for method_name, callbacks in mw_manager.methods.items():"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 531,
                "PatchRowcode": "+        for idx, meth in enumerate(callbacks):"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 532,
                "PatchRowcode": "+            method_cls = meth.__self__.__class__"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 533,
                "PatchRowcode": "+            if method_cls is old_cls:"
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 534,
                "PatchRowcode": "+                new_meth = getattr(new_mw, method_name)"
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 535,
                "PatchRowcode": "+                # logger.debug(\"{} is replaced with {}\".format(meth, new_meth))"
            },
            "146": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 536,
                "PatchRowcode": "+                callbacks[idx] = new_meth"
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "from __future__ import absolute_import",
            "",
            "import copy",
            "import json",
            "import logging",
            "import warnings",
            "from collections import defaultdict",
            "",
            "from six.moves.urllib.parse import urljoin",
            "from six.moves.http_cookiejar import CookieJar",
            "",
            "import scrapy",
            "from scrapy.exceptions import NotConfigured",
            "from scrapy.http.headers import Headers",
            "from scrapy.http.response.text import TextResponse",
            "from scrapy import signals",
            "",
            "from scrapy_splash.responsetypes import responsetypes",
            "from scrapy_splash.cookies import jar_to_har, har_to_jar",
            "from scrapy_splash.utils import (",
            "    scrapy_headers_to_unicode_dict,",
            "    json_based_hash,",
            "    parse_x_splash_saved_arguments_header,",
            ")",
            "from scrapy_splash.response import get_splash_status, get_splash_headers",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SlotPolicy(object):",
            "    PER_DOMAIN = 'per_domain'",
            "    SINGLE_SLOT = 'single_slot'",
            "    SCRAPY_DEFAULT = 'scrapy_default'",
            "",
            "    _known = {PER_DOMAIN, SINGLE_SLOT, SCRAPY_DEFAULT}",
            "",
            "",
            "class SplashCookiesMiddleware(object):",
            "    \"\"\"",
            "    This downloader middleware maintains cookiejars for Splash requests.",
            "",
            "    It gets cookies from 'cookies' field in Splash JSON responses",
            "    and sends current cookies in 'cookies' JSON POST argument instead of",
            "    sending them in http headers.",
            "",
            "    It should process requests before SplashMiddleware, and process responses",
            "    after SplashMiddleware.",
            "    \"\"\"",
            "    def __init__(self, debug=False):",
            "        self.jars = defaultdict(CookieJar)",
            "        self.debug = debug",
            "",
            "    @classmethod",
            "    def from_crawler(cls, crawler):",
            "        return cls(debug=crawler.settings.getbool('SPLASH_COOKIES_DEBUG'))",
            "",
            "    def process_request(self, request, spider):",
            "        \"\"\"",
            "        For Splash requests add 'cookies' key with current",
            "        cookies to ``request.meta['splash']['args']`` and remove cookie",
            "        headers sent to Splash itself.",
            "        \"\"\"",
            "        if 'splash' not in request.meta:",
            "            return",
            "",
            "        if request.meta.get('_splash_processed'):",
            "            request.headers.pop('Cookie', None)",
            "            return",
            "",
            "        splash_options = request.meta['splash']",
            "",
            "        splash_args = splash_options.setdefault('args', {})",
            "        if 'cookies' in splash_args:  # cookies already set",
            "            return",
            "",
            "        if 'session_id' not in splash_options:",
            "            return",
            "",
            "        jar = self.jars[splash_options['session_id']]",
            "",
            "        cookies = self._get_request_cookies(request)",
            "        har_to_jar(jar, cookies)",
            "",
            "        splash_args['cookies'] = jar_to_har(jar)",
            "        self._debug_cookie(request, spider)",
            "",
            "    def process_response(self, request, response, spider):",
            "        \"\"\"",
            "        For Splash JSON responses add all cookies from",
            "        'cookies' in a response to the cookiejar.",
            "        \"\"\"",
            "        from scrapy_splash import SplashJsonResponse",
            "        if not isinstance(response, SplashJsonResponse):",
            "            return response",
            "",
            "        if 'cookies' not in response.data:",
            "            return response",
            "",
            "        if 'splash' not in request.meta:",
            "            return response",
            "",
            "        if not request.meta.get('_splash_processed'):",
            "            warnings.warn(\"SplashCookiesMiddleware requires SplashMiddleware\")",
            "            return response",
            "",
            "        splash_options = request.meta['splash']",
            "        session_id = splash_options.get('new_session_id',",
            "                                        splash_options.get('session_id'))",
            "        if session_id is None:",
            "            return response",
            "",
            "        jar = self.jars[session_id]",
            "        request_cookies = splash_options['args'].get('cookies', [])",
            "        har_to_jar(jar, response.data['cookies'], request_cookies)",
            "        self._debug_set_cookie(response, spider)",
            "        response.cookiejar = jar",
            "        return response",
            "",
            "    def _get_request_cookies(self, request):",
            "        if isinstance(request.cookies, dict):",
            "            return [",
            "                {'name': k, 'value': v} for k, v in request.cookies.items()",
            "            ]",
            "        return request.cookies or []",
            "",
            "    def _debug_cookie(self, request, spider):",
            "        if self.debug:",
            "            cl = request.meta['splash']['args']['cookies']",
            "            if cl:",
            "                cookies = '\\n'.join(",
            "                    'Cookie: {}'.format(self._har_repr(c)) for c in cl)",
            "                msg = 'Sending cookies to: {}\\n{}'.format(request, cookies)",
            "                logger.debug(msg, extra={'spider': spider})",
            "",
            "    def _debug_set_cookie(self, response, spider):",
            "        if self.debug:",
            "            cl = response.data['cookies']",
            "            if cl:",
            "                cookies = '\\n'.join(",
            "                    'Set-Cookie: {}'.format(self._har_repr(c)) for c in cl)",
            "                msg = 'Received cookies from: {}\\n{}'.format(response, cookies)",
            "                logger.debug(msg, extra={'spider': spider})",
            "",
            "    @staticmethod",
            "    def _har_repr(har_cookie):",
            "        return '{}={}'.format(har_cookie['name'], har_cookie['value'])",
            "",
            "",
            "class SplashDeduplicateArgsMiddleware(object):",
            "    \"\"\"",
            "    Spider middleware which allows not to store duplicate Splash argument",
            "    values in request queue. It works together with SplashMiddleware downloader",
            "    middleware.",
            "    \"\"\"",
            "    local_values_key = '_splash_local_values'",
            "",
            "    def process_spider_output(self, response, result, spider):",
            "        for el in result:",
            "            if isinstance(el, scrapy.Request):",
            "                yield self._process_request(el, spider)",
            "            else:",
            "                yield el",
            "",
            "    def process_start_requests(self, start_requests, spider):",
            "        if not hasattr(spider, 'state'):",
            "            spider.state = {}",
            "        spider.state.setdefault(self.local_values_key, {})  # fingerprint => value dict",
            "",
            "        for req in start_requests:",
            "            yield self._process_request(req, spider)",
            "",
            "    def _process_request(self, request, spider):",
            "        \"\"\"",
            "        Replace requested meta['splash']['args'] values with their fingerprints.",
            "        This allows to store values only once in request queue, which helps",
            "        with disk queue size.",
            "",
            "        Downloader middleware should restore the values from fingerprints.",
            "        \"\"\"",
            "        if 'splash' not in request.meta:",
            "            return request",
            "",
            "        if '_replaced_args' in request.meta['splash']:",
            "            # don't process re-scheduled requests",
            "            # XXX: does it work as expected?",
            "            warnings.warn(\"Unexpected request.meta['splash']['_replaced_args']\")",
            "            return request",
            "",
            "        request.meta['splash']['_replaced_args'] = []",
            "        cache_args = request.meta['splash'].get('cache_args', [])",
            "        args = request.meta['splash'].setdefault('args', {})",
            "",
            "        for name in cache_args:",
            "            if name not in args:",
            "                continue",
            "            value = args[name]",
            "            fp = 'LOCAL+' + json_based_hash(value)",
            "            spider.state[self.local_values_key][fp] = value",
            "            args[name] = fp",
            "            request.meta['splash']['_replaced_args'].append(name)",
            "",
            "        return request",
            "",
            "",
            "class SplashMiddleware(object):",
            "    \"\"\"",
            "    Scrapy downloader and spider middleware that passes requests",
            "    through Splash when 'splash' Request.meta key is set.",
            "",
            "    This middleware also works together with SplashDeduplicateArgsMiddleware",
            "    spider middleware to allow not to store duplicate Splash argument values",
            "    in request queue and not to send them multiple times to Splash",
            "    (the latter requires Splash 2.1+).",
            "    \"\"\"",
            "    default_splash_url = 'http://127.0.0.1:8050'",
            "    default_endpoint = \"render.json\"",
            "    splash_extra_timeout = 5.0",
            "    default_policy = SlotPolicy.PER_DOMAIN",
            "    rescheduling_priority_adjust = +100",
            "    retry_498_priority_adjust = +50",
            "    remote_keys_key = '_splash_remote_keys'",
            "",
            "    def __init__(self, crawler, splash_base_url, slot_policy, log_400):",
            "        self.crawler = crawler",
            "        self.splash_base_url = splash_base_url",
            "        self.slot_policy = slot_policy",
            "        self.log_400 = log_400",
            "        self.crawler.signals.connect(self.spider_opened, signals.spider_opened)",
            "",
            "    @classmethod",
            "    def from_crawler(cls, crawler):",
            "        splash_base_url = crawler.settings.get('SPLASH_URL',",
            "                                               cls.default_splash_url)",
            "        log_400 = crawler.settings.getbool('SPLASH_LOG_400', True)",
            "        slot_policy = crawler.settings.get('SPLASH_SLOT_POLICY',",
            "                                           cls.default_policy)",
            "        if slot_policy not in SlotPolicy._known:",
            "            raise NotConfigured(\"Incorrect slot policy: %r\" % slot_policy)",
            "",
            "        return cls(crawler, splash_base_url, slot_policy, log_400)",
            "",
            "    def spider_opened(self, spider):",
            "        if not hasattr(spider, 'state'):",
            "            spider.state = {}",
            "",
            "        # local fingerprint => key returned by splash",
            "        spider.state.setdefault(self.remote_keys_key, {})",
            "",
            "    @property",
            "    def _argument_values(self):",
            "        key = SplashDeduplicateArgsMiddleware.local_values_key",
            "        return self.crawler.spider.state[key]",
            "",
            "    @property",
            "    def _remote_keys(self):",
            "        return self.crawler.spider.state[self.remote_keys_key]",
            "",
            "    def process_request(self, request, spider):",
            "        if 'splash' not in request.meta:",
            "            return",
            "",
            "        if request.method not in {'GET', 'POST'}:",
            "            logger.warning(",
            "                \"Currently only GET and POST requests are supported by \"",
            "                \"SplashMiddleware; %(request)s will be handled without Splash\",",
            "                {'request': request},",
            "                extra={'spider': spider}",
            "            )",
            "            return request",
            "",
            "        if request.meta.get(\"_splash_processed\"):",
            "            # don't process the same request more than once",
            "            return",
            "",
            "        splash_options = request.meta['splash']",
            "        request.meta['_splash_processed'] = True",
            "",
            "        slot_policy = splash_options.get('slot_policy', self.slot_policy)",
            "        self._set_download_slot(request, request.meta, slot_policy)",
            "",
            "        args = splash_options.setdefault('args', {})",
            "",
            "        if '_replaced_args' in splash_options:",
            "            # restore arguments before sending request to the downloader",
            "            load_args = {}",
            "            save_args = []",
            "            local_arg_fingerprints = {}",
            "            for name in splash_options['_replaced_args']:",
            "                fp = args[name]",
            "                # Use remote Splash argument cache: if Splash key",
            "                # for a value is known then don't send the value to Splash;",
            "                # if it is unknown then try to save the value on server using",
            "                # ``save_args``.",
            "                if fp in self._remote_keys:",
            "                    load_args[name] = self._remote_keys[fp]",
            "                    del args[name]",
            "                else:",
            "                    save_args.append(name)",
            "                    args[name] = self._argument_values[fp]",
            "",
            "                local_arg_fingerprints[name] = fp",
            "",
            "            if load_args:",
            "                args['load_args'] = load_args",
            "            if save_args:",
            "                args['save_args'] = save_args",
            "            splash_options['_local_arg_fingerprints'] = local_arg_fingerprints",
            "",
            "            del splash_options['_replaced_args']  # ??",
            "",
            "        args.setdefault('url', request.url)",
            "        if request.method == 'POST':",
            "            args.setdefault('http_method', request.method)",
            "            # XXX: non-UTF8 request bodies are not supported now",
            "            args.setdefault('body', request.body.decode('utf8'))",
            "",
            "        if not splash_options.get('dont_send_headers'):",
            "            headers = scrapy_headers_to_unicode_dict(request.headers)",
            "            if headers:",
            "                args.setdefault('headers', headers)",
            "",
            "        body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)",
            "        # print(body)",
            "",
            "        if 'timeout' in args:",
            "            # User requested a Splash timeout explicitly.",
            "            #",
            "            # We can't catch a case when user requested `download_timeout`",
            "            # explicitly because a default value for `download_timeout`",
            "            # is set by DownloadTimeoutMiddleware.",
            "            #",
            "            # As user requested Splash timeout explicitly, we shouldn't change",
            "            # it. Another reason not to change the requested Splash timeout is",
            "            # because it may cause a validation error on the remote end.",
            "            #",
            "            # But we can change Scrapy `download_timeout`: increase",
            "            # it when it's too small. Decreasing `download_timeout` is not",
            "            # safe.",
            "",
            "            timeout_requested = float(args['timeout'])",
            "            timeout_expected = timeout_requested + self.splash_extra_timeout",
            "",
            "            # no timeout means infinite timeout",
            "            timeout_current = request.meta.get('download_timeout', 1e6)",
            "",
            "            if timeout_expected > timeout_current:",
            "                request.meta['download_timeout'] = timeout_expected",
            "",
            "        endpoint = splash_options.setdefault('endpoint', self.default_endpoint)",
            "        splash_base_url = splash_options.get('splash_url', self.splash_base_url)",
            "        splash_url = urljoin(splash_base_url, endpoint)",
            "",
            "        headers = Headers({'Content-Type': 'application/json'})",
            "        headers.update(splash_options.get('splash_headers', {}))",
            "        new_request = request.replace(",
            "            url=splash_url,",
            "            method='POST',",
            "            body=body,",
            "            headers=headers,",
            "            priority=request.priority + self.rescheduling_priority_adjust",
            "        )",
            "        self.crawler.stats.inc_value('splash/%s/request_count' % endpoint)",
            "        return new_request",
            "",
            "    def process_response(self, request, response, spider):",
            "        if not request.meta.get(\"_splash_processed\"):",
            "            return response",
            "",
            "        splash_options = request.meta['splash']",
            "        if not splash_options:",
            "            return response",
            "",
            "        # update stats",
            "        endpoint = splash_options['endpoint']",
            "        self.crawler.stats.inc_value(",
            "            'splash/%s/response_count/%s' % (endpoint, response.status)",
            "        )",
            "",
            "        # handle save_args/load_args",
            "        self._process_x_splash_saved_arguments(request, response)",
            "        if get_splash_status(response) == 498:",
            "            logger.debug(\"Got HTTP 498 response for {}; \"",
            "                         \"sending arguments again.\".format(request),",
            "                         extra={'spider': spider})",
            "            return self._498_retry_request(request, response)",
            "",
            "        if splash_options.get('dont_process_response', False):",
            "            return response",
            "",
            "        response = self._change_response_class(request, response)",
            "",
            "        if self.log_400 and get_splash_status(response) == 400:",
            "            self._log_400(request, response, spider)",
            "",
            "        return response",
            "",
            "    def _change_response_class(self, request, response):",
            "        from scrapy_splash import SplashResponse, SplashTextResponse",
            "        if not isinstance(response, (SplashResponse, SplashTextResponse)):",
            "            # create a custom Response subclass based on response Content-Type",
            "            # XXX: usually request is assigned to response only when all",
            "            # downloader middlewares are executed. Here it is set earlier.",
            "            # Does it have any negative consequences?",
            "            respcls = responsetypes.from_args(headers=response.headers)",
            "            if isinstance(response, TextResponse) and respcls is SplashResponse:",
            "                # Even if the headers say it's binary, it has already",
            "                # been detected as a text response by scrapy (for example",
            "                # because it was decoded successfully), so we should not",
            "                # convert it to SplashResponse.",
            "                respcls = SplashTextResponse",
            "            response = response.replace(cls=respcls, request=request)",
            "        return response",
            "",
            "    def _log_400(self, request, response, spider):",
            "        from scrapy_splash import SplashJsonResponse",
            "        if isinstance(response, SplashJsonResponse):",
            "            logger.warning(",
            "                \"Bad request to Splash: %s\" % response.data,",
            "                {'request': request},",
            "                extra={'spider': spider}",
            "            )",
            "",
            "    def _process_x_splash_saved_arguments(self, request, response):",
            "        \"\"\" Keep track of arguments saved by Splash. \"\"\"",
            "        saved_args = get_splash_headers(response).get(b'X-Splash-Saved-Arguments')",
            "        if not saved_args:",
            "            return",
            "        saved_args = parse_x_splash_saved_arguments_header(saved_args)",
            "        arg_fingerprints = request.meta['splash']['_local_arg_fingerprints']",
            "        for name, key in saved_args.items():",
            "            fp = arg_fingerprints[name]",
            "            self._remote_keys[fp] = key",
            "",
            "    def _498_retry_request(self, request, response):",
            "        \"\"\"",
            "        Return a retry request for HTTP 498 responses. HTTP 498 means",
            "        load_args are not present on server; client should retry the request",
            "        with full argument values instead of their hashes.",
            "        \"\"\"",
            "        meta = copy.deepcopy(request.meta)",
            "        local_arg_fingerprints = meta['splash']['_local_arg_fingerprints']",
            "        args = meta['splash']['args']",
            "        args.pop('load_args', None)",
            "        args['save_args'] = list(local_arg_fingerprints.keys())",
            "",
            "        for name, fp in local_arg_fingerprints.items():",
            "            args[name] = self._argument_values[fp]",
            "            # print('remote_keys before:', self._remote_keys)",
            "            self._remote_keys.pop(fp, None)",
            "            # print('remote_keys after:', self._remote_keys)",
            "",
            "        body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)",
            "        # print(body)",
            "        request = request.replace(",
            "            meta=meta,",
            "            body=body,",
            "            priority=request.priority+self.retry_498_priority_adjust",
            "        )",
            "        return request",
            "",
            "    def _set_download_slot(self, request, meta, slot_policy):",
            "        if slot_policy == SlotPolicy.PER_DOMAIN:",
            "            # Use the same download slot to (sort of) respect download",
            "            # delays and concurrency options.",
            "            meta['download_slot'] = self._get_slot_key(request)",
            "",
            "        elif slot_policy == SlotPolicy.SINGLE_SLOT:",
            "            # Use a single slot for all Splash requests",
            "            meta['download_slot'] = '__splash__'",
            "",
            "        elif slot_policy == SlotPolicy.SCRAPY_DEFAULT:",
            "            # Use standard Scrapy concurrency setup",
            "            pass",
            "",
            "    def _get_slot_key(self, request_or_response):",
            "        return self.crawler.engine.downloader._get_slot_key(",
            "            request_or_response, None",
            "        )"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "from __future__ import absolute_import",
            "",
            "import copy",
            "import json",
            "import logging",
            "import warnings",
            "from collections import defaultdict",
            "",
            "from six.moves.urllib.parse import urljoin",
            "from six.moves.http_cookiejar import CookieJar",
            "",
            "from w3lib.http import basic_auth_header",
            "import scrapy",
            "from scrapy.exceptions import NotConfigured, IgnoreRequest",
            "from scrapy.http.headers import Headers",
            "from scrapy.http.response.text import TextResponse",
            "from scrapy import signals",
            "from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware",
            "",
            "from scrapy_splash.responsetypes import responsetypes",
            "from scrapy_splash.cookies import jar_to_har, har_to_jar",
            "from scrapy_splash.utils import (",
            "    scrapy_headers_to_unicode_dict,",
            "    json_based_hash,",
            "    parse_x_splash_saved_arguments_header,",
            ")",
            "from scrapy_splash.response import get_splash_status, get_splash_headers",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SlotPolicy(object):",
            "    PER_DOMAIN = 'per_domain'",
            "    SINGLE_SLOT = 'single_slot'",
            "    SCRAPY_DEFAULT = 'scrapy_default'",
            "",
            "    _known = {PER_DOMAIN, SINGLE_SLOT, SCRAPY_DEFAULT}",
            "",
            "",
            "class SplashCookiesMiddleware(object):",
            "    \"\"\"",
            "    This downloader middleware maintains cookiejars for Splash requests.",
            "",
            "    It gets cookies from 'cookies' field in Splash JSON responses",
            "    and sends current cookies in 'cookies' JSON POST argument instead of",
            "    sending them in http headers.",
            "",
            "    It should process requests before SplashMiddleware, and process responses",
            "    after SplashMiddleware.",
            "    \"\"\"",
            "    def __init__(self, debug=False):",
            "        self.jars = defaultdict(CookieJar)",
            "        self.debug = debug",
            "",
            "    @classmethod",
            "    def from_crawler(cls, crawler):",
            "        return cls(debug=crawler.settings.getbool('SPLASH_COOKIES_DEBUG'))",
            "",
            "    def process_request(self, request, spider):",
            "        \"\"\"",
            "        For Splash requests add 'cookies' key with current",
            "        cookies to ``request.meta['splash']['args']`` and remove cookie",
            "        headers sent to Splash itself.",
            "        \"\"\"",
            "        if 'splash' not in request.meta:",
            "            return",
            "",
            "        if request.meta.get('_splash_processed'):",
            "            request.headers.pop('Cookie', None)",
            "            return",
            "",
            "        splash_options = request.meta['splash']",
            "",
            "        splash_args = splash_options.setdefault('args', {})",
            "        if 'cookies' in splash_args:  # cookies already set",
            "            return",
            "",
            "        if 'session_id' not in splash_options:",
            "            return",
            "",
            "        jar = self.jars[splash_options['session_id']]",
            "",
            "        cookies = self._get_request_cookies(request)",
            "        har_to_jar(jar, cookies)",
            "",
            "        splash_args['cookies'] = jar_to_har(jar)",
            "        self._debug_cookie(request, spider)",
            "",
            "    def process_response(self, request, response, spider):",
            "        \"\"\"",
            "        For Splash JSON responses add all cookies from",
            "        'cookies' in a response to the cookiejar.",
            "        \"\"\"",
            "        from scrapy_splash import SplashJsonResponse",
            "        if not isinstance(response, SplashJsonResponse):",
            "            return response",
            "",
            "        if 'cookies' not in response.data:",
            "            return response",
            "",
            "        if 'splash' not in request.meta:",
            "            return response",
            "",
            "        if not request.meta.get('_splash_processed'):",
            "            warnings.warn(\"SplashCookiesMiddleware requires SplashMiddleware\")",
            "            return response",
            "",
            "        splash_options = request.meta['splash']",
            "        session_id = splash_options.get('new_session_id',",
            "                                        splash_options.get('session_id'))",
            "        if session_id is None:",
            "            return response",
            "",
            "        jar = self.jars[session_id]",
            "        request_cookies = splash_options['args'].get('cookies', [])",
            "        har_to_jar(jar, response.data['cookies'], request_cookies)",
            "        self._debug_set_cookie(response, spider)",
            "        response.cookiejar = jar",
            "        return response",
            "",
            "    def _get_request_cookies(self, request):",
            "        if isinstance(request.cookies, dict):",
            "            return [",
            "                {'name': k, 'value': v} for k, v in request.cookies.items()",
            "            ]",
            "        return request.cookies or []",
            "",
            "    def _debug_cookie(self, request, spider):",
            "        if self.debug:",
            "            cl = request.meta['splash']['args']['cookies']",
            "            if cl:",
            "                cookies = '\\n'.join(",
            "                    'Cookie: {}'.format(self._har_repr(c)) for c in cl)",
            "                msg = 'Sending cookies to: {}\\n{}'.format(request, cookies)",
            "                logger.debug(msg, extra={'spider': spider})",
            "",
            "    def _debug_set_cookie(self, response, spider):",
            "        if self.debug:",
            "            cl = response.data['cookies']",
            "            if cl:",
            "                cookies = '\\n'.join(",
            "                    'Set-Cookie: {}'.format(self._har_repr(c)) for c in cl)",
            "                msg = 'Received cookies from: {}\\n{}'.format(response, cookies)",
            "                logger.debug(msg, extra={'spider': spider})",
            "",
            "    @staticmethod",
            "    def _har_repr(har_cookie):",
            "        return '{}={}'.format(har_cookie['name'], har_cookie['value'])",
            "",
            "",
            "class SplashDeduplicateArgsMiddleware(object):",
            "    \"\"\"",
            "    Spider middleware which allows not to store duplicate Splash argument",
            "    values in request queue. It works together with SplashMiddleware downloader",
            "    middleware.",
            "    \"\"\"",
            "    local_values_key = '_splash_local_values'",
            "",
            "    def process_spider_output(self, response, result, spider):",
            "        for el in result:",
            "            if isinstance(el, scrapy.Request):",
            "                yield self._process_request(el, spider)",
            "            else:",
            "                yield el",
            "",
            "    def process_start_requests(self, start_requests, spider):",
            "        if not hasattr(spider, 'state'):",
            "            spider.state = {}",
            "        spider.state.setdefault(self.local_values_key, {})  # fingerprint => value dict",
            "",
            "        for req in start_requests:",
            "            yield self._process_request(req, spider)",
            "",
            "    def _process_request(self, request, spider):",
            "        \"\"\"",
            "        Replace requested meta['splash']['args'] values with their fingerprints.",
            "        This allows to store values only once in request queue, which helps",
            "        with disk queue size.",
            "",
            "        Downloader middleware should restore the values from fingerprints.",
            "        \"\"\"",
            "        if 'splash' not in request.meta:",
            "            return request",
            "",
            "        if '_replaced_args' in request.meta['splash']:",
            "            # don't process re-scheduled requests",
            "            # XXX: does it work as expected?",
            "            warnings.warn(\"Unexpected request.meta['splash']['_replaced_args']\")",
            "            return request",
            "",
            "        request.meta['splash']['_replaced_args'] = []",
            "        cache_args = request.meta['splash'].get('cache_args', [])",
            "        args = request.meta['splash'].setdefault('args', {})",
            "",
            "        for name in cache_args:",
            "            if name not in args:",
            "                continue",
            "            value = args[name]",
            "            fp = 'LOCAL+' + json_based_hash(value)",
            "            spider.state[self.local_values_key][fp] = value",
            "            args[name] = fp",
            "            request.meta['splash']['_replaced_args'].append(name)",
            "",
            "        return request",
            "",
            "",
            "class SplashMiddleware(object):",
            "    \"\"\"",
            "    Scrapy downloader and spider middleware that passes requests",
            "    through Splash when 'splash' Request.meta key is set.",
            "",
            "    This middleware also works together with SplashDeduplicateArgsMiddleware",
            "    spider middleware to allow not to store duplicate Splash argument values",
            "    in request queue and not to send them multiple times to Splash",
            "    (the latter requires Splash 2.1+).",
            "    \"\"\"",
            "    default_splash_url = 'http://127.0.0.1:8050'",
            "    default_endpoint = \"render.json\"",
            "    splash_extra_timeout = 5.0",
            "    default_policy = SlotPolicy.PER_DOMAIN",
            "    rescheduling_priority_adjust = +100",
            "    retry_498_priority_adjust = +50",
            "    remote_keys_key = '_splash_remote_keys'",
            "",
            "    def __init__(self, crawler, splash_base_url, slot_policy, log_400, auth):",
            "        self.crawler = crawler",
            "        self.splash_base_url = splash_base_url",
            "        self.slot_policy = slot_policy",
            "        self.log_400 = log_400",
            "        self.crawler.signals.connect(self.spider_opened, signals.spider_opened)",
            "        self.auth = auth",
            "",
            "    @classmethod",
            "    def from_crawler(cls, crawler):",
            "        s = crawler.settings",
            "        splash_base_url = s.get('SPLASH_URL', cls.default_splash_url)",
            "        log_400 = s.getbool('SPLASH_LOG_400', True)",
            "        slot_policy = s.get('SPLASH_SLOT_POLICY', cls.default_policy)",
            "        if slot_policy not in SlotPolicy._known:",
            "            raise NotConfigured(\"Incorrect slot policy: %r\" % slot_policy)",
            "",
            "        splash_user = s.get('SPLASH_USER', '')",
            "        splash_pass = s.get('SPLASH_PASS', '')",
            "        auth = None",
            "        if splash_user or splash_pass:",
            "            auth = basic_auth_header(splash_user, splash_pass)",
            "        return cls(crawler, splash_base_url, slot_policy, log_400, auth)",
            "",
            "    def spider_opened(self, spider):",
            "        if _http_auth_enabled(spider):",
            "            replace_downloader_middleware(self.crawler, RobotsTxtMiddleware,",
            "                                          SafeRobotsTxtMiddleware)",
            "        if not hasattr(spider, 'state'):",
            "            spider.state = {}",
            "",
            "        # local fingerprint => key returned by splash",
            "        spider.state.setdefault(self.remote_keys_key, {})",
            "",
            "    @property",
            "    def _argument_values(self):",
            "        key = SplashDeduplicateArgsMiddleware.local_values_key",
            "        return self.crawler.spider.state[key]",
            "",
            "    @property",
            "    def _remote_keys(self):",
            "        return self.crawler.spider.state[self.remote_keys_key]",
            "",
            "    def process_request(self, request, spider):",
            "        if 'splash' not in request.meta:",
            "            return",
            "        splash_options = request.meta['splash']",
            "",
            "        if request.method not in {'GET', 'POST'}:",
            "            logger.error(",
            "                \"Currently only GET and POST requests are supported by \"",
            "                \"SplashMiddleware; %(request)s is dropped\",",
            "                {'request': request},",
            "                extra={'spider': spider}",
            "            )",
            "            self.crawler.stats.inc_value('splash/dropped/method/{}'.format(",
            "                request.method))",
            "            raise IgnoreRequest(\"SplashRequest doesn't support \"",
            "                                \"HTTP {} method\".format(request.method))",
            "",
            "        if request.meta.get(\"_splash_processed\"):",
            "            # don't process the same request more than once",
            "            return",
            "",
            "        request.meta['_splash_processed'] = True",
            "",
            "        slot_policy = splash_options.get('slot_policy', self.slot_policy)",
            "        self._set_download_slot(request, request.meta, slot_policy)",
            "",
            "        args = splash_options.setdefault('args', {})",
            "",
            "        if '_replaced_args' in splash_options:",
            "            # restore arguments before sending request to the downloader",
            "            load_args = {}",
            "            save_args = []",
            "            local_arg_fingerprints = {}",
            "            for name in splash_options['_replaced_args']:",
            "                fp = args[name]",
            "                # Use remote Splash argument cache: if Splash key",
            "                # for a value is known then don't send the value to Splash;",
            "                # if it is unknown then try to save the value on server using",
            "                # ``save_args``.",
            "                if fp in self._remote_keys:",
            "                    load_args[name] = self._remote_keys[fp]",
            "                    del args[name]",
            "                else:",
            "                    save_args.append(name)",
            "                    args[name] = self._argument_values[fp]",
            "",
            "                local_arg_fingerprints[name] = fp",
            "",
            "            if load_args:",
            "                args['load_args'] = load_args",
            "            if save_args:",
            "                args['save_args'] = save_args",
            "            splash_options['_local_arg_fingerprints'] = local_arg_fingerprints",
            "",
            "            del splash_options['_replaced_args']  # ??",
            "",
            "        args.setdefault('url', request.url)",
            "        if request.method == 'POST':",
            "            args.setdefault('http_method', request.method)",
            "            # XXX: non-UTF8 request bodies are not supported now",
            "            args.setdefault('body', request.body.decode('utf8'))",
            "",
            "        if not splash_options.get('dont_send_headers'):",
            "            headers = scrapy_headers_to_unicode_dict(request.headers)",
            "            if headers:",
            "                # Headers set by HttpAuthMiddleware should be used for Splash,",
            "                # not for the remote website (backwards compatibility).",
            "                if _http_auth_enabled(spider):",
            "                    headers.pop('Authorization', None)",
            "                args.setdefault('headers', headers)",
            "",
            "        body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)",
            "        # print(body)",
            "",
            "        if 'timeout' in args:",
            "            # User requested a Splash timeout explicitly.",
            "            #",
            "            # We can't catch a case when user requested `download_timeout`",
            "            # explicitly because a default value for `download_timeout`",
            "            # is set by DownloadTimeoutMiddleware.",
            "            #",
            "            # As user requested Splash timeout explicitly, we shouldn't change",
            "            # it. Another reason not to change the requested Splash timeout is",
            "            # because it may cause a validation error on the remote end.",
            "            #",
            "            # But we can change Scrapy `download_timeout`: increase",
            "            # it when it's too small. Decreasing `download_timeout` is not",
            "            # safe.",
            "",
            "            timeout_requested = float(args['timeout'])",
            "            timeout_expected = timeout_requested + self.splash_extra_timeout",
            "",
            "            # no timeout means infinite timeout",
            "            timeout_current = request.meta.get('download_timeout', 1e6)",
            "",
            "            if timeout_expected > timeout_current:",
            "                request.meta['download_timeout'] = timeout_expected",
            "",
            "        endpoint = splash_options.setdefault('endpoint', self.default_endpoint)",
            "        splash_base_url = splash_options.get('splash_url', self.splash_base_url)",
            "        splash_url = urljoin(splash_base_url, endpoint)",
            "",
            "        headers = Headers({'Content-Type': 'application/json'})",
            "        if self.auth is not None:",
            "            headers['Authorization'] = self.auth",
            "        headers.update(splash_options.get('splash_headers', {}))",
            "        new_request = request.replace(",
            "            url=splash_url,",
            "            method='POST',",
            "            body=body,",
            "            headers=headers,",
            "            priority=request.priority + self.rescheduling_priority_adjust",
            "        )",
            "        new_request.meta['dont_obey_robotstxt'] = True",
            "        self.crawler.stats.inc_value('splash/%s/request_count' % endpoint)",
            "        return new_request",
            "",
            "    def process_response(self, request, response, spider):",
            "        if not request.meta.get(\"_splash_processed\"):",
            "            return response",
            "",
            "        splash_options = request.meta['splash']",
            "        if not splash_options:",
            "            return response",
            "",
            "        # update stats",
            "        endpoint = splash_options['endpoint']",
            "        self.crawler.stats.inc_value(",
            "            'splash/%s/response_count/%s' % (endpoint, response.status)",
            "        )",
            "",
            "        # handle save_args/load_args",
            "        self._process_x_splash_saved_arguments(request, response)",
            "        if get_splash_status(response) == 498:",
            "            logger.debug(\"Got HTTP 498 response for {}; \"",
            "                         \"sending arguments again.\".format(request),",
            "                         extra={'spider': spider})",
            "            return self._498_retry_request(request, response)",
            "",
            "        if splash_options.get('dont_process_response', False):",
            "            return response",
            "",
            "        response = self._change_response_class(request, response)",
            "",
            "        if self.log_400 and get_splash_status(response) == 400:",
            "            self._log_400(request, response, spider)",
            "",
            "        return response",
            "",
            "    def _change_response_class(self, request, response):",
            "        from scrapy_splash import SplashResponse, SplashTextResponse",
            "        if not isinstance(response, (SplashResponse, SplashTextResponse)):",
            "            # create a custom Response subclass based on response Content-Type",
            "            # XXX: usually request is assigned to response only when all",
            "            # downloader middlewares are executed. Here it is set earlier.",
            "            # Does it have any negative consequences?",
            "            respcls = responsetypes.from_args(headers=response.headers)",
            "            if isinstance(response, TextResponse) and respcls is SplashResponse:",
            "                # Even if the headers say it's binary, it has already",
            "                # been detected as a text response by scrapy (for example",
            "                # because it was decoded successfully), so we should not",
            "                # convert it to SplashResponse.",
            "                respcls = SplashTextResponse",
            "            response = response.replace(cls=respcls, request=request)",
            "        return response",
            "",
            "    def _log_400(self, request, response, spider):",
            "        from scrapy_splash import SplashJsonResponse",
            "        if isinstance(response, SplashJsonResponse):",
            "            logger.warning(",
            "                \"Bad request to Splash: %s\" % response.data,",
            "                {'request': request},",
            "                extra={'spider': spider}",
            "            )",
            "",
            "    def _process_x_splash_saved_arguments(self, request, response):",
            "        \"\"\" Keep track of arguments saved by Splash. \"\"\"",
            "        saved_args = get_splash_headers(response).get(b'X-Splash-Saved-Arguments')",
            "        if not saved_args:",
            "            return",
            "        saved_args = parse_x_splash_saved_arguments_header(saved_args)",
            "        arg_fingerprints = request.meta['splash']['_local_arg_fingerprints']",
            "        for name, key in saved_args.items():",
            "            fp = arg_fingerprints[name]",
            "            self._remote_keys[fp] = key",
            "",
            "    def _498_retry_request(self, request, response):",
            "        \"\"\"",
            "        Return a retry request for HTTP 498 responses. HTTP 498 means",
            "        load_args are not present on server; client should retry the request",
            "        with full argument values instead of their hashes.",
            "        \"\"\"",
            "        meta = copy.deepcopy(request.meta)",
            "        local_arg_fingerprints = meta['splash']['_local_arg_fingerprints']",
            "        args = meta['splash']['args']",
            "        args.pop('load_args', None)",
            "        args['save_args'] = list(local_arg_fingerprints.keys())",
            "",
            "        for name, fp in local_arg_fingerprints.items():",
            "            args[name] = self._argument_values[fp]",
            "            # print('remote_keys before:', self._remote_keys)",
            "            self._remote_keys.pop(fp, None)",
            "            # print('remote_keys after:', self._remote_keys)",
            "",
            "        body = json.dumps(args, ensure_ascii=False, sort_keys=True, indent=4)",
            "        # print(body)",
            "        request = request.replace(",
            "            meta=meta,",
            "            body=body,",
            "            priority=request.priority+self.retry_498_priority_adjust",
            "        )",
            "        return request",
            "",
            "    def _set_download_slot(self, request, meta, slot_policy):",
            "        if slot_policy == SlotPolicy.PER_DOMAIN:",
            "            # Use the same download slot to (sort of) respect download",
            "            # delays and concurrency options.",
            "            meta['download_slot'] = self._get_slot_key(request)",
            "",
            "        elif slot_policy == SlotPolicy.SINGLE_SLOT:",
            "            # Use a single slot for all Splash requests",
            "            meta['download_slot'] = '__splash__'",
            "",
            "        elif slot_policy == SlotPolicy.SCRAPY_DEFAULT:",
            "            # Use standard Scrapy concurrency setup",
            "            pass",
            "",
            "    def _get_slot_key(self, request_or_response):",
            "        return self.crawler.engine.downloader._get_slot_key(",
            "            request_or_response, None",
            "        )",
            "",
            "",
            "class SafeRobotsTxtMiddleware(RobotsTxtMiddleware):",
            "    def process_request(self, request, spider):",
            "        # disable robots.txt for Splash requests",
            "        if _http_auth_enabled(spider) and 'splash' in request.meta:",
            "            return",
            "        return super(SafeRobotsTxtMiddleware, self).process_request(",
            "            request, spider)",
            "",
            "",
            "def _http_auth_enabled(spider):",
            "    # FIXME: this function should always return False if HttpAuthMiddleware is",
            "    # not in a middleware list.",
            "    return getattr(spider, 'http_user', '') or getattr(spider, 'http_pass', '')",
            "",
            "",
            "def replace_downloader_middleware(crawler, old_cls, new_cls):",
            "    \"\"\" Replace downloader middleware with another one \"\"\"",
            "    try:",
            "        new_mw = new_cls.from_crawler(crawler)",
            "    except NotConfigured:",
            "        return",
            "",
            "    mw_manager = crawler.engine.downloader.middleware",
            "    mw_manager.middlewares = tuple([",
            "        mw if mw.__class__ is not old_cls else new_mw",
            "        for mw in mw_manager.middlewares",
            "    ])",
            "    for method_name, callbacks in mw_manager.methods.items():",
            "        for idx, meth in enumerate(callbacks):",
            "            method_cls = meth.__self__.__class__",
            "            if method_cls is old_cls:",
            "                new_meth = getattr(new_mw, method_name)",
            "                # logger.debug(\"{} is replaced with {}\".format(meth, new_meth))",
            "                callbacks[idx] = new_meth"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "14": [],
            "225": [
                "SplashMiddleware",
                "__init__"
            ],
            "234": [
                "SplashMiddleware",
                "from_crawler"
            ],
            "235": [
                "SplashMiddleware",
                "from_crawler"
            ],
            "236": [
                "SplashMiddleware",
                "from_crawler"
            ],
            "237": [
                "SplashMiddleware",
                "from_crawler"
            ],
            "238": [
                "SplashMiddleware",
                "from_crawler"
            ],
            "242": [
                "SplashMiddleware",
                "from_crawler"
            ],
            "265": [
                "SplashMiddleware",
                "process_request"
            ],
            "267": [
                "SplashMiddleware",
                "process_request"
            ],
            "271": [
                "SplashMiddleware",
                "process_request"
            ],
            "277": [
                "SplashMiddleware",
                "process_request"
            ]
        },
        "addLocation": []
    }
}