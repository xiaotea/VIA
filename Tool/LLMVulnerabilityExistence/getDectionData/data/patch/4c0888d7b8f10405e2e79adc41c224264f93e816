{
    "python/paddle/hapi/hub.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "             hub_dir,"
            },
            "1": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "             check_exist=not force_reload,"
            },
            "2": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": 119,
                "PatchRowcode": "             decompress=False,"
            },
            "3": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            method=('wget' if source == 'gitee' else 'get'),"
            },
            "4": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": 120,
                "PatchRowcode": "         )"
            },
            "5": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": 121,
                "PatchRowcode": "         shutil.move(fpath, cached_file)"
            },
            "6": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 122,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import os",
            "import shutil",
            "import sys",
            "import zipfile",
            "",
            "from paddle.utils.download import get_path_from_url",
            "",
            "__all__ = []",
            "",
            "DEFAULT_CACHE_DIR = '~/.cache'",
            "VAR_DEPENDENCY = 'dependencies'",
            "MODULE_HUBCONF = 'hubconf.py'",
            "HUB_DIR = os.path.expanduser(os.path.join('~', '.cache', 'paddle', 'hub'))",
            "",
            "",
            "def _remove_if_exists(path):",
            "    if os.path.exists(path):",
            "        if os.path.isfile(path):",
            "            os.remove(path)",
            "        else:",
            "            shutil.rmtree(path)",
            "",
            "",
            "def _import_module(name, repo_dir):",
            "    sys.path.insert(0, repo_dir)",
            "    try:",
            "        hub_module = __import__(name)",
            "        sys.modules.pop(name)",
            "    except ImportError:",
            "        sys.path.remove(repo_dir)",
            "        raise RuntimeError(",
            "            'Please make sure config exists or repo error messages above fixed when importing'",
            "        )",
            "",
            "    sys.path.remove(repo_dir)",
            "",
            "    return hub_module",
            "",
            "",
            "def _git_archive_link(repo_owner, repo_name, branch, source):",
            "    if source == 'github':",
            "        return (",
            "            f'https://github.com/{repo_owner}/{repo_name}/archive/{branch}.zip'",
            "        )",
            "    elif source == 'gitee':",
            "        return 'https://gitee.com/{}/{}/repository/archive/{}.zip'.format(",
            "            repo_owner, repo_name, branch",
            "        )",
            "",
            "",
            "def _parse_repo_info(repo, source):",
            "    branch = 'main' if source == 'github' else 'master'",
            "    if ':' in repo:",
            "        repo_info, branch = repo.split(':')",
            "    else:",
            "        repo_info = repo",
            "    repo_owner, repo_name = repo_info.split('/')",
            "    return repo_owner, repo_name, branch",
            "",
            "",
            "def _make_dirs(dirname):",
            "    try:",
            "        from pathlib import Path",
            "    except ImportError:",
            "        from pathlib2 import Path",
            "    Path(dirname).mkdir(exist_ok=True)",
            "",
            "",
            "def _get_cache_or_reload(repo, force_reload, verbose=True, source='github'):",
            "    # Setup hub_dir to save downloaded files",
            "    hub_dir = HUB_DIR",
            "",
            "    _make_dirs(hub_dir)",
            "",
            "    # Parse github/gitee repo information",
            "    repo_owner, repo_name, branch = _parse_repo_info(repo, source)",
            "    # Github allows branch name with slash '/',",
            "    # this causes confusion with path on both Linux and Windows.",
            "    # Backslash is not allowed in Github branch name so no need to",
            "    # to worry about it.",
            "    normalized_br = branch.replace('/', '_')",
            "    # Github renames folder repo/v1.x.x to repo-1.x.x",
            "    # We don't know the repo name before downloading the zip file",
            "    # and inspect name from it.",
            "    # To check if cached repo exists, we need to normalize folder names.",
            "    repo_dir = os.path.join(",
            "        hub_dir, '_'.join([repo_owner, repo_name, normalized_br])",
            "    )",
            "",
            "    use_cache = (not force_reload) and os.path.exists(repo_dir)",
            "",
            "    if use_cache:",
            "        if verbose:",
            "            sys.stderr.write(f'Using cache found in {repo_dir}\\n')",
            "    else:",
            "        cached_file = os.path.join(hub_dir, normalized_br + '.zip')",
            "        _remove_if_exists(cached_file)",
            "",
            "        url = _git_archive_link(repo_owner, repo_name, branch, source=source)",
            "",
            "        fpath = get_path_from_url(",
            "            url,",
            "            hub_dir,",
            "            check_exist=not force_reload,",
            "            decompress=False,",
            "            method=('wget' if source == 'gitee' else 'get'),",
            "        )",
            "        shutil.move(fpath, cached_file)",
            "",
            "        with zipfile.ZipFile(cached_file) as cached_zipfile:",
            "            extracted_repo_name = cached_zipfile.infolist()[0].filename",
            "            extracted_repo = os.path.join(hub_dir, extracted_repo_name)",
            "            _remove_if_exists(extracted_repo)",
            "            # Unzip the code and rename the base folder",
            "            cached_zipfile.extractall(hub_dir)",
            "",
            "        _remove_if_exists(cached_file)",
            "        _remove_if_exists(repo_dir)",
            "        # Rename the repo",
            "        shutil.move(extracted_repo, repo_dir)",
            "",
            "    return repo_dir",
            "",
            "",
            "def _load_entry_from_hubconf(m, name):",
            "    '''load entry from hubconf'''",
            "    if not isinstance(name, str):",
            "        raise ValueError(",
            "            'Invalid input: model should be a str of function name'",
            "        )",
            "",
            "    func = getattr(m, name, None)",
            "",
            "    if func is None or not callable(func):",
            "        raise RuntimeError(f'Cannot find callable {name} in hubconf')",
            "",
            "    return func",
            "",
            "",
            "def _check_module_exists(name):",
            "    try:",
            "        __import__(name)",
            "        return True",
            "    except ImportError:",
            "        return False",
            "",
            "",
            "def _check_dependencies(m):",
            "    dependencies = getattr(m, VAR_DEPENDENCY, None)",
            "",
            "    if dependencies is not None:",
            "        missing_deps = [",
            "            pkg for pkg in dependencies if not _check_module_exists(pkg)",
            "        ]",
            "        if len(missing_deps):",
            "            raise RuntimeError(",
            "                'Missing dependencies: {}'.format(', '.join(missing_deps))",
            "            )",
            "",
            "",
            "def list(repo_dir, source='github', force_reload=False):",
            "    r\"\"\"",
            "    List all entrypoints available in `github` hubconf.",
            "",
            "    Args:",
            "        repo_dir(str): Github or local path.",
            "",
            "            - github path (str): A string with format \"repo_owner/repo_name[:tag_name]\" with an optional",
            "              tag/branch. The default branch is `main` if not specified.",
            "            - local path (str): Local repo path.",
            "",
            "        source (str): `github` | `gitee` | `local`. Default is `github`.",
            "        force_reload (bool, optional): Whether to discard the existing cache and force a fresh download. Default is `False`.",
            "",
            "    Returns:",
            "        entrypoints: A list of available entrypoint names.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> import paddle",
            "",
            "            >>> paddle.hub.list('lyuwenyu/paddlehub_demo:main', source='github', force_reload=False)",
            "",
            "    \"\"\"",
            "    if source not in ('github', 'gitee', 'local'):",
            "        raise ValueError(",
            "            f'Unknown source: \"{source}\". Allowed values: \"github\" | \"gitee\" | \"local\".'",
            "        )",
            "",
            "    if source in ('github', 'gitee'):",
            "        repo_dir = _get_cache_or_reload(",
            "            repo_dir, force_reload, True, source=source",
            "        )",
            "",
            "    hub_module = _import_module(MODULE_HUBCONF.split('.')[0], repo_dir)",
            "",
            "    entrypoints = [",
            "        f",
            "        for f in dir(hub_module)",
            "        if callable(getattr(hub_module, f)) and not f.startswith('_')",
            "    ]",
            "",
            "    return entrypoints",
            "",
            "",
            "def help(repo_dir, model, source='github', force_reload=False):",
            "    \"\"\"",
            "    Show help information of model",
            "",
            "    Args:",
            "        repo_dir(str): Github or local path.",
            "",
            "            - github path (str): A string with format \"repo_owner/repo_name[:tag_name]\" with an optional",
            "              tag/branch. The default branch is `main` if not specified.",
            "            - local path (str): Local repo path.",
            "",
            "        model (str): Model name.",
            "        source (str): `github` | `gitee` | `local`. Default is `github`.",
            "        force_reload (bool, optional): Default is `False`.",
            "",
            "    Returns:",
            "        docs",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> import paddle",
            "",
            "            >>> paddle.hub.help('lyuwenyu/paddlehub_demo:main', model='MM', source='github')",
            "",
            "    \"\"\"",
            "    if source not in ('github', 'gitee', 'local'):",
            "        raise ValueError(",
            "            f'Unknown source: \"{source}\". Allowed values: \"github\" | \"gitee\" | \"local\".'",
            "        )",
            "",
            "    if source in ('github', 'gitee'):",
            "        repo_dir = _get_cache_or_reload(",
            "            repo_dir, force_reload, True, source=source",
            "        )",
            "",
            "    hub_module = _import_module(MODULE_HUBCONF.split('.')[0], repo_dir)",
            "",
            "    entry = _load_entry_from_hubconf(hub_module, model)",
            "",
            "    return entry.__doc__",
            "",
            "",
            "def load(repo_dir, model, source='github', force_reload=False, **kwargs):",
            "    \"\"\"",
            "    Load model",
            "",
            "    Args:",
            "        repo_dir(str): Github or local path.",
            "",
            "            - github path (str): A string with format \"repo_owner/repo_name[:tag_name]\" with an optional",
            "              tag/branch. The default branch is `main` if not specified.",
            "            - local path (str): Local repo path.",
            "",
            "        model (str): Model name.",
            "        source (str): `github` | `gitee` | `local`. Default is `github`.",
            "        force_reload (bool, optional): Default is `False`.",
            "        **kwargs: Parameters using for model.",
            "",
            "    Returns:",
            "        paddle model.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> import paddle",
            "            >>> paddle.hub.load('lyuwenyu/paddlehub_demo:main', model='MM', source='github')",
            "",
            "    \"\"\"",
            "    if source not in ('github', 'gitee', 'local'):",
            "        raise ValueError(",
            "            f'Unknown source: \"{source}\". Allowed values: \"github\" | \"gitee\" | \"local\".'",
            "        )",
            "",
            "    if source in ('github', 'gitee'):",
            "        repo_dir = _get_cache_or_reload(",
            "            repo_dir, force_reload, True, source=source",
            "        )",
            "",
            "    hub_module = _import_module(MODULE_HUBCONF.split('.')[0], repo_dir)",
            "",
            "    _check_dependencies(hub_module)",
            "",
            "    entry = _load_entry_from_hubconf(hub_module, model)",
            "",
            "    return entry(**kwargs)"
        ],
        "afterPatchFile": [
            "# Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import os",
            "import shutil",
            "import sys",
            "import zipfile",
            "",
            "from paddle.utils.download import get_path_from_url",
            "",
            "__all__ = []",
            "",
            "DEFAULT_CACHE_DIR = '~/.cache'",
            "VAR_DEPENDENCY = 'dependencies'",
            "MODULE_HUBCONF = 'hubconf.py'",
            "HUB_DIR = os.path.expanduser(os.path.join('~', '.cache', 'paddle', 'hub'))",
            "",
            "",
            "def _remove_if_exists(path):",
            "    if os.path.exists(path):",
            "        if os.path.isfile(path):",
            "            os.remove(path)",
            "        else:",
            "            shutil.rmtree(path)",
            "",
            "",
            "def _import_module(name, repo_dir):",
            "    sys.path.insert(0, repo_dir)",
            "    try:",
            "        hub_module = __import__(name)",
            "        sys.modules.pop(name)",
            "    except ImportError:",
            "        sys.path.remove(repo_dir)",
            "        raise RuntimeError(",
            "            'Please make sure config exists or repo error messages above fixed when importing'",
            "        )",
            "",
            "    sys.path.remove(repo_dir)",
            "",
            "    return hub_module",
            "",
            "",
            "def _git_archive_link(repo_owner, repo_name, branch, source):",
            "    if source == 'github':",
            "        return (",
            "            f'https://github.com/{repo_owner}/{repo_name}/archive/{branch}.zip'",
            "        )",
            "    elif source == 'gitee':",
            "        return 'https://gitee.com/{}/{}/repository/archive/{}.zip'.format(",
            "            repo_owner, repo_name, branch",
            "        )",
            "",
            "",
            "def _parse_repo_info(repo, source):",
            "    branch = 'main' if source == 'github' else 'master'",
            "    if ':' in repo:",
            "        repo_info, branch = repo.split(':')",
            "    else:",
            "        repo_info = repo",
            "    repo_owner, repo_name = repo_info.split('/')",
            "    return repo_owner, repo_name, branch",
            "",
            "",
            "def _make_dirs(dirname):",
            "    try:",
            "        from pathlib import Path",
            "    except ImportError:",
            "        from pathlib2 import Path",
            "    Path(dirname).mkdir(exist_ok=True)",
            "",
            "",
            "def _get_cache_or_reload(repo, force_reload, verbose=True, source='github'):",
            "    # Setup hub_dir to save downloaded files",
            "    hub_dir = HUB_DIR",
            "",
            "    _make_dirs(hub_dir)",
            "",
            "    # Parse github/gitee repo information",
            "    repo_owner, repo_name, branch = _parse_repo_info(repo, source)",
            "    # Github allows branch name with slash '/',",
            "    # this causes confusion with path on both Linux and Windows.",
            "    # Backslash is not allowed in Github branch name so no need to",
            "    # to worry about it.",
            "    normalized_br = branch.replace('/', '_')",
            "    # Github renames folder repo/v1.x.x to repo-1.x.x",
            "    # We don't know the repo name before downloading the zip file",
            "    # and inspect name from it.",
            "    # To check if cached repo exists, we need to normalize folder names.",
            "    repo_dir = os.path.join(",
            "        hub_dir, '_'.join([repo_owner, repo_name, normalized_br])",
            "    )",
            "",
            "    use_cache = (not force_reload) and os.path.exists(repo_dir)",
            "",
            "    if use_cache:",
            "        if verbose:",
            "            sys.stderr.write(f'Using cache found in {repo_dir}\\n')",
            "    else:",
            "        cached_file = os.path.join(hub_dir, normalized_br + '.zip')",
            "        _remove_if_exists(cached_file)",
            "",
            "        url = _git_archive_link(repo_owner, repo_name, branch, source=source)",
            "",
            "        fpath = get_path_from_url(",
            "            url,",
            "            hub_dir,",
            "            check_exist=not force_reload,",
            "            decompress=False,",
            "        )",
            "        shutil.move(fpath, cached_file)",
            "",
            "        with zipfile.ZipFile(cached_file) as cached_zipfile:",
            "            extracted_repo_name = cached_zipfile.infolist()[0].filename",
            "            extracted_repo = os.path.join(hub_dir, extracted_repo_name)",
            "            _remove_if_exists(extracted_repo)",
            "            # Unzip the code and rename the base folder",
            "            cached_zipfile.extractall(hub_dir)",
            "",
            "        _remove_if_exists(cached_file)",
            "        _remove_if_exists(repo_dir)",
            "        # Rename the repo",
            "        shutil.move(extracted_repo, repo_dir)",
            "",
            "    return repo_dir",
            "",
            "",
            "def _load_entry_from_hubconf(m, name):",
            "    '''load entry from hubconf'''",
            "    if not isinstance(name, str):",
            "        raise ValueError(",
            "            'Invalid input: model should be a str of function name'",
            "        )",
            "",
            "    func = getattr(m, name, None)",
            "",
            "    if func is None or not callable(func):",
            "        raise RuntimeError(f'Cannot find callable {name} in hubconf')",
            "",
            "    return func",
            "",
            "",
            "def _check_module_exists(name):",
            "    try:",
            "        __import__(name)",
            "        return True",
            "    except ImportError:",
            "        return False",
            "",
            "",
            "def _check_dependencies(m):",
            "    dependencies = getattr(m, VAR_DEPENDENCY, None)",
            "",
            "    if dependencies is not None:",
            "        missing_deps = [",
            "            pkg for pkg in dependencies if not _check_module_exists(pkg)",
            "        ]",
            "        if len(missing_deps):",
            "            raise RuntimeError(",
            "                'Missing dependencies: {}'.format(', '.join(missing_deps))",
            "            )",
            "",
            "",
            "def list(repo_dir, source='github', force_reload=False):",
            "    r\"\"\"",
            "    List all entrypoints available in `github` hubconf.",
            "",
            "    Args:",
            "        repo_dir(str): Github or local path.",
            "",
            "            - github path (str): A string with format \"repo_owner/repo_name[:tag_name]\" with an optional",
            "              tag/branch. The default branch is `main` if not specified.",
            "            - local path (str): Local repo path.",
            "",
            "        source (str): `github` | `gitee` | `local`. Default is `github`.",
            "        force_reload (bool, optional): Whether to discard the existing cache and force a fresh download. Default is `False`.",
            "",
            "    Returns:",
            "        entrypoints: A list of available entrypoint names.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> import paddle",
            "",
            "            >>> paddle.hub.list('lyuwenyu/paddlehub_demo:main', source='github', force_reload=False)",
            "",
            "    \"\"\"",
            "    if source not in ('github', 'gitee', 'local'):",
            "        raise ValueError(",
            "            f'Unknown source: \"{source}\". Allowed values: \"github\" | \"gitee\" | \"local\".'",
            "        )",
            "",
            "    if source in ('github', 'gitee'):",
            "        repo_dir = _get_cache_or_reload(",
            "            repo_dir, force_reload, True, source=source",
            "        )",
            "",
            "    hub_module = _import_module(MODULE_HUBCONF.split('.')[0], repo_dir)",
            "",
            "    entrypoints = [",
            "        f",
            "        for f in dir(hub_module)",
            "        if callable(getattr(hub_module, f)) and not f.startswith('_')",
            "    ]",
            "",
            "    return entrypoints",
            "",
            "",
            "def help(repo_dir, model, source='github', force_reload=False):",
            "    \"\"\"",
            "    Show help information of model",
            "",
            "    Args:",
            "        repo_dir(str): Github or local path.",
            "",
            "            - github path (str): A string with format \"repo_owner/repo_name[:tag_name]\" with an optional",
            "              tag/branch. The default branch is `main` if not specified.",
            "            - local path (str): Local repo path.",
            "",
            "        model (str): Model name.",
            "        source (str): `github` | `gitee` | `local`. Default is `github`.",
            "        force_reload (bool, optional): Default is `False`.",
            "",
            "    Returns:",
            "        docs",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> import paddle",
            "",
            "            >>> paddle.hub.help('lyuwenyu/paddlehub_demo:main', model='MM', source='github')",
            "",
            "    \"\"\"",
            "    if source not in ('github', 'gitee', 'local'):",
            "        raise ValueError(",
            "            f'Unknown source: \"{source}\". Allowed values: \"github\" | \"gitee\" | \"local\".'",
            "        )",
            "",
            "    if source in ('github', 'gitee'):",
            "        repo_dir = _get_cache_or_reload(",
            "            repo_dir, force_reload, True, source=source",
            "        )",
            "",
            "    hub_module = _import_module(MODULE_HUBCONF.split('.')[0], repo_dir)",
            "",
            "    entry = _load_entry_from_hubconf(hub_module, model)",
            "",
            "    return entry.__doc__",
            "",
            "",
            "def load(repo_dir, model, source='github', force_reload=False, **kwargs):",
            "    \"\"\"",
            "    Load model",
            "",
            "    Args:",
            "        repo_dir(str): Github or local path.",
            "",
            "            - github path (str): A string with format \"repo_owner/repo_name[:tag_name]\" with an optional",
            "              tag/branch. The default branch is `main` if not specified.",
            "            - local path (str): Local repo path.",
            "",
            "        model (str): Model name.",
            "        source (str): `github` | `gitee` | `local`. Default is `github`.",
            "        force_reload (bool, optional): Default is `False`.",
            "        **kwargs: Parameters using for model.",
            "",
            "    Returns:",
            "        paddle model.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> import paddle",
            "            >>> paddle.hub.load('lyuwenyu/paddlehub_demo:main', model='MM', source='github')",
            "",
            "    \"\"\"",
            "    if source not in ('github', 'gitee', 'local'):",
            "        raise ValueError(",
            "            f'Unknown source: \"{source}\". Allowed values: \"github\" | \"gitee\" | \"local\".'",
            "        )",
            "",
            "    if source in ('github', 'gitee'):",
            "        repo_dir = _get_cache_or_reload(",
            "            repo_dir, force_reload, True, source=source",
            "        )",
            "",
            "    hub_module = _import_module(MODULE_HUBCONF.split('.')[0], repo_dir)",
            "",
            "    _check_dependencies(hub_module)",
            "",
            "    entry = _load_entry_from_hubconf(hub_module, model)",
            "",
            "    return entry(**kwargs)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "120": [
                "_get_cache_or_reload"
            ]
        },
        "addLocation": []
    },
    "python/paddle/utils/download.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " import hashlib"
            },
            "1": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " import os"
            },
            "2": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " import os.path as osp"
            },
            "3": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import shlex"
            },
            "4": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " import shutil"
            },
            "5": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import subprocess"
            },
            "6": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " import sys"
            },
            "7": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " import tarfile"
            },
            "8": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " import time"
            },
            "9": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " import zipfile"
            },
            "10": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from urllib.parse import urlparse"
            },
            "11": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " import httpx"
            },
            "13": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 198,
                "afterPatchRowNumber": 195,
                "PatchRowcode": "         return False"
            },
            "15": {
                "beforePatchRowNumber": 199,
                "afterPatchRowNumber": 196,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 200,
                "afterPatchRowNumber": 197,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 201,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-def _wget_download(url: str, fullname: str):"
            },
            "18": {
                "beforePatchRowNumber": 202,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    try:"
            },
            "19": {
                "beforePatchRowNumber": 203,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        assert urlparse(url).scheme in ("
            },
            "20": {
                "beforePatchRowNumber": 204,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            'http',"
            },
            "21": {
                "beforePatchRowNumber": 205,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            'https',"
            },
            "22": {
                "beforePatchRowNumber": 206,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        ), 'Only support https and http url'"
            },
            "23": {
                "beforePatchRowNumber": 207,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # using wget to download url"
            },
            "24": {
                "beforePatchRowNumber": 208,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        tmp_fullname = shlex.quote(fullname + \"_tmp\")"
            },
            "25": {
                "beforePatchRowNumber": 209,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        url = shlex.quote(url)"
            },
            "26": {
                "beforePatchRowNumber": 210,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        # \u2013user-agent"
            },
            "27": {
                "beforePatchRowNumber": 211,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'"
            },
            "28": {
                "beforePatchRowNumber": 212,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        subprc = subprocess.Popen("
            },
            "29": {
                "beforePatchRowNumber": 213,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE"
            },
            "30": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        )"
            },
            "31": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        _ = subprc.communicate()"
            },
            "32": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "33": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if subprc.returncode != 0:"
            },
            "34": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise RuntimeError("
            },
            "35": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                f'{command} failed. Please make sure `wget` is installed or {url} exists'"
            },
            "36": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "37": {
                "beforePatchRowNumber": 221,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "38": {
                "beforePatchRowNumber": 222,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        shutil.move(tmp_fullname, fullname)"
            },
            "39": {
                "beforePatchRowNumber": 223,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "40": {
                "beforePatchRowNumber": 224,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    except Exception as e:  # requests.exceptions.ConnectionError"
            },
            "41": {
                "beforePatchRowNumber": 225,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        logger.info(f\"Downloading {url} failed with exception {str(e)}\")"
            },
            "42": {
                "beforePatchRowNumber": 226,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return False"
            },
            "43": {
                "beforePatchRowNumber": 227,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "44": {
                "beforePatchRowNumber": 228,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    return fullname"
            },
            "45": {
                "beforePatchRowNumber": 229,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "46": {
                "beforePatchRowNumber": 230,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "47": {
                "beforePatchRowNumber": 231,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-_download_methods = {"
            },
            "48": {
                "beforePatchRowNumber": 232,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    'get': _get_download,"
            },
            "49": {
                "beforePatchRowNumber": 233,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    'wget': _wget_download,"
            },
            "50": {
                "beforePatchRowNumber": 234,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-}"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 198,
                "PatchRowcode": "+_download_methods = {'get': _get_download}"
            },
            "52": {
                "beforePatchRowNumber": 235,
                "afterPatchRowNumber": 199,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": 200,
                "PatchRowcode": " "
            },
            "54": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 201,
                "PatchRowcode": " def _download(url, path, md5sum=None, method='get'):"
            }
        },
        "frontPatchFile": [
            "#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import hashlib",
            "import os",
            "import os.path as osp",
            "import shlex",
            "import shutil",
            "import subprocess",
            "import sys",
            "import tarfile",
            "import time",
            "import zipfile",
            "from urllib.parse import urlparse",
            "",
            "import httpx",
            "",
            "try:",
            "    from tqdm import tqdm",
            "except:",
            "",
            "    class tqdm:",
            "        def __init__(self, total=None):",
            "            self.total = total",
            "            self.n = 0",
            "",
            "        def update(self, n):",
            "            self.n += n",
            "            if self.total is None:",
            "                sys.stderr.write(f\"\\r{self.n:.1f} bytes\")",
            "            else:",
            "                sys.stderr.write(f\"\\r{100 * self.n / float(self.total):.1f}%\")",
            "            sys.stderr.flush()",
            "",
            "        def __enter__(self):",
            "            return self",
            "",
            "        def __exit__(self, exc_type, exc_val, exc_tb):",
            "            sys.stderr.write('\\n')",
            "",
            "",
            "import logging",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "__all__ = ['get_weights_path_from_url']",
            "",
            "WEIGHTS_HOME = osp.expanduser(\"~/.cache/paddle/hapi/weights\")",
            "",
            "DOWNLOAD_RETRY_LIMIT = 3",
            "",
            "",
            "def is_url(path):",
            "    \"\"\"",
            "    Whether path is URL.",
            "    Args:",
            "        path (string): URL string or not.",
            "    \"\"\"",
            "    return path.startswith('http://') or path.startswith('https://')",
            "",
            "",
            "def get_weights_path_from_url(url, md5sum=None):",
            "    \"\"\"Get weights path from WEIGHT_HOME, if not exists,",
            "    download it from url.",
            "",
            "    Args:",
            "        url (str): download url",
            "        md5sum (str): md5 sum of download package",
            "",
            "    Returns:",
            "        str: a local path to save downloaded weights.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> from paddle.utils.download import get_weights_path_from_url",
            "",
            "            >>> resnet18_pretrained_weight_url = 'https://paddle-hapi.bj.bcebos.com/models/resnet18.pdparams'",
            "            >>> local_weight_path = get_weights_path_from_url(resnet18_pretrained_weight_url)",
            "",
            "    \"\"\"",
            "    path = get_path_from_url(url, WEIGHTS_HOME, md5sum)",
            "    return path",
            "",
            "",
            "def _map_path(url, root_dir):",
            "    # parse path after download under root_dir",
            "    fname = osp.split(url)[-1]",
            "    fpath = fname",
            "    return osp.join(root_dir, fpath)",
            "",
            "",
            "def _get_unique_endpoints(trainer_endpoints):",
            "    # Sorting is to avoid different environmental variables for each card",
            "    trainer_endpoints.sort()",
            "    ips = set()",
            "    unique_endpoints = set()",
            "    for endpoint in trainer_endpoints:",
            "        ip = endpoint.split(\":\")[0]",
            "        if ip in ips:",
            "            continue",
            "        ips.add(ip)",
            "        unique_endpoints.add(endpoint)",
            "    logger.info(f\"unique_endpoints {unique_endpoints}\")",
            "    return unique_endpoints",
            "",
            "",
            "def get_path_from_url(",
            "    url, root_dir, md5sum=None, check_exist=True, decompress=True, method='get'",
            "):",
            "    \"\"\"Download from given url to root_dir.",
            "    if file or directory specified by url is exists under",
            "    root_dir, return the path directly, otherwise download",
            "    from url and decompress it, return the path.",
            "",
            "    Args:",
            "        url (str): download url",
            "        root_dir (str): root dir for downloading, it should be",
            "                        WEIGHTS_HOME or DATASET_HOME",
            "        md5sum (str): md5 sum of download package",
            "        decompress (bool): decompress zip or tar file. Default is `True`",
            "        method (str): which download method to use. Support `wget` and `get`. Default is `get`.",
            "",
            "    Returns:",
            "        str: a local path to save downloaded models & weights & datasets.",
            "    \"\"\"",
            "",
            "    from paddle.distributed import ParallelEnv",
            "",
            "    assert is_url(url), f\"downloading from {url} not a url\"",
            "    # parse path after download to decompress under root_dir",
            "    fullpath = _map_path(url, root_dir)",
            "    # Mainly used to solve the problem of downloading data from different",
            "    # machines in the case of multiple machines. Different ips will download",
            "    # data, and the same ip will only download data once.",
            "    unique_endpoints = _get_unique_endpoints(ParallelEnv().trainer_endpoints[:])",
            "    if osp.exists(fullpath) and check_exist and _md5check(fullpath, md5sum):",
            "        logger.info(f\"Found {fullpath}\")",
            "    else:",
            "        if ParallelEnv().current_endpoint in unique_endpoints:",
            "            fullpath = _download(url, root_dir, md5sum, method=method)",
            "        else:",
            "            while not os.path.exists(fullpath):",
            "                time.sleep(1)",
            "",
            "    if ParallelEnv().current_endpoint in unique_endpoints:",
            "        if decompress and (",
            "            tarfile.is_tarfile(fullpath) or zipfile.is_zipfile(fullpath)",
            "        ):",
            "            fullpath = _decompress(fullpath)",
            "",
            "    return fullpath",
            "",
            "",
            "def _get_download(url, fullname):",
            "    # using requests.get method",
            "    fname = osp.basename(fullname)",
            "    try:",
            "        with httpx.stream(",
            "            \"GET\", url, timeout=None, follow_redirects=True",
            "        ) as req:",
            "            if req.status_code != 200:",
            "                raise RuntimeError(",
            "                    f\"Downloading from {url} failed with code \"",
            "                    f\"{req.status_code}!\"",
            "                )",
            "",
            "            tmp_fullname = fullname + \"_tmp\"",
            "            total_size = req.headers.get('content-length')",
            "            with open(tmp_fullname, 'wb') as f:",
            "                if total_size:",
            "                    with tqdm(total=(int(total_size) + 1023) // 1024) as pbar:",
            "                        for chunk in req.iter_bytes(chunk_size=1024):",
            "                            f.write(chunk)",
            "                            pbar.update(1)",
            "                else:",
            "                    for chunk in req.iter_bytes(chunk_size=1024):",
            "                        if chunk:",
            "                            f.write(chunk)",
            "            shutil.move(tmp_fullname, fullname)",
            "            return fullname",
            "",
            "    except Exception as e:  # requests.exceptions.ConnectionError",
            "        logger.info(",
            "            f\"Downloading {fname} from {url} failed with exception {str(e)}\"",
            "        )",
            "        return False",
            "",
            "",
            "def _wget_download(url: str, fullname: str):",
            "    try:",
            "        assert urlparse(url).scheme in (",
            "            'http',",
            "            'https',",
            "        ), 'Only support https and http url'",
            "        # using wget to download url",
            "        tmp_fullname = shlex.quote(fullname + \"_tmp\")",
            "        url = shlex.quote(url)",
            "        # \u2013user-agent",
            "        command = f'wget -O {tmp_fullname} -t {DOWNLOAD_RETRY_LIMIT} {url}'",
            "        subprc = subprocess.Popen(",
            "            command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE",
            "        )",
            "        _ = subprc.communicate()",
            "",
            "        if subprc.returncode != 0:",
            "            raise RuntimeError(",
            "                f'{command} failed. Please make sure `wget` is installed or {url} exists'",
            "            )",
            "",
            "        shutil.move(tmp_fullname, fullname)",
            "",
            "    except Exception as e:  # requests.exceptions.ConnectionError",
            "        logger.info(f\"Downloading {url} failed with exception {str(e)}\")",
            "        return False",
            "",
            "    return fullname",
            "",
            "",
            "_download_methods = {",
            "    'get': _get_download,",
            "    'wget': _wget_download,",
            "}",
            "",
            "",
            "def _download(url, path, md5sum=None, method='get'):",
            "    \"\"\"",
            "    Download from url, save to path.",
            "",
            "    url (str): download url",
            "    path (str): download to given path",
            "    md5sum (str): md5 sum of download package",
            "    method (str): which download method to use. Support `wget` and `get`. Default is `get`.",
            "",
            "    \"\"\"",
            "    assert method in _download_methods, f'make sure `{method}` implemented'",
            "",
            "    if not osp.exists(path):",
            "        os.makedirs(path)",
            "",
            "    fname = osp.split(url)[-1]",
            "    fullname = osp.join(path, fname)",
            "    retry_cnt = 0",
            "",
            "    logger.info(f\"Downloading {fname} from {url}\")",
            "    while not (osp.exists(fullname) and _md5check(fullname, md5sum)):",
            "        logger.info(f\"md5check {fullname} and {md5sum}\")",
            "        if retry_cnt < DOWNLOAD_RETRY_LIMIT:",
            "            retry_cnt += 1",
            "        else:",
            "            raise RuntimeError(",
            "                f\"Download from {url} failed. \" \"Retry limit reached\"",
            "            )",
            "",
            "        if not _download_methods[method](url, fullname):",
            "            time.sleep(1)",
            "            continue",
            "",
            "    return fullname",
            "",
            "",
            "def _md5check(fullname, md5sum=None):",
            "    if md5sum is None:",
            "        return True",
            "",
            "    logger.info(f\"File {fullname} md5 checking...\")",
            "    md5 = hashlib.md5()",
            "    with open(fullname, 'rb') as f:",
            "        for chunk in iter(lambda: f.read(4096), b\"\"):",
            "            md5.update(chunk)",
            "    calc_md5sum = md5.hexdigest()",
            "",
            "    if calc_md5sum != md5sum:",
            "        logger.info(",
            "            f\"File {fullname} md5 check failed, {calc_md5sum}(calc) != \"",
            "            f\"{md5sum}(base)\"",
            "        )",
            "        return False",
            "    return True",
            "",
            "",
            "def _decompress(fname):",
            "    \"\"\"",
            "    Decompress for zip and tar file",
            "    \"\"\"",
            "    logger.info(f\"Decompressing {fname}...\")",
            "",
            "    # For protecting decompressing interrupted,",
            "    # decompress to fpath_tmp directory firstly, if decompress",
            "    # successed, move decompress files to fpath and delete",
            "    # fpath_tmp and remove download compress file.",
            "",
            "    if tarfile.is_tarfile(fname):",
            "        uncompressed_path = _uncompress_file_tar(fname)",
            "    elif zipfile.is_zipfile(fname):",
            "        uncompressed_path = _uncompress_file_zip(fname)",
            "    else:",
            "        raise TypeError(f\"Unsupport compress file type {fname}\")",
            "",
            "    return uncompressed_path",
            "",
            "",
            "def _uncompress_file_zip(filepath):",
            "    with zipfile.ZipFile(filepath, 'r') as files:",
            "        file_list_tmp = files.namelist()",
            "        file_list = []",
            "        for file in file_list_tmp:",
            "            file_list.append(file.replace(\"../\", \"\"))",
            "",
            "        file_dir = os.path.dirname(filepath)",
            "",
            "        if _is_a_single_file(file_list):",
            "            rootpath = file_list[0]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            files.extractall(file_dir)",
            "",
            "        elif _is_a_single_dir(file_list):",
            "            # `strip(os.sep)` to remove `os.sep` in the tail of path",
            "            rootpath = os.path.splitext(file_list[0].strip(os.sep))[0].split(",
            "                os.sep",
            "            )[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "",
            "            files.extractall(file_dir)",
            "        else:",
            "            rootpath = os.path.splitext(filepath)[0].split(os.sep)[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            if not os.path.exists(uncompressed_path):",
            "                os.makedirs(uncompressed_path)",
            "            files.extractall(os.path.join(file_dir, rootpath))",
            "",
            "        return uncompressed_path",
            "",
            "",
            "def _uncompress_file_tar(filepath, mode=\"r:*\"):",
            "    with tarfile.open(filepath, mode) as files:",
            "        file_list_tmp = files.getnames()",
            "        file_list = []",
            "        for file in file_list_tmp:",
            "            file_list.append(file.replace(\"../\", \"\"))",
            "",
            "        file_dir = os.path.dirname(filepath)",
            "",
            "        if _is_a_single_file(file_list):",
            "            rootpath = file_list[0]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            files.extractall(file_dir)",
            "        elif _is_a_single_dir(file_list):",
            "            rootpath = os.path.splitext(file_list[0].strip(os.sep))[0].split(",
            "                os.sep",
            "            )[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            files.extractall(file_dir)",
            "        else:",
            "            rootpath = os.path.splitext(filepath)[0].split(os.sep)[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            if not os.path.exists(uncompressed_path):",
            "                os.makedirs(uncompressed_path)",
            "",
            "            files.extractall(os.path.join(file_dir, rootpath))",
            "",
            "        return uncompressed_path",
            "",
            "",
            "def _is_a_single_file(file_list):",
            "    if len(file_list) == 1 and file_list[0].find(os.sep) < 0:",
            "        return True",
            "    return False",
            "",
            "",
            "def _is_a_single_dir(file_list):",
            "    new_file_list = []",
            "    for file_path in file_list:",
            "        if '/' in file_path:",
            "            file_path = file_path.replace('/', os.sep)",
            "        elif '\\\\' in file_path:",
            "            file_path = file_path.replace('\\\\', os.sep)",
            "        new_file_list.append(file_path)",
            "",
            "    file_name = new_file_list[0].split(os.sep)[0]",
            "    for i in range(1, len(new_file_list)):",
            "        if file_name != new_file_list[i].split(os.sep)[0]:",
            "            return False",
            "    return True"
        ],
        "afterPatchFile": [
            "#   Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import hashlib",
            "import os",
            "import os.path as osp",
            "import shutil",
            "import sys",
            "import tarfile",
            "import time",
            "import zipfile",
            "",
            "import httpx",
            "",
            "try:",
            "    from tqdm import tqdm",
            "except:",
            "",
            "    class tqdm:",
            "        def __init__(self, total=None):",
            "            self.total = total",
            "            self.n = 0",
            "",
            "        def update(self, n):",
            "            self.n += n",
            "            if self.total is None:",
            "                sys.stderr.write(f\"\\r{self.n:.1f} bytes\")",
            "            else:",
            "                sys.stderr.write(f\"\\r{100 * self.n / float(self.total):.1f}%\")",
            "            sys.stderr.flush()",
            "",
            "        def __enter__(self):",
            "            return self",
            "",
            "        def __exit__(self, exc_type, exc_val, exc_tb):",
            "            sys.stderr.write('\\n')",
            "",
            "",
            "import logging",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "__all__ = ['get_weights_path_from_url']",
            "",
            "WEIGHTS_HOME = osp.expanduser(\"~/.cache/paddle/hapi/weights\")",
            "",
            "DOWNLOAD_RETRY_LIMIT = 3",
            "",
            "",
            "def is_url(path):",
            "    \"\"\"",
            "    Whether path is URL.",
            "    Args:",
            "        path (string): URL string or not.",
            "    \"\"\"",
            "    return path.startswith('http://') or path.startswith('https://')",
            "",
            "",
            "def get_weights_path_from_url(url, md5sum=None):",
            "    \"\"\"Get weights path from WEIGHT_HOME, if not exists,",
            "    download it from url.",
            "",
            "    Args:",
            "        url (str): download url",
            "        md5sum (str): md5 sum of download package",
            "",
            "    Returns:",
            "        str: a local path to save downloaded weights.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            >>> from paddle.utils.download import get_weights_path_from_url",
            "",
            "            >>> resnet18_pretrained_weight_url = 'https://paddle-hapi.bj.bcebos.com/models/resnet18.pdparams'",
            "            >>> local_weight_path = get_weights_path_from_url(resnet18_pretrained_weight_url)",
            "",
            "    \"\"\"",
            "    path = get_path_from_url(url, WEIGHTS_HOME, md5sum)",
            "    return path",
            "",
            "",
            "def _map_path(url, root_dir):",
            "    # parse path after download under root_dir",
            "    fname = osp.split(url)[-1]",
            "    fpath = fname",
            "    return osp.join(root_dir, fpath)",
            "",
            "",
            "def _get_unique_endpoints(trainer_endpoints):",
            "    # Sorting is to avoid different environmental variables for each card",
            "    trainer_endpoints.sort()",
            "    ips = set()",
            "    unique_endpoints = set()",
            "    for endpoint in trainer_endpoints:",
            "        ip = endpoint.split(\":\")[0]",
            "        if ip in ips:",
            "            continue",
            "        ips.add(ip)",
            "        unique_endpoints.add(endpoint)",
            "    logger.info(f\"unique_endpoints {unique_endpoints}\")",
            "    return unique_endpoints",
            "",
            "",
            "def get_path_from_url(",
            "    url, root_dir, md5sum=None, check_exist=True, decompress=True, method='get'",
            "):",
            "    \"\"\"Download from given url to root_dir.",
            "    if file or directory specified by url is exists under",
            "    root_dir, return the path directly, otherwise download",
            "    from url and decompress it, return the path.",
            "",
            "    Args:",
            "        url (str): download url",
            "        root_dir (str): root dir for downloading, it should be",
            "                        WEIGHTS_HOME or DATASET_HOME",
            "        md5sum (str): md5 sum of download package",
            "        decompress (bool): decompress zip or tar file. Default is `True`",
            "        method (str): which download method to use. Support `wget` and `get`. Default is `get`.",
            "",
            "    Returns:",
            "        str: a local path to save downloaded models & weights & datasets.",
            "    \"\"\"",
            "",
            "    from paddle.distributed import ParallelEnv",
            "",
            "    assert is_url(url), f\"downloading from {url} not a url\"",
            "    # parse path after download to decompress under root_dir",
            "    fullpath = _map_path(url, root_dir)",
            "    # Mainly used to solve the problem of downloading data from different",
            "    # machines in the case of multiple machines. Different ips will download",
            "    # data, and the same ip will only download data once.",
            "    unique_endpoints = _get_unique_endpoints(ParallelEnv().trainer_endpoints[:])",
            "    if osp.exists(fullpath) and check_exist and _md5check(fullpath, md5sum):",
            "        logger.info(f\"Found {fullpath}\")",
            "    else:",
            "        if ParallelEnv().current_endpoint in unique_endpoints:",
            "            fullpath = _download(url, root_dir, md5sum, method=method)",
            "        else:",
            "            while not os.path.exists(fullpath):",
            "                time.sleep(1)",
            "",
            "    if ParallelEnv().current_endpoint in unique_endpoints:",
            "        if decompress and (",
            "            tarfile.is_tarfile(fullpath) or zipfile.is_zipfile(fullpath)",
            "        ):",
            "            fullpath = _decompress(fullpath)",
            "",
            "    return fullpath",
            "",
            "",
            "def _get_download(url, fullname):",
            "    # using requests.get method",
            "    fname = osp.basename(fullname)",
            "    try:",
            "        with httpx.stream(",
            "            \"GET\", url, timeout=None, follow_redirects=True",
            "        ) as req:",
            "            if req.status_code != 200:",
            "                raise RuntimeError(",
            "                    f\"Downloading from {url} failed with code \"",
            "                    f\"{req.status_code}!\"",
            "                )",
            "",
            "            tmp_fullname = fullname + \"_tmp\"",
            "            total_size = req.headers.get('content-length')",
            "            with open(tmp_fullname, 'wb') as f:",
            "                if total_size:",
            "                    with tqdm(total=(int(total_size) + 1023) // 1024) as pbar:",
            "                        for chunk in req.iter_bytes(chunk_size=1024):",
            "                            f.write(chunk)",
            "                            pbar.update(1)",
            "                else:",
            "                    for chunk in req.iter_bytes(chunk_size=1024):",
            "                        if chunk:",
            "                            f.write(chunk)",
            "            shutil.move(tmp_fullname, fullname)",
            "            return fullname",
            "",
            "    except Exception as e:  # requests.exceptions.ConnectionError",
            "        logger.info(",
            "            f\"Downloading {fname} from {url} failed with exception {str(e)}\"",
            "        )",
            "        return False",
            "",
            "",
            "_download_methods = {'get': _get_download}",
            "",
            "",
            "def _download(url, path, md5sum=None, method='get'):",
            "    \"\"\"",
            "    Download from url, save to path.",
            "",
            "    url (str): download url",
            "    path (str): download to given path",
            "    md5sum (str): md5 sum of download package",
            "    method (str): which download method to use. Support `wget` and `get`. Default is `get`.",
            "",
            "    \"\"\"",
            "    assert method in _download_methods, f'make sure `{method}` implemented'",
            "",
            "    if not osp.exists(path):",
            "        os.makedirs(path)",
            "",
            "    fname = osp.split(url)[-1]",
            "    fullname = osp.join(path, fname)",
            "    retry_cnt = 0",
            "",
            "    logger.info(f\"Downloading {fname} from {url}\")",
            "    while not (osp.exists(fullname) and _md5check(fullname, md5sum)):",
            "        logger.info(f\"md5check {fullname} and {md5sum}\")",
            "        if retry_cnt < DOWNLOAD_RETRY_LIMIT:",
            "            retry_cnt += 1",
            "        else:",
            "            raise RuntimeError(",
            "                f\"Download from {url} failed. \" \"Retry limit reached\"",
            "            )",
            "",
            "        if not _download_methods[method](url, fullname):",
            "            time.sleep(1)",
            "            continue",
            "",
            "    return fullname",
            "",
            "",
            "def _md5check(fullname, md5sum=None):",
            "    if md5sum is None:",
            "        return True",
            "",
            "    logger.info(f\"File {fullname} md5 checking...\")",
            "    md5 = hashlib.md5()",
            "    with open(fullname, 'rb') as f:",
            "        for chunk in iter(lambda: f.read(4096), b\"\"):",
            "            md5.update(chunk)",
            "    calc_md5sum = md5.hexdigest()",
            "",
            "    if calc_md5sum != md5sum:",
            "        logger.info(",
            "            f\"File {fullname} md5 check failed, {calc_md5sum}(calc) != \"",
            "            f\"{md5sum}(base)\"",
            "        )",
            "        return False",
            "    return True",
            "",
            "",
            "def _decompress(fname):",
            "    \"\"\"",
            "    Decompress for zip and tar file",
            "    \"\"\"",
            "    logger.info(f\"Decompressing {fname}...\")",
            "",
            "    # For protecting decompressing interrupted,",
            "    # decompress to fpath_tmp directory firstly, if decompress",
            "    # successed, move decompress files to fpath and delete",
            "    # fpath_tmp and remove download compress file.",
            "",
            "    if tarfile.is_tarfile(fname):",
            "        uncompressed_path = _uncompress_file_tar(fname)",
            "    elif zipfile.is_zipfile(fname):",
            "        uncompressed_path = _uncompress_file_zip(fname)",
            "    else:",
            "        raise TypeError(f\"Unsupport compress file type {fname}\")",
            "",
            "    return uncompressed_path",
            "",
            "",
            "def _uncompress_file_zip(filepath):",
            "    with zipfile.ZipFile(filepath, 'r') as files:",
            "        file_list_tmp = files.namelist()",
            "        file_list = []",
            "        for file in file_list_tmp:",
            "            file_list.append(file.replace(\"../\", \"\"))",
            "",
            "        file_dir = os.path.dirname(filepath)",
            "",
            "        if _is_a_single_file(file_list):",
            "            rootpath = file_list[0]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            files.extractall(file_dir)",
            "",
            "        elif _is_a_single_dir(file_list):",
            "            # `strip(os.sep)` to remove `os.sep` in the tail of path",
            "            rootpath = os.path.splitext(file_list[0].strip(os.sep))[0].split(",
            "                os.sep",
            "            )[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "",
            "            files.extractall(file_dir)",
            "        else:",
            "            rootpath = os.path.splitext(filepath)[0].split(os.sep)[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            if not os.path.exists(uncompressed_path):",
            "                os.makedirs(uncompressed_path)",
            "            files.extractall(os.path.join(file_dir, rootpath))",
            "",
            "        return uncompressed_path",
            "",
            "",
            "def _uncompress_file_tar(filepath, mode=\"r:*\"):",
            "    with tarfile.open(filepath, mode) as files:",
            "        file_list_tmp = files.getnames()",
            "        file_list = []",
            "        for file in file_list_tmp:",
            "            file_list.append(file.replace(\"../\", \"\"))",
            "",
            "        file_dir = os.path.dirname(filepath)",
            "",
            "        if _is_a_single_file(file_list):",
            "            rootpath = file_list[0]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            files.extractall(file_dir)",
            "        elif _is_a_single_dir(file_list):",
            "            rootpath = os.path.splitext(file_list[0].strip(os.sep))[0].split(",
            "                os.sep",
            "            )[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            files.extractall(file_dir)",
            "        else:",
            "            rootpath = os.path.splitext(filepath)[0].split(os.sep)[-1]",
            "            uncompressed_path = os.path.join(file_dir, rootpath)",
            "            if not os.path.exists(uncompressed_path):",
            "                os.makedirs(uncompressed_path)",
            "",
            "            files.extractall(os.path.join(file_dir, rootpath))",
            "",
            "        return uncompressed_path",
            "",
            "",
            "def _is_a_single_file(file_list):",
            "    if len(file_list) == 1 and file_list[0].find(os.sep) < 0:",
            "        return True",
            "    return False",
            "",
            "",
            "def _is_a_single_dir(file_list):",
            "    new_file_list = []",
            "    for file_path in file_list:",
            "        if '/' in file_path:",
            "            file_path = file_path.replace('/', os.sep)",
            "        elif '\\\\' in file_path:",
            "            file_path = file_path.replace('\\\\', os.sep)",
            "        new_file_list.append(file_path)",
            "",
            "    file_name = new_file_list[0].split(os.sep)[0]",
            "    for i in range(1, len(new_file_list)):",
            "        if file_name != new_file_list[i].split(os.sep)[0]:",
            "            return False",
            "    return True"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "18": [],
            "20": [],
            "25": [],
            "201": [
                "_wget_download"
            ],
            "202": [
                "_wget_download"
            ],
            "203": [
                "_wget_download"
            ],
            "204": [
                "_wget_download"
            ],
            "205": [
                "_wget_download"
            ],
            "206": [
                "_wget_download"
            ],
            "207": [
                "_wget_download"
            ],
            "208": [
                "_wget_download"
            ],
            "209": [
                "_wget_download"
            ],
            "210": [
                "_wget_download"
            ],
            "211": [
                "_wget_download"
            ],
            "212": [
                "_wget_download"
            ],
            "213": [
                "_wget_download"
            ],
            "214": [
                "_wget_download"
            ],
            "215": [
                "_wget_download"
            ],
            "216": [
                "_wget_download"
            ],
            "217": [
                "_wget_download"
            ],
            "218": [
                "_wget_download"
            ],
            "219": [
                "_wget_download"
            ],
            "220": [
                "_wget_download"
            ],
            "221": [
                "_wget_download"
            ],
            "222": [
                "_wget_download"
            ],
            "223": [
                "_wget_download"
            ],
            "224": [
                "_wget_download"
            ],
            "225": [
                "_wget_download"
            ],
            "226": [
                "_wget_download"
            ],
            "227": [
                "_wget_download"
            ],
            "228": [
                "_wget_download"
            ],
            "229": [],
            "230": [],
            "231": [
                "_download_methods"
            ],
            "232": [],
            "233": [],
            "234": []
        },
        "addLocation": []
    }
}