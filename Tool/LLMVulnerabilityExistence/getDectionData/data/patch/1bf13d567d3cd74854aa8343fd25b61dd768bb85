{
    "sklearn/svm/_base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 616,
                "afterPatchRowNumber": 616,
                "PatchRowcode": "                     \"the number of samples at training time\""
            },
            "1": {
                "beforePatchRowNumber": 617,
                "afterPatchRowNumber": 617,
                "PatchRowcode": "                     % (X.shape[1], self.shape_fit_[0])"
            },
            "2": {
                "beforePatchRowNumber": 618,
                "afterPatchRowNumber": 618,
                "PatchRowcode": "                 )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 619,
                "PatchRowcode": "+        # Fixes https://nvd.nist.gov/vuln/detail/CVE-2020-28975"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 620,
                "PatchRowcode": "+        # Check that _n_support is consistent with support_vectors"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 621,
                "PatchRowcode": "+        sv = self.support_vectors_"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 622,
                "PatchRowcode": "+        if not self._sparse and sv.size > 0 and self.n_support_.sum() != sv.shape[0]:"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 623,
                "PatchRowcode": "+            raise ValueError("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 624,
                "PatchRowcode": "+                f\"The internal representation of {self.__class__.__name__} was altered\""
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 625,
                "PatchRowcode": "+            )"
            },
            "10": {
                "beforePatchRowNumber": 619,
                "afterPatchRowNumber": 626,
                "PatchRowcode": "         return X"
            },
            "11": {
                "beforePatchRowNumber": 620,
                "afterPatchRowNumber": 627,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 621,
                "afterPatchRowNumber": 628,
                "PatchRowcode": "     @property"
            }
        },
        "frontPatchFile": [
            "import numpy as np",
            "import scipy.sparse as sp",
            "import warnings",
            "from abc import ABCMeta, abstractmethod",
            "",
            "# mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm'",
            "# (and same for other imports)",
            "from . import _libsvm as libsvm  # type: ignore",
            "from . import _liblinear as liblinear  # type: ignore",
            "from . import _libsvm_sparse as libsvm_sparse  # type: ignore",
            "from ..base import BaseEstimator, ClassifierMixin",
            "from ..preprocessing import LabelEncoder",
            "from ..utils.multiclass import _ovr_decision_function",
            "from ..utils import check_array, check_random_state",
            "from ..utils import column_or_1d",
            "from ..utils import compute_class_weight",
            "from ..utils.metaestimators import available_if",
            "from ..utils.deprecation import deprecated",
            "from ..utils.extmath import safe_sparse_dot",
            "from ..utils.validation import check_is_fitted, _check_large_sparse",
            "from ..utils.validation import _num_samples",
            "from ..utils.validation import _check_sample_weight, check_consistent_length",
            "from ..utils.multiclass import check_classification_targets",
            "from ..exceptions import ConvergenceWarning",
            "from ..exceptions import NotFittedError",
            "",
            "",
            "LIBSVM_IMPL = [\"c_svc\", \"nu_svc\", \"one_class\", \"epsilon_svr\", \"nu_svr\"]",
            "",
            "",
            "def _one_vs_one_coef(dual_coef, n_support, support_vectors):",
            "    \"\"\"Generate primal coefficients from dual coefficients",
            "    for the one-vs-one multi class LibSVM in the case",
            "    of a linear kernel.\"\"\"",
            "",
            "    # get 1vs1 weights for all n*(n-1) classifiers.",
            "    # this is somewhat messy.",
            "    # shape of dual_coef_ is nSV * (n_classes -1)",
            "    # see docs for details",
            "    n_class = dual_coef.shape[0] + 1",
            "",
            "    # XXX we could do preallocation of coef but",
            "    # would have to take care in the sparse case",
            "    coef = []",
            "    sv_locs = np.cumsum(np.hstack([[0], n_support]))",
            "    for class1 in range(n_class):",
            "        # SVs for class1:",
            "        sv1 = support_vectors[sv_locs[class1] : sv_locs[class1 + 1], :]",
            "        for class2 in range(class1 + 1, n_class):",
            "            # SVs for class1:",
            "            sv2 = support_vectors[sv_locs[class2] : sv_locs[class2 + 1], :]",
            "",
            "            # dual coef for class1 SVs:",
            "            alpha1 = dual_coef[class2 - 1, sv_locs[class1] : sv_locs[class1 + 1]]",
            "            # dual coef for class2 SVs:",
            "            alpha2 = dual_coef[class1, sv_locs[class2] : sv_locs[class2 + 1]]",
            "            # build weight for class1 vs class2",
            "",
            "            coef.append(safe_sparse_dot(alpha1, sv1) + safe_sparse_dot(alpha2, sv2))",
            "    return coef",
            "",
            "",
            "class BaseLibSVM(BaseEstimator, metaclass=ABCMeta):",
            "    \"\"\"Base class for estimators that use libsvm as backing library.",
            "",
            "    This implements support vector machine classification and regression.",
            "",
            "    Parameter documentation is in the derived `SVC` class.",
            "    \"\"\"",
            "",
            "    # The order of these must match the integer values in LibSVM.",
            "    # XXX These are actually the same in the dense case. Need to factor",
            "    # this out.",
            "    _sparse_kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]",
            "",
            "    @abstractmethod",
            "    def __init__(",
            "        self,",
            "        kernel,",
            "        degree,",
            "        gamma,",
            "        coef0,",
            "        tol,",
            "        C,",
            "        nu,",
            "        epsilon,",
            "        shrinking,",
            "        probability,",
            "        cache_size,",
            "        class_weight,",
            "        verbose,",
            "        max_iter,",
            "        random_state,",
            "    ):",
            "",
            "        if self._impl not in LIBSVM_IMPL:",
            "            raise ValueError(",
            "                \"impl should be one of %s, %s was given\" % (LIBSVM_IMPL, self._impl)",
            "            )",
            "",
            "        if gamma == 0:",
            "            msg = (",
            "                \"The gamma value of 0.0 is invalid. Use 'auto' to set\"",
            "                \" gamma to a value of 1 / n_features.\"",
            "            )",
            "            raise ValueError(msg)",
            "",
            "        self.kernel = kernel",
            "        self.degree = degree",
            "        self.gamma = gamma",
            "        self.coef0 = coef0",
            "        self.tol = tol",
            "        self.C = C",
            "        self.nu = nu",
            "        self.epsilon = epsilon",
            "        self.shrinking = shrinking",
            "        self.probability = probability",
            "        self.cache_size = cache_size",
            "        self.class_weight = class_weight",
            "        self.verbose = verbose",
            "        self.max_iter = max_iter",
            "        self.random_state = random_state",
            "",
            "    def _more_tags(self):",
            "        # Used by cross_val_score.",
            "        return {\"pairwise\": self.kernel == \"precomputed\"}",
            "",
            "    # TODO: Remove in 1.1",
            "    # mypy error: Decorated property not supported",
            "    @deprecated(  # type: ignore",
            "        \"Attribute `_pairwise` was deprecated in \"",
            "        \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\"",
            "    )",
            "    @property",
            "    def _pairwise(self):",
            "        # Used by cross_val_score.",
            "        return self.kernel == \"precomputed\"",
            "",
            "    def fit(self, X, y, sample_weight=None):",
            "        \"\"\"Fit the SVM model according to the given training data.",
            "",
            "        Parameters",
            "        ----------",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features) \\",
            "                or (n_samples, n_samples)",
            "            Training vectors, where `n_samples` is the number of samples",
            "            and `n_features` is the number of features.",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples, n_samples).",
            "",
            "        y : array-like of shape (n_samples,)",
            "            Target values (class labels in classification, real numbers in",
            "            regression).",
            "",
            "        sample_weight : array-like of shape (n_samples,), default=None",
            "            Per-sample weights. Rescale C per sample. Higher weights",
            "            force the classifier to put more emphasis on these points.",
            "",
            "        Returns",
            "        -------",
            "        self : object",
            "            Fitted estimator.",
            "",
            "        Notes",
            "        -----",
            "        If X and y are not C-ordered and contiguous arrays of np.float64 and",
            "        X is not a scipy.sparse.csr_matrix, X and/or y may be copied.",
            "",
            "        If X is a dense array, then the other methods will not support sparse",
            "        matrices as input.",
            "        \"\"\"",
            "",
            "        rnd = check_random_state(self.random_state)",
            "",
            "        sparse = sp.isspmatrix(X)",
            "        if sparse and self.kernel == \"precomputed\":",
            "            raise TypeError(\"Sparse precomputed kernels are not supported.\")",
            "        self._sparse = sparse and not callable(self.kernel)",
            "",
            "        if hasattr(self, \"decision_function_shape\"):",
            "            if self.decision_function_shape not in (\"ovr\", \"ovo\"):",
            "                raise ValueError(",
            "                    \"decision_function_shape must be either 'ovr' or 'ovo', \"",
            "                    f\"got {self.decision_function_shape}.\"",
            "                )",
            "",
            "        if callable(self.kernel):",
            "            check_consistent_length(X, y)",
            "        else:",
            "            X, y = self._validate_data(",
            "                X,",
            "                y,",
            "                dtype=np.float64,",
            "                order=\"C\",",
            "                accept_sparse=\"csr\",",
            "                accept_large_sparse=False,",
            "            )",
            "",
            "        y = self._validate_targets(y)",
            "",
            "        sample_weight = np.asarray(",
            "            [] if sample_weight is None else sample_weight, dtype=np.float64",
            "        )",
            "        solver_type = LIBSVM_IMPL.index(self._impl)",
            "",
            "        # input validation",
            "        n_samples = _num_samples(X)",
            "        if solver_type != 2 and n_samples != y.shape[0]:",
            "            raise ValueError(",
            "                \"X and y have incompatible shapes.\\n\"",
            "                + \"X has %s samples, but y has %s.\" % (n_samples, y.shape[0])",
            "            )",
            "",
            "        if self.kernel == \"precomputed\" and n_samples != X.shape[1]:",
            "            raise ValueError(",
            "                \"Precomputed matrix must be a square matrix.\"",
            "                \" Input is a {}x{} matrix.\".format(X.shape[0], X.shape[1])",
            "            )",
            "",
            "        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != n_samples:",
            "            raise ValueError(",
            "                \"sample_weight and X have incompatible shapes: \"",
            "                \"%r vs %r\\n\"",
            "                \"Note: Sparse matrices cannot be indexed w/\"",
            "                \"boolean masks (use `indices=True` in CV).\"",
            "                % (sample_weight.shape, X.shape)",
            "            )",
            "",
            "        kernel = \"precomputed\" if callable(self.kernel) else self.kernel",
            "",
            "        if kernel == \"precomputed\":",
            "            # unused but needs to be a float for cython code that ignores",
            "            # it anyway",
            "            self._gamma = 0.0",
            "        elif isinstance(self.gamma, str):",
            "            if self.gamma == \"scale\":",
            "                # var = E[X^2] - E[X]^2 if sparse",
            "                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2 if sparse else X.var()",
            "                self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0",
            "            elif self.gamma == \"auto\":",
            "                self._gamma = 1.0 / X.shape[1]",
            "            else:",
            "                raise ValueError(",
            "                    \"When 'gamma' is a string, it should be either 'scale' or \"",
            "                    \"'auto'. Got '{}' instead.\".format(self.gamma)",
            "                )",
            "        else:",
            "            self._gamma = self.gamma",
            "",
            "        fit = self._sparse_fit if self._sparse else self._dense_fit",
            "        if self.verbose:",
            "            print(\"[LibSVM]\", end=\"\")",
            "",
            "        seed = rnd.randint(np.iinfo(\"i\").max)",
            "        fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)",
            "        # see comment on the other call to np.iinfo in this file",
            "",
            "        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples,)",
            "",
            "        # In binary case, we need to flip the sign of coef, intercept and",
            "        # decision function. Use self._intercept_ and self._dual_coef_",
            "        # internally.",
            "        self._intercept_ = self.intercept_.copy()",
            "        self._dual_coef_ = self.dual_coef_",
            "        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:",
            "            self.intercept_ *= -1",
            "            self.dual_coef_ = -self.dual_coef_",
            "",
            "        return self",
            "",
            "    def _validate_targets(self, y):",
            "        \"\"\"Validation of y and class_weight.",
            "",
            "        Default implementation for SVR and one-class; overridden in BaseSVC.",
            "        \"\"\"",
            "        # XXX this is ugly.",
            "        # Regression models should not have a class_weight_ attribute.",
            "        self.class_weight_ = np.empty(0)",
            "        return column_or_1d(y, warn=True).astype(np.float64, copy=False)",
            "",
            "    def _warn_from_fit_status(self):",
            "        assert self.fit_status_ in (0, 1)",
            "        if self.fit_status_ == 1:",
            "            warnings.warn(",
            "                \"Solver terminated early (max_iter=%i).\"",
            "                \"  Consider pre-processing your data with\"",
            "                \" StandardScaler or MinMaxScaler.\"",
            "                % self.max_iter,",
            "                ConvergenceWarning,",
            "            )",
            "",
            "    def _dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):",
            "        if callable(self.kernel):",
            "            # you must store a reference to X to compute the kernel in predict",
            "            # TODO: add keyword copy to copy on demand",
            "            self.__Xfit = X",
            "            X = self._compute_kernel(X)",
            "",
            "            if X.shape[0] != X.shape[1]:",
            "                raise ValueError(\"X.shape[0] should be equal to X.shape[1]\")",
            "",
            "        libsvm.set_verbosity_wrap(self.verbose)",
            "",
            "        # we don't pass **self.get_params() to allow subclasses to",
            "        # add other parameters to __init__",
            "        (",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self.dual_coef_,",
            "            self.intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            self.fit_status_,",
            "        ) = libsvm.fit(",
            "            X,",
            "            y,",
            "            svm_type=solver_type,",
            "            sample_weight=sample_weight,",
            "            class_weight=self.class_weight_,",
            "            kernel=kernel,",
            "            C=self.C,",
            "            nu=self.nu,",
            "            probability=self.probability,",
            "            degree=self.degree,",
            "            shrinking=self.shrinking,",
            "            tol=self.tol,",
            "            cache_size=self.cache_size,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "            epsilon=self.epsilon,",
            "            max_iter=self.max_iter,",
            "            random_seed=random_seed,",
            "        )",
            "",
            "        self._warn_from_fit_status()",
            "",
            "    def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):",
            "        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")",
            "        X.sort_indices()",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        libsvm_sparse.set_verbosity_wrap(self.verbose)",
            "",
            "        (",
            "            self.support_,",
            "            self.support_vectors_,",
            "            dual_coef_data,",
            "            self.intercept_,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "            self.fit_status_,",
            "        ) = libsvm_sparse.libsvm_sparse_train(",
            "            X.shape[1],",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            y,",
            "            solver_type,",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            self.C,",
            "            self.class_weight_,",
            "            sample_weight,",
            "            self.nu,",
            "            self.cache_size,",
            "            self.epsilon,",
            "            int(self.shrinking),",
            "            int(self.probability),",
            "            self.max_iter,",
            "            random_seed,",
            "        )",
            "",
            "        self._warn_from_fit_status()",
            "",
            "        if hasattr(self, \"classes_\"):",
            "            n_class = len(self.classes_) - 1",
            "        else:  # regression",
            "            n_class = 1",
            "        n_SV = self.support_vectors_.shape[0]",
            "",
            "        dual_coef_indices = np.tile(np.arange(n_SV), n_class)",
            "        if not n_SV:",
            "            self.dual_coef_ = sp.csr_matrix([])",
            "        else:",
            "            dual_coef_indptr = np.arange(",
            "                0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class",
            "            )",
            "            self.dual_coef_ = sp.csr_matrix(",
            "                (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)",
            "            )",
            "",
            "    def predict(self, X):",
            "        \"\"\"Perform regression on samples in X.",
            "",
            "        For an one-class model, +1 (inlier) or -1 (outlier) is returned.",
            "",
            "        Parameters",
            "        ----------",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        y_pred : ndarray of shape (n_samples,)",
            "            The predicted values.",
            "        \"\"\"",
            "        X = self._validate_for_predict(X)",
            "        predict = self._sparse_predict if self._sparse else self._dense_predict",
            "        return predict(X)",
            "",
            "    def _dense_predict(self, X):",
            "        X = self._compute_kernel(X)",
            "        if X.ndim == 1:",
            "            X = check_array(X, order=\"C\", accept_large_sparse=False)",
            "",
            "        kernel = self.kernel",
            "        if callable(self.kernel):",
            "            kernel = \"precomputed\"",
            "            if X.shape[1] != self.shape_fit_[0]:",
            "                raise ValueError(",
            "                    \"X.shape[1] = %d should be equal to %d, \"",
            "                    \"the number of samples at training time\"",
            "                    % (X.shape[1], self.shape_fit_[0])",
            "                )",
            "",
            "        svm_type = LIBSVM_IMPL.index(self._impl)",
            "",
            "        return libsvm.predict(",
            "            X,",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self._dual_coef_,",
            "            self._intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            svm_type=svm_type,",
            "            kernel=kernel,",
            "            degree=self.degree,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "            cache_size=self.cache_size,",
            "        )",
            "",
            "    def _sparse_predict(self, X):",
            "        # Precondition: X is a csr_matrix of dtype np.float64.",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        C = 0.0  # C is not useful here",
            "",
            "        return libsvm_sparse.libsvm_sparse_predict(",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            self.support_vectors_.data,",
            "            self.support_vectors_.indices,",
            "            self.support_vectors_.indptr,",
            "            self._dual_coef_.data,",
            "            self._intercept_,",
            "            LIBSVM_IMPL.index(self._impl),",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            C,",
            "            self.class_weight_,",
            "            self.nu,",
            "            self.epsilon,",
            "            self.shrinking,",
            "            self.probability,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "        )",
            "",
            "    def _compute_kernel(self, X):",
            "        \"\"\"Return the data transformed by a callable kernel\"\"\"",
            "        if callable(self.kernel):",
            "            # in the case of precomputed kernel given as a function, we",
            "            # have to compute explicitly the kernel matrix",
            "            kernel = self.kernel(X, self.__Xfit)",
            "            if sp.issparse(kernel):",
            "                kernel = kernel.toarray()",
            "            X = np.asarray(kernel, dtype=np.float64, order=\"C\")",
            "        return X",
            "",
            "    def _decision_function(self, X):",
            "        \"\"\"Evaluates the decision function for the samples in X.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features)",
            "",
            "        Returns",
            "        -------",
            "        X : array-like of shape (n_samples, n_class * (n_class-1) / 2)",
            "            Returns the decision function of the sample for each class",
            "            in the model.",
            "        \"\"\"",
            "        # NOTE: _validate_for_predict contains check for is_fitted",
            "        # hence must be placed before any other attributes are used.",
            "        X = self._validate_for_predict(X)",
            "        X = self._compute_kernel(X)",
            "",
            "        if self._sparse:",
            "            dec_func = self._sparse_decision_function(X)",
            "        else:",
            "            dec_func = self._dense_decision_function(X)",
            "",
            "        # In binary case, we need to flip the sign of coef, intercept and",
            "        # decision function.",
            "        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:",
            "            return -dec_func.ravel()",
            "",
            "        return dec_func",
            "",
            "    def _dense_decision_function(self, X):",
            "        X = check_array(X, dtype=np.float64, order=\"C\", accept_large_sparse=False)",
            "",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        return libsvm.decision_function(",
            "            X,",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self._dual_coef_,",
            "            self._intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            svm_type=LIBSVM_IMPL.index(self._impl),",
            "            kernel=kernel,",
            "            degree=self.degree,",
            "            cache_size=self.cache_size,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "        )",
            "",
            "    def _sparse_decision_function(self, X):",
            "        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")",
            "",
            "        kernel = self.kernel",
            "        if hasattr(kernel, \"__call__\"):",
            "            kernel = \"precomputed\"",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        return libsvm_sparse.libsvm_sparse_decision_function(",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            self.support_vectors_.data,",
            "            self.support_vectors_.indices,",
            "            self.support_vectors_.indptr,",
            "            self._dual_coef_.data,",
            "            self._intercept_,",
            "            LIBSVM_IMPL.index(self._impl),",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            self.C,",
            "            self.class_weight_,",
            "            self.nu,",
            "            self.epsilon,",
            "            self.shrinking,",
            "            self.probability,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "        )",
            "",
            "    def _validate_for_predict(self, X):",
            "        check_is_fitted(self)",
            "",
            "        if not callable(self.kernel):",
            "            X = self._validate_data(",
            "                X,",
            "                accept_sparse=\"csr\",",
            "                dtype=np.float64,",
            "                order=\"C\",",
            "                accept_large_sparse=False,",
            "                reset=False,",
            "            )",
            "",
            "        if self._sparse and not sp.isspmatrix(X):",
            "            X = sp.csr_matrix(X)",
            "        if self._sparse:",
            "            X.sort_indices()",
            "",
            "        if sp.issparse(X) and not self._sparse and not callable(self.kernel):",
            "            raise ValueError(",
            "                \"cannot use sparse input in %r trained on dense data\"",
            "                % type(self).__name__",
            "            )",
            "",
            "        if self.kernel == \"precomputed\":",
            "            if X.shape[1] != self.shape_fit_[0]:",
            "                raise ValueError(",
            "                    \"X.shape[1] = %d should be equal to %d, \"",
            "                    \"the number of samples at training time\"",
            "                    % (X.shape[1], self.shape_fit_[0])",
            "                )",
            "        return X",
            "",
            "    @property",
            "    def coef_(self):",
            "        \"\"\"Weights assigned to the features when `kernel=\"linear\"`.",
            "",
            "        Returns",
            "        -------",
            "        ndarray of shape (n_features, n_classes)",
            "        \"\"\"",
            "        if self.kernel != \"linear\":",
            "            raise AttributeError(\"coef_ is only available when using a linear kernel\")",
            "",
            "        coef = self._get_coef()",
            "",
            "        # coef_ being a read-only property, it's better to mark the value as",
            "        # immutable to avoid hiding potential bugs for the unsuspecting user.",
            "        if sp.issparse(coef):",
            "            # sparse matrix do not have global flags",
            "            coef.data.flags.writeable = False",
            "        else:",
            "            # regular dense array",
            "            coef.flags.writeable = False",
            "        return coef",
            "",
            "    def _get_coef(self):",
            "        return safe_sparse_dot(self._dual_coef_, self.support_vectors_)",
            "",
            "    @property",
            "    def n_support_(self):",
            "        \"\"\"Number of support vectors for each class.\"\"\"",
            "        try:",
            "            check_is_fitted(self)",
            "        except NotFittedError:",
            "            raise AttributeError",
            "",
            "        svm_type = LIBSVM_IMPL.index(self._impl)",
            "        if svm_type in (0, 1):",
            "            return self._n_support",
            "        else:",
            "            # SVR and OneClass",
            "            # _n_support has size 2, we make it size 1",
            "            return np.array([self._n_support[0]])",
            "",
            "",
            "class BaseSVC(ClassifierMixin, BaseLibSVM, metaclass=ABCMeta):",
            "    \"\"\"ABC for LibSVM-based classifiers.\"\"\"",
            "",
            "    @abstractmethod",
            "    def __init__(",
            "        self,",
            "        kernel,",
            "        degree,",
            "        gamma,",
            "        coef0,",
            "        tol,",
            "        C,",
            "        nu,",
            "        shrinking,",
            "        probability,",
            "        cache_size,",
            "        class_weight,",
            "        verbose,",
            "        max_iter,",
            "        decision_function_shape,",
            "        random_state,",
            "        break_ties,",
            "    ):",
            "        self.decision_function_shape = decision_function_shape",
            "        self.break_ties = break_ties",
            "        super().__init__(",
            "            kernel=kernel,",
            "            degree=degree,",
            "            gamma=gamma,",
            "            coef0=coef0,",
            "            tol=tol,",
            "            C=C,",
            "            nu=nu,",
            "            epsilon=0.0,",
            "            shrinking=shrinking,",
            "            probability=probability,",
            "            cache_size=cache_size,",
            "            class_weight=class_weight,",
            "            verbose=verbose,",
            "            max_iter=max_iter,",
            "            random_state=random_state,",
            "        )",
            "",
            "    def _validate_targets(self, y):",
            "        y_ = column_or_1d(y, warn=True)",
            "        check_classification_targets(y)",
            "        cls, y = np.unique(y_, return_inverse=True)",
            "        self.class_weight_ = compute_class_weight(self.class_weight, classes=cls, y=y_)",
            "        if len(cls) < 2:",
            "            raise ValueError(",
            "                \"The number of classes has to be greater than one; got %d class\"",
            "                % len(cls)",
            "            )",
            "",
            "        self.classes_ = cls",
            "",
            "        return np.asarray(y, dtype=np.float64, order=\"C\")",
            "",
            "    def decision_function(self, X):",
            "        \"\"\"Evaluate the decision function for the samples in X.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features)",
            "            The input samples.",
            "",
            "        Returns",
            "        -------",
            "        X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)",
            "            Returns the decision function of the sample for each class",
            "            in the model.",
            "            If decision_function_shape='ovr', the shape is (n_samples,",
            "            n_classes).",
            "",
            "        Notes",
            "        -----",
            "        If decision_function_shape='ovo', the function values are proportional",
            "        to the distance of the samples X to the separating hyperplane. If the",
            "        exact distances are required, divide the function values by the norm of",
            "        the weight vector (``coef_``). See also `this question",
            "        <https://stats.stackexchange.com/questions/14876/",
            "        interpreting-distance-from-hyperplane-in-svm>`_ for further details.",
            "        If decision_function_shape='ovr', the decision function is a monotonic",
            "        transformation of ovo decision function.",
            "        \"\"\"",
            "        dec = self._decision_function(X)",
            "        if self.decision_function_shape == \"ovr\" and len(self.classes_) > 2:",
            "            return _ovr_decision_function(dec < 0, -dec, len(self.classes_))",
            "        return dec",
            "",
            "    def predict(self, X):",
            "        \"\"\"Perform classification on samples in X.",
            "",
            "        For an one-class model, +1 or -1 is returned.",
            "",
            "        Parameters",
            "        ----------",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\",
            "                (n_samples_test, n_samples_train)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        y_pred : ndarray of shape (n_samples,)",
            "            Class labels for samples in X.",
            "        \"\"\"",
            "        check_is_fitted(self)",
            "        if self.break_ties and self.decision_function_shape == \"ovo\":",
            "            raise ValueError(",
            "                \"break_ties must be False when decision_function_shape is 'ovo'\"",
            "            )",
            "",
            "        if (",
            "            self.break_ties",
            "            and self.decision_function_shape == \"ovr\"",
            "            and len(self.classes_) > 2",
            "        ):",
            "            y = np.argmax(self.decision_function(X), axis=1)",
            "        else:",
            "            y = super().predict(X)",
            "        return self.classes_.take(np.asarray(y, dtype=np.intp))",
            "",
            "    # Hacky way of getting predict_proba to raise an AttributeError when",
            "    # probability=False using properties. Do not use this in new code; when",
            "    # probabilities are not available depending on a setting, introduce two",
            "    # estimators.",
            "    def _check_proba(self):",
            "        if not self.probability:",
            "            raise AttributeError(",
            "                \"predict_proba is not available when  probability=False\"",
            "            )",
            "        if self._impl not in (\"c_svc\", \"nu_svc\"):",
            "            raise AttributeError(\"predict_proba only implemented for SVC and NuSVC\")",
            "        return True",
            "",
            "    @available_if(_check_proba)",
            "    def predict_proba(self, X):",
            "        \"\"\"Compute probabilities of possible outcomes for samples in X.",
            "",
            "        The model need to have probability information computed at training",
            "        time: fit with attribute `probability` set to True.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        T : ndarray of shape (n_samples, n_classes)",
            "            Returns the probability of the sample for each class in",
            "            the model. The columns correspond to the classes in sorted",
            "            order, as they appear in the attribute :term:`classes_`.",
            "",
            "        Notes",
            "        -----",
            "        The probability model is created using cross validation, so",
            "        the results can be slightly different than those obtained by",
            "        predict. Also, it will produce meaningless results on very small",
            "        datasets.",
            "        \"\"\"",
            "        X = self._validate_for_predict(X)",
            "        if self.probA_.size == 0 or self.probB_.size == 0:",
            "            raise NotFittedError(",
            "                \"predict_proba is not available when fitted with probability=False\"",
            "            )",
            "        pred_proba = (",
            "            self._sparse_predict_proba if self._sparse else self._dense_predict_proba",
            "        )",
            "        return pred_proba(X)",
            "",
            "    @available_if(_check_proba)",
            "    def predict_log_proba(self, X):",
            "        \"\"\"Compute log probabilities of possible outcomes for samples in X.",
            "",
            "        The model need to have probability information computed at training",
            "        time: fit with attribute `probability` set to True.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features) or \\",
            "                (n_samples_test, n_samples_train)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        T : ndarray of shape (n_samples, n_classes)",
            "            Returns the log-probabilities of the sample for each class in",
            "            the model. The columns correspond to the classes in sorted",
            "            order, as they appear in the attribute :term:`classes_`.",
            "",
            "        Notes",
            "        -----",
            "        The probability model is created using cross validation, so",
            "        the results can be slightly different than those obtained by",
            "        predict. Also, it will produce meaningless results on very small",
            "        datasets.",
            "        \"\"\"",
            "        return np.log(self.predict_proba(X))",
            "",
            "    def _dense_predict_proba(self, X):",
            "        X = self._compute_kernel(X)",
            "",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        svm_type = LIBSVM_IMPL.index(self._impl)",
            "        pprob = libsvm.predict_proba(",
            "            X,",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self._dual_coef_,",
            "            self._intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            svm_type=svm_type,",
            "            kernel=kernel,",
            "            degree=self.degree,",
            "            cache_size=self.cache_size,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "        )",
            "",
            "        return pprob",
            "",
            "    def _sparse_predict_proba(self, X):",
            "        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")",
            "",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        return libsvm_sparse.libsvm_sparse_predict_proba(",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            self.support_vectors_.data,",
            "            self.support_vectors_.indices,",
            "            self.support_vectors_.indptr,",
            "            self._dual_coef_.data,",
            "            self._intercept_,",
            "            LIBSVM_IMPL.index(self._impl),",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            self.C,",
            "            self.class_weight_,",
            "            self.nu,",
            "            self.epsilon,",
            "            self.shrinking,",
            "            self.probability,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "        )",
            "",
            "    def _get_coef(self):",
            "        if self.dual_coef_.shape[0] == 1:",
            "            # binary classifier",
            "            coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)",
            "        else:",
            "            # 1vs1 classifier",
            "            coef = _one_vs_one_coef(",
            "                self.dual_coef_, self._n_support, self.support_vectors_",
            "            )",
            "            if sp.issparse(coef[0]):",
            "                coef = sp.vstack(coef).tocsr()",
            "            else:",
            "                coef = np.vstack(coef)",
            "",
            "        return coef",
            "",
            "    @property",
            "    def probA_(self):",
            "        \"\"\"Parameter learned in Platt scaling when `probability=True`.",
            "",
            "        Returns",
            "        -------",
            "        ndarray of shape  (n_classes * (n_classes - 1) / 2)",
            "        \"\"\"",
            "        return self._probA",
            "",
            "    @property",
            "    def probB_(self):",
            "        \"\"\"Parameter learned in Platt scaling when `probability=True`.",
            "",
            "        Returns",
            "        -------",
            "        ndarray of shape  (n_classes * (n_classes - 1) / 2)",
            "        \"\"\"",
            "        return self._probB",
            "",
            "",
            "def _get_liblinear_solver_type(multi_class, penalty, loss, dual):",
            "    \"\"\"Find the liblinear magic number for the solver.",
            "",
            "    This number depends on the values of the following attributes:",
            "      - multi_class",
            "      - penalty",
            "      - loss",
            "      - dual",
            "",
            "    The same number is also internally used by LibLinear to determine",
            "    which solver to use.",
            "    \"\"\"",
            "    # nested dicts containing level 1: available loss functions,",
            "    # level2: available penalties for the given loss function,",
            "    # level3: whether the dual solver is available for the specified",
            "    # combination of loss function and penalty",
            "    _solver_type_dict = {",
            "        \"logistic_regression\": {\"l1\": {False: 6}, \"l2\": {False: 0, True: 7}},",
            "        \"hinge\": {\"l2\": {True: 3}},",
            "        \"squared_hinge\": {\"l1\": {False: 5}, \"l2\": {False: 2, True: 1}},",
            "        \"epsilon_insensitive\": {\"l2\": {True: 13}},",
            "        \"squared_epsilon_insensitive\": {\"l2\": {False: 11, True: 12}},",
            "        \"crammer_singer\": 4,",
            "    }",
            "",
            "    if multi_class == \"crammer_singer\":",
            "        return _solver_type_dict[multi_class]",
            "    elif multi_class != \"ovr\":",
            "        raise ValueError(",
            "            \"`multi_class` must be one of `ovr`, `crammer_singer`, got %r\" % multi_class",
            "        )",
            "",
            "    _solver_pen = _solver_type_dict.get(loss, None)",
            "    if _solver_pen is None:",
            "        error_string = \"loss='%s' is not supported\" % loss",
            "    else:",
            "        _solver_dual = _solver_pen.get(penalty, None)",
            "        if _solver_dual is None:",
            "            error_string = (",
            "                \"The combination of penalty='%s' and loss='%s' is not supported\"",
            "                % (penalty, loss)",
            "            )",
            "        else:",
            "            solver_num = _solver_dual.get(dual, None)",
            "            if solver_num is None:",
            "                error_string = (",
            "                    \"The combination of penalty='%s' and \"",
            "                    \"loss='%s' are not supported when dual=%s\" % (penalty, loss, dual)",
            "                )",
            "            else:",
            "                return solver_num",
            "    raise ValueError(",
            "        \"Unsupported set of arguments: %s, Parameters: penalty=%r, loss=%r, dual=%r\"",
            "        % (error_string, penalty, loss, dual)",
            "    )",
            "",
            "",
            "def _fit_liblinear(",
            "    X,",
            "    y,",
            "    C,",
            "    fit_intercept,",
            "    intercept_scaling,",
            "    class_weight,",
            "    penalty,",
            "    dual,",
            "    verbose,",
            "    max_iter,",
            "    tol,",
            "    random_state=None,",
            "    multi_class=\"ovr\",",
            "    loss=\"logistic_regression\",",
            "    epsilon=0.1,",
            "    sample_weight=None,",
            "):",
            "    \"\"\"Used by Logistic Regression (and CV) and LinearSVC/LinearSVR.",
            "",
            "    Preprocessing is done in this function before supplying it to liblinear.",
            "",
            "    Parameters",
            "    ----------",
            "    X : {array-like, sparse matrix} of shape (n_samples, n_features)",
            "        Training vector, where `n_samples` is the number of samples and",
            "        `n_features` is the number of features.",
            "",
            "    y : array-like of shape (n_samples,)",
            "        Target vector relative to X",
            "",
            "    C : float",
            "        Inverse of cross-validation parameter. Lower the C, the more",
            "        the penalization.",
            "",
            "    fit_intercept : bool",
            "        Whether or not to fit the intercept, that is to add a intercept",
            "        term to the decision function.",
            "",
            "    intercept_scaling : float",
            "        LibLinear internally penalizes the intercept and this term is subject",
            "        to regularization just like the other terms of the feature vector.",
            "        In order to avoid this, one should increase the intercept_scaling.",
            "        such that the feature vector becomes [x, intercept_scaling].",
            "",
            "    class_weight : dict or 'balanced', default=None",
            "        Weights associated with classes in the form ``{class_label: weight}``.",
            "        If not given, all classes are supposed to have weight one. For",
            "        multi-output problems, a list of dicts can be provided in the same",
            "        order as the columns of y.",
            "",
            "        The \"balanced\" mode uses the values of y to automatically adjust",
            "        weights inversely proportional to class frequencies in the input data",
            "        as ``n_samples / (n_classes * np.bincount(y))``",
            "",
            "    penalty : {'l1', 'l2'}",
            "        The norm of the penalty used in regularization.",
            "",
            "    dual : bool",
            "        Dual or primal formulation,",
            "",
            "    verbose : int",
            "        Set verbose to any positive number for verbosity.",
            "",
            "    max_iter : int",
            "        Number of iterations.",
            "",
            "    tol : float",
            "        Stopping condition.",
            "",
            "    random_state : int, RandomState instance or None, default=None",
            "        Controls the pseudo random number generation for shuffling the data.",
            "        Pass an int for reproducible output across multiple function calls.",
            "        See :term:`Glossary <random_state>`.",
            "",
            "    multi_class : {'ovr', 'crammer_singer'}, default='ovr'",
            "        `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`",
            "        optimizes a joint objective over all classes.",
            "        While `crammer_singer` is interesting from an theoretical perspective",
            "        as it is consistent it is seldom used in practice and rarely leads to",
            "        better accuracy and is more expensive to compute.",
            "        If `crammer_singer` is chosen, the options loss, penalty and dual will",
            "        be ignored.",
            "",
            "    loss : {'logistic_regression', 'hinge', 'squared_hinge', \\",
            "            'epsilon_insensitive', 'squared_epsilon_insensitive}, \\",
            "            default='logistic_regression'",
            "        The loss function used to fit the model.",
            "",
            "    epsilon : float, default=0.1",
            "        Epsilon parameter in the epsilon-insensitive loss function. Note",
            "        that the value of this parameter depends on the scale of the target",
            "        variable y. If unsure, set epsilon=0.",
            "",
            "    sample_weight : array-like of shape (n_samples,), default=None",
            "        Weights assigned to each sample.",
            "",
            "    Returns",
            "    -------",
            "    coef_ : ndarray of shape (n_features, n_features + 1)",
            "        The coefficient vector got by minimizing the objective function.",
            "",
            "    intercept_ : float",
            "        The intercept term added to the vector.",
            "",
            "    n_iter_ : int",
            "        Maximum number of iterations run across all classes.",
            "    \"\"\"",
            "    if loss not in [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"]:",
            "        enc = LabelEncoder()",
            "        y_ind = enc.fit_transform(y)",
            "        classes_ = enc.classes_",
            "        if len(classes_) < 2:",
            "            raise ValueError(",
            "                \"This solver needs samples of at least 2 classes\"",
            "                \" in the data, but the data contains only one\"",
            "                \" class: %r\"",
            "                % classes_[0]",
            "            )",
            "",
            "        class_weight_ = compute_class_weight(class_weight, classes=classes_, y=y)",
            "    else:",
            "        class_weight_ = np.empty(0, dtype=np.float64)",
            "        y_ind = y",
            "    liblinear.set_verbosity_wrap(verbose)",
            "    rnd = check_random_state(random_state)",
            "    if verbose:",
            "        print(\"[LibLinear]\", end=\"\")",
            "",
            "    # LinearSVC breaks when intercept_scaling is <= 0",
            "    bias = -1.0",
            "    if fit_intercept:",
            "        if intercept_scaling <= 0:",
            "            raise ValueError(",
            "                \"Intercept scaling is %r but needs to be greater \"",
            "                \"than 0. To disable fitting an intercept,\"",
            "                \" set fit_intercept=False.\" % intercept_scaling",
            "            )",
            "        else:",
            "            bias = intercept_scaling",
            "",
            "    libsvm.set_verbosity_wrap(verbose)",
            "    libsvm_sparse.set_verbosity_wrap(verbose)",
            "    liblinear.set_verbosity_wrap(verbose)",
            "",
            "    # Liblinear doesn't support 64bit sparse matrix indices yet",
            "    if sp.issparse(X):",
            "        _check_large_sparse(X)",
            "",
            "    # LibLinear wants targets as doubles, even for classification",
            "    y_ind = np.asarray(y_ind, dtype=np.float64).ravel()",
            "    y_ind = np.require(y_ind, requirements=\"W\")",
            "",
            "    sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64)",
            "",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)",
            "    raw_coef_, n_iter_ = liblinear.train_wrap(",
            "        X,",
            "        y_ind,",
            "        sp.isspmatrix(X),",
            "        solver_type,",
            "        tol,",
            "        bias,",
            "        C,",
            "        class_weight_,",
            "        max_iter,",
            "        rnd.randint(np.iinfo(\"i\").max),",
            "        epsilon,",
            "        sample_weight,",
            "    )",
            "    # Regarding rnd.randint(..) in the above signature:",
            "    # seed for srand in range [0..INT_MAX); due to limitations in Numpy",
            "    # on 32-bit platforms, we can't get to the UINT_MAX limit that",
            "    # srand supports",
            "    n_iter_ = max(n_iter_)",
            "    if n_iter_ >= max_iter:",
            "        warnings.warn(",
            "            \"Liblinear failed to converge, increase the number of iterations.\",",
            "            ConvergenceWarning,",
            "        )",
            "",
            "    if fit_intercept:",
            "        coef_ = raw_coef_[:, :-1]",
            "        intercept_ = intercept_scaling * raw_coef_[:, -1]",
            "    else:",
            "        coef_ = raw_coef_",
            "        intercept_ = 0.0",
            "",
            "    return coef_, intercept_, n_iter_"
        ],
        "afterPatchFile": [
            "import numpy as np",
            "import scipy.sparse as sp",
            "import warnings",
            "from abc import ABCMeta, abstractmethod",
            "",
            "# mypy error: error: Module 'sklearn.svm' has no attribute '_libsvm'",
            "# (and same for other imports)",
            "from . import _libsvm as libsvm  # type: ignore",
            "from . import _liblinear as liblinear  # type: ignore",
            "from . import _libsvm_sparse as libsvm_sparse  # type: ignore",
            "from ..base import BaseEstimator, ClassifierMixin",
            "from ..preprocessing import LabelEncoder",
            "from ..utils.multiclass import _ovr_decision_function",
            "from ..utils import check_array, check_random_state",
            "from ..utils import column_or_1d",
            "from ..utils import compute_class_weight",
            "from ..utils.metaestimators import available_if",
            "from ..utils.deprecation import deprecated",
            "from ..utils.extmath import safe_sparse_dot",
            "from ..utils.validation import check_is_fitted, _check_large_sparse",
            "from ..utils.validation import _num_samples",
            "from ..utils.validation import _check_sample_weight, check_consistent_length",
            "from ..utils.multiclass import check_classification_targets",
            "from ..exceptions import ConvergenceWarning",
            "from ..exceptions import NotFittedError",
            "",
            "",
            "LIBSVM_IMPL = [\"c_svc\", \"nu_svc\", \"one_class\", \"epsilon_svr\", \"nu_svr\"]",
            "",
            "",
            "def _one_vs_one_coef(dual_coef, n_support, support_vectors):",
            "    \"\"\"Generate primal coefficients from dual coefficients",
            "    for the one-vs-one multi class LibSVM in the case",
            "    of a linear kernel.\"\"\"",
            "",
            "    # get 1vs1 weights for all n*(n-1) classifiers.",
            "    # this is somewhat messy.",
            "    # shape of dual_coef_ is nSV * (n_classes -1)",
            "    # see docs for details",
            "    n_class = dual_coef.shape[0] + 1",
            "",
            "    # XXX we could do preallocation of coef but",
            "    # would have to take care in the sparse case",
            "    coef = []",
            "    sv_locs = np.cumsum(np.hstack([[0], n_support]))",
            "    for class1 in range(n_class):",
            "        # SVs for class1:",
            "        sv1 = support_vectors[sv_locs[class1] : sv_locs[class1 + 1], :]",
            "        for class2 in range(class1 + 1, n_class):",
            "            # SVs for class1:",
            "            sv2 = support_vectors[sv_locs[class2] : sv_locs[class2 + 1], :]",
            "",
            "            # dual coef for class1 SVs:",
            "            alpha1 = dual_coef[class2 - 1, sv_locs[class1] : sv_locs[class1 + 1]]",
            "            # dual coef for class2 SVs:",
            "            alpha2 = dual_coef[class1, sv_locs[class2] : sv_locs[class2 + 1]]",
            "            # build weight for class1 vs class2",
            "",
            "            coef.append(safe_sparse_dot(alpha1, sv1) + safe_sparse_dot(alpha2, sv2))",
            "    return coef",
            "",
            "",
            "class BaseLibSVM(BaseEstimator, metaclass=ABCMeta):",
            "    \"\"\"Base class for estimators that use libsvm as backing library.",
            "",
            "    This implements support vector machine classification and regression.",
            "",
            "    Parameter documentation is in the derived `SVC` class.",
            "    \"\"\"",
            "",
            "    # The order of these must match the integer values in LibSVM.",
            "    # XXX These are actually the same in the dense case. Need to factor",
            "    # this out.",
            "    _sparse_kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\", \"precomputed\"]",
            "",
            "    @abstractmethod",
            "    def __init__(",
            "        self,",
            "        kernel,",
            "        degree,",
            "        gamma,",
            "        coef0,",
            "        tol,",
            "        C,",
            "        nu,",
            "        epsilon,",
            "        shrinking,",
            "        probability,",
            "        cache_size,",
            "        class_weight,",
            "        verbose,",
            "        max_iter,",
            "        random_state,",
            "    ):",
            "",
            "        if self._impl not in LIBSVM_IMPL:",
            "            raise ValueError(",
            "                \"impl should be one of %s, %s was given\" % (LIBSVM_IMPL, self._impl)",
            "            )",
            "",
            "        if gamma == 0:",
            "            msg = (",
            "                \"The gamma value of 0.0 is invalid. Use 'auto' to set\"",
            "                \" gamma to a value of 1 / n_features.\"",
            "            )",
            "            raise ValueError(msg)",
            "",
            "        self.kernel = kernel",
            "        self.degree = degree",
            "        self.gamma = gamma",
            "        self.coef0 = coef0",
            "        self.tol = tol",
            "        self.C = C",
            "        self.nu = nu",
            "        self.epsilon = epsilon",
            "        self.shrinking = shrinking",
            "        self.probability = probability",
            "        self.cache_size = cache_size",
            "        self.class_weight = class_weight",
            "        self.verbose = verbose",
            "        self.max_iter = max_iter",
            "        self.random_state = random_state",
            "",
            "    def _more_tags(self):",
            "        # Used by cross_val_score.",
            "        return {\"pairwise\": self.kernel == \"precomputed\"}",
            "",
            "    # TODO: Remove in 1.1",
            "    # mypy error: Decorated property not supported",
            "    @deprecated(  # type: ignore",
            "        \"Attribute `_pairwise` was deprecated in \"",
            "        \"version 0.24 and will be removed in 1.1 (renaming of 0.26).\"",
            "    )",
            "    @property",
            "    def _pairwise(self):",
            "        # Used by cross_val_score.",
            "        return self.kernel == \"precomputed\"",
            "",
            "    def fit(self, X, y, sample_weight=None):",
            "        \"\"\"Fit the SVM model according to the given training data.",
            "",
            "        Parameters",
            "        ----------",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features) \\",
            "                or (n_samples, n_samples)",
            "            Training vectors, where `n_samples` is the number of samples",
            "            and `n_features` is the number of features.",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples, n_samples).",
            "",
            "        y : array-like of shape (n_samples,)",
            "            Target values (class labels in classification, real numbers in",
            "            regression).",
            "",
            "        sample_weight : array-like of shape (n_samples,), default=None",
            "            Per-sample weights. Rescale C per sample. Higher weights",
            "            force the classifier to put more emphasis on these points.",
            "",
            "        Returns",
            "        -------",
            "        self : object",
            "            Fitted estimator.",
            "",
            "        Notes",
            "        -----",
            "        If X and y are not C-ordered and contiguous arrays of np.float64 and",
            "        X is not a scipy.sparse.csr_matrix, X and/or y may be copied.",
            "",
            "        If X is a dense array, then the other methods will not support sparse",
            "        matrices as input.",
            "        \"\"\"",
            "",
            "        rnd = check_random_state(self.random_state)",
            "",
            "        sparse = sp.isspmatrix(X)",
            "        if sparse and self.kernel == \"precomputed\":",
            "            raise TypeError(\"Sparse precomputed kernels are not supported.\")",
            "        self._sparse = sparse and not callable(self.kernel)",
            "",
            "        if hasattr(self, \"decision_function_shape\"):",
            "            if self.decision_function_shape not in (\"ovr\", \"ovo\"):",
            "                raise ValueError(",
            "                    \"decision_function_shape must be either 'ovr' or 'ovo', \"",
            "                    f\"got {self.decision_function_shape}.\"",
            "                )",
            "",
            "        if callable(self.kernel):",
            "            check_consistent_length(X, y)",
            "        else:",
            "            X, y = self._validate_data(",
            "                X,",
            "                y,",
            "                dtype=np.float64,",
            "                order=\"C\",",
            "                accept_sparse=\"csr\",",
            "                accept_large_sparse=False,",
            "            )",
            "",
            "        y = self._validate_targets(y)",
            "",
            "        sample_weight = np.asarray(",
            "            [] if sample_weight is None else sample_weight, dtype=np.float64",
            "        )",
            "        solver_type = LIBSVM_IMPL.index(self._impl)",
            "",
            "        # input validation",
            "        n_samples = _num_samples(X)",
            "        if solver_type != 2 and n_samples != y.shape[0]:",
            "            raise ValueError(",
            "                \"X and y have incompatible shapes.\\n\"",
            "                + \"X has %s samples, but y has %s.\" % (n_samples, y.shape[0])",
            "            )",
            "",
            "        if self.kernel == \"precomputed\" and n_samples != X.shape[1]:",
            "            raise ValueError(",
            "                \"Precomputed matrix must be a square matrix.\"",
            "                \" Input is a {}x{} matrix.\".format(X.shape[0], X.shape[1])",
            "            )",
            "",
            "        if sample_weight.shape[0] > 0 and sample_weight.shape[0] != n_samples:",
            "            raise ValueError(",
            "                \"sample_weight and X have incompatible shapes: \"",
            "                \"%r vs %r\\n\"",
            "                \"Note: Sparse matrices cannot be indexed w/\"",
            "                \"boolean masks (use `indices=True` in CV).\"",
            "                % (sample_weight.shape, X.shape)",
            "            )",
            "",
            "        kernel = \"precomputed\" if callable(self.kernel) else self.kernel",
            "",
            "        if kernel == \"precomputed\":",
            "            # unused but needs to be a float for cython code that ignores",
            "            # it anyway",
            "            self._gamma = 0.0",
            "        elif isinstance(self.gamma, str):",
            "            if self.gamma == \"scale\":",
            "                # var = E[X^2] - E[X]^2 if sparse",
            "                X_var = (X.multiply(X)).mean() - (X.mean()) ** 2 if sparse else X.var()",
            "                self._gamma = 1.0 / (X.shape[1] * X_var) if X_var != 0 else 1.0",
            "            elif self.gamma == \"auto\":",
            "                self._gamma = 1.0 / X.shape[1]",
            "            else:",
            "                raise ValueError(",
            "                    \"When 'gamma' is a string, it should be either 'scale' or \"",
            "                    \"'auto'. Got '{}' instead.\".format(self.gamma)",
            "                )",
            "        else:",
            "            self._gamma = self.gamma",
            "",
            "        fit = self._sparse_fit if self._sparse else self._dense_fit",
            "        if self.verbose:",
            "            print(\"[LibSVM]\", end=\"\")",
            "",
            "        seed = rnd.randint(np.iinfo(\"i\").max)",
            "        fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)",
            "        # see comment on the other call to np.iinfo in this file",
            "",
            "        self.shape_fit_ = X.shape if hasattr(X, \"shape\") else (n_samples,)",
            "",
            "        # In binary case, we need to flip the sign of coef, intercept and",
            "        # decision function. Use self._intercept_ and self._dual_coef_",
            "        # internally.",
            "        self._intercept_ = self.intercept_.copy()",
            "        self._dual_coef_ = self.dual_coef_",
            "        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:",
            "            self.intercept_ *= -1",
            "            self.dual_coef_ = -self.dual_coef_",
            "",
            "        return self",
            "",
            "    def _validate_targets(self, y):",
            "        \"\"\"Validation of y and class_weight.",
            "",
            "        Default implementation for SVR and one-class; overridden in BaseSVC.",
            "        \"\"\"",
            "        # XXX this is ugly.",
            "        # Regression models should not have a class_weight_ attribute.",
            "        self.class_weight_ = np.empty(0)",
            "        return column_or_1d(y, warn=True).astype(np.float64, copy=False)",
            "",
            "    def _warn_from_fit_status(self):",
            "        assert self.fit_status_ in (0, 1)",
            "        if self.fit_status_ == 1:",
            "            warnings.warn(",
            "                \"Solver terminated early (max_iter=%i).\"",
            "                \"  Consider pre-processing your data with\"",
            "                \" StandardScaler or MinMaxScaler.\"",
            "                % self.max_iter,",
            "                ConvergenceWarning,",
            "            )",
            "",
            "    def _dense_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):",
            "        if callable(self.kernel):",
            "            # you must store a reference to X to compute the kernel in predict",
            "            # TODO: add keyword copy to copy on demand",
            "            self.__Xfit = X",
            "            X = self._compute_kernel(X)",
            "",
            "            if X.shape[0] != X.shape[1]:",
            "                raise ValueError(\"X.shape[0] should be equal to X.shape[1]\")",
            "",
            "        libsvm.set_verbosity_wrap(self.verbose)",
            "",
            "        # we don't pass **self.get_params() to allow subclasses to",
            "        # add other parameters to __init__",
            "        (",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self.dual_coef_,",
            "            self.intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            self.fit_status_,",
            "        ) = libsvm.fit(",
            "            X,",
            "            y,",
            "            svm_type=solver_type,",
            "            sample_weight=sample_weight,",
            "            class_weight=self.class_weight_,",
            "            kernel=kernel,",
            "            C=self.C,",
            "            nu=self.nu,",
            "            probability=self.probability,",
            "            degree=self.degree,",
            "            shrinking=self.shrinking,",
            "            tol=self.tol,",
            "            cache_size=self.cache_size,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "            epsilon=self.epsilon,",
            "            max_iter=self.max_iter,",
            "            random_seed=random_seed,",
            "        )",
            "",
            "        self._warn_from_fit_status()",
            "",
            "    def _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed):",
            "        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")",
            "        X.sort_indices()",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        libsvm_sparse.set_verbosity_wrap(self.verbose)",
            "",
            "        (",
            "            self.support_,",
            "            self.support_vectors_,",
            "            dual_coef_data,",
            "            self.intercept_,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "            self.fit_status_,",
            "        ) = libsvm_sparse.libsvm_sparse_train(",
            "            X.shape[1],",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            y,",
            "            solver_type,",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            self.C,",
            "            self.class_weight_,",
            "            sample_weight,",
            "            self.nu,",
            "            self.cache_size,",
            "            self.epsilon,",
            "            int(self.shrinking),",
            "            int(self.probability),",
            "            self.max_iter,",
            "            random_seed,",
            "        )",
            "",
            "        self._warn_from_fit_status()",
            "",
            "        if hasattr(self, \"classes_\"):",
            "            n_class = len(self.classes_) - 1",
            "        else:  # regression",
            "            n_class = 1",
            "        n_SV = self.support_vectors_.shape[0]",
            "",
            "        dual_coef_indices = np.tile(np.arange(n_SV), n_class)",
            "        if not n_SV:",
            "            self.dual_coef_ = sp.csr_matrix([])",
            "        else:",
            "            dual_coef_indptr = np.arange(",
            "                0, dual_coef_indices.size + 1, dual_coef_indices.size / n_class",
            "            )",
            "            self.dual_coef_ = sp.csr_matrix(",
            "                (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)",
            "            )",
            "",
            "    def predict(self, X):",
            "        \"\"\"Perform regression on samples in X.",
            "",
            "        For an one-class model, +1 (inlier) or -1 (outlier) is returned.",
            "",
            "        Parameters",
            "        ----------",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        y_pred : ndarray of shape (n_samples,)",
            "            The predicted values.",
            "        \"\"\"",
            "        X = self._validate_for_predict(X)",
            "        predict = self._sparse_predict if self._sparse else self._dense_predict",
            "        return predict(X)",
            "",
            "    def _dense_predict(self, X):",
            "        X = self._compute_kernel(X)",
            "        if X.ndim == 1:",
            "            X = check_array(X, order=\"C\", accept_large_sparse=False)",
            "",
            "        kernel = self.kernel",
            "        if callable(self.kernel):",
            "            kernel = \"precomputed\"",
            "            if X.shape[1] != self.shape_fit_[0]:",
            "                raise ValueError(",
            "                    \"X.shape[1] = %d should be equal to %d, \"",
            "                    \"the number of samples at training time\"",
            "                    % (X.shape[1], self.shape_fit_[0])",
            "                )",
            "",
            "        svm_type = LIBSVM_IMPL.index(self._impl)",
            "",
            "        return libsvm.predict(",
            "            X,",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self._dual_coef_,",
            "            self._intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            svm_type=svm_type,",
            "            kernel=kernel,",
            "            degree=self.degree,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "            cache_size=self.cache_size,",
            "        )",
            "",
            "    def _sparse_predict(self, X):",
            "        # Precondition: X is a csr_matrix of dtype np.float64.",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        C = 0.0  # C is not useful here",
            "",
            "        return libsvm_sparse.libsvm_sparse_predict(",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            self.support_vectors_.data,",
            "            self.support_vectors_.indices,",
            "            self.support_vectors_.indptr,",
            "            self._dual_coef_.data,",
            "            self._intercept_,",
            "            LIBSVM_IMPL.index(self._impl),",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            C,",
            "            self.class_weight_,",
            "            self.nu,",
            "            self.epsilon,",
            "            self.shrinking,",
            "            self.probability,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "        )",
            "",
            "    def _compute_kernel(self, X):",
            "        \"\"\"Return the data transformed by a callable kernel\"\"\"",
            "        if callable(self.kernel):",
            "            # in the case of precomputed kernel given as a function, we",
            "            # have to compute explicitly the kernel matrix",
            "            kernel = self.kernel(X, self.__Xfit)",
            "            if sp.issparse(kernel):",
            "                kernel = kernel.toarray()",
            "            X = np.asarray(kernel, dtype=np.float64, order=\"C\")",
            "        return X",
            "",
            "    def _decision_function(self, X):",
            "        \"\"\"Evaluates the decision function for the samples in X.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features)",
            "",
            "        Returns",
            "        -------",
            "        X : array-like of shape (n_samples, n_class * (n_class-1) / 2)",
            "            Returns the decision function of the sample for each class",
            "            in the model.",
            "        \"\"\"",
            "        # NOTE: _validate_for_predict contains check for is_fitted",
            "        # hence must be placed before any other attributes are used.",
            "        X = self._validate_for_predict(X)",
            "        X = self._compute_kernel(X)",
            "",
            "        if self._sparse:",
            "            dec_func = self._sparse_decision_function(X)",
            "        else:",
            "            dec_func = self._dense_decision_function(X)",
            "",
            "        # In binary case, we need to flip the sign of coef, intercept and",
            "        # decision function.",
            "        if self._impl in [\"c_svc\", \"nu_svc\"] and len(self.classes_) == 2:",
            "            return -dec_func.ravel()",
            "",
            "        return dec_func",
            "",
            "    def _dense_decision_function(self, X):",
            "        X = check_array(X, dtype=np.float64, order=\"C\", accept_large_sparse=False)",
            "",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        return libsvm.decision_function(",
            "            X,",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self._dual_coef_,",
            "            self._intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            svm_type=LIBSVM_IMPL.index(self._impl),",
            "            kernel=kernel,",
            "            degree=self.degree,",
            "            cache_size=self.cache_size,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "        )",
            "",
            "    def _sparse_decision_function(self, X):",
            "        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")",
            "",
            "        kernel = self.kernel",
            "        if hasattr(kernel, \"__call__\"):",
            "            kernel = \"precomputed\"",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        return libsvm_sparse.libsvm_sparse_decision_function(",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            self.support_vectors_.data,",
            "            self.support_vectors_.indices,",
            "            self.support_vectors_.indptr,",
            "            self._dual_coef_.data,",
            "            self._intercept_,",
            "            LIBSVM_IMPL.index(self._impl),",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            self.C,",
            "            self.class_weight_,",
            "            self.nu,",
            "            self.epsilon,",
            "            self.shrinking,",
            "            self.probability,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "        )",
            "",
            "    def _validate_for_predict(self, X):",
            "        check_is_fitted(self)",
            "",
            "        if not callable(self.kernel):",
            "            X = self._validate_data(",
            "                X,",
            "                accept_sparse=\"csr\",",
            "                dtype=np.float64,",
            "                order=\"C\",",
            "                accept_large_sparse=False,",
            "                reset=False,",
            "            )",
            "",
            "        if self._sparse and not sp.isspmatrix(X):",
            "            X = sp.csr_matrix(X)",
            "        if self._sparse:",
            "            X.sort_indices()",
            "",
            "        if sp.issparse(X) and not self._sparse and not callable(self.kernel):",
            "            raise ValueError(",
            "                \"cannot use sparse input in %r trained on dense data\"",
            "                % type(self).__name__",
            "            )",
            "",
            "        if self.kernel == \"precomputed\":",
            "            if X.shape[1] != self.shape_fit_[0]:",
            "                raise ValueError(",
            "                    \"X.shape[1] = %d should be equal to %d, \"",
            "                    \"the number of samples at training time\"",
            "                    % (X.shape[1], self.shape_fit_[0])",
            "                )",
            "        # Fixes https://nvd.nist.gov/vuln/detail/CVE-2020-28975",
            "        # Check that _n_support is consistent with support_vectors",
            "        sv = self.support_vectors_",
            "        if not self._sparse and sv.size > 0 and self.n_support_.sum() != sv.shape[0]:",
            "            raise ValueError(",
            "                f\"The internal representation of {self.__class__.__name__} was altered\"",
            "            )",
            "        return X",
            "",
            "    @property",
            "    def coef_(self):",
            "        \"\"\"Weights assigned to the features when `kernel=\"linear\"`.",
            "",
            "        Returns",
            "        -------",
            "        ndarray of shape (n_features, n_classes)",
            "        \"\"\"",
            "        if self.kernel != \"linear\":",
            "            raise AttributeError(\"coef_ is only available when using a linear kernel\")",
            "",
            "        coef = self._get_coef()",
            "",
            "        # coef_ being a read-only property, it's better to mark the value as",
            "        # immutable to avoid hiding potential bugs for the unsuspecting user.",
            "        if sp.issparse(coef):",
            "            # sparse matrix do not have global flags",
            "            coef.data.flags.writeable = False",
            "        else:",
            "            # regular dense array",
            "            coef.flags.writeable = False",
            "        return coef",
            "",
            "    def _get_coef(self):",
            "        return safe_sparse_dot(self._dual_coef_, self.support_vectors_)",
            "",
            "    @property",
            "    def n_support_(self):",
            "        \"\"\"Number of support vectors for each class.\"\"\"",
            "        try:",
            "            check_is_fitted(self)",
            "        except NotFittedError:",
            "            raise AttributeError",
            "",
            "        svm_type = LIBSVM_IMPL.index(self._impl)",
            "        if svm_type in (0, 1):",
            "            return self._n_support",
            "        else:",
            "            # SVR and OneClass",
            "            # _n_support has size 2, we make it size 1",
            "            return np.array([self._n_support[0]])",
            "",
            "",
            "class BaseSVC(ClassifierMixin, BaseLibSVM, metaclass=ABCMeta):",
            "    \"\"\"ABC for LibSVM-based classifiers.\"\"\"",
            "",
            "    @abstractmethod",
            "    def __init__(",
            "        self,",
            "        kernel,",
            "        degree,",
            "        gamma,",
            "        coef0,",
            "        tol,",
            "        C,",
            "        nu,",
            "        shrinking,",
            "        probability,",
            "        cache_size,",
            "        class_weight,",
            "        verbose,",
            "        max_iter,",
            "        decision_function_shape,",
            "        random_state,",
            "        break_ties,",
            "    ):",
            "        self.decision_function_shape = decision_function_shape",
            "        self.break_ties = break_ties",
            "        super().__init__(",
            "            kernel=kernel,",
            "            degree=degree,",
            "            gamma=gamma,",
            "            coef0=coef0,",
            "            tol=tol,",
            "            C=C,",
            "            nu=nu,",
            "            epsilon=0.0,",
            "            shrinking=shrinking,",
            "            probability=probability,",
            "            cache_size=cache_size,",
            "            class_weight=class_weight,",
            "            verbose=verbose,",
            "            max_iter=max_iter,",
            "            random_state=random_state,",
            "        )",
            "",
            "    def _validate_targets(self, y):",
            "        y_ = column_or_1d(y, warn=True)",
            "        check_classification_targets(y)",
            "        cls, y = np.unique(y_, return_inverse=True)",
            "        self.class_weight_ = compute_class_weight(self.class_weight, classes=cls, y=y_)",
            "        if len(cls) < 2:",
            "            raise ValueError(",
            "                \"The number of classes has to be greater than one; got %d class\"",
            "                % len(cls)",
            "            )",
            "",
            "        self.classes_ = cls",
            "",
            "        return np.asarray(y, dtype=np.float64, order=\"C\")",
            "",
            "    def decision_function(self, X):",
            "        \"\"\"Evaluate the decision function for the samples in X.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features)",
            "            The input samples.",
            "",
            "        Returns",
            "        -------",
            "        X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)",
            "            Returns the decision function of the sample for each class",
            "            in the model.",
            "            If decision_function_shape='ovr', the shape is (n_samples,",
            "            n_classes).",
            "",
            "        Notes",
            "        -----",
            "        If decision_function_shape='ovo', the function values are proportional",
            "        to the distance of the samples X to the separating hyperplane. If the",
            "        exact distances are required, divide the function values by the norm of",
            "        the weight vector (``coef_``). See also `this question",
            "        <https://stats.stackexchange.com/questions/14876/",
            "        interpreting-distance-from-hyperplane-in-svm>`_ for further details.",
            "        If decision_function_shape='ovr', the decision function is a monotonic",
            "        transformation of ovo decision function.",
            "        \"\"\"",
            "        dec = self._decision_function(X)",
            "        if self.decision_function_shape == \"ovr\" and len(self.classes_) > 2:",
            "            return _ovr_decision_function(dec < 0, -dec, len(self.classes_))",
            "        return dec",
            "",
            "    def predict(self, X):",
            "        \"\"\"Perform classification on samples in X.",
            "",
            "        For an one-class model, +1 or -1 is returned.",
            "",
            "        Parameters",
            "        ----------",
            "        X : {array-like, sparse matrix} of shape (n_samples, n_features) or \\",
            "                (n_samples_test, n_samples_train)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        y_pred : ndarray of shape (n_samples,)",
            "            Class labels for samples in X.",
            "        \"\"\"",
            "        check_is_fitted(self)",
            "        if self.break_ties and self.decision_function_shape == \"ovo\":",
            "            raise ValueError(",
            "                \"break_ties must be False when decision_function_shape is 'ovo'\"",
            "            )",
            "",
            "        if (",
            "            self.break_ties",
            "            and self.decision_function_shape == \"ovr\"",
            "            and len(self.classes_) > 2",
            "        ):",
            "            y = np.argmax(self.decision_function(X), axis=1)",
            "        else:",
            "            y = super().predict(X)",
            "        return self.classes_.take(np.asarray(y, dtype=np.intp))",
            "",
            "    # Hacky way of getting predict_proba to raise an AttributeError when",
            "    # probability=False using properties. Do not use this in new code; when",
            "    # probabilities are not available depending on a setting, introduce two",
            "    # estimators.",
            "    def _check_proba(self):",
            "        if not self.probability:",
            "            raise AttributeError(",
            "                \"predict_proba is not available when  probability=False\"",
            "            )",
            "        if self._impl not in (\"c_svc\", \"nu_svc\"):",
            "            raise AttributeError(\"predict_proba only implemented for SVC and NuSVC\")",
            "        return True",
            "",
            "    @available_if(_check_proba)",
            "    def predict_proba(self, X):",
            "        \"\"\"Compute probabilities of possible outcomes for samples in X.",
            "",
            "        The model need to have probability information computed at training",
            "        time: fit with attribute `probability` set to True.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        T : ndarray of shape (n_samples, n_classes)",
            "            Returns the probability of the sample for each class in",
            "            the model. The columns correspond to the classes in sorted",
            "            order, as they appear in the attribute :term:`classes_`.",
            "",
            "        Notes",
            "        -----",
            "        The probability model is created using cross validation, so",
            "        the results can be slightly different than those obtained by",
            "        predict. Also, it will produce meaningless results on very small",
            "        datasets.",
            "        \"\"\"",
            "        X = self._validate_for_predict(X)",
            "        if self.probA_.size == 0 or self.probB_.size == 0:",
            "            raise NotFittedError(",
            "                \"predict_proba is not available when fitted with probability=False\"",
            "            )",
            "        pred_proba = (",
            "            self._sparse_predict_proba if self._sparse else self._dense_predict_proba",
            "        )",
            "        return pred_proba(X)",
            "",
            "    @available_if(_check_proba)",
            "    def predict_log_proba(self, X):",
            "        \"\"\"Compute log probabilities of possible outcomes for samples in X.",
            "",
            "        The model need to have probability information computed at training",
            "        time: fit with attribute `probability` set to True.",
            "",
            "        Parameters",
            "        ----------",
            "        X : array-like of shape (n_samples, n_features) or \\",
            "                (n_samples_test, n_samples_train)",
            "            For kernel=\"precomputed\", the expected shape of X is",
            "            (n_samples_test, n_samples_train).",
            "",
            "        Returns",
            "        -------",
            "        T : ndarray of shape (n_samples, n_classes)",
            "            Returns the log-probabilities of the sample for each class in",
            "            the model. The columns correspond to the classes in sorted",
            "            order, as they appear in the attribute :term:`classes_`.",
            "",
            "        Notes",
            "        -----",
            "        The probability model is created using cross validation, so",
            "        the results can be slightly different than those obtained by",
            "        predict. Also, it will produce meaningless results on very small",
            "        datasets.",
            "        \"\"\"",
            "        return np.log(self.predict_proba(X))",
            "",
            "    def _dense_predict_proba(self, X):",
            "        X = self._compute_kernel(X)",
            "",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        svm_type = LIBSVM_IMPL.index(self._impl)",
            "        pprob = libsvm.predict_proba(",
            "            X,",
            "            self.support_,",
            "            self.support_vectors_,",
            "            self._n_support,",
            "            self._dual_coef_,",
            "            self._intercept_,",
            "            self._probA,",
            "            self._probB,",
            "            svm_type=svm_type,",
            "            kernel=kernel,",
            "            degree=self.degree,",
            "            cache_size=self.cache_size,",
            "            coef0=self.coef0,",
            "            gamma=self._gamma,",
            "        )",
            "",
            "        return pprob",
            "",
            "    def _sparse_predict_proba(self, X):",
            "        X.data = np.asarray(X.data, dtype=np.float64, order=\"C\")",
            "",
            "        kernel = self.kernel",
            "        if callable(kernel):",
            "            kernel = \"precomputed\"",
            "",
            "        kernel_type = self._sparse_kernels.index(kernel)",
            "",
            "        return libsvm_sparse.libsvm_sparse_predict_proba(",
            "            X.data,",
            "            X.indices,",
            "            X.indptr,",
            "            self.support_vectors_.data,",
            "            self.support_vectors_.indices,",
            "            self.support_vectors_.indptr,",
            "            self._dual_coef_.data,",
            "            self._intercept_,",
            "            LIBSVM_IMPL.index(self._impl),",
            "            kernel_type,",
            "            self.degree,",
            "            self._gamma,",
            "            self.coef0,",
            "            self.tol,",
            "            self.C,",
            "            self.class_weight_,",
            "            self.nu,",
            "            self.epsilon,",
            "            self.shrinking,",
            "            self.probability,",
            "            self._n_support,",
            "            self._probA,",
            "            self._probB,",
            "        )",
            "",
            "    def _get_coef(self):",
            "        if self.dual_coef_.shape[0] == 1:",
            "            # binary classifier",
            "            coef = safe_sparse_dot(self.dual_coef_, self.support_vectors_)",
            "        else:",
            "            # 1vs1 classifier",
            "            coef = _one_vs_one_coef(",
            "                self.dual_coef_, self._n_support, self.support_vectors_",
            "            )",
            "            if sp.issparse(coef[0]):",
            "                coef = sp.vstack(coef).tocsr()",
            "            else:",
            "                coef = np.vstack(coef)",
            "",
            "        return coef",
            "",
            "    @property",
            "    def probA_(self):",
            "        \"\"\"Parameter learned in Platt scaling when `probability=True`.",
            "",
            "        Returns",
            "        -------",
            "        ndarray of shape  (n_classes * (n_classes - 1) / 2)",
            "        \"\"\"",
            "        return self._probA",
            "",
            "    @property",
            "    def probB_(self):",
            "        \"\"\"Parameter learned in Platt scaling when `probability=True`.",
            "",
            "        Returns",
            "        -------",
            "        ndarray of shape  (n_classes * (n_classes - 1) / 2)",
            "        \"\"\"",
            "        return self._probB",
            "",
            "",
            "def _get_liblinear_solver_type(multi_class, penalty, loss, dual):",
            "    \"\"\"Find the liblinear magic number for the solver.",
            "",
            "    This number depends on the values of the following attributes:",
            "      - multi_class",
            "      - penalty",
            "      - loss",
            "      - dual",
            "",
            "    The same number is also internally used by LibLinear to determine",
            "    which solver to use.",
            "    \"\"\"",
            "    # nested dicts containing level 1: available loss functions,",
            "    # level2: available penalties for the given loss function,",
            "    # level3: whether the dual solver is available for the specified",
            "    # combination of loss function and penalty",
            "    _solver_type_dict = {",
            "        \"logistic_regression\": {\"l1\": {False: 6}, \"l2\": {False: 0, True: 7}},",
            "        \"hinge\": {\"l2\": {True: 3}},",
            "        \"squared_hinge\": {\"l1\": {False: 5}, \"l2\": {False: 2, True: 1}},",
            "        \"epsilon_insensitive\": {\"l2\": {True: 13}},",
            "        \"squared_epsilon_insensitive\": {\"l2\": {False: 11, True: 12}},",
            "        \"crammer_singer\": 4,",
            "    }",
            "",
            "    if multi_class == \"crammer_singer\":",
            "        return _solver_type_dict[multi_class]",
            "    elif multi_class != \"ovr\":",
            "        raise ValueError(",
            "            \"`multi_class` must be one of `ovr`, `crammer_singer`, got %r\" % multi_class",
            "        )",
            "",
            "    _solver_pen = _solver_type_dict.get(loss, None)",
            "    if _solver_pen is None:",
            "        error_string = \"loss='%s' is not supported\" % loss",
            "    else:",
            "        _solver_dual = _solver_pen.get(penalty, None)",
            "        if _solver_dual is None:",
            "            error_string = (",
            "                \"The combination of penalty='%s' and loss='%s' is not supported\"",
            "                % (penalty, loss)",
            "            )",
            "        else:",
            "            solver_num = _solver_dual.get(dual, None)",
            "            if solver_num is None:",
            "                error_string = (",
            "                    \"The combination of penalty='%s' and \"",
            "                    \"loss='%s' are not supported when dual=%s\" % (penalty, loss, dual)",
            "                )",
            "            else:",
            "                return solver_num",
            "    raise ValueError(",
            "        \"Unsupported set of arguments: %s, Parameters: penalty=%r, loss=%r, dual=%r\"",
            "        % (error_string, penalty, loss, dual)",
            "    )",
            "",
            "",
            "def _fit_liblinear(",
            "    X,",
            "    y,",
            "    C,",
            "    fit_intercept,",
            "    intercept_scaling,",
            "    class_weight,",
            "    penalty,",
            "    dual,",
            "    verbose,",
            "    max_iter,",
            "    tol,",
            "    random_state=None,",
            "    multi_class=\"ovr\",",
            "    loss=\"logistic_regression\",",
            "    epsilon=0.1,",
            "    sample_weight=None,",
            "):",
            "    \"\"\"Used by Logistic Regression (and CV) and LinearSVC/LinearSVR.",
            "",
            "    Preprocessing is done in this function before supplying it to liblinear.",
            "",
            "    Parameters",
            "    ----------",
            "    X : {array-like, sparse matrix} of shape (n_samples, n_features)",
            "        Training vector, where `n_samples` is the number of samples and",
            "        `n_features` is the number of features.",
            "",
            "    y : array-like of shape (n_samples,)",
            "        Target vector relative to X",
            "",
            "    C : float",
            "        Inverse of cross-validation parameter. Lower the C, the more",
            "        the penalization.",
            "",
            "    fit_intercept : bool",
            "        Whether or not to fit the intercept, that is to add a intercept",
            "        term to the decision function.",
            "",
            "    intercept_scaling : float",
            "        LibLinear internally penalizes the intercept and this term is subject",
            "        to regularization just like the other terms of the feature vector.",
            "        In order to avoid this, one should increase the intercept_scaling.",
            "        such that the feature vector becomes [x, intercept_scaling].",
            "",
            "    class_weight : dict or 'balanced', default=None",
            "        Weights associated with classes in the form ``{class_label: weight}``.",
            "        If not given, all classes are supposed to have weight one. For",
            "        multi-output problems, a list of dicts can be provided in the same",
            "        order as the columns of y.",
            "",
            "        The \"balanced\" mode uses the values of y to automatically adjust",
            "        weights inversely proportional to class frequencies in the input data",
            "        as ``n_samples / (n_classes * np.bincount(y))``",
            "",
            "    penalty : {'l1', 'l2'}",
            "        The norm of the penalty used in regularization.",
            "",
            "    dual : bool",
            "        Dual or primal formulation,",
            "",
            "    verbose : int",
            "        Set verbose to any positive number for verbosity.",
            "",
            "    max_iter : int",
            "        Number of iterations.",
            "",
            "    tol : float",
            "        Stopping condition.",
            "",
            "    random_state : int, RandomState instance or None, default=None",
            "        Controls the pseudo random number generation for shuffling the data.",
            "        Pass an int for reproducible output across multiple function calls.",
            "        See :term:`Glossary <random_state>`.",
            "",
            "    multi_class : {'ovr', 'crammer_singer'}, default='ovr'",
            "        `ovr` trains n_classes one-vs-rest classifiers, while `crammer_singer`",
            "        optimizes a joint objective over all classes.",
            "        While `crammer_singer` is interesting from an theoretical perspective",
            "        as it is consistent it is seldom used in practice and rarely leads to",
            "        better accuracy and is more expensive to compute.",
            "        If `crammer_singer` is chosen, the options loss, penalty and dual will",
            "        be ignored.",
            "",
            "    loss : {'logistic_regression', 'hinge', 'squared_hinge', \\",
            "            'epsilon_insensitive', 'squared_epsilon_insensitive}, \\",
            "            default='logistic_regression'",
            "        The loss function used to fit the model.",
            "",
            "    epsilon : float, default=0.1",
            "        Epsilon parameter in the epsilon-insensitive loss function. Note",
            "        that the value of this parameter depends on the scale of the target",
            "        variable y. If unsure, set epsilon=0.",
            "",
            "    sample_weight : array-like of shape (n_samples,), default=None",
            "        Weights assigned to each sample.",
            "",
            "    Returns",
            "    -------",
            "    coef_ : ndarray of shape (n_features, n_features + 1)",
            "        The coefficient vector got by minimizing the objective function.",
            "",
            "    intercept_ : float",
            "        The intercept term added to the vector.",
            "",
            "    n_iter_ : int",
            "        Maximum number of iterations run across all classes.",
            "    \"\"\"",
            "    if loss not in [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"]:",
            "        enc = LabelEncoder()",
            "        y_ind = enc.fit_transform(y)",
            "        classes_ = enc.classes_",
            "        if len(classes_) < 2:",
            "            raise ValueError(",
            "                \"This solver needs samples of at least 2 classes\"",
            "                \" in the data, but the data contains only one\"",
            "                \" class: %r\"",
            "                % classes_[0]",
            "            )",
            "",
            "        class_weight_ = compute_class_weight(class_weight, classes=classes_, y=y)",
            "    else:",
            "        class_weight_ = np.empty(0, dtype=np.float64)",
            "        y_ind = y",
            "    liblinear.set_verbosity_wrap(verbose)",
            "    rnd = check_random_state(random_state)",
            "    if verbose:",
            "        print(\"[LibLinear]\", end=\"\")",
            "",
            "    # LinearSVC breaks when intercept_scaling is <= 0",
            "    bias = -1.0",
            "    if fit_intercept:",
            "        if intercept_scaling <= 0:",
            "            raise ValueError(",
            "                \"Intercept scaling is %r but needs to be greater \"",
            "                \"than 0. To disable fitting an intercept,\"",
            "                \" set fit_intercept=False.\" % intercept_scaling",
            "            )",
            "        else:",
            "            bias = intercept_scaling",
            "",
            "    libsvm.set_verbosity_wrap(verbose)",
            "    libsvm_sparse.set_verbosity_wrap(verbose)",
            "    liblinear.set_verbosity_wrap(verbose)",
            "",
            "    # Liblinear doesn't support 64bit sparse matrix indices yet",
            "    if sp.issparse(X):",
            "        _check_large_sparse(X)",
            "",
            "    # LibLinear wants targets as doubles, even for classification",
            "    y_ind = np.asarray(y_ind, dtype=np.float64).ravel()",
            "    y_ind = np.require(y_ind, requirements=\"W\")",
            "",
            "    sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64)",
            "",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)",
            "    raw_coef_, n_iter_ = liblinear.train_wrap(",
            "        X,",
            "        y_ind,",
            "        sp.isspmatrix(X),",
            "        solver_type,",
            "        tol,",
            "        bias,",
            "        C,",
            "        class_weight_,",
            "        max_iter,",
            "        rnd.randint(np.iinfo(\"i\").max),",
            "        epsilon,",
            "        sample_weight,",
            "    )",
            "    # Regarding rnd.randint(..) in the above signature:",
            "    # seed for srand in range [0..INT_MAX); due to limitations in Numpy",
            "    # on 32-bit platforms, we can't get to the UINT_MAX limit that",
            "    # srand supports",
            "    n_iter_ = max(n_iter_)",
            "    if n_iter_ >= max_iter:",
            "        warnings.warn(",
            "            \"Liblinear failed to converge, increase the number of iterations.\",",
            "            ConvergenceWarning,",
            "        )",
            "",
            "    if fit_intercept:",
            "        coef_ = raw_coef_[:, :-1]",
            "        intercept_ = intercept_scaling * raw_coef_[:, -1]",
            "    else:",
            "        coef_ = raw_coef_",
            "        intercept_ = 0.0",
            "",
            "    return coef_, intercept_, n_iter_"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "sklearn.svm._base.BaseLibSVM.predict",
            "sklearn.svm._base.BaseLibSVM._decision_function",
            "sklearn.svm._base.BaseSVC.predict_proba",
            "src.flask.sessions"
        ]
    },
    "sklearn/svm/tests/test_svm.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1371,
                "afterPatchRowNumber": 1371,
                "PatchRowcode": "     else:  # regressor"
            },
            "1": {
                "beforePatchRowNumber": 1372,
                "afterPatchRowNumber": 1372,
                "PatchRowcode": "         assert_allclose(svc1.predict(data), svc2.predict(X))"
            },
            "2": {
                "beforePatchRowNumber": 1373,
                "afterPatchRowNumber": 1373,
                "PatchRowcode": "         assert_allclose(svc1.predict(data), svc3.predict(K))"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1374,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1375,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1376,
                "PatchRowcode": "+def test_svc_raises_error_internal_representation():"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1377,
                "PatchRowcode": "+    \"\"\"Check that SVC raises error when internal representation is altered."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1378,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1379,
                "PatchRowcode": "+    Non-regression test for #18891 and https://nvd.nist.gov/vuln/detail/CVE-2020-28975"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1380,
                "PatchRowcode": "+    \"\"\""
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1381,
                "PatchRowcode": "+    clf = svm.SVC(kernel=\"linear\").fit(X, Y)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1382,
                "PatchRowcode": "+    clf._n_support[0] = 1000000"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1383,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1384,
                "PatchRowcode": "+    msg = \"The internal representation of SVC was altered\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1385,
                "PatchRowcode": "+    with pytest.raises(ValueError, match=msg):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1386,
                "PatchRowcode": "+        clf.predict(X)"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "Testing for Support Vector Machine module (sklearn.svm)",
            "",
            "TODO: remove hard coded numerical results when possible",
            "\"\"\"",
            "import numpy as np",
            "import itertools",
            "import pytest",
            "import re",
            "",
            "from numpy.testing import assert_array_equal, assert_array_almost_equal",
            "from numpy.testing import assert_almost_equal",
            "from numpy.testing import assert_allclose",
            "from scipy import sparse",
            "from sklearn import svm, linear_model, datasets, metrics, base",
            "from sklearn.svm import LinearSVC",
            "from sklearn.svm import LinearSVR",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.datasets import make_classification, make_blobs",
            "from sklearn.metrics import f1_score",
            "from sklearn.metrics.pairwise import rbf_kernel",
            "from sklearn.utils import check_random_state",
            "from sklearn.utils._testing import ignore_warnings",
            "from sklearn.utils.validation import _num_samples",
            "from sklearn.utils import shuffle",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.exceptions import NotFittedError, UndefinedMetricWarning",
            "from sklearn.multiclass import OneVsRestClassifier",
            "",
            "# mypy error: Module 'sklearn.svm' has no attribute '_libsvm'",
            "from sklearn.svm import _libsvm  # type: ignore",
            "",
            "# toy sample",
            "X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]",
            "Y = [1, 1, 1, 2, 2, 2]",
            "T = [[-1, -1], [2, 2], [3, 2]]",
            "true_result = [1, 2, 2]",
            "",
            "# also load the iris dataset",
            "iris = datasets.load_iris()",
            "rng = check_random_state(42)",
            "perm = rng.permutation(iris.target.size)",
            "iris.data = iris.data[perm]",
            "iris.target = iris.target[perm]",
            "",
            "",
            "def test_libsvm_parameters():",
            "    # Test parameters on classes that make use of libsvm.",
            "    clf = svm.SVC(kernel=\"linear\").fit(X, Y)",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.support_, [1, 3])",
            "    assert_array_equal(clf.support_vectors_, (X[1], X[3]))",
            "    assert_array_equal(clf.intercept_, [0.0])",
            "    assert_array_equal(clf.predict(X), Y)",
            "",
            "",
            "def test_libsvm_iris():",
            "    # Check consistency on dataset iris.",
            "",
            "    # shuffle the dataset so that labels are not ordered",
            "    for k in (\"linear\", \"rbf\"):",
            "        clf = svm.SVC(kernel=k).fit(iris.data, iris.target)",
            "        assert np.mean(clf.predict(iris.data) == iris.target) > 0.9",
            "        assert hasattr(clf, \"coef_\") == (k == \"linear\")",
            "",
            "    assert_array_equal(clf.classes_, np.sort(clf.classes_))",
            "",
            "    # check also the low-level API",
            "    model = _libsvm.fit(iris.data, iris.target.astype(np.float64))",
            "    pred = _libsvm.predict(iris.data, *model)",
            "    assert np.mean(pred == iris.target) > 0.95",
            "",
            "    model = _libsvm.fit(iris.data, iris.target.astype(np.float64), kernel=\"linear\")",
            "    pred = _libsvm.predict(iris.data, *model, kernel=\"linear\")",
            "    assert np.mean(pred == iris.target) > 0.95",
            "",
            "    pred = _libsvm.cross_validation(",
            "        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0",
            "    )",
            "    assert np.mean(pred == iris.target) > 0.95",
            "",
            "    # If random_seed >= 0, the libsvm rng is seeded (by calling `srand`), hence",
            "    # we should get deterministic results (assuming that there is no other",
            "    # thread calling this wrapper calling `srand` concurrently).",
            "    pred2 = _libsvm.cross_validation(",
            "        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0",
            "    )",
            "    assert_array_equal(pred, pred2)",
            "",
            "",
            "def test_precomputed():",
            "    # SVC with a precomputed kernel.",
            "    # We test it with a toy dataset and with iris.",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    # Gram matrix for train data (square matrix)",
            "    # (we use just a linear kernel)",
            "    K = np.dot(X, np.array(X).T)",
            "    clf.fit(K, Y)",
            "    # Gram matrix for test data (rectangular matrix)",
            "    KT = np.dot(T, np.array(X).T)",
            "    pred = clf.predict(KT)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(KT.T)",
            "",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.support_, [1, 3])",
            "    assert_array_equal(clf.intercept_, [0])",
            "    assert_array_almost_equal(clf.support_, [1, 3])",
            "    assert_array_equal(pred, true_result)",
            "",
            "    # Gram matrix for test data but compute KT[i,j]",
            "    # for support vectors j only.",
            "    KT = np.zeros_like(KT)",
            "    for i in range(len(T)):",
            "        for j in clf.support_:",
            "            KT[i, j] = np.dot(T[i], X[j])",
            "",
            "    pred = clf.predict(KT)",
            "    assert_array_equal(pred, true_result)",
            "",
            "    # same as before, but using a callable function instead of the kernel",
            "    # matrix. kernel is just a linear kernel",
            "",
            "    def kfunc(x, y):",
            "        return np.dot(x, y.T)",
            "",
            "    clf = svm.SVC(kernel=kfunc)",
            "    clf.fit(np.array(X), Y)",
            "    pred = clf.predict(T)",
            "",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.intercept_, [0])",
            "    assert_array_almost_equal(clf.support_, [1, 3])",
            "    assert_array_equal(pred, true_result)",
            "",
            "    # test a precomputed kernel with the iris dataset",
            "    # and check parameters against a linear SVC",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    clf2 = svm.SVC(kernel=\"linear\")",
            "    K = np.dot(iris.data, iris.data.T)",
            "    clf.fit(K, iris.target)",
            "    clf2.fit(iris.data, iris.target)",
            "    pred = clf.predict(K)",
            "    assert_array_almost_equal(clf.support_, clf2.support_)",
            "    assert_array_almost_equal(clf.dual_coef_, clf2.dual_coef_)",
            "    assert_array_almost_equal(clf.intercept_, clf2.intercept_)",
            "    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)",
            "",
            "    # Gram matrix for test data but compute KT[i,j]",
            "    # for support vectors j only.",
            "    K = np.zeros_like(K)",
            "    for i in range(len(iris.data)):",
            "        for j in clf.support_:",
            "            K[i, j] = np.dot(iris.data[i], iris.data[j])",
            "",
            "    pred = clf.predict(K)",
            "    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)",
            "",
            "    clf = svm.SVC(kernel=kfunc)",
            "    clf.fit(iris.data, iris.target)",
            "    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)",
            "",
            "",
            "def test_svr():",
            "    # Test Support Vector Regression",
            "",
            "    diabetes = datasets.load_diabetes()",
            "    for clf in (",
            "        svm.NuSVR(kernel=\"linear\", nu=0.4, C=1.0),",
            "        svm.NuSVR(kernel=\"linear\", nu=0.4, C=10.0),",
            "        svm.SVR(kernel=\"linear\", C=10.0),",
            "        svm.LinearSVR(C=10.0),",
            "        svm.LinearSVR(C=10.0),",
            "    ):",
            "        clf.fit(diabetes.data, diabetes.target)",
            "        assert clf.score(diabetes.data, diabetes.target) > 0.02",
            "",
            "    # non-regression test; previously, BaseLibSVM would check that",
            "    # len(np.unique(y)) < 2, which must only be done for SVC",
            "    svm.SVR().fit(diabetes.data, np.ones(len(diabetes.data)))",
            "    svm.LinearSVR().fit(diabetes.data, np.ones(len(diabetes.data)))",
            "",
            "",
            "def test_linearsvr():",
            "    # check that SVR(kernel='linear') and LinearSVC() give",
            "    # comparable results",
            "    diabetes = datasets.load_diabetes()",
            "    lsvr = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target)",
            "    score1 = lsvr.score(diabetes.data, diabetes.target)",
            "",
            "    svr = svm.SVR(kernel=\"linear\", C=1e3).fit(diabetes.data, diabetes.target)",
            "    score2 = svr.score(diabetes.data, diabetes.target)",
            "",
            "    assert_allclose(np.linalg.norm(lsvr.coef_), np.linalg.norm(svr.coef_), 1, 0.0001)",
            "    assert_almost_equal(score1, score2, 2)",
            "",
            "",
            "def test_linearsvr_fit_sampleweight():",
            "    # check correct result when sample_weight is 1",
            "    # check that SVR(kernel='linear') and LinearSVC() give",
            "    # comparable results",
            "    diabetes = datasets.load_diabetes()",
            "    n_samples = len(diabetes.target)",
            "    unit_weight = np.ones(n_samples)",
            "    lsvr = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(",
            "        diabetes.data, diabetes.target, sample_weight=unit_weight",
            "    )",
            "    score1 = lsvr.score(diabetes.data, diabetes.target)",
            "",
            "    lsvr_no_weight = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(",
            "        diabetes.data, diabetes.target",
            "    )",
            "    score2 = lsvr_no_weight.score(diabetes.data, diabetes.target)",
            "",
            "    assert_allclose(",
            "        np.linalg.norm(lsvr.coef_), np.linalg.norm(lsvr_no_weight.coef_), 1, 0.0001",
            "    )",
            "    assert_almost_equal(score1, score2, 2)",
            "",
            "    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",
            "    # X = X1 repeated n1 times, X2 repeated n2 times and so forth",
            "    random_state = check_random_state(0)",
            "    random_weight = random_state.randint(0, 10, n_samples)",
            "    lsvr_unflat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(",
            "        diabetes.data, diabetes.target, sample_weight=random_weight",
            "    )",
            "    score3 = lsvr_unflat.score(",
            "        diabetes.data, diabetes.target, sample_weight=random_weight",
            "    )",
            "",
            "    X_flat = np.repeat(diabetes.data, random_weight, axis=0)",
            "    y_flat = np.repeat(diabetes.target, random_weight, axis=0)",
            "    lsvr_flat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(X_flat, y_flat)",
            "    score4 = lsvr_flat.score(X_flat, y_flat)",
            "",
            "    assert_almost_equal(score3, score4, 2)",
            "",
            "",
            "def test_svr_errors():",
            "    X = [[0.0], [1.0]]",
            "    y = [0.0, 0.5]",
            "",
            "    # Bad kernel",
            "    clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]]))",
            "    clf.fit(X, y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(X)",
            "",
            "",
            "def test_oneclass():",
            "    # Test OneClassSVM",
            "    clf = svm.OneClassSVM()",
            "    clf.fit(X)",
            "    pred = clf.predict(T)",
            "",
            "    assert_array_equal(pred, [1, -1, -1])",
            "    assert pred.dtype == np.dtype(\"intp\")",
            "    assert_array_almost_equal(clf.intercept_, [-1.218], decimal=3)",
            "    assert_array_almost_equal(clf.dual_coef_, [[0.750, 0.750, 0.750, 0.750]], decimal=3)",
            "    with pytest.raises(AttributeError):",
            "        (lambda: clf.coef_)()",
            "",
            "",
            "def test_oneclass_decision_function():",
            "    # Test OneClassSVM decision function",
            "    clf = svm.OneClassSVM()",
            "    rnd = check_random_state(2)",
            "",
            "    # Generate train data",
            "    X = 0.3 * rnd.randn(100, 2)",
            "    X_train = np.r_[X + 2, X - 2]",
            "",
            "    # Generate some regular novel observations",
            "    X = 0.3 * rnd.randn(20, 2)",
            "    X_test = np.r_[X + 2, X - 2]",
            "    # Generate some abnormal novel observations",
            "    X_outliers = rnd.uniform(low=-4, high=4, size=(20, 2))",
            "",
            "    # fit the model",
            "    clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)",
            "    clf.fit(X_train)",
            "",
            "    # predict things",
            "    y_pred_test = clf.predict(X_test)",
            "    assert np.mean(y_pred_test == 1) > 0.9",
            "    y_pred_outliers = clf.predict(X_outliers)",
            "    assert np.mean(y_pred_outliers == -1) > 0.9",
            "    dec_func_test = clf.decision_function(X_test)",
            "    assert_array_equal((dec_func_test > 0).ravel(), y_pred_test == 1)",
            "    dec_func_outliers = clf.decision_function(X_outliers)",
            "    assert_array_equal((dec_func_outliers > 0).ravel(), y_pred_outliers == 1)",
            "",
            "",
            "def test_oneclass_score_samples():",
            "    X_train = [[1, 1], [1, 2], [2, 1]]",
            "    clf = svm.OneClassSVM(gamma=1).fit(X_train)",
            "    assert_array_equal(",
            "        clf.score_samples([[2.0, 2.0]]),",
            "        clf.decision_function([[2.0, 2.0]]) + clf.offset_,",
            "    )",
            "",
            "",
            "# TODO: Remove in v1.2",
            "def test_oneclass_fit_params_is_deprecated():",
            "    clf = svm.OneClassSVM()",
            "    params = {",
            "        \"unused_param\": \"\",",
            "        \"extra_param\": None,",
            "    }",
            "    msg = (",
            "        \"Passing additional keyword parameters has no effect and is deprecated \"",
            "        \"in 1.0. An error will be raised from 1.2 and beyond. The ignored \"",
            "        f\"keyword parameter(s) are: {params.keys()}.\"",
            "    )",
            "    with pytest.warns(FutureWarning, match=re.escape(msg)):",
            "        clf.fit(X, **params)",
            "",
            "",
            "def test_tweak_params():",
            "    # Make sure some tweaking of parameters works.",
            "    # We change clf.dual_coef_ at run time and expect .predict() to change",
            "    # accordingly. Notice that this is not trivial since it involves a lot",
            "    # of C/Python copying in the libsvm bindings.",
            "    # The success of this test ensures that the mapping between libsvm and",
            "    # the python classifier is complete.",
            "    clf = svm.SVC(kernel=\"linear\", C=1.0)",
            "    clf.fit(X, Y)",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.predict([[-0.1, -0.1]]), [1])",
            "    clf._dual_coef_ = np.array([[0.0, 1.0]])",
            "    assert_array_equal(clf.predict([[-0.1, -0.1]]), [2])",
            "",
            "",
            "def test_probability():",
            "    # Predict probabilities using SVC",
            "    # This uses cross validation, so we use a slightly bigger testing set.",
            "",
            "    for clf in (",
            "        svm.SVC(probability=True, random_state=0, C=1.0),",
            "        svm.NuSVC(probability=True, random_state=0),",
            "    ):",
            "        clf.fit(iris.data, iris.target)",
            "",
            "        prob_predict = clf.predict_proba(iris.data)",
            "        assert_array_almost_equal(np.sum(prob_predict, 1), np.ones(iris.data.shape[0]))",
            "        assert np.mean(np.argmax(prob_predict, 1) == clf.predict(iris.data)) > 0.9",
            "",
            "        assert_almost_equal(",
            "            clf.predict_proba(iris.data), np.exp(clf.predict_log_proba(iris.data)), 8",
            "        )",
            "",
            "",
            "def test_decision_function():",
            "    # Test decision_function",
            "    # Sanity check, test that decision_function implemented in python",
            "    # returns the same as the one in libsvm",
            "    # multi class:",
            "    clf = svm.SVC(kernel=\"linear\", C=0.1, decision_function_shape=\"ovo\").fit(",
            "        iris.data, iris.target",
            "    )",
            "",
            "    dec = np.dot(iris.data, clf.coef_.T) + clf.intercept_",
            "",
            "    assert_array_almost_equal(dec, clf.decision_function(iris.data))",
            "",
            "    # binary:",
            "    clf.fit(X, Y)",
            "    dec = np.dot(X, clf.coef_.T) + clf.intercept_",
            "    prediction = clf.predict(X)",
            "    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))",
            "    assert_array_almost_equal(",
            "        prediction, clf.classes_[(clf.decision_function(X) > 0).astype(int)]",
            "    )",
            "    expected = np.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])",
            "    assert_array_almost_equal(clf.decision_function(X), expected, 2)",
            "",
            "    # kernel binary:",
            "    clf = svm.SVC(kernel=\"rbf\", gamma=1, decision_function_shape=\"ovo\")",
            "    clf.fit(X, Y)",
            "",
            "    rbfs = rbf_kernel(X, clf.support_vectors_, gamma=clf.gamma)",
            "    dec = np.dot(rbfs, clf.dual_coef_.T) + clf.intercept_",
            "    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))",
            "",
            "",
            "@pytest.mark.parametrize(\"SVM\", (svm.SVC, svm.NuSVC))",
            "def test_decision_function_shape(SVM):",
            "    # check that decision_function_shape='ovr' or 'ovo' gives",
            "    # correct shape and is consistent with predict",
            "",
            "    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(",
            "        iris.data, iris.target",
            "    )",
            "    dec = clf.decision_function(iris.data)",
            "    assert dec.shape == (len(iris.data), 3)",
            "    assert_array_equal(clf.predict(iris.data), np.argmax(dec, axis=1))",
            "",
            "    # with five classes:",
            "    X, y = make_blobs(n_samples=80, centers=5, random_state=0)",
            "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)",
            "",
            "    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(X_train, y_train)",
            "    dec = clf.decision_function(X_test)",
            "    assert dec.shape == (len(X_test), 5)",
            "    assert_array_equal(clf.predict(X_test), np.argmax(dec, axis=1))",
            "",
            "    # check shape of ovo_decition_function=True",
            "    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovo\").fit(X_train, y_train)",
            "    dec = clf.decision_function(X_train)",
            "    assert dec.shape == (len(X_train), 10)",
            "",
            "    with pytest.raises(ValueError, match=\"must be either 'ovr' or 'ovo'\"):",
            "        SVM(decision_function_shape=\"bad\").fit(X_train, y_train)",
            "",
            "",
            "def test_svr_predict():",
            "    # Test SVR's decision_function",
            "    # Sanity check, test that predict implemented in python",
            "    # returns the same as the one in libsvm",
            "",
            "    X = iris.data",
            "    y = iris.target",
            "",
            "    # linear kernel",
            "    reg = svm.SVR(kernel=\"linear\", C=0.1).fit(X, y)",
            "",
            "    dec = np.dot(X, reg.coef_.T) + reg.intercept_",
            "    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())",
            "",
            "    # rbf kernel",
            "    reg = svm.SVR(kernel=\"rbf\", gamma=1).fit(X, y)",
            "",
            "    rbfs = rbf_kernel(X, reg.support_vectors_, gamma=reg.gamma)",
            "    dec = np.dot(rbfs, reg.dual_coef_.T) + reg.intercept_",
            "    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())",
            "",
            "",
            "def test_weight():",
            "    # Test class weights",
            "    clf = svm.SVC(class_weight={1: 0.1})",
            "    # we give a small weights to class 1",
            "    clf.fit(X, Y)",
            "    # so all predicted values belong to class 2",
            "    assert_array_almost_equal(clf.predict(X), [2] * 6)",
            "",
            "    X_, y_ = make_classification(",
            "        n_samples=200, n_features=10, weights=[0.833, 0.167], random_state=2",
            "    )",
            "",
            "    for clf in (",
            "        linear_model.LogisticRegression(),",
            "        svm.LinearSVC(random_state=0),",
            "        svm.SVC(),",
            "    ):",
            "        clf.set_params(class_weight={0: 0.1, 1: 10})",
            "        clf.fit(X_[:100], y_[:100])",
            "        y_pred = clf.predict(X_[100:])",
            "        assert f1_score(y_[100:], y_pred) > 0.3",
            "",
            "",
            "@pytest.mark.parametrize(\"estimator\", [svm.SVC(C=1e-2), svm.NuSVC()])",
            "def test_svm_classifier_sided_sample_weight(estimator):",
            "    # fit a linear SVM and check that giving more weight to opposed samples",
            "    # in the space will flip the decision toward these samples.",
            "    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]",
            "    estimator.set_params(kernel=\"linear\")",
            "",
            "    # check that with unit weights, a sample is supposed to be predicted on",
            "    # the boundary",
            "    sample_weight = [1] * 6",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.decision_function([[-1.0, 1.0]])",
            "    assert y_pred == pytest.approx(0)",
            "",
            "    # give more weights to opposed samples",
            "    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.decision_function([[-1.0, 1.0]])",
            "    assert y_pred < 0",
            "",
            "    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.decision_function([[-1.0, 1.0]])",
            "    assert y_pred > 0",
            "",
            "",
            "@pytest.mark.parametrize(\"estimator\", [svm.SVR(C=1e-2), svm.NuSVR(C=1e-2)])",
            "def test_svm_regressor_sided_sample_weight(estimator):",
            "    # similar test to test_svm_classifier_sided_sample_weight but for",
            "    # SVM regressors",
            "    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]",
            "    estimator.set_params(kernel=\"linear\")",
            "",
            "    # check that with unit weights, a sample is supposed to be predicted on",
            "    # the boundary",
            "    sample_weight = [1] * 6",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.predict([[-1.0, 1.0]])",
            "    assert y_pred == pytest.approx(1.5)",
            "",
            "    # give more weights to opposed samples",
            "    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.predict([[-1.0, 1.0]])",
            "    assert y_pred < 1.5",
            "",
            "    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.predict([[-1.0, 1.0]])",
            "    assert y_pred > 1.5",
            "",
            "",
            "def test_svm_equivalence_sample_weight_C():",
            "    # test that rescaling all samples is the same as changing C",
            "    clf = svm.SVC()",
            "    clf.fit(X, Y)",
            "    dual_coef_no_weight = clf.dual_coef_",
            "    clf.set_params(C=100)",
            "    clf.fit(X, Y, sample_weight=np.repeat(0.01, len(X)))",
            "    assert_allclose(dual_coef_no_weight, clf.dual_coef_)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Estimator, err_msg\",",
            "    [",
            "        (svm.SVC, \"Invalid input - all samples have zero or negative weights.\"),",
            "        (svm.NuSVC, \"(negative dimensions are not allowed|nu is infeasible)\"),",
            "        (svm.SVR, \"Invalid input - all samples have zero or negative weights.\"),",
            "        (svm.NuSVR, \"Invalid input - all samples have zero or negative weights.\"),",
            "        (svm.OneClassSVM, \"Invalid input - all samples have zero or negative weights.\"),",
            "    ],",
            "    ids=[\"SVC\", \"NuSVC\", \"SVR\", \"NuSVR\", \"OneClassSVM\"],",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight\",",
            "    [[0] * len(Y), [-0.3] * len(Y)],",
            "    ids=[\"weights-are-zero\", \"weights-are-negative\"],",
            ")",
            "def test_negative_sample_weights_mask_all_samples(Estimator, err_msg, sample_weight):",
            "    est = Estimator(kernel=\"linear\")",
            "    with pytest.raises(ValueError, match=err_msg):",
            "        est.fit(X, Y, sample_weight=sample_weight)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Classifier, err_msg\",",
            "    [",
            "        (",
            "            svm.SVC,",
            "            \"Invalid input - all samples with positive weights have the same label\",",
            "        ),",
            "        (svm.NuSVC, \"specified nu is infeasible\"),",
            "    ],",
            "    ids=[\"SVC\", \"NuSVC\"],",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight\",",
            "    [[0, -0.5, 0, 1, 1, 1], [1, 1, 1, 0, -0.1, -0.3]],",
            "    ids=[\"mask-label-1\", \"mask-label-2\"],",
            ")",
            "def test_negative_weights_svc_leave_just_one_label(Classifier, err_msg, sample_weight):",
            "    clf = Classifier(kernel=\"linear\")",
            "    with pytest.raises(ValueError, match=err_msg):",
            "        clf.fit(X, Y, sample_weight=sample_weight)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Classifier, model\",",
            "    [",
            "        (svm.SVC, {\"when-left\": [0.3998, 0.4], \"when-right\": [0.4, 0.3999]}),",
            "        (svm.NuSVC, {\"when-left\": [0.3333, 0.3333], \"when-right\": [0.3333, 0.3333]}),",
            "    ],",
            "    ids=[\"SVC\", \"NuSVC\"],",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight, mask_side\",",
            "    [([1, -0.5, 1, 1, 1, 1], \"when-left\"), ([1, 1, 1, 0, 1, 1], \"when-right\")],",
            "    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],",
            ")",
            "def test_negative_weights_svc_leave_two_labels(",
            "    Classifier, model, sample_weight, mask_side",
            "):",
            "    clf = Classifier(kernel=\"linear\")",
            "    clf.fit(X, Y, sample_weight=sample_weight)",
            "    assert_allclose(clf.coef_, [model[mask_side]], rtol=1e-3)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Estimator\", [svm.SVC, svm.NuSVC, svm.NuSVR], ids=[\"SVC\", \"NuSVC\", \"NuSVR\"]",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight\",",
            "    [[1, -0.5, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1]],",
            "    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],",
            ")",
            "def test_negative_weight_equal_coeffs(Estimator, sample_weight):",
            "    # model generates equal coefficients",
            "    est = Estimator(kernel=\"linear\")",
            "    est.fit(X, Y, sample_weight=sample_weight)",
            "    coef = np.abs(est.coef_).ravel()",
            "    assert coef[0] == pytest.approx(coef[1], rel=1e-3)",
            "",
            "",
            "@ignore_warnings(category=UndefinedMetricWarning)",
            "def test_auto_weight():",
            "    # Test class weights for imbalanced data",
            "    from sklearn.linear_model import LogisticRegression",
            "",
            "    # We take as dataset the two-dimensional projection of iris so",
            "    # that it is not separable and remove half of predictors from",
            "    # class 1.",
            "    # We add one to the targets as a non-regression test:",
            "    # class_weight=\"balanced\"",
            "    # used to work only when the labels where a range [0..K).",
            "    from sklearn.utils import compute_class_weight",
            "",
            "    X, y = iris.data[:, :2], iris.target + 1",
            "    unbalanced = np.delete(np.arange(y.size), np.where(y > 2)[0][::2])",
            "",
            "    classes = np.unique(y[unbalanced])",
            "    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y[unbalanced])",
            "    assert np.argmax(class_weights) == 2",
            "",
            "    for clf in (",
            "        svm.SVC(kernel=\"linear\"),",
            "        svm.LinearSVC(random_state=0),",
            "        LogisticRegression(),",
            "    ):",
            "        # check that score is better when class='balanced' is set.",
            "        y_pred = clf.fit(X[unbalanced], y[unbalanced]).predict(X)",
            "        clf.set_params(class_weight=\"balanced\")",
            "        y_pred_balanced = clf.fit(",
            "            X[unbalanced],",
            "            y[unbalanced],",
            "        ).predict(X)",
            "        assert metrics.f1_score(y, y_pred, average=\"macro\") <= metrics.f1_score(",
            "            y, y_pred_balanced, average=\"macro\"",
            "        )",
            "",
            "",
            "def test_bad_input():",
            "    # Test that it gives proper exception on deficient input",
            "    # impossible value of C",
            "    with pytest.raises(ValueError):",
            "        svm.SVC(C=-1).fit(X, Y)",
            "",
            "    # impossible value of nu",
            "    clf = svm.NuSVC(nu=0.0)",
            "    with pytest.raises(ValueError):",
            "        clf.fit(X, Y)",
            "",
            "    Y2 = Y[:-1]  # wrong dimensions for labels",
            "    with pytest.raises(ValueError):",
            "        clf.fit(X, Y2)",
            "",
            "    # Test with arrays that are non-contiguous.",
            "    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):",
            "        Xf = np.asfortranarray(X)",
            "        assert not Xf.flags[\"C_CONTIGUOUS\"]",
            "        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)",
            "        yf = yf[:, -1]",
            "        assert not yf.flags[\"F_CONTIGUOUS\"]",
            "        assert not yf.flags[\"C_CONTIGUOUS\"]",
            "        clf.fit(Xf, yf)",
            "        assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # error for precomputed kernelsx",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    with pytest.raises(ValueError):",
            "        clf.fit(X, Y)",
            "",
            "    # predict with sparse input when trained with dense",
            "    clf = svm.SVC().fit(X, Y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(sparse.lil_matrix(X))",
            "",
            "    Xt = np.array(X).T",
            "    clf.fit(np.dot(X, Xt), Y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(X)",
            "",
            "    clf = svm.SVC()",
            "    clf.fit(X, Y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(Xt)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Estimator, data\",",
            "    [",
            "        (svm.SVC, datasets.load_iris(return_X_y=True)),",
            "        (svm.NuSVC, datasets.load_iris(return_X_y=True)),",
            "        (svm.SVR, datasets.load_diabetes(return_X_y=True)),",
            "        (svm.NuSVR, datasets.load_diabetes(return_X_y=True)),",
            "        (svm.OneClassSVM, datasets.load_iris(return_X_y=True)),",
            "    ],",
            ")",
            "def test_svm_gamma_error(Estimator, data):",
            "    X, y = data",
            "    est = Estimator(gamma=\"auto_deprecated\")",
            "    err_msg = \"When 'gamma' is a string, it should be either 'scale' or 'auto'\"",
            "    with pytest.raises(ValueError, match=err_msg):",
            "        est.fit(X, y)",
            "",
            "",
            "def test_unicode_kernel():",
            "    # Test that a unicode kernel name does not cause a TypeError",
            "    clf = svm.SVC(kernel=\"linear\", probability=True)",
            "    clf.fit(X, Y)",
            "    clf.predict_proba(T)",
            "    _libsvm.cross_validation(",
            "        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0",
            "    )",
            "",
            "",
            "def test_sparse_precomputed():",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    sparse_gram = sparse.csr_matrix([[1, 0], [0, 1]])",
            "    with pytest.raises(TypeError, match=\"Sparse precomputed\"):",
            "        clf.fit(sparse_gram, [0, 1])",
            "",
            "",
            "def test_sparse_fit_support_vectors_empty():",
            "    # Regression test for #14893",
            "    X_train = sparse.csr_matrix(",
            "        [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]]",
            "    )",
            "    y_train = np.array([0.04, 0.04, 0.10, 0.16])",
            "    model = svm.SVR(kernel=\"linear\")",
            "    model.fit(X_train, y_train)",
            "    assert not model.support_vectors_.data.size",
            "    assert not model.dual_coef_.data.size",
            "",
            "",
            "def test_linearsvc_parameters():",
            "    # Test possible parameter combinations in LinearSVC",
            "    # Generate list of possible parameter combinations",
            "    losses = [\"hinge\", \"squared_hinge\", \"logistic_regression\", \"foo\"]",
            "    penalties, duals = [\"l1\", \"l2\", \"bar\"], [True, False]",
            "",
            "    X, y = make_classification(n_samples=5, n_features=5)",
            "",
            "    for loss, penalty, dual in itertools.product(losses, penalties, duals):",
            "        clf = svm.LinearSVC(penalty=penalty, loss=loss, dual=dual)",
            "        if (",
            "            (loss, penalty) == (\"hinge\", \"l1\")",
            "            or (loss, penalty, dual) == (\"hinge\", \"l2\", False)",
            "            or (penalty, dual) == (\"l1\", True)",
            "            or loss == \"foo\"",
            "            or penalty == \"bar\"",
            "        ):",
            "",
            "            with pytest.raises(",
            "                ValueError,",
            "                match=\"Unsupported set of arguments.*penalty='%s.*loss='%s.*dual=%s\"",
            "                % (penalty, loss, dual),",
            "            ):",
            "                clf.fit(X, y)",
            "        else:",
            "            clf.fit(X, y)",
            "",
            "    # Incorrect loss value - test if explicit error message is raised",
            "    with pytest.raises(ValueError, match=\".*loss='l3' is not supported.*\"):",
            "        svm.LinearSVC(loss=\"l3\").fit(X, y)",
            "",
            "",
            "def test_linear_svx_uppercase_loss_penality_raises_error():",
            "    # Check if Upper case notation raises error at _fit_liblinear",
            "    # which is called by fit",
            "",
            "    X, y = [[0.0], [1.0]], [0, 1]",
            "",
            "    msg = \"loss='SQuared_hinge' is not supported\"",
            "    with pytest.raises(ValueError, match=msg):",
            "        svm.LinearSVC(loss=\"SQuared_hinge\").fit(X, y)",
            "",
            "    msg = \"The combination of penalty='L2' and loss='squared_hinge' is not supported\"",
            "    with pytest.raises(ValueError, match=msg):",
            "        svm.LinearSVC(penalty=\"L2\").fit(X, y)",
            "",
            "",
            "def test_linearsvc():",
            "    # Test basic routines using LinearSVC",
            "    clf = svm.LinearSVC(random_state=0).fit(X, Y)",
            "",
            "    # by default should have intercept",
            "    assert clf.fit_intercept",
            "",
            "    assert_array_equal(clf.predict(T), true_result)",
            "    assert_array_almost_equal(clf.intercept_, [0], decimal=3)",
            "",
            "    # the same with l1 penalty",
            "    clf = svm.LinearSVC(",
            "        penalty=\"l1\", loss=\"squared_hinge\", dual=False, random_state=0",
            "    ).fit(X, Y)",
            "    assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # l2 penalty with dual formulation",
            "    clf = svm.LinearSVC(penalty=\"l2\", dual=True, random_state=0).fit(X, Y)",
            "    assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # l2 penalty, l1 loss",
            "    clf = svm.LinearSVC(penalty=\"l2\", loss=\"hinge\", dual=True, random_state=0)",
            "    clf.fit(X, Y)",
            "    assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # test also decision function",
            "    dec = clf.decision_function(T)",
            "    res = (dec > 0).astype(int) + 1",
            "    assert_array_equal(res, true_result)",
            "",
            "",
            "def test_linearsvc_crammer_singer():",
            "    # Test LinearSVC with crammer_singer multi-class svm",
            "    ovr_clf = svm.LinearSVC(random_state=0).fit(iris.data, iris.target)",
            "    cs_clf = svm.LinearSVC(multi_class=\"crammer_singer\", random_state=0)",
            "    cs_clf.fit(iris.data, iris.target)",
            "",
            "    # similar prediction for ovr and crammer-singer:",
            "    assert (ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() > 0.9",
            "",
            "    # classifiers shouldn't be the same",
            "    assert (ovr_clf.coef_ != cs_clf.coef_).all()",
            "",
            "    # test decision function",
            "    assert_array_equal(",
            "        cs_clf.predict(iris.data),",
            "        np.argmax(cs_clf.decision_function(iris.data), axis=1),",
            "    )",
            "    dec_func = np.dot(iris.data, cs_clf.coef_.T) + cs_clf.intercept_",
            "    assert_array_almost_equal(dec_func, cs_clf.decision_function(iris.data))",
            "",
            "",
            "def test_linearsvc_fit_sampleweight():",
            "    # check correct result when sample_weight is 1",
            "    n_samples = len(X)",
            "    unit_weight = np.ones(n_samples)",
            "    clf = svm.LinearSVC(random_state=0).fit(X, Y)",
            "    clf_unitweight = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(",
            "        X, Y, sample_weight=unit_weight",
            "    )",
            "",
            "    # check if same as sample_weight=None",
            "    assert_array_equal(clf_unitweight.predict(T), clf.predict(T))",
            "    assert_allclose(clf.coef_, clf_unitweight.coef_, 1, 0.0001)",
            "",
            "    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",
            "    # X = X1 repeated n1 times, X2 repeated n2 times and so forth",
            "",
            "    random_state = check_random_state(0)",
            "    random_weight = random_state.randint(0, 10, n_samples)",
            "    lsvc_unflat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(",
            "        X, Y, sample_weight=random_weight",
            "    )",
            "    pred1 = lsvc_unflat.predict(T)",
            "",
            "    X_flat = np.repeat(X, random_weight, axis=0)",
            "    y_flat = np.repeat(Y, random_weight, axis=0)",
            "    lsvc_flat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(",
            "        X_flat, y_flat",
            "    )",
            "    pred2 = lsvc_flat.predict(T)",
            "",
            "    assert_array_equal(pred1, pred2)",
            "    assert_allclose(lsvc_unflat.coef_, lsvc_flat.coef_, 1, 0.0001)",
            "",
            "",
            "def test_crammer_singer_binary():",
            "    # Test Crammer-Singer formulation in the binary case",
            "    X, y = make_classification(n_classes=2, random_state=0)",
            "",
            "    for fit_intercept in (True, False):",
            "        acc = (",
            "            svm.LinearSVC(",
            "                fit_intercept=fit_intercept,",
            "                multi_class=\"crammer_singer\",",
            "                random_state=0,",
            "            )",
            "            .fit(X, y)",
            "            .score(X, y)",
            "        )",
            "        assert acc > 0.9",
            "",
            "",
            "def test_linearsvc_iris():",
            "    # Test that LinearSVC gives plausible predictions on the iris dataset",
            "    # Also, test symbolic class names (classes_).",
            "    target = iris.target_names[iris.target]",
            "    clf = svm.LinearSVC(random_state=0).fit(iris.data, target)",
            "    assert set(clf.classes_) == set(iris.target_names)",
            "    assert np.mean(clf.predict(iris.data) == target) > 0.8",
            "",
            "    dec = clf.decision_function(iris.data)",
            "    pred = iris.target_names[np.argmax(dec, 1)]",
            "    assert_array_equal(pred, clf.predict(iris.data))",
            "",
            "",
            "def test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC):",
            "    # Test that dense liblinear honours intercept_scaling param",
            "    X = [[2, 1], [3, 1], [1, 3], [2, 3]]",
            "    y = [0, 0, 1, 1]",
            "    clf = classifier(",
            "        fit_intercept=True,",
            "        penalty=\"l1\",",
            "        loss=\"squared_hinge\",",
            "        dual=False,",
            "        C=4,",
            "        tol=1e-7,",
            "        random_state=0,",
            "    )",
            "    assert clf.intercept_scaling == 1, clf.intercept_scaling",
            "    assert clf.fit_intercept",
            "",
            "    # when intercept_scaling is low the intercept value is highly \"penalized\"",
            "    # by regularization",
            "    clf.intercept_scaling = 1",
            "    clf.fit(X, y)",
            "    assert_almost_equal(clf.intercept_, 0, decimal=5)",
            "",
            "    # when intercept_scaling is sufficiently high, the intercept value",
            "    # is not affected by regularization",
            "    clf.intercept_scaling = 100",
            "    clf.fit(X, y)",
            "    intercept1 = clf.intercept_",
            "    assert intercept1 < -1",
            "",
            "    # when intercept_scaling is sufficiently high, the intercept value",
            "    # doesn't depend on intercept_scaling value",
            "    clf.intercept_scaling = 1000",
            "    clf.fit(X, y)",
            "    intercept2 = clf.intercept_",
            "    assert_array_almost_equal(intercept1, intercept2, decimal=2)",
            "",
            "",
            "def test_liblinear_set_coef():",
            "    # multi-class case",
            "    clf = svm.LinearSVC().fit(iris.data, iris.target)",
            "    values = clf.decision_function(iris.data)",
            "    clf.coef_ = clf.coef_.copy()",
            "    clf.intercept_ = clf.intercept_.copy()",
            "    values2 = clf.decision_function(iris.data)",
            "    assert_array_almost_equal(values, values2)",
            "",
            "    # binary-class case",
            "    X = [[2, 1], [3, 1], [1, 3], [2, 3]]",
            "    y = [0, 0, 1, 1]",
            "",
            "    clf = svm.LinearSVC().fit(X, y)",
            "    values = clf.decision_function(X)",
            "    clf.coef_ = clf.coef_.copy()",
            "    clf.intercept_ = clf.intercept_.copy()",
            "    values2 = clf.decision_function(X)",
            "    assert_array_equal(values, values2)",
            "",
            "",
            "def test_immutable_coef_property():",
            "    # Check that primal coef modification are not silently ignored",
            "    svms = [",
            "        svm.SVC(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.NuSVC(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.SVR(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.NuSVR(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.OneClassSVM(kernel=\"linear\").fit(iris.data),",
            "    ]",
            "    for clf in svms:",
            "        with pytest.raises(AttributeError):",
            "            clf.__setattr__(\"coef_\", np.arange(3))",
            "        with pytest.raises((RuntimeError, ValueError)):",
            "            clf.coef_.__setitem__((0, 0), 0)",
            "",
            "",
            "def test_linearsvc_verbose():",
            "    # stdout: redirect",
            "    import os",
            "",
            "    stdout = os.dup(1)  # save original stdout",
            "    os.dup2(os.pipe()[1], 1)  # replace it",
            "",
            "    # actual call",
            "    clf = svm.LinearSVC(verbose=1)",
            "    clf.fit(X, Y)",
            "",
            "    # stdout: restore",
            "    os.dup2(stdout, 1)  # restore original stdout",
            "",
            "",
            "def test_svc_clone_with_callable_kernel():",
            "    # create SVM with callable linear kernel, check that results are the same",
            "    # as with built-in linear kernel",
            "    svm_callable = svm.SVC(",
            "        kernel=lambda x, y: np.dot(x, y.T),",
            "        probability=True,",
            "        random_state=0,",
            "        decision_function_shape=\"ovr\",",
            "    )",
            "    # clone for checking clonability with lambda functions..",
            "    svm_cloned = base.clone(svm_callable)",
            "    svm_cloned.fit(iris.data, iris.target)",
            "",
            "    svm_builtin = svm.SVC(",
            "        kernel=\"linear\", probability=True, random_state=0, decision_function_shape=\"ovr\"",
            "    )",
            "    svm_builtin.fit(iris.data, iris.target)",
            "",
            "    assert_array_almost_equal(svm_cloned.dual_coef_, svm_builtin.dual_coef_)",
            "    assert_array_almost_equal(svm_cloned.intercept_, svm_builtin.intercept_)",
            "    assert_array_equal(svm_cloned.predict(iris.data), svm_builtin.predict(iris.data))",
            "",
            "    assert_array_almost_equal(",
            "        svm_cloned.predict_proba(iris.data),",
            "        svm_builtin.predict_proba(iris.data),",
            "        decimal=4,",
            "    )",
            "    assert_array_almost_equal(",
            "        svm_cloned.decision_function(iris.data),",
            "        svm_builtin.decision_function(iris.data),",
            "    )",
            "",
            "",
            "def test_svc_bad_kernel():",
            "    svc = svm.SVC(kernel=lambda x, y: x)",
            "    with pytest.raises(ValueError):",
            "        svc.fit(X, Y)",
            "",
            "",
            "def test_timeout():",
            "    a = svm.SVC(",
            "        kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0, max_iter=1",
            "    )",
            "    warning_msg = (",
            "        r\"Solver terminated early \\(max_iter=1\\).  Consider pre-processing \"",
            "        r\"your data with StandardScaler or MinMaxScaler.\"",
            "    )",
            "    with pytest.warns(ConvergenceWarning, match=warning_msg):",
            "        a.fit(np.array(X), Y)",
            "",
            "",
            "def test_unfitted():",
            "    X = \"foo!\"  # input validation not required when SVM not fitted",
            "",
            "    clf = svm.SVC()",
            "    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):",
            "        clf.predict(X)",
            "",
            "    clf = svm.NuSVR()",
            "    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):",
            "        clf.predict(X)",
            "",
            "",
            "# ignore convergence warnings from max_iter=1",
            "@ignore_warnings",
            "def test_consistent_proba():",
            "    a = svm.SVC(probability=True, max_iter=1, random_state=0)",
            "    proba_1 = a.fit(X, Y).predict_proba(X)",
            "    a = svm.SVC(probability=True, max_iter=1, random_state=0)",
            "    proba_2 = a.fit(X, Y).predict_proba(X)",
            "    assert_array_almost_equal(proba_1, proba_2)",
            "",
            "",
            "def test_linear_svm_convergence_warnings():",
            "    # Test that warnings are raised if model does not converge",
            "",
            "    lsvc = svm.LinearSVC(random_state=0, max_iter=2)",
            "    warning_msg = \"Liblinear failed to converge, increase the number of iterations.\"",
            "    with pytest.warns(ConvergenceWarning, match=warning_msg):",
            "        lsvc.fit(X, Y)",
            "    assert lsvc.n_iter_ == 2",
            "",
            "    lsvr = svm.LinearSVR(random_state=0, max_iter=2)",
            "    with pytest.warns(ConvergenceWarning, match=warning_msg):",
            "        lsvr.fit(iris.data, iris.target)",
            "    assert lsvr.n_iter_ == 2",
            "",
            "",
            "def test_svr_coef_sign():",
            "    # Test that SVR(kernel=\"linear\") has coef_ with the right sign.",
            "    # Non-regression test for #2933.",
            "    X = np.random.RandomState(21).randn(10, 3)",
            "    y = np.random.RandomState(12).randn(10)",
            "",
            "    for svr in [svm.SVR(kernel=\"linear\"), svm.NuSVR(kernel=\"linear\"), svm.LinearSVR()]:",
            "        svr.fit(X, y)",
            "        assert_array_almost_equal(",
            "            svr.predict(X), np.dot(X, svr.coef_.ravel()) + svr.intercept_",
            "        )",
            "",
            "",
            "def test_linear_svc_intercept_scaling():",
            "    # Test that the right error message is thrown when intercept_scaling <= 0",
            "",
            "    for i in [-1, 0]:",
            "        lsvc = svm.LinearSVC(intercept_scaling=i)",
            "",
            "        msg = (",
            "            \"Intercept scaling is %r but needs to be greater than 0.\"",
            "            \" To disable fitting an intercept,\"",
            "            \" set fit_intercept=False.\"",
            "            % lsvc.intercept_scaling",
            "        )",
            "        with pytest.raises(ValueError, match=msg):",
            "            lsvc.fit(X, Y)",
            "",
            "",
            "def test_lsvc_intercept_scaling_zero():",
            "    # Test that intercept_scaling is ignored when fit_intercept is False",
            "",
            "    lsvc = svm.LinearSVC(fit_intercept=False)",
            "    lsvc.fit(X, Y)",
            "    assert lsvc.intercept_ == 0.0",
            "",
            "",
            "def test_hasattr_predict_proba():",
            "    # Method must be (un)available before or after fit, switched by",
            "    # `probability` param",
            "",
            "    G = svm.SVC(probability=True)",
            "    assert hasattr(G, \"predict_proba\")",
            "    G.fit(iris.data, iris.target)",
            "    assert hasattr(G, \"predict_proba\")",
            "",
            "    G = svm.SVC(probability=False)",
            "    assert not hasattr(G, \"predict_proba\")",
            "    G.fit(iris.data, iris.target)",
            "    assert not hasattr(G, \"predict_proba\")",
            "",
            "    # Switching to `probability=True` after fitting should make",
            "    # predict_proba available, but calling it must not work:",
            "    G.probability = True",
            "    assert hasattr(G, \"predict_proba\")",
            "    msg = \"predict_proba is not available when fitted with probability=False\"",
            "",
            "    with pytest.raises(NotFittedError, match=msg):",
            "        G.predict_proba(iris.data)",
            "",
            "",
            "def test_decision_function_shape_two_class():",
            "    for n_classes in [2, 3]:",
            "        X, y = make_blobs(centers=n_classes, random_state=0)",
            "        for estimator in [svm.SVC, svm.NuSVC]:",
            "            clf = OneVsRestClassifier(estimator(decision_function_shape=\"ovr\")).fit(",
            "                X, y",
            "            )",
            "            assert len(clf.predict(X)) == len(y)",
            "",
            "",
            "def test_ovr_decision_function():",
            "    # One point from each quadrant represents one class",
            "    X_train = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])",
            "    y_train = [0, 1, 2, 3]",
            "",
            "    # First point is closer to the decision boundaries than the second point",
            "    base_points = np.array([[5, 5], [10, 10]])",
            "",
            "    # For all the quadrants (classes)",
            "    X_test = np.vstack(",
            "        (",
            "            base_points * [1, 1],  # Q1",
            "            base_points * [-1, 1],  # Q2",
            "            base_points * [-1, -1],  # Q3",
            "            base_points * [1, -1],  # Q4",
            "        )",
            "    )",
            "",
            "    y_test = [0] * 2 + [1] * 2 + [2] * 2 + [3] * 2",
            "",
            "    clf = svm.SVC(kernel=\"linear\", decision_function_shape=\"ovr\")",
            "    clf.fit(X_train, y_train)",
            "",
            "    y_pred = clf.predict(X_test)",
            "",
            "    # Test if the prediction is the same as y",
            "    assert_array_equal(y_pred, y_test)",
            "",
            "    deci_val = clf.decision_function(X_test)",
            "",
            "    # Assert that the predicted class has the maximum value",
            "    assert_array_equal(np.argmax(deci_val, axis=1), y_pred)",
            "",
            "    # Get decision value at test points for the predicted class",
            "    pred_class_deci_val = deci_val[range(8), y_pred].reshape((4, 2))",
            "",
            "    # Assert pred_class_deci_val > 0 here",
            "    assert np.min(pred_class_deci_val) > 0.0",
            "",
            "    # Test if the first point has lower decision value on every quadrant",
            "    # compared to the second point",
            "    assert np.all(pred_class_deci_val[:, 0] < pred_class_deci_val[:, 1])",
            "",
            "",
            "@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])",
            "def test_svc_invalid_break_ties_param(SVCClass):",
            "    X, y = make_blobs(random_state=42)",
            "",
            "    svm = SVCClass(",
            "        kernel=\"linear\", decision_function_shape=\"ovo\", break_ties=True, random_state=42",
            "    ).fit(X, y)",
            "",
            "    with pytest.raises(ValueError, match=\"break_ties must be False\"):",
            "        svm.predict(y)",
            "",
            "",
            "@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])",
            "def test_svc_ovr_tie_breaking(SVCClass):",
            "    \"\"\"Test if predict breaks ties in OVR mode.",
            "    Related issue: https://github.com/scikit-learn/scikit-learn/issues/8277",
            "    \"\"\"",
            "    X, y = make_blobs(random_state=27)",
            "",
            "    xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 1000)",
            "    ys = np.linspace(X[:, 1].min(), X[:, 1].max(), 1000)",
            "    xx, yy = np.meshgrid(xs, ys)",
            "",
            "    svm = SVCClass(",
            "        kernel=\"linear\",",
            "        decision_function_shape=\"ovr\",",
            "        break_ties=False,",
            "        random_state=42,",
            "    ).fit(X, y)",
            "    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])",
            "    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])",
            "    assert not np.all(pred == np.argmax(dv, axis=1))",
            "",
            "    svm = SVCClass(",
            "        kernel=\"linear\", decision_function_shape=\"ovr\", break_ties=True, random_state=42",
            "    ).fit(X, y)",
            "    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])",
            "    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])",
            "    assert np.all(pred == np.argmax(dv, axis=1))",
            "",
            "",
            "def test_gamma_auto():",
            "    X, y = [[0.0, 1.2], [1.0, 1.3]], [0, 1]",
            "",
            "    with pytest.warns(None) as record:",
            "        svm.SVC(kernel=\"linear\").fit(X, y)",
            "    assert not len(record)",
            "",
            "    with pytest.warns(None) as record:",
            "        svm.SVC(kernel=\"precomputed\").fit(X, y)",
            "    assert not len(record)",
            "",
            "",
            "def test_gamma_scale():",
            "    X, y = [[0.0], [1.0]], [0, 1]",
            "",
            "    clf = svm.SVC()",
            "    with pytest.warns(None) as record:",
            "        clf.fit(X, y)",
            "    assert not len(record)",
            "    assert_almost_equal(clf._gamma, 4)",
            "",
            "    # X_var ~= 1 shouldn't raise warning, for when",
            "    # gamma is not explicitly set.",
            "    X, y = [[1, 2], [3, 2 * np.sqrt(6) / 3 + 2]], [0, 1]",
            "    with pytest.warns(None) as record:",
            "        clf.fit(X, y)",
            "    assert not len(record)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"SVM, params\",",
            "    [",
            "        (LinearSVC, {\"penalty\": \"l1\", \"loss\": \"squared_hinge\", \"dual\": False}),",
            "        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": True}),",
            "        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": False}),",
            "        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"hinge\", \"dual\": True}),",
            "        (LinearSVR, {\"loss\": \"epsilon_insensitive\", \"dual\": True}),",
            "        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),",
            "        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),",
            "    ],",
            ")",
            "def test_linearsvm_liblinear_sample_weight(SVM, params):",
            "    X = np.array(",
            "        [",
            "            [1, 3],",
            "            [1, 3],",
            "            [1, 3],",
            "            [1, 3],",
            "            [2, 1],",
            "            [2, 1],",
            "            [2, 1],",
            "            [2, 1],",
            "            [3, 3],",
            "            [3, 3],",
            "            [3, 3],",
            "            [3, 3],",
            "            [4, 1],",
            "            [4, 1],",
            "            [4, 1],",
            "            [4, 1],",
            "        ],",
            "        dtype=np.dtype(\"float\"),",
            "    )",
            "    y = np.array(",
            "        [1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=np.dtype(\"int\")",
            "    )",
            "",
            "    X2 = np.vstack([X, X])",
            "    y2 = np.hstack([y, 3 - y])",
            "    sample_weight = np.ones(shape=len(y) * 2)",
            "    sample_weight[len(y) :] = 0",
            "    X2, y2, sample_weight = shuffle(X2, y2, sample_weight, random_state=0)",
            "",
            "    base_estimator = SVM(random_state=42)",
            "    base_estimator.set_params(**params)",
            "    base_estimator.set_params(tol=1e-12, max_iter=1000)",
            "    est_no_weight = base.clone(base_estimator).fit(X, y)",
            "    est_with_weight = base.clone(base_estimator).fit(",
            "        X2, y2, sample_weight=sample_weight",
            "    )",
            "",
            "    for method in (\"predict\", \"decision_function\"):",
            "        if hasattr(base_estimator, method):",
            "            X_est_no_weight = getattr(est_no_weight, method)(X)",
            "            X_est_with_weight = getattr(est_with_weight, method)(X)",
            "            assert_allclose(X_est_no_weight, X_est_with_weight)",
            "",
            "",
            "def test_n_support_oneclass_svr():",
            "    # Make n_support is correct for oneclass and SVR (used to be",
            "    # non-initialized)",
            "    # this is a non regression test for issue #14774",
            "    X = np.array([[0], [0.44], [0.45], [0.46], [1]])",
            "    clf = svm.OneClassSVM()",
            "    assert not hasattr(clf, \"n_support_\")",
            "    clf.fit(X)",
            "    assert clf.n_support_ == clf.support_vectors_.shape[0]",
            "    assert clf.n_support_.size == 1",
            "    assert clf.n_support_ == 3",
            "",
            "    y = np.arange(X.shape[0])",
            "    reg = svm.SVR().fit(X, y)",
            "    assert reg.n_support_ == reg.support_vectors_.shape[0]",
            "    assert reg.n_support_.size == 1",
            "    assert reg.n_support_ == 4",
            "",
            "",
            "@pytest.mark.parametrize(\"Estimator\", [svm.SVC, svm.SVR])",
            "def test_custom_kernel_not_array_input(Estimator):",
            "    \"\"\"Test using a custom kernel that is not fed with array-like for floats\"\"\"",
            "    data = [\"A A\", \"A\", \"B\", \"B B\", \"A B\"]",
            "    X = np.array([[2, 0], [1, 0], [0, 1], [0, 2], [1, 1]])  # count encoding",
            "    y = np.array([1, 1, 2, 2, 1])",
            "",
            "    def string_kernel(X1, X2):",
            "        assert isinstance(X1[0], str)",
            "        n_samples1 = _num_samples(X1)",
            "        n_samples2 = _num_samples(X2)",
            "        K = np.zeros((n_samples1, n_samples2))",
            "        for ii in range(n_samples1):",
            "            for jj in range(ii, n_samples2):",
            "                K[ii, jj] = X1[ii].count(\"A\") * X2[jj].count(\"A\")",
            "                K[ii, jj] += X1[ii].count(\"B\") * X2[jj].count(\"B\")",
            "                K[jj, ii] = K[ii, jj]",
            "        return K",
            "",
            "    K = string_kernel(data, data)",
            "    assert_array_equal(np.dot(X, X.T), K)",
            "",
            "    svc1 = Estimator(kernel=string_kernel).fit(data, y)",
            "    svc2 = Estimator(kernel=\"linear\").fit(X, y)",
            "    svc3 = Estimator(kernel=\"precomputed\").fit(K, y)",
            "",
            "    assert svc1.score(data, y) == svc3.score(K, y)",
            "    assert svc1.score(data, y) == svc2.score(X, y)",
            "    if hasattr(svc1, \"decision_function\"):  # classifier",
            "        assert_allclose(svc1.decision_function(data), svc2.decision_function(X))",
            "        assert_allclose(svc1.decision_function(data), svc3.decision_function(K))",
            "        assert_array_equal(svc1.predict(data), svc2.predict(X))",
            "        assert_array_equal(svc1.predict(data), svc3.predict(K))",
            "    else:  # regressor",
            "        assert_allclose(svc1.predict(data), svc2.predict(X))",
            "        assert_allclose(svc1.predict(data), svc3.predict(K))"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "Testing for Support Vector Machine module (sklearn.svm)",
            "",
            "TODO: remove hard coded numerical results when possible",
            "\"\"\"",
            "import numpy as np",
            "import itertools",
            "import pytest",
            "import re",
            "",
            "from numpy.testing import assert_array_equal, assert_array_almost_equal",
            "from numpy.testing import assert_almost_equal",
            "from numpy.testing import assert_allclose",
            "from scipy import sparse",
            "from sklearn import svm, linear_model, datasets, metrics, base",
            "from sklearn.svm import LinearSVC",
            "from sklearn.svm import LinearSVR",
            "from sklearn.model_selection import train_test_split",
            "from sklearn.datasets import make_classification, make_blobs",
            "from sklearn.metrics import f1_score",
            "from sklearn.metrics.pairwise import rbf_kernel",
            "from sklearn.utils import check_random_state",
            "from sklearn.utils._testing import ignore_warnings",
            "from sklearn.utils.validation import _num_samples",
            "from sklearn.utils import shuffle",
            "from sklearn.exceptions import ConvergenceWarning",
            "from sklearn.exceptions import NotFittedError, UndefinedMetricWarning",
            "from sklearn.multiclass import OneVsRestClassifier",
            "",
            "# mypy error: Module 'sklearn.svm' has no attribute '_libsvm'",
            "from sklearn.svm import _libsvm  # type: ignore",
            "",
            "# toy sample",
            "X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]",
            "Y = [1, 1, 1, 2, 2, 2]",
            "T = [[-1, -1], [2, 2], [3, 2]]",
            "true_result = [1, 2, 2]",
            "",
            "# also load the iris dataset",
            "iris = datasets.load_iris()",
            "rng = check_random_state(42)",
            "perm = rng.permutation(iris.target.size)",
            "iris.data = iris.data[perm]",
            "iris.target = iris.target[perm]",
            "",
            "",
            "def test_libsvm_parameters():",
            "    # Test parameters on classes that make use of libsvm.",
            "    clf = svm.SVC(kernel=\"linear\").fit(X, Y)",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.support_, [1, 3])",
            "    assert_array_equal(clf.support_vectors_, (X[1], X[3]))",
            "    assert_array_equal(clf.intercept_, [0.0])",
            "    assert_array_equal(clf.predict(X), Y)",
            "",
            "",
            "def test_libsvm_iris():",
            "    # Check consistency on dataset iris.",
            "",
            "    # shuffle the dataset so that labels are not ordered",
            "    for k in (\"linear\", \"rbf\"):",
            "        clf = svm.SVC(kernel=k).fit(iris.data, iris.target)",
            "        assert np.mean(clf.predict(iris.data) == iris.target) > 0.9",
            "        assert hasattr(clf, \"coef_\") == (k == \"linear\")",
            "",
            "    assert_array_equal(clf.classes_, np.sort(clf.classes_))",
            "",
            "    # check also the low-level API",
            "    model = _libsvm.fit(iris.data, iris.target.astype(np.float64))",
            "    pred = _libsvm.predict(iris.data, *model)",
            "    assert np.mean(pred == iris.target) > 0.95",
            "",
            "    model = _libsvm.fit(iris.data, iris.target.astype(np.float64), kernel=\"linear\")",
            "    pred = _libsvm.predict(iris.data, *model, kernel=\"linear\")",
            "    assert np.mean(pred == iris.target) > 0.95",
            "",
            "    pred = _libsvm.cross_validation(",
            "        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0",
            "    )",
            "    assert np.mean(pred == iris.target) > 0.95",
            "",
            "    # If random_seed >= 0, the libsvm rng is seeded (by calling `srand`), hence",
            "    # we should get deterministic results (assuming that there is no other",
            "    # thread calling this wrapper calling `srand` concurrently).",
            "    pred2 = _libsvm.cross_validation(",
            "        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0",
            "    )",
            "    assert_array_equal(pred, pred2)",
            "",
            "",
            "def test_precomputed():",
            "    # SVC with a precomputed kernel.",
            "    # We test it with a toy dataset and with iris.",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    # Gram matrix for train data (square matrix)",
            "    # (we use just a linear kernel)",
            "    K = np.dot(X, np.array(X).T)",
            "    clf.fit(K, Y)",
            "    # Gram matrix for test data (rectangular matrix)",
            "    KT = np.dot(T, np.array(X).T)",
            "    pred = clf.predict(KT)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(KT.T)",
            "",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.support_, [1, 3])",
            "    assert_array_equal(clf.intercept_, [0])",
            "    assert_array_almost_equal(clf.support_, [1, 3])",
            "    assert_array_equal(pred, true_result)",
            "",
            "    # Gram matrix for test data but compute KT[i,j]",
            "    # for support vectors j only.",
            "    KT = np.zeros_like(KT)",
            "    for i in range(len(T)):",
            "        for j in clf.support_:",
            "            KT[i, j] = np.dot(T[i], X[j])",
            "",
            "    pred = clf.predict(KT)",
            "    assert_array_equal(pred, true_result)",
            "",
            "    # same as before, but using a callable function instead of the kernel",
            "    # matrix. kernel is just a linear kernel",
            "",
            "    def kfunc(x, y):",
            "        return np.dot(x, y.T)",
            "",
            "    clf = svm.SVC(kernel=kfunc)",
            "    clf.fit(np.array(X), Y)",
            "    pred = clf.predict(T)",
            "",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.intercept_, [0])",
            "    assert_array_almost_equal(clf.support_, [1, 3])",
            "    assert_array_equal(pred, true_result)",
            "",
            "    # test a precomputed kernel with the iris dataset",
            "    # and check parameters against a linear SVC",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    clf2 = svm.SVC(kernel=\"linear\")",
            "    K = np.dot(iris.data, iris.data.T)",
            "    clf.fit(K, iris.target)",
            "    clf2.fit(iris.data, iris.target)",
            "    pred = clf.predict(K)",
            "    assert_array_almost_equal(clf.support_, clf2.support_)",
            "    assert_array_almost_equal(clf.dual_coef_, clf2.dual_coef_)",
            "    assert_array_almost_equal(clf.intercept_, clf2.intercept_)",
            "    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)",
            "",
            "    # Gram matrix for test data but compute KT[i,j]",
            "    # for support vectors j only.",
            "    K = np.zeros_like(K)",
            "    for i in range(len(iris.data)):",
            "        for j in clf.support_:",
            "            K[i, j] = np.dot(iris.data[i], iris.data[j])",
            "",
            "    pred = clf.predict(K)",
            "    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)",
            "",
            "    clf = svm.SVC(kernel=kfunc)",
            "    clf.fit(iris.data, iris.target)",
            "    assert_almost_equal(np.mean(pred == iris.target), 0.99, decimal=2)",
            "",
            "",
            "def test_svr():",
            "    # Test Support Vector Regression",
            "",
            "    diabetes = datasets.load_diabetes()",
            "    for clf in (",
            "        svm.NuSVR(kernel=\"linear\", nu=0.4, C=1.0),",
            "        svm.NuSVR(kernel=\"linear\", nu=0.4, C=10.0),",
            "        svm.SVR(kernel=\"linear\", C=10.0),",
            "        svm.LinearSVR(C=10.0),",
            "        svm.LinearSVR(C=10.0),",
            "    ):",
            "        clf.fit(diabetes.data, diabetes.target)",
            "        assert clf.score(diabetes.data, diabetes.target) > 0.02",
            "",
            "    # non-regression test; previously, BaseLibSVM would check that",
            "    # len(np.unique(y)) < 2, which must only be done for SVC",
            "    svm.SVR().fit(diabetes.data, np.ones(len(diabetes.data)))",
            "    svm.LinearSVR().fit(diabetes.data, np.ones(len(diabetes.data)))",
            "",
            "",
            "def test_linearsvr():",
            "    # check that SVR(kernel='linear') and LinearSVC() give",
            "    # comparable results",
            "    diabetes = datasets.load_diabetes()",
            "    lsvr = svm.LinearSVR(C=1e3).fit(diabetes.data, diabetes.target)",
            "    score1 = lsvr.score(diabetes.data, diabetes.target)",
            "",
            "    svr = svm.SVR(kernel=\"linear\", C=1e3).fit(diabetes.data, diabetes.target)",
            "    score2 = svr.score(diabetes.data, diabetes.target)",
            "",
            "    assert_allclose(np.linalg.norm(lsvr.coef_), np.linalg.norm(svr.coef_), 1, 0.0001)",
            "    assert_almost_equal(score1, score2, 2)",
            "",
            "",
            "def test_linearsvr_fit_sampleweight():",
            "    # check correct result when sample_weight is 1",
            "    # check that SVR(kernel='linear') and LinearSVC() give",
            "    # comparable results",
            "    diabetes = datasets.load_diabetes()",
            "    n_samples = len(diabetes.target)",
            "    unit_weight = np.ones(n_samples)",
            "    lsvr = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(",
            "        diabetes.data, diabetes.target, sample_weight=unit_weight",
            "    )",
            "    score1 = lsvr.score(diabetes.data, diabetes.target)",
            "",
            "    lsvr_no_weight = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(",
            "        diabetes.data, diabetes.target",
            "    )",
            "    score2 = lsvr_no_weight.score(diabetes.data, diabetes.target)",
            "",
            "    assert_allclose(",
            "        np.linalg.norm(lsvr.coef_), np.linalg.norm(lsvr_no_weight.coef_), 1, 0.0001",
            "    )",
            "    assert_almost_equal(score1, score2, 2)",
            "",
            "    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",
            "    # X = X1 repeated n1 times, X2 repeated n2 times and so forth",
            "    random_state = check_random_state(0)",
            "    random_weight = random_state.randint(0, 10, n_samples)",
            "    lsvr_unflat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(",
            "        diabetes.data, diabetes.target, sample_weight=random_weight",
            "    )",
            "    score3 = lsvr_unflat.score(",
            "        diabetes.data, diabetes.target, sample_weight=random_weight",
            "    )",
            "",
            "    X_flat = np.repeat(diabetes.data, random_weight, axis=0)",
            "    y_flat = np.repeat(diabetes.target, random_weight, axis=0)",
            "    lsvr_flat = svm.LinearSVR(C=1e3, tol=1e-12, max_iter=10000).fit(X_flat, y_flat)",
            "    score4 = lsvr_flat.score(X_flat, y_flat)",
            "",
            "    assert_almost_equal(score3, score4, 2)",
            "",
            "",
            "def test_svr_errors():",
            "    X = [[0.0], [1.0]]",
            "    y = [0.0, 0.5]",
            "",
            "    # Bad kernel",
            "    clf = svm.SVR(kernel=lambda x, y: np.array([[1.0]]))",
            "    clf.fit(X, y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(X)",
            "",
            "",
            "def test_oneclass():",
            "    # Test OneClassSVM",
            "    clf = svm.OneClassSVM()",
            "    clf.fit(X)",
            "    pred = clf.predict(T)",
            "",
            "    assert_array_equal(pred, [1, -1, -1])",
            "    assert pred.dtype == np.dtype(\"intp\")",
            "    assert_array_almost_equal(clf.intercept_, [-1.218], decimal=3)",
            "    assert_array_almost_equal(clf.dual_coef_, [[0.750, 0.750, 0.750, 0.750]], decimal=3)",
            "    with pytest.raises(AttributeError):",
            "        (lambda: clf.coef_)()",
            "",
            "",
            "def test_oneclass_decision_function():",
            "    # Test OneClassSVM decision function",
            "    clf = svm.OneClassSVM()",
            "    rnd = check_random_state(2)",
            "",
            "    # Generate train data",
            "    X = 0.3 * rnd.randn(100, 2)",
            "    X_train = np.r_[X + 2, X - 2]",
            "",
            "    # Generate some regular novel observations",
            "    X = 0.3 * rnd.randn(20, 2)",
            "    X_test = np.r_[X + 2, X - 2]",
            "    # Generate some abnormal novel observations",
            "    X_outliers = rnd.uniform(low=-4, high=4, size=(20, 2))",
            "",
            "    # fit the model",
            "    clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)",
            "    clf.fit(X_train)",
            "",
            "    # predict things",
            "    y_pred_test = clf.predict(X_test)",
            "    assert np.mean(y_pred_test == 1) > 0.9",
            "    y_pred_outliers = clf.predict(X_outliers)",
            "    assert np.mean(y_pred_outliers == -1) > 0.9",
            "    dec_func_test = clf.decision_function(X_test)",
            "    assert_array_equal((dec_func_test > 0).ravel(), y_pred_test == 1)",
            "    dec_func_outliers = clf.decision_function(X_outliers)",
            "    assert_array_equal((dec_func_outliers > 0).ravel(), y_pred_outliers == 1)",
            "",
            "",
            "def test_oneclass_score_samples():",
            "    X_train = [[1, 1], [1, 2], [2, 1]]",
            "    clf = svm.OneClassSVM(gamma=1).fit(X_train)",
            "    assert_array_equal(",
            "        clf.score_samples([[2.0, 2.0]]),",
            "        clf.decision_function([[2.0, 2.0]]) + clf.offset_,",
            "    )",
            "",
            "",
            "# TODO: Remove in v1.2",
            "def test_oneclass_fit_params_is_deprecated():",
            "    clf = svm.OneClassSVM()",
            "    params = {",
            "        \"unused_param\": \"\",",
            "        \"extra_param\": None,",
            "    }",
            "    msg = (",
            "        \"Passing additional keyword parameters has no effect and is deprecated \"",
            "        \"in 1.0. An error will be raised from 1.2 and beyond. The ignored \"",
            "        f\"keyword parameter(s) are: {params.keys()}.\"",
            "    )",
            "    with pytest.warns(FutureWarning, match=re.escape(msg)):",
            "        clf.fit(X, **params)",
            "",
            "",
            "def test_tweak_params():",
            "    # Make sure some tweaking of parameters works.",
            "    # We change clf.dual_coef_ at run time and expect .predict() to change",
            "    # accordingly. Notice that this is not trivial since it involves a lot",
            "    # of C/Python copying in the libsvm bindings.",
            "    # The success of this test ensures that the mapping between libsvm and",
            "    # the python classifier is complete.",
            "    clf = svm.SVC(kernel=\"linear\", C=1.0)",
            "    clf.fit(X, Y)",
            "    assert_array_equal(clf.dual_coef_, [[-0.25, 0.25]])",
            "    assert_array_equal(clf.predict([[-0.1, -0.1]]), [1])",
            "    clf._dual_coef_ = np.array([[0.0, 1.0]])",
            "    assert_array_equal(clf.predict([[-0.1, -0.1]]), [2])",
            "",
            "",
            "def test_probability():",
            "    # Predict probabilities using SVC",
            "    # This uses cross validation, so we use a slightly bigger testing set.",
            "",
            "    for clf in (",
            "        svm.SVC(probability=True, random_state=0, C=1.0),",
            "        svm.NuSVC(probability=True, random_state=0),",
            "    ):",
            "        clf.fit(iris.data, iris.target)",
            "",
            "        prob_predict = clf.predict_proba(iris.data)",
            "        assert_array_almost_equal(np.sum(prob_predict, 1), np.ones(iris.data.shape[0]))",
            "        assert np.mean(np.argmax(prob_predict, 1) == clf.predict(iris.data)) > 0.9",
            "",
            "        assert_almost_equal(",
            "            clf.predict_proba(iris.data), np.exp(clf.predict_log_proba(iris.data)), 8",
            "        )",
            "",
            "",
            "def test_decision_function():",
            "    # Test decision_function",
            "    # Sanity check, test that decision_function implemented in python",
            "    # returns the same as the one in libsvm",
            "    # multi class:",
            "    clf = svm.SVC(kernel=\"linear\", C=0.1, decision_function_shape=\"ovo\").fit(",
            "        iris.data, iris.target",
            "    )",
            "",
            "    dec = np.dot(iris.data, clf.coef_.T) + clf.intercept_",
            "",
            "    assert_array_almost_equal(dec, clf.decision_function(iris.data))",
            "",
            "    # binary:",
            "    clf.fit(X, Y)",
            "    dec = np.dot(X, clf.coef_.T) + clf.intercept_",
            "    prediction = clf.predict(X)",
            "    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))",
            "    assert_array_almost_equal(",
            "        prediction, clf.classes_[(clf.decision_function(X) > 0).astype(int)]",
            "    )",
            "    expected = np.array([-1.0, -0.66, -1.0, 0.66, 1.0, 1.0])",
            "    assert_array_almost_equal(clf.decision_function(X), expected, 2)",
            "",
            "    # kernel binary:",
            "    clf = svm.SVC(kernel=\"rbf\", gamma=1, decision_function_shape=\"ovo\")",
            "    clf.fit(X, Y)",
            "",
            "    rbfs = rbf_kernel(X, clf.support_vectors_, gamma=clf.gamma)",
            "    dec = np.dot(rbfs, clf.dual_coef_.T) + clf.intercept_",
            "    assert_array_almost_equal(dec.ravel(), clf.decision_function(X))",
            "",
            "",
            "@pytest.mark.parametrize(\"SVM\", (svm.SVC, svm.NuSVC))",
            "def test_decision_function_shape(SVM):",
            "    # check that decision_function_shape='ovr' or 'ovo' gives",
            "    # correct shape and is consistent with predict",
            "",
            "    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(",
            "        iris.data, iris.target",
            "    )",
            "    dec = clf.decision_function(iris.data)",
            "    assert dec.shape == (len(iris.data), 3)",
            "    assert_array_equal(clf.predict(iris.data), np.argmax(dec, axis=1))",
            "",
            "    # with five classes:",
            "    X, y = make_blobs(n_samples=80, centers=5, random_state=0)",
            "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)",
            "",
            "    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovr\").fit(X_train, y_train)",
            "    dec = clf.decision_function(X_test)",
            "    assert dec.shape == (len(X_test), 5)",
            "    assert_array_equal(clf.predict(X_test), np.argmax(dec, axis=1))",
            "",
            "    # check shape of ovo_decition_function=True",
            "    clf = SVM(kernel=\"linear\", decision_function_shape=\"ovo\").fit(X_train, y_train)",
            "    dec = clf.decision_function(X_train)",
            "    assert dec.shape == (len(X_train), 10)",
            "",
            "    with pytest.raises(ValueError, match=\"must be either 'ovr' or 'ovo'\"):",
            "        SVM(decision_function_shape=\"bad\").fit(X_train, y_train)",
            "",
            "",
            "def test_svr_predict():",
            "    # Test SVR's decision_function",
            "    # Sanity check, test that predict implemented in python",
            "    # returns the same as the one in libsvm",
            "",
            "    X = iris.data",
            "    y = iris.target",
            "",
            "    # linear kernel",
            "    reg = svm.SVR(kernel=\"linear\", C=0.1).fit(X, y)",
            "",
            "    dec = np.dot(X, reg.coef_.T) + reg.intercept_",
            "    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())",
            "",
            "    # rbf kernel",
            "    reg = svm.SVR(kernel=\"rbf\", gamma=1).fit(X, y)",
            "",
            "    rbfs = rbf_kernel(X, reg.support_vectors_, gamma=reg.gamma)",
            "    dec = np.dot(rbfs, reg.dual_coef_.T) + reg.intercept_",
            "    assert_array_almost_equal(dec.ravel(), reg.predict(X).ravel())",
            "",
            "",
            "def test_weight():",
            "    # Test class weights",
            "    clf = svm.SVC(class_weight={1: 0.1})",
            "    # we give a small weights to class 1",
            "    clf.fit(X, Y)",
            "    # so all predicted values belong to class 2",
            "    assert_array_almost_equal(clf.predict(X), [2] * 6)",
            "",
            "    X_, y_ = make_classification(",
            "        n_samples=200, n_features=10, weights=[0.833, 0.167], random_state=2",
            "    )",
            "",
            "    for clf in (",
            "        linear_model.LogisticRegression(),",
            "        svm.LinearSVC(random_state=0),",
            "        svm.SVC(),",
            "    ):",
            "        clf.set_params(class_weight={0: 0.1, 1: 10})",
            "        clf.fit(X_[:100], y_[:100])",
            "        y_pred = clf.predict(X_[100:])",
            "        assert f1_score(y_[100:], y_pred) > 0.3",
            "",
            "",
            "@pytest.mark.parametrize(\"estimator\", [svm.SVC(C=1e-2), svm.NuSVC()])",
            "def test_svm_classifier_sided_sample_weight(estimator):",
            "    # fit a linear SVM and check that giving more weight to opposed samples",
            "    # in the space will flip the decision toward these samples.",
            "    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]",
            "    estimator.set_params(kernel=\"linear\")",
            "",
            "    # check that with unit weights, a sample is supposed to be predicted on",
            "    # the boundary",
            "    sample_weight = [1] * 6",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.decision_function([[-1.0, 1.0]])",
            "    assert y_pred == pytest.approx(0)",
            "",
            "    # give more weights to opposed samples",
            "    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.decision_function([[-1.0, 1.0]])",
            "    assert y_pred < 0",
            "",
            "    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.decision_function([[-1.0, 1.0]])",
            "    assert y_pred > 0",
            "",
            "",
            "@pytest.mark.parametrize(\"estimator\", [svm.SVR(C=1e-2), svm.NuSVR(C=1e-2)])",
            "def test_svm_regressor_sided_sample_weight(estimator):",
            "    # similar test to test_svm_classifier_sided_sample_weight but for",
            "    # SVM regressors",
            "    X = [[-2, 0], [-1, -1], [0, -2], [0, 2], [1, 1], [2, 0]]",
            "    estimator.set_params(kernel=\"linear\")",
            "",
            "    # check that with unit weights, a sample is supposed to be predicted on",
            "    # the boundary",
            "    sample_weight = [1] * 6",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.predict([[-1.0, 1.0]])",
            "    assert y_pred == pytest.approx(1.5)",
            "",
            "    # give more weights to opposed samples",
            "    sample_weight = [10.0, 0.1, 0.1, 0.1, 0.1, 10]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.predict([[-1.0, 1.0]])",
            "    assert y_pred < 1.5",
            "",
            "    sample_weight = [1.0, 0.1, 10.0, 10.0, 0.1, 0.1]",
            "    estimator.fit(X, Y, sample_weight=sample_weight)",
            "    y_pred = estimator.predict([[-1.0, 1.0]])",
            "    assert y_pred > 1.5",
            "",
            "",
            "def test_svm_equivalence_sample_weight_C():",
            "    # test that rescaling all samples is the same as changing C",
            "    clf = svm.SVC()",
            "    clf.fit(X, Y)",
            "    dual_coef_no_weight = clf.dual_coef_",
            "    clf.set_params(C=100)",
            "    clf.fit(X, Y, sample_weight=np.repeat(0.01, len(X)))",
            "    assert_allclose(dual_coef_no_weight, clf.dual_coef_)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Estimator, err_msg\",",
            "    [",
            "        (svm.SVC, \"Invalid input - all samples have zero or negative weights.\"),",
            "        (svm.NuSVC, \"(negative dimensions are not allowed|nu is infeasible)\"),",
            "        (svm.SVR, \"Invalid input - all samples have zero or negative weights.\"),",
            "        (svm.NuSVR, \"Invalid input - all samples have zero or negative weights.\"),",
            "        (svm.OneClassSVM, \"Invalid input - all samples have zero or negative weights.\"),",
            "    ],",
            "    ids=[\"SVC\", \"NuSVC\", \"SVR\", \"NuSVR\", \"OneClassSVM\"],",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight\",",
            "    [[0] * len(Y), [-0.3] * len(Y)],",
            "    ids=[\"weights-are-zero\", \"weights-are-negative\"],",
            ")",
            "def test_negative_sample_weights_mask_all_samples(Estimator, err_msg, sample_weight):",
            "    est = Estimator(kernel=\"linear\")",
            "    with pytest.raises(ValueError, match=err_msg):",
            "        est.fit(X, Y, sample_weight=sample_weight)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Classifier, err_msg\",",
            "    [",
            "        (",
            "            svm.SVC,",
            "            \"Invalid input - all samples with positive weights have the same label\",",
            "        ),",
            "        (svm.NuSVC, \"specified nu is infeasible\"),",
            "    ],",
            "    ids=[\"SVC\", \"NuSVC\"],",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight\",",
            "    [[0, -0.5, 0, 1, 1, 1], [1, 1, 1, 0, -0.1, -0.3]],",
            "    ids=[\"mask-label-1\", \"mask-label-2\"],",
            ")",
            "def test_negative_weights_svc_leave_just_one_label(Classifier, err_msg, sample_weight):",
            "    clf = Classifier(kernel=\"linear\")",
            "    with pytest.raises(ValueError, match=err_msg):",
            "        clf.fit(X, Y, sample_weight=sample_weight)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Classifier, model\",",
            "    [",
            "        (svm.SVC, {\"when-left\": [0.3998, 0.4], \"when-right\": [0.4, 0.3999]}),",
            "        (svm.NuSVC, {\"when-left\": [0.3333, 0.3333], \"when-right\": [0.3333, 0.3333]}),",
            "    ],",
            "    ids=[\"SVC\", \"NuSVC\"],",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight, mask_side\",",
            "    [([1, -0.5, 1, 1, 1, 1], \"when-left\"), ([1, 1, 1, 0, 1, 1], \"when-right\")],",
            "    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],",
            ")",
            "def test_negative_weights_svc_leave_two_labels(",
            "    Classifier, model, sample_weight, mask_side",
            "):",
            "    clf = Classifier(kernel=\"linear\")",
            "    clf.fit(X, Y, sample_weight=sample_weight)",
            "    assert_allclose(clf.coef_, [model[mask_side]], rtol=1e-3)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Estimator\", [svm.SVC, svm.NuSVC, svm.NuSVR], ids=[\"SVC\", \"NuSVC\", \"NuSVR\"]",
            ")",
            "@pytest.mark.parametrize(",
            "    \"sample_weight\",",
            "    [[1, -0.5, 1, 1, 1, 1], [1, 1, 1, 0, 1, 1]],",
            "    ids=[\"partial-mask-label-1\", \"partial-mask-label-2\"],",
            ")",
            "def test_negative_weight_equal_coeffs(Estimator, sample_weight):",
            "    # model generates equal coefficients",
            "    est = Estimator(kernel=\"linear\")",
            "    est.fit(X, Y, sample_weight=sample_weight)",
            "    coef = np.abs(est.coef_).ravel()",
            "    assert coef[0] == pytest.approx(coef[1], rel=1e-3)",
            "",
            "",
            "@ignore_warnings(category=UndefinedMetricWarning)",
            "def test_auto_weight():",
            "    # Test class weights for imbalanced data",
            "    from sklearn.linear_model import LogisticRegression",
            "",
            "    # We take as dataset the two-dimensional projection of iris so",
            "    # that it is not separable and remove half of predictors from",
            "    # class 1.",
            "    # We add one to the targets as a non-regression test:",
            "    # class_weight=\"balanced\"",
            "    # used to work only when the labels where a range [0..K).",
            "    from sklearn.utils import compute_class_weight",
            "",
            "    X, y = iris.data[:, :2], iris.target + 1",
            "    unbalanced = np.delete(np.arange(y.size), np.where(y > 2)[0][::2])",
            "",
            "    classes = np.unique(y[unbalanced])",
            "    class_weights = compute_class_weight(\"balanced\", classes=classes, y=y[unbalanced])",
            "    assert np.argmax(class_weights) == 2",
            "",
            "    for clf in (",
            "        svm.SVC(kernel=\"linear\"),",
            "        svm.LinearSVC(random_state=0),",
            "        LogisticRegression(),",
            "    ):",
            "        # check that score is better when class='balanced' is set.",
            "        y_pred = clf.fit(X[unbalanced], y[unbalanced]).predict(X)",
            "        clf.set_params(class_weight=\"balanced\")",
            "        y_pred_balanced = clf.fit(",
            "            X[unbalanced],",
            "            y[unbalanced],",
            "        ).predict(X)",
            "        assert metrics.f1_score(y, y_pred, average=\"macro\") <= metrics.f1_score(",
            "            y, y_pred_balanced, average=\"macro\"",
            "        )",
            "",
            "",
            "def test_bad_input():",
            "    # Test that it gives proper exception on deficient input",
            "    # impossible value of C",
            "    with pytest.raises(ValueError):",
            "        svm.SVC(C=-1).fit(X, Y)",
            "",
            "    # impossible value of nu",
            "    clf = svm.NuSVC(nu=0.0)",
            "    with pytest.raises(ValueError):",
            "        clf.fit(X, Y)",
            "",
            "    Y2 = Y[:-1]  # wrong dimensions for labels",
            "    with pytest.raises(ValueError):",
            "        clf.fit(X, Y2)",
            "",
            "    # Test with arrays that are non-contiguous.",
            "    for clf in (svm.SVC(), svm.LinearSVC(random_state=0)):",
            "        Xf = np.asfortranarray(X)",
            "        assert not Xf.flags[\"C_CONTIGUOUS\"]",
            "        yf = np.ascontiguousarray(np.tile(Y, (2, 1)).T)",
            "        yf = yf[:, -1]",
            "        assert not yf.flags[\"F_CONTIGUOUS\"]",
            "        assert not yf.flags[\"C_CONTIGUOUS\"]",
            "        clf.fit(Xf, yf)",
            "        assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # error for precomputed kernelsx",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    with pytest.raises(ValueError):",
            "        clf.fit(X, Y)",
            "",
            "    # predict with sparse input when trained with dense",
            "    clf = svm.SVC().fit(X, Y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(sparse.lil_matrix(X))",
            "",
            "    Xt = np.array(X).T",
            "    clf.fit(np.dot(X, Xt), Y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(X)",
            "",
            "    clf = svm.SVC()",
            "    clf.fit(X, Y)",
            "    with pytest.raises(ValueError):",
            "        clf.predict(Xt)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"Estimator, data\",",
            "    [",
            "        (svm.SVC, datasets.load_iris(return_X_y=True)),",
            "        (svm.NuSVC, datasets.load_iris(return_X_y=True)),",
            "        (svm.SVR, datasets.load_diabetes(return_X_y=True)),",
            "        (svm.NuSVR, datasets.load_diabetes(return_X_y=True)),",
            "        (svm.OneClassSVM, datasets.load_iris(return_X_y=True)),",
            "    ],",
            ")",
            "def test_svm_gamma_error(Estimator, data):",
            "    X, y = data",
            "    est = Estimator(gamma=\"auto_deprecated\")",
            "    err_msg = \"When 'gamma' is a string, it should be either 'scale' or 'auto'\"",
            "    with pytest.raises(ValueError, match=err_msg):",
            "        est.fit(X, y)",
            "",
            "",
            "def test_unicode_kernel():",
            "    # Test that a unicode kernel name does not cause a TypeError",
            "    clf = svm.SVC(kernel=\"linear\", probability=True)",
            "    clf.fit(X, Y)",
            "    clf.predict_proba(T)",
            "    _libsvm.cross_validation(",
            "        iris.data, iris.target.astype(np.float64), 5, kernel=\"linear\", random_seed=0",
            "    )",
            "",
            "",
            "def test_sparse_precomputed():",
            "    clf = svm.SVC(kernel=\"precomputed\")",
            "    sparse_gram = sparse.csr_matrix([[1, 0], [0, 1]])",
            "    with pytest.raises(TypeError, match=\"Sparse precomputed\"):",
            "        clf.fit(sparse_gram, [0, 1])",
            "",
            "",
            "def test_sparse_fit_support_vectors_empty():",
            "    # Regression test for #14893",
            "    X_train = sparse.csr_matrix(",
            "        [[0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]]",
            "    )",
            "    y_train = np.array([0.04, 0.04, 0.10, 0.16])",
            "    model = svm.SVR(kernel=\"linear\")",
            "    model.fit(X_train, y_train)",
            "    assert not model.support_vectors_.data.size",
            "    assert not model.dual_coef_.data.size",
            "",
            "",
            "def test_linearsvc_parameters():",
            "    # Test possible parameter combinations in LinearSVC",
            "    # Generate list of possible parameter combinations",
            "    losses = [\"hinge\", \"squared_hinge\", \"logistic_regression\", \"foo\"]",
            "    penalties, duals = [\"l1\", \"l2\", \"bar\"], [True, False]",
            "",
            "    X, y = make_classification(n_samples=5, n_features=5)",
            "",
            "    for loss, penalty, dual in itertools.product(losses, penalties, duals):",
            "        clf = svm.LinearSVC(penalty=penalty, loss=loss, dual=dual)",
            "        if (",
            "            (loss, penalty) == (\"hinge\", \"l1\")",
            "            or (loss, penalty, dual) == (\"hinge\", \"l2\", False)",
            "            or (penalty, dual) == (\"l1\", True)",
            "            or loss == \"foo\"",
            "            or penalty == \"bar\"",
            "        ):",
            "",
            "            with pytest.raises(",
            "                ValueError,",
            "                match=\"Unsupported set of arguments.*penalty='%s.*loss='%s.*dual=%s\"",
            "                % (penalty, loss, dual),",
            "            ):",
            "                clf.fit(X, y)",
            "        else:",
            "            clf.fit(X, y)",
            "",
            "    # Incorrect loss value - test if explicit error message is raised",
            "    with pytest.raises(ValueError, match=\".*loss='l3' is not supported.*\"):",
            "        svm.LinearSVC(loss=\"l3\").fit(X, y)",
            "",
            "",
            "def test_linear_svx_uppercase_loss_penality_raises_error():",
            "    # Check if Upper case notation raises error at _fit_liblinear",
            "    # which is called by fit",
            "",
            "    X, y = [[0.0], [1.0]], [0, 1]",
            "",
            "    msg = \"loss='SQuared_hinge' is not supported\"",
            "    with pytest.raises(ValueError, match=msg):",
            "        svm.LinearSVC(loss=\"SQuared_hinge\").fit(X, y)",
            "",
            "    msg = \"The combination of penalty='L2' and loss='squared_hinge' is not supported\"",
            "    with pytest.raises(ValueError, match=msg):",
            "        svm.LinearSVC(penalty=\"L2\").fit(X, y)",
            "",
            "",
            "def test_linearsvc():",
            "    # Test basic routines using LinearSVC",
            "    clf = svm.LinearSVC(random_state=0).fit(X, Y)",
            "",
            "    # by default should have intercept",
            "    assert clf.fit_intercept",
            "",
            "    assert_array_equal(clf.predict(T), true_result)",
            "    assert_array_almost_equal(clf.intercept_, [0], decimal=3)",
            "",
            "    # the same with l1 penalty",
            "    clf = svm.LinearSVC(",
            "        penalty=\"l1\", loss=\"squared_hinge\", dual=False, random_state=0",
            "    ).fit(X, Y)",
            "    assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # l2 penalty with dual formulation",
            "    clf = svm.LinearSVC(penalty=\"l2\", dual=True, random_state=0).fit(X, Y)",
            "    assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # l2 penalty, l1 loss",
            "    clf = svm.LinearSVC(penalty=\"l2\", loss=\"hinge\", dual=True, random_state=0)",
            "    clf.fit(X, Y)",
            "    assert_array_equal(clf.predict(T), true_result)",
            "",
            "    # test also decision function",
            "    dec = clf.decision_function(T)",
            "    res = (dec > 0).astype(int) + 1",
            "    assert_array_equal(res, true_result)",
            "",
            "",
            "def test_linearsvc_crammer_singer():",
            "    # Test LinearSVC with crammer_singer multi-class svm",
            "    ovr_clf = svm.LinearSVC(random_state=0).fit(iris.data, iris.target)",
            "    cs_clf = svm.LinearSVC(multi_class=\"crammer_singer\", random_state=0)",
            "    cs_clf.fit(iris.data, iris.target)",
            "",
            "    # similar prediction for ovr and crammer-singer:",
            "    assert (ovr_clf.predict(iris.data) == cs_clf.predict(iris.data)).mean() > 0.9",
            "",
            "    # classifiers shouldn't be the same",
            "    assert (ovr_clf.coef_ != cs_clf.coef_).all()",
            "",
            "    # test decision function",
            "    assert_array_equal(",
            "        cs_clf.predict(iris.data),",
            "        np.argmax(cs_clf.decision_function(iris.data), axis=1),",
            "    )",
            "    dec_func = np.dot(iris.data, cs_clf.coef_.T) + cs_clf.intercept_",
            "    assert_array_almost_equal(dec_func, cs_clf.decision_function(iris.data))",
            "",
            "",
            "def test_linearsvc_fit_sampleweight():",
            "    # check correct result when sample_weight is 1",
            "    n_samples = len(X)",
            "    unit_weight = np.ones(n_samples)",
            "    clf = svm.LinearSVC(random_state=0).fit(X, Y)",
            "    clf_unitweight = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(",
            "        X, Y, sample_weight=unit_weight",
            "    )",
            "",
            "    # check if same as sample_weight=None",
            "    assert_array_equal(clf_unitweight.predict(T), clf.predict(T))",
            "    assert_allclose(clf.coef_, clf_unitweight.coef_, 1, 0.0001)",
            "",
            "    # check that fit(X)  = fit([X1, X2, X3],sample_weight = [n1, n2, n3]) where",
            "    # X = X1 repeated n1 times, X2 repeated n2 times and so forth",
            "",
            "    random_state = check_random_state(0)",
            "    random_weight = random_state.randint(0, 10, n_samples)",
            "    lsvc_unflat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(",
            "        X, Y, sample_weight=random_weight",
            "    )",
            "    pred1 = lsvc_unflat.predict(T)",
            "",
            "    X_flat = np.repeat(X, random_weight, axis=0)",
            "    y_flat = np.repeat(Y, random_weight, axis=0)",
            "    lsvc_flat = svm.LinearSVC(random_state=0, tol=1e-12, max_iter=1000).fit(",
            "        X_flat, y_flat",
            "    )",
            "    pred2 = lsvc_flat.predict(T)",
            "",
            "    assert_array_equal(pred1, pred2)",
            "    assert_allclose(lsvc_unflat.coef_, lsvc_flat.coef_, 1, 0.0001)",
            "",
            "",
            "def test_crammer_singer_binary():",
            "    # Test Crammer-Singer formulation in the binary case",
            "    X, y = make_classification(n_classes=2, random_state=0)",
            "",
            "    for fit_intercept in (True, False):",
            "        acc = (",
            "            svm.LinearSVC(",
            "                fit_intercept=fit_intercept,",
            "                multi_class=\"crammer_singer\",",
            "                random_state=0,",
            "            )",
            "            .fit(X, y)",
            "            .score(X, y)",
            "        )",
            "        assert acc > 0.9",
            "",
            "",
            "def test_linearsvc_iris():",
            "    # Test that LinearSVC gives plausible predictions on the iris dataset",
            "    # Also, test symbolic class names (classes_).",
            "    target = iris.target_names[iris.target]",
            "    clf = svm.LinearSVC(random_state=0).fit(iris.data, target)",
            "    assert set(clf.classes_) == set(iris.target_names)",
            "    assert np.mean(clf.predict(iris.data) == target) > 0.8",
            "",
            "    dec = clf.decision_function(iris.data)",
            "    pred = iris.target_names[np.argmax(dec, 1)]",
            "    assert_array_equal(pred, clf.predict(iris.data))",
            "",
            "",
            "def test_dense_liblinear_intercept_handling(classifier=svm.LinearSVC):",
            "    # Test that dense liblinear honours intercept_scaling param",
            "    X = [[2, 1], [3, 1], [1, 3], [2, 3]]",
            "    y = [0, 0, 1, 1]",
            "    clf = classifier(",
            "        fit_intercept=True,",
            "        penalty=\"l1\",",
            "        loss=\"squared_hinge\",",
            "        dual=False,",
            "        C=4,",
            "        tol=1e-7,",
            "        random_state=0,",
            "    )",
            "    assert clf.intercept_scaling == 1, clf.intercept_scaling",
            "    assert clf.fit_intercept",
            "",
            "    # when intercept_scaling is low the intercept value is highly \"penalized\"",
            "    # by regularization",
            "    clf.intercept_scaling = 1",
            "    clf.fit(X, y)",
            "    assert_almost_equal(clf.intercept_, 0, decimal=5)",
            "",
            "    # when intercept_scaling is sufficiently high, the intercept value",
            "    # is not affected by regularization",
            "    clf.intercept_scaling = 100",
            "    clf.fit(X, y)",
            "    intercept1 = clf.intercept_",
            "    assert intercept1 < -1",
            "",
            "    # when intercept_scaling is sufficiently high, the intercept value",
            "    # doesn't depend on intercept_scaling value",
            "    clf.intercept_scaling = 1000",
            "    clf.fit(X, y)",
            "    intercept2 = clf.intercept_",
            "    assert_array_almost_equal(intercept1, intercept2, decimal=2)",
            "",
            "",
            "def test_liblinear_set_coef():",
            "    # multi-class case",
            "    clf = svm.LinearSVC().fit(iris.data, iris.target)",
            "    values = clf.decision_function(iris.data)",
            "    clf.coef_ = clf.coef_.copy()",
            "    clf.intercept_ = clf.intercept_.copy()",
            "    values2 = clf.decision_function(iris.data)",
            "    assert_array_almost_equal(values, values2)",
            "",
            "    # binary-class case",
            "    X = [[2, 1], [3, 1], [1, 3], [2, 3]]",
            "    y = [0, 0, 1, 1]",
            "",
            "    clf = svm.LinearSVC().fit(X, y)",
            "    values = clf.decision_function(X)",
            "    clf.coef_ = clf.coef_.copy()",
            "    clf.intercept_ = clf.intercept_.copy()",
            "    values2 = clf.decision_function(X)",
            "    assert_array_equal(values, values2)",
            "",
            "",
            "def test_immutable_coef_property():",
            "    # Check that primal coef modification are not silently ignored",
            "    svms = [",
            "        svm.SVC(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.NuSVC(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.SVR(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.NuSVR(kernel=\"linear\").fit(iris.data, iris.target),",
            "        svm.OneClassSVM(kernel=\"linear\").fit(iris.data),",
            "    ]",
            "    for clf in svms:",
            "        with pytest.raises(AttributeError):",
            "            clf.__setattr__(\"coef_\", np.arange(3))",
            "        with pytest.raises((RuntimeError, ValueError)):",
            "            clf.coef_.__setitem__((0, 0), 0)",
            "",
            "",
            "def test_linearsvc_verbose():",
            "    # stdout: redirect",
            "    import os",
            "",
            "    stdout = os.dup(1)  # save original stdout",
            "    os.dup2(os.pipe()[1], 1)  # replace it",
            "",
            "    # actual call",
            "    clf = svm.LinearSVC(verbose=1)",
            "    clf.fit(X, Y)",
            "",
            "    # stdout: restore",
            "    os.dup2(stdout, 1)  # restore original stdout",
            "",
            "",
            "def test_svc_clone_with_callable_kernel():",
            "    # create SVM with callable linear kernel, check that results are the same",
            "    # as with built-in linear kernel",
            "    svm_callable = svm.SVC(",
            "        kernel=lambda x, y: np.dot(x, y.T),",
            "        probability=True,",
            "        random_state=0,",
            "        decision_function_shape=\"ovr\",",
            "    )",
            "    # clone for checking clonability with lambda functions..",
            "    svm_cloned = base.clone(svm_callable)",
            "    svm_cloned.fit(iris.data, iris.target)",
            "",
            "    svm_builtin = svm.SVC(",
            "        kernel=\"linear\", probability=True, random_state=0, decision_function_shape=\"ovr\"",
            "    )",
            "    svm_builtin.fit(iris.data, iris.target)",
            "",
            "    assert_array_almost_equal(svm_cloned.dual_coef_, svm_builtin.dual_coef_)",
            "    assert_array_almost_equal(svm_cloned.intercept_, svm_builtin.intercept_)",
            "    assert_array_equal(svm_cloned.predict(iris.data), svm_builtin.predict(iris.data))",
            "",
            "    assert_array_almost_equal(",
            "        svm_cloned.predict_proba(iris.data),",
            "        svm_builtin.predict_proba(iris.data),",
            "        decimal=4,",
            "    )",
            "    assert_array_almost_equal(",
            "        svm_cloned.decision_function(iris.data),",
            "        svm_builtin.decision_function(iris.data),",
            "    )",
            "",
            "",
            "def test_svc_bad_kernel():",
            "    svc = svm.SVC(kernel=lambda x, y: x)",
            "    with pytest.raises(ValueError):",
            "        svc.fit(X, Y)",
            "",
            "",
            "def test_timeout():",
            "    a = svm.SVC(",
            "        kernel=lambda x, y: np.dot(x, y.T), probability=True, random_state=0, max_iter=1",
            "    )",
            "    warning_msg = (",
            "        r\"Solver terminated early \\(max_iter=1\\).  Consider pre-processing \"",
            "        r\"your data with StandardScaler or MinMaxScaler.\"",
            "    )",
            "    with pytest.warns(ConvergenceWarning, match=warning_msg):",
            "        a.fit(np.array(X), Y)",
            "",
            "",
            "def test_unfitted():",
            "    X = \"foo!\"  # input validation not required when SVM not fitted",
            "",
            "    clf = svm.SVC()",
            "    with pytest.raises(Exception, match=r\".*\\bSVC\\b.*\\bnot\\b.*\\bfitted\\b\"):",
            "        clf.predict(X)",
            "",
            "    clf = svm.NuSVR()",
            "    with pytest.raises(Exception, match=r\".*\\bNuSVR\\b.*\\bnot\\b.*\\bfitted\\b\"):",
            "        clf.predict(X)",
            "",
            "",
            "# ignore convergence warnings from max_iter=1",
            "@ignore_warnings",
            "def test_consistent_proba():",
            "    a = svm.SVC(probability=True, max_iter=1, random_state=0)",
            "    proba_1 = a.fit(X, Y).predict_proba(X)",
            "    a = svm.SVC(probability=True, max_iter=1, random_state=0)",
            "    proba_2 = a.fit(X, Y).predict_proba(X)",
            "    assert_array_almost_equal(proba_1, proba_2)",
            "",
            "",
            "def test_linear_svm_convergence_warnings():",
            "    # Test that warnings are raised if model does not converge",
            "",
            "    lsvc = svm.LinearSVC(random_state=0, max_iter=2)",
            "    warning_msg = \"Liblinear failed to converge, increase the number of iterations.\"",
            "    with pytest.warns(ConvergenceWarning, match=warning_msg):",
            "        lsvc.fit(X, Y)",
            "    assert lsvc.n_iter_ == 2",
            "",
            "    lsvr = svm.LinearSVR(random_state=0, max_iter=2)",
            "    with pytest.warns(ConvergenceWarning, match=warning_msg):",
            "        lsvr.fit(iris.data, iris.target)",
            "    assert lsvr.n_iter_ == 2",
            "",
            "",
            "def test_svr_coef_sign():",
            "    # Test that SVR(kernel=\"linear\") has coef_ with the right sign.",
            "    # Non-regression test for #2933.",
            "    X = np.random.RandomState(21).randn(10, 3)",
            "    y = np.random.RandomState(12).randn(10)",
            "",
            "    for svr in [svm.SVR(kernel=\"linear\"), svm.NuSVR(kernel=\"linear\"), svm.LinearSVR()]:",
            "        svr.fit(X, y)",
            "        assert_array_almost_equal(",
            "            svr.predict(X), np.dot(X, svr.coef_.ravel()) + svr.intercept_",
            "        )",
            "",
            "",
            "def test_linear_svc_intercept_scaling():",
            "    # Test that the right error message is thrown when intercept_scaling <= 0",
            "",
            "    for i in [-1, 0]:",
            "        lsvc = svm.LinearSVC(intercept_scaling=i)",
            "",
            "        msg = (",
            "            \"Intercept scaling is %r but needs to be greater than 0.\"",
            "            \" To disable fitting an intercept,\"",
            "            \" set fit_intercept=False.\"",
            "            % lsvc.intercept_scaling",
            "        )",
            "        with pytest.raises(ValueError, match=msg):",
            "            lsvc.fit(X, Y)",
            "",
            "",
            "def test_lsvc_intercept_scaling_zero():",
            "    # Test that intercept_scaling is ignored when fit_intercept is False",
            "",
            "    lsvc = svm.LinearSVC(fit_intercept=False)",
            "    lsvc.fit(X, Y)",
            "    assert lsvc.intercept_ == 0.0",
            "",
            "",
            "def test_hasattr_predict_proba():",
            "    # Method must be (un)available before or after fit, switched by",
            "    # `probability` param",
            "",
            "    G = svm.SVC(probability=True)",
            "    assert hasattr(G, \"predict_proba\")",
            "    G.fit(iris.data, iris.target)",
            "    assert hasattr(G, \"predict_proba\")",
            "",
            "    G = svm.SVC(probability=False)",
            "    assert not hasattr(G, \"predict_proba\")",
            "    G.fit(iris.data, iris.target)",
            "    assert not hasattr(G, \"predict_proba\")",
            "",
            "    # Switching to `probability=True` after fitting should make",
            "    # predict_proba available, but calling it must not work:",
            "    G.probability = True",
            "    assert hasattr(G, \"predict_proba\")",
            "    msg = \"predict_proba is not available when fitted with probability=False\"",
            "",
            "    with pytest.raises(NotFittedError, match=msg):",
            "        G.predict_proba(iris.data)",
            "",
            "",
            "def test_decision_function_shape_two_class():",
            "    for n_classes in [2, 3]:",
            "        X, y = make_blobs(centers=n_classes, random_state=0)",
            "        for estimator in [svm.SVC, svm.NuSVC]:",
            "            clf = OneVsRestClassifier(estimator(decision_function_shape=\"ovr\")).fit(",
            "                X, y",
            "            )",
            "            assert len(clf.predict(X)) == len(y)",
            "",
            "",
            "def test_ovr_decision_function():",
            "    # One point from each quadrant represents one class",
            "    X_train = np.array([[1, 1], [-1, 1], [-1, -1], [1, -1]])",
            "    y_train = [0, 1, 2, 3]",
            "",
            "    # First point is closer to the decision boundaries than the second point",
            "    base_points = np.array([[5, 5], [10, 10]])",
            "",
            "    # For all the quadrants (classes)",
            "    X_test = np.vstack(",
            "        (",
            "            base_points * [1, 1],  # Q1",
            "            base_points * [-1, 1],  # Q2",
            "            base_points * [-1, -1],  # Q3",
            "            base_points * [1, -1],  # Q4",
            "        )",
            "    )",
            "",
            "    y_test = [0] * 2 + [1] * 2 + [2] * 2 + [3] * 2",
            "",
            "    clf = svm.SVC(kernel=\"linear\", decision_function_shape=\"ovr\")",
            "    clf.fit(X_train, y_train)",
            "",
            "    y_pred = clf.predict(X_test)",
            "",
            "    # Test if the prediction is the same as y",
            "    assert_array_equal(y_pred, y_test)",
            "",
            "    deci_val = clf.decision_function(X_test)",
            "",
            "    # Assert that the predicted class has the maximum value",
            "    assert_array_equal(np.argmax(deci_val, axis=1), y_pred)",
            "",
            "    # Get decision value at test points for the predicted class",
            "    pred_class_deci_val = deci_val[range(8), y_pred].reshape((4, 2))",
            "",
            "    # Assert pred_class_deci_val > 0 here",
            "    assert np.min(pred_class_deci_val) > 0.0",
            "",
            "    # Test if the first point has lower decision value on every quadrant",
            "    # compared to the second point",
            "    assert np.all(pred_class_deci_val[:, 0] < pred_class_deci_val[:, 1])",
            "",
            "",
            "@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])",
            "def test_svc_invalid_break_ties_param(SVCClass):",
            "    X, y = make_blobs(random_state=42)",
            "",
            "    svm = SVCClass(",
            "        kernel=\"linear\", decision_function_shape=\"ovo\", break_ties=True, random_state=42",
            "    ).fit(X, y)",
            "",
            "    with pytest.raises(ValueError, match=\"break_ties must be False\"):",
            "        svm.predict(y)",
            "",
            "",
            "@pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])",
            "def test_svc_ovr_tie_breaking(SVCClass):",
            "    \"\"\"Test if predict breaks ties in OVR mode.",
            "    Related issue: https://github.com/scikit-learn/scikit-learn/issues/8277",
            "    \"\"\"",
            "    X, y = make_blobs(random_state=27)",
            "",
            "    xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 1000)",
            "    ys = np.linspace(X[:, 1].min(), X[:, 1].max(), 1000)",
            "    xx, yy = np.meshgrid(xs, ys)",
            "",
            "    svm = SVCClass(",
            "        kernel=\"linear\",",
            "        decision_function_shape=\"ovr\",",
            "        break_ties=False,",
            "        random_state=42,",
            "    ).fit(X, y)",
            "    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])",
            "    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])",
            "    assert not np.all(pred == np.argmax(dv, axis=1))",
            "",
            "    svm = SVCClass(",
            "        kernel=\"linear\", decision_function_shape=\"ovr\", break_ties=True, random_state=42",
            "    ).fit(X, y)",
            "    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])",
            "    dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])",
            "    assert np.all(pred == np.argmax(dv, axis=1))",
            "",
            "",
            "def test_gamma_auto():",
            "    X, y = [[0.0, 1.2], [1.0, 1.3]], [0, 1]",
            "",
            "    with pytest.warns(None) as record:",
            "        svm.SVC(kernel=\"linear\").fit(X, y)",
            "    assert not len(record)",
            "",
            "    with pytest.warns(None) as record:",
            "        svm.SVC(kernel=\"precomputed\").fit(X, y)",
            "    assert not len(record)",
            "",
            "",
            "def test_gamma_scale():",
            "    X, y = [[0.0], [1.0]], [0, 1]",
            "",
            "    clf = svm.SVC()",
            "    with pytest.warns(None) as record:",
            "        clf.fit(X, y)",
            "    assert not len(record)",
            "    assert_almost_equal(clf._gamma, 4)",
            "",
            "    # X_var ~= 1 shouldn't raise warning, for when",
            "    # gamma is not explicitly set.",
            "    X, y = [[1, 2], [3, 2 * np.sqrt(6) / 3 + 2]], [0, 1]",
            "    with pytest.warns(None) as record:",
            "        clf.fit(X, y)",
            "    assert not len(record)",
            "",
            "",
            "@pytest.mark.parametrize(",
            "    \"SVM, params\",",
            "    [",
            "        (LinearSVC, {\"penalty\": \"l1\", \"loss\": \"squared_hinge\", \"dual\": False}),",
            "        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": True}),",
            "        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"squared_hinge\", \"dual\": False}),",
            "        (LinearSVC, {\"penalty\": \"l2\", \"loss\": \"hinge\", \"dual\": True}),",
            "        (LinearSVR, {\"loss\": \"epsilon_insensitive\", \"dual\": True}),",
            "        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),",
            "        (LinearSVR, {\"loss\": \"squared_epsilon_insensitive\", \"dual\": True}),",
            "    ],",
            ")",
            "def test_linearsvm_liblinear_sample_weight(SVM, params):",
            "    X = np.array(",
            "        [",
            "            [1, 3],",
            "            [1, 3],",
            "            [1, 3],",
            "            [1, 3],",
            "            [2, 1],",
            "            [2, 1],",
            "            [2, 1],",
            "            [2, 1],",
            "            [3, 3],",
            "            [3, 3],",
            "            [3, 3],",
            "            [3, 3],",
            "            [4, 1],",
            "            [4, 1],",
            "            [4, 1],",
            "            [4, 1],",
            "        ],",
            "        dtype=np.dtype(\"float\"),",
            "    )",
            "    y = np.array(",
            "        [1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2], dtype=np.dtype(\"int\")",
            "    )",
            "",
            "    X2 = np.vstack([X, X])",
            "    y2 = np.hstack([y, 3 - y])",
            "    sample_weight = np.ones(shape=len(y) * 2)",
            "    sample_weight[len(y) :] = 0",
            "    X2, y2, sample_weight = shuffle(X2, y2, sample_weight, random_state=0)",
            "",
            "    base_estimator = SVM(random_state=42)",
            "    base_estimator.set_params(**params)",
            "    base_estimator.set_params(tol=1e-12, max_iter=1000)",
            "    est_no_weight = base.clone(base_estimator).fit(X, y)",
            "    est_with_weight = base.clone(base_estimator).fit(",
            "        X2, y2, sample_weight=sample_weight",
            "    )",
            "",
            "    for method in (\"predict\", \"decision_function\"):",
            "        if hasattr(base_estimator, method):",
            "            X_est_no_weight = getattr(est_no_weight, method)(X)",
            "            X_est_with_weight = getattr(est_with_weight, method)(X)",
            "            assert_allclose(X_est_no_weight, X_est_with_weight)",
            "",
            "",
            "def test_n_support_oneclass_svr():",
            "    # Make n_support is correct for oneclass and SVR (used to be",
            "    # non-initialized)",
            "    # this is a non regression test for issue #14774",
            "    X = np.array([[0], [0.44], [0.45], [0.46], [1]])",
            "    clf = svm.OneClassSVM()",
            "    assert not hasattr(clf, \"n_support_\")",
            "    clf.fit(X)",
            "    assert clf.n_support_ == clf.support_vectors_.shape[0]",
            "    assert clf.n_support_.size == 1",
            "    assert clf.n_support_ == 3",
            "",
            "    y = np.arange(X.shape[0])",
            "    reg = svm.SVR().fit(X, y)",
            "    assert reg.n_support_ == reg.support_vectors_.shape[0]",
            "    assert reg.n_support_.size == 1",
            "    assert reg.n_support_ == 4",
            "",
            "",
            "@pytest.mark.parametrize(\"Estimator\", [svm.SVC, svm.SVR])",
            "def test_custom_kernel_not_array_input(Estimator):",
            "    \"\"\"Test using a custom kernel that is not fed with array-like for floats\"\"\"",
            "    data = [\"A A\", \"A\", \"B\", \"B B\", \"A B\"]",
            "    X = np.array([[2, 0], [1, 0], [0, 1], [0, 2], [1, 1]])  # count encoding",
            "    y = np.array([1, 1, 2, 2, 1])",
            "",
            "    def string_kernel(X1, X2):",
            "        assert isinstance(X1[0], str)",
            "        n_samples1 = _num_samples(X1)",
            "        n_samples2 = _num_samples(X2)",
            "        K = np.zeros((n_samples1, n_samples2))",
            "        for ii in range(n_samples1):",
            "            for jj in range(ii, n_samples2):",
            "                K[ii, jj] = X1[ii].count(\"A\") * X2[jj].count(\"A\")",
            "                K[ii, jj] += X1[ii].count(\"B\") * X2[jj].count(\"B\")",
            "                K[jj, ii] = K[ii, jj]",
            "        return K",
            "",
            "    K = string_kernel(data, data)",
            "    assert_array_equal(np.dot(X, X.T), K)",
            "",
            "    svc1 = Estimator(kernel=string_kernel).fit(data, y)",
            "    svc2 = Estimator(kernel=\"linear\").fit(X, y)",
            "    svc3 = Estimator(kernel=\"precomputed\").fit(K, y)",
            "",
            "    assert svc1.score(data, y) == svc3.score(K, y)",
            "    assert svc1.score(data, y) == svc2.score(X, y)",
            "    if hasattr(svc1, \"decision_function\"):  # classifier",
            "        assert_allclose(svc1.decision_function(data), svc2.decision_function(X))",
            "        assert_allclose(svc1.decision_function(data), svc3.decision_function(K))",
            "        assert_array_equal(svc1.predict(data), svc2.predict(X))",
            "        assert_array_equal(svc1.predict(data), svc3.predict(K))",
            "    else:  # regressor",
            "        assert_allclose(svc1.predict(data), svc2.predict(X))",
            "        assert_allclose(svc1.predict(data), svc3.predict(K))",
            "",
            "",
            "def test_svc_raises_error_internal_representation():",
            "    \"\"\"Check that SVC raises error when internal representation is altered.",
            "",
            "    Non-regression test for #18891 and https://nvd.nist.gov/vuln/detail/CVE-2020-28975",
            "    \"\"\"",
            "    clf = svm.SVC(kernel=\"linear\").fit(X, Y)",
            "    clf._n_support[0] = 1000000",
            "",
            "    msg = \"The internal representation of SVC was altered\"",
            "    with pytest.raises(ValueError, match=msg):",
            "        clf.predict(X)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.flask.sessions"
        ]
    }
}