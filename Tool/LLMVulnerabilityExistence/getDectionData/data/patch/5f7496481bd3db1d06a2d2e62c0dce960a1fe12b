{
    "mindsdb/__about__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " __title__ = 'MindsDB'"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " __package_name__ = 'mindsdb'"
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-__version__ = '23.12.4.1'"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3,
                "PatchRowcode": "+__version__ = '23.12.4.2'"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " __description__ = \"MindsDB server, provides server capabilities to mindsdb native python library\""
            },
            "5": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " __email__ = \"jorge@mindsdb.com\""
            },
            "6": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " __author__ = 'MindsDB Inc'"
            }
        },
        "frontPatchFile": [
            "__title__ = 'MindsDB'",
            "__package_name__ = 'mindsdb'",
            "__version__ = '23.12.4.1'",
            "__description__ = \"MindsDB server, provides server capabilities to mindsdb native python library\"",
            "__email__ = \"jorge@mindsdb.com\"",
            "__author__ = 'MindsDB Inc'",
            "__github__ = 'https://github.com/mindsdb/mindsdb'",
            "__pypi__ = 'https://pypi.org/project/mindsdb/'",
            "__license__ = 'ELv2'",
            "__copyright__ = 'Copyright 2018- mindsdb'"
        ],
        "afterPatchFile": [
            "__title__ = 'MindsDB'",
            "__package_name__ = 'mindsdb'",
            "__version__ = '23.12.4.2'",
            "__description__ = \"MindsDB server, provides server capabilities to mindsdb native python library\"",
            "__email__ = \"jorge@mindsdb.com\"",
            "__author__ = 'MindsDB Inc'",
            "__github__ = 'https://github.com/mindsdb/mindsdb'",
            "__pypi__ = 'https://pypi.org/project/mindsdb/'",
            "__license__ = 'ELv2'",
            "__copyright__ = 'Copyright 2018- mindsdb'"
        ],
        "action": [
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "3": [
                "__version__"
            ]
        },
        "addLocation": []
    },
    "mindsdb/integrations/handlers/anyscale_endpoints_handler/anyscale_endpoints_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import os"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import json"
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " import openai"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3,
                "PatchRowcode": "+import json"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import contextlib"
            },
            "5": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from typing import Optional, Dict"
            },
            "6": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "         self.rate_limit = 25  # requests per minute"
            },
            "8": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "         self.max_batch_size = 20"
            },
            "9": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "         self.default_max_tokens = 100"
            },
            "10": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.ft_cls = openai.FineTuningJob  # non-legacy fine-tuning endpoint"
            },
            "11": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 36,
                "PatchRowcode": "     @staticmethod"
            },
            "13": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "     @contextlib.contextmanager"
            },
            "14": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 46,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "     def create(self, target, args=None, **kwargs):"
            },
            "16": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "         with self._anyscale_base_api():"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 49,
                "PatchRowcode": "+            # load base and fine-tuned models, then hand over"
            },
            "18": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "             self._set_models(args.get('using', {}))"
            },
            "19": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "20": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # load fine-tuned models, then hand over"
            },
            "21": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "             _args = self.model_storage.json_get('args')"
            },
            "22": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "             base_models = self.chat_completion_models"
            },
            "23": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "             self.chat_completion_models = _args.get('chat_completion_models', base_models) if _args else base_models"
            },
            "24": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "             super().create(target, args, **kwargs)"
            },
            "25": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": 55,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "     def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame:"
            },
            "27": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": 57,
                "PatchRowcode": "         with self._anyscale_base_api():"
            },
            "28": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            # load fine-tuned models, then hand over"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+            # load base and fine-tuned models, then hand over"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+            self._set_models(args.get('using', {}))"
            },
            "31": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "             _args = self.model_storage.json_get('args')"
            },
            "32": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "             base_models = self.chat_completion_models"
            },
            "33": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "             self.chat_completion_models = _args.get('chat_completion_models', base_models) if _args else base_models"
            },
            "34": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "             return super().predict(df, args)"
            },
            "35": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " "
            },
            "36": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:"
            },
            "37": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "         with self._anyscale_base_api():"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+            self._set_models(args.get('using', {}))"
            },
            "39": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "             super().finetune(df, args)"
            },
            "40": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "             # rewrite chat_completion_models to include the newly fine-tuned model"
            },
            "41": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "             args = self.model_storage.json_get('args')"
            },
            "42": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 90,
                "PatchRowcode": "     def _set_models(self, args):"
            },
            "43": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "         if 'api_key' in args:"
            },
            "44": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "             args['openai_api_key'] = args['api_key']  # remove this once #7496 is fixed"
            },
            "45": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.all_models = [m['id'] for m in openai.Model.list("
            },
            "46": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            api_key=get_api_key('openai', args, self.engine_storage),"
            },
            "47": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            api_base=ANYSCALE_API_BASE)['data']]"
            },
            "48": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.chat_completion_models = self.all_models"
            },
            "49": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.supported_ft_models = self.all_models  # base models compatible with fine-tuning"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+        client = self._get_client(get_api_key('openai', args, self.engine_storage))"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+        self.all_models = [m.id for m in client.models.list()]"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+        self.chat_completion_models = [m.id for m in client.models.list() if m.rayllm_metadata['engine_config']['model_type'] == 'text-generation']  # noqa"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+        self.supported_ft_models = self.chat_completion_models  # base models compatible with fine-tuning"
            },
            "54": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 97,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 98,
                "PatchRowcode": "     @staticmethod"
            },
            "56": {
                "beforePatchRowNumber": 100,
                "afterPatchRowNumber": 99,
                "PatchRowcode": "     def _check_ft_cols(df, cols):"
            },
            "57": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "             df: has exactly two columns, `role` and `content`. Rows contain >= 1 chats in long (stacked) format."
            },
            "58": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "             For more details, check `FineTuning -> Data Format` in the Anyscale API reference."
            },
            "59": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "         \"\"\""
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+        def _is_valid(chat):"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+            \"\"\" Check if chat is valid according to Anyscale criteria.\"\"\""
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+            roles = [m['role'] for m in chat]"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+            transitions = {None: ['system', 'user'], 'system': ['user'], 'user': ['assistant'], 'assistant': ['user']}"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+            # check base condition"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+            if not ('user' in roles and 'assistant' in roles):"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+                return False"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+            # check order is valid"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+            state = None"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+            for role in roles:"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+                if role not in transitions[state]:"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+                    return False"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+                else:"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+                    state = role"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+            # chat is valid, return"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+            return True"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+"
            },
            "80": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 129,
                "PatchRowcode": "         # 1. aggregate each chat sequence into one row"
            },
            "81": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "         chats = []"
            },
            "82": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "         chat = []"
            },
            "83": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "         for i, row in df.iterrows():"
            },
            "84": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "             if row['role'] == 'system' and len(chat) > 0:"
            },
            "85": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                chats.append({'messages': chat})"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+                if _is_valid(chat):"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+                    chats.append({'messages': chat})"
            },
            "88": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "                 chat = []"
            },
            "89": {
                "beforePatchRowNumber": 117,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "             event = {'role': row['role'], 'content': row['content']}"
            },
            "90": {
                "beforePatchRowNumber": 118,
                "afterPatchRowNumber": 138,
                "PatchRowcode": "             chat.append(event)"
            },
            "91": {
                "beforePatchRowNumber": 119,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        chats.append({'messages': chat})"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+        if _is_valid(chat):"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+            chats.append({'messages': chat})"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+"
            },
            "96": {
                "beforePatchRowNumber": 120,
                "afterPatchRowNumber": 143,
                "PatchRowcode": "         series = pd.Series(chats)"
            },
            "97": {
                "beforePatchRowNumber": 121,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        train = series.iloc[:int(len(series) * (1 - test_size))]"
            },
            "98": {
                "beforePatchRowNumber": 122,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        val = series.iloc[-int(len(series) * test_size) - 1:]"
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 144,
                "PatchRowcode": "+        if len(series) < 20 * 2:"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 145,
                "PatchRowcode": "+            raise Exception(\"Dataset is too small to finetune. Please include at least 40 samples (complete chats).\")"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 146,
                "PatchRowcode": "+        val_size = max(20, int(len(series) * test_size))  # at least 20 samples required by Anyscale"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 147,
                "PatchRowcode": "+        train = series.iloc[:-val_size]"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 148,
                "PatchRowcode": "+        val = series.iloc[-val_size:]"
            },
            "104": {
                "beforePatchRowNumber": 123,
                "afterPatchRowNumber": 149,
                "PatchRowcode": " "
            },
            "105": {
                "beforePatchRowNumber": 124,
                "afterPatchRowNumber": 150,
                "PatchRowcode": "         # 2. write as jsonl"
            },
            "106": {
                "beforePatchRowNumber": 125,
                "afterPatchRowNumber": 151,
                "PatchRowcode": "         file_names = {"
            },
            "107": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 216,
                "PatchRowcode": "             check_data_for_format_errors(items)"
            },
            "108": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "         except Exception as e:"
            },
            "109": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "             raise Exception(f\"Fine-tuning data format is not valid. Got: {e}\")"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 219,
                "PatchRowcode": "+"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 220,
                "PatchRowcode": "+    @staticmethod"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 221,
                "PatchRowcode": "+    def _get_client(api_key, base_url=ANYSCALE_API_BASE, org=None):"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 222,
                "PatchRowcode": "+        return openai.OpenAI(api_key=api_key, base_url=base_url, organization=org)"
            }
        },
        "frontPatchFile": [
            "import os",
            "import json",
            "import openai",
            "import contextlib",
            "from typing import Optional, Dict",
            "",
            "import pandas as pd",
            "",
            "from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler",
            "from mindsdb.integrations.handlers.openai_handler.constants import OPENAI_API_BASE",
            "from mindsdb.integrations.utilities.handler_utils import get_api_key",
            "from mindsdb.utilities import log",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "ANYSCALE_API_BASE = 'https://api.endpoints.anyscale.com/v1'",
            "",
            "",
            "class AnyscaleEndpointsHandler(OpenAIHandler):",
            "    name = 'anyscale_endpoints'",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        super().__init__(*args, **kwargs)",
            "        self.all_models = []",
            "        self.chat_completion_models = []",
            "        self.supported_ft_models = []",
            "        self.default_model = 'meta-llama/Llama-2-7b-chat-hf'",
            "        self.base_api = ANYSCALE_API_BASE",
            "        self.default_mode = 'default'  # can also be 'conversational' or 'conversational-full'",
            "        self.supported_modes = ['default', 'conversational', 'conversational-full']",
            "        self.rate_limit = 25  # requests per minute",
            "        self.max_batch_size = 20",
            "        self.default_max_tokens = 100",
            "        self.ft_cls = openai.FineTuningJob  # non-legacy fine-tuning endpoint",
            "",
            "    @staticmethod",
            "    @contextlib.contextmanager",
            "    def _anyscale_base_api(key='OPENAI_API_BASE'):",
            "        \"\"\" Temporarily updates the API base env var to point towards the Anyscale URL. \"\"\"",
            "        old_base = os.environ.get(key, OPENAI_API_BASE)",
            "        os.environ[key] = ANYSCALE_API_BASE",
            "        try:",
            "            yield  # enter",
            "        finally:",
            "            os.environ[key] = old_base  # exit",
            "",
            "    def create(self, target, args=None, **kwargs):",
            "        with self._anyscale_base_api():",
            "            self._set_models(args.get('using', {}))",
            "",
            "            # load fine-tuned models, then hand over",
            "            _args = self.model_storage.json_get('args')",
            "            base_models = self.chat_completion_models",
            "            self.chat_completion_models = _args.get('chat_completion_models', base_models) if _args else base_models",
            "            super().create(target, args, **kwargs)",
            "",
            "    def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame:",
            "        with self._anyscale_base_api():",
            "            # load fine-tuned models, then hand over",
            "            _args = self.model_storage.json_get('args')",
            "            base_models = self.chat_completion_models",
            "            self.chat_completion_models = _args.get('chat_completion_models', base_models) if _args else base_models",
            "            return super().predict(df, args)",
            "",
            "    def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:",
            "        with self._anyscale_base_api():",
            "            super().finetune(df, args)",
            "            # rewrite chat_completion_models to include the newly fine-tuned model",
            "            args = self.model_storage.json_get('args')",
            "            args['chat_completion_models'] = list(self.chat_completion_models) + [args['model_name']]",
            "            self.model_storage.json_set('args', args)",
            "",
            "    def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        args = self.model_storage.json_get('args')",
            "        if 'api_key' in args:",
            "            del args['api_key']",
            "",
            "        if attribute == 'args':",
            "            return pd.DataFrame(args.items(), columns=['key', 'value'])",
            "        elif attribute == 'metadata':",
            "            # we opt for the URL because some models require completing a form to access their artifacts",
            "            model_name = args.get('model_name', self.default_model)",
            "            model_card_url = 'https://huggingface.co/' + model_name",
            "            return pd.DataFrame({'model_name': [model_name], 'model_card': [model_card_url]})",
            "        else:",
            "            tables = ['args', 'metadata']",
            "            return pd.DataFrame(tables, columns=['tables'])",
            "",
            "    def _set_models(self, args):",
            "        if 'api_key' in args:",
            "            args['openai_api_key'] = args['api_key']  # remove this once #7496 is fixed",
            "        self.all_models = [m['id'] for m in openai.Model.list(",
            "            api_key=get_api_key('openai', args, self.engine_storage),",
            "            api_base=ANYSCALE_API_BASE)['data']]",
            "        self.chat_completion_models = self.all_models",
            "        self.supported_ft_models = self.all_models  # base models compatible with fine-tuning",
            "",
            "    @staticmethod",
            "    def _check_ft_cols(df, cols):",
            "        for col in ['role', 'content']:",
            "            if col not in set(df.columns):",
            "                raise Exception(f\"To fine-tune this model, format your select data query to have a `role` column and a `content` column.\")  # noqa",
            "",
            "    def _prepare_ft_jsonl(self, df, temp_storage_path, temp_filename, _, test_size=0.2):",
            "        \"\"\"",
            "            df: has exactly two columns, `role` and `content`. Rows contain >= 1 chats in long (stacked) format.",
            "            For more details, check `FineTuning -> Data Format` in the Anyscale API reference.",
            "        \"\"\"",
            "        # 1. aggregate each chat sequence into one row",
            "        chats = []",
            "        chat = []",
            "        for i, row in df.iterrows():",
            "            if row['role'] == 'system' and len(chat) > 0:",
            "                chats.append({'messages': chat})",
            "                chat = []",
            "            event = {'role': row['role'], 'content': row['content']}",
            "            chat.append(event)",
            "        chats.append({'messages': chat})",
            "        series = pd.Series(chats)",
            "        train = series.iloc[:int(len(series) * (1 - test_size))]",
            "        val = series.iloc[-int(len(series) * test_size) - 1:]",
            "",
            "        # 2. write as jsonl",
            "        file_names = {",
            "            'train': f'{temp_filename}_prepared_train.jsonl',",
            "            'val': f'{temp_filename}_prepared_valid.jsonl',",
            "        }",
            "        train.to_json(os.path.join(temp_storage_path, file_names['train']), orient='records', lines=True)",
            "        val.to_json(os.path.join(temp_storage_path, file_names['val']), orient='records', lines=True)",
            "",
            "        # 3. validate",
            "        self._validate_jsonl(os.path.join(temp_storage_path, file_names['train']))",
            "        self._validate_jsonl(os.path.join(temp_storage_path, file_names['val']))",
            "        return file_names",
            "",
            "    def _get_ft_model_type(self, model_name: str):",
            "        for base_model in self.chat_completion_models:",
            "            if base_model.lower() in model_name.lower():",
            "                return base_model",
            "        logger.warning(f'Cannot recognize model {model_name}. Finetuning may fail.')",
            "        return model_name.lower()",
            "",
            "    @staticmethod",
            "    def _add_extra_ft_params(ft_params, using_args):",
            "        hyperparameters = {}",
            "        # we populate separately because keys with `None` break the API",
            "        for key in ('n_epochs', 'context_length'):",
            "            if using_args.get(key, None):",
            "                hyperparameters[key] = using_args[key]",
            "        if hyperparameters:",
            "            return {**ft_params, **{'hyperparameters': hyperparameters}}",
            "        else:",
            "            return ft_params",
            "",
            "    @staticmethod",
            "    def _validate_jsonl(jsonl_path):",
            "        \"\"\" Borrowed from Anyscale docs. We may want something customized in the future though. \"\"\"",
            "        with open(jsonl_path, 'r', encoding='utf-8') as f:",
            "            items = [json.loads(line) for line in f]",
            "",
            "        def check_data_for_format_errors(items: list):",
            "            for line_num, batch in enumerate(items):",
            "                prefix = f\"Error in line #{line_num + 1}: \"",
            "                if not isinstance(batch, dict):",
            "                    raise Exception(f\"{prefix}Each line in the provided data should be a dictionary\")",
            "",
            "                if \"messages\" not in batch:",
            "                    raise Exception(f\"{prefix}Each line in the provided data should have a 'messages' key\")",
            "",
            "                if not isinstance(batch[\"messages\"], list):",
            "                    raise Exception(f\"{prefix}Each line in the provided data should have a 'messages' key with a list of messages\")  # noqa",
            "",
            "                messages = batch[\"messages\"]",
            "                if not any(message.get(\"role\", None) == \"assistant\" for message in messages):",
            "                    raise Exception(f\"{prefix}Each message list should have at least one message with role 'assistant'\")  # noqa",
            "",
            "                for message_num, message in enumerate(messages):",
            "                    prefix = f\"Error in line #{line_num + 1}, message #{message_num + 1}: \"",
            "                    if \"role\" not in message or \"content\" not in message:",
            "                        raise Exception(f\"{prefix}Each message should have a 'role' and 'content' key\")",
            "",
            "                    if any(k not in (\"role\", \"content\", \"name\") for k in message):",
            "                        raise Exception(f\"{prefix}Each message should only have 'role', 'content', and 'name' keys, any other key is not allowed\")  # noqa",
            "",
            "                    if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):",
            "                        raise Exception(f\"{prefix}Each message should have a valid role (system, user, or assistant)\")  # noqa",
            "",
            "        try:",
            "            check_data_for_format_errors(items)",
            "        except Exception as e:",
            "            raise Exception(f\"Fine-tuning data format is not valid. Got: {e}\")"
        ],
        "afterPatchFile": [
            "import os",
            "import openai",
            "import json",
            "import contextlib",
            "from typing import Optional, Dict",
            "",
            "import pandas as pd",
            "",
            "from mindsdb.integrations.handlers.openai_handler.openai_handler import OpenAIHandler",
            "from mindsdb.integrations.handlers.openai_handler.constants import OPENAI_API_BASE",
            "from mindsdb.integrations.utilities.handler_utils import get_api_key",
            "from mindsdb.utilities import log",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "ANYSCALE_API_BASE = 'https://api.endpoints.anyscale.com/v1'",
            "",
            "",
            "class AnyscaleEndpointsHandler(OpenAIHandler):",
            "    name = 'anyscale_endpoints'",
            "",
            "    def __init__(self, *args, **kwargs):",
            "        super().__init__(*args, **kwargs)",
            "        self.all_models = []",
            "        self.chat_completion_models = []",
            "        self.supported_ft_models = []",
            "        self.default_model = 'meta-llama/Llama-2-7b-chat-hf'",
            "        self.base_api = ANYSCALE_API_BASE",
            "        self.default_mode = 'default'  # can also be 'conversational' or 'conversational-full'",
            "        self.supported_modes = ['default', 'conversational', 'conversational-full']",
            "        self.rate_limit = 25  # requests per minute",
            "        self.max_batch_size = 20",
            "        self.default_max_tokens = 100",
            "",
            "    @staticmethod",
            "    @contextlib.contextmanager",
            "    def _anyscale_base_api(key='OPENAI_API_BASE'):",
            "        \"\"\" Temporarily updates the API base env var to point towards the Anyscale URL. \"\"\"",
            "        old_base = os.environ.get(key, OPENAI_API_BASE)",
            "        os.environ[key] = ANYSCALE_API_BASE",
            "        try:",
            "            yield  # enter",
            "        finally:",
            "            os.environ[key] = old_base  # exit",
            "",
            "    def create(self, target, args=None, **kwargs):",
            "        with self._anyscale_base_api():",
            "            # load base and fine-tuned models, then hand over",
            "            self._set_models(args.get('using', {}))",
            "            _args = self.model_storage.json_get('args')",
            "            base_models = self.chat_completion_models",
            "            self.chat_completion_models = _args.get('chat_completion_models', base_models) if _args else base_models",
            "            super().create(target, args, **kwargs)",
            "",
            "    def predict(self, df: pd.DataFrame, args: Optional[Dict] = None) -> pd.DataFrame:",
            "        with self._anyscale_base_api():",
            "            # load base and fine-tuned models, then hand over",
            "            self._set_models(args.get('using', {}))",
            "            _args = self.model_storage.json_get('args')",
            "            base_models = self.chat_completion_models",
            "            self.chat_completion_models = _args.get('chat_completion_models', base_models) if _args else base_models",
            "            return super().predict(df, args)",
            "",
            "    def finetune(self, df: Optional[pd.DataFrame] = None, args: Optional[Dict] = None) -> None:",
            "        with self._anyscale_base_api():",
            "            self._set_models(args.get('using', {}))",
            "            super().finetune(df, args)",
            "            # rewrite chat_completion_models to include the newly fine-tuned model",
            "            args = self.model_storage.json_get('args')",
            "            args['chat_completion_models'] = list(self.chat_completion_models) + [args['model_name']]",
            "            self.model_storage.json_set('args', args)",
            "",
            "    def describe(self, attribute: Optional[str] = None) -> pd.DataFrame:",
            "        args = self.model_storage.json_get('args')",
            "        if 'api_key' in args:",
            "            del args['api_key']",
            "",
            "        if attribute == 'args':",
            "            return pd.DataFrame(args.items(), columns=['key', 'value'])",
            "        elif attribute == 'metadata':",
            "            # we opt for the URL because some models require completing a form to access their artifacts",
            "            model_name = args.get('model_name', self.default_model)",
            "            model_card_url = 'https://huggingface.co/' + model_name",
            "            return pd.DataFrame({'model_name': [model_name], 'model_card': [model_card_url]})",
            "        else:",
            "            tables = ['args', 'metadata']",
            "            return pd.DataFrame(tables, columns=['tables'])",
            "",
            "    def _set_models(self, args):",
            "        if 'api_key' in args:",
            "            args['openai_api_key'] = args['api_key']  # remove this once #7496 is fixed",
            "        client = self._get_client(get_api_key('openai', args, self.engine_storage))",
            "        self.all_models = [m.id for m in client.models.list()]",
            "        self.chat_completion_models = [m.id for m in client.models.list() if m.rayllm_metadata['engine_config']['model_type'] == 'text-generation']  # noqa",
            "        self.supported_ft_models = self.chat_completion_models  # base models compatible with fine-tuning",
            "",
            "    @staticmethod",
            "    def _check_ft_cols(df, cols):",
            "        for col in ['role', 'content']:",
            "            if col not in set(df.columns):",
            "                raise Exception(f\"To fine-tune this model, format your select data query to have a `role` column and a `content` column.\")  # noqa",
            "",
            "    def _prepare_ft_jsonl(self, df, temp_storage_path, temp_filename, _, test_size=0.2):",
            "        \"\"\"",
            "            df: has exactly two columns, `role` and `content`. Rows contain >= 1 chats in long (stacked) format.",
            "            For more details, check `FineTuning -> Data Format` in the Anyscale API reference.",
            "        \"\"\"",
            "        def _is_valid(chat):",
            "            \"\"\" Check if chat is valid according to Anyscale criteria.\"\"\"",
            "            roles = [m['role'] for m in chat]",
            "            transitions = {None: ['system', 'user'], 'system': ['user'], 'user': ['assistant'], 'assistant': ['user']}",
            "",
            "            # check base condition",
            "            if not ('user' in roles and 'assistant' in roles):",
            "                return False",
            "",
            "            # check order is valid",
            "            state = None",
            "            for role in roles:",
            "                if role not in transitions[state]:",
            "                    return False",
            "                else:",
            "                    state = role",
            "",
            "            # chat is valid, return",
            "            return True",
            "",
            "        # 1. aggregate each chat sequence into one row",
            "        chats = []",
            "        chat = []",
            "        for i, row in df.iterrows():",
            "            if row['role'] == 'system' and len(chat) > 0:",
            "                if _is_valid(chat):",
            "                    chats.append({'messages': chat})",
            "                chat = []",
            "            event = {'role': row['role'], 'content': row['content']}",
            "            chat.append(event)",
            "",
            "        if _is_valid(chat):",
            "            chats.append({'messages': chat})",
            "",
            "        series = pd.Series(chats)",
            "        if len(series) < 20 * 2:",
            "            raise Exception(\"Dataset is too small to finetune. Please include at least 40 samples (complete chats).\")",
            "        val_size = max(20, int(len(series) * test_size))  # at least 20 samples required by Anyscale",
            "        train = series.iloc[:-val_size]",
            "        val = series.iloc[-val_size:]",
            "",
            "        # 2. write as jsonl",
            "        file_names = {",
            "            'train': f'{temp_filename}_prepared_train.jsonl',",
            "            'val': f'{temp_filename}_prepared_valid.jsonl',",
            "        }",
            "        train.to_json(os.path.join(temp_storage_path, file_names['train']), orient='records', lines=True)",
            "        val.to_json(os.path.join(temp_storage_path, file_names['val']), orient='records', lines=True)",
            "",
            "        # 3. validate",
            "        self._validate_jsonl(os.path.join(temp_storage_path, file_names['train']))",
            "        self._validate_jsonl(os.path.join(temp_storage_path, file_names['val']))",
            "        return file_names",
            "",
            "    def _get_ft_model_type(self, model_name: str):",
            "        for base_model in self.chat_completion_models:",
            "            if base_model.lower() in model_name.lower():",
            "                return base_model",
            "        logger.warning(f'Cannot recognize model {model_name}. Finetuning may fail.')",
            "        return model_name.lower()",
            "",
            "    @staticmethod",
            "    def _add_extra_ft_params(ft_params, using_args):",
            "        hyperparameters = {}",
            "        # we populate separately because keys with `None` break the API",
            "        for key in ('n_epochs', 'context_length'):",
            "            if using_args.get(key, None):",
            "                hyperparameters[key] = using_args[key]",
            "        if hyperparameters:",
            "            return {**ft_params, **{'hyperparameters': hyperparameters}}",
            "        else:",
            "            return ft_params",
            "",
            "    @staticmethod",
            "    def _validate_jsonl(jsonl_path):",
            "        \"\"\" Borrowed from Anyscale docs. We may want something customized in the future though. \"\"\"",
            "        with open(jsonl_path, 'r', encoding='utf-8') as f:",
            "            items = [json.loads(line) for line in f]",
            "",
            "        def check_data_for_format_errors(items: list):",
            "            for line_num, batch in enumerate(items):",
            "                prefix = f\"Error in line #{line_num + 1}: \"",
            "                if not isinstance(batch, dict):",
            "                    raise Exception(f\"{prefix}Each line in the provided data should be a dictionary\")",
            "",
            "                if \"messages\" not in batch:",
            "                    raise Exception(f\"{prefix}Each line in the provided data should have a 'messages' key\")",
            "",
            "                if not isinstance(batch[\"messages\"], list):",
            "                    raise Exception(f\"{prefix}Each line in the provided data should have a 'messages' key with a list of messages\")  # noqa",
            "",
            "                messages = batch[\"messages\"]",
            "                if not any(message.get(\"role\", None) == \"assistant\" for message in messages):",
            "                    raise Exception(f\"{prefix}Each message list should have at least one message with role 'assistant'\")  # noqa",
            "",
            "                for message_num, message in enumerate(messages):",
            "                    prefix = f\"Error in line #{line_num + 1}, message #{message_num + 1}: \"",
            "                    if \"role\" not in message or \"content\" not in message:",
            "                        raise Exception(f\"{prefix}Each message should have a 'role' and 'content' key\")",
            "",
            "                    if any(k not in (\"role\", \"content\", \"name\") for k in message):",
            "                        raise Exception(f\"{prefix}Each message should only have 'role', 'content', and 'name' keys, any other key is not allowed\")  # noqa",
            "",
            "                    if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):",
            "                        raise Exception(f\"{prefix}Each message should have a valid role (system, user, or assistant)\")  # noqa",
            "",
            "        try:",
            "            check_data_for_format_errors(items)",
            "        except Exception as e:",
            "            raise Exception(f\"Fine-tuning data format is not valid. Got: {e}\")",
            "",
            "    @staticmethod",
            "    def _get_client(api_key, base_url=ANYSCALE_API_BASE, org=None):",
            "        return openai.OpenAI(api_key=api_key, base_url=base_url, organization=org)"
        ],
        "action": [
            "0",
            "1",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "2": [],
            "35": [
                "AnyscaleEndpointsHandler",
                "__init__"
            ],
            "51": [
                "AnyscaleEndpointsHandler",
                "create"
            ],
            "52": [
                "AnyscaleEndpointsHandler",
                "create"
            ],
            "60": [
                "AnyscaleEndpointsHandler",
                "predict"
            ],
            "93": [
                "AnyscaleEndpointsHandler",
                "_set_models"
            ],
            "94": [
                "AnyscaleEndpointsHandler",
                "_set_models"
            ],
            "95": [
                "AnyscaleEndpointsHandler",
                "_set_models"
            ],
            "96": [
                "AnyscaleEndpointsHandler",
                "_set_models"
            ],
            "97": [
                "AnyscaleEndpointsHandler",
                "_set_models"
            ],
            "115": [
                "AnyscaleEndpointsHandler",
                "_prepare_ft_jsonl"
            ],
            "119": [
                "AnyscaleEndpointsHandler",
                "_prepare_ft_jsonl"
            ],
            "121": [
                "AnyscaleEndpointsHandler",
                "_prepare_ft_jsonl"
            ],
            "122": [
                "AnyscaleEndpointsHandler",
                "_prepare_ft_jsonl"
            ]
        },
        "addLocation": []
    },
    "mindsdb/integrations/handlers/instatus_handler/instatus_handler.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from mindsdb.integrations.handlers.instatus_handler.instatus_tables import StatusPages"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+from mindsdb.integrations.handlers.instatus_handler.instatus_tables import StatusPages, Components"
            },
            "2": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " from mindsdb.integrations.libs.api_handler import APIHandler"
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse"
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " from mindsdb.utilities import log"
            },
            "5": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "         _tables = ["
            },
            "7": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "             StatusPages,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+            Components"
            },
            "9": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "         ]"
            },
            "10": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "11": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "         for Table in _tables:"
            },
            "12": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "         ast = parse_sql(query, dialect=\"mindsdb\")"
            },
            "13": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "         return self.query(ast)"
            },
            "14": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 94,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def call_instatus_api(self, endpoint: str, method: str = 'GET', params: dict = None, data=None) -> pd.DataFrame:"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+    def call_instatus_api(self, endpoint: str, method: str = 'GET', params: dict = None, json_data: dict = {}) -> pd.DataFrame:"
            },
            "17": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "         if not params:"
            },
            "18": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "             params = {}"
            },
            "19": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 98,
                "PatchRowcode": " "
            },
            "20": {
                "beforePatchRowNumber": 101,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "         if method.upper() in ('GET', 'POST', 'PUT', 'DELETE'):"
            },
            "21": {
                "beforePatchRowNumber": 102,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "             headers['Content-Type'] = 'application/json'"
            },
            "22": {
                "beforePatchRowNumber": 103,
                "afterPatchRowNumber": 104,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 104,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            if method.upper() in ('POST', 'PUT', 'DELETE'):"
            },
            "24": {
                "beforePatchRowNumber": 105,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                response = requests.request(method, url, headers=headers, params=params, data=data)"
            },
            "25": {
                "beforePatchRowNumber": 106,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            else:"
            },
            "26": {
                "beforePatchRowNumber": 107,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                response = requests.get(url, headers=headers, params=params)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+            response = requests.request(method, url, headers=headers, params=params, json=json_data)"
            },
            "28": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 106,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "             if response.status_code == 200:"
            },
            "30": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "                 data = response.json()"
            },
            "31": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                return pd.DataFrame(data) if isinstance(data, list) else pd.DataFrame({"
            },
            "32": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    'data': data"
            },
            "33": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                })"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+                return pd.DataFrame(data) if isinstance(data, list) else pd.DataFrame([data])"
            },
            "35": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "             else:"
            },
            "36": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "                 raise Exception(f\"Error connecting to Instatus API: {response.status_code} - {response.text}\")"
            },
            "37": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 112,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "from mindsdb.integrations.handlers.instatus_handler.instatus_tables import StatusPages",
            "from mindsdb.integrations.libs.api_handler import APIHandler",
            "from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse",
            "from mindsdb.utilities import log",
            "from mindsdb_sql import parse_sql",
            "import requests",
            "import pandas as pd",
            "from collections import OrderedDict",
            "from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "class InstatusHandler(APIHandler):",
            "    def __init__(self, name: str, **kwargs) -> None:",
            "        \"\"\"initializer method",
            "",
            "        Args:",
            "            name (str): handler name",
            "        \"\"\"",
            "        super().__init__(name)",
            "        self._base_url = \"https://api.instatus.com\"",
            "        self._api_key = None",
            "",
            "        args = kwargs.get('connection_data', {})",
            "        if 'api_key' in args:",
            "            self._api_key = args['api_key']",
            "",
            "        self.connection = None",
            "        self.is_connected = False",
            "",
            "        _tables = [",
            "            StatusPages,",
            "        ]",
            "",
            "        for Table in _tables:",
            "            self._register_table(Table.name, Table(self))",
            "",
            "    def check_connection(self) -> StatusResponse:",
            "        \"\"\"checking the connection",
            "",
            "        Returns:",
            "            StatusResponse: whether the connection is still up",
            "        \"\"\"",
            "        response = StatusResponse(False)",
            "",
            "        try:",
            "            self.connect()",
            "            response.success = True",
            "        except Exception as e:",
            "            logger.error(f'Error connecting to Instatus API: {e}!')",
            "            response.error_message = e",
            "",
            "        self.is_connected = response.success",
            "        return response",
            "",
            "    def connect(self) -> StatusResponse:",
            "        # If already connected, return the existing connection",
            "        if self.is_connected and self.connection:",
            "            return self.connection",
            "",
            "        if self._api_key:",
            "            try:",
            "                headers = {\"Authorization\": f\"Bearer {self._api_key}\"}",
            "                response = requests.get(f\"{self._base_url}/v2/pages\", headers=headers)",
            "",
            "                if response.status_code == 200:",
            "                    self.connection = response",
            "                    self.is_connected = True",
            "                    return StatusResponse(True)",
            "                else:",
            "                    raise Exception(f\"Error connecting to Instatus API: {response.status_code} - {response.text}\")",
            "            except requests.RequestException as e:",
            "                raise Exception(f\"Request to Instatus API failed: {str(e)}\")",
            "",
            "        raise Exception(\"API key is missing\")",
            "",
            "    def native_query(self, query: str) -> StatusResponse:",
            "        \"\"\"Receive and process a raw query.",
            "",
            "        Parameters",
            "        ----------",
            "        query : str",
            "            query in a native format",
            "",
            "        Returns",
            "        -------",
            "        StatusResponse",
            "            Request status",
            "        \"\"\"",
            "        ast = parse_sql(query, dialect=\"mindsdb\")",
            "        return self.query(ast)",
            "",
            "    def call_instatus_api(self, endpoint: str, method: str = 'GET', params: dict = None, data=None) -> pd.DataFrame:",
            "        if not params:",
            "            params = {}",
            "",
            "        headers = {\"Authorization\": f\"Bearer {self._api_key}\"}",
            "        url = f\"{self._base_url}{endpoint}\"",
            "",
            "        if method.upper() in ('GET', 'POST', 'PUT', 'DELETE'):",
            "            headers['Content-Type'] = 'application/json'",
            "",
            "            if method.upper() in ('POST', 'PUT', 'DELETE'):",
            "                response = requests.request(method, url, headers=headers, params=params, data=data)",
            "            else:",
            "                response = requests.get(url, headers=headers, params=params)",
            "",
            "            if response.status_code == 200:",
            "                data = response.json()",
            "                return pd.DataFrame(data) if isinstance(data, list) else pd.DataFrame({",
            "                    'data': data",
            "                })",
            "            else:",
            "                raise Exception(f\"Error connecting to Instatus API: {response.status_code} - {response.text}\")",
            "",
            "        return pd.DataFrame()",
            "",
            "",
            "connection_args = OrderedDict(",
            "    api_key={",
            "        \"type\": ARG_TYPE.PWD,",
            "        \"description\": \"Instatus API key to use for authentication.\",",
            "        \"required\": True,",
            "        \"label\": \"Api key\",",
            "    },",
            ")",
            "",
            "connection_args_example = OrderedDict(",
            "    api_key=\"d25509b171ad79395dc2c51b099ee6d0\"",
            ")"
        ],
        "afterPatchFile": [
            "from mindsdb.integrations.handlers.instatus_handler.instatus_tables import StatusPages, Components",
            "from mindsdb.integrations.libs.api_handler import APIHandler",
            "from mindsdb.integrations.libs.response import HandlerStatusResponse as StatusResponse",
            "from mindsdb.utilities import log",
            "from mindsdb_sql import parse_sql",
            "import requests",
            "import pandas as pd",
            "from collections import OrderedDict",
            "from mindsdb.integrations.libs.const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE",
            "",
            "logger = log.getLogger(__name__)",
            "",
            "",
            "class InstatusHandler(APIHandler):",
            "    def __init__(self, name: str, **kwargs) -> None:",
            "        \"\"\"initializer method",
            "",
            "        Args:",
            "            name (str): handler name",
            "        \"\"\"",
            "        super().__init__(name)",
            "        self._base_url = \"https://api.instatus.com\"",
            "        self._api_key = None",
            "",
            "        args = kwargs.get('connection_data', {})",
            "        if 'api_key' in args:",
            "            self._api_key = args['api_key']",
            "",
            "        self.connection = None",
            "        self.is_connected = False",
            "",
            "        _tables = [",
            "            StatusPages,",
            "            Components",
            "        ]",
            "",
            "        for Table in _tables:",
            "            self._register_table(Table.name, Table(self))",
            "",
            "    def check_connection(self) -> StatusResponse:",
            "        \"\"\"checking the connection",
            "",
            "        Returns:",
            "            StatusResponse: whether the connection is still up",
            "        \"\"\"",
            "        response = StatusResponse(False)",
            "",
            "        try:",
            "            self.connect()",
            "            response.success = True",
            "        except Exception as e:",
            "            logger.error(f'Error connecting to Instatus API: {e}!')",
            "            response.error_message = e",
            "",
            "        self.is_connected = response.success",
            "        return response",
            "",
            "    def connect(self) -> StatusResponse:",
            "        # If already connected, return the existing connection",
            "        if self.is_connected and self.connection:",
            "            return self.connection",
            "",
            "        if self._api_key:",
            "            try:",
            "                headers = {\"Authorization\": f\"Bearer {self._api_key}\"}",
            "                response = requests.get(f\"{self._base_url}/v2/pages\", headers=headers)",
            "",
            "                if response.status_code == 200:",
            "                    self.connection = response",
            "                    self.is_connected = True",
            "                    return StatusResponse(True)",
            "                else:",
            "                    raise Exception(f\"Error connecting to Instatus API: {response.status_code} - {response.text}\")",
            "            except requests.RequestException as e:",
            "                raise Exception(f\"Request to Instatus API failed: {str(e)}\")",
            "",
            "        raise Exception(\"API key is missing\")",
            "",
            "    def native_query(self, query: str) -> StatusResponse:",
            "        \"\"\"Receive and process a raw query.",
            "",
            "        Parameters",
            "        ----------",
            "        query : str",
            "            query in a native format",
            "",
            "        Returns",
            "        -------",
            "        StatusResponse",
            "            Request status",
            "        \"\"\"",
            "        ast = parse_sql(query, dialect=\"mindsdb\")",
            "        return self.query(ast)",
            "",
            "    def call_instatus_api(self, endpoint: str, method: str = 'GET', params: dict = None, json_data: dict = {}) -> pd.DataFrame:",
            "        if not params:",
            "            params = {}",
            "",
            "        headers = {\"Authorization\": f\"Bearer {self._api_key}\"}",
            "        url = f\"{self._base_url}{endpoint}\"",
            "",
            "        if method.upper() in ('GET', 'POST', 'PUT', 'DELETE'):",
            "            headers['Content-Type'] = 'application/json'",
            "",
            "            response = requests.request(method, url, headers=headers, params=params, json=json_data)",
            "",
            "            if response.status_code == 200:",
            "                data = response.json()",
            "                return pd.DataFrame(data) if isinstance(data, list) else pd.DataFrame([data])",
            "            else:",
            "                raise Exception(f\"Error connecting to Instatus API: {response.status_code} - {response.text}\")",
            "",
            "        return pd.DataFrame()",
            "",
            "",
            "connection_args = OrderedDict(",
            "    api_key={",
            "        \"type\": ARG_TYPE.PWD,",
            "        \"description\": \"Instatus API key to use for authentication.\",",
            "        \"required\": True,",
            "        \"label\": \"Api key\",",
            "    },",
            ")",
            "",
            "connection_args_example = OrderedDict(",
            "    api_key=\"d25509b171ad79395dc2c51b099ee6d0\"",
            ")"
        ],
        "action": [
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1": [],
            "94": [
                "InstatusHandler",
                "call_instatus_api"
            ],
            "104": [
                "InstatusHandler",
                "call_instatus_api"
            ],
            "105": [
                "InstatusHandler",
                "call_instatus_api"
            ],
            "106": [
                "InstatusHandler",
                "call_instatus_api"
            ],
            "107": [
                "InstatusHandler",
                "call_instatus_api"
            ],
            "111": [
                "InstatusHandler",
                "call_instatus_api"
            ],
            "112": [
                "InstatusHandler",
                "call_instatus_api"
            ],
            "113": [
                "InstatusHandler",
                "call_instatus_api"
            ]
        },
        "addLocation": []
    }
}