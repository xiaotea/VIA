{
    "tensorflow/python/kernel_tests/quantization_ops/quantization_ops_test.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 77,
                "PatchRowcode": "               inputs=inputs, min=[0.0], max=[1.0, 1.1]))"
            },
            "1": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 78,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": 79,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 80,
                "PatchRowcode": "+class FakeQuantWithMinMaxVarsGradientOpTest(test_util.TensorFlowTestCase):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+  @test_util.run_in_graph_and_eager_modes"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+  def test_invalid_inputs(self):"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+    gradients = constant_op.constant("
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+    inputs = constant_op.constant("
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+                                \"must be equal rank|must be rank 0\"):"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+      self.evaluate("
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+          array_ops.fake_quant_with_min_max_vars_gradient("
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+              gradients=gradients,"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+              inputs=inputs,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+              min=0.0,"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+              max=[[1.0], [2.0], [4.0]]))"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                                \"must be rank 0\"):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+      self.evaluate("
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+          array_ops.fake_quant_with_min_max_vars_gradient("
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+              gradients=gradients,"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+              inputs=inputs,"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+              min=[[1.0], [2.0], [4.0]],"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+              max=[[1.0], [2.0], [4.0]]))"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+class FakeQuantWithMinMaxVarsPerChannelGradientOpTest("
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+    test_util.TensorFlowTestCase):"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+  @test_util.run_in_graph_and_eager_modes"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+  def test_invalid_inputs(self):"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+    gradients = constant_op.constant("
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+    inputs = constant_op.constant("
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 116,
                "PatchRowcode": "+        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 117,
                "PatchRowcode": "+"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 118,
                "PatchRowcode": "+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 119,
                "PatchRowcode": "+                                \"Shapes must be equal rank|must be rank 1\"):"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 120,
                "PatchRowcode": "+      self.evaluate("
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 121,
                "PatchRowcode": "+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient("
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 122,
                "PatchRowcode": "+              gradients=gradients, inputs=inputs, min=[[0.0]], max=[1.0]))"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 123,
                "PatchRowcode": "+"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 124,
                "PatchRowcode": "+    with self.assertRaisesRegex("
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 125,
                "PatchRowcode": "+        (ValueError, errors.InvalidArgumentError),"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 126,
                "PatchRowcode": "+        \"Dimension 0 in both shapes must be equal|incorrect size\"):"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+      self.evaluate("
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient("
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+              gradients=gradients, inputs=inputs, min=[0.0, 0.1], max=[1.0]))"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+                                \"Shapes must be equal rank|must be rank 1\"):"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+      self.evaluate("
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient("
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+              gradients=gradients, inputs=inputs, min=[1.0], max=[[1.0]]))"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 136,
                "PatchRowcode": "+"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 137,
                "PatchRowcode": "+    with self.assertRaisesRegex("
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+        (ValueError, errors.InvalidArgumentError),"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 139,
                "PatchRowcode": "+        \"Dimension 0 in both shapes must be equal|incorrect size\"):"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 140,
                "PatchRowcode": "+      self.evaluate("
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 141,
                "PatchRowcode": "+          array_ops.fake_quant_with_min_max_vars_per_channel_gradient("
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 142,
                "PatchRowcode": "+              gradients=gradients, inputs=inputs, min=[0.0], max=[1.0, 1.1]))"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 143,
                "PatchRowcode": "+"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 144,
                "PatchRowcode": "+"
            },
            "68": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 145,
                "PatchRowcode": " class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):"
            },
            "69": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 146,
                "PatchRowcode": " "
            },
            "70": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 147,
                "PatchRowcode": "   @test_util.run_in_graph_and_eager_modes"
            },
            "71": {
                "beforePatchRowNumber": 337,
                "afterPatchRowNumber": 402,
                "PatchRowcode": "     with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),"
            },
            "72": {
                "beforePatchRowNumber": 338,
                "afterPatchRowNumber": 403,
                "PatchRowcode": "                                 \"must be rank 0\"):"
            },
            "73": {
                "beforePatchRowNumber": 339,
                "afterPatchRowNumber": 404,
                "PatchRowcode": "       self.evaluate("
            },
            "74": {
                "beforePatchRowNumber": 340,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-          math_ops.quantize_down_and_shrink_range(input=inputs,"
            },
            "75": {
                "beforePatchRowNumber": 341,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                                  input_min=[],"
            },
            "76": {
                "beforePatchRowNumber": 342,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                                  input_max=4.0,"
            },
            "77": {
                "beforePatchRowNumber": 343,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                                  out_type=dtypes.quint8))"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 405,
                "PatchRowcode": "+          math_ops.quantize_down_and_shrink_range("
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 406,
                "PatchRowcode": "+              input=inputs, input_min=[], input_max=4.0,"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 407,
                "PatchRowcode": "+              out_type=dtypes.quint8))"
            },
            "81": {
                "beforePatchRowNumber": 344,
                "afterPatchRowNumber": 408,
                "PatchRowcode": " "
            },
            "82": {
                "beforePatchRowNumber": 345,
                "afterPatchRowNumber": 409,
                "PatchRowcode": " "
            },
            "83": {
                "beforePatchRowNumber": 346,
                "afterPatchRowNumber": 410,
                "PatchRowcode": " if __name__ == \"__main__\":"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for tf.quantize ops.\"\"\"",
            "import numpy as np",
            "",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import nn_ops",
            "from tensorflow.python.platform import googletest",
            "",
            "",
            "class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))",
            "",
            "",
            "class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[[0.0]], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0, 0.1], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[1.0], max=[[1.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0], max=[1.0, 1.1]))",
            "",
            "",
            "class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)",
            "    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=[],",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=[],",
            "              out_type=dtypes.qint32))",
            "",
            "",
            "class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))",
            "",
            "",
            "class QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    ksize = [1, 1, 1, 1]",
            "    strides = [1, 1, 1, 1]",
            "    padding = \"SAME\"",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_avg_pool(",
            "              input=inputs,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_avg_pool(",
            "              input=inputs,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "",
            "class QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    ksize = [1, 1, 1, 1]",
            "    strides = [1, 1, 1, 1]",
            "    padding = \"SAME\"",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_max_pool(",
            "              input=inputs,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_max_pool(",
            "              input=inputs,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "",
            "class RequantizeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=[],",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=[],",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=[],",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=[],",
            "              out_type=dtypes.qint8))",
            "",
            "",
            "class QuantizedAddOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    x = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantized_add(",
            "              x=x,",
            "              y=y,",
            "              min_x=[],",
            "              max_x=1.0,",
            "              min_y=0.0,",
            "              max_y=1.0,",
            "              Toutput=dtypes.qint32))",
            "",
            "",
            "class QuantizedReluOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizedRelu6OpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu6(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantize_down_and_shrink_range(input=inputs,",
            "                                                  input_min=[],",
            "                                                  input_max=4.0,",
            "                                                  out_type=dtypes.quint8))",
            "",
            "",
            "if __name__ == \"__main__\":",
            "  googletest.main()"
        ],
        "afterPatchFile": [
            "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "# ==============================================================================",
            "\"\"\"Tests for tf.quantize ops.\"\"\"",
            "import numpy as np",
            "",
            "from tensorflow.python.framework import constant_op",
            "from tensorflow.python.framework import dtypes",
            "from tensorflow.python.framework import errors",
            "from tensorflow.python.framework import test_util",
            "from tensorflow.python.ops import array_ops",
            "from tensorflow.python.ops import math_ops",
            "from tensorflow.python.ops import nn_ops",
            "from tensorflow.python.platform import googletest",
            "",
            "",
            "class FakeQuantWithMinMaxVarsOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=0.0, max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars(",
            "              inputs=inputs, min=[[1.0], [2.0], [4.0]], max=1.0))",
            "",
            "",
            "class FakeQuantWithMinMaxVarsPerChannelOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[[0.0]], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0, 0.1], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[1.0], max=[[1.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Dimensions must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel(",
            "              inputs=inputs, min=[0.0], max=[1.0, 1.1]))",
            "",
            "",
            "class FakeQuantWithMinMaxVarsGradientOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    gradients = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be equal rank|must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_gradient(",
            "              gradients=gradients,",
            "              inputs=inputs,",
            "              min=0.0,",
            "              max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_gradient(",
            "              gradients=gradients,",
            "              inputs=inputs,",
            "              min=[[1.0], [2.0], [4.0]],",
            "              max=[[1.0], [2.0], [4.0]]))",
            "",
            "",
            "class FakeQuantWithMinMaxVarsPerChannelGradientOpTest(",
            "    test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    gradients = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "    inputs = constant_op.constant(",
            "        value=[[1.0], [2.0], [4.0]], dtype=dtypes.float32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Shapes must be equal rank|must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(",
            "              gradients=gradients, inputs=inputs, min=[[0.0]], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex(",
            "        (ValueError, errors.InvalidArgumentError),",
            "        \"Dimension 0 in both shapes must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(",
            "              gradients=gradients, inputs=inputs, min=[0.0, 0.1], max=[1.0]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"Shapes must be equal rank|must be rank 1\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(",
            "              gradients=gradients, inputs=inputs, min=[1.0], max=[[1.0]]))",
            "",
            "    with self.assertRaisesRegex(",
            "        (ValueError, errors.InvalidArgumentError),",
            "        \"Dimension 0 in both shapes must be equal|incorrect size\"):",
            "      self.evaluate(",
            "          array_ops.fake_quant_with_min_max_vars_per_channel_gradient(",
            "              gradients=gradients, inputs=inputs, min=[0.0], max=[1.0, 1.1]))",
            "",
            "",
            "class QuantizedBiasedAddTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.qint8)",
            "    bias = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.qint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              min_bias=0.0,",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=[],",
            "              max_bias=1.0,",
            "              out_type=dtypes.qint32))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_bias_add(",
            "              input=inputs,",
            "              bias=bias,",
            "              min_input=0.0,",
            "              max_input=1.0,",
            "              min_bias=0.0,",
            "              max_bias=[],",
            "              out_type=dtypes.qint32))",
            "",
            "",
            "class QuantizedInstanceNormOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=0.0, x_max=[[1.0], [2.0], [4.0]]))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          array_ops.quantized_instance_norm(",
            "              x=inputs, x_min=[[1.0], [2.0], [4.0]], x_max=1.0))",
            "",
            "",
            "class QuantizedAvgPoolingOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    ksize = [1, 1, 1, 1]",
            "    strides = [1, 1, 1, 1]",
            "    padding = \"SAME\"",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_avg_pool(",
            "              input=inputs,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_avg_pool(",
            "              input=inputs,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "",
            "class QuantizedMaxPoolingOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.uint8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    ksize = [1, 1, 1, 1]",
            "    strides = [1, 1, 1, 1]",
            "    padding = \"SAME\"",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_max_pool(",
            "              input=inputs,",
            "              min_input=[],",
            "              max_input=1.0,",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "    with self.assertRaisesRegex((errors.InvalidArgumentError, ValueError),",
            "                                \"must be.* rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_max_pool(",
            "              input=inputs,",
            "              min_input=0.0,",
            "              max_input=[],",
            "              ksize=ksize,",
            "              strides=strides,",
            "              padding=padding))",
            "",
            "",
            "class RequantizeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=[],",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=[],",
            "              requested_output_min=0.0,",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=[],",
            "              requested_output_max=1.0,",
            "              out_type=dtypes.qint8))",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.requantize(",
            "              input=inputs,",
            "              input_min=0.0,",
            "              input_max=1.0,",
            "              requested_output_min=0.0,",
            "              requested_output_max=[],",
            "              out_type=dtypes.qint8))",
            "",
            "",
            "class QuantizedAddOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    x = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "    y = constant_op.constant(np.int8(0), shape=[3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantized_add(",
            "              x=x,",
            "              y=y,",
            "              min_x=[],",
            "              max_x=1.0,",
            "              min_y=0.0,",
            "              max_y=1.0,",
            "              Toutput=dtypes.qint32))",
            "",
            "",
            "class QuantizedReluOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizedRelu6OpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int8(0), shape=[3, 3, 3, 3], dtype=dtypes.quint8)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          nn_ops.quantized_relu6(",
            "              features=inputs,",
            "              min_features=[],",
            "              max_features=127.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "class QuantizeDownAndShrinkRangeOpTest(test_util.TensorFlowTestCase):",
            "",
            "  @test_util.run_in_graph_and_eager_modes",
            "  def test_invalid_inputs(self):",
            "    inputs = constant_op.constant(",
            "        np.int32(0), shape=[3, 3, 3, 3], dtype=dtypes.qint32)",
            "",
            "    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",
            "                                \"must be rank 0\"):",
            "      self.evaluate(",
            "          math_ops.quantize_down_and_shrink_range(",
            "              input=inputs, input_min=[], input_max=4.0,",
            "              out_type=dtypes.quint8))",
            "",
            "",
            "if __name__ == \"__main__\":",
            "  googletest.main()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "340": [
                "QuantizeDownAndShrinkRangeOpTest",
                "test_invalid_inputs"
            ],
            "341": [
                "QuantizeDownAndShrinkRangeOpTest",
                "test_invalid_inputs"
            ],
            "342": [
                "QuantizeDownAndShrinkRangeOpTest",
                "test_invalid_inputs"
            ],
            "343": [
                "QuantizeDownAndShrinkRangeOpTest",
                "test_invalid_inputs"
            ]
        },
        "addLocation": [
            "tensorflow.python.kernel_tests.quantization_ops.quantization_ops_test"
        ]
    }
}