{
    "dashboard/agent.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "         ), \"Accessing unsupported API (GcsAioPublisher) in a minimal ray.\""
            },
            "1": {
                "beforePatchRowNumber": 181,
                "afterPatchRowNumber": 181,
                "PatchRowcode": "         return self.aio_publisher"
            },
            "2": {
                "beforePatchRowNumber": 182,
                "afterPatchRowNumber": 182,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 183,
                "PatchRowcode": "+    def get_node_id(self) -> str:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 184,
                "PatchRowcode": "+        return self.node_id"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 185,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 186,
                "PatchRowcode": "     async def run(self):"
            },
            "7": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 187,
                "PatchRowcode": "         # Start a grpc asyncio server."
            },
            "8": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "         if self.server:"
            }
        },
        "frontPatchFile": [
            "import argparse",
            "import asyncio",
            "import json",
            "import logging",
            "import logging.handlers",
            "import os",
            "import pathlib",
            "import sys",
            "import signal",
            "",
            "import ray",
            "import ray._private.ray_constants as ray_constants",
            "import ray._private.services",
            "import ray._private.utils",
            "import ray.dashboard.consts as dashboard_consts",
            "import ray.dashboard.utils as dashboard_utils",
            "from ray._raylet import GcsClient",
            "from ray._private.process_watcher import create_check_raylet_task",
            "from ray._private.gcs_utils import GcsAioClient",
            "from ray._private.ray_logging import (",
            "    setup_component_logger,",
            "    configure_log_file,",
            ")",
            "from ray.experimental.internal_kv import (",
            "    _initialize_internal_kv,",
            "    _internal_kv_initialized,",
            ")",
            "from ray._private.ray_constants import AGENT_GRPC_MAX_MESSAGE_LENGTH",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DashboardAgent:",
            "    def __init__(",
            "        self,",
            "        node_ip_address,",
            "        dashboard_agent_port,",
            "        gcs_address,",
            "        minimal,",
            "        metrics_export_port=None,",
            "        node_manager_port=None,",
            "        listen_port=ray_constants.DEFAULT_DASHBOARD_AGENT_LISTEN_PORT,",
            "        disable_metrics_collection: bool = False,",
            "        *,  # the following are required kwargs",
            "        object_store_name: str,",
            "        raylet_name: str,",
            "        log_dir: str,",
            "        temp_dir: str,",
            "        session_dir: str,",
            "        logging_params: dict,",
            "        agent_id: int,",
            "        session_name: str,",
            "    ):",
            "        \"\"\"Initialize the DashboardAgent object.\"\"\"",
            "        # Public attributes are accessible for all agent modules.",
            "        self.ip = node_ip_address",
            "        self.minimal = minimal",
            "",
            "        assert gcs_address is not None",
            "        self.gcs_address = gcs_address",
            "",
            "        self.temp_dir = temp_dir",
            "        self.session_dir = session_dir",
            "        self.log_dir = log_dir",
            "        self.dashboard_agent_port = dashboard_agent_port",
            "        self.metrics_export_port = metrics_export_port",
            "        self.node_manager_port = node_manager_port",
            "        self.listen_port = listen_port",
            "        self.object_store_name = object_store_name",
            "        self.raylet_name = raylet_name",
            "        self.logging_params = logging_params",
            "        self.node_id = os.environ[\"RAY_NODE_ID\"]",
            "        self.metrics_collection_disabled = disable_metrics_collection",
            "        self.agent_id = agent_id",
            "        self.session_name = session_name",
            "",
            "        # grpc server is None in mininal.",
            "        self.server = None",
            "        # http_server is None in minimal.",
            "        self.http_server = None",
            "",
            "        # Used by the agent and sub-modules.",
            "        # TODO(architkulkarni): Remove gcs_client once the agent exclusively uses",
            "        # gcs_aio_client and not gcs_client.",
            "        self.gcs_client = GcsClient(address=self.gcs_address)",
            "        _initialize_internal_kv(self.gcs_client)",
            "        assert _internal_kv_initialized()",
            "        self.gcs_aio_client = GcsAioClient(address=self.gcs_address)",
            "",
            "        if not self.minimal:",
            "            self._init_non_minimal()",
            "",
            "    def _init_non_minimal(self):",
            "        from ray._private.gcs_pubsub import GcsAioPublisher",
            "",
            "        self.aio_publisher = GcsAioPublisher(address=self.gcs_address)",
            "",
            "        try:",
            "            from grpc import aio as aiogrpc",
            "        except ImportError:",
            "            from grpc.experimental import aio as aiogrpc",
            "",
            "        # We would want to suppress deprecating warnings from aiogrpc library",
            "        # with the usage of asyncio.get_event_loop() in python version >=3.10",
            "        # This could be removed once https://github.com/grpc/grpc/issues/32526",
            "        # is released, and we used higher versions of grpcio that that.",
            "        if sys.version_info.major >= 3 and sys.version_info.minor >= 10:",
            "            import warnings",
            "",
            "            with warnings.catch_warnings():",
            "                warnings.simplefilter(\"ignore\", category=DeprecationWarning)",
            "                aiogrpc.init_grpc_aio()",
            "        else:",
            "            aiogrpc.init_grpc_aio()",
            "",
            "        self.server = aiogrpc.server(",
            "            options=(",
            "                (\"grpc.so_reuseport\", 0),",
            "                (",
            "                    \"grpc.max_send_message_length\",",
            "                    AGENT_GRPC_MAX_MESSAGE_LENGTH,",
            "                ),  # noqa",
            "                (",
            "                    \"grpc.max_receive_message_length\",",
            "                    AGENT_GRPC_MAX_MESSAGE_LENGTH,",
            "                ),",
            "            )  # noqa",
            "        )",
            "        grpc_ip = \"127.0.0.1\" if self.ip == \"127.0.0.1\" else \"0.0.0.0\"",
            "        try:",
            "            self.grpc_port = ray._private.tls_utils.add_port_to_grpc_server(",
            "                self.server, f\"{grpc_ip}:{self.dashboard_agent_port}\"",
            "            )",
            "        except Exception:",
            "            # TODO(SongGuyang): Catch the exception here because there is",
            "            # port conflict issue which brought from static port. We should",
            "            # remove this after we find better port resolution.",
            "            logger.exception(",
            "                \"Failed to add port to grpc server. Agent will stay alive but \"",
            "                \"disable the grpc service.\"",
            "            )",
            "            self.server = None",
            "            self.grpc_port = None",
            "        else:",
            "            logger.info(\"Dashboard agent grpc address: %s:%s\", grpc_ip, self.grpc_port)",
            "",
            "    async def _configure_http_server(self, modules):",
            "        from ray.dashboard.http_server_agent import HttpServerAgent",
            "",
            "        http_server = HttpServerAgent(self.ip, self.listen_port)",
            "        await http_server.start(modules)",
            "        return http_server",
            "",
            "    def _load_modules(self):",
            "        \"\"\"Load dashboard agent modules.\"\"\"",
            "        modules = []",
            "        agent_cls_list = dashboard_utils.get_all_modules(",
            "            dashboard_utils.DashboardAgentModule",
            "        )",
            "        for cls in agent_cls_list:",
            "            logger.info(",
            "                \"Loading %s: %s\", dashboard_utils.DashboardAgentModule.__name__, cls",
            "            )",
            "            c = cls(self)",
            "            modules.append(c)",
            "        logger.info(\"Loaded %d modules.\", len(modules))",
            "        return modules",
            "",
            "    @property",
            "    def http_session(self):",
            "        assert (",
            "            self.http_server",
            "        ), \"Accessing unsupported API (HttpServerAgent) in a minimal ray.\"",
            "        return self.http_server.http_session",
            "",
            "    @property",
            "    def publisher(self):",
            "        assert (",
            "            self.aio_publisher",
            "        ), \"Accessing unsupported API (GcsAioPublisher) in a minimal ray.\"",
            "        return self.aio_publisher",
            "",
            "    async def run(self):",
            "        # Start a grpc asyncio server.",
            "        if self.server:",
            "            await self.server.start()",
            "",
            "        modules = self._load_modules()",
            "",
            "        # Setup http server if necessary.",
            "        if not self.minimal:",
            "            # If the agent is not minimal it should start the http server",
            "            # to communicate with the dashboard in a head node.",
            "            # Http server is not started in the minimal version because",
            "            # it requires additional dependencies that are not",
            "            # included in the minimal ray package.",
            "            try:",
            "                self.http_server = await self._configure_http_server(modules)",
            "            except Exception:",
            "                # TODO(SongGuyang): Catch the exception here because there is",
            "                # port conflict issue which brought from static port. We should",
            "                # remove this after we find better port resolution.",
            "                logger.exception(",
            "                    \"Failed to start http server. Agent will stay alive but \"",
            "                    \"disable the http service.\"",
            "                )",
            "",
            "        # Write the dashboard agent port to kv.",
            "        # TODO: Use async version if performance is an issue",
            "        # -1 should indicate that http server is not started.",
            "        http_port = -1 if not self.http_server else self.http_server.http_port",
            "        grpc_port = -1 if not self.server else self.grpc_port",
            "        await self.gcs_aio_client.internal_kv_put(",
            "            f\"{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}\".encode(),",
            "            json.dumps([http_port, grpc_port]).encode(),",
            "            True,",
            "            namespace=ray_constants.KV_NAMESPACE_DASHBOARD,",
            "        )",
            "",
            "        tasks = [m.run(self.server) for m in modules]",
            "        if sys.platform not in [\"win32\", \"cygwin\"]:",
            "",
            "            def callback():",
            "                logger.info(",
            "                    f\"Terminated Raylet: ip={self.ip}, node_id={self.node_id}. \"",
            "                )",
            "",
            "            check_parent_task = create_check_raylet_task(",
            "                self.log_dir, self.gcs_address, callback, loop",
            "            )",
            "            tasks.append(check_parent_task)",
            "        await asyncio.gather(*tasks)",
            "",
            "        if self.server:",
            "            await self.server.wait_for_termination()",
            "        else:",
            "            while True:",
            "                await asyncio.sleep(3600)  # waits forever",
            "",
            "        if self.http_server:",
            "            await self.http_server.cleanup()",
            "",
            "",
            "def open_capture_files(log_dir):",
            "    filename = f\"agent-{args.agent_id}\"",
            "    return (",
            "        ray._private.utils.open_log(pathlib.Path(log_dir) / f\"{filename}.out\"),",
            "        ray._private.utils.open_log(pathlib.Path(log_dir) / f\"{filename}.err\"),",
            "    )",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    parser = argparse.ArgumentParser(description=\"Dashboard agent.\")",
            "    parser.add_argument(",
            "        \"--node-ip-address\",",
            "        required=True,",
            "        type=str,",
            "        help=\"the IP address of this node.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--gcs-address\", required=True, type=str, help=\"The address (ip:port) of GCS.\"",
            "    )",
            "    parser.add_argument(",
            "        \"--metrics-export-port\",",
            "        required=True,",
            "        type=int,",
            "        help=\"The port to expose metrics through Prometheus.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--dashboard-agent-port\",",
            "        required=True,",
            "        type=int,",
            "        help=\"The port on which the dashboard agent will receive GRPCs.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--node-manager-port\",",
            "        required=True,",
            "        type=int,",
            "        help=\"The port to use for starting the node manager\",",
            "    )",
            "    parser.add_argument(",
            "        \"--object-store-name\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"The socket name of the plasma store\",",
            "    )",
            "    parser.add_argument(",
            "        \"--listen-port\",",
            "        required=False,",
            "        type=int,",
            "        default=ray_constants.DEFAULT_DASHBOARD_AGENT_LISTEN_PORT,",
            "        help=\"Port for HTTP server to listen on\",",
            "    )",
            "    parser.add_argument(",
            "        \"--raylet-name\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"The socket path of the raylet process\",",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-level\",",
            "        required=False,",
            "        type=lambda s: logging.getLevelName(s.upper()),",
            "        default=ray_constants.LOGGER_LEVEL,",
            "        choices=ray_constants.LOGGER_LEVEL_CHOICES,",
            "        help=ray_constants.LOGGER_LEVEL_HELP,",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-format\",",
            "        required=False,",
            "        type=str,",
            "        default=ray_constants.LOGGER_FORMAT,",
            "        help=ray_constants.LOGGER_FORMAT_HELP,",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-filename\",",
            "        required=False,",
            "        type=str,",
            "        default=dashboard_consts.DASHBOARD_AGENT_LOG_FILENAME,",
            "        help=\"Specify the name of log file, \"",
            "        'log to stdout if set empty, default is \"{}\".'.format(",
            "            dashboard_consts.DASHBOARD_AGENT_LOG_FILENAME",
            "        ),",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-rotate-bytes\",",
            "        required=False,",
            "        type=int,",
            "        default=ray_constants.LOGGING_ROTATE_BYTES,",
            "        help=\"Specify the max bytes for rotating \"",
            "        \"log file, default is {} bytes.\".format(ray_constants.LOGGING_ROTATE_BYTES),",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-rotate-backup-count\",",
            "        required=False,",
            "        type=int,",
            "        default=ray_constants.LOGGING_ROTATE_BACKUP_COUNT,",
            "        help=\"Specify the backup count of rotated log file, default is {}.\".format(",
            "            ray_constants.LOGGING_ROTATE_BACKUP_COUNT",
            "        ),",
            "    )",
            "    parser.add_argument(",
            "        \"--log-dir\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"Specify the path of log directory.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--temp-dir\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"Specify the path of the temporary directory use by Ray process.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--session-dir\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"Specify the path of this session.\",",
            "    )",
            "",
            "    parser.add_argument(",
            "        \"--minimal\",",
            "        action=\"store_true\",",
            "        help=(",
            "            \"Minimal agent only contains a subset of features that don't \"",
            "            \"require additional dependencies installed when ray is installed \"",
            "            \"by `pip install 'ray[default]'`.\"",
            "        ),",
            "    )",
            "    parser.add_argument(",
            "        \"--disable-metrics-collection\",",
            "        action=\"store_true\",",
            "        help=(\"If this arg is set, metrics report won't be enabled from the agent.\"),",
            "    )",
            "    parser.add_argument(",
            "        \"--agent-id\",",
            "        required=True,",
            "        type=int,",
            "        help=\"ID to report when registering with raylet\",",
            "        default=os.getpid(),",
            "    )",
            "    parser.add_argument(",
            "        \"--session-name\",",
            "        required=False,",
            "        type=str,",
            "        default=None,",
            "        help=\"The session name (cluster id) of this cluster.\",",
            "    )",
            "",
            "    args = parser.parse_args()",
            "",
            "    try:",
            "        logging_params = dict(",
            "            logging_level=args.logging_level,",
            "            logging_format=args.logging_format,",
            "            log_dir=args.log_dir,",
            "            filename=args.logging_filename,",
            "            max_bytes=args.logging_rotate_bytes,",
            "            backup_count=args.logging_rotate_backup_count,",
            "        )",
            "        logger = setup_component_logger(**logging_params)",
            "",
            "        # Initialize event loop, see Dashboard init code for caveat",
            "        # w.r.t grpc server init in the DashboardAgent initializer.",
            "        loop = ray._private.utils.get_or_create_event_loop()",
            "",
            "        # Setup stdout/stderr redirect files",
            "        out_file, err_file = open_capture_files(args.log_dir)",
            "        configure_log_file(out_file, err_file)",
            "",
            "        agent = DashboardAgent(",
            "            args.node_ip_address,",
            "            args.dashboard_agent_port,",
            "            args.gcs_address,",
            "            args.minimal,",
            "            temp_dir=args.temp_dir,",
            "            session_dir=args.session_dir,",
            "            log_dir=args.log_dir,",
            "            metrics_export_port=args.metrics_export_port,",
            "            node_manager_port=args.node_manager_port,",
            "            listen_port=args.listen_port,",
            "            object_store_name=args.object_store_name,",
            "            raylet_name=args.raylet_name,",
            "            logging_params=logging_params,",
            "            disable_metrics_collection=args.disable_metrics_collection,",
            "            agent_id=args.agent_id,",
            "            session_name=args.session_name,",
            "        )",
            "",
            "        def sigterm_handler():",
            "            logger.warning(\"Exiting with SIGTERM immediately...\")",
            "            # Exit code 0 will be considered as an expected shutdown",
            "            os._exit(signal.SIGTERM)",
            "",
            "        if sys.platform != \"win32\":",
            "            # TODO(rickyyx): we currently do not have any logic for actual",
            "            # graceful termination in the agent. Most of the underlying",
            "            # async tasks run by the agent head doesn't handle CancelledError.",
            "            # So a truly graceful shutdown is not trivial w/o much refactoring.",
            "            # Re-open the issue: https://github.com/ray-project/ray/issues/25518",
            "            # if a truly graceful shutdown is required.",
            "            loop.add_signal_handler(signal.SIGTERM, sigterm_handler)",
            "",
            "        loop.run_until_complete(agent.run())",
            "    except Exception:",
            "        logger.exception(\"Agent is working abnormally. It will exit immediately.\")",
            "        exit(1)"
        ],
        "afterPatchFile": [
            "import argparse",
            "import asyncio",
            "import json",
            "import logging",
            "import logging.handlers",
            "import os",
            "import pathlib",
            "import sys",
            "import signal",
            "",
            "import ray",
            "import ray._private.ray_constants as ray_constants",
            "import ray._private.services",
            "import ray._private.utils",
            "import ray.dashboard.consts as dashboard_consts",
            "import ray.dashboard.utils as dashboard_utils",
            "from ray._raylet import GcsClient",
            "from ray._private.process_watcher import create_check_raylet_task",
            "from ray._private.gcs_utils import GcsAioClient",
            "from ray._private.ray_logging import (",
            "    setup_component_logger,",
            "    configure_log_file,",
            ")",
            "from ray.experimental.internal_kv import (",
            "    _initialize_internal_kv,",
            "    _internal_kv_initialized,",
            ")",
            "from ray._private.ray_constants import AGENT_GRPC_MAX_MESSAGE_LENGTH",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DashboardAgent:",
            "    def __init__(",
            "        self,",
            "        node_ip_address,",
            "        dashboard_agent_port,",
            "        gcs_address,",
            "        minimal,",
            "        metrics_export_port=None,",
            "        node_manager_port=None,",
            "        listen_port=ray_constants.DEFAULT_DASHBOARD_AGENT_LISTEN_PORT,",
            "        disable_metrics_collection: bool = False,",
            "        *,  # the following are required kwargs",
            "        object_store_name: str,",
            "        raylet_name: str,",
            "        log_dir: str,",
            "        temp_dir: str,",
            "        session_dir: str,",
            "        logging_params: dict,",
            "        agent_id: int,",
            "        session_name: str,",
            "    ):",
            "        \"\"\"Initialize the DashboardAgent object.\"\"\"",
            "        # Public attributes are accessible for all agent modules.",
            "        self.ip = node_ip_address",
            "        self.minimal = minimal",
            "",
            "        assert gcs_address is not None",
            "        self.gcs_address = gcs_address",
            "",
            "        self.temp_dir = temp_dir",
            "        self.session_dir = session_dir",
            "        self.log_dir = log_dir",
            "        self.dashboard_agent_port = dashboard_agent_port",
            "        self.metrics_export_port = metrics_export_port",
            "        self.node_manager_port = node_manager_port",
            "        self.listen_port = listen_port",
            "        self.object_store_name = object_store_name",
            "        self.raylet_name = raylet_name",
            "        self.logging_params = logging_params",
            "        self.node_id = os.environ[\"RAY_NODE_ID\"]",
            "        self.metrics_collection_disabled = disable_metrics_collection",
            "        self.agent_id = agent_id",
            "        self.session_name = session_name",
            "",
            "        # grpc server is None in mininal.",
            "        self.server = None",
            "        # http_server is None in minimal.",
            "        self.http_server = None",
            "",
            "        # Used by the agent and sub-modules.",
            "        # TODO(architkulkarni): Remove gcs_client once the agent exclusively uses",
            "        # gcs_aio_client and not gcs_client.",
            "        self.gcs_client = GcsClient(address=self.gcs_address)",
            "        _initialize_internal_kv(self.gcs_client)",
            "        assert _internal_kv_initialized()",
            "        self.gcs_aio_client = GcsAioClient(address=self.gcs_address)",
            "",
            "        if not self.minimal:",
            "            self._init_non_minimal()",
            "",
            "    def _init_non_minimal(self):",
            "        from ray._private.gcs_pubsub import GcsAioPublisher",
            "",
            "        self.aio_publisher = GcsAioPublisher(address=self.gcs_address)",
            "",
            "        try:",
            "            from grpc import aio as aiogrpc",
            "        except ImportError:",
            "            from grpc.experimental import aio as aiogrpc",
            "",
            "        # We would want to suppress deprecating warnings from aiogrpc library",
            "        # with the usage of asyncio.get_event_loop() in python version >=3.10",
            "        # This could be removed once https://github.com/grpc/grpc/issues/32526",
            "        # is released, and we used higher versions of grpcio that that.",
            "        if sys.version_info.major >= 3 and sys.version_info.minor >= 10:",
            "            import warnings",
            "",
            "            with warnings.catch_warnings():",
            "                warnings.simplefilter(\"ignore\", category=DeprecationWarning)",
            "                aiogrpc.init_grpc_aio()",
            "        else:",
            "            aiogrpc.init_grpc_aio()",
            "",
            "        self.server = aiogrpc.server(",
            "            options=(",
            "                (\"grpc.so_reuseport\", 0),",
            "                (",
            "                    \"grpc.max_send_message_length\",",
            "                    AGENT_GRPC_MAX_MESSAGE_LENGTH,",
            "                ),  # noqa",
            "                (",
            "                    \"grpc.max_receive_message_length\",",
            "                    AGENT_GRPC_MAX_MESSAGE_LENGTH,",
            "                ),",
            "            )  # noqa",
            "        )",
            "        grpc_ip = \"127.0.0.1\" if self.ip == \"127.0.0.1\" else \"0.0.0.0\"",
            "        try:",
            "            self.grpc_port = ray._private.tls_utils.add_port_to_grpc_server(",
            "                self.server, f\"{grpc_ip}:{self.dashboard_agent_port}\"",
            "            )",
            "        except Exception:",
            "            # TODO(SongGuyang): Catch the exception here because there is",
            "            # port conflict issue which brought from static port. We should",
            "            # remove this after we find better port resolution.",
            "            logger.exception(",
            "                \"Failed to add port to grpc server. Agent will stay alive but \"",
            "                \"disable the grpc service.\"",
            "            )",
            "            self.server = None",
            "            self.grpc_port = None",
            "        else:",
            "            logger.info(\"Dashboard agent grpc address: %s:%s\", grpc_ip, self.grpc_port)",
            "",
            "    async def _configure_http_server(self, modules):",
            "        from ray.dashboard.http_server_agent import HttpServerAgent",
            "",
            "        http_server = HttpServerAgent(self.ip, self.listen_port)",
            "        await http_server.start(modules)",
            "        return http_server",
            "",
            "    def _load_modules(self):",
            "        \"\"\"Load dashboard agent modules.\"\"\"",
            "        modules = []",
            "        agent_cls_list = dashboard_utils.get_all_modules(",
            "            dashboard_utils.DashboardAgentModule",
            "        )",
            "        for cls in agent_cls_list:",
            "            logger.info(",
            "                \"Loading %s: %s\", dashboard_utils.DashboardAgentModule.__name__, cls",
            "            )",
            "            c = cls(self)",
            "            modules.append(c)",
            "        logger.info(\"Loaded %d modules.\", len(modules))",
            "        return modules",
            "",
            "    @property",
            "    def http_session(self):",
            "        assert (",
            "            self.http_server",
            "        ), \"Accessing unsupported API (HttpServerAgent) in a minimal ray.\"",
            "        return self.http_server.http_session",
            "",
            "    @property",
            "    def publisher(self):",
            "        assert (",
            "            self.aio_publisher",
            "        ), \"Accessing unsupported API (GcsAioPublisher) in a minimal ray.\"",
            "        return self.aio_publisher",
            "",
            "    def get_node_id(self) -> str:",
            "        return self.node_id",
            "",
            "    async def run(self):",
            "        # Start a grpc asyncio server.",
            "        if self.server:",
            "            await self.server.start()",
            "",
            "        modules = self._load_modules()",
            "",
            "        # Setup http server if necessary.",
            "        if not self.minimal:",
            "            # If the agent is not minimal it should start the http server",
            "            # to communicate with the dashboard in a head node.",
            "            # Http server is not started in the minimal version because",
            "            # it requires additional dependencies that are not",
            "            # included in the minimal ray package.",
            "            try:",
            "                self.http_server = await self._configure_http_server(modules)",
            "            except Exception:",
            "                # TODO(SongGuyang): Catch the exception here because there is",
            "                # port conflict issue which brought from static port. We should",
            "                # remove this after we find better port resolution.",
            "                logger.exception(",
            "                    \"Failed to start http server. Agent will stay alive but \"",
            "                    \"disable the http service.\"",
            "                )",
            "",
            "        # Write the dashboard agent port to kv.",
            "        # TODO: Use async version if performance is an issue",
            "        # -1 should indicate that http server is not started.",
            "        http_port = -1 if not self.http_server else self.http_server.http_port",
            "        grpc_port = -1 if not self.server else self.grpc_port",
            "        await self.gcs_aio_client.internal_kv_put(",
            "            f\"{dashboard_consts.DASHBOARD_AGENT_PORT_PREFIX}{self.node_id}\".encode(),",
            "            json.dumps([http_port, grpc_port]).encode(),",
            "            True,",
            "            namespace=ray_constants.KV_NAMESPACE_DASHBOARD,",
            "        )",
            "",
            "        tasks = [m.run(self.server) for m in modules]",
            "        if sys.platform not in [\"win32\", \"cygwin\"]:",
            "",
            "            def callback():",
            "                logger.info(",
            "                    f\"Terminated Raylet: ip={self.ip}, node_id={self.node_id}. \"",
            "                )",
            "",
            "            check_parent_task = create_check_raylet_task(",
            "                self.log_dir, self.gcs_address, callback, loop",
            "            )",
            "            tasks.append(check_parent_task)",
            "        await asyncio.gather(*tasks)",
            "",
            "        if self.server:",
            "            await self.server.wait_for_termination()",
            "        else:",
            "            while True:",
            "                await asyncio.sleep(3600)  # waits forever",
            "",
            "        if self.http_server:",
            "            await self.http_server.cleanup()",
            "",
            "",
            "def open_capture_files(log_dir):",
            "    filename = f\"agent-{args.agent_id}\"",
            "    return (",
            "        ray._private.utils.open_log(pathlib.Path(log_dir) / f\"{filename}.out\"),",
            "        ray._private.utils.open_log(pathlib.Path(log_dir) / f\"{filename}.err\"),",
            "    )",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    parser = argparse.ArgumentParser(description=\"Dashboard agent.\")",
            "    parser.add_argument(",
            "        \"--node-ip-address\",",
            "        required=True,",
            "        type=str,",
            "        help=\"the IP address of this node.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--gcs-address\", required=True, type=str, help=\"The address (ip:port) of GCS.\"",
            "    )",
            "    parser.add_argument(",
            "        \"--metrics-export-port\",",
            "        required=True,",
            "        type=int,",
            "        help=\"The port to expose metrics through Prometheus.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--dashboard-agent-port\",",
            "        required=True,",
            "        type=int,",
            "        help=\"The port on which the dashboard agent will receive GRPCs.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--node-manager-port\",",
            "        required=True,",
            "        type=int,",
            "        help=\"The port to use for starting the node manager\",",
            "    )",
            "    parser.add_argument(",
            "        \"--object-store-name\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"The socket name of the plasma store\",",
            "    )",
            "    parser.add_argument(",
            "        \"--listen-port\",",
            "        required=False,",
            "        type=int,",
            "        default=ray_constants.DEFAULT_DASHBOARD_AGENT_LISTEN_PORT,",
            "        help=\"Port for HTTP server to listen on\",",
            "    )",
            "    parser.add_argument(",
            "        \"--raylet-name\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"The socket path of the raylet process\",",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-level\",",
            "        required=False,",
            "        type=lambda s: logging.getLevelName(s.upper()),",
            "        default=ray_constants.LOGGER_LEVEL,",
            "        choices=ray_constants.LOGGER_LEVEL_CHOICES,",
            "        help=ray_constants.LOGGER_LEVEL_HELP,",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-format\",",
            "        required=False,",
            "        type=str,",
            "        default=ray_constants.LOGGER_FORMAT,",
            "        help=ray_constants.LOGGER_FORMAT_HELP,",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-filename\",",
            "        required=False,",
            "        type=str,",
            "        default=dashboard_consts.DASHBOARD_AGENT_LOG_FILENAME,",
            "        help=\"Specify the name of log file, \"",
            "        'log to stdout if set empty, default is \"{}\".'.format(",
            "            dashboard_consts.DASHBOARD_AGENT_LOG_FILENAME",
            "        ),",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-rotate-bytes\",",
            "        required=False,",
            "        type=int,",
            "        default=ray_constants.LOGGING_ROTATE_BYTES,",
            "        help=\"Specify the max bytes for rotating \"",
            "        \"log file, default is {} bytes.\".format(ray_constants.LOGGING_ROTATE_BYTES),",
            "    )",
            "    parser.add_argument(",
            "        \"--logging-rotate-backup-count\",",
            "        required=False,",
            "        type=int,",
            "        default=ray_constants.LOGGING_ROTATE_BACKUP_COUNT,",
            "        help=\"Specify the backup count of rotated log file, default is {}.\".format(",
            "            ray_constants.LOGGING_ROTATE_BACKUP_COUNT",
            "        ),",
            "    )",
            "    parser.add_argument(",
            "        \"--log-dir\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"Specify the path of log directory.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--temp-dir\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"Specify the path of the temporary directory use by Ray process.\",",
            "    )",
            "    parser.add_argument(",
            "        \"--session-dir\",",
            "        required=True,",
            "        type=str,",
            "        default=None,",
            "        help=\"Specify the path of this session.\",",
            "    )",
            "",
            "    parser.add_argument(",
            "        \"--minimal\",",
            "        action=\"store_true\",",
            "        help=(",
            "            \"Minimal agent only contains a subset of features that don't \"",
            "            \"require additional dependencies installed when ray is installed \"",
            "            \"by `pip install 'ray[default]'`.\"",
            "        ),",
            "    )",
            "    parser.add_argument(",
            "        \"--disable-metrics-collection\",",
            "        action=\"store_true\",",
            "        help=(\"If this arg is set, metrics report won't be enabled from the agent.\"),",
            "    )",
            "    parser.add_argument(",
            "        \"--agent-id\",",
            "        required=True,",
            "        type=int,",
            "        help=\"ID to report when registering with raylet\",",
            "        default=os.getpid(),",
            "    )",
            "    parser.add_argument(",
            "        \"--session-name\",",
            "        required=False,",
            "        type=str,",
            "        default=None,",
            "        help=\"The session name (cluster id) of this cluster.\",",
            "    )",
            "",
            "    args = parser.parse_args()",
            "",
            "    try:",
            "        logging_params = dict(",
            "            logging_level=args.logging_level,",
            "            logging_format=args.logging_format,",
            "            log_dir=args.log_dir,",
            "            filename=args.logging_filename,",
            "            max_bytes=args.logging_rotate_bytes,",
            "            backup_count=args.logging_rotate_backup_count,",
            "        )",
            "        logger = setup_component_logger(**logging_params)",
            "",
            "        # Initialize event loop, see Dashboard init code for caveat",
            "        # w.r.t grpc server init in the DashboardAgent initializer.",
            "        loop = ray._private.utils.get_or_create_event_loop()",
            "",
            "        # Setup stdout/stderr redirect files",
            "        out_file, err_file = open_capture_files(args.log_dir)",
            "        configure_log_file(out_file, err_file)",
            "",
            "        agent = DashboardAgent(",
            "            args.node_ip_address,",
            "            args.dashboard_agent_port,",
            "            args.gcs_address,",
            "            args.minimal,",
            "            temp_dir=args.temp_dir,",
            "            session_dir=args.session_dir,",
            "            log_dir=args.log_dir,",
            "            metrics_export_port=args.metrics_export_port,",
            "            node_manager_port=args.node_manager_port,",
            "            listen_port=args.listen_port,",
            "            object_store_name=args.object_store_name,",
            "            raylet_name=args.raylet_name,",
            "            logging_params=logging_params,",
            "            disable_metrics_collection=args.disable_metrics_collection,",
            "            agent_id=args.agent_id,",
            "            session_name=args.session_name,",
            "        )",
            "",
            "        def sigterm_handler():",
            "            logger.warning(\"Exiting with SIGTERM immediately...\")",
            "            # Exit code 0 will be considered as an expected shutdown",
            "            os._exit(signal.SIGTERM)",
            "",
            "        if sys.platform != \"win32\":",
            "            # TODO(rickyyx): we currently do not have any logic for actual",
            "            # graceful termination in the agent. Most of the underlying",
            "            # async tasks run by the agent head doesn't handle CancelledError.",
            "            # So a truly graceful shutdown is not trivial w/o much refactoring.",
            "            # Re-open the issue: https://github.com/ray-project/ray/issues/25518",
            "            # if a truly graceful shutdown is required.",
            "            loop.add_signal_handler(signal.SIGTERM, sigterm_handler)",
            "",
            "        loop.run_until_complete(agent.run())",
            "    except Exception:",
            "        logger.exception(\"Agent is working abnormally. It will exit immediately.\")",
            "        exit(1)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "dashboard.agent.DashboardAgent.self",
            "src.PIL.IcnsImagePlugin.IcnsFile.dataforsize",
            "dashboard.agent"
        ]
    },
    "dashboard/modules/log/log_agent.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import logging"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing import Tuple"
            },
            "2": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+from typing import Optional, Tuple"
            },
            "3": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " "
            },
            "4": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import concurrent.futures"
            },
            "5": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import ray.dashboard.modules.log.log_utils as log_utils"
            },
            "6": {
                "beforePatchRowNumber": 275,
                "afterPatchRowNumber": 275,
                "PatchRowcode": "         if server:"
            },
            "7": {
                "beforePatchRowNumber": 276,
                "afterPatchRowNumber": 276,
                "PatchRowcode": "             reporter_pb2_grpc.add_LogServiceServicer_to_server(self, server)"
            },
            "8": {
                "beforePatchRowNumber": 277,
                "afterPatchRowNumber": 277,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 278,
                "PatchRowcode": "+    @property"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 279,
                "PatchRowcode": "+    def node_id(self) -> Optional[str]:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 280,
                "PatchRowcode": "+        return self._dashboard_agent.get_node_id()"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 281,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": 278,
                "afterPatchRowNumber": 282,
                "PatchRowcode": "     @staticmethod"
            },
            "14": {
                "beforePatchRowNumber": 279,
                "afterPatchRowNumber": 283,
                "PatchRowcode": "     def is_minimal_module():"
            },
            "15": {
                "beforePatchRowNumber": 280,
                "afterPatchRowNumber": 284,
                "PatchRowcode": "         # Dashboard is only available with non-minimal install now."
            },
            "16": {
                "beforePatchRowNumber": 370,
                "afterPatchRowNumber": 374,
                "PatchRowcode": " "
            },
            "17": {
                "beforePatchRowNumber": 371,
                "afterPatchRowNumber": 375,
                "PatchRowcode": "         return start_offset, end_offset"
            },
            "18": {
                "beforePatchRowNumber": 372,
                "afterPatchRowNumber": 376,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 377,
                "PatchRowcode": "+    @classmethod"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 378,
                "PatchRowcode": "+    def _resolve_filename(cls, root_log_dir: Path, filename: str) -> Path:"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 379,
                "PatchRowcode": "+        \"\"\""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 380,
                "PatchRowcode": "+        Resolves the file path relative to the root log directory."
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 381,
                "PatchRowcode": "+"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 382,
                "PatchRowcode": "+        Args:"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 383,
                "PatchRowcode": "+            root_log_dir: Root log directory."
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 384,
                "PatchRowcode": "+            filename: File path relative to the root log directory."
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 385,
                "PatchRowcode": "+"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 386,
                "PatchRowcode": "+        Raises:"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 387,
                "PatchRowcode": "+            FileNotFoundError: If the file path is invalid."
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 388,
                "PatchRowcode": "+"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 389,
                "PatchRowcode": "+        Returns:"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 390,
                "PatchRowcode": "+            The absolute file path resolved from the root log directory."
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 391,
                "PatchRowcode": "+        \"\"\""
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 392,
                "PatchRowcode": "+        if not Path(filename).is_absolute():"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 393,
                "PatchRowcode": "+            filepath = root_log_dir / filename"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 394,
                "PatchRowcode": "+        else:"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 395,
                "PatchRowcode": "+            filepath = Path(filename)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 396,
                "PatchRowcode": "+"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 397,
                "PatchRowcode": "+        # We want to allow relative paths that include symlinks pointing outside of the"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 398,
                "PatchRowcode": "+        # `root_log_dir`, so use `os.path.abspath` instead of `Path.resolve()` because"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 399,
                "PatchRowcode": "+        # `os.path.abspath` does not resolve symlinks."
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 400,
                "PatchRowcode": "+        filepath = Path(os.path.abspath(filepath))"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 401,
                "PatchRowcode": "+"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 402,
                "PatchRowcode": "+        if not filepath.is_file():"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 403,
                "PatchRowcode": "+            raise FileNotFoundError(f\"A file is not found at: {filepath}\")"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 404,
                "PatchRowcode": "+"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 405,
                "PatchRowcode": "+        try:"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 406,
                "PatchRowcode": "+            filepath.relative_to(root_log_dir)"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 407,
                "PatchRowcode": "+        except ValueError as e:"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 408,
                "PatchRowcode": "+            raise FileNotFoundError(f\"{filepath} not in {root_log_dir}: {e}\")"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 409,
                "PatchRowcode": "+"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 410,
                "PatchRowcode": "+        # Fully resolve the path before returning (including following symlinks)."
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 411,
                "PatchRowcode": "+        return filepath.resolve()"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 412,
                "PatchRowcode": "+"
            },
            "55": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 413,
                "PatchRowcode": "     async def StreamLog(self, request, context):"
            },
            "56": {
                "beforePatchRowNumber": 374,
                "afterPatchRowNumber": 414,
                "PatchRowcode": "         \"\"\""
            },
            "57": {
                "beforePatchRowNumber": 375,
                "afterPatchRowNumber": 415,
                "PatchRowcode": "         Streams the log in real time starting from `request.lines` number of lines from"
            },
            "58": {
                "beforePatchRowNumber": 384,
                "afterPatchRowNumber": 424,
                "PatchRowcode": "         # be automatically terminated."
            },
            "59": {
                "beforePatchRowNumber": 385,
                "afterPatchRowNumber": 425,
                "PatchRowcode": "         lines = request.lines if request.lines else 1000"
            },
            "60": {
                "beforePatchRowNumber": 386,
                "afterPatchRowNumber": 426,
                "PatchRowcode": " "
            },
            "61": {
                "beforePatchRowNumber": 387,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not Path(request.log_file_name).is_absolute():"
            },
            "62": {
                "beforePatchRowNumber": 388,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            filepath = Path(self._dashboard_agent.log_dir) / request.log_file_name"
            },
            "63": {
                "beforePatchRowNumber": 389,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        else:"
            },
            "64": {
                "beforePatchRowNumber": 390,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            filepath = Path(request.log_file_name)"
            },
            "65": {
                "beforePatchRowNumber": 391,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "66": {
                "beforePatchRowNumber": 392,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if not filepath.is_file():"
            },
            "67": {
                "beforePatchRowNumber": 393,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            await context.send_initial_metadata("
            },
            "68": {
                "beforePatchRowNumber": 394,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                [[log_consts.LOG_GRPC_ERROR, log_consts.FILE_NOT_FOUND]]"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 427,
                "PatchRowcode": "+        try:"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 428,
                "PatchRowcode": "+            filepath = self._resolve_filename("
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 429,
                "PatchRowcode": "+                Path(self._dashboard_agent.log_dir), request.log_file_name"
            },
            "72": {
                "beforePatchRowNumber": 395,
                "afterPatchRowNumber": 430,
                "PatchRowcode": "             )"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 431,
                "PatchRowcode": "+        except FileNotFoundError as e:"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 432,
                "PatchRowcode": "+            await context.send_initial_metadata([[log_consts.LOG_GRPC_ERROR, str(e)]])"
            },
            "75": {
                "beforePatchRowNumber": 396,
                "afterPatchRowNumber": 433,
                "PatchRowcode": "         else:"
            },
            "76": {
                "beforePatchRowNumber": 397,
                "afterPatchRowNumber": 434,
                "PatchRowcode": "             with open(filepath, \"rb\") as f:"
            },
            "77": {
                "beforePatchRowNumber": 398,
                "afterPatchRowNumber": 435,
                "PatchRowcode": "                 await context.send_initial_metadata([])"
            }
        },
        "frontPatchFile": [
            "import logging",
            "from typing import Tuple",
            "",
            "import concurrent.futures",
            "import ray.dashboard.modules.log.log_utils as log_utils",
            "import ray.dashboard.modules.log.log_consts as log_consts",
            "import ray.dashboard.utils as dashboard_utils",
            "import ray.dashboard.optional_utils as dashboard_optional_utils",
            "from ray._private.ray_constants import env_integer",
            "import asyncio",
            "import grpc",
            "import io",
            "import os",
            "",
            "",
            "from pathlib import Path",
            "",
            "from ray.core.generated import reporter_pb2",
            "from ray.core.generated import reporter_pb2_grpc",
            "from ray._private.ray_constants import (",
            "    LOG_PREFIX_TASK_ATTEMPT_START,",
            "    LOG_PREFIX_TASK_ATTEMPT_END,",
            ")",
            "",
            "logger = logging.getLogger(__name__)",
            "routes = dashboard_optional_utils.DashboardAgentRouteTable",
            "",
            "# 64 KB",
            "BLOCK_SIZE = 1 << 16",
            "",
            "# Keep-alive interval for reading the file",
            "DEFAULT_KEEP_ALIVE_INTERVAL_SEC = 1",
            "",
            "RAY_DASHBOARD_LOG_TASK_LOG_SEARCH_MAX_WORKER_COUNT = env_integer(",
            "    \"RAY_DASHBOARD_LOG_TASK_LOG_SEARCH_MAX_WORKER_COUNT\", default=2",
            ")",
            "",
            "",
            "def find_offset_of_content_in_file(",
            "    file: io.BufferedIOBase, content: bytes, start_offset: int = 0",
            ") -> int:",
            "    \"\"\"Find the offset of the first occurrence of content in a file.",
            "",
            "    Args:",
            "        file: File object",
            "        content: Content to find",
            "        start_offset: Start offset to read from, inclusive.",
            "",
            "    Returns:",
            "        Offset of the first occurrence of content in a file.",
            "    \"\"\"",
            "    logger.debug(f\"Finding offset of content {content} in file\")",
            "    file.seek(start_offset, io.SEEK_SET)  # move file pointer to start of file",
            "    offset = start_offset",
            "    while True:",
            "        # Read in block",
            "        block_data = file.read(BLOCK_SIZE)",
            "        if block_data == b\"\":",
            "            # Stop reading",
            "            return -1",
            "        # Find the offset of the first occurrence of content in the block",
            "        block_offset = block_data.find(content)",
            "        if block_offset != -1:",
            "            # Found the offset in the block",
            "            return offset + block_offset",
            "        # Continue reading",
            "        offset += len(block_data)",
            "",
            "",
            "def find_end_offset_file(file: io.BufferedIOBase) -> int:",
            "    \"\"\"",
            "    Find the offset of the end of a file without changing the file pointer.",
            "",
            "    Args:",
            "        file: File object",
            "",
            "    Returns:",
            "        Offset of the end of a file.",
            "    \"\"\"",
            "    old_pos = file.tell()  # store old position",
            "    file.seek(0, io.SEEK_END)  # move file pointer to end of file",
            "    end = file.tell()  # return end of file offset",
            "    file.seek(old_pos, io.SEEK_SET)",
            "    return end",
            "",
            "",
            "def find_end_offset_next_n_lines_from_offset(",
            "    file: io.BufferedIOBase, start_offset: int, n: int",
            ") -> int:",
            "    \"\"\"",
            "    Find the offsets of next n lines from a start offset.",
            "",
            "    Args:",
            "        file: File object",
            "        start_offset: Start offset to read from, inclusive.",
            "        n: Number of lines to find.",
            "",
            "    Returns:",
            "        Offset of the end of the next n line (exclusive)",
            "    \"\"\"",
            "    file.seek(start_offset)  # move file pointer to start offset",
            "    end_offset = None",
            "    for _ in range(n):  # loop until we find n lines or reach end of file",
            "        line = file.readline()  # read a line and consume new line character",
            "        if not line:  # end of file",
            "            break",
            "        end_offset = file.tell()  # end offset.",
            "",
            "    logger.debug(f\"Found next {n} lines from {start_offset} offset\")",
            "    return (",
            "        end_offset if end_offset is not None else file.seek(0, io.SEEK_END)",
            "    )  # return last line offset or end of file offset if no lines found",
            "",
            "",
            "def find_start_offset_last_n_lines_from_offset(",
            "    file: io.BufferedIOBase, offset: int, n: int, block_size: int = BLOCK_SIZE",
            ") -> int:",
            "    \"\"\"",
            "    Find the offset of the beginning of the line of the last X lines from an offset.",
            "",
            "    Args:",
            "        file: File object",
            "        offset: Start offset from which to find last X lines, -1 means end of file.",
            "            The offset is exclusive, i.e. data at the offset is not included",
            "            in the result.",
            "        n: Number of lines to find",
            "        block_size: Block size to read from file",
            "",
            "    Returns:",
            "        Offset of the beginning of the line of the last X lines from a start offset.",
            "    \"\"\"",
            "    logger.debug(f\"Finding last {n} lines from {offset} offset\")",
            "    if offset == -1:",
            "        offset = file.seek(0, io.SEEK_END)  # move file pointer to end of file",
            "    else:",
            "        file.seek(offset, io.SEEK_SET)  # move file pointer to start offset",
            "",
            "    if n == 0:",
            "        return offset",
            "    nbytes_from_end = (",
            "        0  # Number of bytes that should be tailed from the end of the file",
            "    )",
            "    # Non new line terminating offset, adjust the line count and treat the non-newline",
            "    # terminated line as the last line. e.g. line 1\\nline 2",
            "    file.seek(max(0, offset - 1), os.SEEK_SET)",
            "    if file.read(1) != b\"\\n\":",
            "        n -= 1",
            "",
            "    # Remaining number of lines to tail",
            "    lines_more = n",
            "    read_offset = max(0, offset - block_size)",
            "    # So that we know how much to read on the last block (the block 0)",
            "    prev_offset = offset",
            "",
            "    while lines_more >= 0 and read_offset >= 0:",
            "        # Seek to the current block start",
            "        file.seek(read_offset, 0)",
            "        # Read the current block (or less than block) data",
            "        block_data = file.read(min(block_size, prev_offset - read_offset))",
            "        num_lines = block_data.count(b\"\\n\")",
            "        if num_lines > lines_more:",
            "            # This is the last block to read.",
            "            # Need to find the offset of exact number of lines to tail",
            "            # in the block.",
            "            # Use `split` here to split away the extra lines, i.e.",
            "            # first `num_lines - lines_more` lines.",
            "            lines = block_data.split(b\"\\n\", num_lines - lines_more)",
            "            # Added the len of those lines that at the end of the block.",
            "            nbytes_from_end += len(lines[-1])",
            "            break",
            "",
            "        # Need to read more blocks.",
            "        lines_more -= num_lines",
            "        nbytes_from_end += len(block_data)",
            "",
            "        if read_offset == 0:",
            "            # We have read all blocks (since the start)",
            "            break",
            "        # Continuing with the previous block",
            "        prev_offset = read_offset",
            "        read_offset = max(0, read_offset - block_size)",
            "",
            "    offset_read_start = offset - nbytes_from_end",
            "    assert (",
            "        offset_read_start >= 0",
            "    ), f\"Read start offset({offset_read_start}) should be non-negative\"",
            "    return offset_read_start",
            "",
            "",
            "async def _stream_log_in_chunk(",
            "    context: grpc.aio.ServicerContext,",
            "    file: io.BufferedIOBase,",
            "    start_offset: int,",
            "    end_offset: int = -1,",
            "    keep_alive_interval_sec: int = -1,",
            "    block_size: int = BLOCK_SIZE,",
            "):",
            "    \"\"\"Streaming log in chunk from start to end offset.",
            "",
            "    Stream binary file content in chunks from start offset to an end",
            "    offset if provided, else to the end of the file.",
            "",
            "    Args:",
            "        context: gRPC server side context",
            "        file: Binary file to stream",
            "        start_offset: File offset where streaming starts",
            "        end_offset: If -1, implying streaming til the EOF.",
            "        keep_alive_interval_sec: Duration for which streaming will be",
            "            retried when reaching the file end, -1 means no retry.",
            "        block_size: Number of bytes per chunk, exposed for testing",
            "",
            "    Return:",
            "        Async generator of StreamReply",
            "    \"\"\"",
            "    assert \"b\" in file.mode, \"Only binary file is supported.\"",
            "    assert not (",
            "        keep_alive_interval_sec >= 0 and end_offset != -1",
            "    ), \"Keep-alive is not allowed when specifying an end offset\"",
            "",
            "    file.seek(start_offset, 0)",
            "    cur_offset = start_offset",
            "",
            "    # Until gRPC is done",
            "    while not context.done():",
            "        # Read in block",
            "        if end_offset != -1:",
            "            to_read = min(end_offset - cur_offset, block_size)",
            "        else:",
            "            to_read = block_size",
            "",
            "        bytes = file.read(to_read)",
            "",
            "        if bytes == b\"\":",
            "            # Stop reading",
            "            if keep_alive_interval_sec >= 0:",
            "                await asyncio.sleep(keep_alive_interval_sec)",
            "                # Try reading again",
            "                continue",
            "",
            "            # Have read the entire file, done",
            "            break",
            "        logger.debug(f\"Sending {len(bytes)} bytes at {cur_offset}\")",
            "        yield reporter_pb2.StreamLogReply(data=bytes)",
            "",
            "        # Have read the requested section [start_offset, end_offset), done",
            "        cur_offset += len(bytes)",
            "        if end_offset != -1 and cur_offset >= end_offset:",
            "            break",
            "",
            "",
            "class LogAgent(dashboard_utils.DashboardAgentModule):",
            "    def __init__(self, dashboard_agent):",
            "        super().__init__(dashboard_agent)",
            "        log_utils.register_mimetypes()",
            "        routes.static(\"/logs\", self._dashboard_agent.log_dir, show_index=True)",
            "",
            "    async def run(self, server):",
            "        pass",
            "",
            "    @staticmethod",
            "    def is_minimal_module():",
            "        return False",
            "",
            "",
            "_task_log_search_worker_pool = concurrent.futures.ThreadPoolExecutor(",
            "    max_workers=RAY_DASHBOARD_LOG_TASK_LOG_SEARCH_MAX_WORKER_COUNT",
            ")",
            "",
            "",
            "class LogAgentV1Grpc(dashboard_utils.DashboardAgentModule):",
            "    def __init__(self, dashboard_agent):",
            "        super().__init__(dashboard_agent)",
            "",
            "    async def run(self, server):",
            "        if server:",
            "            reporter_pb2_grpc.add_LogServiceServicer_to_server(self, server)",
            "",
            "    @staticmethod",
            "    def is_minimal_module():",
            "        # Dashboard is only available with non-minimal install now.",
            "        return False",
            "",
            "    async def ListLogs(self, request, context):",
            "        \"\"\"",
            "        Lists all files in the active Ray logs directory.",
            "",
            "        Part of `LogService` gRPC.",
            "",
            "        NOTE: These RPCs are used by state_head.py, not log_head.py",
            "        \"\"\"",
            "        path = Path(self._dashboard_agent.log_dir)",
            "        if not path.exists():",
            "            raise FileNotFoundError(",
            "                f\"Could not find log dir at path: {self._dashboard_agent.log_dir}\"",
            "                \"It is unexpected. Please report an issue to Ray Github.\"",
            "            )",
            "        log_files = []",
            "        for p in path.glob(request.glob_filter):",
            "            log_files.append(str(p.relative_to(path)))",
            "        return reporter_pb2.ListLogsReply(log_files=log_files)",
            "",
            "    @classmethod",
            "    async def _find_task_log_offsets(",
            "        cls, task_id: str, attempt_number: int, lines: int, f: io.BufferedIOBase",
            "    ) -> Tuple[int, int]:",
            "        \"\"\"Find the start and end offsets in the log file for a task attempt",
            "        Current task log is in the format of below:",
            "",
            "            :job_id:xxx",
            "            :task_name:xxx",
            "            :task_attempt_start:<task_id>-<attempt_number>",
            "            ...",
            "            actual user logs",
            "            ...",
            "            :task_attempt_end:<task_id>-<attempt_number>",
            "            ... (other tasks)",
            "",
            "",
            "        For async actor tasks, task logs from multiple tasks might however",
            "        be interleaved.",
            "        \"\"\"",
            "",
            "        # Find start",
            "        task_attempt_start_magic_line = (",
            "            f\"{LOG_PREFIX_TASK_ATTEMPT_START}{task_id}-{attempt_number}\\n\"",
            "        )",
            "",
            "        # Offload the heavy IO CPU work to a thread pool to avoid blocking the",
            "        # event loop for concurrent requests.",
            "        task_attempt_magic_line_offset = (",
            "            await asyncio.get_running_loop().run_in_executor(",
            "                _task_log_search_worker_pool,",
            "                find_offset_of_content_in_file,",
            "                f,",
            "                task_attempt_start_magic_line.encode(),",
            "            )",
            "        )",
            "",
            "        if task_attempt_magic_line_offset == -1:",
            "            raise FileNotFoundError(",
            "                f\"Log for task attempt({task_id},{attempt_number}) not found\"",
            "            )",
            "        start_offset = task_attempt_magic_line_offset + len(",
            "            task_attempt_start_magic_line",
            "        )",
            "",
            "        # Find the end of the task log, which is the start of the next task log if any",
            "        # with the LOG_PREFIX_TASK_ATTEMPT_END magic line.",
            "        task_attempt_end_magic_line = (",
            "            f\"{LOG_PREFIX_TASK_ATTEMPT_END}{task_id}-{attempt_number}\\n\"",
            "        )",
            "        end_offset = await asyncio.get_running_loop().run_in_executor(",
            "            _task_log_search_worker_pool,",
            "            find_offset_of_content_in_file,",
            "            f,",
            "            task_attempt_end_magic_line.encode(),",
            "            start_offset,",
            "        )",
            "",
            "        if end_offset == -1:",
            "            # No other tasks (might still be running), stream til the end.",
            "            end_offset = find_end_offset_file(f)",
            "",
            "        if lines != -1:",
            "            # Tail lines specified, find end_offset - lines offsets.",
            "            start_offset = max(",
            "                find_start_offset_last_n_lines_from_offset(f, end_offset, lines),",
            "                start_offset,",
            "            )",
            "",
            "        return start_offset, end_offset",
            "",
            "    async def StreamLog(self, request, context):",
            "        \"\"\"",
            "        Streams the log in real time starting from `request.lines` number of lines from",
            "        the end of the file if `request.keep_alive == True`. Else, it terminates the",
            "        stream once there are no more bytes to read from the log file.",
            "",
            "        Part of `LogService` gRPC.",
            "",
            "        NOTE: These RPCs are used by state_head.py, not log_head.py",
            "        \"\"\"",
            "        # NOTE: If the client side connection is closed, this handler will",
            "        # be automatically terminated.",
            "        lines = request.lines if request.lines else 1000",
            "",
            "        if not Path(request.log_file_name).is_absolute():",
            "            filepath = Path(self._dashboard_agent.log_dir) / request.log_file_name",
            "        else:",
            "            filepath = Path(request.log_file_name)",
            "",
            "        if not filepath.is_file():",
            "            await context.send_initial_metadata(",
            "                [[log_consts.LOG_GRPC_ERROR, log_consts.FILE_NOT_FOUND]]",
            "            )",
            "        else:",
            "            with open(filepath, \"rb\") as f:",
            "                await context.send_initial_metadata([])",
            "",
            "                # Default stream entire file",
            "                start_offset = (",
            "                    request.start_offset if request.HasField(\"start_offset\") else 0",
            "                )",
            "                end_offset = (",
            "                    request.end_offset",
            "                    if request.HasField(\"end_offset\")",
            "                    else find_end_offset_file(f)",
            "                )",
            "",
            "                if lines != -1:",
            "                    # If specified tail line number, cap the start offset",
            "                    # with lines from the current end offset",
            "                    start_offset = max(",
            "                        find_start_offset_last_n_lines_from_offset(",
            "                            f, offset=end_offset, n=lines",
            "                        ),",
            "                        start_offset,",
            "                    )",
            "",
            "                # If keep alive: following the log every 'interval'",
            "                keep_alive_interval_sec = -1",
            "                if request.keep_alive:",
            "                    keep_alive_interval_sec = (",
            "                        request.interval",
            "                        if request.interval",
            "                        else DEFAULT_KEEP_ALIVE_INTERVAL_SEC",
            "                    )",
            "",
            "                    # When following (keep_alive), it will read beyond the end",
            "                    end_offset = -1",
            "",
            "                logger.info(",
            "                    f\"Tailing logs from {start_offset} to {end_offset} for \"",
            "                    f\"lines={lines}, with keep_alive={keep_alive_interval_sec}\"",
            "                )",
            "",
            "                # Read and send the file data in chunk",
            "                async for chunk_res in _stream_log_in_chunk(",
            "                    context=context,",
            "                    file=f,",
            "                    start_offset=start_offset,",
            "                    end_offset=end_offset,",
            "                    keep_alive_interval_sec=keep_alive_interval_sec,",
            "                ):",
            "                    yield chunk_res"
        ],
        "afterPatchFile": [
            "import logging",
            "from typing import Optional, Tuple",
            "",
            "import concurrent.futures",
            "import ray.dashboard.modules.log.log_utils as log_utils",
            "import ray.dashboard.modules.log.log_consts as log_consts",
            "import ray.dashboard.utils as dashboard_utils",
            "import ray.dashboard.optional_utils as dashboard_optional_utils",
            "from ray._private.ray_constants import env_integer",
            "import asyncio",
            "import grpc",
            "import io",
            "import os",
            "",
            "",
            "from pathlib import Path",
            "",
            "from ray.core.generated import reporter_pb2",
            "from ray.core.generated import reporter_pb2_grpc",
            "from ray._private.ray_constants import (",
            "    LOG_PREFIX_TASK_ATTEMPT_START,",
            "    LOG_PREFIX_TASK_ATTEMPT_END,",
            ")",
            "",
            "logger = logging.getLogger(__name__)",
            "routes = dashboard_optional_utils.DashboardAgentRouteTable",
            "",
            "# 64 KB",
            "BLOCK_SIZE = 1 << 16",
            "",
            "# Keep-alive interval for reading the file",
            "DEFAULT_KEEP_ALIVE_INTERVAL_SEC = 1",
            "",
            "RAY_DASHBOARD_LOG_TASK_LOG_SEARCH_MAX_WORKER_COUNT = env_integer(",
            "    \"RAY_DASHBOARD_LOG_TASK_LOG_SEARCH_MAX_WORKER_COUNT\", default=2",
            ")",
            "",
            "",
            "def find_offset_of_content_in_file(",
            "    file: io.BufferedIOBase, content: bytes, start_offset: int = 0",
            ") -> int:",
            "    \"\"\"Find the offset of the first occurrence of content in a file.",
            "",
            "    Args:",
            "        file: File object",
            "        content: Content to find",
            "        start_offset: Start offset to read from, inclusive.",
            "",
            "    Returns:",
            "        Offset of the first occurrence of content in a file.",
            "    \"\"\"",
            "    logger.debug(f\"Finding offset of content {content} in file\")",
            "    file.seek(start_offset, io.SEEK_SET)  # move file pointer to start of file",
            "    offset = start_offset",
            "    while True:",
            "        # Read in block",
            "        block_data = file.read(BLOCK_SIZE)",
            "        if block_data == b\"\":",
            "            # Stop reading",
            "            return -1",
            "        # Find the offset of the first occurrence of content in the block",
            "        block_offset = block_data.find(content)",
            "        if block_offset != -1:",
            "            # Found the offset in the block",
            "            return offset + block_offset",
            "        # Continue reading",
            "        offset += len(block_data)",
            "",
            "",
            "def find_end_offset_file(file: io.BufferedIOBase) -> int:",
            "    \"\"\"",
            "    Find the offset of the end of a file without changing the file pointer.",
            "",
            "    Args:",
            "        file: File object",
            "",
            "    Returns:",
            "        Offset of the end of a file.",
            "    \"\"\"",
            "    old_pos = file.tell()  # store old position",
            "    file.seek(0, io.SEEK_END)  # move file pointer to end of file",
            "    end = file.tell()  # return end of file offset",
            "    file.seek(old_pos, io.SEEK_SET)",
            "    return end",
            "",
            "",
            "def find_end_offset_next_n_lines_from_offset(",
            "    file: io.BufferedIOBase, start_offset: int, n: int",
            ") -> int:",
            "    \"\"\"",
            "    Find the offsets of next n lines from a start offset.",
            "",
            "    Args:",
            "        file: File object",
            "        start_offset: Start offset to read from, inclusive.",
            "        n: Number of lines to find.",
            "",
            "    Returns:",
            "        Offset of the end of the next n line (exclusive)",
            "    \"\"\"",
            "    file.seek(start_offset)  # move file pointer to start offset",
            "    end_offset = None",
            "    for _ in range(n):  # loop until we find n lines or reach end of file",
            "        line = file.readline()  # read a line and consume new line character",
            "        if not line:  # end of file",
            "            break",
            "        end_offset = file.tell()  # end offset.",
            "",
            "    logger.debug(f\"Found next {n} lines from {start_offset} offset\")",
            "    return (",
            "        end_offset if end_offset is not None else file.seek(0, io.SEEK_END)",
            "    )  # return last line offset or end of file offset if no lines found",
            "",
            "",
            "def find_start_offset_last_n_lines_from_offset(",
            "    file: io.BufferedIOBase, offset: int, n: int, block_size: int = BLOCK_SIZE",
            ") -> int:",
            "    \"\"\"",
            "    Find the offset of the beginning of the line of the last X lines from an offset.",
            "",
            "    Args:",
            "        file: File object",
            "        offset: Start offset from which to find last X lines, -1 means end of file.",
            "            The offset is exclusive, i.e. data at the offset is not included",
            "            in the result.",
            "        n: Number of lines to find",
            "        block_size: Block size to read from file",
            "",
            "    Returns:",
            "        Offset of the beginning of the line of the last X lines from a start offset.",
            "    \"\"\"",
            "    logger.debug(f\"Finding last {n} lines from {offset} offset\")",
            "    if offset == -1:",
            "        offset = file.seek(0, io.SEEK_END)  # move file pointer to end of file",
            "    else:",
            "        file.seek(offset, io.SEEK_SET)  # move file pointer to start offset",
            "",
            "    if n == 0:",
            "        return offset",
            "    nbytes_from_end = (",
            "        0  # Number of bytes that should be tailed from the end of the file",
            "    )",
            "    # Non new line terminating offset, adjust the line count and treat the non-newline",
            "    # terminated line as the last line. e.g. line 1\\nline 2",
            "    file.seek(max(0, offset - 1), os.SEEK_SET)",
            "    if file.read(1) != b\"\\n\":",
            "        n -= 1",
            "",
            "    # Remaining number of lines to tail",
            "    lines_more = n",
            "    read_offset = max(0, offset - block_size)",
            "    # So that we know how much to read on the last block (the block 0)",
            "    prev_offset = offset",
            "",
            "    while lines_more >= 0 and read_offset >= 0:",
            "        # Seek to the current block start",
            "        file.seek(read_offset, 0)",
            "        # Read the current block (or less than block) data",
            "        block_data = file.read(min(block_size, prev_offset - read_offset))",
            "        num_lines = block_data.count(b\"\\n\")",
            "        if num_lines > lines_more:",
            "            # This is the last block to read.",
            "            # Need to find the offset of exact number of lines to tail",
            "            # in the block.",
            "            # Use `split` here to split away the extra lines, i.e.",
            "            # first `num_lines - lines_more` lines.",
            "            lines = block_data.split(b\"\\n\", num_lines - lines_more)",
            "            # Added the len of those lines that at the end of the block.",
            "            nbytes_from_end += len(lines[-1])",
            "            break",
            "",
            "        # Need to read more blocks.",
            "        lines_more -= num_lines",
            "        nbytes_from_end += len(block_data)",
            "",
            "        if read_offset == 0:",
            "            # We have read all blocks (since the start)",
            "            break",
            "        # Continuing with the previous block",
            "        prev_offset = read_offset",
            "        read_offset = max(0, read_offset - block_size)",
            "",
            "    offset_read_start = offset - nbytes_from_end",
            "    assert (",
            "        offset_read_start >= 0",
            "    ), f\"Read start offset({offset_read_start}) should be non-negative\"",
            "    return offset_read_start",
            "",
            "",
            "async def _stream_log_in_chunk(",
            "    context: grpc.aio.ServicerContext,",
            "    file: io.BufferedIOBase,",
            "    start_offset: int,",
            "    end_offset: int = -1,",
            "    keep_alive_interval_sec: int = -1,",
            "    block_size: int = BLOCK_SIZE,",
            "):",
            "    \"\"\"Streaming log in chunk from start to end offset.",
            "",
            "    Stream binary file content in chunks from start offset to an end",
            "    offset if provided, else to the end of the file.",
            "",
            "    Args:",
            "        context: gRPC server side context",
            "        file: Binary file to stream",
            "        start_offset: File offset where streaming starts",
            "        end_offset: If -1, implying streaming til the EOF.",
            "        keep_alive_interval_sec: Duration for which streaming will be",
            "            retried when reaching the file end, -1 means no retry.",
            "        block_size: Number of bytes per chunk, exposed for testing",
            "",
            "    Return:",
            "        Async generator of StreamReply",
            "    \"\"\"",
            "    assert \"b\" in file.mode, \"Only binary file is supported.\"",
            "    assert not (",
            "        keep_alive_interval_sec >= 0 and end_offset != -1",
            "    ), \"Keep-alive is not allowed when specifying an end offset\"",
            "",
            "    file.seek(start_offset, 0)",
            "    cur_offset = start_offset",
            "",
            "    # Until gRPC is done",
            "    while not context.done():",
            "        # Read in block",
            "        if end_offset != -1:",
            "            to_read = min(end_offset - cur_offset, block_size)",
            "        else:",
            "            to_read = block_size",
            "",
            "        bytes = file.read(to_read)",
            "",
            "        if bytes == b\"\":",
            "            # Stop reading",
            "            if keep_alive_interval_sec >= 0:",
            "                await asyncio.sleep(keep_alive_interval_sec)",
            "                # Try reading again",
            "                continue",
            "",
            "            # Have read the entire file, done",
            "            break",
            "        logger.debug(f\"Sending {len(bytes)} bytes at {cur_offset}\")",
            "        yield reporter_pb2.StreamLogReply(data=bytes)",
            "",
            "        # Have read the requested section [start_offset, end_offset), done",
            "        cur_offset += len(bytes)",
            "        if end_offset != -1 and cur_offset >= end_offset:",
            "            break",
            "",
            "",
            "class LogAgent(dashboard_utils.DashboardAgentModule):",
            "    def __init__(self, dashboard_agent):",
            "        super().__init__(dashboard_agent)",
            "        log_utils.register_mimetypes()",
            "        routes.static(\"/logs\", self._dashboard_agent.log_dir, show_index=True)",
            "",
            "    async def run(self, server):",
            "        pass",
            "",
            "    @staticmethod",
            "    def is_minimal_module():",
            "        return False",
            "",
            "",
            "_task_log_search_worker_pool = concurrent.futures.ThreadPoolExecutor(",
            "    max_workers=RAY_DASHBOARD_LOG_TASK_LOG_SEARCH_MAX_WORKER_COUNT",
            ")",
            "",
            "",
            "class LogAgentV1Grpc(dashboard_utils.DashboardAgentModule):",
            "    def __init__(self, dashboard_agent):",
            "        super().__init__(dashboard_agent)",
            "",
            "    async def run(self, server):",
            "        if server:",
            "            reporter_pb2_grpc.add_LogServiceServicer_to_server(self, server)",
            "",
            "    @property",
            "    def node_id(self) -> Optional[str]:",
            "        return self._dashboard_agent.get_node_id()",
            "",
            "    @staticmethod",
            "    def is_minimal_module():",
            "        # Dashboard is only available with non-minimal install now.",
            "        return False",
            "",
            "    async def ListLogs(self, request, context):",
            "        \"\"\"",
            "        Lists all files in the active Ray logs directory.",
            "",
            "        Part of `LogService` gRPC.",
            "",
            "        NOTE: These RPCs are used by state_head.py, not log_head.py",
            "        \"\"\"",
            "        path = Path(self._dashboard_agent.log_dir)",
            "        if not path.exists():",
            "            raise FileNotFoundError(",
            "                f\"Could not find log dir at path: {self._dashboard_agent.log_dir}\"",
            "                \"It is unexpected. Please report an issue to Ray Github.\"",
            "            )",
            "        log_files = []",
            "        for p in path.glob(request.glob_filter):",
            "            log_files.append(str(p.relative_to(path)))",
            "        return reporter_pb2.ListLogsReply(log_files=log_files)",
            "",
            "    @classmethod",
            "    async def _find_task_log_offsets(",
            "        cls, task_id: str, attempt_number: int, lines: int, f: io.BufferedIOBase",
            "    ) -> Tuple[int, int]:",
            "        \"\"\"Find the start and end offsets in the log file for a task attempt",
            "        Current task log is in the format of below:",
            "",
            "            :job_id:xxx",
            "            :task_name:xxx",
            "            :task_attempt_start:<task_id>-<attempt_number>",
            "            ...",
            "            actual user logs",
            "            ...",
            "            :task_attempt_end:<task_id>-<attempt_number>",
            "            ... (other tasks)",
            "",
            "",
            "        For async actor tasks, task logs from multiple tasks might however",
            "        be interleaved.",
            "        \"\"\"",
            "",
            "        # Find start",
            "        task_attempt_start_magic_line = (",
            "            f\"{LOG_PREFIX_TASK_ATTEMPT_START}{task_id}-{attempt_number}\\n\"",
            "        )",
            "",
            "        # Offload the heavy IO CPU work to a thread pool to avoid blocking the",
            "        # event loop for concurrent requests.",
            "        task_attempt_magic_line_offset = (",
            "            await asyncio.get_running_loop().run_in_executor(",
            "                _task_log_search_worker_pool,",
            "                find_offset_of_content_in_file,",
            "                f,",
            "                task_attempt_start_magic_line.encode(),",
            "            )",
            "        )",
            "",
            "        if task_attempt_magic_line_offset == -1:",
            "            raise FileNotFoundError(",
            "                f\"Log for task attempt({task_id},{attempt_number}) not found\"",
            "            )",
            "        start_offset = task_attempt_magic_line_offset + len(",
            "            task_attempt_start_magic_line",
            "        )",
            "",
            "        # Find the end of the task log, which is the start of the next task log if any",
            "        # with the LOG_PREFIX_TASK_ATTEMPT_END magic line.",
            "        task_attempt_end_magic_line = (",
            "            f\"{LOG_PREFIX_TASK_ATTEMPT_END}{task_id}-{attempt_number}\\n\"",
            "        )",
            "        end_offset = await asyncio.get_running_loop().run_in_executor(",
            "            _task_log_search_worker_pool,",
            "            find_offset_of_content_in_file,",
            "            f,",
            "            task_attempt_end_magic_line.encode(),",
            "            start_offset,",
            "        )",
            "",
            "        if end_offset == -1:",
            "            # No other tasks (might still be running), stream til the end.",
            "            end_offset = find_end_offset_file(f)",
            "",
            "        if lines != -1:",
            "            # Tail lines specified, find end_offset - lines offsets.",
            "            start_offset = max(",
            "                find_start_offset_last_n_lines_from_offset(f, end_offset, lines),",
            "                start_offset,",
            "            )",
            "",
            "        return start_offset, end_offset",
            "",
            "    @classmethod",
            "    def _resolve_filename(cls, root_log_dir: Path, filename: str) -> Path:",
            "        \"\"\"",
            "        Resolves the file path relative to the root log directory.",
            "",
            "        Args:",
            "            root_log_dir: Root log directory.",
            "            filename: File path relative to the root log directory.",
            "",
            "        Raises:",
            "            FileNotFoundError: If the file path is invalid.",
            "",
            "        Returns:",
            "            The absolute file path resolved from the root log directory.",
            "        \"\"\"",
            "        if not Path(filename).is_absolute():",
            "            filepath = root_log_dir / filename",
            "        else:",
            "            filepath = Path(filename)",
            "",
            "        # We want to allow relative paths that include symlinks pointing outside of the",
            "        # `root_log_dir`, so use `os.path.abspath` instead of `Path.resolve()` because",
            "        # `os.path.abspath` does not resolve symlinks.",
            "        filepath = Path(os.path.abspath(filepath))",
            "",
            "        if not filepath.is_file():",
            "            raise FileNotFoundError(f\"A file is not found at: {filepath}\")",
            "",
            "        try:",
            "            filepath.relative_to(root_log_dir)",
            "        except ValueError as e:",
            "            raise FileNotFoundError(f\"{filepath} not in {root_log_dir}: {e}\")",
            "",
            "        # Fully resolve the path before returning (including following symlinks).",
            "        return filepath.resolve()",
            "",
            "    async def StreamLog(self, request, context):",
            "        \"\"\"",
            "        Streams the log in real time starting from `request.lines` number of lines from",
            "        the end of the file if `request.keep_alive == True`. Else, it terminates the",
            "        stream once there are no more bytes to read from the log file.",
            "",
            "        Part of `LogService` gRPC.",
            "",
            "        NOTE: These RPCs are used by state_head.py, not log_head.py",
            "        \"\"\"",
            "        # NOTE: If the client side connection is closed, this handler will",
            "        # be automatically terminated.",
            "        lines = request.lines if request.lines else 1000",
            "",
            "        try:",
            "            filepath = self._resolve_filename(",
            "                Path(self._dashboard_agent.log_dir), request.log_file_name",
            "            )",
            "        except FileNotFoundError as e:",
            "            await context.send_initial_metadata([[log_consts.LOG_GRPC_ERROR, str(e)]])",
            "        else:",
            "            with open(filepath, \"rb\") as f:",
            "                await context.send_initial_metadata([])",
            "",
            "                # Default stream entire file",
            "                start_offset = (",
            "                    request.start_offset if request.HasField(\"start_offset\") else 0",
            "                )",
            "                end_offset = (",
            "                    request.end_offset",
            "                    if request.HasField(\"end_offset\")",
            "                    else find_end_offset_file(f)",
            "                )",
            "",
            "                if lines != -1:",
            "                    # If specified tail line number, cap the start offset",
            "                    # with lines from the current end offset",
            "                    start_offset = max(",
            "                        find_start_offset_last_n_lines_from_offset(",
            "                            f, offset=end_offset, n=lines",
            "                        ),",
            "                        start_offset,",
            "                    )",
            "",
            "                # If keep alive: following the log every 'interval'",
            "                keep_alive_interval_sec = -1",
            "                if request.keep_alive:",
            "                    keep_alive_interval_sec = (",
            "                        request.interval",
            "                        if request.interval",
            "                        else DEFAULT_KEEP_ALIVE_INTERVAL_SEC",
            "                    )",
            "",
            "                    # When following (keep_alive), it will read beyond the end",
            "                    end_offset = -1",
            "",
            "                logger.info(",
            "                    f\"Tailing logs from {start_offset} to {end_offset} for \"",
            "                    f\"lines={lines}, with keep_alive={keep_alive_interval_sec}\"",
            "                )",
            "",
            "                # Read and send the file data in chunk",
            "                async for chunk_res in _stream_log_in_chunk(",
            "                    context=context,",
            "                    file=f,",
            "                    start_offset=start_offset,",
            "                    end_offset=end_offset,",
            "                    keep_alive_interval_sec=keep_alive_interval_sec,",
            "                ):",
            "                    yield chunk_res"
        ],
        "action": [
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "2": [],
            "387": [
                "LogAgentV1Grpc"
            ],
            "388": [
                "LogAgentV1Grpc"
            ],
            "389": [
                "LogAgentV1Grpc"
            ],
            "390": [
                "LogAgentV1Grpc"
            ],
            "391": [
                "LogAgentV1Grpc"
            ],
            "392": [
                "LogAgentV1Grpc"
            ],
            "393": [
                "LogAgentV1Grpc"
            ],
            "394": [
                "LogAgentV1Grpc"
            ]
        },
        "addLocation": []
    },
    "dashboard/modules/log/log_consts.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " }"
            },
            "1": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " LOG_GRPC_ERROR = \"log_grpc_status\""
            },
            "3": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-FILE_NOT_FOUND = \"LOG_GRPC_ERROR: file_not_found\""
            },
            "4": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " # 10 seconds"
            },
            "6": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " GRPC_TIMEOUT = 10"
            }
        },
        "frontPatchFile": [
            "MIME_TYPES = {",
            "    \"text/plain\": [\".err\", \".out\", \".log\"],",
            "}",
            "",
            "LOG_GRPC_ERROR = \"log_grpc_status\"",
            "FILE_NOT_FOUND = \"LOG_GRPC_ERROR: file_not_found\"",
            "",
            "# 10 seconds",
            "GRPC_TIMEOUT = 10"
        ],
        "afterPatchFile": [
            "MIME_TYPES = {",
            "    \"text/plain\": [\".err\", \".out\", \".log\"],",
            "}",
            "",
            "LOG_GRPC_ERROR = \"log_grpc_status\"",
            "",
            "# 10 seconds",
            "GRPC_TIMEOUT = 10"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "6": [
                "FILE_NOT_FOUND"
            ]
        },
        "addLocation": []
    },
    "python/ray/tests/conftest.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1248,
                "afterPatchRowNumber": 1248,
                "PatchRowcode": "         yield fp"
            },
            "1": {
                "beforePatchRowNumber": 1249,
                "afterPatchRowNumber": 1249,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 1250,
                "afterPatchRowNumber": 1250,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1251,
                "PatchRowcode": "+@pytest.fixture(scope=\"function\")"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1252,
                "PatchRowcode": "+def temp_dir(request):"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1253,
                "PatchRowcode": "+    with tempfile.TemporaryDirectory(\"r+b\") as d:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1254,
                "PatchRowcode": "+        yield d"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1255,
                "PatchRowcode": "+"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1256,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": 1251,
                "afterPatchRowNumber": 1257,
                "PatchRowcode": " @pytest.fixture(scope=\"module\")"
            },
            "10": {
                "beforePatchRowNumber": 1252,
                "afterPatchRowNumber": 1258,
                "PatchRowcode": " def random_ascii_file(request):"
            },
            "11": {
                "beforePatchRowNumber": 1253,
                "afterPatchRowNumber": 1259,
                "PatchRowcode": "     import random"
            }
        },
        "frontPatchFile": [
            "\"\"\"",
            "This file defines the common pytest fixtures used in current directory.",
            "\"\"\"",
            "import json",
            "import logging",
            "import os",
            "import platform",
            "import shutil",
            "import socket",
            "import subprocess",
            "import tempfile",
            "import time",
            "from contextlib import contextmanager",
            "from pathlib import Path",
            "from tempfile import gettempdir",
            "from typing import List, Tuple",
            "from unittest import mock",
            "import psutil",
            "import pytest",
            "",
            "import ray",
            "import ray._private.ray_constants as ray_constants",
            "from ray._private.conftest_utils import set_override_dashboard_url  # noqa: F401",
            "from ray._private.runtime_env.pip import PipProcessor",
            "from ray._private.runtime_env.plugin_schema_manager import RuntimeEnvPluginSchemaManager",
            "",
            "from ray._private.test_utils import (",
            "    get_and_run_node_killer,",
            "    init_error_pubsub,",
            "    init_log_pubsub,",
            "    setup_tls,",
            "    teardown_tls,",
            "    enable_external_redis,",
            "    redis_replicas,",
            "    get_redis_cli,",
            "    start_redis_instance,",
            "    find_available_port,",
            "    wait_for_condition,",
            "    find_free_port,",
            ")",
            "from ray.cluster_utils import AutoscalingCluster, Cluster, cluster_not_supported",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "START_REDIS_WAIT_RETRIES = int(os.environ.get(\"RAY_START_REDIS_WAIT_RETRIES\", \"60\"))",
            "",
            "",
            "@pytest.fixture(autouse=True)",
            "def pre_envs(monkeypatch):",
            "    # To make test run faster",
            "    monkeypatch.setenv(\"RAY_NUM_REDIS_GET_RETRIES\", \"2\")",
            "    ray_constants.NUM_REDIS_GET_RETRIES = 2",
            "    yield",
            "",
            "",
            "def wait_for_redis_to_start(redis_ip_address: str, redis_port: bool, password=None):",
            "    \"\"\"Wait for a Redis server to be available.",
            "",
            "    This is accomplished by creating a Redis client and sending a random",
            "    command to the server until the command gets through.",
            "",
            "    Args:",
            "        redis_ip_address: The IP address of the redis server.",
            "        redis_port: The port of the redis server.",
            "        password: The password of the redis server.",
            "",
            "    Raises:",
            "        Exception: An exception is raised if we could not connect with Redis.",
            "    \"\"\"",
            "    import redis",
            "",
            "    redis_client = redis.StrictRedis(",
            "        host=redis_ip_address, port=redis_port, password=password",
            "    )",
            "    # Wait for the Redis server to start.",
            "    num_retries = START_REDIS_WAIT_RETRIES",
            "",
            "    delay = 0.001",
            "    for i in range(num_retries):",
            "        try:",
            "            # Run some random command and see if it worked.",
            "            logger.debug(",
            "                \"Waiting for redis server at {}:{} to respond...\".format(",
            "                    redis_ip_address, redis_port",
            "                )",
            "            )",
            "            redis_client.client_list()",
            "        # If the Redis service is delayed getting set up for any reason, we may",
            "        # get a redis.ConnectionError: Error 111 connecting to host:port.",
            "        # Connection refused.",
            "        # Unfortunately, redis.ConnectionError is also the base class of",
            "        # redis.AuthenticationError. We *don't* want to obscure a",
            "        # redis.AuthenticationError, because that indicates the user provided a",
            "        # bad password. Thus a double except clause to ensure a",
            "        # redis.AuthenticationError isn't trapped here.",
            "        except redis.AuthenticationError as authEx:",
            "            raise RuntimeError(",
            "                f\"Unable to connect to Redis at {redis_ip_address}:{redis_port}.\"",
            "            ) from authEx",
            "        except redis.ConnectionError as connEx:",
            "            if i >= num_retries - 1:",
            "                raise RuntimeError(",
            "                    f\"Unable to connect to Redis at {redis_ip_address}:\"",
            "                    f\"{redis_port} after {num_retries} retries. Check that \"",
            "                    f\"{redis_ip_address}:{redis_port} is reachable from this \"",
            "                    \"machine. If it is not, your firewall may be blocking \"",
            "                    \"this port. If the problem is a flaky connection, try \"",
            "                    \"setting the environment variable \"",
            "                    \"`RAY_START_REDIS_WAIT_RETRIES` to increase the number of\"",
            "                    \" attempts to ping the Redis server.\"",
            "                ) from connEx",
            "            # Wait a little bit.",
            "            time.sleep(delay)",
            "            # Make sure the retry interval doesn't increase too large, which will",
            "            # affect the delivery time of the Ray cluster.",
            "            delay = min(1, delay * 2)",
            "        else:",
            "            break",
            "    else:",
            "        raise RuntimeError(",
            "            f\"Unable to connect to Redis (after {num_retries} retries). \"",
            "            \"If the Redis instance is on a different machine, check that \"",
            "            \"your firewall and relevant Ray ports are configured properly. \"",
            "            \"You can also set the environment variable \"",
            "            \"`RAY_START_REDIS_WAIT_RETRIES` to increase the number of \"",
            "            \"attempts to ping the Redis server.\"",
            "        )",
            "",
            "",
            "def get_default_fixure_system_config():",
            "    system_config = {",
            "        \"object_timeout_milliseconds\": 200,",
            "        \"health_check_initial_delay_ms\": 0,",
            "        \"health_check_failure_threshold\": 10,",
            "        \"object_store_full_delay_ms\": 100,",
            "    }",
            "    return system_config",
            "",
            "",
            "def get_default_fixture_ray_kwargs():",
            "    system_config = get_default_fixure_system_config()",
            "    ray_kwargs = {",
            "        \"num_cpus\": 1,",
            "        \"object_store_memory\": 150 * 1024 * 1024,",
            "        \"dashboard_port\": None,",
            "        \"namespace\": \"default_test_namespace\",",
            "        \"_system_config\": system_config,",
            "    }",
            "    return ray_kwargs",
            "",
            "",
            "def is_process_listen_to_port(pid, port):",
            "    retry_num = 10",
            "    interval_time = 0.5",
            "    for _ in range(retry_num):",
            "        try:",
            "            proc = psutil.Process(pid)",
            "            for conn in proc.connections():",
            "                if conn.status == \"LISTEN\" and conn.laddr.port == port:",
            "                    return True",
            "        except Exception:",
            "            pass",
            "        finally:",
            "            time.sleep(interval_time)",
            "    print(",
            "        f\"Process({pid}) has not listened to port {port} \"",
            "        + f\"for more than {retry_num * interval_time}s.\"",
            "    )",
            "    return False",
            "",
            "",
            "def redis_alive(port, enable_tls):",
            "    try:",
            "        # If there is no redis libs installed, skip the check.",
            "        # This could happen In minimal test, where we don't have",
            "        # redis.",
            "        import redis",
            "    except Exception:",
            "        return True",
            "",
            "    params = {}",
            "    if enable_tls:",
            "        from ray._raylet import Config",
            "",
            "        params = {\"ssl\": True, \"ssl_cert_reqs\": \"required\"}",
            "        if Config.REDIS_CA_CERT():",
            "            params[\"ssl_ca_certs\"] = Config.REDIS_CA_CERT()",
            "        if Config.REDIS_CLIENT_CERT():",
            "            params[\"ssl_certfile\"] = Config.REDIS_CLIENT_CERT()",
            "        if Config.REDIS_CLIENT_KEY():",
            "            params[\"ssl_keyfile\"] = Config.REDIS_CLIENT_KEY()",
            "",
            "    cli = redis.Redis(\"localhost\", port, **params)",
            "",
            "    try:",
            "        return cli.ping()",
            "    except Exception:",
            "        pass",
            "    return False",
            "",
            "",
            "def start_redis(db_dir):",
            "    retry_num = 0",
            "    while True:",
            "        is_need_restart = False",
            "        # Setup external Redis and env var for initialization.",
            "        redis_ports = find_available_port(49159, 55535, redis_replicas() * 2)",
            "        redis_ports = list(",
            "            zip(redis_ports[0 : redis_replicas()], redis_ports[redis_replicas() :])",
            "        )",
            "        processes = []",
            "        enable_tls = \"RAY_REDIS_CA_CERT\" in os.environ",
            "        leader_port = None",
            "        leader_id = None",
            "        redis_ports = []",
            "        while len(redis_ports) != redis_replicas():",
            "            temp_dir = ray._private.utils.get_ray_temp_dir()",
            "            port, free_port = find_available_port(49159, 55535, 2)",
            "            node_id, proc = start_redis_instance(",
            "                temp_dir,",
            "                port,",
            "                enable_tls=enable_tls,",
            "                replica_of=leader_port,",
            "                leader_id=leader_id,",
            "                db_dir=db_dir,",
            "                free_port=free_port,",
            "            )",
            "            try:",
            "                wait_for_condition(",
            "                    redis_alive, 3, 100, port=port, enable_tls=enable_tls",
            "                )",
            "            except Exception as e:",
            "                print(e)",
            "                continue",
            "            redis_ports.append(port)",
            "            if leader_port is None:",
            "                leader_port = port",
            "                leader_id = node_id",
            "            processes.append(proc)",
            "            # Check if th redis has started successfully and is listening on the port.",
            "            if not is_process_listen_to_port(proc.process.pid, port):",
            "                is_need_restart = True",
            "                break",
            "",
            "        if is_need_restart:",
            "            retry_num += 1",
            "            for proc in processes:",
            "                proc.process.kill()",
            "",
            "            if retry_num > 5:",
            "                raise RuntimeError(\"Failed to start redis after {retry_num} attempts.\")",
            "            print(",
            "                \"Retry to start redis because the process failed to \"",
            "                + f\"listen to the port({port}), retry num:{retry_num}.\"",
            "            )",
            "            continue",
            "",
            "        if redis_replicas() > 1:",
            "",
            "            redis_cli = get_redis_cli(str(leader_port), enable_tls)",
            "            while redis_cli.cluster(\"info\")[\"cluster_state\"] != \"ok\":",
            "                pass",
            "",
            "        scheme = \"rediss://\" if enable_tls else \"\"",
            "        address_str = f\"{scheme}127.0.0.1:{redis_ports[-1]}\"",
            "        return address_str, processes",
            "",
            "",
            "def kill_all_redis_server():",
            "    import psutil",
            "",
            "    # Find Redis server processes",
            "    redis_procs = []",
            "    for proc in psutil.process_iter([\"name\", \"cmdline\"]):",
            "        if proc.name() == \"redis-server\":",
            "            redis_procs.append(proc)",
            "",
            "    # Kill Redis server processes",
            "    for proc in redis_procs:",
            "        proc.kill()",
            "",
            "",
            "@contextmanager",
            "def _setup_redis(request):",
            "    with tempfile.TemporaryDirectory() as tmpdirname:",
            "        kill_all_redis_server()",
            "        address_str, processes = start_redis(tmpdirname)",
            "        old_addr = os.environ.get(\"RAY_REDIS_ADDRESS\")",
            "        os.environ[\"RAY_REDIS_ADDRESS\"] = address_str",
            "        import uuid",
            "",
            "        ns = str(uuid.uuid4())",
            "        old_ns = os.environ.get(\"RAY_external_storage_namespace\")",
            "        os.environ[\"RAY_external_storage_namespace\"] = ns",
            "",
            "        yield",
            "        if old_addr is not None:",
            "            os.environ[\"RAY_REDIS_ADDRESS\"] = old_addr",
            "        else:",
            "            del os.environ[\"RAY_REDIS_ADDRESS\"]",
            "",
            "        if old_ns is not None:",
            "            os.environ[\"RAY_external_storage_namespace\"] = old_ns",
            "        else:",
            "            del os.environ[\"RAY_external_storage_namespace\"]",
            "",
            "        for proc in processes:",
            "            proc.process.kill()",
            "        kill_all_redis_server()",
            "",
            "",
            "@pytest.fixture",
            "def maybe_external_redis(request):",
            "    if enable_external_redis():",
            "        with _setup_redis(request):",
            "            yield",
            "    else:",
            "        yield",
            "",
            "",
            "@pytest.fixture",
            "def external_redis(request):",
            "    with _setup_redis(request):",
            "        yield",
            "",
            "",
            "@pytest.fixture",
            "def shutdown_only(maybe_external_redis):",
            "    yield None",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def propagate_logs():",
            "    # Ensure that logs are propagated to ancestor handles. This is required if using the",
            "    # caplog fixture with Ray's logging.",
            "    # NOTE: This only enables log propagation in the driver process, not the workers!",
            "    logger = logging.getLogger(\"ray\")",
            "    logger.propagate = True",
            "    yield",
            "    logger.propagate = False",
            "",
            "",
            "# Provide a shared Ray instance for a test class",
            "@pytest.fixture(scope=\"class\")",
            "def class_ray_instance():",
            "    yield ray.init()",
            "    ray.shutdown()",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@contextmanager",
            "def _ray_start(**kwargs):",
            "    init_kwargs = get_default_fixture_ray_kwargs()",
            "    init_kwargs.update(kwargs)",
            "    # Start the Ray processes.",
            "    address_info = ray.init(\"local\", **init_kwargs)",
            "",
            "    yield address_info",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_with_dashboard(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    if param.get(\"num_cpus\") is None:",
            "        param[\"num_cpus\"] = 1",
            "    with _ray_start(include_dashboard=True, **param) as address_info:",
            "        yield address_info",
            "",
            "",
            "@pytest.fixture",
            "def make_sure_dashboard_http_port_unused():",
            "    \"\"\"Make sure the dashboard agent http port is unused.\"\"\"",
            "    for process in psutil.process_iter():",
            "        should_kill = False",
            "        try:",
            "            for conn in process.connections():",
            "                if conn.laddr.port == ray_constants.DEFAULT_DASHBOARD_AGENT_LISTEN_PORT:",
            "                    should_kill = True",
            "                    break",
            "        except Exception:",
            "            continue",
            "        if should_kill:",
            "            try:",
            "                process.kill()",
            "                process.wait()",
            "            except Exception:",
            "                pass",
            "    yield",
            "",
            "",
            "# The following fixture will start ray with 0 cpu.",
            "@pytest.fixture",
            "def ray_start_no_cpu(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(num_cpus=0, **param) as res:",
            "        yield res",
            "",
            "",
            "# The following fixture will start ray with 1 cpu.",
            "@pytest.fixture",
            "def ray_start_regular(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "# We can compose external_redis and ray_start_regular instead of creating this",
            "# separate fixture, if there is a good way to ensure external_redis runs before",
            "# ray_start_regular.",
            "@pytest.fixture",
            "def ray_start_regular_with_external_redis(request, external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture(scope=\"module\")",
            "def ray_start_regular_shared(request):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture(scope=\"module\", params=[{\"local_mode\": True}, {\"local_mode\": False}])",
            "def ray_start_shared_local_modes(request):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_2_cpus(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(num_cpus=2, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_10_cpus(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(num_cpus=10, **param) as res:",
            "        yield res",
            "",
            "",
            "@contextmanager",
            "def _ray_start_cluster(**kwargs):",
            "    cluster_not_supported_ = kwargs.pop(\"skip_cluster\", cluster_not_supported)",
            "    if cluster_not_supported_:",
            "        pytest.skip(\"Cluster not supported\")",
            "    init_kwargs = get_default_fixture_ray_kwargs()",
            "    num_nodes = 0",
            "    do_init = False",
            "    # num_nodes & do_init are not arguments for ray.init, so delete them.",
            "    if \"num_nodes\" in kwargs:",
            "        num_nodes = kwargs[\"num_nodes\"]",
            "        del kwargs[\"num_nodes\"]",
            "    if \"do_init\" in kwargs:",
            "        do_init = kwargs[\"do_init\"]",
            "        del kwargs[\"do_init\"]",
            "    elif num_nodes > 0:",
            "        do_init = True",
            "    init_kwargs.update(kwargs)",
            "    namespace = init_kwargs.pop(\"namespace\")",
            "    cluster = Cluster()",
            "    remote_nodes = []",
            "    for i in range(num_nodes):",
            "        if i > 0 and \"_system_config\" in init_kwargs:",
            "            del init_kwargs[\"_system_config\"]",
            "        remote_nodes.append(cluster.add_node(**init_kwargs))",
            "        # We assume driver will connect to the head (first node),",
            "        # so ray init will be invoked if do_init is true",
            "        if len(remote_nodes) == 1 and do_init:",
            "            ray.init(address=cluster.address, namespace=namespace)",
            "    yield cluster",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    cluster.shutdown()",
            "",
            "",
            "# This fixture will start a cluster with empty nodes.",
            "@pytest.fixture",
            "def ray_start_cluster(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_enabled(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    param[\"skip_cluster\"] = False",
            "    with _ray_start_cluster(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_init(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_head(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, num_nodes=1, **param) as res:",
            "        yield res",
            "",
            "",
            "# We can compose external_redis and ray_start_cluster_head instead of creating",
            "# this separate fixture, if there is a good way to ensure external_redis runs",
            "# before ray_start_cluster_head.",
            "@pytest.fixture",
            "def ray_start_cluster_head_with_external_redis(request, external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, num_nodes=1, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_head_with_env_vars(request, maybe_external_redis, monkeypatch):",
            "    param = getattr(request, \"param\", {})",
            "    env_vars = param.pop(\"env_vars\", {})",
            "    for k, v in env_vars.items():",
            "        monkeypatch.setenv(k, v)",
            "    with _ray_start_cluster(do_init=True, num_nodes=1, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_2_nodes(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, num_nodes=2, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_object_store_memory(request, maybe_external_redis):",
            "    # Start the Ray processes.",
            "    store_size = request.param",
            "    system_config = get_default_fixure_system_config()",
            "    init_kwargs = {",
            "        \"num_cpus\": 1,",
            "        \"_system_config\": system_config,",
            "        \"object_store_memory\": store_size,",
            "    }",
            "    ray.init(\"local\", **init_kwargs)",
            "    yield store_size",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "",
            "",
            "@pytest.fixture",
            "def call_ray_start(request):",
            "    with call_ray_start_context(request) as address:",
            "        yield address",
            "",
            "",
            "@contextmanager",
            "def call_ray_start_context(request):",
            "    default_cmd = (",
            "        \"ray start --head --num-cpus=1 --min-worker-port=0 \"",
            "        \"--max-worker-port=0 --port 0\"",
            "    )",
            "    parameter = getattr(request, \"param\", default_cmd)",
            "    env = None",
            "",
            "    if isinstance(parameter, dict):",
            "        if \"env\" in parameter:",
            "            env = {**os.environ, **parameter.get(\"env\")}",
            "",
            "        parameter = parameter.get(\"cmd\", default_cmd)",
            "",
            "    command_args = parameter.split(\" \")",
            "",
            "    try:",
            "        out = ray._private.utils.decode(",
            "            subprocess.check_output(command_args, stderr=subprocess.STDOUT, env=env)",
            "        )",
            "    except Exception as e:",
            "        print(type(e), e)",
            "        raise",
            "",
            "    # Get the redis address from the output.",
            "    redis_substring_prefix = \"--address='\"",
            "    idx = out.find(redis_substring_prefix)",
            "    if idx >= 0:",
            "        address_location = idx + len(redis_substring_prefix)",
            "        address = out[address_location:]",
            "        address = address.split(\"'\")[0]",
            "    else:",
            "        address = None",
            "",
            "    yield address",
            "",
            "    # Disconnect from the Ray cluster.",
            "    ray.shutdown()",
            "    # Kill the Ray cluster.",
            "    subprocess.check_call([\"ray\", \"stop\"], env=env)",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def call_ray_start_with_external_redis(request):",
            "    ports = getattr(request, \"param\", \"6379\")",
            "    port_list = ports.split(\",\")",
            "    for port in port_list:",
            "        temp_dir = ray._private.utils.get_ray_temp_dir()",
            "        start_redis_instance(temp_dir, int(port), password=\"123\")",
            "    address_str = \",\".join(map(lambda x: \"localhost:\" + x, port_list))",
            "    cmd = f\"ray start --head --address={address_str} --redis-password=123\"",
            "    subprocess.call(cmd.split(\" \"))",
            "",
            "    yield address_str.split(\",\")[0]",
            "",
            "    # Disconnect from the Ray cluster.",
            "    ray.shutdown()",
            "    # Kill the Ray cluster.",
            "    subprocess.check_call([\"ray\", \"stop\"])",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def init_and_serve():",
            "    import ray.util.client.server.server as ray_client_server",
            "",
            "    server_handle, _ = ray_client_server.init_and_serve(\"localhost:50051\")",
            "    yield server_handle",
            "    ray_client_server.shutdown_with_server(server_handle.grpc_server)",
            "    time.sleep(2)",
            "",
            "",
            "@pytest.fixture",
            "def call_ray_stop_only():",
            "    yield",
            "    subprocess.check_call([\"ray\", \"stop\"])",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "# Used to test both Ray Client and non-Ray Client codepaths.",
            "# Usage: In your test, call `ray.init(address)`.",
            "@pytest.fixture(scope=\"function\", params=[\"ray_client\", \"no_ray_client\"])",
            "def start_cluster(ray_start_cluster_enabled, request):",
            "    assert request.param in {\"ray_client\", \"no_ray_client\"}",
            "    use_ray_client: bool = request.param == \"ray_client\"",
            "    if os.environ.get(\"RAY_MINIMAL\") == \"1\" and use_ray_client:",
            "        pytest.skip(\"Skipping due to we don't have ray client in minimal.\")",
            "",
            "    cluster = ray_start_cluster_enabled",
            "    cluster.add_node(num_cpus=4, dashboard_agent_listen_port=find_free_port())",
            "    if use_ray_client:",
            "        cluster.head_node._ray_params.ray_client_server_port = \"10004\"",
            "        cluster.head_node.start_ray_client_server()",
            "        address = \"ray://localhost:10004\"",
            "    else:",
            "        address = cluster.address",
            "",
            "    yield cluster, address",
            "",
            "",
            "@pytest.fixture(scope=\"function\")",
            "def tmp_working_dir():",
            "    with tempfile.TemporaryDirectory() as tmp_dir:",
            "        path = Path(tmp_dir)",
            "",
            "        hello_file = path / \"hello\"",
            "        with hello_file.open(mode=\"w\") as f:",
            "            f.write(\"world\")",
            "",
            "        module_path = path / \"test_module\"",
            "        module_path.mkdir(parents=True)",
            "",
            "        test_file = module_path / \"test.py\"",
            "        with test_file.open(mode=\"w\") as f:",
            "            f.write(\"def one():\\n\")",
            "            f.write(\"    return 1\\n\")",
            "",
            "        init_file = module_path / \"__init__.py\"",
            "        with init_file.open(mode=\"w\") as f:",
            "            f.write(\"from test_module.test import one\\n\")",
            "",
            "        yield tmp_dir",
            "",
            "",
            "@pytest.fixture",
            "def enable_pickle_debug():",
            "    os.environ[\"RAY_PICKLE_VERBOSE_DEBUG\"] = \"1\"",
            "    yield",
            "    del os.environ[\"RAY_PICKLE_VERBOSE_DEBUG\"]",
            "",
            "",
            "@pytest.fixture",
            "def set_enable_auto_connect(enable_auto_connect: str = \"0\"):",
            "    try:",
            "        os.environ[\"RAY_ENABLE_AUTO_CONNECT\"] = enable_auto_connect",
            "        yield enable_auto_connect",
            "    finally:",
            "        del os.environ[\"RAY_ENABLE_AUTO_CONNECT\"]",
            "",
            "",
            "@pytest.fixture",
            "def enable_mac_large_object_store():",
            "    os.environ[\"RAY_ENABLE_MAC_LARGE_OBJECT_STORE\"] = \"1\"",
            "    yield",
            "    del os.environ[\"RAY_ENABLE_MAC_LARGE_OBJECT_STORE\"]",
            "",
            "",
            "@pytest.fixture()",
            "def two_node_cluster():",
            "    system_config = {",
            "        \"object_timeout_milliseconds\": 200,",
            "    }",
            "    if cluster_not_supported:",
            "        pytest.skip(\"Cluster not supported\")",
            "    cluster = ray.cluster_utils.Cluster(",
            "        head_node_args={\"_system_config\": system_config}",
            "    )",
            "    for _ in range(2):",
            "        remote_node = cluster.add_node(num_cpus=1)",
            "    ray.init(address=cluster.address)",
            "    yield cluster, remote_node",
            "",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    cluster.shutdown()",
            "",
            "",
            "@pytest.fixture()",
            "def error_pubsub():",
            "    p = init_error_pubsub()",
            "    yield p",
            "    p.close()",
            "",
            "",
            "@pytest.fixture()",
            "def log_pubsub():",
            "    p = init_log_pubsub()",
            "    yield p",
            "    p.close()",
            "",
            "",
            "@pytest.fixture",
            "def use_tls(request):",
            "    if request.param:",
            "        key_filepath, cert_filepath, temp_dir = setup_tls()",
            "    yield request.param",
            "    if request.param:",
            "        teardown_tls(key_filepath, cert_filepath, temp_dir)",
            "",
            "",
            "\"\"\"",
            "Object spilling test fixture",
            "\"\"\"",
            "# -- Smart open param --",
            "bucket_name = \"object-spilling-test\"",
            "",
            "# -- File system param --",
            "spill_local_path = \"/tmp/spill\"",
            "",
            "# -- Spilling configs --",
            "file_system_object_spilling_config = {",
            "    \"type\": \"filesystem\",",
            "    \"params\": {\"directory_path\": spill_local_path},",
            "}",
            "",
            "buffer_object_spilling_config = {",
            "    \"type\": \"filesystem\",",
            "    \"params\": {\"directory_path\": spill_local_path, \"buffer_size\": 1_000_000},",
            "}",
            "",
            "# Since we have differet protocol for a local external storage (e.g., fs)",
            "# and distributed external storage (e.g., S3), we need to test both cases.",
            "# This mocks the distributed fs with cluster utils.",
            "mock_distributed_fs_object_spilling_config = {",
            "    \"type\": \"mock_distributed_fs\",",
            "    \"params\": {\"directory_path\": spill_local_path},",
            "}",
            "smart_open_object_spilling_config = {",
            "    \"type\": \"smart_open\",",
            "    \"params\": {\"uri\": f\"s3://{bucket_name}/\"},",
            "}",
            "ray_storage_object_spilling_config = {",
            "    \"type\": \"ray_storage\",",
            "    # Force the storage config so we don't need to patch each test to separately",
            "    # configure the storage param under this.",
            "    \"params\": {\"_force_storage_for_testing\": spill_local_path},",
            "}",
            "buffer_open_object_spilling_config = {",
            "    \"type\": \"smart_open\",",
            "    \"params\": {\"uri\": f\"s3://{bucket_name}/\", \"buffer_size\": 1000},",
            "}",
            "multi_smart_open_object_spilling_config = {",
            "    \"type\": \"smart_open\",",
            "    \"params\": {\"uri\": [f\"s3://{bucket_name}/{i}\" for i in range(3)]},",
            "}",
            "",
            "unstable_object_spilling_config = {",
            "    \"type\": \"unstable_fs\",",
            "    \"params\": {",
            "        \"directory_path\": spill_local_path,",
            "    },",
            "}",
            "slow_object_spilling_config = {",
            "    \"type\": \"slow_fs\",",
            "    \"params\": {",
            "        \"directory_path\": spill_local_path,",
            "    },",
            "}",
            "",
            "",
            "def create_object_spilling_config(request, tmp_path):",
            "    temp_folder = tmp_path / \"spill\"",
            "    temp_folder.mkdir()",
            "    if (",
            "        request.param[\"type\"] == \"filesystem\"",
            "        or request.param[\"type\"] == \"mock_distributed_fs\"",
            "    ):",
            "        request.param[\"params\"][\"directory_path\"] = str(temp_folder)",
            "    return json.dumps(request.param), temp_folder",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        file_system_object_spilling_config,",
            "    ],",
            ")",
            "def fs_only_object_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        file_system_object_spilling_config,",
            "        ray_storage_object_spilling_config,",
            "        # TODO(sang): Add a mock dependency to test S3.",
            "        # smart_open_object_spilling_config,",
            "    ],",
            ")",
            "def object_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        file_system_object_spilling_config,",
            "        mock_distributed_fs_object_spilling_config,",
            "    ],",
            ")",
            "def multi_node_object_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        unstable_object_spilling_config,",
            "    ],",
            ")",
            "def unstable_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        slow_object_spilling_config,",
            "    ],",
            ")",
            "def slow_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "def _ray_start_chaos_cluster(request):",
            "    param = getattr(request, \"param\", {})",
            "    kill_interval = param.pop(\"kill_interval\", None)",
            "    config = param.pop(\"_system_config\", {})",
            "    config.update(",
            "        {",
            "            \"task_retry_delay_ms\": 100,",
            "        }",
            "    )",
            "    # Config of workers that are re-started.",
            "    head_resources = param.pop(\"head_resources\")",
            "    worker_node_types = param.pop(\"worker_node_types\")",
            "    cluster = AutoscalingCluster(",
            "        head_resources,",
            "        worker_node_types,",
            "        idle_timeout_minutes=10,  # Don't take down nodes.",
            "        **param,",
            "    )",
            "    cluster.start(_system_config=config)",
            "    ray.init(\"auto\")",
            "    nodes = ray.nodes()",
            "    assert len(nodes) == 1",
            "",
            "    if kill_interval is not None:",
            "        node_killer = get_and_run_node_killer(kill_interval)",
            "",
            "    yield cluster",
            "",
            "    if kill_interval is not None:",
            "        ray.get(node_killer.stop_run.remote())",
            "        killed = ray.get(node_killer.get_total_killed_nodes.remote())",
            "        assert len(killed) > 0",
            "        died = {node[\"NodeID\"] for node in ray.nodes() if not node[\"Alive\"]}",
            "        assert died.issubset(",
            "            killed",
            "        ), f\"Raylets {died - killed} that we did not kill crashed\"",
            "",
            "    ray.shutdown()",
            "    cluster.shutdown()",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_chaos_cluster(request):",
            "    \"\"\"Returns the cluster and chaos thread.\"\"\"",
            "    for x in _ray_start_chaos_cluster(request):",
            "        yield x",
            "",
            "",
            "# Set scope to \"class\" to force this to run before start_cluster, whose scope",
            "# is \"function\".  We need these env vars to be set before Ray is started.",
            "@pytest.fixture(scope=\"class\")",
            "def runtime_env_disable_URI_cache():",
            "    with mock.patch.dict(",
            "        os.environ,",
            "        {",
            "            \"RAY_RUNTIME_ENV_CONDA_CACHE_SIZE_GB\": \"0\",",
            "            \"RAY_RUNTIME_ENV_PIP_CACHE_SIZE_GB\": \"0\",",
            "            \"RAY_RUNTIME_ENV_WORKING_DIR_CACHE_SIZE_GB\": \"0\",",
            "            \"RAY_RUNTIME_ENV_PY_MODULES_CACHE_SIZE_GB\": \"0\",",
            "        },",
            "    ):",
            "        print(",
            "            \"URI caching disabled (conda, pip, working_dir, py_modules cache \"",
            "            \"size set to 0).\"",
            "        )",
            "        yield",
            "",
            "",
            "# Use to create virtualenv that clone from current python env.",
            "# The difference between this fixture and `pytest_virtual.virtual` is that",
            "# `pytest_virtual.virtual` will not inherit current python env's site-package.",
            "# Note: Can't use in virtualenv, this must be noted when testing locally.",
            "@pytest.fixture(scope=\"function\")",
            "def cloned_virtualenv():",
            "    # Lazy import pytest_virtualenv,",
            "    # aviod import `pytest_virtualenv` in test case `Minimal install`",
            "    from pytest_virtualenv import VirtualEnv",
            "",
            "    if PipProcessor._is_in_virtualenv():",
            "        raise RuntimeError(\"Forbid the use of this fixture in virtualenv\")",
            "",
            "    venv = VirtualEnv(",
            "        args=[",
            "            \"--system-site-packages\",",
            "            \"--reset-app-data\",",
            "            \"--no-periodic-update\",",
            "            \"--no-download\",",
            "        ],",
            "    )",
            "    yield venv",
            "    venv.teardown()",
            "",
            "",
            "@pytest.fixture",
            "def set_runtime_env_retry_times(request):",
            "    runtime_env_retry_times = getattr(request, \"param\", \"0\")",
            "    try:",
            "        os.environ[\"RUNTIME_ENV_RETRY_TIMES\"] = runtime_env_retry_times",
            "        yield runtime_env_retry_times",
            "    finally:",
            "        del os.environ[\"RUNTIME_ENV_RETRY_TIMES\"]",
            "",
            "",
            "@pytest.fixture",
            "def listen_port(request):",
            "    port = getattr(request, \"param\", 0)",
            "    try:",
            "        sock = socket.socket()",
            "        if hasattr(socket, \"SO_REUSEPORT\"):",
            "            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 0)",
            "        sock.bind((\"127.0.0.1\", port))",
            "        yield port",
            "    finally:",
            "        sock.close()",
            "",
            "",
            "@pytest.fixture",
            "def set_bad_runtime_env_cache_ttl_seconds(request):",
            "    ttl = getattr(request, \"param\", \"0\")",
            "    os.environ[\"BAD_RUNTIME_ENV_CACHE_TTL_SECONDS\"] = ttl",
            "    yield ttl",
            "    del os.environ[\"BAD_RUNTIME_ENV_CACHE_TTL_SECONDS\"]",
            "",
            "",
            "@pytest.hookimpl(tryfirst=True, hookwrapper=True)",
            "def pytest_runtest_makereport(item, call):",
            "    # execute all other hooks to obtain the report object",
            "    outcome = yield",
            "    rep = outcome.get_result()",
            "",
            "    try:",
            "        append_short_test_summary(rep)",
            "    except Exception as e:",
            "        print(f\"+++ Error creating PyTest summary\\n{e}\")",
            "    try:",
            "        create_ray_logs_for_failed_test(rep)",
            "    except Exception as e:",
            "        print(f\"+++ Error saving Ray logs for failing test\\n{e}\")",
            "",
            "",
            "def append_short_test_summary(rep):",
            "    \"\"\"Writes a short summary txt for failed tests to be printed later.\"\"\"",
            "    if rep.when != \"call\":",
            "        return",
            "",
            "    summary_dir = os.environ.get(\"RAY_TEST_SUMMARY_DIR\")",
            "",
            "    if platform.system() != \"Linux\":",
            "        summary_dir = os.environ.get(\"RAY_TEST_SUMMARY_DIR_HOST\")",
            "",
            "    if not summary_dir:",
            "        return",
            "",
            "    if not os.path.exists(summary_dir):",
            "        os.makedirs(summary_dir, exist_ok=True)",
            "",
            "    test_name = rep.nodeid.replace(os.sep, \"::\")",
            "",
            "    if os.name == \"nt\":",
            "        # \":\" is not legal in filenames in windows",
            "        test_name.replace(\":\", \"$\")",
            "",
            "    header_file = os.path.join(summary_dir, \"000_header.txt\")",
            "    summary_file = os.path.join(summary_dir, test_name + \".txt\")",
            "",
            "    if rep.passed and os.path.exists(summary_file):",
            "        # The test succeeded after failing, thus it is flaky.",
            "        # We do not want to annotate flaky tests just now, so remove report.",
            "        os.remove(summary_file)",
            "        return",
            "",
            "    # Only consider failed tests from now on",
            "    if not rep.failed:",
            "        return",
            "",
            "    # No failing test information",
            "    if rep.longrepr is None:",
            "        return",
            "",
            "    # No failing test information",
            "    if not hasattr(rep.longrepr, \"chain\"):",
            "        return",
            "",
            "    if not os.path.exists(header_file):",
            "        with open(header_file, \"wt\") as fp:",
            "            test_label = os.environ.get(\"BUILDKITE_LABEL\", \"Unknown\")",
            "            job_id = os.environ.get(\"BUILDKITE_JOB_ID\")",
            "",
            "            fp.write(f\"### Pytest failures for: [{test_label}](#{job_id})\\n\\n\")",
            "",
            "    # Use `wt` here to overwrite so we only have one result per test (exclude retries)",
            "    with open(summary_file, \"wt\") as fp:",
            "        fp.write(_get_markdown_annotation(rep))",
            "",
            "",
            "def _get_markdown_annotation(rep) -> str:",
            "    # Main traceback is the last in the chain (where the last error is raised)",
            "    main_tb, main_loc, _ = rep.longrepr.chain[-1]",
            "    markdown = \"\"",
            "",
            "    # Only keep last line of the message",
            "    short_message = list(filter(None, main_loc.message.split(\"\\n\")))[-1]",
            "",
            "    # Header: Main error message",
            "    markdown += f\"#### {rep.nodeid}\\n\\n\"",
            "    markdown += \"<details>\\n\"",
            "    markdown += f\"<summary>{short_message}</summary>\\n\\n\"",
            "",
            "    # Add link to test definition",
            "    test_file, test_lineno, _test_node = rep.location",
            "    test_path, test_url = _get_repo_github_path_and_link(",
            "        os.path.abspath(test_file), test_lineno",
            "    )",
            "    markdown += f\"Link to test: [{test_path}:{test_lineno}]({test_url})\\n\\n\"",
            "",
            "    # Print main traceback",
            "    markdown += \"##### Traceback\\n\\n\"",
            "    markdown += \"```\\n\"",
            "    markdown += str(main_tb)",
            "    markdown += \"\\n```\\n\\n\"",
            "",
            "    # Print link to test definition in github",
            "    path, url = _get_repo_github_path_and_link(main_loc.path, main_loc.lineno)",
            "    markdown += f\"[{path}:{main_loc.lineno}]({url})\\n\\n\"",
            "",
            "    # If this is a longer exception chain, users can expand the full traceback",
            "    if len(rep.longrepr.chain) > 1:",
            "        markdown += \"<details><summary>Full traceback</summary>\\n\\n\"",
            "",
            "        # Here we just print each traceback and the link to the respective",
            "        # lines in GutHub",
            "        for tb, loc, _ in rep.longrepr.chain:",
            "            if loc:",
            "                path, url = _get_repo_github_path_and_link(loc.path, loc.lineno)",
            "                github_link = f\"[{path}:{loc.lineno}]({url})\\n\\n\"",
            "            else:",
            "                github_link = \"\"",
            "",
            "            markdown += \"```\\n\"",
            "            markdown += str(tb)",
            "            markdown += \"\\n```\\n\\n\"",
            "            markdown += github_link",
            "",
            "        markdown += \"</details>\\n\"",
            "",
            "    markdown += \"<details><summary>PIP packages</summary>\\n\\n\"",
            "    markdown += \"```\\n\"",
            "    markdown += \"\\n\".join(_get_pip_packages())",
            "    markdown += \"\\n```\\n\\n\"",
            "    markdown += \"</details>\\n\"",
            "",
            "    markdown += \"</details>\\n\\n\"",
            "    return markdown",
            "",
            "",
            "def _get_pip_packages() -> List[str]:",
            "    try:",
            "        from pip._internal.operations import freeze",
            "",
            "        return list(freeze.freeze())",
            "    except Exception:",
            "        return [\"invalid\"]",
            "",
            "",
            "def _get_repo_github_path_and_link(file: str, lineno: int) -> Tuple[str, str]:",
            "    base_url = \"https://github.com/ray-project/ray/blob/{commit}/{path}#L{lineno}\"",
            "",
            "    commit = os.environ.get(\"BUILDKITE_COMMIT\")",
            "",
            "    if not commit:",
            "        return file, \"\"",
            "",
            "    path = os.path.relpath(file, \"/ray\")",
            "",
            "    return path, base_url.format(commit=commit, path=path, lineno=lineno)",
            "",
            "",
            "def create_ray_logs_for_failed_test(rep):",
            "    \"\"\"Creates artifact zip of /tmp/ray/session_latest/logs for failed tests\"\"\"",
            "",
            "    # We temporarily restrict to Linux until we have artifact dirs",
            "    # for Windows and Mac",
            "    if platform.system() != \"Linux\":",
            "        return",
            "",
            "    # Only archive failed tests after the \"call\" phase of the test",
            "    if rep.when != \"call\" or not rep.failed:",
            "        return",
            "",
            "    # Get dir to write zipped logs to",
            "    archive_dir = os.environ.get(\"RAY_TEST_FAILURE_LOGS_ARCHIVE_DIR\")",
            "",
            "    if not archive_dir:",
            "        return",
            "",
            "    if not os.path.exists(archive_dir):",
            "        os.makedirs(archive_dir)",
            "",
            "    # Get logs dir from the latest ray session",
            "    tmp_dir = gettempdir()",
            "    logs_dir = os.path.join(tmp_dir, \"ray\", \"session_latest\", \"logs\")",
            "",
            "    if not os.path.exists(logs_dir):",
            "        return",
            "",
            "    # Write zipped logs to logs archive dir",
            "    test_name = rep.nodeid.replace(os.sep, \"::\")",
            "    output_file = os.path.join(archive_dir, f\"{test_name}_{time.time():.4f}\")",
            "    shutil.make_archive(output_file, \"zip\", logs_dir)",
            "",
            "",
            "@pytest.fixture(params=[True, False])",
            "def start_http_proxy(request):",
            "    env = {}",
            "",
            "    proxy = None",
            "    try:",
            "        if request.param:",
            "            # the `proxy` command is from the proxy.py package.",
            "            proxy = subprocess.Popen(",
            "                [\"proxy\", \"--port\", \"8899\", \"--log-level\", \"ERROR\"]",
            "            )",
            "            env[\"RAY_grpc_enable_http_proxy\"] = \"1\"",
            "            proxy_url = \"http://localhost:8899\"",
            "        else:",
            "            proxy_url = \"http://example.com\"",
            "        env[\"http_proxy\"] = proxy_url",
            "        env[\"https_proxy\"] = proxy_url",
            "        yield env",
            "    finally:",
            "        if proxy:",
            "            proxy.terminate()",
            "            proxy.wait()",
            "",
            "",
            "@pytest.fixture",
            "def set_runtime_env_plugins(request):",
            "    runtime_env_plugins = getattr(request, \"param\", \"0\")",
            "    try:",
            "        os.environ[\"RAY_RUNTIME_ENV_PLUGINS\"] = runtime_env_plugins",
            "        yield runtime_env_plugins",
            "    finally:",
            "        del os.environ[\"RAY_RUNTIME_ENV_PLUGINS\"]",
            "",
            "",
            "@pytest.fixture",
            "def set_runtime_env_plugin_schemas(request):",
            "    runtime_env_plugin_schemas = getattr(request, \"param\", \"0\")",
            "    try:",
            "        os.environ[\"RAY_RUNTIME_ENV_PLUGIN_SCHEMAS\"] = runtime_env_plugin_schemas",
            "        # Clear and reload schemas.",
            "        RuntimeEnvPluginSchemaManager.clear()",
            "        yield runtime_env_plugin_schemas",
            "    finally:",
            "        del os.environ[\"RAY_RUNTIME_ENV_PLUGIN_SCHEMAS\"]",
            "",
            "",
            "@pytest.fixture(scope=\"function\")",
            "def temp_file(request):",
            "    with tempfile.NamedTemporaryFile(\"r+b\") as fp:",
            "        yield fp",
            "",
            "",
            "@pytest.fixture(scope=\"module\")",
            "def random_ascii_file(request):",
            "    import random",
            "    import string",
            "",
            "    file_size = getattr(request, \"param\", 1 << 10)",
            "",
            "    with tempfile.NamedTemporaryFile(mode=\"r+b\") as fp:",
            "        fp.write(\"\".join(random.choices(string.ascii_letters, k=file_size)).encode())",
            "        fp.flush()",
            "",
            "        yield fp"
        ],
        "afterPatchFile": [
            "\"\"\"",
            "This file defines the common pytest fixtures used in current directory.",
            "\"\"\"",
            "import json",
            "import logging",
            "import os",
            "import platform",
            "import shutil",
            "import socket",
            "import subprocess",
            "import tempfile",
            "import time",
            "from contextlib import contextmanager",
            "from pathlib import Path",
            "from tempfile import gettempdir",
            "from typing import List, Tuple",
            "from unittest import mock",
            "import psutil",
            "import pytest",
            "",
            "import ray",
            "import ray._private.ray_constants as ray_constants",
            "from ray._private.conftest_utils import set_override_dashboard_url  # noqa: F401",
            "from ray._private.runtime_env.pip import PipProcessor",
            "from ray._private.runtime_env.plugin_schema_manager import RuntimeEnvPluginSchemaManager",
            "",
            "from ray._private.test_utils import (",
            "    get_and_run_node_killer,",
            "    init_error_pubsub,",
            "    init_log_pubsub,",
            "    setup_tls,",
            "    teardown_tls,",
            "    enable_external_redis,",
            "    redis_replicas,",
            "    get_redis_cli,",
            "    start_redis_instance,",
            "    find_available_port,",
            "    wait_for_condition,",
            "    find_free_port,",
            ")",
            "from ray.cluster_utils import AutoscalingCluster, Cluster, cluster_not_supported",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "START_REDIS_WAIT_RETRIES = int(os.environ.get(\"RAY_START_REDIS_WAIT_RETRIES\", \"60\"))",
            "",
            "",
            "@pytest.fixture(autouse=True)",
            "def pre_envs(monkeypatch):",
            "    # To make test run faster",
            "    monkeypatch.setenv(\"RAY_NUM_REDIS_GET_RETRIES\", \"2\")",
            "    ray_constants.NUM_REDIS_GET_RETRIES = 2",
            "    yield",
            "",
            "",
            "def wait_for_redis_to_start(redis_ip_address: str, redis_port: bool, password=None):",
            "    \"\"\"Wait for a Redis server to be available.",
            "",
            "    This is accomplished by creating a Redis client and sending a random",
            "    command to the server until the command gets through.",
            "",
            "    Args:",
            "        redis_ip_address: The IP address of the redis server.",
            "        redis_port: The port of the redis server.",
            "        password: The password of the redis server.",
            "",
            "    Raises:",
            "        Exception: An exception is raised if we could not connect with Redis.",
            "    \"\"\"",
            "    import redis",
            "",
            "    redis_client = redis.StrictRedis(",
            "        host=redis_ip_address, port=redis_port, password=password",
            "    )",
            "    # Wait for the Redis server to start.",
            "    num_retries = START_REDIS_WAIT_RETRIES",
            "",
            "    delay = 0.001",
            "    for i in range(num_retries):",
            "        try:",
            "            # Run some random command and see if it worked.",
            "            logger.debug(",
            "                \"Waiting for redis server at {}:{} to respond...\".format(",
            "                    redis_ip_address, redis_port",
            "                )",
            "            )",
            "            redis_client.client_list()",
            "        # If the Redis service is delayed getting set up for any reason, we may",
            "        # get a redis.ConnectionError: Error 111 connecting to host:port.",
            "        # Connection refused.",
            "        # Unfortunately, redis.ConnectionError is also the base class of",
            "        # redis.AuthenticationError. We *don't* want to obscure a",
            "        # redis.AuthenticationError, because that indicates the user provided a",
            "        # bad password. Thus a double except clause to ensure a",
            "        # redis.AuthenticationError isn't trapped here.",
            "        except redis.AuthenticationError as authEx:",
            "            raise RuntimeError(",
            "                f\"Unable to connect to Redis at {redis_ip_address}:{redis_port}.\"",
            "            ) from authEx",
            "        except redis.ConnectionError as connEx:",
            "            if i >= num_retries - 1:",
            "                raise RuntimeError(",
            "                    f\"Unable to connect to Redis at {redis_ip_address}:\"",
            "                    f\"{redis_port} after {num_retries} retries. Check that \"",
            "                    f\"{redis_ip_address}:{redis_port} is reachable from this \"",
            "                    \"machine. If it is not, your firewall may be blocking \"",
            "                    \"this port. If the problem is a flaky connection, try \"",
            "                    \"setting the environment variable \"",
            "                    \"`RAY_START_REDIS_WAIT_RETRIES` to increase the number of\"",
            "                    \" attempts to ping the Redis server.\"",
            "                ) from connEx",
            "            # Wait a little bit.",
            "            time.sleep(delay)",
            "            # Make sure the retry interval doesn't increase too large, which will",
            "            # affect the delivery time of the Ray cluster.",
            "            delay = min(1, delay * 2)",
            "        else:",
            "            break",
            "    else:",
            "        raise RuntimeError(",
            "            f\"Unable to connect to Redis (after {num_retries} retries). \"",
            "            \"If the Redis instance is on a different machine, check that \"",
            "            \"your firewall and relevant Ray ports are configured properly. \"",
            "            \"You can also set the environment variable \"",
            "            \"`RAY_START_REDIS_WAIT_RETRIES` to increase the number of \"",
            "            \"attempts to ping the Redis server.\"",
            "        )",
            "",
            "",
            "def get_default_fixure_system_config():",
            "    system_config = {",
            "        \"object_timeout_milliseconds\": 200,",
            "        \"health_check_initial_delay_ms\": 0,",
            "        \"health_check_failure_threshold\": 10,",
            "        \"object_store_full_delay_ms\": 100,",
            "    }",
            "    return system_config",
            "",
            "",
            "def get_default_fixture_ray_kwargs():",
            "    system_config = get_default_fixure_system_config()",
            "    ray_kwargs = {",
            "        \"num_cpus\": 1,",
            "        \"object_store_memory\": 150 * 1024 * 1024,",
            "        \"dashboard_port\": None,",
            "        \"namespace\": \"default_test_namespace\",",
            "        \"_system_config\": system_config,",
            "    }",
            "    return ray_kwargs",
            "",
            "",
            "def is_process_listen_to_port(pid, port):",
            "    retry_num = 10",
            "    interval_time = 0.5",
            "    for _ in range(retry_num):",
            "        try:",
            "            proc = psutil.Process(pid)",
            "            for conn in proc.connections():",
            "                if conn.status == \"LISTEN\" and conn.laddr.port == port:",
            "                    return True",
            "        except Exception:",
            "            pass",
            "        finally:",
            "            time.sleep(interval_time)",
            "    print(",
            "        f\"Process({pid}) has not listened to port {port} \"",
            "        + f\"for more than {retry_num * interval_time}s.\"",
            "    )",
            "    return False",
            "",
            "",
            "def redis_alive(port, enable_tls):",
            "    try:",
            "        # If there is no redis libs installed, skip the check.",
            "        # This could happen In minimal test, where we don't have",
            "        # redis.",
            "        import redis",
            "    except Exception:",
            "        return True",
            "",
            "    params = {}",
            "    if enable_tls:",
            "        from ray._raylet import Config",
            "",
            "        params = {\"ssl\": True, \"ssl_cert_reqs\": \"required\"}",
            "        if Config.REDIS_CA_CERT():",
            "            params[\"ssl_ca_certs\"] = Config.REDIS_CA_CERT()",
            "        if Config.REDIS_CLIENT_CERT():",
            "            params[\"ssl_certfile\"] = Config.REDIS_CLIENT_CERT()",
            "        if Config.REDIS_CLIENT_KEY():",
            "            params[\"ssl_keyfile\"] = Config.REDIS_CLIENT_KEY()",
            "",
            "    cli = redis.Redis(\"localhost\", port, **params)",
            "",
            "    try:",
            "        return cli.ping()",
            "    except Exception:",
            "        pass",
            "    return False",
            "",
            "",
            "def start_redis(db_dir):",
            "    retry_num = 0",
            "    while True:",
            "        is_need_restart = False",
            "        # Setup external Redis and env var for initialization.",
            "        redis_ports = find_available_port(49159, 55535, redis_replicas() * 2)",
            "        redis_ports = list(",
            "            zip(redis_ports[0 : redis_replicas()], redis_ports[redis_replicas() :])",
            "        )",
            "        processes = []",
            "        enable_tls = \"RAY_REDIS_CA_CERT\" in os.environ",
            "        leader_port = None",
            "        leader_id = None",
            "        redis_ports = []",
            "        while len(redis_ports) != redis_replicas():",
            "            temp_dir = ray._private.utils.get_ray_temp_dir()",
            "            port, free_port = find_available_port(49159, 55535, 2)",
            "            node_id, proc = start_redis_instance(",
            "                temp_dir,",
            "                port,",
            "                enable_tls=enable_tls,",
            "                replica_of=leader_port,",
            "                leader_id=leader_id,",
            "                db_dir=db_dir,",
            "                free_port=free_port,",
            "            )",
            "            try:",
            "                wait_for_condition(",
            "                    redis_alive, 3, 100, port=port, enable_tls=enable_tls",
            "                )",
            "            except Exception as e:",
            "                print(e)",
            "                continue",
            "            redis_ports.append(port)",
            "            if leader_port is None:",
            "                leader_port = port",
            "                leader_id = node_id",
            "            processes.append(proc)",
            "            # Check if th redis has started successfully and is listening on the port.",
            "            if not is_process_listen_to_port(proc.process.pid, port):",
            "                is_need_restart = True",
            "                break",
            "",
            "        if is_need_restart:",
            "            retry_num += 1",
            "            for proc in processes:",
            "                proc.process.kill()",
            "",
            "            if retry_num > 5:",
            "                raise RuntimeError(\"Failed to start redis after {retry_num} attempts.\")",
            "            print(",
            "                \"Retry to start redis because the process failed to \"",
            "                + f\"listen to the port({port}), retry num:{retry_num}.\"",
            "            )",
            "            continue",
            "",
            "        if redis_replicas() > 1:",
            "",
            "            redis_cli = get_redis_cli(str(leader_port), enable_tls)",
            "            while redis_cli.cluster(\"info\")[\"cluster_state\"] != \"ok\":",
            "                pass",
            "",
            "        scheme = \"rediss://\" if enable_tls else \"\"",
            "        address_str = f\"{scheme}127.0.0.1:{redis_ports[-1]}\"",
            "        return address_str, processes",
            "",
            "",
            "def kill_all_redis_server():",
            "    import psutil",
            "",
            "    # Find Redis server processes",
            "    redis_procs = []",
            "    for proc in psutil.process_iter([\"name\", \"cmdline\"]):",
            "        if proc.name() == \"redis-server\":",
            "            redis_procs.append(proc)",
            "",
            "    # Kill Redis server processes",
            "    for proc in redis_procs:",
            "        proc.kill()",
            "",
            "",
            "@contextmanager",
            "def _setup_redis(request):",
            "    with tempfile.TemporaryDirectory() as tmpdirname:",
            "        kill_all_redis_server()",
            "        address_str, processes = start_redis(tmpdirname)",
            "        old_addr = os.environ.get(\"RAY_REDIS_ADDRESS\")",
            "        os.environ[\"RAY_REDIS_ADDRESS\"] = address_str",
            "        import uuid",
            "",
            "        ns = str(uuid.uuid4())",
            "        old_ns = os.environ.get(\"RAY_external_storage_namespace\")",
            "        os.environ[\"RAY_external_storage_namespace\"] = ns",
            "",
            "        yield",
            "        if old_addr is not None:",
            "            os.environ[\"RAY_REDIS_ADDRESS\"] = old_addr",
            "        else:",
            "            del os.environ[\"RAY_REDIS_ADDRESS\"]",
            "",
            "        if old_ns is not None:",
            "            os.environ[\"RAY_external_storage_namespace\"] = old_ns",
            "        else:",
            "            del os.environ[\"RAY_external_storage_namespace\"]",
            "",
            "        for proc in processes:",
            "            proc.process.kill()",
            "        kill_all_redis_server()",
            "",
            "",
            "@pytest.fixture",
            "def maybe_external_redis(request):",
            "    if enable_external_redis():",
            "        with _setup_redis(request):",
            "            yield",
            "    else:",
            "        yield",
            "",
            "",
            "@pytest.fixture",
            "def external_redis(request):",
            "    with _setup_redis(request):",
            "        yield",
            "",
            "",
            "@pytest.fixture",
            "def shutdown_only(maybe_external_redis):",
            "    yield None",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def propagate_logs():",
            "    # Ensure that logs are propagated to ancestor handles. This is required if using the",
            "    # caplog fixture with Ray's logging.",
            "    # NOTE: This only enables log propagation in the driver process, not the workers!",
            "    logger = logging.getLogger(\"ray\")",
            "    logger.propagate = True",
            "    yield",
            "    logger.propagate = False",
            "",
            "",
            "# Provide a shared Ray instance for a test class",
            "@pytest.fixture(scope=\"class\")",
            "def class_ray_instance():",
            "    yield ray.init()",
            "    ray.shutdown()",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@contextmanager",
            "def _ray_start(**kwargs):",
            "    init_kwargs = get_default_fixture_ray_kwargs()",
            "    init_kwargs.update(kwargs)",
            "    # Start the Ray processes.",
            "    address_info = ray.init(\"local\", **init_kwargs)",
            "",
            "    yield address_info",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_with_dashboard(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    if param.get(\"num_cpus\") is None:",
            "        param[\"num_cpus\"] = 1",
            "    with _ray_start(include_dashboard=True, **param) as address_info:",
            "        yield address_info",
            "",
            "",
            "@pytest.fixture",
            "def make_sure_dashboard_http_port_unused():",
            "    \"\"\"Make sure the dashboard agent http port is unused.\"\"\"",
            "    for process in psutil.process_iter():",
            "        should_kill = False",
            "        try:",
            "            for conn in process.connections():",
            "                if conn.laddr.port == ray_constants.DEFAULT_DASHBOARD_AGENT_LISTEN_PORT:",
            "                    should_kill = True",
            "                    break",
            "        except Exception:",
            "            continue",
            "        if should_kill:",
            "            try:",
            "                process.kill()",
            "                process.wait()",
            "            except Exception:",
            "                pass",
            "    yield",
            "",
            "",
            "# The following fixture will start ray with 0 cpu.",
            "@pytest.fixture",
            "def ray_start_no_cpu(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(num_cpus=0, **param) as res:",
            "        yield res",
            "",
            "",
            "# The following fixture will start ray with 1 cpu.",
            "@pytest.fixture",
            "def ray_start_regular(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "# We can compose external_redis and ray_start_regular instead of creating this",
            "# separate fixture, if there is a good way to ensure external_redis runs before",
            "# ray_start_regular.",
            "@pytest.fixture",
            "def ray_start_regular_with_external_redis(request, external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture(scope=\"module\")",
            "def ray_start_regular_shared(request):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture(scope=\"module\", params=[{\"local_mode\": True}, {\"local_mode\": False}])",
            "def ray_start_shared_local_modes(request):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_2_cpus(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(num_cpus=2, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_10_cpus(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start(num_cpus=10, **param) as res:",
            "        yield res",
            "",
            "",
            "@contextmanager",
            "def _ray_start_cluster(**kwargs):",
            "    cluster_not_supported_ = kwargs.pop(\"skip_cluster\", cluster_not_supported)",
            "    if cluster_not_supported_:",
            "        pytest.skip(\"Cluster not supported\")",
            "    init_kwargs = get_default_fixture_ray_kwargs()",
            "    num_nodes = 0",
            "    do_init = False",
            "    # num_nodes & do_init are not arguments for ray.init, so delete them.",
            "    if \"num_nodes\" in kwargs:",
            "        num_nodes = kwargs[\"num_nodes\"]",
            "        del kwargs[\"num_nodes\"]",
            "    if \"do_init\" in kwargs:",
            "        do_init = kwargs[\"do_init\"]",
            "        del kwargs[\"do_init\"]",
            "    elif num_nodes > 0:",
            "        do_init = True",
            "    init_kwargs.update(kwargs)",
            "    namespace = init_kwargs.pop(\"namespace\")",
            "    cluster = Cluster()",
            "    remote_nodes = []",
            "    for i in range(num_nodes):",
            "        if i > 0 and \"_system_config\" in init_kwargs:",
            "            del init_kwargs[\"_system_config\"]",
            "        remote_nodes.append(cluster.add_node(**init_kwargs))",
            "        # We assume driver will connect to the head (first node),",
            "        # so ray init will be invoked if do_init is true",
            "        if len(remote_nodes) == 1 and do_init:",
            "            ray.init(address=cluster.address, namespace=namespace)",
            "    yield cluster",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    cluster.shutdown()",
            "",
            "",
            "# This fixture will start a cluster with empty nodes.",
            "@pytest.fixture",
            "def ray_start_cluster(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_enabled(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    param[\"skip_cluster\"] = False",
            "    with _ray_start_cluster(**param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_init(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_head(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, num_nodes=1, **param) as res:",
            "        yield res",
            "",
            "",
            "# We can compose external_redis and ray_start_cluster_head instead of creating",
            "# this separate fixture, if there is a good way to ensure external_redis runs",
            "# before ray_start_cluster_head.",
            "@pytest.fixture",
            "def ray_start_cluster_head_with_external_redis(request, external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, num_nodes=1, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_head_with_env_vars(request, maybe_external_redis, monkeypatch):",
            "    param = getattr(request, \"param\", {})",
            "    env_vars = param.pop(\"env_vars\", {})",
            "    for k, v in env_vars.items():",
            "        monkeypatch.setenv(k, v)",
            "    with _ray_start_cluster(do_init=True, num_nodes=1, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_cluster_2_nodes(request, maybe_external_redis):",
            "    param = getattr(request, \"param\", {})",
            "    with _ray_start_cluster(do_init=True, num_nodes=2, **param) as res:",
            "        yield res",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_object_store_memory(request, maybe_external_redis):",
            "    # Start the Ray processes.",
            "    store_size = request.param",
            "    system_config = get_default_fixure_system_config()",
            "    init_kwargs = {",
            "        \"num_cpus\": 1,",
            "        \"_system_config\": system_config,",
            "        \"object_store_memory\": store_size,",
            "    }",
            "    ray.init(\"local\", **init_kwargs)",
            "    yield store_size",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "",
            "",
            "@pytest.fixture",
            "def call_ray_start(request):",
            "    with call_ray_start_context(request) as address:",
            "        yield address",
            "",
            "",
            "@contextmanager",
            "def call_ray_start_context(request):",
            "    default_cmd = (",
            "        \"ray start --head --num-cpus=1 --min-worker-port=0 \"",
            "        \"--max-worker-port=0 --port 0\"",
            "    )",
            "    parameter = getattr(request, \"param\", default_cmd)",
            "    env = None",
            "",
            "    if isinstance(parameter, dict):",
            "        if \"env\" in parameter:",
            "            env = {**os.environ, **parameter.get(\"env\")}",
            "",
            "        parameter = parameter.get(\"cmd\", default_cmd)",
            "",
            "    command_args = parameter.split(\" \")",
            "",
            "    try:",
            "        out = ray._private.utils.decode(",
            "            subprocess.check_output(command_args, stderr=subprocess.STDOUT, env=env)",
            "        )",
            "    except Exception as e:",
            "        print(type(e), e)",
            "        raise",
            "",
            "    # Get the redis address from the output.",
            "    redis_substring_prefix = \"--address='\"",
            "    idx = out.find(redis_substring_prefix)",
            "    if idx >= 0:",
            "        address_location = idx + len(redis_substring_prefix)",
            "        address = out[address_location:]",
            "        address = address.split(\"'\")[0]",
            "    else:",
            "        address = None",
            "",
            "    yield address",
            "",
            "    # Disconnect from the Ray cluster.",
            "    ray.shutdown()",
            "    # Kill the Ray cluster.",
            "    subprocess.check_call([\"ray\", \"stop\"], env=env)",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def call_ray_start_with_external_redis(request):",
            "    ports = getattr(request, \"param\", \"6379\")",
            "    port_list = ports.split(\",\")",
            "    for port in port_list:",
            "        temp_dir = ray._private.utils.get_ray_temp_dir()",
            "        start_redis_instance(temp_dir, int(port), password=\"123\")",
            "    address_str = \",\".join(map(lambda x: \"localhost:\" + x, port_list))",
            "    cmd = f\"ray start --head --address={address_str} --redis-password=123\"",
            "    subprocess.call(cmd.split(\" \"))",
            "",
            "    yield address_str.split(\",\")[0]",
            "",
            "    # Disconnect from the Ray cluster.",
            "    ray.shutdown()",
            "    # Kill the Ray cluster.",
            "    subprocess.check_call([\"ray\", \"stop\"])",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "@pytest.fixture",
            "def init_and_serve():",
            "    import ray.util.client.server.server as ray_client_server",
            "",
            "    server_handle, _ = ray_client_server.init_and_serve(\"localhost:50051\")",
            "    yield server_handle",
            "    ray_client_server.shutdown_with_server(server_handle.grpc_server)",
            "    time.sleep(2)",
            "",
            "",
            "@pytest.fixture",
            "def call_ray_stop_only():",
            "    yield",
            "    subprocess.check_call([\"ray\", \"stop\"])",
            "    # Delete the cluster address just in case.",
            "    ray._private.utils.reset_ray_address()",
            "",
            "",
            "# Used to test both Ray Client and non-Ray Client codepaths.",
            "# Usage: In your test, call `ray.init(address)`.",
            "@pytest.fixture(scope=\"function\", params=[\"ray_client\", \"no_ray_client\"])",
            "def start_cluster(ray_start_cluster_enabled, request):",
            "    assert request.param in {\"ray_client\", \"no_ray_client\"}",
            "    use_ray_client: bool = request.param == \"ray_client\"",
            "    if os.environ.get(\"RAY_MINIMAL\") == \"1\" and use_ray_client:",
            "        pytest.skip(\"Skipping due to we don't have ray client in minimal.\")",
            "",
            "    cluster = ray_start_cluster_enabled",
            "    cluster.add_node(num_cpus=4, dashboard_agent_listen_port=find_free_port())",
            "    if use_ray_client:",
            "        cluster.head_node._ray_params.ray_client_server_port = \"10004\"",
            "        cluster.head_node.start_ray_client_server()",
            "        address = \"ray://localhost:10004\"",
            "    else:",
            "        address = cluster.address",
            "",
            "    yield cluster, address",
            "",
            "",
            "@pytest.fixture(scope=\"function\")",
            "def tmp_working_dir():",
            "    with tempfile.TemporaryDirectory() as tmp_dir:",
            "        path = Path(tmp_dir)",
            "",
            "        hello_file = path / \"hello\"",
            "        with hello_file.open(mode=\"w\") as f:",
            "            f.write(\"world\")",
            "",
            "        module_path = path / \"test_module\"",
            "        module_path.mkdir(parents=True)",
            "",
            "        test_file = module_path / \"test.py\"",
            "        with test_file.open(mode=\"w\") as f:",
            "            f.write(\"def one():\\n\")",
            "            f.write(\"    return 1\\n\")",
            "",
            "        init_file = module_path / \"__init__.py\"",
            "        with init_file.open(mode=\"w\") as f:",
            "            f.write(\"from test_module.test import one\\n\")",
            "",
            "        yield tmp_dir",
            "",
            "",
            "@pytest.fixture",
            "def enable_pickle_debug():",
            "    os.environ[\"RAY_PICKLE_VERBOSE_DEBUG\"] = \"1\"",
            "    yield",
            "    del os.environ[\"RAY_PICKLE_VERBOSE_DEBUG\"]",
            "",
            "",
            "@pytest.fixture",
            "def set_enable_auto_connect(enable_auto_connect: str = \"0\"):",
            "    try:",
            "        os.environ[\"RAY_ENABLE_AUTO_CONNECT\"] = enable_auto_connect",
            "        yield enable_auto_connect",
            "    finally:",
            "        del os.environ[\"RAY_ENABLE_AUTO_CONNECT\"]",
            "",
            "",
            "@pytest.fixture",
            "def enable_mac_large_object_store():",
            "    os.environ[\"RAY_ENABLE_MAC_LARGE_OBJECT_STORE\"] = \"1\"",
            "    yield",
            "    del os.environ[\"RAY_ENABLE_MAC_LARGE_OBJECT_STORE\"]",
            "",
            "",
            "@pytest.fixture()",
            "def two_node_cluster():",
            "    system_config = {",
            "        \"object_timeout_milliseconds\": 200,",
            "    }",
            "    if cluster_not_supported:",
            "        pytest.skip(\"Cluster not supported\")",
            "    cluster = ray.cluster_utils.Cluster(",
            "        head_node_args={\"_system_config\": system_config}",
            "    )",
            "    for _ in range(2):",
            "        remote_node = cluster.add_node(num_cpus=1)",
            "    ray.init(address=cluster.address)",
            "    yield cluster, remote_node",
            "",
            "    # The code after the yield will run as teardown code.",
            "    ray.shutdown()",
            "    cluster.shutdown()",
            "",
            "",
            "@pytest.fixture()",
            "def error_pubsub():",
            "    p = init_error_pubsub()",
            "    yield p",
            "    p.close()",
            "",
            "",
            "@pytest.fixture()",
            "def log_pubsub():",
            "    p = init_log_pubsub()",
            "    yield p",
            "    p.close()",
            "",
            "",
            "@pytest.fixture",
            "def use_tls(request):",
            "    if request.param:",
            "        key_filepath, cert_filepath, temp_dir = setup_tls()",
            "    yield request.param",
            "    if request.param:",
            "        teardown_tls(key_filepath, cert_filepath, temp_dir)",
            "",
            "",
            "\"\"\"",
            "Object spilling test fixture",
            "\"\"\"",
            "# -- Smart open param --",
            "bucket_name = \"object-spilling-test\"",
            "",
            "# -- File system param --",
            "spill_local_path = \"/tmp/spill\"",
            "",
            "# -- Spilling configs --",
            "file_system_object_spilling_config = {",
            "    \"type\": \"filesystem\",",
            "    \"params\": {\"directory_path\": spill_local_path},",
            "}",
            "",
            "buffer_object_spilling_config = {",
            "    \"type\": \"filesystem\",",
            "    \"params\": {\"directory_path\": spill_local_path, \"buffer_size\": 1_000_000},",
            "}",
            "",
            "# Since we have differet protocol for a local external storage (e.g., fs)",
            "# and distributed external storage (e.g., S3), we need to test both cases.",
            "# This mocks the distributed fs with cluster utils.",
            "mock_distributed_fs_object_spilling_config = {",
            "    \"type\": \"mock_distributed_fs\",",
            "    \"params\": {\"directory_path\": spill_local_path},",
            "}",
            "smart_open_object_spilling_config = {",
            "    \"type\": \"smart_open\",",
            "    \"params\": {\"uri\": f\"s3://{bucket_name}/\"},",
            "}",
            "ray_storage_object_spilling_config = {",
            "    \"type\": \"ray_storage\",",
            "    # Force the storage config so we don't need to patch each test to separately",
            "    # configure the storage param under this.",
            "    \"params\": {\"_force_storage_for_testing\": spill_local_path},",
            "}",
            "buffer_open_object_spilling_config = {",
            "    \"type\": \"smart_open\",",
            "    \"params\": {\"uri\": f\"s3://{bucket_name}/\", \"buffer_size\": 1000},",
            "}",
            "multi_smart_open_object_spilling_config = {",
            "    \"type\": \"smart_open\",",
            "    \"params\": {\"uri\": [f\"s3://{bucket_name}/{i}\" for i in range(3)]},",
            "}",
            "",
            "unstable_object_spilling_config = {",
            "    \"type\": \"unstable_fs\",",
            "    \"params\": {",
            "        \"directory_path\": spill_local_path,",
            "    },",
            "}",
            "slow_object_spilling_config = {",
            "    \"type\": \"slow_fs\",",
            "    \"params\": {",
            "        \"directory_path\": spill_local_path,",
            "    },",
            "}",
            "",
            "",
            "def create_object_spilling_config(request, tmp_path):",
            "    temp_folder = tmp_path / \"spill\"",
            "    temp_folder.mkdir()",
            "    if (",
            "        request.param[\"type\"] == \"filesystem\"",
            "        or request.param[\"type\"] == \"mock_distributed_fs\"",
            "    ):",
            "        request.param[\"params\"][\"directory_path\"] = str(temp_folder)",
            "    return json.dumps(request.param), temp_folder",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        file_system_object_spilling_config,",
            "    ],",
            ")",
            "def fs_only_object_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        file_system_object_spilling_config,",
            "        ray_storage_object_spilling_config,",
            "        # TODO(sang): Add a mock dependency to test S3.",
            "        # smart_open_object_spilling_config,",
            "    ],",
            ")",
            "def object_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        file_system_object_spilling_config,",
            "        mock_distributed_fs_object_spilling_config,",
            "    ],",
            ")",
            "def multi_node_object_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        unstable_object_spilling_config,",
            "    ],",
            ")",
            "def unstable_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "@pytest.fixture(",
            "    scope=\"function\",",
            "    params=[",
            "        slow_object_spilling_config,",
            "    ],",
            ")",
            "def slow_spilling_config(request, tmp_path):",
            "    yield create_object_spilling_config(request, tmp_path)",
            "",
            "",
            "def _ray_start_chaos_cluster(request):",
            "    param = getattr(request, \"param\", {})",
            "    kill_interval = param.pop(\"kill_interval\", None)",
            "    config = param.pop(\"_system_config\", {})",
            "    config.update(",
            "        {",
            "            \"task_retry_delay_ms\": 100,",
            "        }",
            "    )",
            "    # Config of workers that are re-started.",
            "    head_resources = param.pop(\"head_resources\")",
            "    worker_node_types = param.pop(\"worker_node_types\")",
            "    cluster = AutoscalingCluster(",
            "        head_resources,",
            "        worker_node_types,",
            "        idle_timeout_minutes=10,  # Don't take down nodes.",
            "        **param,",
            "    )",
            "    cluster.start(_system_config=config)",
            "    ray.init(\"auto\")",
            "    nodes = ray.nodes()",
            "    assert len(nodes) == 1",
            "",
            "    if kill_interval is not None:",
            "        node_killer = get_and_run_node_killer(kill_interval)",
            "",
            "    yield cluster",
            "",
            "    if kill_interval is not None:",
            "        ray.get(node_killer.stop_run.remote())",
            "        killed = ray.get(node_killer.get_total_killed_nodes.remote())",
            "        assert len(killed) > 0",
            "        died = {node[\"NodeID\"] for node in ray.nodes() if not node[\"Alive\"]}",
            "        assert died.issubset(",
            "            killed",
            "        ), f\"Raylets {died - killed} that we did not kill crashed\"",
            "",
            "    ray.shutdown()",
            "    cluster.shutdown()",
            "",
            "",
            "@pytest.fixture",
            "def ray_start_chaos_cluster(request):",
            "    \"\"\"Returns the cluster and chaos thread.\"\"\"",
            "    for x in _ray_start_chaos_cluster(request):",
            "        yield x",
            "",
            "",
            "# Set scope to \"class\" to force this to run before start_cluster, whose scope",
            "# is \"function\".  We need these env vars to be set before Ray is started.",
            "@pytest.fixture(scope=\"class\")",
            "def runtime_env_disable_URI_cache():",
            "    with mock.patch.dict(",
            "        os.environ,",
            "        {",
            "            \"RAY_RUNTIME_ENV_CONDA_CACHE_SIZE_GB\": \"0\",",
            "            \"RAY_RUNTIME_ENV_PIP_CACHE_SIZE_GB\": \"0\",",
            "            \"RAY_RUNTIME_ENV_WORKING_DIR_CACHE_SIZE_GB\": \"0\",",
            "            \"RAY_RUNTIME_ENV_PY_MODULES_CACHE_SIZE_GB\": \"0\",",
            "        },",
            "    ):",
            "        print(",
            "            \"URI caching disabled (conda, pip, working_dir, py_modules cache \"",
            "            \"size set to 0).\"",
            "        )",
            "        yield",
            "",
            "",
            "# Use to create virtualenv that clone from current python env.",
            "# The difference between this fixture and `pytest_virtual.virtual` is that",
            "# `pytest_virtual.virtual` will not inherit current python env's site-package.",
            "# Note: Can't use in virtualenv, this must be noted when testing locally.",
            "@pytest.fixture(scope=\"function\")",
            "def cloned_virtualenv():",
            "    # Lazy import pytest_virtualenv,",
            "    # aviod import `pytest_virtualenv` in test case `Minimal install`",
            "    from pytest_virtualenv import VirtualEnv",
            "",
            "    if PipProcessor._is_in_virtualenv():",
            "        raise RuntimeError(\"Forbid the use of this fixture in virtualenv\")",
            "",
            "    venv = VirtualEnv(",
            "        args=[",
            "            \"--system-site-packages\",",
            "            \"--reset-app-data\",",
            "            \"--no-periodic-update\",",
            "            \"--no-download\",",
            "        ],",
            "    )",
            "    yield venv",
            "    venv.teardown()",
            "",
            "",
            "@pytest.fixture",
            "def set_runtime_env_retry_times(request):",
            "    runtime_env_retry_times = getattr(request, \"param\", \"0\")",
            "    try:",
            "        os.environ[\"RUNTIME_ENV_RETRY_TIMES\"] = runtime_env_retry_times",
            "        yield runtime_env_retry_times",
            "    finally:",
            "        del os.environ[\"RUNTIME_ENV_RETRY_TIMES\"]",
            "",
            "",
            "@pytest.fixture",
            "def listen_port(request):",
            "    port = getattr(request, \"param\", 0)",
            "    try:",
            "        sock = socket.socket()",
            "        if hasattr(socket, \"SO_REUSEPORT\"):",
            "            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 0)",
            "        sock.bind((\"127.0.0.1\", port))",
            "        yield port",
            "    finally:",
            "        sock.close()",
            "",
            "",
            "@pytest.fixture",
            "def set_bad_runtime_env_cache_ttl_seconds(request):",
            "    ttl = getattr(request, \"param\", \"0\")",
            "    os.environ[\"BAD_RUNTIME_ENV_CACHE_TTL_SECONDS\"] = ttl",
            "    yield ttl",
            "    del os.environ[\"BAD_RUNTIME_ENV_CACHE_TTL_SECONDS\"]",
            "",
            "",
            "@pytest.hookimpl(tryfirst=True, hookwrapper=True)",
            "def pytest_runtest_makereport(item, call):",
            "    # execute all other hooks to obtain the report object",
            "    outcome = yield",
            "    rep = outcome.get_result()",
            "",
            "    try:",
            "        append_short_test_summary(rep)",
            "    except Exception as e:",
            "        print(f\"+++ Error creating PyTest summary\\n{e}\")",
            "    try:",
            "        create_ray_logs_for_failed_test(rep)",
            "    except Exception as e:",
            "        print(f\"+++ Error saving Ray logs for failing test\\n{e}\")",
            "",
            "",
            "def append_short_test_summary(rep):",
            "    \"\"\"Writes a short summary txt for failed tests to be printed later.\"\"\"",
            "    if rep.when != \"call\":",
            "        return",
            "",
            "    summary_dir = os.environ.get(\"RAY_TEST_SUMMARY_DIR\")",
            "",
            "    if platform.system() != \"Linux\":",
            "        summary_dir = os.environ.get(\"RAY_TEST_SUMMARY_DIR_HOST\")",
            "",
            "    if not summary_dir:",
            "        return",
            "",
            "    if not os.path.exists(summary_dir):",
            "        os.makedirs(summary_dir, exist_ok=True)",
            "",
            "    test_name = rep.nodeid.replace(os.sep, \"::\")",
            "",
            "    if os.name == \"nt\":",
            "        # \":\" is not legal in filenames in windows",
            "        test_name.replace(\":\", \"$\")",
            "",
            "    header_file = os.path.join(summary_dir, \"000_header.txt\")",
            "    summary_file = os.path.join(summary_dir, test_name + \".txt\")",
            "",
            "    if rep.passed and os.path.exists(summary_file):",
            "        # The test succeeded after failing, thus it is flaky.",
            "        # We do not want to annotate flaky tests just now, so remove report.",
            "        os.remove(summary_file)",
            "        return",
            "",
            "    # Only consider failed tests from now on",
            "    if not rep.failed:",
            "        return",
            "",
            "    # No failing test information",
            "    if rep.longrepr is None:",
            "        return",
            "",
            "    # No failing test information",
            "    if not hasattr(rep.longrepr, \"chain\"):",
            "        return",
            "",
            "    if not os.path.exists(header_file):",
            "        with open(header_file, \"wt\") as fp:",
            "            test_label = os.environ.get(\"BUILDKITE_LABEL\", \"Unknown\")",
            "            job_id = os.environ.get(\"BUILDKITE_JOB_ID\")",
            "",
            "            fp.write(f\"### Pytest failures for: [{test_label}](#{job_id})\\n\\n\")",
            "",
            "    # Use `wt` here to overwrite so we only have one result per test (exclude retries)",
            "    with open(summary_file, \"wt\") as fp:",
            "        fp.write(_get_markdown_annotation(rep))",
            "",
            "",
            "def _get_markdown_annotation(rep) -> str:",
            "    # Main traceback is the last in the chain (where the last error is raised)",
            "    main_tb, main_loc, _ = rep.longrepr.chain[-1]",
            "    markdown = \"\"",
            "",
            "    # Only keep last line of the message",
            "    short_message = list(filter(None, main_loc.message.split(\"\\n\")))[-1]",
            "",
            "    # Header: Main error message",
            "    markdown += f\"#### {rep.nodeid}\\n\\n\"",
            "    markdown += \"<details>\\n\"",
            "    markdown += f\"<summary>{short_message}</summary>\\n\\n\"",
            "",
            "    # Add link to test definition",
            "    test_file, test_lineno, _test_node = rep.location",
            "    test_path, test_url = _get_repo_github_path_and_link(",
            "        os.path.abspath(test_file), test_lineno",
            "    )",
            "    markdown += f\"Link to test: [{test_path}:{test_lineno}]({test_url})\\n\\n\"",
            "",
            "    # Print main traceback",
            "    markdown += \"##### Traceback\\n\\n\"",
            "    markdown += \"```\\n\"",
            "    markdown += str(main_tb)",
            "    markdown += \"\\n```\\n\\n\"",
            "",
            "    # Print link to test definition in github",
            "    path, url = _get_repo_github_path_and_link(main_loc.path, main_loc.lineno)",
            "    markdown += f\"[{path}:{main_loc.lineno}]({url})\\n\\n\"",
            "",
            "    # If this is a longer exception chain, users can expand the full traceback",
            "    if len(rep.longrepr.chain) > 1:",
            "        markdown += \"<details><summary>Full traceback</summary>\\n\\n\"",
            "",
            "        # Here we just print each traceback and the link to the respective",
            "        # lines in GutHub",
            "        for tb, loc, _ in rep.longrepr.chain:",
            "            if loc:",
            "                path, url = _get_repo_github_path_and_link(loc.path, loc.lineno)",
            "                github_link = f\"[{path}:{loc.lineno}]({url})\\n\\n\"",
            "            else:",
            "                github_link = \"\"",
            "",
            "            markdown += \"```\\n\"",
            "            markdown += str(tb)",
            "            markdown += \"\\n```\\n\\n\"",
            "            markdown += github_link",
            "",
            "        markdown += \"</details>\\n\"",
            "",
            "    markdown += \"<details><summary>PIP packages</summary>\\n\\n\"",
            "    markdown += \"```\\n\"",
            "    markdown += \"\\n\".join(_get_pip_packages())",
            "    markdown += \"\\n```\\n\\n\"",
            "    markdown += \"</details>\\n\"",
            "",
            "    markdown += \"</details>\\n\\n\"",
            "    return markdown",
            "",
            "",
            "def _get_pip_packages() -> List[str]:",
            "    try:",
            "        from pip._internal.operations import freeze",
            "",
            "        return list(freeze.freeze())",
            "    except Exception:",
            "        return [\"invalid\"]",
            "",
            "",
            "def _get_repo_github_path_and_link(file: str, lineno: int) -> Tuple[str, str]:",
            "    base_url = \"https://github.com/ray-project/ray/blob/{commit}/{path}#L{lineno}\"",
            "",
            "    commit = os.environ.get(\"BUILDKITE_COMMIT\")",
            "",
            "    if not commit:",
            "        return file, \"\"",
            "",
            "    path = os.path.relpath(file, \"/ray\")",
            "",
            "    return path, base_url.format(commit=commit, path=path, lineno=lineno)",
            "",
            "",
            "def create_ray_logs_for_failed_test(rep):",
            "    \"\"\"Creates artifact zip of /tmp/ray/session_latest/logs for failed tests\"\"\"",
            "",
            "    # We temporarily restrict to Linux until we have artifact dirs",
            "    # for Windows and Mac",
            "    if platform.system() != \"Linux\":",
            "        return",
            "",
            "    # Only archive failed tests after the \"call\" phase of the test",
            "    if rep.when != \"call\" or not rep.failed:",
            "        return",
            "",
            "    # Get dir to write zipped logs to",
            "    archive_dir = os.environ.get(\"RAY_TEST_FAILURE_LOGS_ARCHIVE_DIR\")",
            "",
            "    if not archive_dir:",
            "        return",
            "",
            "    if not os.path.exists(archive_dir):",
            "        os.makedirs(archive_dir)",
            "",
            "    # Get logs dir from the latest ray session",
            "    tmp_dir = gettempdir()",
            "    logs_dir = os.path.join(tmp_dir, \"ray\", \"session_latest\", \"logs\")",
            "",
            "    if not os.path.exists(logs_dir):",
            "        return",
            "",
            "    # Write zipped logs to logs archive dir",
            "    test_name = rep.nodeid.replace(os.sep, \"::\")",
            "    output_file = os.path.join(archive_dir, f\"{test_name}_{time.time():.4f}\")",
            "    shutil.make_archive(output_file, \"zip\", logs_dir)",
            "",
            "",
            "@pytest.fixture(params=[True, False])",
            "def start_http_proxy(request):",
            "    env = {}",
            "",
            "    proxy = None",
            "    try:",
            "        if request.param:",
            "            # the `proxy` command is from the proxy.py package.",
            "            proxy = subprocess.Popen(",
            "                [\"proxy\", \"--port\", \"8899\", \"--log-level\", \"ERROR\"]",
            "            )",
            "            env[\"RAY_grpc_enable_http_proxy\"] = \"1\"",
            "            proxy_url = \"http://localhost:8899\"",
            "        else:",
            "            proxy_url = \"http://example.com\"",
            "        env[\"http_proxy\"] = proxy_url",
            "        env[\"https_proxy\"] = proxy_url",
            "        yield env",
            "    finally:",
            "        if proxy:",
            "            proxy.terminate()",
            "            proxy.wait()",
            "",
            "",
            "@pytest.fixture",
            "def set_runtime_env_plugins(request):",
            "    runtime_env_plugins = getattr(request, \"param\", \"0\")",
            "    try:",
            "        os.environ[\"RAY_RUNTIME_ENV_PLUGINS\"] = runtime_env_plugins",
            "        yield runtime_env_plugins",
            "    finally:",
            "        del os.environ[\"RAY_RUNTIME_ENV_PLUGINS\"]",
            "",
            "",
            "@pytest.fixture",
            "def set_runtime_env_plugin_schemas(request):",
            "    runtime_env_plugin_schemas = getattr(request, \"param\", \"0\")",
            "    try:",
            "        os.environ[\"RAY_RUNTIME_ENV_PLUGIN_SCHEMAS\"] = runtime_env_plugin_schemas",
            "        # Clear and reload schemas.",
            "        RuntimeEnvPluginSchemaManager.clear()",
            "        yield runtime_env_plugin_schemas",
            "    finally:",
            "        del os.environ[\"RAY_RUNTIME_ENV_PLUGIN_SCHEMAS\"]",
            "",
            "",
            "@pytest.fixture(scope=\"function\")",
            "def temp_file(request):",
            "    with tempfile.NamedTemporaryFile(\"r+b\") as fp:",
            "        yield fp",
            "",
            "",
            "@pytest.fixture(scope=\"function\")",
            "def temp_dir(request):",
            "    with tempfile.TemporaryDirectory(\"r+b\") as d:",
            "        yield d",
            "",
            "",
            "@pytest.fixture(scope=\"module\")",
            "def random_ascii_file(request):",
            "    import random",
            "    import string",
            "",
            "    file_size = getattr(request, \"param\", 1 << 10)",
            "",
            "    with tempfile.NamedTemporaryFile(mode=\"r+b\") as fp:",
            "        fp.write(\"\".join(random.choices(string.ascii_letters, k=file_size)).encode())",
            "        fp.flush()",
            "",
            "        yield fp"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.PIL.IcnsImagePlugin.IcnsFile.dataforsize"
        ]
    },
    "python/ray/tests/test_state_api_log.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 417,
                "PatchRowcode": "     assert start_offset == expected_start + len(exclude_tail_content)"
            },
            "1": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 418,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 419,
                "afterPatchRowNumber": 419,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 420,
                "PatchRowcode": "+def test_log_agent_resolve_filename(temp_dir):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 421,
                "PatchRowcode": "+    \"\"\""
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 422,
                "PatchRowcode": "+    Test that LogAgentV1Grpc.resolve_filename(root, filename) works:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 423,
                "PatchRowcode": "+    1. Not possible to resolve a file that doesn't exist."
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 424,
                "PatchRowcode": "+    2. Not able to resolve files outside of the temp dir root."
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 425,
                "PatchRowcode": "+        - with a absolute path."
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 426,
                "PatchRowcode": "+        - with a relative path recursive up."
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 427,
                "PatchRowcode": "+    3. Permits a file in a directory that's symlinked into the root dir."
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 428,
                "PatchRowcode": "+    \"\"\""
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 429,
                "PatchRowcode": "+    root = Path(temp_dir)"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 430,
                "PatchRowcode": "+    # Create a file in the temp dir."
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 431,
                "PatchRowcode": "+    file = root / \"valid_file\""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 432,
                "PatchRowcode": "+    file.touch()"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 433,
                "PatchRowcode": "+    subdir = root / \"subdir\""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 434,
                "PatchRowcode": "+    subdir.mkdir()"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 435,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 436,
                "PatchRowcode": "+    # Create a directory in the root that contains a valid file and"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 437,
                "PatchRowcode": "+    # is symlinked to by a path in the subdir."
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 438,
                "PatchRowcode": "+    symlinked_dir = root / \"symlinked\""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 439,
                "PatchRowcode": "+    symlinked_dir.mkdir()"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 440,
                "PatchRowcode": "+    symlinked_file = symlinked_dir / \"valid_file\""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 441,
                "PatchRowcode": "+    symlinked_file.touch()"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 442,
                "PatchRowcode": "+    symlinked_path_in_subdir = subdir / \"symlink_to_outside_dir\""
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 443,
                "PatchRowcode": "+    symlinked_path_in_subdir.symlink_to(symlinked_dir)"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 444,
                "PatchRowcode": "+"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 445,
                "PatchRowcode": "+    # Test file doesn't exist"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 446,
                "PatchRowcode": "+    with pytest.raises(FileNotFoundError):"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 447,
                "PatchRowcode": "+        LogAgentV1Grpc._resolve_filename(root, \"non-exist-file\")"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 448,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 449,
                "PatchRowcode": "+    # Test absolute path outside of root is not allowed"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 450,
                "PatchRowcode": "+    with pytest.raises(FileNotFoundError):"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 451,
                "PatchRowcode": "+        LogAgentV1Grpc._resolve_filename(subdir, root.resolve() / \"valid_file\")"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 452,
                "PatchRowcode": "+"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 453,
                "PatchRowcode": "+    # Test relative path recursive up is not allowed"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 454,
                "PatchRowcode": "+    with pytest.raises(FileNotFoundError):"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 455,
                "PatchRowcode": "+        LogAgentV1Grpc._resolve_filename(subdir, \"../valid_file\")"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 456,
                "PatchRowcode": "+"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 457,
                "PatchRowcode": "+    # Test relative path a valid file is allowed"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 458,
                "PatchRowcode": "+    assert ("
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 459,
                "PatchRowcode": "+        LogAgentV1Grpc._resolve_filename(root, \"valid_file\")"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 460,
                "PatchRowcode": "+        == (root / \"valid_file\").resolve()"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 461,
                "PatchRowcode": "+    )"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 462,
                "PatchRowcode": "+"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 463,
                "PatchRowcode": "+    # Test relative path to a valid file following a symlink is allowed"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 464,
                "PatchRowcode": "+    assert ("
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 465,
                "PatchRowcode": "+        LogAgentV1Grpc._resolve_filename(subdir, \"symlink_to_outside_dir/valid_file\")"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 466,
                "PatchRowcode": "+        == (root / \"symlinked\" / \"valid_file\").resolve()"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 467,
                "PatchRowcode": "+    )"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 468,
                "PatchRowcode": "+"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 469,
                "PatchRowcode": "+"
            },
            "53": {
                "beforePatchRowNumber": 420,
                "afterPatchRowNumber": 470,
                "PatchRowcode": " # Unit Tests (LogsManager)"
            },
            "54": {
                "beforePatchRowNumber": 421,
                "afterPatchRowNumber": 471,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 422,
                "afterPatchRowNumber": 472,
                "PatchRowcode": " "
            },
            "56": {
                "beforePatchRowNumber": 1066,
                "afterPatchRowNumber": 1116,
                "PatchRowcode": "     wait_for_condition(verify)"
            },
            "57": {
                "beforePatchRowNumber": 1067,
                "afterPatchRowNumber": 1117,
                "PatchRowcode": " "
            },
            "58": {
                "beforePatchRowNumber": 1068,
                "afterPatchRowNumber": 1118,
                "PatchRowcode": " "
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1119,
                "PatchRowcode": "+def test_log_get_invalid_filenames(ray_start_with_dashboard, temp_file):"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1120,
                "PatchRowcode": "+    assert ("
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1121,
                "PatchRowcode": "+        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1122,
                "PatchRowcode": "+        is True"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1123,
                "PatchRowcode": "+    )"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1124,
                "PatchRowcode": "+    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1125,
                "PatchRowcode": "+    webui_url = format_web_url(webui_url)"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1126,
                "PatchRowcode": "+    node_id = list_nodes()[0][\"node_id\"]"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1127,
                "PatchRowcode": "+"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1128,
                "PatchRowcode": "+    # log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1129,
                "PatchRowcode": "+"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1130,
                "PatchRowcode": "+    def verify():"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1131,
                "PatchRowcode": "+        # Kind of hack that we know the file node_ip_address.json exists in ray."
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1132,
                "PatchRowcode": "+        with pytest.raises(RayStateApiException) as e:"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1133,
                "PatchRowcode": "+            logs = \"\".join(get_log(node_id=node_id, filename=\"../node_ip_address.json\"))"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1134,
                "PatchRowcode": "+            print(logs)"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1135,
                "PatchRowcode": "+            assert \"does not start with \" in str(e.value)"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1136,
                "PatchRowcode": "+        return True"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1137,
                "PatchRowcode": "+"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1138,
                "PatchRowcode": "+    wait_for_condition(verify)"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1139,
                "PatchRowcode": "+"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1140,
                "PatchRowcode": "+    # Verify that reading file outside of the log directory is not allowed"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1141,
                "PatchRowcode": "+    # with absolute path."
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1142,
                "PatchRowcode": "+    def verify():"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1143,
                "PatchRowcode": "+        # Kind of hack that we know the file node_ip_address.json exists in ray."
            },
            "84": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1144,
                "PatchRowcode": "+        temp_file_abs_path = str(Path(temp_file.name).resolve())"
            },
            "85": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1145,
                "PatchRowcode": "+        with pytest.raises(RayStateApiException) as e:"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1146,
                "PatchRowcode": "+            logs = \"\".join(get_log(node_id=node_id, filename=temp_file_abs_path))"
            },
            "87": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1147,
                "PatchRowcode": "+            print(logs)"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1148,
                "PatchRowcode": "+            assert \"does not start with \" in str(e.value)"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1149,
                "PatchRowcode": "+        return True"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1150,
                "PatchRowcode": "+"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1151,
                "PatchRowcode": "+    wait_for_condition(verify)"
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1152,
                "PatchRowcode": "+"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1153,
                "PatchRowcode": "+"
            },
            "94": {
                "beforePatchRowNumber": 1069,
                "afterPatchRowNumber": 1154,
                "PatchRowcode": " def test_log_get_subdir(ray_start_with_dashboard):"
            },
            "95": {
                "beforePatchRowNumber": 1070,
                "afterPatchRowNumber": 1155,
                "PatchRowcode": "     assert ("
            },
            "96": {
                "beforePatchRowNumber": 1071,
                "afterPatchRowNumber": 1156,
                "PatchRowcode": "         wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])"
            }
        },
        "frontPatchFile": [
            "import json",
            "import os",
            "import sys",
            "import asyncio",
            "from typing import List",
            "import urllib",
            "from unittest.mock import MagicMock",
            "",
            "import pytest",
            "from ray.util.state.state_cli import logs_state_cli_group",
            "from ray.util.state import list_jobs",
            "import requests",
            "from click.testing import CliRunner",
            "import grpc",
            "",
            "from pathlib import Path",
            "",
            "import ray",
            "from ray._private.test_utils import (",
            "    format_web_url,",
            "    wait_for_condition,",
            "    wait_until_server_available,",
            ")",
            "",
            "from ray._private.ray_constants import (",
            "    LOG_PREFIX_TASK_ATTEMPT_START,",
            "    LOG_PREFIX_TASK_ATTEMPT_END,",
            ")",
            "from ray._raylet import ActorID, NodeID, TaskID, WorkerID",
            "from ray.core.generated.common_pb2 import Address",
            "from ray.core.generated.gcs_service_pb2 import GetTaskEventsReply",
            "from ray.core.generated.reporter_pb2 import ListLogsReply, StreamLogReply",
            "from ray.core.generated.gcs_pb2 import (",
            "    ActorTableData,",
            "    TaskEvents,",
            "    TaskStateUpdate,",
            "    TaskLogInfo,",
            ")",
            "from ray.dashboard.modules.actor.actor_head import actor_table_data_to_dict",
            "from ray.dashboard.modules.log.log_agent import (",
            "    find_offset_of_content_in_file,",
            "    find_end_offset_file,",
            "    find_end_offset_next_n_lines_from_offset,",
            "    find_start_offset_last_n_lines_from_offset,",
            "    LogAgentV1Grpc,",
            ")",
            "from ray.dashboard.modules.log.log_agent import _stream_log_in_chunk",
            "from ray.dashboard.modules.log.log_manager import LogsManager",
            "from ray.dashboard.tests.conftest import *  # noqa",
            "from ray.util.state import get_log, list_logs, list_nodes, list_workers",
            "from ray.util.state.common import GetLogOptions",
            "from ray.util.state.exception import DataSourceUnavailable, RayStateApiException",
            "from ray.util.state.state_manager import StateDataSourceClient",
            "",
            "if sys.version_info >= (3, 8, 0):",
            "    from unittest.mock import AsyncMock",
            "else:",
            "    from asyncmock import AsyncMock",
            "",
            "",
            "ASYNCMOCK_MIN_PYTHON_VER = (3, 8)",
            "",
            "",
            "def generate_task_event(",
            "    task_id,",
            "    node_id,",
            "    attempt_number,",
            "    worker_id,",
            "    stdout_file=None,",
            "    stderr_file=None,",
            "    stdout_start=None,",
            "    stderr_start=None,",
            "    stdout_end=None,",
            "    stderr_end=None,",
            "):",
            "    task_event = TaskEvents(",
            "        task_id=task_id.binary(),",
            "        attempt_number=attempt_number,",
            "        job_id=b\"\",",
            "        state_updates=TaskStateUpdate(",
            "            node_id=node_id.binary(),",
            "            worker_id=worker_id.binary(),",
            "            task_log_info=TaskLogInfo(",
            "                stdout_file=stdout_file,",
            "                stderr_file=stderr_file,",
            "                stdout_start=stdout_start,",
            "                stderr_start=stderr_start,",
            "                stdout_end=stdout_end,",
            "                stderr_end=stderr_end,",
            "            ),",
            "        ),",
            "    )",
            "",
            "    return task_event",
            "",
            "",
            "def generate_actor_data(id, node_id, worker_id):",
            "    if worker_id:",
            "        worker_id = worker_id.binary()",
            "    message = ActorTableData(",
            "        actor_id=id.binary(),",
            "        state=ActorTableData.ActorState.ALIVE,",
            "        name=\"abc\",",
            "        pid=1234,",
            "        class_name=\"class\",",
            "        address=Address(",
            "            raylet_id=node_id.binary(),",
            "            ip_address=\"127.0.0.1\",",
            "            port=1234,",
            "            worker_id=worker_id,",
            "        ),",
            "    )",
            "    return actor_table_data_to_dict(message)",
            "",
            "",
            "# Unit Tests (Log Agent)",
            "def _read_file(fp, start, end):",
            "    \"\"\"Help func to read a file with offsets\"\"\"",
            "    fp.seek(start, 0)",
            "    if end == -1:",
            "        return fp.read()",
            "    return fp.read(end - start)",
            "",
            "",
            "async def _stream_log(context, fp, start, end):",
            "    \"\"\"Help func to stream a log with offsets\"\"\"",
            "    result = bytearray()",
            "    async for chunk_res in _stream_log_in_chunk(",
            "        context=context,",
            "        file=fp,",
            "        start_offset=start,",
            "        end_offset=end,",
            "        keep_alive_interval_sec=-1,",
            "    ):",
            "        result += chunk_res.data",
            "    return result",
            "",
            "",
            "TEST_LINE_TEMPLATE = \"{}-test-line\"",
            "",
            "",
            "def _write_lines_and_get_offset_at_index(",
            "    f, num_lines, start_offset=0, trailing_new_line=True",
            "):",
            "    \"\"\"",
            "    Write multiple lines into a file, and record offsets",
            "",
            "    Args:",
            "        f: a binary file object that's writable",
            "        num_lines: Number of lines to write",
            "        start_offset: The offset to start writing",
            "        trailing_new_line: True if a '\\n' is added at the end of the",
            "            lines.",
            "",
            "    Return:",
            "        offsets: A list of offsets of the lines.",
            "        offset_end: The offset of the end of file.",
            "    \"\"\"",
            "    f.seek(start_offset, 0)",
            "",
            "    offsets = []",
            "    for i in range(num_lines):",
            "        offsets.append(f.tell())",
            "        if i == num_lines - 1 and not trailing_new_line:",
            "            # Last line no newline",
            "            line = TEST_LINE_TEMPLATE.format(i)",
            "        else:",
            "            line = TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "        f.write(line.encode(\"utf-8\"))",
            "",
            "    f.flush()",
            "    f.seek(0, 2)",
            "    offset_end = f.tell()",
            "",
            "    return offsets, offset_end",
            "",
            "",
            "@pytest.mark.parametrize(\"new_line\", [True, False])",
            "@pytest.mark.parametrize(\"block_size\", [4, 16, 256])",
            "def test_find_start_offset_last_n_lines_from_offset(new_line, temp_file, block_size):",
            "    file = temp_file",
            "    o, end_file = _write_lines_and_get_offset_at_index(",
            "        file, num_lines=50, start_offset=0, trailing_new_line=new_line",
            "    )",
            "    # Test the function with different offsets and number of lines to find",
            "    assert find_start_offset_last_n_lines_from_offset(file, o[3], 1, block_size) == o[2]",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[10], 10, block_size) == o[0]",
            "    )",
            "",
            "    # Test end of file last 1 line",
            "    assert find_start_offset_last_n_lines_from_offset(file, -1, 1, block_size) == o[-1]",
            "",
            "    # Test end of file no line",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, -1, 0, block_size) == end_file",
            "    )",
            "",
            "    # Test no line from middle of file",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[30], 0, block_size) == o[30]",
            "    )",
            "",
            "    # Test more lines than file",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[30], 100, block_size) == o[0]",
            "    )",
            "",
            "    # Test offsets in the middle of a line",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[2] + 1, 1, block_size)",
            "        == o[2]",
            "    )",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[2] - 1, 1, block_size)",
            "        == o[1]",
            "    )",
            "",
            "",
            "def test_find_end_offset_next_n_lines_from_offset(temp_file):",
            "    file = temp_file",
            "    o, end_file = _write_lines_and_get_offset_at_index(",
            "        file, num_lines=10, start_offset=0",
            "    )",
            "    # Test the function with different offsets and number of lines to find",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[3], 1) == o[4]",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[3], 2) == o[5]",
            "    assert find_end_offset_next_n_lines_from_offset(file, 0, 1) == o[1]",
            "",
            "    # Test end of file",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[3], 999) == end_file",
            "",
            "    # Test offset diff",
            "    assert find_end_offset_next_n_lines_from_offset(file, 1, 1) == o[1]",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[1] - 1, 1) == o[1]",
            "",
            "",
            "def test_find_offset_of_content_in_file(temp_file):",
            "    file = temp_file",
            "    o, end_file = _write_lines_and_get_offset_at_index(file, num_lines=10)",
            "",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(0).encode(\"utf-8\")",
            "        )",
            "        == o[0]",
            "    )",
            "",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(3).encode(\"utf-8\"), o[1] + 1",
            "        )",
            "        == o[3]",
            "    )",
            "",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(4).encode(\"utf-8\"), o[1] - 1",
            "        )",
            "        == o[4]",
            "    )",
            "",
            "    # Not found",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(1000).encode(\"utf-8\"), o[1] - 1",
            "        )",
            "        == -1",
            "    )",
            "",
            "",
            "@pytest.mark.asyncio",
            "@pytest.mark.parametrize(\"random_ascii_file\", [1 << 20], indirect=True)",
            "@pytest.mark.parametrize(",
            "    \"start_offset,end_offset\",",
            "    [",
            "        (0, 1 << 20),",
            "        (1 << 20, 1 << 20),",
            "        (0, 0),",
            "        (0, 1),",
            "        (1 << 16, 1 << 20),",
            "        (1024, 2042),",
            "    ],",
            ")",
            "async def test_stream_log_in_chunk(random_ascii_file, start_offset, end_offset):",
            "    \"\"\"Test streaming of a file from different offsets\"\"\"",
            "    test_file = random_ascii_file",
            "    context = MagicMock(grpc.aio.ServicerContext)",
            "    context.done.return_value = False",
            "",
            "    expected_file_content = _read_file(test_file, start_offset, end_offset)",
            "    actual_log_content = await _stream_log(context, test_file, start_offset, end_offset)",
            "",
            "    assert (",
            "        expected_file_content == actual_log_content",
            "    ), \"Non-matching content from log streamed\"",
            "",
            "",
            "@pytest.mark.asyncio",
            "@pytest.mark.parametrize(",
            "    \"lines_to_tail,total_lines\",",
            "    [(0, 100), (100, 100), (10, 100), (1, 100), (99, 100)],",
            ")",
            "@pytest.mark.parametrize(\"trailing_new_line\", [True, False])",
            "async def test_log_tails(lines_to_tail, total_lines, trailing_new_line, temp_file):",
            "    \"\"\"Test tailing a file works\"\"\"",
            "    _write_lines_and_get_offset_at_index(",
            "        temp_file,",
            "        total_lines,",
            "        trailing_new_line=trailing_new_line,",
            "    )",
            "    test_file = temp_file",
            "    context = MagicMock(grpc.aio.ServicerContext)",
            "    context.done.return_value = False",
            "    start_offset = find_start_offset_last_n_lines_from_offset(",
            "        test_file, offset=-1, n=lines_to_tail",
            "    )",
            "",
            "    actual_data = await _stream_log(context, test_file, start_offset, -1)",
            "    expected_data = _read_file(test_file, start_offset, -1)",
            "",
            "    assert actual_data == expected_data, \"Non-matching data from stream log\"",
            "",
            "    all_lines = actual_data.decode(\"utf-8\")",
            "    assert all_lines.count(\"\\n\") == (",
            "        lines_to_tail if trailing_new_line or lines_to_tail == 0 else lines_to_tail - 1",
            "    ), \"Non-matching number of lines tailed\"",
            "",
            "",
            "@pytest.mark.asyncio",
            "@pytest.mark.parametrize(",
            "    \"lines_to_tail,total_lines\",",
            "    [(0, 5), (5, 5), (2, 5), (1, 5), (4, 5)],",
            ")",
            "async def test_log_tails_with_appends(lines_to_tail, total_lines, temp_file):",
            "    \"\"\"Test tailing a log file that grows at the same time\"\"\"",
            "    _write_lines_and_get_offset_at_index(temp_file, total_lines)",
            "    test_file = temp_file",
            "    context = MagicMock(grpc.aio.ServicerContext)",
            "    context.done.return_value = False",
            "    start_offset = find_start_offset_last_n_lines_from_offset(",
            "        test_file, offset=-1, n=lines_to_tail",
            "    )",
            "",
            "    actual_data = await _stream_log(context, test_file, start_offset, -1)",
            "",
            "    end_offset = find_end_offset_file(test_file)",
            "    expected_data = _read_file(test_file, start_offset, end_offset)",
            "    assert actual_data == expected_data, \"Non-matching data from stream log\"",
            "",
            "    all_lines = actual_data.decode(\"utf-8\")",
            "    assert all_lines.count(\"\\n\") == lines_to_tail, \"Non-matching number of lines tailed\"",
            "",
            "    # Modify the file with append here",
            "    num_new_lines = 2",
            "    _write_lines_and_get_offset_at_index(",
            "        temp_file, num_new_lines, start_offset=end_offset",
            "    )",
            "",
            "    # Tail again should read the new lines written",
            "    start_offset = find_start_offset_last_n_lines_from_offset(",
            "        test_file, offset=-1, n=lines_to_tail + num_new_lines",
            "    )",
            "",
            "    expected_data = _read_file(test_file, start_offset, -1)",
            "    actual_data = await _stream_log(context, test_file, start_offset, -1)",
            "",
            "    assert (",
            "        actual_data == expected_data",
            "    ), \"Non-matching data from stream log after append\"",
            "",
            "    all_lines = actual_data.decode(\"utf-8\")",
            "    assert (",
            "        all_lines.count(\"\\n\") == lines_to_tail + num_new_lines",
            "    ), \"Non-matching number of lines tailed after append\"",
            "",
            "",
            "@pytest.mark.asyncio",
            "async def test_log_agent_find_task_log_offsets(temp_file):",
            "    log_file_content = \"\"",
            "    task_id = \"taskid1234\"",
            "    attempt_number = 0",
            "    # Previous data",
            "    for i in range(3):",
            "        log_file_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "    # Task's logs",
            "    log_file_content += f\"{LOG_PREFIX_TASK_ATTEMPT_START}{task_id}-{attempt_number}\\n\"",
            "    expected_start = len(log_file_content)",
            "    for i in range(10):",
            "        log_file_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "    expected_end = len(log_file_content)",
            "    log_file_content += f\"{LOG_PREFIX_TASK_ATTEMPT_END}{task_id}-{attempt_number}\\n\"",
            "",
            "    # Next data",
            "    for i in range(3):",
            "        log_file_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "",
            "    # Write to files",
            "    temp_file.write(log_file_content.encode(\"utf-8\"))",
            "",
            "    # Test all task logs",
            "    start_offset, end_offset = await LogAgentV1Grpc._find_task_log_offsets(",
            "        task_id, attempt_number, -1, temp_file",
            "    )",
            "    assert start_offset == expected_start",
            "    assert end_offset == expected_end",
            "",
            "    # Test tailing last X lines",
            "    num_tail = 3",
            "    start_offset, end_offset = await LogAgentV1Grpc._find_task_log_offsets(",
            "        task_id, attempt_number, num_tail, temp_file",
            "    )",
            "    assert end_offset == expected_end",
            "    exclude_tail_content = \"\"",
            "    for i in range(10 - num_tail):",
            "        exclude_tail_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "    assert start_offset == expected_start + len(exclude_tail_content)",
            "",
            "",
            "# Unit Tests (LogsManager)",
            "",
            "",
            "@pytest.fixture",
            "def logs_manager():",
            "    if sys.version_info < ASYNCMOCK_MIN_PYTHON_VER:",
            "        raise Exception(f\"Unsupported for this version of python {sys.version_info}\")",
            "    from unittest.mock import AsyncMock",
            "",
            "    client = AsyncMock(StateDataSourceClient)",
            "    manager = LogsManager(client)",
            "    yield manager",
            "",
            "",
            "def generate_list_logs(log_files: List[str]):",
            "    return ListLogsReply(log_files=log_files)",
            "",
            "",
            "def generate_logs_stream_chunk(index: int):",
            "    return f\"{str(index)*10}\"",
            "",
            "",
            "async def generate_logs_stream(num_chunks: int):",
            "    for i in range(num_chunks):",
            "        data = generate_logs_stream_chunk(index=i)",
            "        yield StreamLogReply(data=data.encode())",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_list_logs(logs_manager):",
            "    logs_client = logs_manager.data_source_client",
            "",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [\"1\", \"2\"]",
            "",
            "    logs_client.list_logs.side_effect = [",
            "        generate_list_logs([\"gcs_server.out\"]),",
            "        DataSourceUnavailable(),",
            "    ]",
            "",
            "    # Unregistered node id should raise a DataSourceUnavailable.",
            "    with pytest.raises(DataSourceUnavailable):",
            "        result = await logs_manager.list_logs(",
            "            node_id=\"3\", timeout=30, glob_filter=\"*gcs*\"",
            "        )",
            "",
            "    result = await logs_manager.list_logs(node_id=\"2\", timeout=30, glob_filter=\"*gcs*\")",
            "    assert len(result) == 1",
            "    assert result[\"gcs_server\"] == [\"gcs_server.out\"]",
            "    assert result[\"raylet\"] == []",
            "    logs_client.get_all_registered_log_agent_ids.assert_called()",
            "    logs_client.list_logs.assert_awaited_with(\"2\", \"*gcs*\", timeout=30)",
            "",
            "    # The second call raises DataSourceUnavailable, which will",
            "    # return DataSourceUnavailable to the caller.",
            "    with pytest.raises(DataSourceUnavailable):",
            "        result = await logs_manager.list_logs(",
            "            node_id=\"1\", timeout=30, glob_filter=\"*gcs*\"",
            "        )",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_resolve_file(logs_manager):",
            "    node_id = NodeID(b\"1\" * 28)",
            "    \"\"\"",
            "    Test filename is given.",
            "    \"\"\"",
            "    logs_client = logs_manager.data_source_client",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [node_id.hex()]",
            "    expected_filename = \"filename\"",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=expected_filename,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=None,",
            "        get_actor_fn=lambda _: True,",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    assert log_file_name == expected_filename",
            "    assert n == node_id.hex()",
            "    \"\"\"",
            "    Test actor id is given.",
            "    \"\"\"",
            "    # Actor doesn't exist.",
            "    with pytest.raises(ValueError):",
            "        actor_id = ActorID(b\"2\" * 16)",
            "",
            "        def get_actor_fn(id):",
            "            if id == actor_id:",
            "                return None",
            "            assert False, \"Not reachable.\"",
            "",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=actor_id,",
            "            task_id=None,",
            "            pid=None,",
            "            get_actor_fn=get_actor_fn,",
            "            timeout=10,",
            "        )",
            "",
            "    # Actor exists, but it is not scheduled yet.",
            "    actor_id = ActorID(b\"2\" * 16)",
            "",
            "    with pytest.raises(ValueError):",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=actor_id,",
            "            task_id=None,",
            "            pid=None,",
            "            get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, None),",
            "            timeout=10,",
            "        )",
            "",
            "    # Actor exists.",
            "    actor_id = ActorID(b\"2\" * 16)",
            "    worker_id = WorkerID(b\"3\" * 28)",
            "    logs_manager.list_logs = AsyncMock()",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [f\"worker-{worker_id.hex()}-123-123.out\"],",
            "        \"worker_err\": [],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=actor_id,",
            "        task_id=None,",
            "        pid=None,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{worker_id.hex()}*out\"",
            "    )",
            "    assert log_file_name == f\"worker-{worker_id.hex()}-123-123.out\"",
            "    assert n == node_id.hex()",
            "",
            "    \"\"\"",
            "    Test task id is given.",
            "    \"\"\"",
            "    task_id = TaskID(b\"2\" * 24)",
            "    logs_client = logs_manager.data_source_client",
            "    logs_client.get_all_task_info = AsyncMock()",
            "    logs_client.get_all_task_info.return_value = GetTaskEventsReply(",
            "        events_by_task=[",
            "            generate_task_event(",
            "                task_id,",
            "                node_id,",
            "                attempt_number=1,",
            "                worker_id=worker_id,",
            "                stdout_file=f\"worker-{worker_id.hex()}-123-123.out\",",
            "            )",
            "        ]",
            "    )",
            "",
            "    # Expect resolved file.",
            "    res = await logs_manager.resolve_filename(task_id=task_id, attempt_number=1)",
            "    filename, n = res.filename, res.node_id",
            "    # Default out file. See generate_task_event() for filename",
            "    assert filename == f\"worker-{worker_id.hex()}-123-123.out\"",
            "    assert n == node_id.hex()",
            "",
            "    # Wrong task attempt",
            "    with pytest.raises(FileNotFoundError):",
            "        await logs_manager.resolve_filename(task_id=task_id, attempt_number=0)",
            "",
            "    # No task found",
            "    logs_client.get_all_task_info.return_value = GetTaskEventsReply(events_by_task=[])",
            "    with pytest.raises(FileNotFoundError):",
            "        await logs_manager.resolve_filename(task_id=TaskID(b\"1\" * 24), attempt_number=1)",
            "",
            "    \"\"\"",
            "    Test pid is given.",
            "    \"\"\"",
            "    # Pid doesn't exist.",
            "    with pytest.raises(FileNotFoundError):",
            "        pid = 456",
            "        logs_manager.list_logs = AsyncMock()",
            "        # Provide the wrong pid.",
            "        logs_manager.list_logs.return_value = {",
            "            \"worker_out\": [\"worker-123-123-123.out\"],",
            "            \"worker_err\": [],",
            "        }",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=None,",
            "            task_id=None,",
            "            pid=pid,",
            "            get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "            timeout=10,",
            "        )",
            "",
            "    # Pid exists.",
            "    pid = 123",
            "    logs_manager.list_logs = AsyncMock()",
            "    # Provide the wrong pid.",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [f\"worker-123-123-{pid}.out\"],",
            "        \"worker_err\": [],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=pid,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{pid}*out\"",
            "    )",
            "    assert log_file_name == f\"worker-123-123-{pid}.out\"",
            "",
            "    \"\"\"",
            "    Test nothing is given.",
            "    \"\"\"",
            "    with pytest.raises(FileNotFoundError):",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=None,",
            "            task_id=None,",
            "            pid=None,",
            "            get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "            timeout=10,",
            "        )",
            "",
            "    \"\"\"",
            "    Test suffix is specified",
            "    \"\"\"",
            "    pid = 123",
            "    logs_manager.list_logs = AsyncMock()",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [f\"worker-123-123-{pid}.out\"],",
            "        \"worker_err\": [],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=pid,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{pid}*out\"",
            "    )",
            "    assert log_file_name == f\"worker-123-123-{pid}.out\"",
            "",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [],",
            "        \"worker_err\": [f\"worker-123-123-{pid}.err\"],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=pid,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "        suffix=\"err\",",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{pid}*err\"",
            "    )",
            "    assert log_file_name == f\"worker-123-123-{pid}.err\"",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_stream_log(logs_manager):",
            "    NUM_LOG_CHUNKS = 10",
            "    logs_client = logs_manager.data_source_client",
            "",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [\"1\", \"2\"]",
            "    logs_client.ip_to_node_id = MagicMock()",
            "    logs_client.stream_log.return_value = generate_logs_stream(NUM_LOG_CHUNKS)",
            "",
            "    # Test file_name, media_type=\"file\", node_id",
            "    options = GetLogOptions(",
            "        timeout=30, media_type=\"file\", lines=10, node_id=\"1\", filename=\"raylet.out\"",
            "    )",
            "",
            "    i = 0",
            "    async for chunk in logs_manager.stream_logs(options):",
            "        assert chunk.decode(\"utf-8\") == generate_logs_stream_chunk(index=i)",
            "        i += 1",
            "    assert i == NUM_LOG_CHUNKS",
            "    logs_client.stream_log.assert_awaited_with(",
            "        node_id=\"1\",",
            "        log_file_name=\"raylet.out\",",
            "        keep_alive=False,",
            "        lines=10,",
            "        interval=None,",
            "        timeout=30,",
            "        start_offset=None,",
            "        end_offset=None,",
            "    )",
            "",
            "    # Test pid, media_type = \"stream\", node_ip",
            "",
            "    logs_client.ip_to_node_id.return_value = \"1\"",
            "    logs_client.list_logs.side_effect = [",
            "        generate_list_logs(",
            "            [\"worker-0-0-10.out\", \"worker-0-0-11.out\", \"worker-0-0-10.err\"]",
            "        ),",
            "    ]",
            "    options = GetLogOptions(",
            "        timeout=30, media_type=\"stream\", lines=10, interval=0.5, node_id=\"1\", pid=\"10\"",
            "    )",
            "",
            "    logs_client.stream_log.return_value = generate_logs_stream(NUM_LOG_CHUNKS)",
            "    i = 0",
            "    async for chunk in logs_manager.stream_logs(options):",
            "        assert chunk.decode(\"utf-8\") == generate_logs_stream_chunk(index=i)",
            "        i += 1",
            "    assert i == NUM_LOG_CHUNKS",
            "    logs_client.stream_log.assert_awaited_with(",
            "        node_id=\"1\",",
            "        log_file_name=\"worker-0-0-10.out\",",
            "        keep_alive=True,",
            "        lines=10,",
            "        interval=0.5,",
            "        timeout=None,",
            "        start_offset=None,",
            "        end_offset=None,",
            "    )",
            "",
            "    # Currently cannot test actor_id with AsyncMock.",
            "    # It will be tested by the integration test.",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_keepalive_no_timeout(logs_manager):",
            "    \"\"\"Test when --follow is specified, there's no timeout.",
            "",
            "    Related: https://github.com/ray-project/ray/issues/25721",
            "    \"\"\"",
            "    NUM_LOG_CHUNKS = 10",
            "    logs_client = logs_manager.data_source_client",
            "",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [\"1\", \"2\"]",
            "    logs_client.ip_to_node_id = MagicMock()",
            "    logs_client.stream_log.return_value = generate_logs_stream(NUM_LOG_CHUNKS)",
            "",
            "    # Test file_name, media_type=\"file\", node_id",
            "    options = GetLogOptions(",
            "        timeout=30, media_type=\"stream\", lines=10, node_id=\"1\", filename=\"raylet.out\"",
            "    )",
            "",
            "    async for chunk in logs_manager.stream_logs(options):",
            "        pass",
            "",
            "    # Make sure timeout == None when media_type == stream. This is to avoid",
            "    # closing the connection due to DEADLINE_EXCEEDED when --follow is specified.",
            "    logs_client.stream_log.assert_awaited_with(",
            "        node_id=\"1\",",
            "        log_file_name=\"raylet.out\",",
            "        keep_alive=True,",
            "        lines=10,",
            "        interval=None,",
            "        timeout=None,",
            "        start_offset=None,",
            "        end_offset=None,",
            "    )",
            "",
            "",
            "# Integration tests",
            "",
            "",
            "def test_logs_list(ray_start_with_dashboard):",
            "    assert (",
            "        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])",
            "        is True",
            "    )",
            "    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    def verify():",
            "        response = requests.get(webui_url + f\"/api/v0/logs?node_id={node_id}\")",
            "        response.raise_for_status()",
            "        result = json.loads(response.text)",
            "        assert result[\"result\"]",
            "        logs = result[\"data\"][\"result\"]",
            "",
            "        # Test worker logs",
            "        outs = logs[\"worker_out\"]",
            "        errs = logs[\"worker_err\"]",
            "        core_worker_logs = logs[\"core_worker\"]",
            "",
            "        assert len(outs) == len(errs) == len(core_worker_logs)",
            "        assert len(outs) > 0",
            "",
            "        # Test gcs / raylet / dashboard",
            "        for file in [\"gcs_server.out\", \"gcs_server.err\"]:",
            "            assert file in logs[\"gcs_server\"]",
            "        for file in [\"raylet.out\", \"raylet.err\"]:",
            "            assert file in logs[\"raylet\"]",
            "        for file in [\"dashboard.log\"]:",
            "            assert file in logs[\"dashboard\"]",
            "        for file in [\"dashboard_agent.log\"]:",
            "            assert file in logs[\"agent\"]",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    def verify_filter():",
            "        # Test that logs/list can be filtered",
            "        response = requests.get(",
            "            webui_url + f\"/api/v0/logs?node_id={node_id}&glob=*gcs*\"",
            "        )",
            "        response.raise_for_status()",
            "        result = json.loads(response.text)",
            "        assert result[\"result\"]",
            "        logs = result[\"data\"][\"result\"]",
            "        assert \"gcs_server\" in logs",
            "        assert \"internal\" in logs",
            "        assert len(logs) == 2",
            "        assert \"gcs_server.out\" in logs[\"gcs_server\"]",
            "        assert \"gcs_server.err\" in logs[\"gcs_server\"]",
            "        assert \"debug_state_gcs.txt\" in logs[\"internal\"]",
            "        return True",
            "",
            "    wait_for_condition(verify_filter)",
            "",
            "    def verify_worker_logs():",
            "        response = requests.get(",
            "            webui_url + f\"/api/v0/logs?node_id={node_id}&glob=*worker*\"",
            "        )",
            "        response.raise_for_status()",
            "        result = json.loads(response.text)",
            "        assert result[\"result\"]",
            "        logs = result[\"data\"][\"result\"]",
            "        worker_log_categories = [",
            "            \"core_worker\",",
            "            \"worker_out\",",
            "            \"worker_err\",",
            "        ]",
            "        assert all([cat in logs for cat in worker_log_categories])",
            "        num_workers = len(",
            "            list(filter(lambda w: w[\"worker_type\"] == \"WORKER\", list_workers()))",
            "        )",
            "        assert (",
            "            len(logs[\"worker_out\"])",
            "            == len(logs[\"worker_err\"])",
            "            == len(logs[\"worker_out\"])",
            "        )",
            "        assert num_workers == len(logs[\"worker_out\"])",
            "        return True",
            "",
            "    wait_for_condition(verify_worker_logs)",
            "",
            "",
            "@pytest.mark.skipif(sys.platform == \"win32\", reason=\"File path incorrect on Windows.\")",
            "def test_logs_stream_and_tail(ray_start_with_dashboard):",
            "    assert (",
            "        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])",
            "        is True",
            "    )",
            "    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    def verify_basic():",
            "        stream_response = requests.get(",
            "            webui_url",
            "            + f\"/api/v0/logs/file?node_id={node_id}&filename=gcs_server.out&lines=5\",",
            "            stream=True,",
            "        )",
            "        if stream_response.status_code != 200:",
            "            raise ValueError(stream_response.content.decode(\"utf-8\"))",
            "        lines = []",
            "        for line in stream_response.iter_lines():",
            "            lines.append(line.decode(\"utf-8\"))",
            "        assert len(lines) == 5 or len(lines) == 6",
            "        return True",
            "",
            "    wait_for_condition(verify_basic)",
            "",
            "    @ray.remote",
            "    class Actor:",
            "        def write_log(self, strings):",
            "            for s in strings:",
            "                print(s)",
            "",
            "        def getpid(self):",
            "            return os.getpid()",
            "",
            "    test_log_text = \"test_log_text_\u65e5\u5fd7_{}\"",
            "    actor = Actor.remote()",
            "    ray.get(actor.write_log.remote([test_log_text.format(\"XXXXXX\")]))",
            "",
            "    # Test stream and fetching by actor id",
            "    stream_response = requests.get(",
            "        webui_url",
            "        + \"/api/v0/logs/stream?&lines=-1\"",
            "        + f\"&actor_id={actor._ray_actor_id.hex()}\",",
            "        stream=True,",
            "    )",
            "    if stream_response.status_code != 200:",
            "        raise ValueError(stream_response.content.decode(\"utf-8\"))",
            "    stream_iterator = stream_response.iter_content(chunk_size=None)",
            "    actual_output = next(stream_iterator).decode(\"utf-8\")",
            "    assert \"actor_name:Actor\\n\" in actual_output",
            "    assert test_log_text.format(\"XXXXXX\") in actual_output",
            "",
            "    streamed_string = \"\"",
            "    for i in range(5):",
            "        strings = []",
            "        for j in range(3):",
            "            strings.append(test_log_text.format(f\"{3*i + j:06d}\"))",
            "",
            "        ray.get(actor.write_log.remote(strings))",
            "",
            "        string = \"\"",
            "        for s in strings:",
            "            string += s + \"\\n\"",
            "        streamed_string += string",
            "        # NOTE: Prefix 1 indicates the stream has succeeded.",
            "        assert string in next(stream_iterator).decode(\"utf-8\")",
            "    del stream_response",
            "",
            "    # Test tailing log by actor id",
            "    LINES = 150",
            "    file_response = requests.get(",
            "        webui_url",
            "        + f\"/api/v0/logs/file?&lines={LINES}\"",
            "        + \"&actor_id=\"",
            "        + actor._ray_actor_id.hex(),",
            "    ).content.decode(\"utf-8\")",
            "    # NOTE: Prefix 1 indicates the stream has succeeded.",
            "    for line in streamed_string.split(\"\\n\")[-(LINES + 1) :]:",
            "        assert line in file_response",
            "",
            "    # Test query by pid & node_ip instead of actor id.",
            "    node_ip = list(ray.nodes())[0][\"NodeManagerAddress\"]",
            "    pid = ray.get(actor.getpid.remote())",
            "    file_response = requests.get(",
            "        webui_url",
            "        + f\"/api/v0/logs/file?node_ip={node_ip}&lines={LINES}\"",
            "        + f\"&pid={pid}\",",
            "    ).content.decode(\"utf-8\")",
            "    # NOTE: Prefix 1 indicates the stream has succeeded.",
            "    for line in streamed_string.split(\"\\n\")[-(LINES + 1) :]:",
            "        assert line in file_response",
            "",
            "",
            "def test_log_list(ray_start_cluster):",
            "    cluster = ray_start_cluster",
            "    num_nodes = 5",
            "    for _ in range(num_nodes):",
            "        cluster.add_node(num_cpus=0)",
            "    ray.init(address=cluster.address)",
            "",
            "    def verify():",
            "        for node in list_nodes():",
            "            # When glob filter is not provided, it should provide all logs",
            "            logs = list_logs(node_id=node[\"node_id\"])",
            "            assert \"raylet\" in logs",
            "            assert \"gcs_server\" in logs",
            "            assert \"dashboard\" in logs",
            "            assert \"agent\" in logs",
            "            assert \"internal\" in logs",
            "            assert \"driver\" in logs",
            "            assert \"autoscaler\" in logs",
            "",
            "            # Test glob works.",
            "            logs = list_logs(node_id=node[\"node_id\"], glob_filter=\"raylet*\")",
            "            assert len(logs) == 1",
            "            return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    node_id = \"XXXX\"",
            "    with pytest.raises(requests.HTTPError) as e:",
            "        list_logs(node_id=node_id)",
            "",
            "    e.match(f\"Given node id {node_id} is not available\")",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.platform == \"win32\", reason=\"Job submission is failing on windows.\"",
            ")",
            "def test_log_job(ray_start_with_dashboard):",
            "    assert wait_until_server_available(ray_start_with_dashboard[\"webui_url\"]) is True",
            "    webui_url = ray_start_with_dashboard[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    # Submit a job",
            "    from ray.job_submission import JobSubmissionClient",
            "",
            "    JOB_LOG = \"test-job-log\"",
            "    client = JobSubmissionClient(webui_url)",
            "    entrypoint = f\"python -c \\\"print('{JOB_LOG}')\\\"\"",
            "    job_id = client.submit_job(entrypoint=entrypoint)",
            "",
            "    def job_done():",
            "        jobs = list_jobs(filters=[(\"submission_id\", \"=\", job_id)])",
            "        assert len(jobs) == 1",
            "        assert jobs[0].status == \"SUCCEEDED\"",
            "        return True",
            "",
            "    wait_for_condition(job_done)",
            "",
            "    def verify():",
            "        logs = \"\".join(get_log(submission_id=job_id, node_id=node_id))",
            "        assert JOB_LOG + \"\\n\" == logs",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "def test_log_get_subdir(ray_start_with_dashboard):",
            "    assert (",
            "        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])",
            "        is True",
            "    )",
            "    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()",
            "    subdir = \"test_subdir\"",
            "    file = \"test_#file.log\"",
            "    path = Path(log_dir) / subdir / file",
            "    path.parent.mkdir(parents=True, exist_ok=True)",
            "    path.write_text(\"test log\")",
            "",
            "    # HTTP endpoint",
            "    def verify():",
            "        # Direct logs stream",
            "        response = requests.get(",
            "            webui_url",
            "            + f\"/api/v0/logs/file?node_id={node_id}\"",
            "            + f\"&filename={urllib.parse.quote('test_subdir/test_#file.log')}\"",
            "        )",
            "        assert response.status_code == 200, response.reason",
            "        assert \"test log\" in response.text",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # get log SDK",
            "    def verify():",
            "        logs = \"\".join(get_log(node_id=node_id, filename=\"test_subdir/test_#file.log\"))",
            "        assert \"test log\" in logs",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "def test_log_get(ray_start_cluster):",
            "    cluster = ray_start_cluster",
            "    cluster.add_node(num_cpus=0)",
            "    ray.init(address=cluster.address)",
            "    head_node = list_nodes()[0]",
            "    cluster.add_node(num_cpus=1)",
            "",
            "    @ray.remote(num_cpus=1)",
            "    class Actor:",
            "        def print(self, i):",
            "            for _ in range(i):",
            "                print(\"1\")",
            "",
            "        def getpid(self):",
            "            import os",
            "",
            "            return os.getpid()",
            "",
            "    \"\"\"",
            "    Test filename match",
            "    \"\"\"",
            "",
            "    def verify():",
            "        # By default, node id should be configured to the head node.",
            "        for log in get_log(",
            "            node_id=head_node[\"node_id\"], filename=\"raylet.out\", tail=10",
            "        ):",
            "            # + 1 since the last line is just empty.",
            "            assert len(log.split(\"\\n\")) == 11",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    \"\"\"",
            "    Test worker pid / IP match",
            "    \"\"\"",
            "    a = Actor.remote()",
            "    pid = ray.get(a.getpid.remote())",
            "    ray.get(a.print.remote(20))",
            "",
            "    def verify():",
            "        # By default, node id should be configured to the head node.",
            "        for log in get_log(node_ip=head_node[\"node_ip\"], pid=pid, tail=10):",
            "            # + 1 since the last line is just empty.",
            "            assert len(log.split(\"\\n\")) == 11",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    \"\"\"",
            "    Test actor logs.",
            "    \"\"\"",
            "    actor_id = a._actor_id.hex()",
            "",
            "    def verify():",
            "        # By default, node id should be configured to the head node.",
            "        for log in get_log(actor_id=actor_id, tail=10):",
            "            # + 1 since the last line is just empty.",
            "            assert len(log.split(\"\\n\")) == 11",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    del a",
            "    \"\"\"",
            "    Test log suffix selection for worker/actor",
            "    \"\"\"",
            "    ACTOR_LOG_LINE = \"{dest}:test actor log\"",
            "",
            "    @ray.remote",
            "    class Actor:",
            "        def __init__(self):",
            "            import sys",
            "",
            "            print(ACTOR_LOG_LINE.format(dest=\"out\"))",
            "            print(ACTOR_LOG_LINE.format(dest=\"err\"), file=sys.stderr)",
            "",
            "    actor = Actor.remote()",
            "    actor_id = actor._actor_id.hex()",
            "",
            "    WORKER_LOG_LINE = \"{dest}:test worker log\"",
            "",
            "    @ray.remote",
            "    def worker_func():",
            "        import os",
            "        import sys",
            "",
            "        print(WORKER_LOG_LINE.format(dest=\"out\"))",
            "        print(WORKER_LOG_LINE.format(dest=\"err\"), file=sys.stderr)",
            "        return os.getpid()",
            "",
            "    pid = ray.get(worker_func.remote())",
            "",
            "    def verify():",
            "        # Test actors",
            "        lines = get_log(actor_id=actor_id, suffix=\"err\")",
            "        assert ACTOR_LOG_LINE.format(dest=\"err\") in \"\".join(lines)",
            "",
            "        lines = get_log(actor_id=actor_id, suffix=\"out\")",
            "        assert ACTOR_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        # Default to out",
            "        lines = get_log(actor_id=actor_id)",
            "        assert ACTOR_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        # Test workers",
            "        lines = get_log(node_ip=head_node[\"node_ip\"], pid=pid, suffix=\"err\")",
            "        assert WORKER_LOG_LINE.format(dest=\"err\") in \"\".join(lines)",
            "",
            "        lines = get_log(node_ip=head_node[\"node_ip\"], pid=pid, suffix=\"out\")",
            "        assert WORKER_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        lines = get_log(node_ip=head_node[\"node_ip\"], pid=pid)",
            "        assert WORKER_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    def verify():",
            "        runner = CliRunner()",
            "        result = runner.invoke(",
            "            logs_state_cli_group,",
            "            [\"actor\", \"--id\", actor_id],",
            "        )",
            "        assert result.exit_code == 0, result.exception",
            "        assert ACTOR_LOG_LINE.format(dest=\"out\") in result.output",
            "",
            "        result = runner.invoke(",
            "            logs_state_cli_group,",
            "            [",
            "                \"actor\",",
            "                \"--id\",",
            "                actor_id,",
            "                \"--err\",",
            "            ],",
            "        )",
            "        assert result.exit_code == 0, result.exception",
            "        assert ACTOR_LOG_LINE.format(dest=\"err\") in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "    ##############################",
            "    # Test binary files and encodings.",
            "    ##############################",
            "    # Write a binary file to ray log directory.",
            "    log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()",
            "    file = \"test.bin\"",
            "    binary_file = os.path.join(log_dir, file)",
            "    with open(binary_file, \"wb\") as f:",
            "        data = bytearray(i for i in range(256))",
            "        f.write(data)",
            "",
            "    # Get the log",
            "    def verify():",
            "        for read in get_log(node_ip=head_node[\"node_ip\"], filename=file, encoding=None):",
            "            assert read == data",
            "",
            "        # Default utf-8",
            "        for read in get_log(",
            "            node_ip=head_node[\"node_ip\"], filename=file, errors=\"replace\"",
            "        ):",
            "            assert read == data.decode(encoding=\"utf-8\", errors=\"replace\")",
            "",
            "        for read in get_log(",
            "            node_ip=head_node[\"node_ip\"],",
            "            filename=file,",
            "            encoding=\"iso-8859-1\",",
            "            errors=\"replace\",",
            "        ):",
            "            assert read == data.decode(encoding=\"iso-8859-1\", errors=\"replace\")",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test running task logs",
            "    @ray.remote",
            "    def sleep_task(out_msg):",
            "        print(out_msg, end=\"\", file=sys.stdout)",
            "        import time",
            "",
            "        time.sleep(10)",
            "",
            "    expected_out = \"This is a test log from stdout\\n\"",
            "    task = sleep_task.remote(expected_out)",
            "",
            "    def verify():",
            "        lines = get_log(task_id=task.task_id().hex())",
            "        assert expected_out == \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.platform == \"win32\", reason=\"Windows has logging race from tasks.\"",
            ")",
            "def test_log_task(shutdown_only):",
            "    from ray.runtime_env import RuntimeEnv",
            "",
            "    ray.init()",
            "",
            "    # Test get log by multiple task id",
            "    @ray.remote",
            "    def task_log():",
            "        out_msg = \"This is a test log from stdout\\n\"",
            "        print(out_msg, end=\"\", file=sys.stdout)",
            "        err_msg = \"THIS IS A TEST LOG FROM STDERR\\n\"",
            "        print(err_msg, end=\"\", file=sys.stderr)",
            "",
            "        return out_msg, err_msg",
            "",
            "    # Run some other tasks before and after to make sure task",
            "    # log only outputs the task's log.",
            "    ray.get(task_log.remote())",
            "    task = task_log.remote()",
            "    expected_out, expected_err = ray.get(task)",
            "    ray.get(task_log.remote())",
            "",
            "    def verify():",
            "        lines = get_log(task_id=task.task_id().hex())",
            "        assert expected_out in \"\".join(lines)",
            "",
            "        # Test suffix",
            "        lines = get_log(task_id=task.task_id().hex(), suffix=\"err\")",
            "        assert expected_err in \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    enabled_actor_task_log_runtime_env = RuntimeEnv(",
            "        env_vars={\"RAY_ENABLE_RECORD_ACTOR_TASK_LOGGING\": \"1\"}",
            "    )",
            "",
            "    # Test actor task logs",
            "    @ray.remote",
            "    class Actor:",
            "        def print_log(self, out_msg):",
            "            for _ in range(3):",
            "                print(out_msg, end=\"\", file=sys.stdout)",
            "                print(out_msg, end=\"\", file=sys.stderr)",
            "",
            "    a = Actor.options(runtime_env=enabled_actor_task_log_runtime_env).remote()",
            "    out_msg = \"This is a test log\\n\"",
            "    t = a.print_log.remote(out_msg)",
            "    ray.get(t)",
            "",
            "    def verify():",
            "        lines = get_log(task_id=t.task_id().hex())",
            "        assert out_msg * 3 == \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test actor task logs with interleaving logs should raise",
            "    # errors to ask users to user actor log instead.",
            "    @ray.remote",
            "    class AsyncActor:",
            "        async def print_log(self, out_msg):",
            "            for _ in range(3):",
            "                print(out_msg, end=\"\", file=sys.stdout)",
            "                await asyncio.sleep(1)",
            "",
            "    actor = AsyncActor.options(",
            "        max_concurrency=2, runtime_env=enabled_actor_task_log_runtime_env",
            "    ).remote()",
            "    out_msg = \"[{name}]: This is a test log from stdout\\n\"",
            "    task_a = actor.print_log.remote(out_msg.format(name=\"a\"))",
            "    task_b = actor.print_log.remote(out_msg.format(name=\"b\"))",
            "    ray.get([task_a, task_b])",
            "",
            "    def verify():",
            "        lines = get_log(task_id=task_a.task_id().hex())",
            "        assert \"\".join(lines).count(out_msg.format(name=\"a\")) == 3",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    def verify_actor_task_error(task_id, actor_id):",
            "        with pytest.raises(RayStateApiException) as e:",
            "            for log in get_log(task_id=task_id):",
            "                pass",
            "",
            "        assert \"For actor task, please query actor log\" in str(e.value), str(e.value)",
            "        assert f\"ray logs actor --id {actor_id}\" in str(e.value), str(e.value)",
            "",
            "        return True",
            "",
            "    # Getting task logs from actor with actor task log not enabled should raise errors.",
            "    a = Actor.remote()",
            "    t = a.print_log.remote(out_msg)",
            "    ray.get(t)",
            "    wait_for_condition(",
            "        verify_actor_task_error, task_id=t.task_id().hex(), actor_id=a._actor_id.hex()",
            "    )",
            "",
            "    a = AsyncActor.options(max_concurrency=2).remote()",
            "    t = a.print_log.remote(out_msg)",
            "    ray.get(t)",
            "    wait_for_condition(",
            "        verify_actor_task_error, task_id=t.task_id().hex(), actor_id=a._actor_id.hex()",
            "    )",
            "",
            "    # Test task logs tail with lines.",
            "    expected_out = [f\"task-{i}\\n\" for i in range(5)]",
            "",
            "    @ray.remote",
            "    def f():",
            "        print(\"\".join(expected_out), end=\"\", file=sys.stdout)",
            "",
            "    t = f.remote()",
            "    ray.get(t)",
            "",
            "    def verify():",
            "        lines = get_log(task_id=t.task_id().hex(), tail=2)",
            "        actual_output = \"\".join(lines)",
            "        assert actual_output == \"\".join(expected_out[-2:])",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "def test_log_cli(shutdown_only):",
            "    ray.init(num_cpus=1)",
            "    runner = CliRunner()",
            "",
            "    # Test the head node is chosen by default.",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"cluster\"])",
            "        assert result.exit_code == 0, result.exception",
            "        assert \"raylet.out\" in result.output",
            "        assert \"raylet.err\" in result.output",
            "        assert \"gcs_server.out\" in result.output",
            "        assert \"gcs_server.err\" in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test when there's only 1 match, it prints logs.",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"cluster\", \"raylet.out\"])",
            "        assert result.exit_code == 0",
            "        assert \"raylet.out\" not in result.output",
            "        assert \"raylet.err\" not in result.output",
            "        assert \"gcs_server.out\" not in result.output",
            "        assert \"gcs_server.err\" not in result.output",
            "        # Make sure it prints the log message.",
            "        assert \"NodeManager server started\" in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test when there's more than 1 match, it prints a list of logs.",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"cluster\", \"raylet.*\"])",
            "        assert result.exit_code == 0, result.exception",
            "        assert \"raylet.out\" in result.output",
            "        assert \"raylet.err\" in result.output",
            "        assert \"gcs_server.out\" not in result.output",
            "        assert \"gcs_server.err\" not in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test actor log: `ray logs actor`",
            "    ACTOR_LOG_LINE = \"test actor log\"",
            "",
            "    @ray.remote",
            "    class Actor:",
            "        def __init__(self):",
            "            print(ACTOR_LOG_LINE)",
            "",
            "    actor = Actor.remote()",
            "    actor_id = actor._actor_id.hex()",
            "",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"actor\", \"--id\", actor_id])",
            "        assert result.exit_code == 0, result.exception",
            "        assert ACTOR_LOG_LINE in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test worker log: `ray logs worker`",
            "    WORKER_LOG_LINE = \"test worker log\"",
            "",
            "    @ray.remote",
            "    def worker_func():",
            "        import os",
            "",
            "        print(WORKER_LOG_LINE)",
            "        return os.getpid()",
            "",
            "    pid = ray.get(worker_func.remote())",
            "",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"worker\", \"--pid\", pid])",
            "        assert result.exit_code == 0, result.exception",
            "        assert WORKER_LOG_LINE in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test `ray logs raylet.*` forwarding to `ray logs cluster raylet.*`",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"raylet.*\"])",
            "        assert result.exit_code == 0, result.exception",
            "        assert \"raylet.out\" in result.output",
            "        assert \"raylet.err\" in result.output",
            "        assert \"gcs_server.out\" not in result.output",
            "        assert \"gcs_server.err\" not in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test binary binary files and encodings.",
            "    log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()",
            "    file = \"test.bin\"",
            "    binary_file = os.path.join(log_dir, file)",
            "    with open(binary_file, \"wb\") as f:",
            "        data = bytearray(i for i in range(256))",
            "        f.write(data)",
            "",
            "    def verify():",
            "        # Tailing with lines is not supported for binary files, thus the `tail=-1`",
            "        result = runner.invoke(",
            "            logs_state_cli_group,",
            "            [",
            "                file,",
            "                \"--encoding\",",
            "                \"iso-8859-1\",",
            "                \"--encoding-errors\",",
            "                \"replace\",",
            "                \"--tail\",",
            "                \"-1\",",
            "            ],",
            "        )",
            "        assert result.exit_code == 0, result.exception",
            "        assert result.output == data.decode(encoding=\"iso-8859-1\", errors=\"replace\")",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    import sys",
            "",
            "    if os.environ.get(\"PARALLEL_CI\"):",
            "        sys.exit(pytest.main([\"-n\", \"auto\", \"--boxed\", \"-vs\", __file__]))",
            "    else:",
            "        sys.exit(pytest.main([\"-sv\", __file__]))"
        ],
        "afterPatchFile": [
            "import json",
            "import os",
            "import sys",
            "import asyncio",
            "from typing import List",
            "import urllib",
            "from unittest.mock import MagicMock",
            "",
            "import pytest",
            "from ray.util.state.state_cli import logs_state_cli_group",
            "from ray.util.state import list_jobs",
            "import requests",
            "from click.testing import CliRunner",
            "import grpc",
            "",
            "from pathlib import Path",
            "",
            "import ray",
            "from ray._private.test_utils import (",
            "    format_web_url,",
            "    wait_for_condition,",
            "    wait_until_server_available,",
            ")",
            "",
            "from ray._private.ray_constants import (",
            "    LOG_PREFIX_TASK_ATTEMPT_START,",
            "    LOG_PREFIX_TASK_ATTEMPT_END,",
            ")",
            "from ray._raylet import ActorID, NodeID, TaskID, WorkerID",
            "from ray.core.generated.common_pb2 import Address",
            "from ray.core.generated.gcs_service_pb2 import GetTaskEventsReply",
            "from ray.core.generated.reporter_pb2 import ListLogsReply, StreamLogReply",
            "from ray.core.generated.gcs_pb2 import (",
            "    ActorTableData,",
            "    TaskEvents,",
            "    TaskStateUpdate,",
            "    TaskLogInfo,",
            ")",
            "from ray.dashboard.modules.actor.actor_head import actor_table_data_to_dict",
            "from ray.dashboard.modules.log.log_agent import (",
            "    find_offset_of_content_in_file,",
            "    find_end_offset_file,",
            "    find_end_offset_next_n_lines_from_offset,",
            "    find_start_offset_last_n_lines_from_offset,",
            "    LogAgentV1Grpc,",
            ")",
            "from ray.dashboard.modules.log.log_agent import _stream_log_in_chunk",
            "from ray.dashboard.modules.log.log_manager import LogsManager",
            "from ray.dashboard.tests.conftest import *  # noqa",
            "from ray.util.state import get_log, list_logs, list_nodes, list_workers",
            "from ray.util.state.common import GetLogOptions",
            "from ray.util.state.exception import DataSourceUnavailable, RayStateApiException",
            "from ray.util.state.state_manager import StateDataSourceClient",
            "",
            "if sys.version_info >= (3, 8, 0):",
            "    from unittest.mock import AsyncMock",
            "else:",
            "    from asyncmock import AsyncMock",
            "",
            "",
            "ASYNCMOCK_MIN_PYTHON_VER = (3, 8)",
            "",
            "",
            "def generate_task_event(",
            "    task_id,",
            "    node_id,",
            "    attempt_number,",
            "    worker_id,",
            "    stdout_file=None,",
            "    stderr_file=None,",
            "    stdout_start=None,",
            "    stderr_start=None,",
            "    stdout_end=None,",
            "    stderr_end=None,",
            "):",
            "    task_event = TaskEvents(",
            "        task_id=task_id.binary(),",
            "        attempt_number=attempt_number,",
            "        job_id=b\"\",",
            "        state_updates=TaskStateUpdate(",
            "            node_id=node_id.binary(),",
            "            worker_id=worker_id.binary(),",
            "            task_log_info=TaskLogInfo(",
            "                stdout_file=stdout_file,",
            "                stderr_file=stderr_file,",
            "                stdout_start=stdout_start,",
            "                stderr_start=stderr_start,",
            "                stdout_end=stdout_end,",
            "                stderr_end=stderr_end,",
            "            ),",
            "        ),",
            "    )",
            "",
            "    return task_event",
            "",
            "",
            "def generate_actor_data(id, node_id, worker_id):",
            "    if worker_id:",
            "        worker_id = worker_id.binary()",
            "    message = ActorTableData(",
            "        actor_id=id.binary(),",
            "        state=ActorTableData.ActorState.ALIVE,",
            "        name=\"abc\",",
            "        pid=1234,",
            "        class_name=\"class\",",
            "        address=Address(",
            "            raylet_id=node_id.binary(),",
            "            ip_address=\"127.0.0.1\",",
            "            port=1234,",
            "            worker_id=worker_id,",
            "        ),",
            "    )",
            "    return actor_table_data_to_dict(message)",
            "",
            "",
            "# Unit Tests (Log Agent)",
            "def _read_file(fp, start, end):",
            "    \"\"\"Help func to read a file with offsets\"\"\"",
            "    fp.seek(start, 0)",
            "    if end == -1:",
            "        return fp.read()",
            "    return fp.read(end - start)",
            "",
            "",
            "async def _stream_log(context, fp, start, end):",
            "    \"\"\"Help func to stream a log with offsets\"\"\"",
            "    result = bytearray()",
            "    async for chunk_res in _stream_log_in_chunk(",
            "        context=context,",
            "        file=fp,",
            "        start_offset=start,",
            "        end_offset=end,",
            "        keep_alive_interval_sec=-1,",
            "    ):",
            "        result += chunk_res.data",
            "    return result",
            "",
            "",
            "TEST_LINE_TEMPLATE = \"{}-test-line\"",
            "",
            "",
            "def _write_lines_and_get_offset_at_index(",
            "    f, num_lines, start_offset=0, trailing_new_line=True",
            "):",
            "    \"\"\"",
            "    Write multiple lines into a file, and record offsets",
            "",
            "    Args:",
            "        f: a binary file object that's writable",
            "        num_lines: Number of lines to write",
            "        start_offset: The offset to start writing",
            "        trailing_new_line: True if a '\\n' is added at the end of the",
            "            lines.",
            "",
            "    Return:",
            "        offsets: A list of offsets of the lines.",
            "        offset_end: The offset of the end of file.",
            "    \"\"\"",
            "    f.seek(start_offset, 0)",
            "",
            "    offsets = []",
            "    for i in range(num_lines):",
            "        offsets.append(f.tell())",
            "        if i == num_lines - 1 and not trailing_new_line:",
            "            # Last line no newline",
            "            line = TEST_LINE_TEMPLATE.format(i)",
            "        else:",
            "            line = TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "        f.write(line.encode(\"utf-8\"))",
            "",
            "    f.flush()",
            "    f.seek(0, 2)",
            "    offset_end = f.tell()",
            "",
            "    return offsets, offset_end",
            "",
            "",
            "@pytest.mark.parametrize(\"new_line\", [True, False])",
            "@pytest.mark.parametrize(\"block_size\", [4, 16, 256])",
            "def test_find_start_offset_last_n_lines_from_offset(new_line, temp_file, block_size):",
            "    file = temp_file",
            "    o, end_file = _write_lines_and_get_offset_at_index(",
            "        file, num_lines=50, start_offset=0, trailing_new_line=new_line",
            "    )",
            "    # Test the function with different offsets and number of lines to find",
            "    assert find_start_offset_last_n_lines_from_offset(file, o[3], 1, block_size) == o[2]",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[10], 10, block_size) == o[0]",
            "    )",
            "",
            "    # Test end of file last 1 line",
            "    assert find_start_offset_last_n_lines_from_offset(file, -1, 1, block_size) == o[-1]",
            "",
            "    # Test end of file no line",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, -1, 0, block_size) == end_file",
            "    )",
            "",
            "    # Test no line from middle of file",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[30], 0, block_size) == o[30]",
            "    )",
            "",
            "    # Test more lines than file",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[30], 100, block_size) == o[0]",
            "    )",
            "",
            "    # Test offsets in the middle of a line",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[2] + 1, 1, block_size)",
            "        == o[2]",
            "    )",
            "    assert (",
            "        find_start_offset_last_n_lines_from_offset(file, o[2] - 1, 1, block_size)",
            "        == o[1]",
            "    )",
            "",
            "",
            "def test_find_end_offset_next_n_lines_from_offset(temp_file):",
            "    file = temp_file",
            "    o, end_file = _write_lines_and_get_offset_at_index(",
            "        file, num_lines=10, start_offset=0",
            "    )",
            "    # Test the function with different offsets and number of lines to find",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[3], 1) == o[4]",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[3], 2) == o[5]",
            "    assert find_end_offset_next_n_lines_from_offset(file, 0, 1) == o[1]",
            "",
            "    # Test end of file",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[3], 999) == end_file",
            "",
            "    # Test offset diff",
            "    assert find_end_offset_next_n_lines_from_offset(file, 1, 1) == o[1]",
            "    assert find_end_offset_next_n_lines_from_offset(file, o[1] - 1, 1) == o[1]",
            "",
            "",
            "def test_find_offset_of_content_in_file(temp_file):",
            "    file = temp_file",
            "    o, end_file = _write_lines_and_get_offset_at_index(file, num_lines=10)",
            "",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(0).encode(\"utf-8\")",
            "        )",
            "        == o[0]",
            "    )",
            "",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(3).encode(\"utf-8\"), o[1] + 1",
            "        )",
            "        == o[3]",
            "    )",
            "",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(4).encode(\"utf-8\"), o[1] - 1",
            "        )",
            "        == o[4]",
            "    )",
            "",
            "    # Not found",
            "    assert (",
            "        find_offset_of_content_in_file(",
            "            file, TEST_LINE_TEMPLATE.format(1000).encode(\"utf-8\"), o[1] - 1",
            "        )",
            "        == -1",
            "    )",
            "",
            "",
            "@pytest.mark.asyncio",
            "@pytest.mark.parametrize(\"random_ascii_file\", [1 << 20], indirect=True)",
            "@pytest.mark.parametrize(",
            "    \"start_offset,end_offset\",",
            "    [",
            "        (0, 1 << 20),",
            "        (1 << 20, 1 << 20),",
            "        (0, 0),",
            "        (0, 1),",
            "        (1 << 16, 1 << 20),",
            "        (1024, 2042),",
            "    ],",
            ")",
            "async def test_stream_log_in_chunk(random_ascii_file, start_offset, end_offset):",
            "    \"\"\"Test streaming of a file from different offsets\"\"\"",
            "    test_file = random_ascii_file",
            "    context = MagicMock(grpc.aio.ServicerContext)",
            "    context.done.return_value = False",
            "",
            "    expected_file_content = _read_file(test_file, start_offset, end_offset)",
            "    actual_log_content = await _stream_log(context, test_file, start_offset, end_offset)",
            "",
            "    assert (",
            "        expected_file_content == actual_log_content",
            "    ), \"Non-matching content from log streamed\"",
            "",
            "",
            "@pytest.mark.asyncio",
            "@pytest.mark.parametrize(",
            "    \"lines_to_tail,total_lines\",",
            "    [(0, 100), (100, 100), (10, 100), (1, 100), (99, 100)],",
            ")",
            "@pytest.mark.parametrize(\"trailing_new_line\", [True, False])",
            "async def test_log_tails(lines_to_tail, total_lines, trailing_new_line, temp_file):",
            "    \"\"\"Test tailing a file works\"\"\"",
            "    _write_lines_and_get_offset_at_index(",
            "        temp_file,",
            "        total_lines,",
            "        trailing_new_line=trailing_new_line,",
            "    )",
            "    test_file = temp_file",
            "    context = MagicMock(grpc.aio.ServicerContext)",
            "    context.done.return_value = False",
            "    start_offset = find_start_offset_last_n_lines_from_offset(",
            "        test_file, offset=-1, n=lines_to_tail",
            "    )",
            "",
            "    actual_data = await _stream_log(context, test_file, start_offset, -1)",
            "    expected_data = _read_file(test_file, start_offset, -1)",
            "",
            "    assert actual_data == expected_data, \"Non-matching data from stream log\"",
            "",
            "    all_lines = actual_data.decode(\"utf-8\")",
            "    assert all_lines.count(\"\\n\") == (",
            "        lines_to_tail if trailing_new_line or lines_to_tail == 0 else lines_to_tail - 1",
            "    ), \"Non-matching number of lines tailed\"",
            "",
            "",
            "@pytest.mark.asyncio",
            "@pytest.mark.parametrize(",
            "    \"lines_to_tail,total_lines\",",
            "    [(0, 5), (5, 5), (2, 5), (1, 5), (4, 5)],",
            ")",
            "async def test_log_tails_with_appends(lines_to_tail, total_lines, temp_file):",
            "    \"\"\"Test tailing a log file that grows at the same time\"\"\"",
            "    _write_lines_and_get_offset_at_index(temp_file, total_lines)",
            "    test_file = temp_file",
            "    context = MagicMock(grpc.aio.ServicerContext)",
            "    context.done.return_value = False",
            "    start_offset = find_start_offset_last_n_lines_from_offset(",
            "        test_file, offset=-1, n=lines_to_tail",
            "    )",
            "",
            "    actual_data = await _stream_log(context, test_file, start_offset, -1)",
            "",
            "    end_offset = find_end_offset_file(test_file)",
            "    expected_data = _read_file(test_file, start_offset, end_offset)",
            "    assert actual_data == expected_data, \"Non-matching data from stream log\"",
            "",
            "    all_lines = actual_data.decode(\"utf-8\")",
            "    assert all_lines.count(\"\\n\") == lines_to_tail, \"Non-matching number of lines tailed\"",
            "",
            "    # Modify the file with append here",
            "    num_new_lines = 2",
            "    _write_lines_and_get_offset_at_index(",
            "        temp_file, num_new_lines, start_offset=end_offset",
            "    )",
            "",
            "    # Tail again should read the new lines written",
            "    start_offset = find_start_offset_last_n_lines_from_offset(",
            "        test_file, offset=-1, n=lines_to_tail + num_new_lines",
            "    )",
            "",
            "    expected_data = _read_file(test_file, start_offset, -1)",
            "    actual_data = await _stream_log(context, test_file, start_offset, -1)",
            "",
            "    assert (",
            "        actual_data == expected_data",
            "    ), \"Non-matching data from stream log after append\"",
            "",
            "    all_lines = actual_data.decode(\"utf-8\")",
            "    assert (",
            "        all_lines.count(\"\\n\") == lines_to_tail + num_new_lines",
            "    ), \"Non-matching number of lines tailed after append\"",
            "",
            "",
            "@pytest.mark.asyncio",
            "async def test_log_agent_find_task_log_offsets(temp_file):",
            "    log_file_content = \"\"",
            "    task_id = \"taskid1234\"",
            "    attempt_number = 0",
            "    # Previous data",
            "    for i in range(3):",
            "        log_file_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "    # Task's logs",
            "    log_file_content += f\"{LOG_PREFIX_TASK_ATTEMPT_START}{task_id}-{attempt_number}\\n\"",
            "    expected_start = len(log_file_content)",
            "    for i in range(10):",
            "        log_file_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "    expected_end = len(log_file_content)",
            "    log_file_content += f\"{LOG_PREFIX_TASK_ATTEMPT_END}{task_id}-{attempt_number}\\n\"",
            "",
            "    # Next data",
            "    for i in range(3):",
            "        log_file_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "",
            "    # Write to files",
            "    temp_file.write(log_file_content.encode(\"utf-8\"))",
            "",
            "    # Test all task logs",
            "    start_offset, end_offset = await LogAgentV1Grpc._find_task_log_offsets(",
            "        task_id, attempt_number, -1, temp_file",
            "    )",
            "    assert start_offset == expected_start",
            "    assert end_offset == expected_end",
            "",
            "    # Test tailing last X lines",
            "    num_tail = 3",
            "    start_offset, end_offset = await LogAgentV1Grpc._find_task_log_offsets(",
            "        task_id, attempt_number, num_tail, temp_file",
            "    )",
            "    assert end_offset == expected_end",
            "    exclude_tail_content = \"\"",
            "    for i in range(10 - num_tail):",
            "        exclude_tail_content += TEST_LINE_TEMPLATE.format(i) + \"\\n\"",
            "    assert start_offset == expected_start + len(exclude_tail_content)",
            "",
            "",
            "def test_log_agent_resolve_filename(temp_dir):",
            "    \"\"\"",
            "    Test that LogAgentV1Grpc.resolve_filename(root, filename) works:",
            "    1. Not possible to resolve a file that doesn't exist.",
            "    2. Not able to resolve files outside of the temp dir root.",
            "        - with a absolute path.",
            "        - with a relative path recursive up.",
            "    3. Permits a file in a directory that's symlinked into the root dir.",
            "    \"\"\"",
            "    root = Path(temp_dir)",
            "    # Create a file in the temp dir.",
            "    file = root / \"valid_file\"",
            "    file.touch()",
            "    subdir = root / \"subdir\"",
            "    subdir.mkdir()",
            "",
            "    # Create a directory in the root that contains a valid file and",
            "    # is symlinked to by a path in the subdir.",
            "    symlinked_dir = root / \"symlinked\"",
            "    symlinked_dir.mkdir()",
            "    symlinked_file = symlinked_dir / \"valid_file\"",
            "    symlinked_file.touch()",
            "    symlinked_path_in_subdir = subdir / \"symlink_to_outside_dir\"",
            "    symlinked_path_in_subdir.symlink_to(symlinked_dir)",
            "",
            "    # Test file doesn't exist",
            "    with pytest.raises(FileNotFoundError):",
            "        LogAgentV1Grpc._resolve_filename(root, \"non-exist-file\")",
            "",
            "    # Test absolute path outside of root is not allowed",
            "    with pytest.raises(FileNotFoundError):",
            "        LogAgentV1Grpc._resolve_filename(subdir, root.resolve() / \"valid_file\")",
            "",
            "    # Test relative path recursive up is not allowed",
            "    with pytest.raises(FileNotFoundError):",
            "        LogAgentV1Grpc._resolve_filename(subdir, \"../valid_file\")",
            "",
            "    # Test relative path a valid file is allowed",
            "    assert (",
            "        LogAgentV1Grpc._resolve_filename(root, \"valid_file\")",
            "        == (root / \"valid_file\").resolve()",
            "    )",
            "",
            "    # Test relative path to a valid file following a symlink is allowed",
            "    assert (",
            "        LogAgentV1Grpc._resolve_filename(subdir, \"symlink_to_outside_dir/valid_file\")",
            "        == (root / \"symlinked\" / \"valid_file\").resolve()",
            "    )",
            "",
            "",
            "# Unit Tests (LogsManager)",
            "",
            "",
            "@pytest.fixture",
            "def logs_manager():",
            "    if sys.version_info < ASYNCMOCK_MIN_PYTHON_VER:",
            "        raise Exception(f\"Unsupported for this version of python {sys.version_info}\")",
            "    from unittest.mock import AsyncMock",
            "",
            "    client = AsyncMock(StateDataSourceClient)",
            "    manager = LogsManager(client)",
            "    yield manager",
            "",
            "",
            "def generate_list_logs(log_files: List[str]):",
            "    return ListLogsReply(log_files=log_files)",
            "",
            "",
            "def generate_logs_stream_chunk(index: int):",
            "    return f\"{str(index)*10}\"",
            "",
            "",
            "async def generate_logs_stream(num_chunks: int):",
            "    for i in range(num_chunks):",
            "        data = generate_logs_stream_chunk(index=i)",
            "        yield StreamLogReply(data=data.encode())",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_list_logs(logs_manager):",
            "    logs_client = logs_manager.data_source_client",
            "",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [\"1\", \"2\"]",
            "",
            "    logs_client.list_logs.side_effect = [",
            "        generate_list_logs([\"gcs_server.out\"]),",
            "        DataSourceUnavailable(),",
            "    ]",
            "",
            "    # Unregistered node id should raise a DataSourceUnavailable.",
            "    with pytest.raises(DataSourceUnavailable):",
            "        result = await logs_manager.list_logs(",
            "            node_id=\"3\", timeout=30, glob_filter=\"*gcs*\"",
            "        )",
            "",
            "    result = await logs_manager.list_logs(node_id=\"2\", timeout=30, glob_filter=\"*gcs*\")",
            "    assert len(result) == 1",
            "    assert result[\"gcs_server\"] == [\"gcs_server.out\"]",
            "    assert result[\"raylet\"] == []",
            "    logs_client.get_all_registered_log_agent_ids.assert_called()",
            "    logs_client.list_logs.assert_awaited_with(\"2\", \"*gcs*\", timeout=30)",
            "",
            "    # The second call raises DataSourceUnavailable, which will",
            "    # return DataSourceUnavailable to the caller.",
            "    with pytest.raises(DataSourceUnavailable):",
            "        result = await logs_manager.list_logs(",
            "            node_id=\"1\", timeout=30, glob_filter=\"*gcs*\"",
            "        )",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_resolve_file(logs_manager):",
            "    node_id = NodeID(b\"1\" * 28)",
            "    \"\"\"",
            "    Test filename is given.",
            "    \"\"\"",
            "    logs_client = logs_manager.data_source_client",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [node_id.hex()]",
            "    expected_filename = \"filename\"",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=expected_filename,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=None,",
            "        get_actor_fn=lambda _: True,",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    assert log_file_name == expected_filename",
            "    assert n == node_id.hex()",
            "    \"\"\"",
            "    Test actor id is given.",
            "    \"\"\"",
            "    # Actor doesn't exist.",
            "    with pytest.raises(ValueError):",
            "        actor_id = ActorID(b\"2\" * 16)",
            "",
            "        def get_actor_fn(id):",
            "            if id == actor_id:",
            "                return None",
            "            assert False, \"Not reachable.\"",
            "",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=actor_id,",
            "            task_id=None,",
            "            pid=None,",
            "            get_actor_fn=get_actor_fn,",
            "            timeout=10,",
            "        )",
            "",
            "    # Actor exists, but it is not scheduled yet.",
            "    actor_id = ActorID(b\"2\" * 16)",
            "",
            "    with pytest.raises(ValueError):",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=actor_id,",
            "            task_id=None,",
            "            pid=None,",
            "            get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, None),",
            "            timeout=10,",
            "        )",
            "",
            "    # Actor exists.",
            "    actor_id = ActorID(b\"2\" * 16)",
            "    worker_id = WorkerID(b\"3\" * 28)",
            "    logs_manager.list_logs = AsyncMock()",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [f\"worker-{worker_id.hex()}-123-123.out\"],",
            "        \"worker_err\": [],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=actor_id,",
            "        task_id=None,",
            "        pid=None,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{worker_id.hex()}*out\"",
            "    )",
            "    assert log_file_name == f\"worker-{worker_id.hex()}-123-123.out\"",
            "    assert n == node_id.hex()",
            "",
            "    \"\"\"",
            "    Test task id is given.",
            "    \"\"\"",
            "    task_id = TaskID(b\"2\" * 24)",
            "    logs_client = logs_manager.data_source_client",
            "    logs_client.get_all_task_info = AsyncMock()",
            "    logs_client.get_all_task_info.return_value = GetTaskEventsReply(",
            "        events_by_task=[",
            "            generate_task_event(",
            "                task_id,",
            "                node_id,",
            "                attempt_number=1,",
            "                worker_id=worker_id,",
            "                stdout_file=f\"worker-{worker_id.hex()}-123-123.out\",",
            "            )",
            "        ]",
            "    )",
            "",
            "    # Expect resolved file.",
            "    res = await logs_manager.resolve_filename(task_id=task_id, attempt_number=1)",
            "    filename, n = res.filename, res.node_id",
            "    # Default out file. See generate_task_event() for filename",
            "    assert filename == f\"worker-{worker_id.hex()}-123-123.out\"",
            "    assert n == node_id.hex()",
            "",
            "    # Wrong task attempt",
            "    with pytest.raises(FileNotFoundError):",
            "        await logs_manager.resolve_filename(task_id=task_id, attempt_number=0)",
            "",
            "    # No task found",
            "    logs_client.get_all_task_info.return_value = GetTaskEventsReply(events_by_task=[])",
            "    with pytest.raises(FileNotFoundError):",
            "        await logs_manager.resolve_filename(task_id=TaskID(b\"1\" * 24), attempt_number=1)",
            "",
            "    \"\"\"",
            "    Test pid is given.",
            "    \"\"\"",
            "    # Pid doesn't exist.",
            "    with pytest.raises(FileNotFoundError):",
            "        pid = 456",
            "        logs_manager.list_logs = AsyncMock()",
            "        # Provide the wrong pid.",
            "        logs_manager.list_logs.return_value = {",
            "            \"worker_out\": [\"worker-123-123-123.out\"],",
            "            \"worker_err\": [],",
            "        }",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=None,",
            "            task_id=None,",
            "            pid=pid,",
            "            get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "            timeout=10,",
            "        )",
            "",
            "    # Pid exists.",
            "    pid = 123",
            "    logs_manager.list_logs = AsyncMock()",
            "    # Provide the wrong pid.",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [f\"worker-123-123-{pid}.out\"],",
            "        \"worker_err\": [],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=pid,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{pid}*out\"",
            "    )",
            "    assert log_file_name == f\"worker-123-123-{pid}.out\"",
            "",
            "    \"\"\"",
            "    Test nothing is given.",
            "    \"\"\"",
            "    with pytest.raises(FileNotFoundError):",
            "        await logs_manager.resolve_filename(",
            "            node_id=node_id.hex(),",
            "            log_filename=None,",
            "            actor_id=None,",
            "            task_id=None,",
            "            pid=None,",
            "            get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "            timeout=10,",
            "        )",
            "",
            "    \"\"\"",
            "    Test suffix is specified",
            "    \"\"\"",
            "    pid = 123",
            "    logs_manager.list_logs = AsyncMock()",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [f\"worker-123-123-{pid}.out\"],",
            "        \"worker_err\": [],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=pid,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{pid}*out\"",
            "    )",
            "    assert log_file_name == f\"worker-123-123-{pid}.out\"",
            "",
            "    logs_manager.list_logs.return_value = {",
            "        \"worker_out\": [],",
            "        \"worker_err\": [f\"worker-123-123-{pid}.err\"],",
            "    }",
            "    res = await logs_manager.resolve_filename(",
            "        node_id=node_id.hex(),",
            "        log_filename=None,",
            "        actor_id=None,",
            "        task_id=None,",
            "        pid=pid,",
            "        get_actor_fn=lambda _: generate_actor_data(actor_id, node_id, worker_id),",
            "        timeout=10,",
            "        suffix=\"err\",",
            "    )",
            "    log_file_name, n = res.filename, res.node_id",
            "    logs_manager.list_logs.assert_awaited_with(",
            "        node_id.hex(), 10, glob_filter=f\"*{pid}*err\"",
            "    )",
            "    assert log_file_name == f\"worker-123-123-{pid}.err\"",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_stream_log(logs_manager):",
            "    NUM_LOG_CHUNKS = 10",
            "    logs_client = logs_manager.data_source_client",
            "",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [\"1\", \"2\"]",
            "    logs_client.ip_to_node_id = MagicMock()",
            "    logs_client.stream_log.return_value = generate_logs_stream(NUM_LOG_CHUNKS)",
            "",
            "    # Test file_name, media_type=\"file\", node_id",
            "    options = GetLogOptions(",
            "        timeout=30, media_type=\"file\", lines=10, node_id=\"1\", filename=\"raylet.out\"",
            "    )",
            "",
            "    i = 0",
            "    async for chunk in logs_manager.stream_logs(options):",
            "        assert chunk.decode(\"utf-8\") == generate_logs_stream_chunk(index=i)",
            "        i += 1",
            "    assert i == NUM_LOG_CHUNKS",
            "    logs_client.stream_log.assert_awaited_with(",
            "        node_id=\"1\",",
            "        log_file_name=\"raylet.out\",",
            "        keep_alive=False,",
            "        lines=10,",
            "        interval=None,",
            "        timeout=30,",
            "        start_offset=None,",
            "        end_offset=None,",
            "    )",
            "",
            "    # Test pid, media_type = \"stream\", node_ip",
            "",
            "    logs_client.ip_to_node_id.return_value = \"1\"",
            "    logs_client.list_logs.side_effect = [",
            "        generate_list_logs(",
            "            [\"worker-0-0-10.out\", \"worker-0-0-11.out\", \"worker-0-0-10.err\"]",
            "        ),",
            "    ]",
            "    options = GetLogOptions(",
            "        timeout=30, media_type=\"stream\", lines=10, interval=0.5, node_id=\"1\", pid=\"10\"",
            "    )",
            "",
            "    logs_client.stream_log.return_value = generate_logs_stream(NUM_LOG_CHUNKS)",
            "    i = 0",
            "    async for chunk in logs_manager.stream_logs(options):",
            "        assert chunk.decode(\"utf-8\") == generate_logs_stream_chunk(index=i)",
            "        i += 1",
            "    assert i == NUM_LOG_CHUNKS",
            "    logs_client.stream_log.assert_awaited_with(",
            "        node_id=\"1\",",
            "        log_file_name=\"worker-0-0-10.out\",",
            "        keep_alive=True,",
            "        lines=10,",
            "        interval=0.5,",
            "        timeout=None,",
            "        start_offset=None,",
            "        end_offset=None,",
            "    )",
            "",
            "    # Currently cannot test actor_id with AsyncMock.",
            "    # It will be tested by the integration test.",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.version_info < ASYNCMOCK_MIN_PYTHON_VER,",
            "    reason=f\"unittest.mock.AsyncMock requires python {ASYNCMOCK_MIN_PYTHON_VER}\"",
            "    \" or higher\",",
            ")",
            "@pytest.mark.asyncio",
            "async def test_logs_manager_keepalive_no_timeout(logs_manager):",
            "    \"\"\"Test when --follow is specified, there's no timeout.",
            "",
            "    Related: https://github.com/ray-project/ray/issues/25721",
            "    \"\"\"",
            "    NUM_LOG_CHUNKS = 10",
            "    logs_client = logs_manager.data_source_client",
            "",
            "    logs_client.get_all_registered_log_agent_ids = MagicMock()",
            "    logs_client.get_all_registered_log_agent_ids.return_value = [\"1\", \"2\"]",
            "    logs_client.ip_to_node_id = MagicMock()",
            "    logs_client.stream_log.return_value = generate_logs_stream(NUM_LOG_CHUNKS)",
            "",
            "    # Test file_name, media_type=\"file\", node_id",
            "    options = GetLogOptions(",
            "        timeout=30, media_type=\"stream\", lines=10, node_id=\"1\", filename=\"raylet.out\"",
            "    )",
            "",
            "    async for chunk in logs_manager.stream_logs(options):",
            "        pass",
            "",
            "    # Make sure timeout == None when media_type == stream. This is to avoid",
            "    # closing the connection due to DEADLINE_EXCEEDED when --follow is specified.",
            "    logs_client.stream_log.assert_awaited_with(",
            "        node_id=\"1\",",
            "        log_file_name=\"raylet.out\",",
            "        keep_alive=True,",
            "        lines=10,",
            "        interval=None,",
            "        timeout=None,",
            "        start_offset=None,",
            "        end_offset=None,",
            "    )",
            "",
            "",
            "# Integration tests",
            "",
            "",
            "def test_logs_list(ray_start_with_dashboard):",
            "    assert (",
            "        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])",
            "        is True",
            "    )",
            "    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    def verify():",
            "        response = requests.get(webui_url + f\"/api/v0/logs?node_id={node_id}\")",
            "        response.raise_for_status()",
            "        result = json.loads(response.text)",
            "        assert result[\"result\"]",
            "        logs = result[\"data\"][\"result\"]",
            "",
            "        # Test worker logs",
            "        outs = logs[\"worker_out\"]",
            "        errs = logs[\"worker_err\"]",
            "        core_worker_logs = logs[\"core_worker\"]",
            "",
            "        assert len(outs) == len(errs) == len(core_worker_logs)",
            "        assert len(outs) > 0",
            "",
            "        # Test gcs / raylet / dashboard",
            "        for file in [\"gcs_server.out\", \"gcs_server.err\"]:",
            "            assert file in logs[\"gcs_server\"]",
            "        for file in [\"raylet.out\", \"raylet.err\"]:",
            "            assert file in logs[\"raylet\"]",
            "        for file in [\"dashboard.log\"]:",
            "            assert file in logs[\"dashboard\"]",
            "        for file in [\"dashboard_agent.log\"]:",
            "            assert file in logs[\"agent\"]",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    def verify_filter():",
            "        # Test that logs/list can be filtered",
            "        response = requests.get(",
            "            webui_url + f\"/api/v0/logs?node_id={node_id}&glob=*gcs*\"",
            "        )",
            "        response.raise_for_status()",
            "        result = json.loads(response.text)",
            "        assert result[\"result\"]",
            "        logs = result[\"data\"][\"result\"]",
            "        assert \"gcs_server\" in logs",
            "        assert \"internal\" in logs",
            "        assert len(logs) == 2",
            "        assert \"gcs_server.out\" in logs[\"gcs_server\"]",
            "        assert \"gcs_server.err\" in logs[\"gcs_server\"]",
            "        assert \"debug_state_gcs.txt\" in logs[\"internal\"]",
            "        return True",
            "",
            "    wait_for_condition(verify_filter)",
            "",
            "    def verify_worker_logs():",
            "        response = requests.get(",
            "            webui_url + f\"/api/v0/logs?node_id={node_id}&glob=*worker*\"",
            "        )",
            "        response.raise_for_status()",
            "        result = json.loads(response.text)",
            "        assert result[\"result\"]",
            "        logs = result[\"data\"][\"result\"]",
            "        worker_log_categories = [",
            "            \"core_worker\",",
            "            \"worker_out\",",
            "            \"worker_err\",",
            "        ]",
            "        assert all([cat in logs for cat in worker_log_categories])",
            "        num_workers = len(",
            "            list(filter(lambda w: w[\"worker_type\"] == \"WORKER\", list_workers()))",
            "        )",
            "        assert (",
            "            len(logs[\"worker_out\"])",
            "            == len(logs[\"worker_err\"])",
            "            == len(logs[\"worker_out\"])",
            "        )",
            "        assert num_workers == len(logs[\"worker_out\"])",
            "        return True",
            "",
            "    wait_for_condition(verify_worker_logs)",
            "",
            "",
            "@pytest.mark.skipif(sys.platform == \"win32\", reason=\"File path incorrect on Windows.\")",
            "def test_logs_stream_and_tail(ray_start_with_dashboard):",
            "    assert (",
            "        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])",
            "        is True",
            "    )",
            "    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    def verify_basic():",
            "        stream_response = requests.get(",
            "            webui_url",
            "            + f\"/api/v0/logs/file?node_id={node_id}&filename=gcs_server.out&lines=5\",",
            "            stream=True,",
            "        )",
            "        if stream_response.status_code != 200:",
            "            raise ValueError(stream_response.content.decode(\"utf-8\"))",
            "        lines = []",
            "        for line in stream_response.iter_lines():",
            "            lines.append(line.decode(\"utf-8\"))",
            "        assert len(lines) == 5 or len(lines) == 6",
            "        return True",
            "",
            "    wait_for_condition(verify_basic)",
            "",
            "    @ray.remote",
            "    class Actor:",
            "        def write_log(self, strings):",
            "            for s in strings:",
            "                print(s)",
            "",
            "        def getpid(self):",
            "            return os.getpid()",
            "",
            "    test_log_text = \"test_log_text_\u65e5\u5fd7_{}\"",
            "    actor = Actor.remote()",
            "    ray.get(actor.write_log.remote([test_log_text.format(\"XXXXXX\")]))",
            "",
            "    # Test stream and fetching by actor id",
            "    stream_response = requests.get(",
            "        webui_url",
            "        + \"/api/v0/logs/stream?&lines=-1\"",
            "        + f\"&actor_id={actor._ray_actor_id.hex()}\",",
            "        stream=True,",
            "    )",
            "    if stream_response.status_code != 200:",
            "        raise ValueError(stream_response.content.decode(\"utf-8\"))",
            "    stream_iterator = stream_response.iter_content(chunk_size=None)",
            "    actual_output = next(stream_iterator).decode(\"utf-8\")",
            "    assert \"actor_name:Actor\\n\" in actual_output",
            "    assert test_log_text.format(\"XXXXXX\") in actual_output",
            "",
            "    streamed_string = \"\"",
            "    for i in range(5):",
            "        strings = []",
            "        for j in range(3):",
            "            strings.append(test_log_text.format(f\"{3*i + j:06d}\"))",
            "",
            "        ray.get(actor.write_log.remote(strings))",
            "",
            "        string = \"\"",
            "        for s in strings:",
            "            string += s + \"\\n\"",
            "        streamed_string += string",
            "        # NOTE: Prefix 1 indicates the stream has succeeded.",
            "        assert string in next(stream_iterator).decode(\"utf-8\")",
            "    del stream_response",
            "",
            "    # Test tailing log by actor id",
            "    LINES = 150",
            "    file_response = requests.get(",
            "        webui_url",
            "        + f\"/api/v0/logs/file?&lines={LINES}\"",
            "        + \"&actor_id=\"",
            "        + actor._ray_actor_id.hex(),",
            "    ).content.decode(\"utf-8\")",
            "    # NOTE: Prefix 1 indicates the stream has succeeded.",
            "    for line in streamed_string.split(\"\\n\")[-(LINES + 1) :]:",
            "        assert line in file_response",
            "",
            "    # Test query by pid & node_ip instead of actor id.",
            "    node_ip = list(ray.nodes())[0][\"NodeManagerAddress\"]",
            "    pid = ray.get(actor.getpid.remote())",
            "    file_response = requests.get(",
            "        webui_url",
            "        + f\"/api/v0/logs/file?node_ip={node_ip}&lines={LINES}\"",
            "        + f\"&pid={pid}\",",
            "    ).content.decode(\"utf-8\")",
            "    # NOTE: Prefix 1 indicates the stream has succeeded.",
            "    for line in streamed_string.split(\"\\n\")[-(LINES + 1) :]:",
            "        assert line in file_response",
            "",
            "",
            "def test_log_list(ray_start_cluster):",
            "    cluster = ray_start_cluster",
            "    num_nodes = 5",
            "    for _ in range(num_nodes):",
            "        cluster.add_node(num_cpus=0)",
            "    ray.init(address=cluster.address)",
            "",
            "    def verify():",
            "        for node in list_nodes():",
            "            # When glob filter is not provided, it should provide all logs",
            "            logs = list_logs(node_id=node[\"node_id\"])",
            "            assert \"raylet\" in logs",
            "            assert \"gcs_server\" in logs",
            "            assert \"dashboard\" in logs",
            "            assert \"agent\" in logs",
            "            assert \"internal\" in logs",
            "            assert \"driver\" in logs",
            "            assert \"autoscaler\" in logs",
            "",
            "            # Test glob works.",
            "            logs = list_logs(node_id=node[\"node_id\"], glob_filter=\"raylet*\")",
            "            assert len(logs) == 1",
            "            return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    node_id = \"XXXX\"",
            "    with pytest.raises(requests.HTTPError) as e:",
            "        list_logs(node_id=node_id)",
            "",
            "    e.match(f\"Given node id {node_id} is not available\")",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.platform == \"win32\", reason=\"Job submission is failing on windows.\"",
            ")",
            "def test_log_job(ray_start_with_dashboard):",
            "    assert wait_until_server_available(ray_start_with_dashboard[\"webui_url\"]) is True",
            "    webui_url = ray_start_with_dashboard[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    # Submit a job",
            "    from ray.job_submission import JobSubmissionClient",
            "",
            "    JOB_LOG = \"test-job-log\"",
            "    client = JobSubmissionClient(webui_url)",
            "    entrypoint = f\"python -c \\\"print('{JOB_LOG}')\\\"\"",
            "    job_id = client.submit_job(entrypoint=entrypoint)",
            "",
            "    def job_done():",
            "        jobs = list_jobs(filters=[(\"submission_id\", \"=\", job_id)])",
            "        assert len(jobs) == 1",
            "        assert jobs[0].status == \"SUCCEEDED\"",
            "        return True",
            "",
            "    wait_for_condition(job_done)",
            "",
            "    def verify():",
            "        logs = \"\".join(get_log(submission_id=job_id, node_id=node_id))",
            "        assert JOB_LOG + \"\\n\" == logs",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "def test_log_get_invalid_filenames(ray_start_with_dashboard, temp_file):",
            "    assert (",
            "        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])",
            "        is True",
            "    )",
            "    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    # log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()",
            "",
            "    def verify():",
            "        # Kind of hack that we know the file node_ip_address.json exists in ray.",
            "        with pytest.raises(RayStateApiException) as e:",
            "            logs = \"\".join(get_log(node_id=node_id, filename=\"../node_ip_address.json\"))",
            "            print(logs)",
            "            assert \"does not start with \" in str(e.value)",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Verify that reading file outside of the log directory is not allowed",
            "    # with absolute path.",
            "    def verify():",
            "        # Kind of hack that we know the file node_ip_address.json exists in ray.",
            "        temp_file_abs_path = str(Path(temp_file.name).resolve())",
            "        with pytest.raises(RayStateApiException) as e:",
            "            logs = \"\".join(get_log(node_id=node_id, filename=temp_file_abs_path))",
            "            print(logs)",
            "            assert \"does not start with \" in str(e.value)",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "def test_log_get_subdir(ray_start_with_dashboard):",
            "    assert (",
            "        wait_until_server_available(ray_start_with_dashboard.address_info[\"webui_url\"])",
            "        is True",
            "    )",
            "    webui_url = ray_start_with_dashboard.address_info[\"webui_url\"]",
            "    webui_url = format_web_url(webui_url)",
            "    node_id = list_nodes()[0][\"node_id\"]",
            "",
            "    log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()",
            "    subdir = \"test_subdir\"",
            "    file = \"test_#file.log\"",
            "    path = Path(log_dir) / subdir / file",
            "    path.parent.mkdir(parents=True, exist_ok=True)",
            "    path.write_text(\"test log\")",
            "",
            "    # HTTP endpoint",
            "    def verify():",
            "        # Direct logs stream",
            "        response = requests.get(",
            "            webui_url",
            "            + f\"/api/v0/logs/file?node_id={node_id}\"",
            "            + f\"&filename={urllib.parse.quote('test_subdir/test_#file.log')}\"",
            "        )",
            "        assert response.status_code == 200, response.reason",
            "        assert \"test log\" in response.text",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # get log SDK",
            "    def verify():",
            "        logs = \"\".join(get_log(node_id=node_id, filename=\"test_subdir/test_#file.log\"))",
            "        assert \"test log\" in logs",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "def test_log_get(ray_start_cluster):",
            "    cluster = ray_start_cluster",
            "    cluster.add_node(num_cpus=0)",
            "    ray.init(address=cluster.address)",
            "    head_node = list_nodes()[0]",
            "    cluster.add_node(num_cpus=1)",
            "",
            "    @ray.remote(num_cpus=1)",
            "    class Actor:",
            "        def print(self, i):",
            "            for _ in range(i):",
            "                print(\"1\")",
            "",
            "        def getpid(self):",
            "            import os",
            "",
            "            return os.getpid()",
            "",
            "    \"\"\"",
            "    Test filename match",
            "    \"\"\"",
            "",
            "    def verify():",
            "        # By default, node id should be configured to the head node.",
            "        for log in get_log(",
            "            node_id=head_node[\"node_id\"], filename=\"raylet.out\", tail=10",
            "        ):",
            "            # + 1 since the last line is just empty.",
            "            assert len(log.split(\"\\n\")) == 11",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    \"\"\"",
            "    Test worker pid / IP match",
            "    \"\"\"",
            "    a = Actor.remote()",
            "    pid = ray.get(a.getpid.remote())",
            "    ray.get(a.print.remote(20))",
            "",
            "    def verify():",
            "        # By default, node id should be configured to the head node.",
            "        for log in get_log(node_ip=head_node[\"node_ip\"], pid=pid, tail=10):",
            "            # + 1 since the last line is just empty.",
            "            assert len(log.split(\"\\n\")) == 11",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    \"\"\"",
            "    Test actor logs.",
            "    \"\"\"",
            "    actor_id = a._actor_id.hex()",
            "",
            "    def verify():",
            "        # By default, node id should be configured to the head node.",
            "        for log in get_log(actor_id=actor_id, tail=10):",
            "            # + 1 since the last line is just empty.",
            "            assert len(log.split(\"\\n\")) == 11",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    del a",
            "    \"\"\"",
            "    Test log suffix selection for worker/actor",
            "    \"\"\"",
            "    ACTOR_LOG_LINE = \"{dest}:test actor log\"",
            "",
            "    @ray.remote",
            "    class Actor:",
            "        def __init__(self):",
            "            import sys",
            "",
            "            print(ACTOR_LOG_LINE.format(dest=\"out\"))",
            "            print(ACTOR_LOG_LINE.format(dest=\"err\"), file=sys.stderr)",
            "",
            "    actor = Actor.remote()",
            "    actor_id = actor._actor_id.hex()",
            "",
            "    WORKER_LOG_LINE = \"{dest}:test worker log\"",
            "",
            "    @ray.remote",
            "    def worker_func():",
            "        import os",
            "        import sys",
            "",
            "        print(WORKER_LOG_LINE.format(dest=\"out\"))",
            "        print(WORKER_LOG_LINE.format(dest=\"err\"), file=sys.stderr)",
            "        return os.getpid()",
            "",
            "    pid = ray.get(worker_func.remote())",
            "",
            "    def verify():",
            "        # Test actors",
            "        lines = get_log(actor_id=actor_id, suffix=\"err\")",
            "        assert ACTOR_LOG_LINE.format(dest=\"err\") in \"\".join(lines)",
            "",
            "        lines = get_log(actor_id=actor_id, suffix=\"out\")",
            "        assert ACTOR_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        # Default to out",
            "        lines = get_log(actor_id=actor_id)",
            "        assert ACTOR_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        # Test workers",
            "        lines = get_log(node_ip=head_node[\"node_ip\"], pid=pid, suffix=\"err\")",
            "        assert WORKER_LOG_LINE.format(dest=\"err\") in \"\".join(lines)",
            "",
            "        lines = get_log(node_ip=head_node[\"node_ip\"], pid=pid, suffix=\"out\")",
            "        assert WORKER_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        lines = get_log(node_ip=head_node[\"node_ip\"], pid=pid)",
            "        assert WORKER_LOG_LINE.format(dest=\"out\") in \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    def verify():",
            "        runner = CliRunner()",
            "        result = runner.invoke(",
            "            logs_state_cli_group,",
            "            [\"actor\", \"--id\", actor_id],",
            "        )",
            "        assert result.exit_code == 0, result.exception",
            "        assert ACTOR_LOG_LINE.format(dest=\"out\") in result.output",
            "",
            "        result = runner.invoke(",
            "            logs_state_cli_group,",
            "            [",
            "                \"actor\",",
            "                \"--id\",",
            "                actor_id,",
            "                \"--err\",",
            "            ],",
            "        )",
            "        assert result.exit_code == 0, result.exception",
            "        assert ACTOR_LOG_LINE.format(dest=\"err\") in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "    ##############################",
            "    # Test binary files and encodings.",
            "    ##############################",
            "    # Write a binary file to ray log directory.",
            "    log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()",
            "    file = \"test.bin\"",
            "    binary_file = os.path.join(log_dir, file)",
            "    with open(binary_file, \"wb\") as f:",
            "        data = bytearray(i for i in range(256))",
            "        f.write(data)",
            "",
            "    # Get the log",
            "    def verify():",
            "        for read in get_log(node_ip=head_node[\"node_ip\"], filename=file, encoding=None):",
            "            assert read == data",
            "",
            "        # Default utf-8",
            "        for read in get_log(",
            "            node_ip=head_node[\"node_ip\"], filename=file, errors=\"replace\"",
            "        ):",
            "            assert read == data.decode(encoding=\"utf-8\", errors=\"replace\")",
            "",
            "        for read in get_log(",
            "            node_ip=head_node[\"node_ip\"],",
            "            filename=file,",
            "            encoding=\"iso-8859-1\",",
            "            errors=\"replace\",",
            "        ):",
            "            assert read == data.decode(encoding=\"iso-8859-1\", errors=\"replace\")",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test running task logs",
            "    @ray.remote",
            "    def sleep_task(out_msg):",
            "        print(out_msg, end=\"\", file=sys.stdout)",
            "        import time",
            "",
            "        time.sleep(10)",
            "",
            "    expected_out = \"This is a test log from stdout\\n\"",
            "    task = sleep_task.remote(expected_out)",
            "",
            "    def verify():",
            "        lines = get_log(task_id=task.task_id().hex())",
            "        assert expected_out == \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "@pytest.mark.skipif(",
            "    sys.platform == \"win32\", reason=\"Windows has logging race from tasks.\"",
            ")",
            "def test_log_task(shutdown_only):",
            "    from ray.runtime_env import RuntimeEnv",
            "",
            "    ray.init()",
            "",
            "    # Test get log by multiple task id",
            "    @ray.remote",
            "    def task_log():",
            "        out_msg = \"This is a test log from stdout\\n\"",
            "        print(out_msg, end=\"\", file=sys.stdout)",
            "        err_msg = \"THIS IS A TEST LOG FROM STDERR\\n\"",
            "        print(err_msg, end=\"\", file=sys.stderr)",
            "",
            "        return out_msg, err_msg",
            "",
            "    # Run some other tasks before and after to make sure task",
            "    # log only outputs the task's log.",
            "    ray.get(task_log.remote())",
            "    task = task_log.remote()",
            "    expected_out, expected_err = ray.get(task)",
            "    ray.get(task_log.remote())",
            "",
            "    def verify():",
            "        lines = get_log(task_id=task.task_id().hex())",
            "        assert expected_out in \"\".join(lines)",
            "",
            "        # Test suffix",
            "        lines = get_log(task_id=task.task_id().hex(), suffix=\"err\")",
            "        assert expected_err in \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    enabled_actor_task_log_runtime_env = RuntimeEnv(",
            "        env_vars={\"RAY_ENABLE_RECORD_ACTOR_TASK_LOGGING\": \"1\"}",
            "    )",
            "",
            "    # Test actor task logs",
            "    @ray.remote",
            "    class Actor:",
            "        def print_log(self, out_msg):",
            "            for _ in range(3):",
            "                print(out_msg, end=\"\", file=sys.stdout)",
            "                print(out_msg, end=\"\", file=sys.stderr)",
            "",
            "    a = Actor.options(runtime_env=enabled_actor_task_log_runtime_env).remote()",
            "    out_msg = \"This is a test log\\n\"",
            "    t = a.print_log.remote(out_msg)",
            "    ray.get(t)",
            "",
            "    def verify():",
            "        lines = get_log(task_id=t.task_id().hex())",
            "        assert out_msg * 3 == \"\".join(lines)",
            "",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test actor task logs with interleaving logs should raise",
            "    # errors to ask users to user actor log instead.",
            "    @ray.remote",
            "    class AsyncActor:",
            "        async def print_log(self, out_msg):",
            "            for _ in range(3):",
            "                print(out_msg, end=\"\", file=sys.stdout)",
            "                await asyncio.sleep(1)",
            "",
            "    actor = AsyncActor.options(",
            "        max_concurrency=2, runtime_env=enabled_actor_task_log_runtime_env",
            "    ).remote()",
            "    out_msg = \"[{name}]: This is a test log from stdout\\n\"",
            "    task_a = actor.print_log.remote(out_msg.format(name=\"a\"))",
            "    task_b = actor.print_log.remote(out_msg.format(name=\"b\"))",
            "    ray.get([task_a, task_b])",
            "",
            "    def verify():",
            "        lines = get_log(task_id=task_a.task_id().hex())",
            "        assert \"\".join(lines).count(out_msg.format(name=\"a\")) == 3",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    def verify_actor_task_error(task_id, actor_id):",
            "        with pytest.raises(RayStateApiException) as e:",
            "            for log in get_log(task_id=task_id):",
            "                pass",
            "",
            "        assert \"For actor task, please query actor log\" in str(e.value), str(e.value)",
            "        assert f\"ray logs actor --id {actor_id}\" in str(e.value), str(e.value)",
            "",
            "        return True",
            "",
            "    # Getting task logs from actor with actor task log not enabled should raise errors.",
            "    a = Actor.remote()",
            "    t = a.print_log.remote(out_msg)",
            "    ray.get(t)",
            "    wait_for_condition(",
            "        verify_actor_task_error, task_id=t.task_id().hex(), actor_id=a._actor_id.hex()",
            "    )",
            "",
            "    a = AsyncActor.options(max_concurrency=2).remote()",
            "    t = a.print_log.remote(out_msg)",
            "    ray.get(t)",
            "    wait_for_condition(",
            "        verify_actor_task_error, task_id=t.task_id().hex(), actor_id=a._actor_id.hex()",
            "    )",
            "",
            "    # Test task logs tail with lines.",
            "    expected_out = [f\"task-{i}\\n\" for i in range(5)]",
            "",
            "    @ray.remote",
            "    def f():",
            "        print(\"\".join(expected_out), end=\"\", file=sys.stdout)",
            "",
            "    t = f.remote()",
            "    ray.get(t)",
            "",
            "    def verify():",
            "        lines = get_log(task_id=t.task_id().hex(), tail=2)",
            "        actual_output = \"\".join(lines)",
            "        assert actual_output == \"\".join(expected_out[-2:])",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "def test_log_cli(shutdown_only):",
            "    ray.init(num_cpus=1)",
            "    runner = CliRunner()",
            "",
            "    # Test the head node is chosen by default.",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"cluster\"])",
            "        assert result.exit_code == 0, result.exception",
            "        assert \"raylet.out\" in result.output",
            "        assert \"raylet.err\" in result.output",
            "        assert \"gcs_server.out\" in result.output",
            "        assert \"gcs_server.err\" in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test when there's only 1 match, it prints logs.",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"cluster\", \"raylet.out\"])",
            "        assert result.exit_code == 0",
            "        assert \"raylet.out\" not in result.output",
            "        assert \"raylet.err\" not in result.output",
            "        assert \"gcs_server.out\" not in result.output",
            "        assert \"gcs_server.err\" not in result.output",
            "        # Make sure it prints the log message.",
            "        assert \"NodeManager server started\" in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test when there's more than 1 match, it prints a list of logs.",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"cluster\", \"raylet.*\"])",
            "        assert result.exit_code == 0, result.exception",
            "        assert \"raylet.out\" in result.output",
            "        assert \"raylet.err\" in result.output",
            "        assert \"gcs_server.out\" not in result.output",
            "        assert \"gcs_server.err\" not in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test actor log: `ray logs actor`",
            "    ACTOR_LOG_LINE = \"test actor log\"",
            "",
            "    @ray.remote",
            "    class Actor:",
            "        def __init__(self):",
            "            print(ACTOR_LOG_LINE)",
            "",
            "    actor = Actor.remote()",
            "    actor_id = actor._actor_id.hex()",
            "",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"actor\", \"--id\", actor_id])",
            "        assert result.exit_code == 0, result.exception",
            "        assert ACTOR_LOG_LINE in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test worker log: `ray logs worker`",
            "    WORKER_LOG_LINE = \"test worker log\"",
            "",
            "    @ray.remote",
            "    def worker_func():",
            "        import os",
            "",
            "        print(WORKER_LOG_LINE)",
            "        return os.getpid()",
            "",
            "    pid = ray.get(worker_func.remote())",
            "",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"worker\", \"--pid\", pid])",
            "        assert result.exit_code == 0, result.exception",
            "        assert WORKER_LOG_LINE in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test `ray logs raylet.*` forwarding to `ray logs cluster raylet.*`",
            "    def verify():",
            "        result = runner.invoke(logs_state_cli_group, [\"raylet.*\"])",
            "        assert result.exit_code == 0, result.exception",
            "        assert \"raylet.out\" in result.output",
            "        assert \"raylet.err\" in result.output",
            "        assert \"gcs_server.out\" not in result.output",
            "        assert \"gcs_server.err\" not in result.output",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "    # Test binary binary files and encodings.",
            "    log_dir = ray._private.worker.global_worker.node.get_logs_dir_path()",
            "    file = \"test.bin\"",
            "    binary_file = os.path.join(log_dir, file)",
            "    with open(binary_file, \"wb\") as f:",
            "        data = bytearray(i for i in range(256))",
            "        f.write(data)",
            "",
            "    def verify():",
            "        # Tailing with lines is not supported for binary files, thus the `tail=-1`",
            "        result = runner.invoke(",
            "            logs_state_cli_group,",
            "            [",
            "                file,",
            "                \"--encoding\",",
            "                \"iso-8859-1\",",
            "                \"--encoding-errors\",",
            "                \"replace\",",
            "                \"--tail\",",
            "                \"-1\",",
            "            ],",
            "        )",
            "        assert result.exit_code == 0, result.exception",
            "        assert result.output == data.decode(encoding=\"iso-8859-1\", errors=\"replace\")",
            "        return True",
            "",
            "    wait_for_condition(verify)",
            "",
            "",
            "if __name__ == \"__main__\":",
            "    import sys",
            "",
            "    if os.environ.get(\"PARALLEL_CI\"):",
            "        sys.exit(pytest.main([\"-n\", \"auto\", \"--boxed\", \"-vs\", __file__]))",
            "    else:",
            "        sys.exit(pytest.main([\"-sv\", __file__]))"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.PIL.IcnsImagePlugin.IcnsFile.dataforsize"
        ]
    },
    "python/ray/util/state/api.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1355,
                "afterPatchRowNumber": 1355,
                "PatchRowcode": "     r = requests.get("
            },
            "1": {
                "beforePatchRowNumber": 1356,
                "afterPatchRowNumber": 1356,
                "PatchRowcode": "         f\"{api_server_url}/api/v0/logs?{urllib.parse.urlencode(options_dict)}\""
            },
            "2": {
                "beforePatchRowNumber": 1357,
                "afterPatchRowNumber": 1357,
                "PatchRowcode": "     )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1358,
                "PatchRowcode": "+    # TODO(rickyx): we could do better at error handling here."
            },
            "4": {
                "beforePatchRowNumber": 1358,
                "afterPatchRowNumber": 1359,
                "PatchRowcode": "     r.raise_for_status()"
            },
            "5": {
                "beforePatchRowNumber": 1359,
                "afterPatchRowNumber": 1360,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 1360,
                "afterPatchRowNumber": 1361,
                "PatchRowcode": "     response = r.json()"
            }
        },
        "frontPatchFile": [
            "import logging",
            "import threading",
            "import urllib",
            "import warnings",
            "from contextlib import contextmanager",
            "from dataclasses import fields",
            "from typing import Any, Dict, Generator, List, Optional, Tuple, Union",
            "",
            "import requests",
            "",
            "from ray.dashboard.modules.dashboard_sdk import SubmissionClient",
            "from ray.dashboard.utils import (",
            "    get_address_for_submission_client,",
            "    ray_address_to_api_server_url,",
            ")",
            "from ray.util.annotations import DeveloperAPI",
            "from ray.util.state.common import (",
            "    DEFAULT_LIMIT,",
            "    DEFAULT_RPC_TIMEOUT,",
            "    ActorState,",
            "    ClusterEventState,",
            "    GetApiOptions,",
            "    GetLogOptions,",
            "    JobState,",
            "    ListApiOptions,",
            "    NodeState,",
            "    ObjectState,",
            "    PlacementGroupState,",
            "    PredicateType,",
            "    RuntimeEnvState,",
            "    StateResource,",
            "    SummaryApiOptions,",
            "    SummaryResource,",
            "    SupportedFilterType,",
            "    TaskState,",
            "    WorkerState,",
            "    dict_to_state,",
            ")",
            "from ray.util.state.exception import RayStateApiException, ServerUnavailable",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@contextmanager",
            "def warnings_on_slow_request(",
            "    *, address: str, endpoint: str, timeout: float, explain: bool",
            "):",
            "    \"\"\"A context manager to print warnings if the request is replied slowly.",
            "",
            "    Warnings are printed 3 times",
            "",
            "    Args:",
            "        address: The address of the endpoint.",
            "        endpoint: The name of the endpoint.",
            "        timeout: Request timeout in seconds.",
            "        explain: Whether ot not it will print the warning.",
            "    \"\"\"",
            "    # Do nothing if explain is not specified.",
            "    if not explain:",
            "        yield",
            "        return",
            "",
            "    # Prepare timers to print warning.",
            "    # Print 3 times with exponential backoff. timeout / 2, timeout / 4, timeout / 8",
            "    def print_warning(elapsed: float):",
            "        logger.info(",
            "            f\"({round(elapsed, 2)} / {timeout} seconds) \"",
            "            \"Waiting for the response from the API server \"",
            "            f\"address {address}{endpoint}.\",",
            "        )",
            "",
            "    warning_timers = [",
            "        threading.Timer(timeout / i, print_warning, args=[timeout / i])",
            "        for i in [2, 4, 8]",
            "    ]",
            "",
            "    try:",
            "        for timer in warning_timers:",
            "            timer.start()",
            "        yield",
            "    finally:",
            "        # Make sure all timers are cancelled once request is terminated.",
            "        for timer in warning_timers:",
            "            timer.cancel()",
            "",
            "",
            "\"\"\"",
            "This file contains API client and methods for querying ray state.",
            "",
            "Usage:",
            "    1. [Recommended] With StateApiClient:",
            "    ```",
            "        client = StateApiClient(address=\"auto\")",
            "        data = client.list(StateResource.NODES)",
            "        ...",
            "    ```",
            "",
            "    2. With SDK APIs:",
            "    The API creates a `StateApiClient` for each invocation. So if multiple",
            "    invocations of listing are used, it is better to reuse the `StateApiClient`",
            "    as suggested above.",
            "    ```",
            "        data = list_nodes(address=\"auto\")",
            "    ```",
            "\"\"\"",
            "",
            "",
            "@DeveloperAPI",
            "class StateApiClient(SubmissionClient):",
            "    \"\"\"State API Client issues REST GET requests to the server for resource states.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        address: Optional[str] = None,",
            "        cookies: Optional[Dict[str, Any]] = None,",
            "        headers: Optional[Dict[str, Any]] = None,",
            "    ):",
            "        \"\"\"Initialize a StateApiClient and check the connection to the cluster.",
            "",
            "        Args:",
            "            address: Ray bootstrap address (e.g. `127.0.0.0:6379`, `auto`), or Ray",
            "                Client adress (e.g. `ray://<head-node-ip>:10001`), or Ray dashboard",
            "                address (e.g. `http://<head-node-ip>:8265`).",
            "                If not provided, it will be detected automatically from any running",
            "                local Ray cluster.",
            "            cookies: Cookies to use when sending requests to the HTTP job server.",
            "            headers: Headers to use when sending requests to the HTTP job server, used",
            "                for cases like authentication to a remote cluster.",
            "        \"\"\"",
            "        if requests is None:",
            "            raise RuntimeError(",
            "                \"The Ray state CLI & SDK require the ray[default] \"",
            "                \"installation: `pip install 'ray[default']``\"",
            "            )",
            "        if not headers:",
            "            headers = {\"Content-Type\": \"application/json\"}",
            "",
            "        # Resolve API server URL",
            "        api_server_url = get_address_for_submission_client(address)",
            "",
            "        super().__init__(",
            "            address=api_server_url,",
            "            create_cluster_if_needed=False,",
            "            headers=headers,",
            "            cookies=cookies,",
            "        )",
            "",
            "    @classmethod",
            "    def _make_param(cls, options: Union[ListApiOptions, GetApiOptions]) -> Dict:",
            "        options_dict = {}",
            "        for field in fields(options):",
            "            # TODO(rickyyx): We will need to find a way to pass server side timeout",
            "            # TODO(rickyyx): We will have to convert filter option",
            "            # slightly differently for now. But could we do k,v pair rather than this?",
            "            # I see we are also converting dict to XXXApiOptions later on, we could",
            "            # probably organize the marshaling a bit better.",
            "            if field.name == \"filters\":",
            "                options_dict[\"filter_keys\"] = []",
            "                options_dict[\"filter_predicates\"] = []",
            "                options_dict[\"filter_values\"] = []",
            "                for filter in options.filters:",
            "                    if len(filter) != 3:",
            "                        raise ValueError(",
            "                            f\"The given filter has incorrect input type, {filter}. \"",
            "                            \"Provide (key, predicate, value) tuples.\"",
            "                        )",
            "                    filter_k, filter_predicate, filter_val = filter",
            "                    options_dict[\"filter_keys\"].append(filter_k)",
            "                    options_dict[\"filter_predicates\"].append(filter_predicate)",
            "                    options_dict[\"filter_values\"].append(filter_val)",
            "                continue",
            "",
            "            option_val = getattr(options, field.name)",
            "            if option_val is not None:",
            "                options_dict[field.name] = option_val",
            "",
            "        return options_dict",
            "",
            "    def _make_http_get_request(",
            "        self,",
            "        endpoint: str,",
            "        params: Dict,",
            "        timeout: float,",
            "        _explain: bool = False,",
            "    ) -> Dict:",
            "        with warnings_on_slow_request(",
            "            address=self._address, endpoint=endpoint, timeout=timeout, explain=_explain",
            "        ):",
            "            # Send a request.",
            "            response = None",
            "            try:",
            "                response = self._do_request(",
            "                    \"GET\",",
            "                    endpoint,",
            "                    timeout=timeout,",
            "                    params=params,",
            "                )",
            "                # If we have a valid JSON error, don't raise a generic exception but",
            "                # instead let the caller parse it to raise a more precise exception.",
            "                if (",
            "                    response.status_code == 500",
            "                    and \"application/json\"",
            "                    not in response.headers.get(\"Content-Type\", \"\")",
            "                ):",
            "                    response.raise_for_status()",
            "            except requests.exceptions.RequestException as e:",
            "                err_str = f\"Failed to make request to {self._address}{endpoint}. \"",
            "",
            "                # Best-effort to give hints to users on potential reasons of connection",
            "                # failure.",
            "                err_str += (",
            "                    \"Failed to connect to API server. Please check the API server \"",
            "                    \"log for details. Make sure dependencies are installed with \"",
            "                    \"`pip install ray[default]`. Please also check dashboard is \"",
            "                    \"available, and included when starting ray cluster, \"",
            "                    \"i.e. `ray start --include-dashboard=True --head`. \"",
            "                )",
            "                if response is None:",
            "                    raise ServerUnavailable(err_str)",
            "",
            "                err_str += f\"Response(url={response.url},status={response.status_code})\"",
            "                raise RayStateApiException(err_str) from e",
            "",
            "        # Process the response.",
            "        response = response.json()",
            "        if response[\"result\"] is False:",
            "            raise RayStateApiException(",
            "                \"API server internal error. See dashboard.log file for more details. \"",
            "                f\"Error: {response['msg']}\"",
            "            )",
            "",
            "        # Dictionary of `ListApiResponse` or `SummaryApiResponse`",
            "        return response[\"data\"][\"result\"]",
            "",
            "    def get(",
            "        self,",
            "        resource: StateResource,",
            "        id: str,",
            "        options: Optional[GetApiOptions],",
            "        _explain: bool = False,",
            "    ) -> Optional[",
            "        Union[",
            "            ActorState,",
            "            PlacementGroupState,",
            "            NodeState,",
            "            WorkerState,",
            "            TaskState,",
            "            List[ObjectState],",
            "            JobState,",
            "        ]",
            "    ]:",
            "        \"\"\"Get resources states by id",
            "",
            "        Args:",
            "            resource_name: Resource names, i.e. 'workers', 'actors', 'nodes',",
            "                'placement_groups', 'tasks', 'objects'.",
            "                'jobs' and 'runtime-envs' are not supported yet.",
            "            id: ID for the resource, i.e. 'node_id' for nodes.",
            "            options: Get options. See `GetApiOptions` for details.",
            "            _explain: Print the API information such as API",
            "                latency or failed query information.",
            "",
            "        Returns:",
            "            None if not found, and if found:",
            "            - ActorState for actors",
            "            - PlacementGroupState for placement groups",
            "            - NodeState for nodes",
            "            - WorkerState for workers",
            "            - TaskState for tasks",
            "            - JobState for jobs",
            "",
            "            Empty list for objects if not found, or list of ObjectState for objects",
            "",
            "        Raises:",
            "            This doesn't catch any exceptions raised when the underlying request",
            "            call raises exceptions. For example, it could raise `requests.Timeout`",
            "            when timeout occurs.",
            "",
            "            ValueError:",
            "                if the resource could not be GET by id, i.e. jobs and runtime-envs.",
            "",
            "        \"\"\"",
            "        # TODO(rickyyx): Make GET not using filters on list operation",
            "        params = self._make_param(options)",
            "",
            "        RESOURCE_ID_KEY_NAME = {",
            "            StateResource.NODES: \"node_id\",",
            "            StateResource.ACTORS: \"actor_id\",",
            "            StateResource.PLACEMENT_GROUPS: \"placement_group_id\",",
            "            StateResource.WORKERS: \"worker_id\",",
            "            StateResource.TASKS: \"task_id\",",
            "            StateResource.OBJECTS: \"object_id\",",
            "            StateResource.JOBS: \"submission_id\",",
            "        }",
            "        if resource not in RESOURCE_ID_KEY_NAME:",
            "            raise ValueError(f\"Can't get {resource.name} by id.\")",
            "",
            "        params[\"filter_keys\"] = [RESOURCE_ID_KEY_NAME[resource]]",
            "        params[\"filter_predicates\"] = [\"=\"]",
            "        params[\"filter_values\"] = [id]",
            "        params[\"detail\"] = True",
            "        endpoint = f\"/api/v0/{resource.value}\"",
            "",
            "        list_api_response = self._make_http_get_request(",
            "            endpoint=endpoint,",
            "            params=params,",
            "            timeout=options.timeout,",
            "            _explain=_explain,",
            "        )",
            "        result = list_api_response[\"result\"]",
            "",
            "        # Empty result",
            "        if len(result) == 0:",
            "            return None",
            "",
            "        result = [dict_to_state(d, resource) for d in result]",
            "        if resource == StateResource.OBJECTS:",
            "            # NOTE(rickyyx):",
            "            # There might be multiple object entries for a single object id",
            "            # because a single object could be referenced at different places",
            "            # e.g. pinned as local variable, used as parameter",
            "            return result",
            "",
            "        if resource == StateResource.TASKS:",
            "            # There might be multiple task attempts given a task id due to",
            "            # task retries.",
            "            if len(result) == 1:",
            "                return result[0]",
            "            return result",
            "",
            "        # For the rest of the resources, there should only be a single entry",
            "        # for a particular id.",
            "        assert len(result) == 1",
            "        return result[0]",
            "",
            "    def _print_api_warning(",
            "        self,",
            "        resource: StateResource,",
            "        api_response: dict,",
            "        warn_data_source_not_available: bool = True,",
            "        warn_data_truncation: bool = True,",
            "        warn_limit: bool = True,",
            "        warn_server_side_warnings: bool = True,",
            "    ):",
            "        \"\"\"Print the API warnings.",
            "",
            "        Args:",
            "            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',",
            "                see `StateResource` for details.",
            "            api_response: The dictionarified `ListApiResponse` or `SummaryApiResponse`.",
            "            warn_data_source_not_available: Warn when some data sources",
            "                are not available.",
            "            warn_data_truncation: Warn when results were truncated at",
            "                the data source.",
            "            warn_limit: Warn when results were limited.",
            "            warn_server_side_warnings: Warn when the server side generates warnings",
            "                (E.g., when callsites not enabled for listing objects)",
            "        \"\"\"",
            "        # Print warnings if anything was given.",
            "        if warn_data_source_not_available:",
            "            warning_msgs = api_response.get(\"partial_failure_warning\", None)",
            "            if warning_msgs:",
            "                warnings.warn(warning_msgs)",
            "",
            "        if warn_data_truncation:",
            "            # Print warnings if data is truncated at the data source.",
            "            num_after_truncation = api_response[\"num_after_truncation\"]",
            "            total = api_response[\"total\"]",
            "            if total > num_after_truncation:",
            "                # NOTE(rickyyx): For now, there's not much users",
            "                # could do (neither can we), with hard truncation.",
            "                # Unless we allow users to set a higher",
            "                # `RAY_MAX_LIMIT_FROM_DATA_SOURCE`, the data will",
            "                # always be truncated at the data source.",
            "                warnings.warn(",
            "                    (",
            "                        \"The returned data may contain incomplete result. \"",
            "                        f\"{num_after_truncation} ({total} total from the cluster) \"",
            "                        f\"{resource.value} are retrieved from the data source. \"",
            "                        f\"{total - num_after_truncation} entries have been truncated. \"",
            "                        f\"Max of {num_after_truncation} entries are retrieved \"",
            "                        \"from data source to prevent over-sized payloads.\"",
            "                    ),",
            "                )",
            "",
            "        if warn_limit:",
            "            # Print warnings if return data is limited at the API server due to",
            "            # limit enforced at the server side",
            "            num_filtered = api_response[\"num_filtered\"]",
            "            data = api_response[\"result\"]",
            "            if num_filtered > len(data):",
            "                warnings.warn(",
            "                    (",
            "                        f\"Limit last {len(data)} entries \"",
            "                        f\"(Total {num_filtered}). Use `--filter` to reduce \"",
            "                        \"the amount of data to return or \"",
            "                        \"setting a higher limit with `--limit` to see all data. \"",
            "                    ),",
            "                )",
            "",
            "        if warn_server_side_warnings:",
            "            # Print the additional warnings.",
            "            warnings_to_print = api_response.get(\"warnings\", [])",
            "            if warnings_to_print:",
            "                for warning_to_print in warnings_to_print:",
            "                    warnings.warn(warning_to_print)",
            "",
            "    def _raise_on_missing_output(self, resource: StateResource, api_response: dict):",
            "        \"\"\"Raise an exception when the API resopnse contains a missing output.",
            "",
            "        Output can be missing if (1) Failures on some of data source queries (e.g.,",
            "        `ray list tasks` queries all raylets, and if some of queries fail, it will",
            "        contain missing output. If all queries fail, it will just fail). (2) Data",
            "        is truncated because the output is too large.",
            "",
            "        Args:",
            "            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',",
            "                see `StateResource` for details.",
            "            api_response: The dictionarified `ListApiResponse` or `SummaryApiResponse`.",
            "        \"\"\"",
            "        # Raise an exception if there are partial failures that cause missing output.",
            "        warning_msgs = api_response.get(\"partial_failure_warning\", None)",
            "        if warning_msgs:",
            "            raise RayStateApiException(",
            "                f\"Failed to retrieve all {resource.value} from the cluster because\"",
            "                \"they are not reachable due to query failures to the data sources. \"",
            "                \"To avoid raising an exception and allow having missing output, \"",
            "                \"set `raise_on_missing_output=False`. \"",
            "            )",
            "        # Raise an exception is there is data truncation that cause missing output.",
            "        total = api_response[\"total\"]",
            "        num_after_truncation = api_response[\"num_after_truncation\"]",
            "",
            "        if total != num_after_truncation:",
            "            raise RayStateApiException(",
            "                f\"Failed to retrieve all {resource.value} from the cluster because \"",
            "                \"they are not reachable due to data truncation. It happens \"",
            "                \"when the returned data is too large \"",
            "                # When the data is truncated, the truncation",
            "                # threshold == num_after_truncation. We cannot set this to env",
            "                # var because the CLI side might not have the correct env var.",
            "                f\"(> {num_after_truncation}) \"",
            "                \"To avoid raising an exception and allow having missing output, \"",
            "                \"set `raise_on_missing_output=False`. \"",
            "            )",
            "",
            "    def list(",
            "        self,",
            "        resource: StateResource,",
            "        options: ListApiOptions,",
            "        raise_on_missing_output: bool,",
            "        _explain: bool = False,",
            "    ) -> List[",
            "        Union[",
            "            ActorState,",
            "            JobState,",
            "            NodeState,",
            "            TaskState,",
            "            ObjectState,",
            "            PlacementGroupState,",
            "            RuntimeEnvState,",
            "            WorkerState,",
            "            ClusterEventState,",
            "        ]",
            "    ]:",
            "        \"\"\"List resources states",
            "",
            "        Args:",
            "            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',",
            "                see `StateResource` for details.",
            "            options: List options. See `ListApiOptions` for details.",
            "            raise_on_missing_output: When True, raise an exception if the output",
            "                is incomplete. Output can be incomplete if",
            "                (1) there's a partial network failure when the source is distributed.",
            "                (2) data is truncated because it is too large.",
            "                Set it to False to avoid throwing an exception on missing data.",
            "            _explain: Print the API information such as API",
            "                latency or failed query information.",
            "",
            "        Returns:",
            "            A list of queried result from `ListApiResponse`,",
            "",
            "        Raises:",
            "            This doesn't catch any exceptions raised when the underlying request",
            "            call raises exceptions. For example, it could raise `requests.Timeout`",
            "            when timeout occurs.",
            "",
            "        \"\"\"",
            "",
            "        endpoint = f\"/api/v0/{resource.value}\"",
            "        params = self._make_param(options)",
            "        list_api_response = self._make_http_get_request(",
            "            endpoint=endpoint,",
            "            params=params,",
            "            timeout=options.timeout,",
            "            _explain=_explain,",
            "        )",
            "        if raise_on_missing_output:",
            "            self._raise_on_missing_output(resource, list_api_response)",
            "        if _explain:",
            "            self._print_api_warning(resource, list_api_response)",
            "        return [dict_to_state(d, resource) for d in list_api_response[\"result\"]]",
            "",
            "    def summary(",
            "        self,",
            "        resource: SummaryResource,",
            "        *,",
            "        options: SummaryApiOptions,",
            "        raise_on_missing_output: bool,",
            "        _explain: bool = False,",
            "    ) -> Dict:",
            "        \"\"\"Summarize resources states",
            "",
            "        Args:",
            "            resource_name: Resource names,",
            "                see `SummaryResource` for details.",
            "            options: summary options. See `SummaryApiOptions` for details.",
            "            raise_on_missing_output: Raise an exception if the output has missing data.",
            "                Output can have missing data if (1) there's a partial network failure",
            "                when the source is distributed. (2) data is truncated",
            "                because it is too large.",
            "            _explain: Print the API information such as API",
            "                latency or failed query information.",
            "",
            "        Returns:",
            "            A dictionary of queried result from `SummaryApiResponse`.",
            "",
            "        Raises:",
            "            This doesn't catch any exceptions raised when the underlying request",
            "            call raises exceptions. For example, it could raise `requests.Timeout`",
            "            when timeout occurs.",
            "        \"\"\"",
            "        params = {\"timeout\": options.timeout}",
            "        endpoint = f\"/api/v0/{resource.value}/summarize\"",
            "        summary_api_response = self._make_http_get_request(",
            "            endpoint=endpoint,",
            "            params=params,",
            "            timeout=options.timeout,",
            "            _explain=_explain,",
            "        )",
            "        if raise_on_missing_output:",
            "            self._raise_on_missing_output(resource, summary_api_response)",
            "        if _explain:",
            "            # There's no limit applied to summary, so we shouldn't warn.",
            "            self._print_api_warning(resource, summary_api_response, warn_limit=False)",
            "        return summary_api_response[\"result\"][\"node_id_to_summary\"]",
            "",
            "",
            "@DeveloperAPI",
            "def get_actor(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[Dict]:",
            "    \"\"\"Get an actor by id.",
            "",
            "    Args:",
            "        id: Id of the actor",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state API requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`ActorState <ray.util.state.common.ActorState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.ACTORS, id, GetApiOptions(timeout=timeout), _explain=_explain",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_job(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[JobState]:",
            "    \"\"\"Get a submission job detail by id.",
            "",
            "    Args:",
            "        id: Submission ID obtained from job API.",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state API requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if job not found, or",
            "        :class:`JobState <ray.util.state.common.JobState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.JOBS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_placement_group(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[PlacementGroupState]:",
            "    \"\"\"Get a placement group by id.",
            "",
            "    Args:",
            "        id: Id of the placement group",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`~ray.util.state.common.PlacementGroupState`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.PLACEMENT_GROUPS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_node(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[NodeState]:",
            "    \"\"\"Get a node by id.",
            "",
            "    Args:",
            "        id: Id of the node.",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`NodeState <ray.util.state.common.NodeState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI is failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.NODES,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_worker(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[WorkerState]:",
            "    \"\"\"Get a worker by id.",
            "",
            "    Args:",
            "        id: Id of the worker",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.WORKERS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_task(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[TaskState]:",
            "    \"\"\"Get task attempts of a task by id.",
            "",
            "    Args:",
            "        id: Id of the task",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if task not found, or a list of",
            "        :class:`~ray.util.state.common.TaskState`",
            "        from the task attempts.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.TASKS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_objects(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> List[ObjectState]:",
            "    \"\"\"Get objects by id.",
            "",
            "    There could be more than 1 entry returned since an object could be",
            "    referenced at different places.",
            "",
            "    Args:",
            "        id: Id of the object",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`~ray.util.state.common.ObjectState`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`  if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.OBJECTS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_actors(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[ActorState]:",
            "    \"\"\"List actors in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"id\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `ActorState`)",
            "            will be queried and returned. See",
            "            :class:`ActorState <ray.util.state.common.ActorState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`ActorState <ray.util.state.common.ActorState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.ACTORS,",
            "        options=ListApiOptions(",
            "            limit=limit,",
            "            timeout=timeout,",
            "            filters=filters,",
            "            detail=detail,",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_placement_groups(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[PlacementGroupState]:",
            "    \"\"\"List placement groups in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"state\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `PlacementGroupState`)",
            "            will be queried and returned. See",
            "            :class:`~ray.util.state.common.PlacementGroupState`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of :class:`~ray.util.state.common.PlacementGroupState`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.PLACEMENT_GROUPS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_nodes(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[NodeState]:",
            "    \"\"\"List nodes in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"node_name\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `NodeState`)",
            "            will be queried and returned. See",
            "            :class:`NodeState <ray.util.state.common.NodeState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of dictionarified",
            "        :class:`NodeState <ray.util.state.common.NodeState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.NODES,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_jobs(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[JobState]:",
            "    \"\"\"List jobs submitted to the cluster by :ref: `ray job submission <jobs-overview>`.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"status\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `JobState`)",
            "            will be queried and returned. See",
            "            :class:`JobState <ray.util.state.common.JobState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of dictionarified",
            "        :class:`JobState <ray.util.state.common.JobState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.JOBS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_workers(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[WorkerState]:",
            "    \"\"\"List workers in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"is_alive\", \"=\", \"True\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `WorkerState`)",
            "            will be queried and returned. See",
            "            :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.WORKERS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_tasks(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[TaskState]:",
            "    \"\"\"List tasks in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"is_alive\", \"=\", \"True\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `WorkerState`)",
            "            will be queried and returned. See",
            "            :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`TaskState <ray.util.state.common.TaskState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.TASKS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_objects(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[ObjectState]:",
            "    \"\"\"List objects in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"ip\", \"=\", \"0.0.0.0\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `ObjectState`)",
            "            will be queried and returned. See",
            "            :class:`ObjectState <ray.util.state.common.ObjectState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`ObjectState <ray.util.state.common.ObjectState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.OBJECTS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_runtime_envs(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[RuntimeEnvState]:",
            "    \"\"\"List runtime environments in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"node_id\", \"=\", \"abcdef\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `RuntimeEnvState`)",
            "            will be queried and returned. See",
            "            :class:`RuntimeEnvState <ray.util.state.common.RuntimeEnvState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`RuntimeEnvState <ray.util.state.common.RuntimeEnvState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.RUNTIME_ENVS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_cluster_events(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[Dict]:",
            "    return StateApiClient(address=address).list(",
            "        StateResource.CLUSTER_EVENTS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "\"\"\"",
            "Log APIs",
            "\"\"\"",
            "",
            "",
            "@DeveloperAPI",
            "def get_log(",
            "    address: Optional[str] = None,",
            "    node_id: Optional[str] = None,",
            "    node_ip: Optional[str] = None,",
            "    filename: Optional[str] = None,",
            "    actor_id: Optional[str] = None,",
            "    task_id: Optional[str] = None,",
            "    pid: Optional[int] = None,",
            "    follow: bool = False,",
            "    tail: int = -1,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    suffix: str = \"out\",",
            "    encoding: Optional[str] = \"utf-8\",",
            "    errors: Optional[str] = \"strict\",",
            "    submission_id: Optional[str] = None,",
            "    attempt_number: int = 0,",
            "    _interval: Optional[float] = None,",
            ") -> Generator[str, None, None]:",
            "    \"\"\"Retrieve log file based on file name or some entities ids (pid, actor id, task id).",
            "",
            "    Examples:",
            "        .. testcode::",
            "            :hide:",
            "",
            "            import ray",
            "            import time",
            "",
            "            ray.shutdown()",
            "            ray.init()",
            "",
            "            # Wait for the node to be registered to the dashboard",
            "            time.sleep(5)",
            "",
            "        .. testcode::",
            "",
            "            import ray",
            "            from ray.util.state import get_log",
            "",
            "            # Node id could be retrieved from list_nodes() or ray.nodes()",
            "            node_id = ray.nodes()[0][\"NodeID\"]",
            "            filename = \"raylet.out\"",
            "            for l in get_log(filename=filename, node_id=node_id):",
            "               print(l)",
            "",
            "        .. testoutput::",
            "            :options: +MOCK",
            "",
            "            [2023-05-19 12:35:18,347 I 4259 68399276] (raylet) io_service_pool.cc:35: IOServicePool is running with 1 io_service.",
            "            [2023-05-19 12:35:18,348 I 4259 68399276] (raylet) store_runner.cc:32: Allowing the Plasma store to use up to 2.14748GB of memory.",
            "            [2023-05-19 12:35:18,348 I 4259 68399276] (raylet) store_runner.cc:48: Starting object store with directory /tmp, fallback /tmp/ray, and huge page support disabled",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If not specified, it will be retrieved from the initialized ray cluster.",
            "        node_id: Id of the node containing the logs .",
            "        node_ip: Ip of the node containing the logs. (At least one of the node_id and",
            "            node_ip have to be supplied when identifying a node).",
            "        filename: Name of the file (relative to the ray log directory) to be retrieved.",
            "        actor_id: Id of the actor if getting logs from an actor.",
            "        task_id: Id of the task if getting logs from a non concurrent actor.",
            "            For concurrent actor, please query the log with actor_id.",
            "        pid: PID of the worker if getting logs generated by a worker. When querying",
            "            with pid, either node_id or node_ip must be supplied.",
            "        follow: When set to True, logs will be streamed and followed.",
            "        tail: Number of lines to get from the end of the log file. Set to -1 for getting",
            "            the entire log.",
            "        timeout: Max timeout for requests made when getting the logs.",
            "        suffix: The suffix of the log file if query by id of tasks/workers/actors. Default to \"out\".",
            "        encoding: The encoding used to decode the content of the log file. Default is",
            "            \"utf-8\". Use None to get binary data directly.",
            "        errors: The error handling scheme to use for decoding errors. Default is",
            "            \"strict\". See https://docs.python.org/3/library/codecs.html#error-handlers",
            "        submission_id: Job submission ID if getting log from a submission job.",
            "        attempt_number: The attempt number of the task if getting logs generated by a task.",
            "        _interval: The interval in secs to print new logs when `follow=True`.",
            "",
            "    Return:",
            "        A Generator of log line, None for SendType and ReturnType.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "",
            "    api_server_url = ray_address_to_api_server_url(address)",
            "    media_type = \"stream\" if follow else \"file\"",
            "",
            "    options = GetLogOptions(",
            "        node_id=node_id,",
            "        node_ip=node_ip,",
            "        filename=filename,",
            "        actor_id=actor_id,",
            "        task_id=task_id,",
            "        pid=pid,",
            "        lines=tail,",
            "        interval=_interval,",
            "        media_type=media_type,",
            "        timeout=timeout,",
            "        suffix=suffix,",
            "        submission_id=submission_id,",
            "        attempt_number=attempt_number,",
            "    )",
            "    options_dict = {}",
            "    for field in fields(options):",
            "        option_val = getattr(options, field.name)",
            "        if option_val is not None:",
            "            options_dict[field.name] = option_val",
            "",
            "    with requests.get(",
            "        f\"{api_server_url}/api/v0/logs/{media_type}?\"",
            "        f\"{urllib.parse.urlencode(options_dict)}\",",
            "        stream=True,",
            "    ) as r:",
            "        if r.status_code != 200:",
            "            raise RayStateApiException(r.text)",
            "        for bytes in r.iter_content(chunk_size=None):",
            "            bytes = bytearray(bytes)",
            "            # First byte 1 means success.",
            "            if bytes.startswith(b\"1\"):",
            "                bytes.pop(0)",
            "                logs = bytes",
            "                if encoding is not None:",
            "                    logs = bytes.decode(encoding=encoding, errors=errors)",
            "            else:",
            "                assert bytes.startswith(b\"0\")",
            "                error_msg = bytes.decode(\"utf-8\")",
            "                raise RayStateApiException(error_msg)",
            "            yield logs",
            "",
            "",
            "@DeveloperAPI",
            "def list_logs(",
            "    address: Optional[str] = None,",
            "    node_id: Optional[str] = None,",
            "    node_ip: Optional[str] = None,",
            "    glob_filter: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            ") -> Dict[str, List[str]]:",
            "    \"\"\"Listing log files available.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If not specified, it will be retrieved from the initialized ray cluster.",
            "        node_id: Id of the node containing the logs.",
            "        node_ip: Ip of the node containing the logs.",
            "        glob_filter: Name of the file (relative to the ray log directory) to be",
            "            retrieved. E.g. `glob_filter=\"*worker*\"` for all worker logs.",
            "        actor_id: Id of the actor if getting logs from an actor.",
            "        timeout: Max timeout for requests made when getting the logs.",
            "        _interval: The interval in secs to print new logs when `follow=True`.",
            "",
            "    Return:",
            "        A dictionary where the keys are log groups (e.g. gcs, raylet, worker), and",
            "        values are list of log filenames.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data, or ConnectionError if failed to resolve the",
            "            ray address.",
            "    \"\"\"  # noqa: E501",
            "    assert (",
            "        node_ip is not None or node_id is not None",
            "    ), \"At least one of node ip and node id is required\"",
            "",
            "    api_server_url = ray_address_to_api_server_url(address)",
            "",
            "    if not glob_filter:",
            "        glob_filter = \"*\"",
            "",
            "    options_dict = {}",
            "    if node_ip:",
            "        options_dict[\"node_ip\"] = node_ip",
            "    if node_id:",
            "        options_dict[\"node_id\"] = node_id",
            "    if glob_filter:",
            "        options_dict[\"glob\"] = glob_filter",
            "    options_dict[\"timeout\"] = timeout",
            "",
            "    r = requests.get(",
            "        f\"{api_server_url}/api/v0/logs?{urllib.parse.urlencode(options_dict)}\"",
            "    )",
            "    r.raise_for_status()",
            "",
            "    response = r.json()",
            "    if response[\"result\"] is False:",
            "        raise RayStateApiException(",
            "            \"API server internal error. See dashboard.log file for more details. \"",
            "            f\"Error: {response['msg']}\"",
            "        )",
            "    return response[\"data\"][\"result\"]",
            "",
            "",
            "\"\"\"",
            "Summary APIs",
            "\"\"\"",
            "",
            "",
            "@DeveloperAPI",
            "def summarize_tasks(",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> Dict:",
            "    \"\"\"Summarize the tasks in cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout for requests made when getting the states.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Return:",
            "        Dictionarified",
            "        :class:`~ray.util.state.common.TaskSummaries`",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI is failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).summary(",
            "        SummaryResource.TASKS,",
            "        options=SummaryApiOptions(timeout=timeout),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def summarize_actors(",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> Dict:",
            "    \"\"\"Summarize the actors in cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout for requests made when getting the states.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Return:",
            "        Dictionarified",
            "        :class:`~ray.util.state.common.ActorSummaries`",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).summary(",
            "        SummaryResource.ACTORS,",
            "        options=SummaryApiOptions(timeout=timeout),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def summarize_objects(",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> Dict:",
            "    \"\"\"Summarize the objects in cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout for requests made when getting the states.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Return:",
            "        Dictionarified :class:`~ray.util.state.common.ObjectSummaries`",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).summary(",
            "        SummaryResource.OBJECTS,",
            "        options=SummaryApiOptions(timeout=timeout),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )"
        ],
        "afterPatchFile": [
            "import logging",
            "import threading",
            "import urllib",
            "import warnings",
            "from contextlib import contextmanager",
            "from dataclasses import fields",
            "from typing import Any, Dict, Generator, List, Optional, Tuple, Union",
            "",
            "import requests",
            "",
            "from ray.dashboard.modules.dashboard_sdk import SubmissionClient",
            "from ray.dashboard.utils import (",
            "    get_address_for_submission_client,",
            "    ray_address_to_api_server_url,",
            ")",
            "from ray.util.annotations import DeveloperAPI",
            "from ray.util.state.common import (",
            "    DEFAULT_LIMIT,",
            "    DEFAULT_RPC_TIMEOUT,",
            "    ActorState,",
            "    ClusterEventState,",
            "    GetApiOptions,",
            "    GetLogOptions,",
            "    JobState,",
            "    ListApiOptions,",
            "    NodeState,",
            "    ObjectState,",
            "    PlacementGroupState,",
            "    PredicateType,",
            "    RuntimeEnvState,",
            "    StateResource,",
            "    SummaryApiOptions,",
            "    SummaryResource,",
            "    SupportedFilterType,",
            "    TaskState,",
            "    WorkerState,",
            "    dict_to_state,",
            ")",
            "from ray.util.state.exception import RayStateApiException, ServerUnavailable",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "@contextmanager",
            "def warnings_on_slow_request(",
            "    *, address: str, endpoint: str, timeout: float, explain: bool",
            "):",
            "    \"\"\"A context manager to print warnings if the request is replied slowly.",
            "",
            "    Warnings are printed 3 times",
            "",
            "    Args:",
            "        address: The address of the endpoint.",
            "        endpoint: The name of the endpoint.",
            "        timeout: Request timeout in seconds.",
            "        explain: Whether ot not it will print the warning.",
            "    \"\"\"",
            "    # Do nothing if explain is not specified.",
            "    if not explain:",
            "        yield",
            "        return",
            "",
            "    # Prepare timers to print warning.",
            "    # Print 3 times with exponential backoff. timeout / 2, timeout / 4, timeout / 8",
            "    def print_warning(elapsed: float):",
            "        logger.info(",
            "            f\"({round(elapsed, 2)} / {timeout} seconds) \"",
            "            \"Waiting for the response from the API server \"",
            "            f\"address {address}{endpoint}.\",",
            "        )",
            "",
            "    warning_timers = [",
            "        threading.Timer(timeout / i, print_warning, args=[timeout / i])",
            "        for i in [2, 4, 8]",
            "    ]",
            "",
            "    try:",
            "        for timer in warning_timers:",
            "            timer.start()",
            "        yield",
            "    finally:",
            "        # Make sure all timers are cancelled once request is terminated.",
            "        for timer in warning_timers:",
            "            timer.cancel()",
            "",
            "",
            "\"\"\"",
            "This file contains API client and methods for querying ray state.",
            "",
            "Usage:",
            "    1. [Recommended] With StateApiClient:",
            "    ```",
            "        client = StateApiClient(address=\"auto\")",
            "        data = client.list(StateResource.NODES)",
            "        ...",
            "    ```",
            "",
            "    2. With SDK APIs:",
            "    The API creates a `StateApiClient` for each invocation. So if multiple",
            "    invocations of listing are used, it is better to reuse the `StateApiClient`",
            "    as suggested above.",
            "    ```",
            "        data = list_nodes(address=\"auto\")",
            "    ```",
            "\"\"\"",
            "",
            "",
            "@DeveloperAPI",
            "class StateApiClient(SubmissionClient):",
            "    \"\"\"State API Client issues REST GET requests to the server for resource states.\"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        address: Optional[str] = None,",
            "        cookies: Optional[Dict[str, Any]] = None,",
            "        headers: Optional[Dict[str, Any]] = None,",
            "    ):",
            "        \"\"\"Initialize a StateApiClient and check the connection to the cluster.",
            "",
            "        Args:",
            "            address: Ray bootstrap address (e.g. `127.0.0.0:6379`, `auto`), or Ray",
            "                Client adress (e.g. `ray://<head-node-ip>:10001`), or Ray dashboard",
            "                address (e.g. `http://<head-node-ip>:8265`).",
            "                If not provided, it will be detected automatically from any running",
            "                local Ray cluster.",
            "            cookies: Cookies to use when sending requests to the HTTP job server.",
            "            headers: Headers to use when sending requests to the HTTP job server, used",
            "                for cases like authentication to a remote cluster.",
            "        \"\"\"",
            "        if requests is None:",
            "            raise RuntimeError(",
            "                \"The Ray state CLI & SDK require the ray[default] \"",
            "                \"installation: `pip install 'ray[default']``\"",
            "            )",
            "        if not headers:",
            "            headers = {\"Content-Type\": \"application/json\"}",
            "",
            "        # Resolve API server URL",
            "        api_server_url = get_address_for_submission_client(address)",
            "",
            "        super().__init__(",
            "            address=api_server_url,",
            "            create_cluster_if_needed=False,",
            "            headers=headers,",
            "            cookies=cookies,",
            "        )",
            "",
            "    @classmethod",
            "    def _make_param(cls, options: Union[ListApiOptions, GetApiOptions]) -> Dict:",
            "        options_dict = {}",
            "        for field in fields(options):",
            "            # TODO(rickyyx): We will need to find a way to pass server side timeout",
            "            # TODO(rickyyx): We will have to convert filter option",
            "            # slightly differently for now. But could we do k,v pair rather than this?",
            "            # I see we are also converting dict to XXXApiOptions later on, we could",
            "            # probably organize the marshaling a bit better.",
            "            if field.name == \"filters\":",
            "                options_dict[\"filter_keys\"] = []",
            "                options_dict[\"filter_predicates\"] = []",
            "                options_dict[\"filter_values\"] = []",
            "                for filter in options.filters:",
            "                    if len(filter) != 3:",
            "                        raise ValueError(",
            "                            f\"The given filter has incorrect input type, {filter}. \"",
            "                            \"Provide (key, predicate, value) tuples.\"",
            "                        )",
            "                    filter_k, filter_predicate, filter_val = filter",
            "                    options_dict[\"filter_keys\"].append(filter_k)",
            "                    options_dict[\"filter_predicates\"].append(filter_predicate)",
            "                    options_dict[\"filter_values\"].append(filter_val)",
            "                continue",
            "",
            "            option_val = getattr(options, field.name)",
            "            if option_val is not None:",
            "                options_dict[field.name] = option_val",
            "",
            "        return options_dict",
            "",
            "    def _make_http_get_request(",
            "        self,",
            "        endpoint: str,",
            "        params: Dict,",
            "        timeout: float,",
            "        _explain: bool = False,",
            "    ) -> Dict:",
            "        with warnings_on_slow_request(",
            "            address=self._address, endpoint=endpoint, timeout=timeout, explain=_explain",
            "        ):",
            "            # Send a request.",
            "            response = None",
            "            try:",
            "                response = self._do_request(",
            "                    \"GET\",",
            "                    endpoint,",
            "                    timeout=timeout,",
            "                    params=params,",
            "                )",
            "                # If we have a valid JSON error, don't raise a generic exception but",
            "                # instead let the caller parse it to raise a more precise exception.",
            "                if (",
            "                    response.status_code == 500",
            "                    and \"application/json\"",
            "                    not in response.headers.get(\"Content-Type\", \"\")",
            "                ):",
            "                    response.raise_for_status()",
            "            except requests.exceptions.RequestException as e:",
            "                err_str = f\"Failed to make request to {self._address}{endpoint}. \"",
            "",
            "                # Best-effort to give hints to users on potential reasons of connection",
            "                # failure.",
            "                err_str += (",
            "                    \"Failed to connect to API server. Please check the API server \"",
            "                    \"log for details. Make sure dependencies are installed with \"",
            "                    \"`pip install ray[default]`. Please also check dashboard is \"",
            "                    \"available, and included when starting ray cluster, \"",
            "                    \"i.e. `ray start --include-dashboard=True --head`. \"",
            "                )",
            "                if response is None:",
            "                    raise ServerUnavailable(err_str)",
            "",
            "                err_str += f\"Response(url={response.url},status={response.status_code})\"",
            "                raise RayStateApiException(err_str) from e",
            "",
            "        # Process the response.",
            "        response = response.json()",
            "        if response[\"result\"] is False:",
            "            raise RayStateApiException(",
            "                \"API server internal error. See dashboard.log file for more details. \"",
            "                f\"Error: {response['msg']}\"",
            "            )",
            "",
            "        # Dictionary of `ListApiResponse` or `SummaryApiResponse`",
            "        return response[\"data\"][\"result\"]",
            "",
            "    def get(",
            "        self,",
            "        resource: StateResource,",
            "        id: str,",
            "        options: Optional[GetApiOptions],",
            "        _explain: bool = False,",
            "    ) -> Optional[",
            "        Union[",
            "            ActorState,",
            "            PlacementGroupState,",
            "            NodeState,",
            "            WorkerState,",
            "            TaskState,",
            "            List[ObjectState],",
            "            JobState,",
            "        ]",
            "    ]:",
            "        \"\"\"Get resources states by id",
            "",
            "        Args:",
            "            resource_name: Resource names, i.e. 'workers', 'actors', 'nodes',",
            "                'placement_groups', 'tasks', 'objects'.",
            "                'jobs' and 'runtime-envs' are not supported yet.",
            "            id: ID for the resource, i.e. 'node_id' for nodes.",
            "            options: Get options. See `GetApiOptions` for details.",
            "            _explain: Print the API information such as API",
            "                latency or failed query information.",
            "",
            "        Returns:",
            "            None if not found, and if found:",
            "            - ActorState for actors",
            "            - PlacementGroupState for placement groups",
            "            - NodeState for nodes",
            "            - WorkerState for workers",
            "            - TaskState for tasks",
            "            - JobState for jobs",
            "",
            "            Empty list for objects if not found, or list of ObjectState for objects",
            "",
            "        Raises:",
            "            This doesn't catch any exceptions raised when the underlying request",
            "            call raises exceptions. For example, it could raise `requests.Timeout`",
            "            when timeout occurs.",
            "",
            "            ValueError:",
            "                if the resource could not be GET by id, i.e. jobs and runtime-envs.",
            "",
            "        \"\"\"",
            "        # TODO(rickyyx): Make GET not using filters on list operation",
            "        params = self._make_param(options)",
            "",
            "        RESOURCE_ID_KEY_NAME = {",
            "            StateResource.NODES: \"node_id\",",
            "            StateResource.ACTORS: \"actor_id\",",
            "            StateResource.PLACEMENT_GROUPS: \"placement_group_id\",",
            "            StateResource.WORKERS: \"worker_id\",",
            "            StateResource.TASKS: \"task_id\",",
            "            StateResource.OBJECTS: \"object_id\",",
            "            StateResource.JOBS: \"submission_id\",",
            "        }",
            "        if resource not in RESOURCE_ID_KEY_NAME:",
            "            raise ValueError(f\"Can't get {resource.name} by id.\")",
            "",
            "        params[\"filter_keys\"] = [RESOURCE_ID_KEY_NAME[resource]]",
            "        params[\"filter_predicates\"] = [\"=\"]",
            "        params[\"filter_values\"] = [id]",
            "        params[\"detail\"] = True",
            "        endpoint = f\"/api/v0/{resource.value}\"",
            "",
            "        list_api_response = self._make_http_get_request(",
            "            endpoint=endpoint,",
            "            params=params,",
            "            timeout=options.timeout,",
            "            _explain=_explain,",
            "        )",
            "        result = list_api_response[\"result\"]",
            "",
            "        # Empty result",
            "        if len(result) == 0:",
            "            return None",
            "",
            "        result = [dict_to_state(d, resource) for d in result]",
            "        if resource == StateResource.OBJECTS:",
            "            # NOTE(rickyyx):",
            "            # There might be multiple object entries for a single object id",
            "            # because a single object could be referenced at different places",
            "            # e.g. pinned as local variable, used as parameter",
            "            return result",
            "",
            "        if resource == StateResource.TASKS:",
            "            # There might be multiple task attempts given a task id due to",
            "            # task retries.",
            "            if len(result) == 1:",
            "                return result[0]",
            "            return result",
            "",
            "        # For the rest of the resources, there should only be a single entry",
            "        # for a particular id.",
            "        assert len(result) == 1",
            "        return result[0]",
            "",
            "    def _print_api_warning(",
            "        self,",
            "        resource: StateResource,",
            "        api_response: dict,",
            "        warn_data_source_not_available: bool = True,",
            "        warn_data_truncation: bool = True,",
            "        warn_limit: bool = True,",
            "        warn_server_side_warnings: bool = True,",
            "    ):",
            "        \"\"\"Print the API warnings.",
            "",
            "        Args:",
            "            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',",
            "                see `StateResource` for details.",
            "            api_response: The dictionarified `ListApiResponse` or `SummaryApiResponse`.",
            "            warn_data_source_not_available: Warn when some data sources",
            "                are not available.",
            "            warn_data_truncation: Warn when results were truncated at",
            "                the data source.",
            "            warn_limit: Warn when results were limited.",
            "            warn_server_side_warnings: Warn when the server side generates warnings",
            "                (E.g., when callsites not enabled for listing objects)",
            "        \"\"\"",
            "        # Print warnings if anything was given.",
            "        if warn_data_source_not_available:",
            "            warning_msgs = api_response.get(\"partial_failure_warning\", None)",
            "            if warning_msgs:",
            "                warnings.warn(warning_msgs)",
            "",
            "        if warn_data_truncation:",
            "            # Print warnings if data is truncated at the data source.",
            "            num_after_truncation = api_response[\"num_after_truncation\"]",
            "            total = api_response[\"total\"]",
            "            if total > num_after_truncation:",
            "                # NOTE(rickyyx): For now, there's not much users",
            "                # could do (neither can we), with hard truncation.",
            "                # Unless we allow users to set a higher",
            "                # `RAY_MAX_LIMIT_FROM_DATA_SOURCE`, the data will",
            "                # always be truncated at the data source.",
            "                warnings.warn(",
            "                    (",
            "                        \"The returned data may contain incomplete result. \"",
            "                        f\"{num_after_truncation} ({total} total from the cluster) \"",
            "                        f\"{resource.value} are retrieved from the data source. \"",
            "                        f\"{total - num_after_truncation} entries have been truncated. \"",
            "                        f\"Max of {num_after_truncation} entries are retrieved \"",
            "                        \"from data source to prevent over-sized payloads.\"",
            "                    ),",
            "                )",
            "",
            "        if warn_limit:",
            "            # Print warnings if return data is limited at the API server due to",
            "            # limit enforced at the server side",
            "            num_filtered = api_response[\"num_filtered\"]",
            "            data = api_response[\"result\"]",
            "            if num_filtered > len(data):",
            "                warnings.warn(",
            "                    (",
            "                        f\"Limit last {len(data)} entries \"",
            "                        f\"(Total {num_filtered}). Use `--filter` to reduce \"",
            "                        \"the amount of data to return or \"",
            "                        \"setting a higher limit with `--limit` to see all data. \"",
            "                    ),",
            "                )",
            "",
            "        if warn_server_side_warnings:",
            "            # Print the additional warnings.",
            "            warnings_to_print = api_response.get(\"warnings\", [])",
            "            if warnings_to_print:",
            "                for warning_to_print in warnings_to_print:",
            "                    warnings.warn(warning_to_print)",
            "",
            "    def _raise_on_missing_output(self, resource: StateResource, api_response: dict):",
            "        \"\"\"Raise an exception when the API resopnse contains a missing output.",
            "",
            "        Output can be missing if (1) Failures on some of data source queries (e.g.,",
            "        `ray list tasks` queries all raylets, and if some of queries fail, it will",
            "        contain missing output. If all queries fail, it will just fail). (2) Data",
            "        is truncated because the output is too large.",
            "",
            "        Args:",
            "            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',",
            "                see `StateResource` for details.",
            "            api_response: The dictionarified `ListApiResponse` or `SummaryApiResponse`.",
            "        \"\"\"",
            "        # Raise an exception if there are partial failures that cause missing output.",
            "        warning_msgs = api_response.get(\"partial_failure_warning\", None)",
            "        if warning_msgs:",
            "            raise RayStateApiException(",
            "                f\"Failed to retrieve all {resource.value} from the cluster because\"",
            "                \"they are not reachable due to query failures to the data sources. \"",
            "                \"To avoid raising an exception and allow having missing output, \"",
            "                \"set `raise_on_missing_output=False`. \"",
            "            )",
            "        # Raise an exception is there is data truncation that cause missing output.",
            "        total = api_response[\"total\"]",
            "        num_after_truncation = api_response[\"num_after_truncation\"]",
            "",
            "        if total != num_after_truncation:",
            "            raise RayStateApiException(",
            "                f\"Failed to retrieve all {resource.value} from the cluster because \"",
            "                \"they are not reachable due to data truncation. It happens \"",
            "                \"when the returned data is too large \"",
            "                # When the data is truncated, the truncation",
            "                # threshold == num_after_truncation. We cannot set this to env",
            "                # var because the CLI side might not have the correct env var.",
            "                f\"(> {num_after_truncation}) \"",
            "                \"To avoid raising an exception and allow having missing output, \"",
            "                \"set `raise_on_missing_output=False`. \"",
            "            )",
            "",
            "    def list(",
            "        self,",
            "        resource: StateResource,",
            "        options: ListApiOptions,",
            "        raise_on_missing_output: bool,",
            "        _explain: bool = False,",
            "    ) -> List[",
            "        Union[",
            "            ActorState,",
            "            JobState,",
            "            NodeState,",
            "            TaskState,",
            "            ObjectState,",
            "            PlacementGroupState,",
            "            RuntimeEnvState,",
            "            WorkerState,",
            "            ClusterEventState,",
            "        ]",
            "    ]:",
            "        \"\"\"List resources states",
            "",
            "        Args:",
            "            resource: Resource names, i.e. 'jobs', 'actors', 'nodes',",
            "                see `StateResource` for details.",
            "            options: List options. See `ListApiOptions` for details.",
            "            raise_on_missing_output: When True, raise an exception if the output",
            "                is incomplete. Output can be incomplete if",
            "                (1) there's a partial network failure when the source is distributed.",
            "                (2) data is truncated because it is too large.",
            "                Set it to False to avoid throwing an exception on missing data.",
            "            _explain: Print the API information such as API",
            "                latency or failed query information.",
            "",
            "        Returns:",
            "            A list of queried result from `ListApiResponse`,",
            "",
            "        Raises:",
            "            This doesn't catch any exceptions raised when the underlying request",
            "            call raises exceptions. For example, it could raise `requests.Timeout`",
            "            when timeout occurs.",
            "",
            "        \"\"\"",
            "",
            "        endpoint = f\"/api/v0/{resource.value}\"",
            "        params = self._make_param(options)",
            "        list_api_response = self._make_http_get_request(",
            "            endpoint=endpoint,",
            "            params=params,",
            "            timeout=options.timeout,",
            "            _explain=_explain,",
            "        )",
            "        if raise_on_missing_output:",
            "            self._raise_on_missing_output(resource, list_api_response)",
            "        if _explain:",
            "            self._print_api_warning(resource, list_api_response)",
            "        return [dict_to_state(d, resource) for d in list_api_response[\"result\"]]",
            "",
            "    def summary(",
            "        self,",
            "        resource: SummaryResource,",
            "        *,",
            "        options: SummaryApiOptions,",
            "        raise_on_missing_output: bool,",
            "        _explain: bool = False,",
            "    ) -> Dict:",
            "        \"\"\"Summarize resources states",
            "",
            "        Args:",
            "            resource_name: Resource names,",
            "                see `SummaryResource` for details.",
            "            options: summary options. See `SummaryApiOptions` for details.",
            "            raise_on_missing_output: Raise an exception if the output has missing data.",
            "                Output can have missing data if (1) there's a partial network failure",
            "                when the source is distributed. (2) data is truncated",
            "                because it is too large.",
            "            _explain: Print the API information such as API",
            "                latency or failed query information.",
            "",
            "        Returns:",
            "            A dictionary of queried result from `SummaryApiResponse`.",
            "",
            "        Raises:",
            "            This doesn't catch any exceptions raised when the underlying request",
            "            call raises exceptions. For example, it could raise `requests.Timeout`",
            "            when timeout occurs.",
            "        \"\"\"",
            "        params = {\"timeout\": options.timeout}",
            "        endpoint = f\"/api/v0/{resource.value}/summarize\"",
            "        summary_api_response = self._make_http_get_request(",
            "            endpoint=endpoint,",
            "            params=params,",
            "            timeout=options.timeout,",
            "            _explain=_explain,",
            "        )",
            "        if raise_on_missing_output:",
            "            self._raise_on_missing_output(resource, summary_api_response)",
            "        if _explain:",
            "            # There's no limit applied to summary, so we shouldn't warn.",
            "            self._print_api_warning(resource, summary_api_response, warn_limit=False)",
            "        return summary_api_response[\"result\"][\"node_id_to_summary\"]",
            "",
            "",
            "@DeveloperAPI",
            "def get_actor(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[Dict]:",
            "    \"\"\"Get an actor by id.",
            "",
            "    Args:",
            "        id: Id of the actor",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state API requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`ActorState <ray.util.state.common.ActorState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.ACTORS, id, GetApiOptions(timeout=timeout), _explain=_explain",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_job(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[JobState]:",
            "    \"\"\"Get a submission job detail by id.",
            "",
            "    Args:",
            "        id: Submission ID obtained from job API.",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state API requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if job not found, or",
            "        :class:`JobState <ray.util.state.common.JobState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.JOBS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_placement_group(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[PlacementGroupState]:",
            "    \"\"\"Get a placement group by id.",
            "",
            "    Args:",
            "        id: Id of the placement group",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`~ray.util.state.common.PlacementGroupState`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.PLACEMENT_GROUPS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_node(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[NodeState]:",
            "    \"\"\"Get a node by id.",
            "",
            "    Args:",
            "        id: Id of the node.",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`NodeState <ray.util.state.common.NodeState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI is failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.NODES,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_worker(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[WorkerState]:",
            "    \"\"\"Get a worker by id.",
            "",
            "    Args:",
            "        id: Id of the worker",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if actor not found, or",
            "        :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.WORKERS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_task(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> Optional[TaskState]:",
            "    \"\"\"Get task attempts of a task by id.",
            "",
            "    Args:",
            "        id: Id of the task",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        None if task not found, or a list of",
            "        :class:`~ray.util.state.common.TaskState`",
            "        from the task attempts.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.TASKS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def get_objects(",
            "    id: str,",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    _explain: bool = False,",
            ") -> List[ObjectState]:",
            "    \"\"\"Get objects by id.",
            "",
            "    There could be more than 1 entry returned since an object could be",
            "    referenced at different places.",
            "",
            "    Args:",
            "        id: Id of the object",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`~ray.util.state.common.ObjectState`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`  if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).get(",
            "        StateResource.OBJECTS,",
            "        id,",
            "        GetApiOptions(timeout=timeout),",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_actors(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[ActorState]:",
            "    \"\"\"List actors in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"id\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `ActorState`)",
            "            will be queried and returned. See",
            "            :class:`ActorState <ray.util.state.common.ActorState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`ActorState <ray.util.state.common.ActorState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.ACTORS,",
            "        options=ListApiOptions(",
            "            limit=limit,",
            "            timeout=timeout,",
            "            filters=filters,",
            "            detail=detail,",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_placement_groups(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[PlacementGroupState]:",
            "    \"\"\"List placement groups in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"state\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `PlacementGroupState`)",
            "            will be queried and returned. See",
            "            :class:`~ray.util.state.common.PlacementGroupState`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of :class:`~ray.util.state.common.PlacementGroupState`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.PLACEMENT_GROUPS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_nodes(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[NodeState]:",
            "    \"\"\"List nodes in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"node_name\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `NodeState`)",
            "            will be queried and returned. See",
            "            :class:`NodeState <ray.util.state.common.NodeState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of dictionarified",
            "        :class:`NodeState <ray.util.state.common.NodeState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.NODES,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_jobs(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[JobState]:",
            "    \"\"\"List jobs submitted to the cluster by :ref: `ray job submission <jobs-overview>`.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"status\", \"=\", \"abcd\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `JobState`)",
            "            will be queried and returned. See",
            "            :class:`JobState <ray.util.state.common.JobState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of dictionarified",
            "        :class:`JobState <ray.util.state.common.JobState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.JOBS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_workers(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[WorkerState]:",
            "    \"\"\"List workers in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"is_alive\", \"=\", \"True\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `WorkerState`)",
            "            will be queried and returned. See",
            "            :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.WORKERS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_tasks(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[TaskState]:",
            "    \"\"\"List tasks in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"is_alive\", \"=\", \"True\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `WorkerState`)",
            "            will be queried and returned. See",
            "            :class:`WorkerState <ray.util.state.common.WorkerState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`TaskState <ray.util.state.common.TaskState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.TASKS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_objects(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[ObjectState]:",
            "    \"\"\"List objects in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"ip\", \"=\", \"0.0.0.0\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `ObjectState`)",
            "            will be queried and returned. See",
            "            :class:`ObjectState <ray.util.state.common.ObjectState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`ObjectState <ray.util.state.common.ObjectState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.OBJECTS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_runtime_envs(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[RuntimeEnvState]:",
            "    \"\"\"List runtime environments in the cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        filters: List of tuples of filter key, predicate (=, or !=), and",
            "            the filter value. E.g., `(\"node_id\", \"=\", \"abcdef\")`",
            "            String filter values are case-insensitive.",
            "        limit: Max number of entries returned by the state backend.",
            "        timeout: Max timeout value for the state APIs requests made.",
            "        detail: When True, more details info (specified in `RuntimeEnvState`)",
            "            will be queried and returned. See",
            "            :class:`RuntimeEnvState <ray.util.state.common.RuntimeEnvState>`.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Returns:",
            "        List of",
            "        :class:`RuntimeEnvState <ray.util.state.common.RuntimeEnvState>`.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).list(",
            "        StateResource.RUNTIME_ENVS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def list_cluster_events(",
            "    address: Optional[str] = None,",
            "    filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    limit: int = DEFAULT_LIMIT,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    detail: bool = False,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> List[Dict]:",
            "    return StateApiClient(address=address).list(",
            "        StateResource.CLUSTER_EVENTS,",
            "        options=ListApiOptions(",
            "            limit=limit, timeout=timeout, filters=filters, detail=detail",
            "        ),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "\"\"\"",
            "Log APIs",
            "\"\"\"",
            "",
            "",
            "@DeveloperAPI",
            "def get_log(",
            "    address: Optional[str] = None,",
            "    node_id: Optional[str] = None,",
            "    node_ip: Optional[str] = None,",
            "    filename: Optional[str] = None,",
            "    actor_id: Optional[str] = None,",
            "    task_id: Optional[str] = None,",
            "    pid: Optional[int] = None,",
            "    follow: bool = False,",
            "    tail: int = -1,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    suffix: str = \"out\",",
            "    encoding: Optional[str] = \"utf-8\",",
            "    errors: Optional[str] = \"strict\",",
            "    submission_id: Optional[str] = None,",
            "    attempt_number: int = 0,",
            "    _interval: Optional[float] = None,",
            ") -> Generator[str, None, None]:",
            "    \"\"\"Retrieve log file based on file name or some entities ids (pid, actor id, task id).",
            "",
            "    Examples:",
            "        .. testcode::",
            "            :hide:",
            "",
            "            import ray",
            "            import time",
            "",
            "            ray.shutdown()",
            "            ray.init()",
            "",
            "            # Wait for the node to be registered to the dashboard",
            "            time.sleep(5)",
            "",
            "        .. testcode::",
            "",
            "            import ray",
            "            from ray.util.state import get_log",
            "",
            "            # Node id could be retrieved from list_nodes() or ray.nodes()",
            "            node_id = ray.nodes()[0][\"NodeID\"]",
            "            filename = \"raylet.out\"",
            "            for l in get_log(filename=filename, node_id=node_id):",
            "               print(l)",
            "",
            "        .. testoutput::",
            "            :options: +MOCK",
            "",
            "            [2023-05-19 12:35:18,347 I 4259 68399276] (raylet) io_service_pool.cc:35: IOServicePool is running with 1 io_service.",
            "            [2023-05-19 12:35:18,348 I 4259 68399276] (raylet) store_runner.cc:32: Allowing the Plasma store to use up to 2.14748GB of memory.",
            "            [2023-05-19 12:35:18,348 I 4259 68399276] (raylet) store_runner.cc:48: Starting object store with directory /tmp, fallback /tmp/ray, and huge page support disabled",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If not specified, it will be retrieved from the initialized ray cluster.",
            "        node_id: Id of the node containing the logs .",
            "        node_ip: Ip of the node containing the logs. (At least one of the node_id and",
            "            node_ip have to be supplied when identifying a node).",
            "        filename: Name of the file (relative to the ray log directory) to be retrieved.",
            "        actor_id: Id of the actor if getting logs from an actor.",
            "        task_id: Id of the task if getting logs from a non concurrent actor.",
            "            For concurrent actor, please query the log with actor_id.",
            "        pid: PID of the worker if getting logs generated by a worker. When querying",
            "            with pid, either node_id or node_ip must be supplied.",
            "        follow: When set to True, logs will be streamed and followed.",
            "        tail: Number of lines to get from the end of the log file. Set to -1 for getting",
            "            the entire log.",
            "        timeout: Max timeout for requests made when getting the logs.",
            "        suffix: The suffix of the log file if query by id of tasks/workers/actors. Default to \"out\".",
            "        encoding: The encoding used to decode the content of the log file. Default is",
            "            \"utf-8\". Use None to get binary data directly.",
            "        errors: The error handling scheme to use for decoding errors. Default is",
            "            \"strict\". See https://docs.python.org/3/library/codecs.html#error-handlers",
            "        submission_id: Job submission ID if getting log from a submission job.",
            "        attempt_number: The attempt number of the task if getting logs generated by a task.",
            "        _interval: The interval in secs to print new logs when `follow=True`.",
            "",
            "    Return:",
            "        A Generator of log line, None for SendType and ReturnType.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "",
            "    api_server_url = ray_address_to_api_server_url(address)",
            "    media_type = \"stream\" if follow else \"file\"",
            "",
            "    options = GetLogOptions(",
            "        node_id=node_id,",
            "        node_ip=node_ip,",
            "        filename=filename,",
            "        actor_id=actor_id,",
            "        task_id=task_id,",
            "        pid=pid,",
            "        lines=tail,",
            "        interval=_interval,",
            "        media_type=media_type,",
            "        timeout=timeout,",
            "        suffix=suffix,",
            "        submission_id=submission_id,",
            "        attempt_number=attempt_number,",
            "    )",
            "    options_dict = {}",
            "    for field in fields(options):",
            "        option_val = getattr(options, field.name)",
            "        if option_val is not None:",
            "            options_dict[field.name] = option_val",
            "",
            "    with requests.get(",
            "        f\"{api_server_url}/api/v0/logs/{media_type}?\"",
            "        f\"{urllib.parse.urlencode(options_dict)}\",",
            "        stream=True,",
            "    ) as r:",
            "        if r.status_code != 200:",
            "            raise RayStateApiException(r.text)",
            "        for bytes in r.iter_content(chunk_size=None):",
            "            bytes = bytearray(bytes)",
            "            # First byte 1 means success.",
            "            if bytes.startswith(b\"1\"):",
            "                bytes.pop(0)",
            "                logs = bytes",
            "                if encoding is not None:",
            "                    logs = bytes.decode(encoding=encoding, errors=errors)",
            "            else:",
            "                assert bytes.startswith(b\"0\")",
            "                error_msg = bytes.decode(\"utf-8\")",
            "                raise RayStateApiException(error_msg)",
            "            yield logs",
            "",
            "",
            "@DeveloperAPI",
            "def list_logs(",
            "    address: Optional[str] = None,",
            "    node_id: Optional[str] = None,",
            "    node_ip: Optional[str] = None,",
            "    glob_filter: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            ") -> Dict[str, List[str]]:",
            "    \"\"\"Listing log files available.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If not specified, it will be retrieved from the initialized ray cluster.",
            "        node_id: Id of the node containing the logs.",
            "        node_ip: Ip of the node containing the logs.",
            "        glob_filter: Name of the file (relative to the ray log directory) to be",
            "            retrieved. E.g. `glob_filter=\"*worker*\"` for all worker logs.",
            "        actor_id: Id of the actor if getting logs from an actor.",
            "        timeout: Max timeout for requests made when getting the logs.",
            "        _interval: The interval in secs to print new logs when `follow=True`.",
            "",
            "    Return:",
            "        A dictionary where the keys are log groups (e.g. gcs, raylet, worker), and",
            "        values are list of log filenames.",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data, or ConnectionError if failed to resolve the",
            "            ray address.",
            "    \"\"\"  # noqa: E501",
            "    assert (",
            "        node_ip is not None or node_id is not None",
            "    ), \"At least one of node ip and node id is required\"",
            "",
            "    api_server_url = ray_address_to_api_server_url(address)",
            "",
            "    if not glob_filter:",
            "        glob_filter = \"*\"",
            "",
            "    options_dict = {}",
            "    if node_ip:",
            "        options_dict[\"node_ip\"] = node_ip",
            "    if node_id:",
            "        options_dict[\"node_id\"] = node_id",
            "    if glob_filter:",
            "        options_dict[\"glob\"] = glob_filter",
            "    options_dict[\"timeout\"] = timeout",
            "",
            "    r = requests.get(",
            "        f\"{api_server_url}/api/v0/logs?{urllib.parse.urlencode(options_dict)}\"",
            "    )",
            "    # TODO(rickyx): we could do better at error handling here.",
            "    r.raise_for_status()",
            "",
            "    response = r.json()",
            "    if response[\"result\"] is False:",
            "        raise RayStateApiException(",
            "            \"API server internal error. See dashboard.log file for more details. \"",
            "            f\"Error: {response['msg']}\"",
            "        )",
            "    return response[\"data\"][\"result\"]",
            "",
            "",
            "\"\"\"",
            "Summary APIs",
            "\"\"\"",
            "",
            "",
            "@DeveloperAPI",
            "def summarize_tasks(",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> Dict:",
            "    \"\"\"Summarize the tasks in cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout for requests made when getting the states.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Return:",
            "        Dictionarified",
            "        :class:`~ray.util.state.common.TaskSummaries`",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",
            "            if the CLI is failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).summary(",
            "        SummaryResource.TASKS,",
            "        options=SummaryApiOptions(timeout=timeout),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def summarize_actors(",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> Dict:",
            "    \"\"\"Summarize the actors in cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout for requests made when getting the states.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Return:",
            "        Dictionarified",
            "        :class:`~ray.util.state.common.ActorSummaries`",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).summary(",
            "        SummaryResource.ACTORS,",
            "        options=SummaryApiOptions(timeout=timeout),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )",
            "",
            "",
            "@DeveloperAPI",
            "def summarize_objects(",
            "    address: Optional[str] = None,",
            "    timeout: int = DEFAULT_RPC_TIMEOUT,",
            "    raise_on_missing_output: bool = True,",
            "    _explain: bool = False,",
            ") -> Dict:",
            "    \"\"\"Summarize the objects in cluster.",
            "",
            "    Args:",
            "        address: Ray bootstrap address, could be `auto`, `localhost:6379`.",
            "            If None, it will be resolved automatically from an initialized ray.",
            "        timeout: Max timeout for requests made when getting the states.",
            "        raise_on_missing_output: When True, exceptions will be raised if",
            "            there is missing data due to truncation/data source unavailable.",
            "        _explain: Print the API information such as API latency or",
            "            failed query information.",
            "",
            "    Return:",
            "        Dictionarified :class:`~ray.util.state.common.ObjectSummaries`",
            "",
            "    Raises:",
            "        Exceptions: :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>` if the CLI",
            "            failed to query the data.",
            "    \"\"\"  # noqa: E501",
            "    return StateApiClient(address=address).summary(",
            "        SummaryResource.OBJECTS,",
            "        options=SummaryApiOptions(timeout=timeout),",
            "        raise_on_missing_output=raise_on_missing_output,",
            "        _explain=_explain,",
            "    )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "python.ray.util.state.api.list_logs.options_dict",
            "src.PIL.IcnsImagePlugin.IcnsFile.dataforsize"
        ]
    },
    "python/ray/util/state/state_manager.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 477,
                "afterPatchRowNumber": 477,
                "PatchRowcode": "             timeout=timeout,"
            },
            "1": {
                "beforePatchRowNumber": 478,
                "afterPatchRowNumber": 478,
                "PatchRowcode": "         )"
            },
            "2": {
                "beforePatchRowNumber": 479,
                "afterPatchRowNumber": 479,
                "PatchRowcode": "         metadata = await stream.initial_metadata()"
            },
            "3": {
                "beforePatchRowNumber": 480,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if metadata.get(log_consts.LOG_GRPC_ERROR) == log_consts.FILE_NOT_FOUND:"
            },
            "4": {
                "beforePatchRowNumber": 481,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            raise ValueError(f'File \"{log_file_name}\" not found on node {node_id}')"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 480,
                "PatchRowcode": "+        if metadata.get(log_consts.LOG_GRPC_ERROR) is not None:"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 481,
                "PatchRowcode": "+            raise ValueError(metadata.get(log_consts.LOG_GRPC_ERROR))"
            },
            "7": {
                "beforePatchRowNumber": 482,
                "afterPatchRowNumber": 482,
                "PatchRowcode": "         return stream"
            }
        },
        "frontPatchFile": [
            "import dataclasses",
            "import inspect",
            "import logging",
            "from collections import defaultdict",
            "from functools import wraps",
            "from typing import List, Optional, Tuple",
            "",
            "import aiohttp",
            "import grpc",
            "from grpc.aio._call import UnaryStreamCall",
            "",
            "import ray",
            "import ray.dashboard.modules.log.log_consts as log_consts",
            "from ray._private import ray_constants",
            "from ray._private.gcs_utils import GcsAioClient",
            "from ray._private.utils import hex_to_binary",
            "from ray._raylet import ActorID, JobID, TaskID",
            "from ray.core.generated import gcs_service_pb2_grpc",
            "from ray.core.generated.gcs_pb2 import ActorTableData",
            "from ray.core.generated.gcs_service_pb2 import (",
            "    GetAllActorInfoReply,",
            "    GetAllActorInfoRequest,",
            "    GetAllNodeInfoReply,",
            "    GetAllNodeInfoRequest,",
            "    GetAllPlacementGroupReply,",
            "    GetAllPlacementGroupRequest,",
            "    GetAllWorkerInfoReply,",
            "    GetAllWorkerInfoRequest,",
            "    GetTaskEventsReply,",
            "    GetTaskEventsRequest,",
            ")",
            "from ray.core.generated.node_manager_pb2 import (",
            "    GetObjectsInfoReply,",
            "    GetObjectsInfoRequest,",
            "    GetTasksInfoReply,",
            "    GetTasksInfoRequest,",
            ")",
            "from ray.core.generated.node_manager_pb2_grpc import NodeManagerServiceStub",
            "from ray.core.generated.reporter_pb2 import (",
            "    ListLogsReply,",
            "    ListLogsRequest,",
            "    StreamLogRequest,",
            ")",
            "from ray.core.generated.reporter_pb2_grpc import LogServiceStub",
            "from ray.core.generated.runtime_env_agent_pb2 import (",
            "    GetRuntimeEnvsInfoReply,",
            "    GetRuntimeEnvsInfoRequest,",
            ")",
            "from ray.dashboard.datacenter import DataSource",
            "from ray.dashboard.modules.job.common import JobInfoStorageClient",
            "from ray.dashboard.modules.job.pydantic_models import JobDetails, JobType",
            "from ray.dashboard.modules.job.utils import get_driver_jobs",
            "from ray.dashboard.utils import Dict as Dictionary",
            "from ray.util.state.common import (",
            "    RAY_MAX_LIMIT_FROM_DATA_SOURCE,",
            "    PredicateType,",
            "    SupportedFilterType,",
            ")",
            "from ray.util.state.exception import DataSourceUnavailable",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "_STATE_MANAGER_GRPC_OPTIONS = [",
            "    *ray_constants.GLOBAL_GRPC_OPTIONS,",
            "    (\"grpc.max_send_message_length\", ray_constants.GRPC_CPP_MAX_MESSAGE_SIZE),",
            "    (\"grpc.max_receive_message_length\", ray_constants.GRPC_CPP_MAX_MESSAGE_SIZE),",
            "]",
            "",
            "",
            "def handle_grpc_network_errors(func):",
            "    \"\"\"Decorator to add a network handling logic.",
            "",
            "    It is a helper method for `StateDataSourceClient`.",
            "    The method can only be used for async methods.",
            "    \"\"\"",
            "    assert inspect.iscoroutinefunction(func)",
            "",
            "    @wraps(func)",
            "    async def api_with_network_error_handler(*args, **kwargs):",
            "        \"\"\"Apply the network error handling logic to each APIs,",
            "        such as retry or exception policies.",
            "",
            "        Returns:",
            "            If RPC succeeds, it returns what the original function returns.",
            "            If RPC fails, it raises exceptions.",
            "        Exceptions:",
            "            DataSourceUnavailable: if the source is unavailable because it is down",
            "                or there's a slow network issue causing timeout.",
            "            Otherwise, the raw network exceptions (e.g., gRPC) will be raised.",
            "        \"\"\"",
            "        try:",
            "            return await func(*args, **kwargs)",
            "        except grpc.aio.AioRpcError as e:",
            "            if (",
            "                e.code() == grpc.StatusCode.DEADLINE_EXCEEDED",
            "                or e.code() == grpc.StatusCode.UNAVAILABLE",
            "            ):",
            "                raise DataSourceUnavailable(",
            "                    \"Failed to query the data source. \"",
            "                    \"It is either there's a network issue, or the source is down.\"",
            "                )",
            "            else:",
            "                logger.exception(e)",
            "                raise e",
            "",
            "    return api_with_network_error_handler",
            "",
            "",
            "class IdToIpMap:",
            "    def __init__(self):",
            "        # Node IP to node ID mapping.",
            "        self._ip_to_node_id = defaultdict(str)",
            "        # Node ID to node IP mapping.",
            "        self._node_id_to_ip = defaultdict(str)",
            "",
            "    def put(self, node_id: str, address: str):",
            "        self._ip_to_node_id[address] = node_id",
            "        self._node_id_to_ip[node_id] = address",
            "",
            "    def get_ip(self, node_id: str):",
            "        return self._node_id_to_ip.get(node_id)",
            "",
            "    def get_node_id(self, address: str):",
            "        return self._ip_to_node_id.get(address)",
            "",
            "    def pop(self, node_id: str):",
            "        \"\"\"Pop the given node id.",
            "",
            "        Returns:",
            "            False if the corresponding node id doesn't exist.",
            "            True if it pops correctly.",
            "        \"\"\"",
            "        ip = self._node_id_to_ip.get(node_id)",
            "        if not ip:",
            "            return None",
            "        assert ip in self._ip_to_node_id",
            "        self._node_id_to_ip.pop(node_id)",
            "        self._ip_to_node_id.pop(ip)",
            "        return True",
            "",
            "",
            "class StateDataSourceClient:",
            "    \"\"\"The client to query states from various data sources such as Raylet, GCS, Agents.",
            "",
            "    Note that it doesn't directly query core workers. They are proxied through raylets.",
            "",
            "    The module is not in charge of service discovery. The caller is responsible for",
            "    finding services and register stubs through `register*` APIs.",
            "",
            "    Non `register*` APIs",
            "    - Return the protobuf directly if it succeeds to query the source.",
            "    - Raises an exception if there's any network issue.",
            "    - throw a ValueError if it cannot find the source.",
            "    \"\"\"",
            "",
            "    def __init__(self, gcs_channel: grpc.aio.Channel, gcs_aio_client: GcsAioClient):",
            "        self.register_gcs_client(gcs_channel)",
            "        self._raylet_stubs = {}",
            "        self._runtime_env_agent_addresses = {}  # {node_id -> url}",
            "        self._log_agent_stub = {}",
            "        self._job_client = JobInfoStorageClient(gcs_aio_client)",
            "        self._id_id_map = IdToIpMap()",
            "        self._gcs_aio_client = gcs_aio_client",
            "        self._client_session = aiohttp.ClientSession()",
            "",
            "    def register_gcs_client(self, gcs_channel: grpc.aio.Channel):",
            "        self._gcs_actor_info_stub = gcs_service_pb2_grpc.ActorInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_pg_info_stub = gcs_service_pb2_grpc.PlacementGroupInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_node_info_stub = gcs_service_pb2_grpc.NodeInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_worker_info_stub = gcs_service_pb2_grpc.WorkerInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_task_info_stub = gcs_service_pb2_grpc.TaskInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "",
            "    def register_raylet_client(",
            "        self, node_id: str, address: str, port: int, runtime_env_agent_port: int",
            "    ):",
            "        full_addr = f\"{address}:{port}\"",
            "        options = _STATE_MANAGER_GRPC_OPTIONS",
            "        channel = ray._private.utils.init_grpc_channel(",
            "            full_addr, options, asynchronous=True",
            "        )",
            "        self._raylet_stubs[node_id] = NodeManagerServiceStub(channel)",
            "        # TODO(ryw): runtime env agent is on the raylet's address, not node manager's.",
            "        # So the correct way is to use",
            "        # f\"http://{raylet_ip_address}:{runtime_env_agent_port}\".",
            "        # However we don't have a good way to get *all* node's raylet_ip_address, as",
            "        # this value is not exposed in GcsNodeInfo and hence isn't available via",
            "        # GetClusterInfo. In practice, this should not matter a lot until we see a",
            "        # raylet ip != node manager ip case, which should break more thing than just",
            "        # runtime env agent connectivity.",
            "        self._runtime_env_agent_addresses[",
            "            node_id",
            "        ] = f\"http://{address}:{runtime_env_agent_port}\"",
            "        self._id_id_map.put(node_id, address)",
            "",
            "    def unregister_raylet_client(self, node_id: str):",
            "        self._raylet_stubs.pop(node_id)",
            "        self._runtime_env_agent_addresses.pop(node_id)",
            "        self._id_id_map.pop(node_id)",
            "",
            "    def register_agent_client(self, node_id, address: str, port: int):",
            "        options = _STATE_MANAGER_GRPC_OPTIONS",
            "        channel = ray._private.utils.init_grpc_channel(",
            "            f\"{address}:{port}\", options=options, asynchronous=True",
            "        )",
            "        self._log_agent_stub[node_id] = LogServiceStub(channel)",
            "        self._id_id_map.put(node_id, address)",
            "",
            "    def unregister_agent_client(self, node_id: str):",
            "        self._log_agent_stub.pop(node_id)",
            "        self._id_id_map.pop(node_id)",
            "",
            "    def get_all_registered_raylet_ids(self) -> List[str]:",
            "        return self._raylet_stubs.keys()",
            "",
            "    # Returns all node_ids who has runtime_env_agent listening.",
            "    def get_all_registered_runtime_env_agent_ids(self) -> List[str]:",
            "        return self._runtime_env_agent_addresses.keys()",
            "",
            "    # Returns all nod_ids which registered their log_agent_stub.",
            "    def get_all_registered_log_agent_ids(self) -> List[str]:",
            "        return self._log_agent_stub.keys()",
            "",
            "    def ip_to_node_id(self, ip: Optional[str]) -> Optional[str]:",
            "        \"\"\"Return the node id that corresponds to the given ip.",
            "",
            "        Args:",
            "            ip: The ip address.",
            "",
            "        Returns:",
            "            None if the corresponding id doesn't exist.",
            "            Node id otherwise. If None node_ip is given,",
            "            it will also return None.",
            "        \"\"\"",
            "        if not ip:",
            "            return None",
            "        return self._id_id_map.get_node_id(ip)",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_actor_info(",
            "        self,",
            "        timeout: int = None,",
            "        limit: int = None,",
            "        filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    ) -> Optional[GetAllActorInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "        if filters is None:",
            "            filters = []",
            "",
            "        req_filters = GetAllActorInfoRequest.Filters()",
            "        for filter in filters:",
            "            key, predicate, value = filter",
            "            if predicate != \"=\":",
            "                # We only support EQUAL predicate for source side filtering.",
            "                continue",
            "            if key == \"actor_id\":",
            "                req_filters.actor_id = ActorID(hex_to_binary(value)).binary()",
            "            elif key == \"state\":",
            "                # Convert to uppercase.",
            "                value = value.upper()",
            "                if value not in ActorTableData.ActorState.keys():",
            "                    raise ValueError(f\"Invalid actor state for filtering: {value}\")",
            "                req_filters.state = ActorTableData.ActorState.Value(value)",
            "            elif key == \"job_id\":",
            "                req_filters.job_id = JobID(hex_to_binary(value)).binary()",
            "",
            "        request = GetAllActorInfoRequest(limit=limit, filters=req_filters)",
            "        reply = await self._gcs_actor_info_stub.GetAllActorInfo(",
            "            request, timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_task_info(",
            "        self,",
            "        timeout: int = None,",
            "        limit: int = None,",
            "        filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "        exclude_driver: bool = False,",
            "    ) -> Optional[GetTaskEventsReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        if filters is None:",
            "            filters = []",
            "",
            "        req_filters = GetTaskEventsRequest.Filters()",
            "        for filter in filters:",
            "            key, predicate, value = filter",
            "            if predicate != \"=\":",
            "                # We only support EQUAL predicate for source side filtering.",
            "                continue",
            "",
            "            if key == \"actor_id\":",
            "                req_filters.actor_id = ActorID(hex_to_binary(value)).binary()",
            "            elif key == \"job_id\":",
            "                req_filters.job_id = JobID(hex_to_binary(value)).binary()",
            "            elif key == \"task_id\":",
            "                req_filters.task_ids.append(TaskID(hex_to_binary(value)).binary())",
            "            else:",
            "                continue",
            "",
            "        req_filters.exclude_driver = exclude_driver",
            "",
            "        request = GetTaskEventsRequest(limit=limit, filters=req_filters)",
            "        reply = await self._gcs_task_info_stub.GetTaskEvents(request, timeout=timeout)",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_placement_group_info(",
            "        self, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetAllPlacementGroupReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        request = GetAllPlacementGroupRequest(limit=limit)",
            "        reply = await self._gcs_pg_info_stub.GetAllPlacementGroup(",
            "            request, timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_node_info(",
            "        self, timeout: int = None",
            "    ) -> Optional[GetAllNodeInfoReply]:",
            "        request = GetAllNodeInfoRequest()",
            "        reply = await self._gcs_node_info_stub.GetAllNodeInfo(request, timeout=timeout)",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_worker_info(",
            "        self, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetAllWorkerInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        request = GetAllWorkerInfoRequest(limit=limit)",
            "        reply = await self._gcs_worker_info_stub.GetAllWorkerInfo(",
            "            request, timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    # TODO(rickyx):",
            "    # This is currently mirroring dashboard/modules/job/job_head.py::list_jobs",
            "    # We should eventually unify the logic.",
            "    async def get_job_info(self, timeout: int = None) -> List[JobDetails]:",
            "        # Cannot use @handle_grpc_network_errors because async def is not supported yet.",
            "",
            "        driver_jobs, submission_job_drivers = await get_driver_jobs(",
            "            self._gcs_aio_client, timeout=timeout",
            "        )",
            "        submission_jobs = await self._job_client.get_all_jobs(timeout=timeout)",
            "        submission_jobs = [",
            "            JobDetails(",
            "                **dataclasses.asdict(job),",
            "                submission_id=submission_id,",
            "                job_id=submission_job_drivers.get(submission_id).id",
            "                if submission_id in submission_job_drivers",
            "                else None,",
            "                driver_info=submission_job_drivers.get(submission_id),",
            "                type=JobType.SUBMISSION,",
            "            )",
            "            for submission_id, job in submission_jobs.items()",
            "        ]",
            "",
            "        return list(driver_jobs.values()) + submission_jobs",
            "",
            "    async def get_all_cluster_events(self) -> Dictionary:",
            "        return DataSource.events",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_task_info(",
            "        self, node_id: str, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetTasksInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "        stub = self._raylet_stubs.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Raylet for a node id, {node_id} doesn't exist.\")",
            "",
            "        reply = await stub.GetTasksInfo(",
            "            GetTasksInfoRequest(limit=limit), timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_object_info(",
            "        self, node_id: str, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetObjectsInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        stub = self._raylet_stubs.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Raylet for a node id, {node_id} doesn't exist.\")",
            "",
            "        reply = await stub.GetObjectsInfo(",
            "            GetObjectsInfoRequest(limit=limit),",
            "            timeout=timeout,",
            "        )",
            "        return reply",
            "",
            "    async def get_runtime_envs_info(",
            "        self, node_id: str, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetRuntimeEnvsInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        address = self._runtime_env_agent_addresses.get(node_id)",
            "        if not address:",
            "            raise ValueError(",
            "                f\"Runtime Env Agent for a node id, {node_id} doesn't exist.\"",
            "            )",
            "        timeout = aiohttp.ClientTimeout(total=timeout)",
            "        url = f\"{address}/get_runtime_envs_info\"",
            "        request = GetRuntimeEnvsInfoRequest(limit=limit)",
            "        data = request.SerializeToString()",
            "        async with self._client_session.post(url, data=data, timeout=timeout) as resp:",
            "            if resp.status >= 200 and resp.status < 300:",
            "                response_data = await resp.read()",
            "                reply = GetRuntimeEnvsInfoReply()",
            "                reply.ParseFromString(response_data)",
            "                return reply",
            "            else:",
            "                raise DataSourceUnavailable(",
            "                    \"Failed to query the runtime env agent for get_runtime_envs_info. \"",
            "                    \"Either there's a network issue, or the source is down. \"",
            "                    f\"Response is {resp.status}, reason {resp.reason}\"",
            "                )",
            "",
            "    @handle_grpc_network_errors",
            "    async def list_logs(",
            "        self, node_id: str, glob_filter: str, timeout: int = None",
            "    ) -> ListLogsReply:",
            "        stub = self._log_agent_stub.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Agent for node id: {node_id} doesn't exist.\")",
            "        return await stub.ListLogs(",
            "            ListLogsRequest(glob_filter=glob_filter), timeout=timeout",
            "        )",
            "",
            "    @handle_grpc_network_errors",
            "    async def stream_log(",
            "        self,",
            "        node_id: str,",
            "        log_file_name: str,",
            "        keep_alive: bool,",
            "        lines: int,",
            "        interval: Optional[float],",
            "        timeout: int,",
            "        start_offset: Optional[int] = None,",
            "        end_offset: Optional[int] = None,",
            "    ) -> UnaryStreamCall:",
            "        stub = self._log_agent_stub.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Agent for node id: {node_id} doesn't exist.\")",
            "",
            "        stream = stub.StreamLog(",
            "            StreamLogRequest(",
            "                keep_alive=keep_alive,",
            "                log_file_name=log_file_name,",
            "                lines=lines,",
            "                interval=interval,",
            "                start_offset=start_offset,",
            "                end_offset=end_offset,",
            "            ),",
            "            timeout=timeout,",
            "        )",
            "        metadata = await stream.initial_metadata()",
            "        if metadata.get(log_consts.LOG_GRPC_ERROR) == log_consts.FILE_NOT_FOUND:",
            "            raise ValueError(f'File \"{log_file_name}\" not found on node {node_id}')",
            "        return stream"
        ],
        "afterPatchFile": [
            "import dataclasses",
            "import inspect",
            "import logging",
            "from collections import defaultdict",
            "from functools import wraps",
            "from typing import List, Optional, Tuple",
            "",
            "import aiohttp",
            "import grpc",
            "from grpc.aio._call import UnaryStreamCall",
            "",
            "import ray",
            "import ray.dashboard.modules.log.log_consts as log_consts",
            "from ray._private import ray_constants",
            "from ray._private.gcs_utils import GcsAioClient",
            "from ray._private.utils import hex_to_binary",
            "from ray._raylet import ActorID, JobID, TaskID",
            "from ray.core.generated import gcs_service_pb2_grpc",
            "from ray.core.generated.gcs_pb2 import ActorTableData",
            "from ray.core.generated.gcs_service_pb2 import (",
            "    GetAllActorInfoReply,",
            "    GetAllActorInfoRequest,",
            "    GetAllNodeInfoReply,",
            "    GetAllNodeInfoRequest,",
            "    GetAllPlacementGroupReply,",
            "    GetAllPlacementGroupRequest,",
            "    GetAllWorkerInfoReply,",
            "    GetAllWorkerInfoRequest,",
            "    GetTaskEventsReply,",
            "    GetTaskEventsRequest,",
            ")",
            "from ray.core.generated.node_manager_pb2 import (",
            "    GetObjectsInfoReply,",
            "    GetObjectsInfoRequest,",
            "    GetTasksInfoReply,",
            "    GetTasksInfoRequest,",
            ")",
            "from ray.core.generated.node_manager_pb2_grpc import NodeManagerServiceStub",
            "from ray.core.generated.reporter_pb2 import (",
            "    ListLogsReply,",
            "    ListLogsRequest,",
            "    StreamLogRequest,",
            ")",
            "from ray.core.generated.reporter_pb2_grpc import LogServiceStub",
            "from ray.core.generated.runtime_env_agent_pb2 import (",
            "    GetRuntimeEnvsInfoReply,",
            "    GetRuntimeEnvsInfoRequest,",
            ")",
            "from ray.dashboard.datacenter import DataSource",
            "from ray.dashboard.modules.job.common import JobInfoStorageClient",
            "from ray.dashboard.modules.job.pydantic_models import JobDetails, JobType",
            "from ray.dashboard.modules.job.utils import get_driver_jobs",
            "from ray.dashboard.utils import Dict as Dictionary",
            "from ray.util.state.common import (",
            "    RAY_MAX_LIMIT_FROM_DATA_SOURCE,",
            "    PredicateType,",
            "    SupportedFilterType,",
            ")",
            "from ray.util.state.exception import DataSourceUnavailable",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "_STATE_MANAGER_GRPC_OPTIONS = [",
            "    *ray_constants.GLOBAL_GRPC_OPTIONS,",
            "    (\"grpc.max_send_message_length\", ray_constants.GRPC_CPP_MAX_MESSAGE_SIZE),",
            "    (\"grpc.max_receive_message_length\", ray_constants.GRPC_CPP_MAX_MESSAGE_SIZE),",
            "]",
            "",
            "",
            "def handle_grpc_network_errors(func):",
            "    \"\"\"Decorator to add a network handling logic.",
            "",
            "    It is a helper method for `StateDataSourceClient`.",
            "    The method can only be used for async methods.",
            "    \"\"\"",
            "    assert inspect.iscoroutinefunction(func)",
            "",
            "    @wraps(func)",
            "    async def api_with_network_error_handler(*args, **kwargs):",
            "        \"\"\"Apply the network error handling logic to each APIs,",
            "        such as retry or exception policies.",
            "",
            "        Returns:",
            "            If RPC succeeds, it returns what the original function returns.",
            "            If RPC fails, it raises exceptions.",
            "        Exceptions:",
            "            DataSourceUnavailable: if the source is unavailable because it is down",
            "                or there's a slow network issue causing timeout.",
            "            Otherwise, the raw network exceptions (e.g., gRPC) will be raised.",
            "        \"\"\"",
            "        try:",
            "            return await func(*args, **kwargs)",
            "        except grpc.aio.AioRpcError as e:",
            "            if (",
            "                e.code() == grpc.StatusCode.DEADLINE_EXCEEDED",
            "                or e.code() == grpc.StatusCode.UNAVAILABLE",
            "            ):",
            "                raise DataSourceUnavailable(",
            "                    \"Failed to query the data source. \"",
            "                    \"It is either there's a network issue, or the source is down.\"",
            "                )",
            "            else:",
            "                logger.exception(e)",
            "                raise e",
            "",
            "    return api_with_network_error_handler",
            "",
            "",
            "class IdToIpMap:",
            "    def __init__(self):",
            "        # Node IP to node ID mapping.",
            "        self._ip_to_node_id = defaultdict(str)",
            "        # Node ID to node IP mapping.",
            "        self._node_id_to_ip = defaultdict(str)",
            "",
            "    def put(self, node_id: str, address: str):",
            "        self._ip_to_node_id[address] = node_id",
            "        self._node_id_to_ip[node_id] = address",
            "",
            "    def get_ip(self, node_id: str):",
            "        return self._node_id_to_ip.get(node_id)",
            "",
            "    def get_node_id(self, address: str):",
            "        return self._ip_to_node_id.get(address)",
            "",
            "    def pop(self, node_id: str):",
            "        \"\"\"Pop the given node id.",
            "",
            "        Returns:",
            "            False if the corresponding node id doesn't exist.",
            "            True if it pops correctly.",
            "        \"\"\"",
            "        ip = self._node_id_to_ip.get(node_id)",
            "        if not ip:",
            "            return None",
            "        assert ip in self._ip_to_node_id",
            "        self._node_id_to_ip.pop(node_id)",
            "        self._ip_to_node_id.pop(ip)",
            "        return True",
            "",
            "",
            "class StateDataSourceClient:",
            "    \"\"\"The client to query states from various data sources such as Raylet, GCS, Agents.",
            "",
            "    Note that it doesn't directly query core workers. They are proxied through raylets.",
            "",
            "    The module is not in charge of service discovery. The caller is responsible for",
            "    finding services and register stubs through `register*` APIs.",
            "",
            "    Non `register*` APIs",
            "    - Return the protobuf directly if it succeeds to query the source.",
            "    - Raises an exception if there's any network issue.",
            "    - throw a ValueError if it cannot find the source.",
            "    \"\"\"",
            "",
            "    def __init__(self, gcs_channel: grpc.aio.Channel, gcs_aio_client: GcsAioClient):",
            "        self.register_gcs_client(gcs_channel)",
            "        self._raylet_stubs = {}",
            "        self._runtime_env_agent_addresses = {}  # {node_id -> url}",
            "        self._log_agent_stub = {}",
            "        self._job_client = JobInfoStorageClient(gcs_aio_client)",
            "        self._id_id_map = IdToIpMap()",
            "        self._gcs_aio_client = gcs_aio_client",
            "        self._client_session = aiohttp.ClientSession()",
            "",
            "    def register_gcs_client(self, gcs_channel: grpc.aio.Channel):",
            "        self._gcs_actor_info_stub = gcs_service_pb2_grpc.ActorInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_pg_info_stub = gcs_service_pb2_grpc.PlacementGroupInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_node_info_stub = gcs_service_pb2_grpc.NodeInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_worker_info_stub = gcs_service_pb2_grpc.WorkerInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "        self._gcs_task_info_stub = gcs_service_pb2_grpc.TaskInfoGcsServiceStub(",
            "            gcs_channel",
            "        )",
            "",
            "    def register_raylet_client(",
            "        self, node_id: str, address: str, port: int, runtime_env_agent_port: int",
            "    ):",
            "        full_addr = f\"{address}:{port}\"",
            "        options = _STATE_MANAGER_GRPC_OPTIONS",
            "        channel = ray._private.utils.init_grpc_channel(",
            "            full_addr, options, asynchronous=True",
            "        )",
            "        self._raylet_stubs[node_id] = NodeManagerServiceStub(channel)",
            "        # TODO(ryw): runtime env agent is on the raylet's address, not node manager's.",
            "        # So the correct way is to use",
            "        # f\"http://{raylet_ip_address}:{runtime_env_agent_port}\".",
            "        # However we don't have a good way to get *all* node's raylet_ip_address, as",
            "        # this value is not exposed in GcsNodeInfo and hence isn't available via",
            "        # GetClusterInfo. In practice, this should not matter a lot until we see a",
            "        # raylet ip != node manager ip case, which should break more thing than just",
            "        # runtime env agent connectivity.",
            "        self._runtime_env_agent_addresses[",
            "            node_id",
            "        ] = f\"http://{address}:{runtime_env_agent_port}\"",
            "        self._id_id_map.put(node_id, address)",
            "",
            "    def unregister_raylet_client(self, node_id: str):",
            "        self._raylet_stubs.pop(node_id)",
            "        self._runtime_env_agent_addresses.pop(node_id)",
            "        self._id_id_map.pop(node_id)",
            "",
            "    def register_agent_client(self, node_id, address: str, port: int):",
            "        options = _STATE_MANAGER_GRPC_OPTIONS",
            "        channel = ray._private.utils.init_grpc_channel(",
            "            f\"{address}:{port}\", options=options, asynchronous=True",
            "        )",
            "        self._log_agent_stub[node_id] = LogServiceStub(channel)",
            "        self._id_id_map.put(node_id, address)",
            "",
            "    def unregister_agent_client(self, node_id: str):",
            "        self._log_agent_stub.pop(node_id)",
            "        self._id_id_map.pop(node_id)",
            "",
            "    def get_all_registered_raylet_ids(self) -> List[str]:",
            "        return self._raylet_stubs.keys()",
            "",
            "    # Returns all node_ids who has runtime_env_agent listening.",
            "    def get_all_registered_runtime_env_agent_ids(self) -> List[str]:",
            "        return self._runtime_env_agent_addresses.keys()",
            "",
            "    # Returns all nod_ids which registered their log_agent_stub.",
            "    def get_all_registered_log_agent_ids(self) -> List[str]:",
            "        return self._log_agent_stub.keys()",
            "",
            "    def ip_to_node_id(self, ip: Optional[str]) -> Optional[str]:",
            "        \"\"\"Return the node id that corresponds to the given ip.",
            "",
            "        Args:",
            "            ip: The ip address.",
            "",
            "        Returns:",
            "            None if the corresponding id doesn't exist.",
            "            Node id otherwise. If None node_ip is given,",
            "            it will also return None.",
            "        \"\"\"",
            "        if not ip:",
            "            return None",
            "        return self._id_id_map.get_node_id(ip)",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_actor_info(",
            "        self,",
            "        timeout: int = None,",
            "        limit: int = None,",
            "        filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "    ) -> Optional[GetAllActorInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "        if filters is None:",
            "            filters = []",
            "",
            "        req_filters = GetAllActorInfoRequest.Filters()",
            "        for filter in filters:",
            "            key, predicate, value = filter",
            "            if predicate != \"=\":",
            "                # We only support EQUAL predicate for source side filtering.",
            "                continue",
            "            if key == \"actor_id\":",
            "                req_filters.actor_id = ActorID(hex_to_binary(value)).binary()",
            "            elif key == \"state\":",
            "                # Convert to uppercase.",
            "                value = value.upper()",
            "                if value not in ActorTableData.ActorState.keys():",
            "                    raise ValueError(f\"Invalid actor state for filtering: {value}\")",
            "                req_filters.state = ActorTableData.ActorState.Value(value)",
            "            elif key == \"job_id\":",
            "                req_filters.job_id = JobID(hex_to_binary(value)).binary()",
            "",
            "        request = GetAllActorInfoRequest(limit=limit, filters=req_filters)",
            "        reply = await self._gcs_actor_info_stub.GetAllActorInfo(",
            "            request, timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_task_info(",
            "        self,",
            "        timeout: int = None,",
            "        limit: int = None,",
            "        filters: Optional[List[Tuple[str, PredicateType, SupportedFilterType]]] = None,",
            "        exclude_driver: bool = False,",
            "    ) -> Optional[GetTaskEventsReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        if filters is None:",
            "            filters = []",
            "",
            "        req_filters = GetTaskEventsRequest.Filters()",
            "        for filter in filters:",
            "            key, predicate, value = filter",
            "            if predicate != \"=\":",
            "                # We only support EQUAL predicate for source side filtering.",
            "                continue",
            "",
            "            if key == \"actor_id\":",
            "                req_filters.actor_id = ActorID(hex_to_binary(value)).binary()",
            "            elif key == \"job_id\":",
            "                req_filters.job_id = JobID(hex_to_binary(value)).binary()",
            "            elif key == \"task_id\":",
            "                req_filters.task_ids.append(TaskID(hex_to_binary(value)).binary())",
            "            else:",
            "                continue",
            "",
            "        req_filters.exclude_driver = exclude_driver",
            "",
            "        request = GetTaskEventsRequest(limit=limit, filters=req_filters)",
            "        reply = await self._gcs_task_info_stub.GetTaskEvents(request, timeout=timeout)",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_placement_group_info(",
            "        self, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetAllPlacementGroupReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        request = GetAllPlacementGroupRequest(limit=limit)",
            "        reply = await self._gcs_pg_info_stub.GetAllPlacementGroup(",
            "            request, timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_node_info(",
            "        self, timeout: int = None",
            "    ) -> Optional[GetAllNodeInfoReply]:",
            "        request = GetAllNodeInfoRequest()",
            "        reply = await self._gcs_node_info_stub.GetAllNodeInfo(request, timeout=timeout)",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_all_worker_info(",
            "        self, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetAllWorkerInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        request = GetAllWorkerInfoRequest(limit=limit)",
            "        reply = await self._gcs_worker_info_stub.GetAllWorkerInfo(",
            "            request, timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    # TODO(rickyx):",
            "    # This is currently mirroring dashboard/modules/job/job_head.py::list_jobs",
            "    # We should eventually unify the logic.",
            "    async def get_job_info(self, timeout: int = None) -> List[JobDetails]:",
            "        # Cannot use @handle_grpc_network_errors because async def is not supported yet.",
            "",
            "        driver_jobs, submission_job_drivers = await get_driver_jobs(",
            "            self._gcs_aio_client, timeout=timeout",
            "        )",
            "        submission_jobs = await self._job_client.get_all_jobs(timeout=timeout)",
            "        submission_jobs = [",
            "            JobDetails(",
            "                **dataclasses.asdict(job),",
            "                submission_id=submission_id,",
            "                job_id=submission_job_drivers.get(submission_id).id",
            "                if submission_id in submission_job_drivers",
            "                else None,",
            "                driver_info=submission_job_drivers.get(submission_id),",
            "                type=JobType.SUBMISSION,",
            "            )",
            "            for submission_id, job in submission_jobs.items()",
            "        ]",
            "",
            "        return list(driver_jobs.values()) + submission_jobs",
            "",
            "    async def get_all_cluster_events(self) -> Dictionary:",
            "        return DataSource.events",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_task_info(",
            "        self, node_id: str, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetTasksInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "        stub = self._raylet_stubs.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Raylet for a node id, {node_id} doesn't exist.\")",
            "",
            "        reply = await stub.GetTasksInfo(",
            "            GetTasksInfoRequest(limit=limit), timeout=timeout",
            "        )",
            "        return reply",
            "",
            "    @handle_grpc_network_errors",
            "    async def get_object_info(",
            "        self, node_id: str, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetObjectsInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        stub = self._raylet_stubs.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Raylet for a node id, {node_id} doesn't exist.\")",
            "",
            "        reply = await stub.GetObjectsInfo(",
            "            GetObjectsInfoRequest(limit=limit),",
            "            timeout=timeout,",
            "        )",
            "        return reply",
            "",
            "    async def get_runtime_envs_info(",
            "        self, node_id: str, timeout: int = None, limit: int = None",
            "    ) -> Optional[GetRuntimeEnvsInfoReply]:",
            "        if not limit:",
            "            limit = RAY_MAX_LIMIT_FROM_DATA_SOURCE",
            "",
            "        address = self._runtime_env_agent_addresses.get(node_id)",
            "        if not address:",
            "            raise ValueError(",
            "                f\"Runtime Env Agent for a node id, {node_id} doesn't exist.\"",
            "            )",
            "        timeout = aiohttp.ClientTimeout(total=timeout)",
            "        url = f\"{address}/get_runtime_envs_info\"",
            "        request = GetRuntimeEnvsInfoRequest(limit=limit)",
            "        data = request.SerializeToString()",
            "        async with self._client_session.post(url, data=data, timeout=timeout) as resp:",
            "            if resp.status >= 200 and resp.status < 300:",
            "                response_data = await resp.read()",
            "                reply = GetRuntimeEnvsInfoReply()",
            "                reply.ParseFromString(response_data)",
            "                return reply",
            "            else:",
            "                raise DataSourceUnavailable(",
            "                    \"Failed to query the runtime env agent for get_runtime_envs_info. \"",
            "                    \"Either there's a network issue, or the source is down. \"",
            "                    f\"Response is {resp.status}, reason {resp.reason}\"",
            "                )",
            "",
            "    @handle_grpc_network_errors",
            "    async def list_logs(",
            "        self, node_id: str, glob_filter: str, timeout: int = None",
            "    ) -> ListLogsReply:",
            "        stub = self._log_agent_stub.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Agent for node id: {node_id} doesn't exist.\")",
            "        return await stub.ListLogs(",
            "            ListLogsRequest(glob_filter=glob_filter), timeout=timeout",
            "        )",
            "",
            "    @handle_grpc_network_errors",
            "    async def stream_log(",
            "        self,",
            "        node_id: str,",
            "        log_file_name: str,",
            "        keep_alive: bool,",
            "        lines: int,",
            "        interval: Optional[float],",
            "        timeout: int,",
            "        start_offset: Optional[int] = None,",
            "        end_offset: Optional[int] = None,",
            "    ) -> UnaryStreamCall:",
            "        stub = self._log_agent_stub.get(node_id)",
            "        if not stub:",
            "            raise ValueError(f\"Agent for node id: {node_id} doesn't exist.\")",
            "",
            "        stream = stub.StreamLog(",
            "            StreamLogRequest(",
            "                keep_alive=keep_alive,",
            "                log_file_name=log_file_name,",
            "                lines=lines,",
            "                interval=interval,",
            "                start_offset=start_offset,",
            "                end_offset=end_offset,",
            "            ),",
            "            timeout=timeout,",
            "        )",
            "        metadata = await stream.initial_metadata()",
            "        if metadata.get(log_consts.LOG_GRPC_ERROR) is not None:",
            "            raise ValueError(metadata.get(log_consts.LOG_GRPC_ERROR))",
            "        return stream"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0"
        ],
        "dele_reviseLocation": {
            "480": [
                "StateDataSourceClient"
            ],
            "481": [
                "StateDataSourceClient"
            ]
        },
        "addLocation": []
    }
}