{
    "bleach/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " # yyyymmdd"
            },
            "3": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-__releasedate__ = \"20210126\""
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+__releasedate__ = \"20210201\""
            },
            "5": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " # x.y.z or x.y.z.dev0 -- semver"
            },
            "6": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-__version__ = \"3.2.3\""
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+__version__ = \"3.3.0\""
            },
            "8": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " VERSION = packaging.version.Version(__version__)"
            },
            "9": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "# -*- coding: utf-8 -*-",
            "",
            "from __future__ import unicode_literals",
            "",
            "import packaging.version",
            "",
            "from bleach.linkifier import (",
            "    DEFAULT_CALLBACKS,",
            "    Linker,",
            ")",
            "from bleach.sanitizer import (",
            "    ALLOWED_ATTRIBUTES,",
            "    ALLOWED_PROTOCOLS,",
            "    ALLOWED_STYLES,",
            "    ALLOWED_TAGS,",
            "    Cleaner,",
            ")",
            "",
            "",
            "# yyyymmdd",
            "__releasedate__ = \"20210126\"",
            "# x.y.z or x.y.z.dev0 -- semver",
            "__version__ = \"3.2.3\"",
            "VERSION = packaging.version.Version(__version__)",
            "",
            "",
            "__all__ = [\"clean\", \"linkify\"]",
            "",
            "",
            "def clean(",
            "    text,",
            "    tags=ALLOWED_TAGS,",
            "    attributes=ALLOWED_ATTRIBUTES,",
            "    styles=ALLOWED_STYLES,",
            "    protocols=ALLOWED_PROTOCOLS,",
            "    strip=False,",
            "    strip_comments=True,",
            "):",
            "    \"\"\"Clean an HTML fragment of malicious content and return it",
            "",
            "    This function is a security-focused function whose sole purpose is to",
            "    remove malicious content from a string such that it can be displayed as",
            "    content in a web page.",
            "",
            "    This function is not designed to use to transform content to be used in",
            "    non-web-page contexts.",
            "",
            "    Example::",
            "",
            "        import bleach",
            "",
            "        better_text = bleach.clean(yucky_text)",
            "",
            "",
            "    .. Note::",
            "",
            "       If you're cleaning a lot of text and passing the same argument values or",
            "       you want more configurability, consider using a",
            "       :py:class:`bleach.sanitizer.Cleaner` instance.",
            "",
            "    :arg str text: the text to clean",
            "",
            "    :arg list tags: allowed list of tags; defaults to",
            "        ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "    :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "        defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "    :arg list styles: allowed list of css styles; defaults to",
            "        ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "    :arg list protocols: allowed list of protocols for links; defaults",
            "        to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "    :arg bool strip: whether or not to strip disallowed elements",
            "",
            "    :arg bool strip_comments: whether or not to strip HTML comments",
            "",
            "    :returns: cleaned text as unicode",
            "",
            "    \"\"\"",
            "    cleaner = Cleaner(",
            "        tags=tags,",
            "        attributes=attributes,",
            "        styles=styles,",
            "        protocols=protocols,",
            "        strip=strip,",
            "        strip_comments=strip_comments,",
            "    )",
            "    return cleaner.clean(text)",
            "",
            "",
            "def linkify(text, callbacks=DEFAULT_CALLBACKS, skip_tags=None, parse_email=False):",
            "    \"\"\"Convert URL-like strings in an HTML fragment to links",
            "",
            "    This function converts strings that look like URLs, domain names and email",
            "    addresses in text that may be an HTML fragment to links, while preserving:",
            "",
            "    1. links already in the string",
            "    2. urls found in attributes",
            "    3. email addresses",
            "",
            "    linkify does a best-effort approach and tries to recover from bad",
            "    situations due to crazy text.",
            "",
            "    .. Note::",
            "",
            "       If you're linking a lot of text and passing the same argument values or",
            "       you want more configurability, consider using a",
            "       :py:class:`bleach.linkifier.Linker` instance.",
            "",
            "    .. Note::",
            "",
            "       If you have text that you want to clean and then linkify, consider using",
            "       the :py:class:`bleach.linkifier.LinkifyFilter` as a filter in the clean",
            "       pass. That way you're not parsing the HTML twice.",
            "",
            "    :arg str text: the text to linkify",
            "",
            "    :arg list callbacks: list of callbacks to run when adjusting tag attributes;",
            "        defaults to ``bleach.linkifier.DEFAULT_CALLBACKS``",
            "",
            "    :arg list skip_tags: list of tags that you don't want to linkify the",
            "        contents of; for example, you could set this to ``['pre']`` to skip",
            "        linkifying contents of ``pre`` tags",
            "",
            "    :arg bool parse_email: whether or not to linkify email addresses",
            "",
            "    :returns: linkified text as unicode",
            "",
            "    \"\"\"",
            "    linker = Linker(callbacks=callbacks, skip_tags=skip_tags, parse_email=parse_email)",
            "    return linker.linkify(text)"
        ],
        "afterPatchFile": [
            "# -*- coding: utf-8 -*-",
            "",
            "from __future__ import unicode_literals",
            "",
            "import packaging.version",
            "",
            "from bleach.linkifier import (",
            "    DEFAULT_CALLBACKS,",
            "    Linker,",
            ")",
            "from bleach.sanitizer import (",
            "    ALLOWED_ATTRIBUTES,",
            "    ALLOWED_PROTOCOLS,",
            "    ALLOWED_STYLES,",
            "    ALLOWED_TAGS,",
            "    Cleaner,",
            ")",
            "",
            "",
            "# yyyymmdd",
            "__releasedate__ = \"20210201\"",
            "# x.y.z or x.y.z.dev0 -- semver",
            "__version__ = \"3.3.0\"",
            "VERSION = packaging.version.Version(__version__)",
            "",
            "",
            "__all__ = [\"clean\", \"linkify\"]",
            "",
            "",
            "def clean(",
            "    text,",
            "    tags=ALLOWED_TAGS,",
            "    attributes=ALLOWED_ATTRIBUTES,",
            "    styles=ALLOWED_STYLES,",
            "    protocols=ALLOWED_PROTOCOLS,",
            "    strip=False,",
            "    strip_comments=True,",
            "):",
            "    \"\"\"Clean an HTML fragment of malicious content and return it",
            "",
            "    This function is a security-focused function whose sole purpose is to",
            "    remove malicious content from a string such that it can be displayed as",
            "    content in a web page.",
            "",
            "    This function is not designed to use to transform content to be used in",
            "    non-web-page contexts.",
            "",
            "    Example::",
            "",
            "        import bleach",
            "",
            "        better_text = bleach.clean(yucky_text)",
            "",
            "",
            "    .. Note::",
            "",
            "       If you're cleaning a lot of text and passing the same argument values or",
            "       you want more configurability, consider using a",
            "       :py:class:`bleach.sanitizer.Cleaner` instance.",
            "",
            "    :arg str text: the text to clean",
            "",
            "    :arg list tags: allowed list of tags; defaults to",
            "        ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "    :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "        defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "    :arg list styles: allowed list of css styles; defaults to",
            "        ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "    :arg list protocols: allowed list of protocols for links; defaults",
            "        to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "    :arg bool strip: whether or not to strip disallowed elements",
            "",
            "    :arg bool strip_comments: whether or not to strip HTML comments",
            "",
            "    :returns: cleaned text as unicode",
            "",
            "    \"\"\"",
            "    cleaner = Cleaner(",
            "        tags=tags,",
            "        attributes=attributes,",
            "        styles=styles,",
            "        protocols=protocols,",
            "        strip=strip,",
            "        strip_comments=strip_comments,",
            "    )",
            "    return cleaner.clean(text)",
            "",
            "",
            "def linkify(text, callbacks=DEFAULT_CALLBACKS, skip_tags=None, parse_email=False):",
            "    \"\"\"Convert URL-like strings in an HTML fragment to links",
            "",
            "    This function converts strings that look like URLs, domain names and email",
            "    addresses in text that may be an HTML fragment to links, while preserving:",
            "",
            "    1. links already in the string",
            "    2. urls found in attributes",
            "    3. email addresses",
            "",
            "    linkify does a best-effort approach and tries to recover from bad",
            "    situations due to crazy text.",
            "",
            "    .. Note::",
            "",
            "       If you're linking a lot of text and passing the same argument values or",
            "       you want more configurability, consider using a",
            "       :py:class:`bleach.linkifier.Linker` instance.",
            "",
            "    .. Note::",
            "",
            "       If you have text that you want to clean and then linkify, consider using",
            "       the :py:class:`bleach.linkifier.LinkifyFilter` as a filter in the clean",
            "       pass. That way you're not parsing the HTML twice.",
            "",
            "    :arg str text: the text to linkify",
            "",
            "    :arg list callbacks: list of callbacks to run when adjusting tag attributes;",
            "        defaults to ``bleach.linkifier.DEFAULT_CALLBACKS``",
            "",
            "    :arg list skip_tags: list of tags that you don't want to linkify the",
            "        contents of; for example, you could set this to ``['pre']`` to skip",
            "        linkifying contents of ``pre`` tags",
            "",
            "    :arg bool parse_email: whether or not to linkify email addresses",
            "",
            "    :returns: linkified text as unicode",
            "",
            "    \"\"\"",
            "    linker = Linker(callbacks=callbacks, skip_tags=skip_tags, parse_email=parse_email)",
            "    return linker.linkify(text)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "21": [
                "__releasedate__"
            ],
            "23": [
                "__version__"
            ]
        },
        "addLocation": []
    },
    "bleach/html5lib_shim.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": "     HTMLInputStream,"
            },
            "1": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": " )  # noqa: E402 module level import not at top of file"
            },
            "2": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " from bleach._vendor.html5lib.serializer import ("
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+    escape,"
            },
            "4": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "     HTMLSerializer,"
            },
            "5": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 53,
                "PatchRowcode": " )  # noqa: E402 module level import not at top of file"
            },
            "6": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " from bleach._vendor.html5lib._tokenizer import ("
            }
        },
        "frontPatchFile": [
            "# flake8: noqa",
            "\"\"\"",
            "Shim module between Bleach and html5lib. This makes it easier to upgrade the",
            "html5lib library without having to change a lot of code.",
            "\"\"\"",
            "",
            "from __future__ import unicode_literals",
            "",
            "import re",
            "import string",
            "import warnings",
            "",
            "import six",
            "",
            "# ignore html5lib deprecation warnings to use bleach; we are bleach",
            "# apply before we import submodules that import html5lib",
            "warnings.filterwarnings(",
            "    \"ignore\",",
            "    message=\"html5lib's sanitizer is deprecated\",",
            "    category=DeprecationWarning,",
            "    module=\"bleach._vendor.html5lib\",",
            ")",
            "",
            "from bleach._vendor.html5lib import (  # noqa: E402 module level import not at top of file",
            "    HTMLParser,",
            "    getTreeWalker,",
            ")",
            "from bleach._vendor.html5lib import (",
            "    constants,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.constants import (  # noqa: E402 module level import not at top of file",
            "    namespaces,",
            "    prefixes,",
            ")",
            "from bleach._vendor.html5lib.constants import (",
            "    _ReparseException as ReparseException,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.filters.base import (",
            "    Filter,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.filters.sanitizer import (",
            "    allowed_protocols,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.filters.sanitizer import (",
            "    Filter as SanitizerFilter,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib._inputstream import (",
            "    HTMLInputStream,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.serializer import (",
            "    HTMLSerializer,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib._tokenizer import (",
            "    attributeMap,",
            "    HTMLTokenizer,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib._trie import (",
            "    Trie,",
            ")  # noqa: E402 module level import not at top of file",
            "",
            "",
            "#: Map of entity name to expanded entity",
            "ENTITIES = constants.entities",
            "",
            "#: Trie of html entity string -> character representation",
            "ENTITIES_TRIE = Trie(ENTITIES)",
            "",
            "#: Token type constants--these never change",
            "TAG_TOKEN_TYPES = {",
            "    constants.tokenTypes[\"StartTag\"],",
            "    constants.tokenTypes[\"EndTag\"],",
            "    constants.tokenTypes[\"EmptyTag\"],",
            "}",
            "CHARACTERS_TYPE = constants.tokenTypes[\"Characters\"]",
            "PARSEERROR_TYPE = constants.tokenTypes[\"ParseError\"]",
            "",
            "",
            "#: List of valid HTML tags, from WHATWG HTML Living Standard as of 2018-10-17",
            "#: https://html.spec.whatwg.org/multipage/indices.html#elements-3",
            "HTML_TAGS = [",
            "    \"a\",",
            "    \"abbr\",",
            "    \"address\",",
            "    \"area\",",
            "    \"article\",",
            "    \"aside\",",
            "    \"audio\",",
            "    \"b\",",
            "    \"base\",",
            "    \"bdi\",",
            "    \"bdo\",",
            "    \"blockquote\",",
            "    \"body\",",
            "    \"br\",",
            "    \"button\",",
            "    \"canvas\",",
            "    \"caption\",",
            "    \"cite\",",
            "    \"code\",",
            "    \"col\",",
            "    \"colgroup\",",
            "    \"data\",",
            "    \"datalist\",",
            "    \"dd\",",
            "    \"del\",",
            "    \"details\",",
            "    \"dfn\",",
            "    \"dialog\",",
            "    \"div\",",
            "    \"dl\",",
            "    \"dt\",",
            "    \"em\",",
            "    \"embed\",",
            "    \"fieldset\",",
            "    \"figcaption\",",
            "    \"figure\",",
            "    \"footer\",",
            "    \"form\",",
            "    \"h1\",",
            "    \"h2\",",
            "    \"h3\",",
            "    \"h4\",",
            "    \"h5\",",
            "    \"h6\",",
            "    \"head\",",
            "    \"header\",",
            "    \"hgroup\",",
            "    \"hr\",",
            "    \"html\",",
            "    \"i\",",
            "    \"iframe\",",
            "    \"img\",",
            "    \"input\",",
            "    \"ins\",",
            "    \"kbd\",",
            "    \"keygen\",",
            "    \"label\",",
            "    \"legend\",",
            "    \"li\",",
            "    \"link\",",
            "    \"map\",",
            "    \"mark\",",
            "    \"menu\",",
            "    \"meta\",",
            "    \"meter\",",
            "    \"nav\",",
            "    \"noscript\",",
            "    \"object\",",
            "    \"ol\",",
            "    \"optgroup\",",
            "    \"option\",",
            "    \"output\",",
            "    \"p\",",
            "    \"param\",",
            "    \"picture\",",
            "    \"pre\",",
            "    \"progress\",",
            "    \"q\",",
            "    \"rp\",",
            "    \"rt\",",
            "    \"ruby\",",
            "    \"s\",",
            "    \"samp\",",
            "    \"script\",",
            "    \"section\",",
            "    \"select\",",
            "    \"slot\",",
            "    \"small\",",
            "    \"source\",",
            "    \"span\",",
            "    \"strong\",",
            "    \"style\",",
            "    \"sub\",",
            "    \"summary\",",
            "    \"sup\",",
            "    \"table\",",
            "    \"tbody\",",
            "    \"td\",",
            "    \"template\",",
            "    \"textarea\",",
            "    \"tfoot\",",
            "    \"th\",",
            "    \"thead\",",
            "    \"time\",",
            "    \"title\",",
            "    \"tr\",",
            "    \"track\",",
            "    \"u\",",
            "    \"ul\",",
            "    \"var\",",
            "    \"video\",",
            "    \"wbr\",",
            "]",
            "",
            "",
            "class InputStreamWithMemory(object):",
            "    \"\"\"Wraps an HTMLInputStream to remember characters since last <",
            "",
            "    This wraps existing HTMLInputStream classes to keep track of the stream",
            "    since the last < which marked an open tag state.",
            "",
            "    \"\"\"",
            "",
            "    def __init__(self, inner_stream):",
            "        self._inner_stream = inner_stream",
            "        self.reset = self._inner_stream.reset",
            "        self.position = self._inner_stream.position",
            "        self._buffer = []",
            "",
            "    @property",
            "    def errors(self):",
            "        return self._inner_stream.errors",
            "",
            "    @property",
            "    def charEncoding(self):",
            "        return self._inner_stream.charEncoding",
            "",
            "    @property",
            "    def changeEncoding(self):",
            "        return self._inner_stream.changeEncoding",
            "",
            "    def char(self):",
            "        c = self._inner_stream.char()",
            "        # char() can return None if EOF, so ignore that",
            "        if c:",
            "            self._buffer.append(c)",
            "        return c",
            "",
            "    def charsUntil(self, characters, opposite=False):",
            "        chars = self._inner_stream.charsUntil(characters, opposite=opposite)",
            "        self._buffer.extend(list(chars))",
            "        return chars",
            "",
            "    def unget(self, char):",
            "        if self._buffer:",
            "            self._buffer.pop(-1)",
            "        return self._inner_stream.unget(char)",
            "",
            "    def get_tag(self):",
            "        \"\"\"Returns the stream history since last '<'",
            "",
            "        Since the buffer starts at the last '<' as as seen by tagOpenState(),",
            "        we know that everything from that point to when this method is called",
            "        is the \"tag\" that is being tokenized.",
            "",
            "        \"\"\"",
            "        return six.text_type(\"\").join(self._buffer)",
            "",
            "    def start_tag(self):",
            "        \"\"\"Resets stream history to just '<'",
            "",
            "        This gets called by tagOpenState() which marks a '<' that denotes an",
            "        open tag. Any time we see that, we reset the buffer.",
            "",
            "        \"\"\"",
            "        self._buffer = [\"<\"]",
            "",
            "",
            "class BleachHTMLTokenizer(HTMLTokenizer):",
            "    \"\"\"Tokenizer that doesn't consume character entities\"\"\"",
            "",
            "    def __init__(self, consume_entities=False, **kwargs):",
            "        super(BleachHTMLTokenizer, self).__init__(**kwargs)",
            "",
            "        self.consume_entities = consume_entities",
            "",
            "        # Wrap the stream with one that remembers the history",
            "        self.stream = InputStreamWithMemory(self.stream)",
            "",
            "    def __iter__(self):",
            "        last_error_token = None",
            "",
            "        for token in super(BleachHTMLTokenizer, self).__iter__():",
            "            if last_error_token is not None:",
            "                if (",
            "                    last_error_token[\"data\"] == \"invalid-character-in-attribute-name\"",
            "                    and token[\"type\"] in TAG_TOKEN_TYPES",
            "                    and token.get(\"data\")",
            "                ):",
            "                    # token[\"data\"] is an html5lib attributeMap",
            "                    # (OrderedDict 3.7+ and dict otherwise)",
            "                    # of attr name to attr value",
            "                    #",
            "                    # Remove attribute names that have ', \" or < in them",
            "                    # because those characters are invalid for attribute names.",
            "                    token[\"data\"] = attributeMap(",
            "                        (attr_name, attr_value)",
            "                        for attr_name, attr_value in token[\"data\"].items()",
            "                        if (",
            "                            '\"' not in attr_name",
            "                            and \"'\" not in attr_name",
            "                            and \"<\" not in attr_name",
            "                        )",
            "                    )",
            "                    last_error_token = None",
            "                    yield token",
            "",
            "                elif (",
            "                    last_error_token[\"data\"] == \"expected-closing-tag-but-got-char\"",
            "                    and self.parser.tags is not None",
            "                    and token[\"data\"].lower().strip() not in self.parser.tags",
            "                ):",
            "                    # We've got either a malformed tag or a pseudo-tag or",
            "                    # something that html5lib wants to turn into a malformed",
            "                    # comment which Bleach clean() will drop so we interfere",
            "                    # with the token stream to handle it more correctly.",
            "                    #",
            "                    # If this is an allowed tag, it's malformed and we just let",
            "                    # the html5lib parser deal with it--we don't enter into this",
            "                    # block.",
            "                    #",
            "                    # If this is not an allowed tag, then we convert it to",
            "                    # characters and it'll get escaped in the sanitizer.",
            "                    token[\"data\"] = self.stream.get_tag()",
            "                    token[\"type\"] = CHARACTERS_TYPE",
            "",
            "                    last_error_token = None",
            "                    yield token",
            "",
            "                elif token[\"type\"] == PARSEERROR_TYPE:",
            "                    # If the token is a parse error, then let the last_error_token",
            "                    # go, and make token the new last_error_token",
            "                    yield last_error_token",
            "                    last_error_token = token",
            "",
            "                else:",
            "                    yield last_error_token",
            "                    yield token",
            "                    last_error_token = None",
            "",
            "                continue",
            "",
            "            # If the token is a ParseError, we hold on to it so we can get the",
            "            # next token and potentially fix it.",
            "            if token[\"type\"] == PARSEERROR_TYPE:",
            "                last_error_token = token",
            "                continue",
            "",
            "            yield token",
            "",
            "        if last_error_token:",
            "            yield last_error_token",
            "",
            "    def consumeEntity(self, allowedChar=None, fromAttribute=False):",
            "        # If this tokenizer is set to consume entities, then we can let the",
            "        # superclass do its thing.",
            "        if self.consume_entities:",
            "            return super(BleachHTMLTokenizer, self).consumeEntity(",
            "                allowedChar, fromAttribute",
            "            )",
            "",
            "        # If this tokenizer is set to not consume entities, then we don't want",
            "        # to consume and convert them, so this overrides the html5lib tokenizer's",
            "        # consumeEntity so that it's now a no-op.",
            "        #",
            "        # However, when that gets called, it's consumed an &, so we put that back in",
            "        # the stream.",
            "        if fromAttribute:",
            "            self.currentToken[\"data\"][-1][1] += \"&\"",
            "",
            "        else:",
            "            self.tokenQueue.append({\"type\": CHARACTERS_TYPE, \"data\": \"&\"})",
            "",
            "    def tagOpenState(self):",
            "        # This state marks a < that is either a StartTag, EndTag, EmptyTag,",
            "        # or ParseError. In all cases, we want to drop any stream history",
            "        # we've collected so far and we do that by calling start_tag() on",
            "        # the input stream wrapper.",
            "        self.stream.start_tag()",
            "        return super(BleachHTMLTokenizer, self).tagOpenState()",
            "",
            "    def emitCurrentToken(self):",
            "        token = self.currentToken",
            "",
            "        if (",
            "            self.parser.tags is not None",
            "            and token[\"type\"] in TAG_TOKEN_TYPES",
            "            and token[\"name\"].lower() not in self.parser.tags",
            "        ):",
            "            # If this is a start/end/empty tag for a tag that's not in our",
            "            # allowed list, then it gets stripped or escaped. In both of these",
            "            # cases it gets converted to a Characters token.",
            "            if self.parser.strip:",
            "                # If we're stripping the token, we just throw in an empty",
            "                # string token.",
            "                new_data = \"\"",
            "",
            "            else:",
            "                # If we're escaping the token, we want to escape the exact",
            "                # original string. Since tokenizing also normalizes data",
            "                # and this is a tag-like thing, we've lost some information.",
            "                # So we go back through the stream to get the original",
            "                # string and use that.",
            "                new_data = self.stream.get_tag()",
            "",
            "            new_token = {\"type\": CHARACTERS_TYPE, \"data\": new_data}",
            "",
            "            self.currentToken = new_token",
            "            self.tokenQueue.append(new_token)",
            "            self.state = self.dataState",
            "            return",
            "",
            "        super(BleachHTMLTokenizer, self).emitCurrentToken()",
            "",
            "",
            "class BleachHTMLParser(HTMLParser):",
            "    \"\"\"Parser that uses BleachHTMLTokenizer\"\"\"",
            "",
            "    def __init__(self, tags, strip, consume_entities, **kwargs):",
            "        \"\"\"",
            "        :arg tags: list of allowed tags--everything else is either stripped or",
            "            escaped; if None, then this doesn't look at tags at all",
            "        :arg strip: whether to strip disallowed tags (True) or escape them (False);",
            "            if tags=None, then this doesn't have any effect",
            "        :arg consume_entities: whether to consume entities (default behavior) or",
            "            leave them as is when tokenizing (BleachHTMLTokenizer-added behavior)",
            "",
            "        \"\"\"",
            "        self.tags = [tag.lower() for tag in tags] if tags is not None else None",
            "        self.strip = strip",
            "        self.consume_entities = consume_entities",
            "        super(BleachHTMLParser, self).__init__(**kwargs)",
            "",
            "    def _parse(",
            "        self, stream, innerHTML=False, container=\"div\", scripting=True, **kwargs",
            "    ):",
            "        # set scripting=True to parse <noscript> as though JS is enabled to",
            "        # match the expected context in browsers",
            "        #",
            "        # https://html.spec.whatwg.org/multipage/scripting.html#the-noscript-element",
            "        #",
            "        # Override HTMLParser so we can swap out the tokenizer for our own.",
            "        self.innerHTMLMode = innerHTML",
            "        self.container = container",
            "        self.scripting = scripting",
            "        self.tokenizer = BleachHTMLTokenizer(",
            "            stream=stream, consume_entities=self.consume_entities, parser=self, **kwargs",
            "        )",
            "        self.reset()",
            "",
            "        try:",
            "            self.mainLoop()",
            "        except ReparseException:",
            "            self.reset()",
            "            self.mainLoop()",
            "",
            "",
            "def convert_entity(value):",
            "    \"\"\"Convert an entity (minus the & and ; part) into what it represents",
            "",
            "    This handles numeric, hex, and text entities.",
            "",
            "    :arg value: the string (minus the ``&`` and ``;`` part) to convert",
            "",
            "    :returns: unicode character or None if it's an ambiguous ampersand that",
            "        doesn't match a character entity",
            "",
            "    \"\"\"",
            "    if value[0] == \"#\":",
            "        if len(value) < 2:",
            "            return None",
            "",
            "        if value[1] in (\"x\", \"X\"):",
            "            # hex-encoded code point",
            "            int_as_string, base = value[2:], 16",
            "        else:",
            "            # decimal code point",
            "            int_as_string, base = value[1:], 10",
            "",
            "        if int_as_string == \"\":",
            "            return None",
            "",
            "        code_point = int(int_as_string, base)",
            "        if 0 < code_point < 0x110000:",
            "            return six.unichr(code_point)",
            "        else:",
            "            return None",
            "",
            "    return ENTITIES.get(value, None)",
            "",
            "",
            "def convert_entities(text):",
            "    \"\"\"Converts all found entities in the text",
            "",
            "    :arg text: the text to convert entities in",
            "",
            "    :returns: unicode text with converted entities",
            "",
            "    \"\"\"",
            "    if \"&\" not in text:",
            "        return text",
            "",
            "    new_text = []",
            "    for part in next_possible_entity(text):",
            "        if not part:",
            "            continue",
            "",
            "        if part.startswith(\"&\"):",
            "            entity = match_entity(part)",
            "            if entity is not None:",
            "                converted = convert_entity(entity)",
            "",
            "                # If it's not an ambiguous ampersand, then replace with the",
            "                # unicode character. Otherwise, we leave the entity in.",
            "                if converted is not None:",
            "                    new_text.append(converted)",
            "                    remainder = part[len(entity) + 2 :]",
            "                    if part:",
            "                        new_text.append(remainder)",
            "                    continue",
            "",
            "        new_text.append(part)",
            "",
            "    return \"\".join(new_text)",
            "",
            "",
            "def match_entity(stream):",
            "    \"\"\"Returns first entity in stream or None if no entity exists",
            "",
            "    Note: For Bleach purposes, entities must start with a \"&\" and end with",
            "    a \";\". This ignoresambiguous character entities that have no \";\" at the",
            "    end.",
            "",
            "    :arg stream: the character stream",
            "",
            "    :returns: ``None`` or the entity string without \"&\" or \";\"",
            "",
            "    \"\"\"",
            "    # Nix the & at the beginning",
            "    if stream[0] != \"&\":",
            "        raise ValueError('Stream should begin with \"&\"')",
            "",
            "    stream = stream[1:]",
            "",
            "    stream = list(stream)",
            "    possible_entity = \"\"",
            "    end_characters = \"<&=;\" + string.whitespace",
            "",
            "    # Handle number entities",
            "    if stream and stream[0] == \"#\":",
            "        possible_entity = \"#\"",
            "        stream.pop(0)",
            "",
            "        if stream and stream[0] in (\"x\", \"X\"):",
            "            allowed = \"0123456789abcdefABCDEF\"",
            "            possible_entity += stream.pop(0)",
            "        else:",
            "            allowed = \"0123456789\"",
            "",
            "        # FIXME(willkg): Do we want to make sure these are valid number",
            "        # entities? This doesn't do that currently.",
            "        while stream and stream[0] not in end_characters:",
            "            c = stream.pop(0)",
            "            if c not in allowed:",
            "                break",
            "            possible_entity += c",
            "",
            "        if possible_entity and stream and stream[0] == \";\":",
            "            return possible_entity",
            "        return None",
            "",
            "    # Handle character entities",
            "    while stream and stream[0] not in end_characters:",
            "        c = stream.pop(0)",
            "        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):",
            "            break",
            "        possible_entity += c",
            "",
            "    if possible_entity and stream and stream[0] == \";\":",
            "        return possible_entity",
            "",
            "    return None",
            "",
            "",
            "AMP_SPLIT_RE = re.compile(\"(&)\")",
            "",
            "",
            "def next_possible_entity(text):",
            "    \"\"\"Takes a text and generates a list of possible entities",
            "",
            "    :arg text: the text to look at",
            "",
            "    :returns: generator where each part (except the first) starts with an",
            "        \"&\"",
            "",
            "    \"\"\"",
            "    for i, part in enumerate(AMP_SPLIT_RE.split(text)):",
            "        if i == 0:",
            "            yield part",
            "        elif i % 2 == 0:",
            "            yield \"&\" + part",
            "",
            "",
            "class BleachHTMLSerializer(HTMLSerializer):",
            "    \"\"\"HTMLSerializer that undoes & -> &amp; in attributes and sets",
            "    escape_rcdata to True",
            "    \"\"\"",
            "",
            "    # per the HTMLSerializer.__init__ docstring:",
            "    #",
            "    # Whether to escape characters that need to be",
            "    # escaped within normal elements within rcdata elements such as",
            "    # style.",
            "    #",
            "    escape_rcdata = True",
            "",
            "    def escape_base_amp(self, stoken):",
            "        \"\"\"Escapes just bare & in HTML attribute values\"\"\"",
            "        # First, undo escaping of &. We need to do this because html5lib's",
            "        # HTMLSerializer expected the tokenizer to consume all the character",
            "        # entities and convert them to their respective characters, but the",
            "        # BleachHTMLTokenizer doesn't do that. For example, this fixes",
            "        # &amp;entity; back to &entity; .",
            "        stoken = stoken.replace(\"&amp;\", \"&\")",
            "",
            "        # However, we do want all bare & that are not marking character",
            "        # entities to be changed to &amp;, so let's do that carefully here.",
            "        for part in next_possible_entity(stoken):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith(\"&\"):",
            "                entity = match_entity(part)",
            "                # Only leave entities in that are not ambiguous. If they're",
            "                # ambiguous, then we escape the ampersand.",
            "                if entity is not None and convert_entity(entity) is not None:",
            "                    yield \"&\" + entity + \";\"",
            "",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and one for ; at the end",
            "                    part = part[len(entity) + 2 :]",
            "                    if part:",
            "                        yield part",
            "                    continue",
            "",
            "            yield part.replace(\"&\", \"&amp;\")",
            "",
            "    def serialize(self, treewalker, encoding=None):",
            "        \"\"\"Wrap HTMLSerializer.serialize and conver & to &amp; in attribute values",
            "",
            "        Note that this converts & to &amp; in attribute values where the & isn't",
            "        already part of an unambiguous character entity.",
            "",
            "        \"\"\"",
            "        in_tag = False",
            "        after_equals = False",
            "",
            "        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):",
            "            if in_tag:",
            "                if stoken == \">\":",
            "                    in_tag = False",
            "",
            "                elif after_equals:",
            "                    if stoken != '\"':",
            "                        for part in self.escape_base_amp(stoken):",
            "                            yield part",
            "",
            "                        after_equals = False",
            "                        continue",
            "",
            "                elif stoken == \"=\":",
            "                    after_equals = True",
            "",
            "                yield stoken",
            "            else:",
            "                if stoken.startswith(\"<\"):",
            "                    in_tag = True",
            "                yield stoken"
        ],
        "afterPatchFile": [
            "# flake8: noqa",
            "\"\"\"",
            "Shim module between Bleach and html5lib. This makes it easier to upgrade the",
            "html5lib library without having to change a lot of code.",
            "\"\"\"",
            "",
            "from __future__ import unicode_literals",
            "",
            "import re",
            "import string",
            "import warnings",
            "",
            "import six",
            "",
            "# ignore html5lib deprecation warnings to use bleach; we are bleach",
            "# apply before we import submodules that import html5lib",
            "warnings.filterwarnings(",
            "    \"ignore\",",
            "    message=\"html5lib's sanitizer is deprecated\",",
            "    category=DeprecationWarning,",
            "    module=\"bleach._vendor.html5lib\",",
            ")",
            "",
            "from bleach._vendor.html5lib import (  # noqa: E402 module level import not at top of file",
            "    HTMLParser,",
            "    getTreeWalker,",
            ")",
            "from bleach._vendor.html5lib import (",
            "    constants,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.constants import (  # noqa: E402 module level import not at top of file",
            "    namespaces,",
            "    prefixes,",
            ")",
            "from bleach._vendor.html5lib.constants import (",
            "    _ReparseException as ReparseException,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.filters.base import (",
            "    Filter,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.filters.sanitizer import (",
            "    allowed_protocols,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.filters.sanitizer import (",
            "    Filter as SanitizerFilter,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib._inputstream import (",
            "    HTMLInputStream,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib.serializer import (",
            "    escape,",
            "    HTMLSerializer,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib._tokenizer import (",
            "    attributeMap,",
            "    HTMLTokenizer,",
            ")  # noqa: E402 module level import not at top of file",
            "from bleach._vendor.html5lib._trie import (",
            "    Trie,",
            ")  # noqa: E402 module level import not at top of file",
            "",
            "",
            "#: Map of entity name to expanded entity",
            "ENTITIES = constants.entities",
            "",
            "#: Trie of html entity string -> character representation",
            "ENTITIES_TRIE = Trie(ENTITIES)",
            "",
            "#: Token type constants--these never change",
            "TAG_TOKEN_TYPES = {",
            "    constants.tokenTypes[\"StartTag\"],",
            "    constants.tokenTypes[\"EndTag\"],",
            "    constants.tokenTypes[\"EmptyTag\"],",
            "}",
            "CHARACTERS_TYPE = constants.tokenTypes[\"Characters\"]",
            "PARSEERROR_TYPE = constants.tokenTypes[\"ParseError\"]",
            "",
            "",
            "#: List of valid HTML tags, from WHATWG HTML Living Standard as of 2018-10-17",
            "#: https://html.spec.whatwg.org/multipage/indices.html#elements-3",
            "HTML_TAGS = [",
            "    \"a\",",
            "    \"abbr\",",
            "    \"address\",",
            "    \"area\",",
            "    \"article\",",
            "    \"aside\",",
            "    \"audio\",",
            "    \"b\",",
            "    \"base\",",
            "    \"bdi\",",
            "    \"bdo\",",
            "    \"blockquote\",",
            "    \"body\",",
            "    \"br\",",
            "    \"button\",",
            "    \"canvas\",",
            "    \"caption\",",
            "    \"cite\",",
            "    \"code\",",
            "    \"col\",",
            "    \"colgroup\",",
            "    \"data\",",
            "    \"datalist\",",
            "    \"dd\",",
            "    \"del\",",
            "    \"details\",",
            "    \"dfn\",",
            "    \"dialog\",",
            "    \"div\",",
            "    \"dl\",",
            "    \"dt\",",
            "    \"em\",",
            "    \"embed\",",
            "    \"fieldset\",",
            "    \"figcaption\",",
            "    \"figure\",",
            "    \"footer\",",
            "    \"form\",",
            "    \"h1\",",
            "    \"h2\",",
            "    \"h3\",",
            "    \"h4\",",
            "    \"h5\",",
            "    \"h6\",",
            "    \"head\",",
            "    \"header\",",
            "    \"hgroup\",",
            "    \"hr\",",
            "    \"html\",",
            "    \"i\",",
            "    \"iframe\",",
            "    \"img\",",
            "    \"input\",",
            "    \"ins\",",
            "    \"kbd\",",
            "    \"keygen\",",
            "    \"label\",",
            "    \"legend\",",
            "    \"li\",",
            "    \"link\",",
            "    \"map\",",
            "    \"mark\",",
            "    \"menu\",",
            "    \"meta\",",
            "    \"meter\",",
            "    \"nav\",",
            "    \"noscript\",",
            "    \"object\",",
            "    \"ol\",",
            "    \"optgroup\",",
            "    \"option\",",
            "    \"output\",",
            "    \"p\",",
            "    \"param\",",
            "    \"picture\",",
            "    \"pre\",",
            "    \"progress\",",
            "    \"q\",",
            "    \"rp\",",
            "    \"rt\",",
            "    \"ruby\",",
            "    \"s\",",
            "    \"samp\",",
            "    \"script\",",
            "    \"section\",",
            "    \"select\",",
            "    \"slot\",",
            "    \"small\",",
            "    \"source\",",
            "    \"span\",",
            "    \"strong\",",
            "    \"style\",",
            "    \"sub\",",
            "    \"summary\",",
            "    \"sup\",",
            "    \"table\",",
            "    \"tbody\",",
            "    \"td\",",
            "    \"template\",",
            "    \"textarea\",",
            "    \"tfoot\",",
            "    \"th\",",
            "    \"thead\",",
            "    \"time\",",
            "    \"title\",",
            "    \"tr\",",
            "    \"track\",",
            "    \"u\",",
            "    \"ul\",",
            "    \"var\",",
            "    \"video\",",
            "    \"wbr\",",
            "]",
            "",
            "",
            "class InputStreamWithMemory(object):",
            "    \"\"\"Wraps an HTMLInputStream to remember characters since last <",
            "",
            "    This wraps existing HTMLInputStream classes to keep track of the stream",
            "    since the last < which marked an open tag state.",
            "",
            "    \"\"\"",
            "",
            "    def __init__(self, inner_stream):",
            "        self._inner_stream = inner_stream",
            "        self.reset = self._inner_stream.reset",
            "        self.position = self._inner_stream.position",
            "        self._buffer = []",
            "",
            "    @property",
            "    def errors(self):",
            "        return self._inner_stream.errors",
            "",
            "    @property",
            "    def charEncoding(self):",
            "        return self._inner_stream.charEncoding",
            "",
            "    @property",
            "    def changeEncoding(self):",
            "        return self._inner_stream.changeEncoding",
            "",
            "    def char(self):",
            "        c = self._inner_stream.char()",
            "        # char() can return None if EOF, so ignore that",
            "        if c:",
            "            self._buffer.append(c)",
            "        return c",
            "",
            "    def charsUntil(self, characters, opposite=False):",
            "        chars = self._inner_stream.charsUntil(characters, opposite=opposite)",
            "        self._buffer.extend(list(chars))",
            "        return chars",
            "",
            "    def unget(self, char):",
            "        if self._buffer:",
            "            self._buffer.pop(-1)",
            "        return self._inner_stream.unget(char)",
            "",
            "    def get_tag(self):",
            "        \"\"\"Returns the stream history since last '<'",
            "",
            "        Since the buffer starts at the last '<' as as seen by tagOpenState(),",
            "        we know that everything from that point to when this method is called",
            "        is the \"tag\" that is being tokenized.",
            "",
            "        \"\"\"",
            "        return six.text_type(\"\").join(self._buffer)",
            "",
            "    def start_tag(self):",
            "        \"\"\"Resets stream history to just '<'",
            "",
            "        This gets called by tagOpenState() which marks a '<' that denotes an",
            "        open tag. Any time we see that, we reset the buffer.",
            "",
            "        \"\"\"",
            "        self._buffer = [\"<\"]",
            "",
            "",
            "class BleachHTMLTokenizer(HTMLTokenizer):",
            "    \"\"\"Tokenizer that doesn't consume character entities\"\"\"",
            "",
            "    def __init__(self, consume_entities=False, **kwargs):",
            "        super(BleachHTMLTokenizer, self).__init__(**kwargs)",
            "",
            "        self.consume_entities = consume_entities",
            "",
            "        # Wrap the stream with one that remembers the history",
            "        self.stream = InputStreamWithMemory(self.stream)",
            "",
            "    def __iter__(self):",
            "        last_error_token = None",
            "",
            "        for token in super(BleachHTMLTokenizer, self).__iter__():",
            "            if last_error_token is not None:",
            "                if (",
            "                    last_error_token[\"data\"] == \"invalid-character-in-attribute-name\"",
            "                    and token[\"type\"] in TAG_TOKEN_TYPES",
            "                    and token.get(\"data\")",
            "                ):",
            "                    # token[\"data\"] is an html5lib attributeMap",
            "                    # (OrderedDict 3.7+ and dict otherwise)",
            "                    # of attr name to attr value",
            "                    #",
            "                    # Remove attribute names that have ', \" or < in them",
            "                    # because those characters are invalid for attribute names.",
            "                    token[\"data\"] = attributeMap(",
            "                        (attr_name, attr_value)",
            "                        for attr_name, attr_value in token[\"data\"].items()",
            "                        if (",
            "                            '\"' not in attr_name",
            "                            and \"'\" not in attr_name",
            "                            and \"<\" not in attr_name",
            "                        )",
            "                    )",
            "                    last_error_token = None",
            "                    yield token",
            "",
            "                elif (",
            "                    last_error_token[\"data\"] == \"expected-closing-tag-but-got-char\"",
            "                    and self.parser.tags is not None",
            "                    and token[\"data\"].lower().strip() not in self.parser.tags",
            "                ):",
            "                    # We've got either a malformed tag or a pseudo-tag or",
            "                    # something that html5lib wants to turn into a malformed",
            "                    # comment which Bleach clean() will drop so we interfere",
            "                    # with the token stream to handle it more correctly.",
            "                    #",
            "                    # If this is an allowed tag, it's malformed and we just let",
            "                    # the html5lib parser deal with it--we don't enter into this",
            "                    # block.",
            "                    #",
            "                    # If this is not an allowed tag, then we convert it to",
            "                    # characters and it'll get escaped in the sanitizer.",
            "                    token[\"data\"] = self.stream.get_tag()",
            "                    token[\"type\"] = CHARACTERS_TYPE",
            "",
            "                    last_error_token = None",
            "                    yield token",
            "",
            "                elif token[\"type\"] == PARSEERROR_TYPE:",
            "                    # If the token is a parse error, then let the last_error_token",
            "                    # go, and make token the new last_error_token",
            "                    yield last_error_token",
            "                    last_error_token = token",
            "",
            "                else:",
            "                    yield last_error_token",
            "                    yield token",
            "                    last_error_token = None",
            "",
            "                continue",
            "",
            "            # If the token is a ParseError, we hold on to it so we can get the",
            "            # next token and potentially fix it.",
            "            if token[\"type\"] == PARSEERROR_TYPE:",
            "                last_error_token = token",
            "                continue",
            "",
            "            yield token",
            "",
            "        if last_error_token:",
            "            yield last_error_token",
            "",
            "    def consumeEntity(self, allowedChar=None, fromAttribute=False):",
            "        # If this tokenizer is set to consume entities, then we can let the",
            "        # superclass do its thing.",
            "        if self.consume_entities:",
            "            return super(BleachHTMLTokenizer, self).consumeEntity(",
            "                allowedChar, fromAttribute",
            "            )",
            "",
            "        # If this tokenizer is set to not consume entities, then we don't want",
            "        # to consume and convert them, so this overrides the html5lib tokenizer's",
            "        # consumeEntity so that it's now a no-op.",
            "        #",
            "        # However, when that gets called, it's consumed an &, so we put that back in",
            "        # the stream.",
            "        if fromAttribute:",
            "            self.currentToken[\"data\"][-1][1] += \"&\"",
            "",
            "        else:",
            "            self.tokenQueue.append({\"type\": CHARACTERS_TYPE, \"data\": \"&\"})",
            "",
            "    def tagOpenState(self):",
            "        # This state marks a < that is either a StartTag, EndTag, EmptyTag,",
            "        # or ParseError. In all cases, we want to drop any stream history",
            "        # we've collected so far and we do that by calling start_tag() on",
            "        # the input stream wrapper.",
            "        self.stream.start_tag()",
            "        return super(BleachHTMLTokenizer, self).tagOpenState()",
            "",
            "    def emitCurrentToken(self):",
            "        token = self.currentToken",
            "",
            "        if (",
            "            self.parser.tags is not None",
            "            and token[\"type\"] in TAG_TOKEN_TYPES",
            "            and token[\"name\"].lower() not in self.parser.tags",
            "        ):",
            "            # If this is a start/end/empty tag for a tag that's not in our",
            "            # allowed list, then it gets stripped or escaped. In both of these",
            "            # cases it gets converted to a Characters token.",
            "            if self.parser.strip:",
            "                # If we're stripping the token, we just throw in an empty",
            "                # string token.",
            "                new_data = \"\"",
            "",
            "            else:",
            "                # If we're escaping the token, we want to escape the exact",
            "                # original string. Since tokenizing also normalizes data",
            "                # and this is a tag-like thing, we've lost some information.",
            "                # So we go back through the stream to get the original",
            "                # string and use that.",
            "                new_data = self.stream.get_tag()",
            "",
            "            new_token = {\"type\": CHARACTERS_TYPE, \"data\": new_data}",
            "",
            "            self.currentToken = new_token",
            "            self.tokenQueue.append(new_token)",
            "            self.state = self.dataState",
            "            return",
            "",
            "        super(BleachHTMLTokenizer, self).emitCurrentToken()",
            "",
            "",
            "class BleachHTMLParser(HTMLParser):",
            "    \"\"\"Parser that uses BleachHTMLTokenizer\"\"\"",
            "",
            "    def __init__(self, tags, strip, consume_entities, **kwargs):",
            "        \"\"\"",
            "        :arg tags: list of allowed tags--everything else is either stripped or",
            "            escaped; if None, then this doesn't look at tags at all",
            "        :arg strip: whether to strip disallowed tags (True) or escape them (False);",
            "            if tags=None, then this doesn't have any effect",
            "        :arg consume_entities: whether to consume entities (default behavior) or",
            "            leave them as is when tokenizing (BleachHTMLTokenizer-added behavior)",
            "",
            "        \"\"\"",
            "        self.tags = [tag.lower() for tag in tags] if tags is not None else None",
            "        self.strip = strip",
            "        self.consume_entities = consume_entities",
            "        super(BleachHTMLParser, self).__init__(**kwargs)",
            "",
            "    def _parse(",
            "        self, stream, innerHTML=False, container=\"div\", scripting=True, **kwargs",
            "    ):",
            "        # set scripting=True to parse <noscript> as though JS is enabled to",
            "        # match the expected context in browsers",
            "        #",
            "        # https://html.spec.whatwg.org/multipage/scripting.html#the-noscript-element",
            "        #",
            "        # Override HTMLParser so we can swap out the tokenizer for our own.",
            "        self.innerHTMLMode = innerHTML",
            "        self.container = container",
            "        self.scripting = scripting",
            "        self.tokenizer = BleachHTMLTokenizer(",
            "            stream=stream, consume_entities=self.consume_entities, parser=self, **kwargs",
            "        )",
            "        self.reset()",
            "",
            "        try:",
            "            self.mainLoop()",
            "        except ReparseException:",
            "            self.reset()",
            "            self.mainLoop()",
            "",
            "",
            "def convert_entity(value):",
            "    \"\"\"Convert an entity (minus the & and ; part) into what it represents",
            "",
            "    This handles numeric, hex, and text entities.",
            "",
            "    :arg value: the string (minus the ``&`` and ``;`` part) to convert",
            "",
            "    :returns: unicode character or None if it's an ambiguous ampersand that",
            "        doesn't match a character entity",
            "",
            "    \"\"\"",
            "    if value[0] == \"#\":",
            "        if len(value) < 2:",
            "            return None",
            "",
            "        if value[1] in (\"x\", \"X\"):",
            "            # hex-encoded code point",
            "            int_as_string, base = value[2:], 16",
            "        else:",
            "            # decimal code point",
            "            int_as_string, base = value[1:], 10",
            "",
            "        if int_as_string == \"\":",
            "            return None",
            "",
            "        code_point = int(int_as_string, base)",
            "        if 0 < code_point < 0x110000:",
            "            return six.unichr(code_point)",
            "        else:",
            "            return None",
            "",
            "    return ENTITIES.get(value, None)",
            "",
            "",
            "def convert_entities(text):",
            "    \"\"\"Converts all found entities in the text",
            "",
            "    :arg text: the text to convert entities in",
            "",
            "    :returns: unicode text with converted entities",
            "",
            "    \"\"\"",
            "    if \"&\" not in text:",
            "        return text",
            "",
            "    new_text = []",
            "    for part in next_possible_entity(text):",
            "        if not part:",
            "            continue",
            "",
            "        if part.startswith(\"&\"):",
            "            entity = match_entity(part)",
            "            if entity is not None:",
            "                converted = convert_entity(entity)",
            "",
            "                # If it's not an ambiguous ampersand, then replace with the",
            "                # unicode character. Otherwise, we leave the entity in.",
            "                if converted is not None:",
            "                    new_text.append(converted)",
            "                    remainder = part[len(entity) + 2 :]",
            "                    if part:",
            "                        new_text.append(remainder)",
            "                    continue",
            "",
            "        new_text.append(part)",
            "",
            "    return \"\".join(new_text)",
            "",
            "",
            "def match_entity(stream):",
            "    \"\"\"Returns first entity in stream or None if no entity exists",
            "",
            "    Note: For Bleach purposes, entities must start with a \"&\" and end with",
            "    a \";\". This ignoresambiguous character entities that have no \";\" at the",
            "    end.",
            "",
            "    :arg stream: the character stream",
            "",
            "    :returns: ``None`` or the entity string without \"&\" or \";\"",
            "",
            "    \"\"\"",
            "    # Nix the & at the beginning",
            "    if stream[0] != \"&\":",
            "        raise ValueError('Stream should begin with \"&\"')",
            "",
            "    stream = stream[1:]",
            "",
            "    stream = list(stream)",
            "    possible_entity = \"\"",
            "    end_characters = \"<&=;\" + string.whitespace",
            "",
            "    # Handle number entities",
            "    if stream and stream[0] == \"#\":",
            "        possible_entity = \"#\"",
            "        stream.pop(0)",
            "",
            "        if stream and stream[0] in (\"x\", \"X\"):",
            "            allowed = \"0123456789abcdefABCDEF\"",
            "            possible_entity += stream.pop(0)",
            "        else:",
            "            allowed = \"0123456789\"",
            "",
            "        # FIXME(willkg): Do we want to make sure these are valid number",
            "        # entities? This doesn't do that currently.",
            "        while stream and stream[0] not in end_characters:",
            "            c = stream.pop(0)",
            "            if c not in allowed:",
            "                break",
            "            possible_entity += c",
            "",
            "        if possible_entity and stream and stream[0] == \";\":",
            "            return possible_entity",
            "        return None",
            "",
            "    # Handle character entities",
            "    while stream and stream[0] not in end_characters:",
            "        c = stream.pop(0)",
            "        if not ENTITIES_TRIE.has_keys_with_prefix(possible_entity):",
            "            break",
            "        possible_entity += c",
            "",
            "    if possible_entity and stream and stream[0] == \";\":",
            "        return possible_entity",
            "",
            "    return None",
            "",
            "",
            "AMP_SPLIT_RE = re.compile(\"(&)\")",
            "",
            "",
            "def next_possible_entity(text):",
            "    \"\"\"Takes a text and generates a list of possible entities",
            "",
            "    :arg text: the text to look at",
            "",
            "    :returns: generator where each part (except the first) starts with an",
            "        \"&\"",
            "",
            "    \"\"\"",
            "    for i, part in enumerate(AMP_SPLIT_RE.split(text)):",
            "        if i == 0:",
            "            yield part",
            "        elif i % 2 == 0:",
            "            yield \"&\" + part",
            "",
            "",
            "class BleachHTMLSerializer(HTMLSerializer):",
            "    \"\"\"HTMLSerializer that undoes & -> &amp; in attributes and sets",
            "    escape_rcdata to True",
            "    \"\"\"",
            "",
            "    # per the HTMLSerializer.__init__ docstring:",
            "    #",
            "    # Whether to escape characters that need to be",
            "    # escaped within normal elements within rcdata elements such as",
            "    # style.",
            "    #",
            "    escape_rcdata = True",
            "",
            "    def escape_base_amp(self, stoken):",
            "        \"\"\"Escapes just bare & in HTML attribute values\"\"\"",
            "        # First, undo escaping of &. We need to do this because html5lib's",
            "        # HTMLSerializer expected the tokenizer to consume all the character",
            "        # entities and convert them to their respective characters, but the",
            "        # BleachHTMLTokenizer doesn't do that. For example, this fixes",
            "        # &amp;entity; back to &entity; .",
            "        stoken = stoken.replace(\"&amp;\", \"&\")",
            "",
            "        # However, we do want all bare & that are not marking character",
            "        # entities to be changed to &amp;, so let's do that carefully here.",
            "        for part in next_possible_entity(stoken):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith(\"&\"):",
            "                entity = match_entity(part)",
            "                # Only leave entities in that are not ambiguous. If they're",
            "                # ambiguous, then we escape the ampersand.",
            "                if entity is not None and convert_entity(entity) is not None:",
            "                    yield \"&\" + entity + \";\"",
            "",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and one for ; at the end",
            "                    part = part[len(entity) + 2 :]",
            "                    if part:",
            "                        yield part",
            "                    continue",
            "",
            "            yield part.replace(\"&\", \"&amp;\")",
            "",
            "    def serialize(self, treewalker, encoding=None):",
            "        \"\"\"Wrap HTMLSerializer.serialize and conver & to &amp; in attribute values",
            "",
            "        Note that this converts & to &amp; in attribute values where the & isn't",
            "        already part of an unambiguous character entity.",
            "",
            "        \"\"\"",
            "        in_tag = False",
            "        after_equals = False",
            "",
            "        for stoken in super(BleachHTMLSerializer, self).serialize(treewalker, encoding):",
            "            if in_tag:",
            "                if stoken == \">\":",
            "                    in_tag = False",
            "",
            "                elif after_equals:",
            "                    if stoken != '\"':",
            "                        for part in self.escape_base_amp(stoken):",
            "                            yield part",
            "",
            "                        after_equals = False",
            "                        continue",
            "",
            "                elif stoken == \"=\":",
            "                    after_equals = True",
            "",
            "                yield stoken",
            "            else:",
            "                if stoken.startswith(\"<\"):",
            "                    in_tag = True",
            "                yield stoken"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "bleach/sanitizer.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 371,
                "afterPatchRowNumber": 371,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 372,
                "afterPatchRowNumber": 372,
                "PatchRowcode": "         elif token_type == \"Comment\":"
            },
            "2": {
                "beforePatchRowNumber": 373,
                "afterPatchRowNumber": 373,
                "PatchRowcode": "             if not self.strip_html_comments:"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 374,
                "PatchRowcode": "+                # call lxml.sax.saxutils to escape &, <, and > in addition to \" and '"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 375,
                "PatchRowcode": "+                token[\"data\"] = html5lib_shim.escape("
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 376,
                "PatchRowcode": "+                    token[\"data\"], entities={'\"': \"&quot;\", \"'\": \"&#x27;\"}"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 377,
                "PatchRowcode": "+                )"
            },
            "7": {
                "beforePatchRowNumber": 374,
                "afterPatchRowNumber": 378,
                "PatchRowcode": "                 return token"
            },
            "8": {
                "beforePatchRowNumber": 375,
                "afterPatchRowNumber": 379,
                "PatchRowcode": "             else:"
            },
            "9": {
                "beforePatchRowNumber": 376,
                "afterPatchRowNumber": 380,
                "PatchRowcode": "                 return None"
            }
        },
        "frontPatchFile": [
            "from __future__ import unicode_literals",
            "",
            "from itertools import chain",
            "import re",
            "import warnings",
            "",
            "import six",
            "from six.moves.urllib.parse import urlparse",
            "from xml.sax.saxutils import unescape",
            "",
            "from bleach import html5lib_shim",
            "from bleach.utils import alphabetize_attributes, force_unicode",
            "",
            "",
            "#: List of allowed tags",
            "ALLOWED_TAGS = [",
            "    \"a\",",
            "    \"abbr\",",
            "    \"acronym\",",
            "    \"b\",",
            "    \"blockquote\",",
            "    \"code\",",
            "    \"em\",",
            "    \"i\",",
            "    \"li\",",
            "    \"ol\",",
            "    \"strong\",",
            "    \"ul\",",
            "]",
            "",
            "",
            "#: Map of allowed attributes by tag",
            "ALLOWED_ATTRIBUTES = {",
            "    \"a\": [\"href\", \"title\"],",
            "    \"abbr\": [\"title\"],",
            "    \"acronym\": [\"title\"],",
            "}",
            "",
            "#: List of allowed styles",
            "ALLOWED_STYLES = []",
            "",
            "#: List of allowed protocols",
            "ALLOWED_PROTOCOLS = [\"http\", \"https\", \"mailto\"]",
            "",
            "#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)",
            "INVISIBLE_CHARACTERS = \"\".join(",
            "    [chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))]",
            ")",
            "",
            "#: Regexp for characters that are invisible",
            "INVISIBLE_CHARACTERS_RE = re.compile(\"[\" + INVISIBLE_CHARACTERS + \"]\", re.UNICODE)",
            "",
            "#: String to replace invisible characters with. This can be a character, a",
            "#: string, or even a function that takes a Python re matchobj",
            "INVISIBLE_REPLACEMENT_CHAR = \"?\"",
            "",
            "",
            "class Cleaner(object):",
            "    \"\"\"Cleaner for cleaning HTML fragments of malicious content",
            "",
            "    This cleaner is a security-focused function whose sole purpose is to remove",
            "    malicious content from a string such that it can be displayed as content in",
            "    a web page.",
            "",
            "    To use::",
            "",
            "        from bleach.sanitizer import Cleaner",
            "",
            "        cleaner = Cleaner()",
            "",
            "        for text in all_the_yucky_things:",
            "            sanitized = cleaner.clean(text)",
            "",
            "    .. Note::",
            "",
            "       This cleaner is not designed to use to transform content to be used in",
            "       non-web-page contexts.",
            "",
            "    .. Warning::",
            "",
            "       This cleaner is not thread-safe--the html parser has internal state.",
            "       Create a separate cleaner per thread!",
            "",
            "",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        tags=ALLOWED_TAGS,",
            "        attributes=ALLOWED_ATTRIBUTES,",
            "        styles=ALLOWED_STYLES,",
            "        protocols=ALLOWED_PROTOCOLS,",
            "        strip=False,",
            "        strip_comments=True,",
            "        filters=None,",
            "    ):",
            "        \"\"\"Initializes a Cleaner",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip: whether or not to strip disallowed elements",
            "",
            "        :arg bool strip_comments: whether or not to strip HTML comments",
            "",
            "        :arg list filters: list of html5lib Filter classes to pass streamed content through",
            "",
            "            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters",
            "",
            "            .. Warning::",
            "",
            "               Using filters changes the output of ``bleach.Cleaner.clean``.",
            "               Make sure the way the filters change the output are secure.",
            "",
            "        \"\"\"",
            "        self.tags = tags",
            "        self.attributes = attributes",
            "        self.styles = styles",
            "        self.protocols = protocols",
            "        self.strip = strip",
            "        self.strip_comments = strip_comments",
            "        self.filters = filters or []",
            "",
            "        self.parser = html5lib_shim.BleachHTMLParser(",
            "            tags=self.tags,",
            "            strip=self.strip,",
            "            consume_entities=False,",
            "            namespaceHTMLElements=False,",
            "        )",
            "        self.walker = html5lib_shim.getTreeWalker(\"etree\")",
            "        self.serializer = html5lib_shim.BleachHTMLSerializer(",
            "            quote_attr_values=\"always\",",
            "            omit_optional_tags=False,",
            "            escape_lt_in_attrs=True,",
            "            # We want to leave entities as they are without escaping or",
            "            # resolving or expanding",
            "            resolve_entities=False,",
            "            # Bleach has its own sanitizer, so don't use the html5lib one",
            "            sanitize=False,",
            "            # Bleach sanitizer alphabetizes already, so don't use the html5lib one",
            "            alphabetical_attributes=False,",
            "        )",
            "",
            "    def clean(self, text):",
            "        \"\"\"Cleans text and returns sanitized result as unicode",
            "",
            "        :arg str text: text to be cleaned",
            "",
            "        :returns: sanitized text as unicode",
            "",
            "        :raises TypeError: if ``text`` is not a text type",
            "",
            "        \"\"\"",
            "        if not isinstance(text, six.string_types):",
            "            message = (",
            "                \"argument cannot be of '{name}' type, must be of text type\".format(",
            "                    name=text.__class__.__name__",
            "                )",
            "            )",
            "            raise TypeError(message)",
            "",
            "        if not text:",
            "            return \"\"",
            "",
            "        text = force_unicode(text)",
            "",
            "        dom = self.parser.parseFragment(text)",
            "        filtered = BleachSanitizerFilter(",
            "            source=self.walker(dom),",
            "            # Bleach-sanitizer-specific things",
            "            attributes=self.attributes,",
            "            strip_disallowed_elements=self.strip,",
            "            strip_html_comments=self.strip_comments,",
            "            # html5lib-sanitizer things",
            "            allowed_elements=self.tags,",
            "            allowed_css_properties=self.styles,",
            "            allowed_protocols=self.protocols,",
            "            allowed_svg_properties=[],",
            "        )",
            "",
            "        # Apply any filters after the BleachSanitizerFilter",
            "        for filter_class in self.filters:",
            "            filtered = filter_class(source=filtered)",
            "",
            "        return self.serializer.render(filtered)",
            "",
            "",
            "def attribute_filter_factory(attributes):",
            "    \"\"\"Generates attribute filter function for the given attributes value",
            "",
            "    The attributes value can take one of several shapes. This returns a filter",
            "    function appropriate to the attributes value. One nice thing about this is",
            "    that there's less if/then shenanigans in the ``allow_token`` method.",
            "",
            "    \"\"\"",
            "    if callable(attributes):",
            "        return attributes",
            "",
            "    if isinstance(attributes, dict):",
            "",
            "        def _attr_filter(tag, attr, value):",
            "            if tag in attributes:",
            "                attr_val = attributes[tag]",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                if attr in attr_val:",
            "                    return True",
            "",
            "            if \"*\" in attributes:",
            "                attr_val = attributes[\"*\"]",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                return attr in attr_val",
            "",
            "            return False",
            "",
            "        return _attr_filter",
            "",
            "    if isinstance(attributes, list):",
            "",
            "        def _attr_filter(tag, attr, value):",
            "            return attr in attributes",
            "",
            "        return _attr_filter",
            "",
            "    raise ValueError(\"attributes needs to be a callable, a list or a dict\")",
            "",
            "",
            "class BleachSanitizerFilter(html5lib_shim.SanitizerFilter):",
            "    \"\"\"html5lib Filter that sanitizes text",
            "",
            "    This filter can be used anywhere html5lib filters can be used.",
            "",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        source,",
            "        attributes=ALLOWED_ATTRIBUTES,",
            "        strip_disallowed_elements=False,",
            "        strip_html_comments=True,",
            "        **kwargs",
            "    ):",
            "        \"\"\"Creates a BleachSanitizerFilter instance",
            "",
            "        :arg Treewalker source: stream",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip_disallowed_elements: whether or not to strip disallowed",
            "            elements",
            "",
            "        :arg bool strip_html_comments: whether or not to strip HTML comments",
            "",
            "        \"\"\"",
            "        self.attr_filter = attribute_filter_factory(attributes)",
            "        self.strip_disallowed_elements = strip_disallowed_elements",
            "        self.strip_html_comments = strip_html_comments",
            "",
            "        # filter out html5lib deprecation warnings to use bleach from BleachSanitizerFilter init",
            "        warnings.filterwarnings(",
            "            \"ignore\",",
            "            message=\"html5lib's sanitizer is deprecated\",",
            "            category=DeprecationWarning,",
            "            module=\"bleach._vendor.html5lib\",",
            "        )",
            "        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)",
            "",
            "    def sanitize_stream(self, token_iterator):",
            "        for token in token_iterator:",
            "            ret = self.sanitize_token(token)",
            "",
            "            if not ret:",
            "                continue",
            "",
            "            if isinstance(ret, list):",
            "                for subtoken in ret:",
            "                    yield subtoken",
            "            else:",
            "                yield ret",
            "",
            "    def merge_characters(self, token_iterator):",
            "        \"\"\"Merge consecutive Characters tokens in a stream\"\"\"",
            "        characters_buffer = []",
            "",
            "        for token in token_iterator:",
            "            if characters_buffer:",
            "                if token[\"type\"] == \"Characters\":",
            "                    characters_buffer.append(token)",
            "                    continue",
            "                else:",
            "                    # Merge all the characters tokens together into one and then",
            "                    # operate on it.",
            "                    new_token = {",
            "                        \"data\": \"\".join(",
            "                            [char_token[\"data\"] for char_token in characters_buffer]",
            "                        ),",
            "                        \"type\": \"Characters\",",
            "                    }",
            "                    characters_buffer = []",
            "                    yield new_token",
            "",
            "            elif token[\"type\"] == \"Characters\":",
            "                characters_buffer.append(token)",
            "                continue",
            "",
            "            yield token",
            "",
            "        new_token = {",
            "            \"data\": \"\".join([char_token[\"data\"] for char_token in characters_buffer]),",
            "            \"type\": \"Characters\",",
            "        }",
            "        yield new_token",
            "",
            "    def __iter__(self):",
            "        return self.merge_characters(",
            "            self.sanitize_stream(html5lib_shim.Filter.__iter__(self))",
            "        )",
            "",
            "    def sanitize_token(self, token):",
            "        \"\"\"Sanitize a token either by HTML-encoding or dropping.",
            "",
            "        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':",
            "        ['attribute', 'pairs'], 'tag': callable}.",
            "",
            "        Here callable is a function with two arguments of attribute name and",
            "        value. It should return true of false.",
            "",
            "        Also gives the option to strip tags instead of encoding.",
            "",
            "        :arg dict token: token to sanitize",
            "",
            "        :returns: token or list of tokens",
            "",
            "        \"\"\"",
            "        token_type = token[\"type\"]",
            "        if token_type in [\"StartTag\", \"EndTag\", \"EmptyTag\"]:",
            "            if token[\"name\"] in self.allowed_elements:",
            "                return self.allow_token(token)",
            "",
            "            elif self.strip_disallowed_elements:",
            "                return None",
            "",
            "            else:",
            "                if \"data\" in token:",
            "                    # Alphabetize the attributes before calling .disallowed_token()",
            "                    # so that the resulting string is stable",
            "                    token[\"data\"] = alphabetize_attributes(token[\"data\"])",
            "                return self.disallowed_token(token)",
            "",
            "        elif token_type == \"Comment\":",
            "            if not self.strip_html_comments:",
            "                return token",
            "            else:",
            "                return None",
            "",
            "        elif token_type == \"Characters\":",
            "            return self.sanitize_characters(token)",
            "",
            "        else:",
            "            return token",
            "",
            "    def sanitize_characters(self, token):",
            "        \"\"\"Handles Characters tokens",
            "",
            "        Our overridden tokenizer doesn't do anything with entities. However,",
            "        that means that the serializer will convert all ``&`` in Characters",
            "        tokens to ``&amp;``.",
            "",
            "        Since we don't want that, we extract entities here and convert them to",
            "        Entity tokens so the serializer will let them be.",
            "",
            "        :arg token: the Characters token to work on",
            "",
            "        :returns: a list of tokens",
            "",
            "        \"\"\"",
            "        data = token.get(\"data\", \"\")",
            "",
            "        if not data:",
            "            return token",
            "",
            "        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)",
            "        token[\"data\"] = data",
            "",
            "        # If there isn't a & in the data, we can return now",
            "        if \"&\" not in data:",
            "            return token",
            "",
            "        new_tokens = []",
            "",
            "        # For each possible entity that starts with a \"&\", we try to extract an",
            "        # actual entity and re-tokenize accordingly",
            "        for part in html5lib_shim.next_possible_entity(data):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith(\"&\"):",
            "                entity = html5lib_shim.match_entity(part)",
            "                if entity is not None:",
            "                    if entity == \"amp\":",
            "                        # LinkifyFilter can't match urls across token boundaries",
            "                        # which is problematic with &amp; since that shows up in",
            "                        # querystrings all the time. This special-cases &amp;",
            "                        # and converts it to a & and sticks it in as a",
            "                        # Characters token. It'll get merged with surrounding",
            "                        # tokens in the BleachSanitizerfilter.__iter__ and",
            "                        # escaped in the serializer.",
            "                        new_tokens.append({\"type\": \"Characters\", \"data\": \"&\"})",
            "                    else:",
            "                        new_tokens.append({\"type\": \"Entity\", \"name\": entity})",
            "",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and one for ; at the end",
            "                    remainder = part[len(entity) + 2 :]",
            "                    if remainder:",
            "                        new_tokens.append({\"type\": \"Characters\", \"data\": remainder})",
            "                    continue",
            "",
            "            new_tokens.append({\"type\": \"Characters\", \"data\": part})",
            "",
            "        return new_tokens",
            "",
            "    def sanitize_uri_value(self, value, allowed_protocols):",
            "        \"\"\"Checks a uri value to see if it's allowed",
            "",
            "        :arg value: the uri value to sanitize",
            "        :arg allowed_protocols: list of allowed protocols",
            "",
            "        :returns: allowed value or None",
            "",
            "        \"\"\"",
            "        # NOTE(willkg): This transforms the value into one that's easier to",
            "        # match and verify, but shouldn't get returned since it's vastly",
            "        # different than the original value.",
            "",
            "        # Convert all character entities in the value",
            "        new_value = html5lib_shim.convert_entities(value)",
            "",
            "        # Nix backtick, space characters, and control characters",
            "        new_value = re.sub(r\"[`\\000-\\040\\177-\\240\\s]+\", \"\", new_value)",
            "",
            "        # Remove REPLACEMENT characters",
            "        new_value = new_value.replace(\"\\ufffd\", \"\")",
            "",
            "        # Lowercase it--this breaks the value, but makes it easier to match",
            "        # against",
            "        new_value = new_value.lower()",
            "",
            "        try:",
            "            # Drop attributes with uri values that have protocols that aren't",
            "            # allowed",
            "            parsed = urlparse(new_value)",
            "        except ValueError:",
            "            # URI is impossible to parse, therefore it's not allowed",
            "            return None",
            "",
            "        if parsed.scheme:",
            "            # If urlparse found a scheme, check that",
            "            if parsed.scheme in allowed_protocols:",
            "                return value",
            "",
            "        else:",
            "            # Allow uris that are just an anchor",
            "            if new_value.startswith(\"#\"):",
            "                return value",
            "",
            "            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"",
            "            if \":\" in new_value and new_value.split(\":\")[0] in allowed_protocols:",
            "                return value",
            "",
            "            # If there's no protocol/scheme specified, then assume it's \"http\"",
            "            # and see if that's allowed",
            "            if \"http\" in allowed_protocols:",
            "                return value",
            "",
            "        return None",
            "",
            "    def allow_token(self, token):",
            "        \"\"\"Handles the case where we're allowing the tag\"\"\"",
            "        if \"data\" in token:",
            "            # Loop through all the attributes and drop the ones that are not",
            "            # allowed, are unsafe or break other rules. Additionally, fix",
            "            # attribute values that need fixing.",
            "            #",
            "            # At the end of this loop, we have the final set of attributes",
            "            # we're keeping.",
            "            attrs = {}",
            "            for namespaced_name, val in token[\"data\"].items():",
            "                namespace, name = namespaced_name",
            "",
            "                # Drop attributes that are not explicitly allowed",
            "                #",
            "                # NOTE(willkg): We pass in the attribute name--not a namespaced",
            "                # name.",
            "                if not self.attr_filter(token[\"name\"], name, val):",
            "                    continue",
            "",
            "                # Drop attributes with uri values that use a disallowed protocol",
            "                # Sanitize attributes with uri values",
            "                if namespaced_name in self.attr_val_is_uri:",
            "                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)",
            "                    if new_value is None:",
            "                        continue",
            "                    val = new_value",
            "",
            "                # Drop values in svg attrs with non-local IRIs",
            "                if namespaced_name in self.svg_attr_val_allows_ref:",
            "                    new_val = re.sub(r\"url\\s*\\(\\s*[^#\\s][^)]+?\\)\", \" \", unescape(val))",
            "                    new_val = new_val.strip()",
            "                    if not new_val:",
            "                        continue",
            "",
            "                    else:",
            "                        # Replace the val with the unescaped version because",
            "                        # it's a iri",
            "                        val = new_val",
            "",
            "                # Drop href and xlink:href attr for svg elements with non-local IRIs",
            "                if (None, token[\"name\"]) in self.svg_allow_local_href:",
            "                    if namespaced_name in [",
            "                        (None, \"href\"),",
            "                        (html5lib_shim.namespaces[\"xlink\"], \"href\"),",
            "                    ]:",
            "                        if re.search(r\"^\\s*[^#\\s]\", val):",
            "                            continue",
            "",
            "                # If it's a style attribute, sanitize it",
            "                if namespaced_name == (None, \"style\"):",
            "                    val = self.sanitize_css(val)",
            "",
            "                # At this point, we want to keep the attribute, so add it in",
            "                attrs[namespaced_name] = val",
            "",
            "            token[\"data\"] = alphabetize_attributes(attrs)",
            "",
            "        return token",
            "",
            "    def disallowed_token(self, token):",
            "        token_type = token[\"type\"]",
            "        if token_type == \"EndTag\":",
            "            token[\"data\"] = \"</%s>\" % token[\"name\"]",
            "",
            "        elif token[\"data\"]:",
            "            assert token_type in (\"StartTag\", \"EmptyTag\")",
            "            attrs = []",
            "            for (ns, name), v in token[\"data\"].items():",
            "                # If we end up with a namespace, but no name, switch them so we",
            "                # have a valid name to use.",
            "                if ns and not name:",
            "                    ns, name = name, ns",
            "",
            "                # Figure out namespaced name if the namespace is appropriate",
            "                # and exists; if the ns isn't in prefixes, then drop it.",
            "                if ns is None or ns not in html5lib_shim.prefixes:",
            "                    namespaced_name = name",
            "                else:",
            "                    namespaced_name = \"%s:%s\" % (html5lib_shim.prefixes[ns], name)",
            "",
            "                attrs.append(",
            "                    ' %s=\"%s\"'",
            "                    % (",
            "                        namespaced_name,",
            "                        # NOTE(willkg): HTMLSerializer escapes attribute values",
            "                        # already, so if we do it here (like HTMLSerializer does),",
            "                        # then we end up double-escaping.",
            "                        v,",
            "                    )",
            "                )",
            "            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], \"\".join(attrs))",
            "",
            "        else:",
            "            token[\"data\"] = \"<%s>\" % token[\"name\"]",
            "",
            "        if token.get(\"selfClosing\"):",
            "            token[\"data\"] = token[\"data\"][:-1] + \"/>\"",
            "",
            "        token[\"type\"] = \"Characters\"",
            "",
            "        del token[\"name\"]",
            "        return token",
            "",
            "    def sanitize_css(self, style):",
            "        \"\"\"Sanitizes css in style tags\"\"\"",
            "        # Convert entities in the style so that it can be parsed as CSS",
            "        style = html5lib_shim.convert_entities(style)",
            "",
            "        # Drop any url values before we do anything else",
            "        style = re.compile(r\"url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*\").sub(\" \", style)",
            "",
            "        # The gauntlet of sanitization",
            "",
            "        # Validate the css in the style tag and if it's not valid, then drop",
            "        # the whole thing.",
            "        parts = style.split(\";\")",
            "        gauntlet = re.compile(",
            "            r\"\"\"^(  # consider a style attribute value as composed of:",
            "[/:,#%!.\\s\\w]    # a non-newline character",
            "|\\w-\\w           # 3 characters in the form \\w-\\w",
            "|'[\\s\\w]+'\\s*    # a single quoted string of [\\s\\w]+ with trailing space",
            "|\"[\\s\\w]+\"       # a double quoted string of [\\s\\w]+",
            "|\\([\\d,%\\.\\s]+\\) # a parenthesized string of one or more digits, commas, periods, ...",
            ")*$\"\"\",  # ... percent signs, or whitespace e.g. from 'color: hsl(30,100%,50%)'",
            "            flags=re.U | re.VERBOSE,",
            "        )",
            "",
            "        for part in parts:",
            "            if not gauntlet.match(part):",
            "                return \"\"",
            "",
            "        if not re.match(r\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):",
            "            return \"\"",
            "",
            "        clean = []",
            "        for prop, value in re.findall(r\"([-\\w]+)\\s*:\\s*([^:;]*)\", style):",
            "            if not value:",
            "                continue",
            "",
            "            if prop.lower() in self.allowed_css_properties:",
            "                clean.append(prop + \": \" + value + \";\")",
            "",
            "            elif prop.lower() in self.allowed_svg_properties:",
            "                clean.append(prop + \": \" + value + \";\")",
            "",
            "        return \" \".join(clean)"
        ],
        "afterPatchFile": [
            "from __future__ import unicode_literals",
            "",
            "from itertools import chain",
            "import re",
            "import warnings",
            "",
            "import six",
            "from six.moves.urllib.parse import urlparse",
            "from xml.sax.saxutils import unescape",
            "",
            "from bleach import html5lib_shim",
            "from bleach.utils import alphabetize_attributes, force_unicode",
            "",
            "",
            "#: List of allowed tags",
            "ALLOWED_TAGS = [",
            "    \"a\",",
            "    \"abbr\",",
            "    \"acronym\",",
            "    \"b\",",
            "    \"blockquote\",",
            "    \"code\",",
            "    \"em\",",
            "    \"i\",",
            "    \"li\",",
            "    \"ol\",",
            "    \"strong\",",
            "    \"ul\",",
            "]",
            "",
            "",
            "#: Map of allowed attributes by tag",
            "ALLOWED_ATTRIBUTES = {",
            "    \"a\": [\"href\", \"title\"],",
            "    \"abbr\": [\"title\"],",
            "    \"acronym\": [\"title\"],",
            "}",
            "",
            "#: List of allowed styles",
            "ALLOWED_STYLES = []",
            "",
            "#: List of allowed protocols",
            "ALLOWED_PROTOCOLS = [\"http\", \"https\", \"mailto\"]",
            "",
            "#: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)",
            "INVISIBLE_CHARACTERS = \"\".join(",
            "    [chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))]",
            ")",
            "",
            "#: Regexp for characters that are invisible",
            "INVISIBLE_CHARACTERS_RE = re.compile(\"[\" + INVISIBLE_CHARACTERS + \"]\", re.UNICODE)",
            "",
            "#: String to replace invisible characters with. This can be a character, a",
            "#: string, or even a function that takes a Python re matchobj",
            "INVISIBLE_REPLACEMENT_CHAR = \"?\"",
            "",
            "",
            "class Cleaner(object):",
            "    \"\"\"Cleaner for cleaning HTML fragments of malicious content",
            "",
            "    This cleaner is a security-focused function whose sole purpose is to remove",
            "    malicious content from a string such that it can be displayed as content in",
            "    a web page.",
            "",
            "    To use::",
            "",
            "        from bleach.sanitizer import Cleaner",
            "",
            "        cleaner = Cleaner()",
            "",
            "        for text in all_the_yucky_things:",
            "            sanitized = cleaner.clean(text)",
            "",
            "    .. Note::",
            "",
            "       This cleaner is not designed to use to transform content to be used in",
            "       non-web-page contexts.",
            "",
            "    .. Warning::",
            "",
            "       This cleaner is not thread-safe--the html parser has internal state.",
            "       Create a separate cleaner per thread!",
            "",
            "",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        tags=ALLOWED_TAGS,",
            "        attributes=ALLOWED_ATTRIBUTES,",
            "        styles=ALLOWED_STYLES,",
            "        protocols=ALLOWED_PROTOCOLS,",
            "        strip=False,",
            "        strip_comments=True,",
            "        filters=None,",
            "    ):",
            "        \"\"\"Initializes a Cleaner",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip: whether or not to strip disallowed elements",
            "",
            "        :arg bool strip_comments: whether or not to strip HTML comments",
            "",
            "        :arg list filters: list of html5lib Filter classes to pass streamed content through",
            "",
            "            .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters",
            "",
            "            .. Warning::",
            "",
            "               Using filters changes the output of ``bleach.Cleaner.clean``.",
            "               Make sure the way the filters change the output are secure.",
            "",
            "        \"\"\"",
            "        self.tags = tags",
            "        self.attributes = attributes",
            "        self.styles = styles",
            "        self.protocols = protocols",
            "        self.strip = strip",
            "        self.strip_comments = strip_comments",
            "        self.filters = filters or []",
            "",
            "        self.parser = html5lib_shim.BleachHTMLParser(",
            "            tags=self.tags,",
            "            strip=self.strip,",
            "            consume_entities=False,",
            "            namespaceHTMLElements=False,",
            "        )",
            "        self.walker = html5lib_shim.getTreeWalker(\"etree\")",
            "        self.serializer = html5lib_shim.BleachHTMLSerializer(",
            "            quote_attr_values=\"always\",",
            "            omit_optional_tags=False,",
            "            escape_lt_in_attrs=True,",
            "            # We want to leave entities as they are without escaping or",
            "            # resolving or expanding",
            "            resolve_entities=False,",
            "            # Bleach has its own sanitizer, so don't use the html5lib one",
            "            sanitize=False,",
            "            # Bleach sanitizer alphabetizes already, so don't use the html5lib one",
            "            alphabetical_attributes=False,",
            "        )",
            "",
            "    def clean(self, text):",
            "        \"\"\"Cleans text and returns sanitized result as unicode",
            "",
            "        :arg str text: text to be cleaned",
            "",
            "        :returns: sanitized text as unicode",
            "",
            "        :raises TypeError: if ``text`` is not a text type",
            "",
            "        \"\"\"",
            "        if not isinstance(text, six.string_types):",
            "            message = (",
            "                \"argument cannot be of '{name}' type, must be of text type\".format(",
            "                    name=text.__class__.__name__",
            "                )",
            "            )",
            "            raise TypeError(message)",
            "",
            "        if not text:",
            "            return \"\"",
            "",
            "        text = force_unicode(text)",
            "",
            "        dom = self.parser.parseFragment(text)",
            "        filtered = BleachSanitizerFilter(",
            "            source=self.walker(dom),",
            "            # Bleach-sanitizer-specific things",
            "            attributes=self.attributes,",
            "            strip_disallowed_elements=self.strip,",
            "            strip_html_comments=self.strip_comments,",
            "            # html5lib-sanitizer things",
            "            allowed_elements=self.tags,",
            "            allowed_css_properties=self.styles,",
            "            allowed_protocols=self.protocols,",
            "            allowed_svg_properties=[],",
            "        )",
            "",
            "        # Apply any filters after the BleachSanitizerFilter",
            "        for filter_class in self.filters:",
            "            filtered = filter_class(source=filtered)",
            "",
            "        return self.serializer.render(filtered)",
            "",
            "",
            "def attribute_filter_factory(attributes):",
            "    \"\"\"Generates attribute filter function for the given attributes value",
            "",
            "    The attributes value can take one of several shapes. This returns a filter",
            "    function appropriate to the attributes value. One nice thing about this is",
            "    that there's less if/then shenanigans in the ``allow_token`` method.",
            "",
            "    \"\"\"",
            "    if callable(attributes):",
            "        return attributes",
            "",
            "    if isinstance(attributes, dict):",
            "",
            "        def _attr_filter(tag, attr, value):",
            "            if tag in attributes:",
            "                attr_val = attributes[tag]",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                if attr in attr_val:",
            "                    return True",
            "",
            "            if \"*\" in attributes:",
            "                attr_val = attributes[\"*\"]",
            "                if callable(attr_val):",
            "                    return attr_val(tag, attr, value)",
            "",
            "                return attr in attr_val",
            "",
            "            return False",
            "",
            "        return _attr_filter",
            "",
            "    if isinstance(attributes, list):",
            "",
            "        def _attr_filter(tag, attr, value):",
            "            return attr in attributes",
            "",
            "        return _attr_filter",
            "",
            "    raise ValueError(\"attributes needs to be a callable, a list or a dict\")",
            "",
            "",
            "class BleachSanitizerFilter(html5lib_shim.SanitizerFilter):",
            "    \"\"\"html5lib Filter that sanitizes text",
            "",
            "    This filter can be used anywhere html5lib filters can be used.",
            "",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        source,",
            "        attributes=ALLOWED_ATTRIBUTES,",
            "        strip_disallowed_elements=False,",
            "        strip_html_comments=True,",
            "        **kwargs",
            "    ):",
            "        \"\"\"Creates a BleachSanitizerFilter instance",
            "",
            "        :arg Treewalker source: stream",
            "",
            "        :arg list tags: allowed list of tags; defaults to",
            "            ``bleach.sanitizer.ALLOWED_TAGS``",
            "",
            "        :arg dict attributes: allowed attributes; can be a callable, list or dict;",
            "            defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``",
            "",
            "        :arg list styles: allowed list of css styles; defaults to",
            "            ``bleach.sanitizer.ALLOWED_STYLES``",
            "",
            "        :arg list protocols: allowed list of protocols for links; defaults",
            "            to ``bleach.sanitizer.ALLOWED_PROTOCOLS``",
            "",
            "        :arg bool strip_disallowed_elements: whether or not to strip disallowed",
            "            elements",
            "",
            "        :arg bool strip_html_comments: whether or not to strip HTML comments",
            "",
            "        \"\"\"",
            "        self.attr_filter = attribute_filter_factory(attributes)",
            "        self.strip_disallowed_elements = strip_disallowed_elements",
            "        self.strip_html_comments = strip_html_comments",
            "",
            "        # filter out html5lib deprecation warnings to use bleach from BleachSanitizerFilter init",
            "        warnings.filterwarnings(",
            "            \"ignore\",",
            "            message=\"html5lib's sanitizer is deprecated\",",
            "            category=DeprecationWarning,",
            "            module=\"bleach._vendor.html5lib\",",
            "        )",
            "        return super(BleachSanitizerFilter, self).__init__(source, **kwargs)",
            "",
            "    def sanitize_stream(self, token_iterator):",
            "        for token in token_iterator:",
            "            ret = self.sanitize_token(token)",
            "",
            "            if not ret:",
            "                continue",
            "",
            "            if isinstance(ret, list):",
            "                for subtoken in ret:",
            "                    yield subtoken",
            "            else:",
            "                yield ret",
            "",
            "    def merge_characters(self, token_iterator):",
            "        \"\"\"Merge consecutive Characters tokens in a stream\"\"\"",
            "        characters_buffer = []",
            "",
            "        for token in token_iterator:",
            "            if characters_buffer:",
            "                if token[\"type\"] == \"Characters\":",
            "                    characters_buffer.append(token)",
            "                    continue",
            "                else:",
            "                    # Merge all the characters tokens together into one and then",
            "                    # operate on it.",
            "                    new_token = {",
            "                        \"data\": \"\".join(",
            "                            [char_token[\"data\"] for char_token in characters_buffer]",
            "                        ),",
            "                        \"type\": \"Characters\",",
            "                    }",
            "                    characters_buffer = []",
            "                    yield new_token",
            "",
            "            elif token[\"type\"] == \"Characters\":",
            "                characters_buffer.append(token)",
            "                continue",
            "",
            "            yield token",
            "",
            "        new_token = {",
            "            \"data\": \"\".join([char_token[\"data\"] for char_token in characters_buffer]),",
            "            \"type\": \"Characters\",",
            "        }",
            "        yield new_token",
            "",
            "    def __iter__(self):",
            "        return self.merge_characters(",
            "            self.sanitize_stream(html5lib_shim.Filter.__iter__(self))",
            "        )",
            "",
            "    def sanitize_token(self, token):",
            "        \"\"\"Sanitize a token either by HTML-encoding or dropping.",
            "",
            "        Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':",
            "        ['attribute', 'pairs'], 'tag': callable}.",
            "",
            "        Here callable is a function with two arguments of attribute name and",
            "        value. It should return true of false.",
            "",
            "        Also gives the option to strip tags instead of encoding.",
            "",
            "        :arg dict token: token to sanitize",
            "",
            "        :returns: token or list of tokens",
            "",
            "        \"\"\"",
            "        token_type = token[\"type\"]",
            "        if token_type in [\"StartTag\", \"EndTag\", \"EmptyTag\"]:",
            "            if token[\"name\"] in self.allowed_elements:",
            "                return self.allow_token(token)",
            "",
            "            elif self.strip_disallowed_elements:",
            "                return None",
            "",
            "            else:",
            "                if \"data\" in token:",
            "                    # Alphabetize the attributes before calling .disallowed_token()",
            "                    # so that the resulting string is stable",
            "                    token[\"data\"] = alphabetize_attributes(token[\"data\"])",
            "                return self.disallowed_token(token)",
            "",
            "        elif token_type == \"Comment\":",
            "            if not self.strip_html_comments:",
            "                # call lxml.sax.saxutils to escape &, <, and > in addition to \" and '",
            "                token[\"data\"] = html5lib_shim.escape(",
            "                    token[\"data\"], entities={'\"': \"&quot;\", \"'\": \"&#x27;\"}",
            "                )",
            "                return token",
            "            else:",
            "                return None",
            "",
            "        elif token_type == \"Characters\":",
            "            return self.sanitize_characters(token)",
            "",
            "        else:",
            "            return token",
            "",
            "    def sanitize_characters(self, token):",
            "        \"\"\"Handles Characters tokens",
            "",
            "        Our overridden tokenizer doesn't do anything with entities. However,",
            "        that means that the serializer will convert all ``&`` in Characters",
            "        tokens to ``&amp;``.",
            "",
            "        Since we don't want that, we extract entities here and convert them to",
            "        Entity tokens so the serializer will let them be.",
            "",
            "        :arg token: the Characters token to work on",
            "",
            "        :returns: a list of tokens",
            "",
            "        \"\"\"",
            "        data = token.get(\"data\", \"\")",
            "",
            "        if not data:",
            "            return token",
            "",
            "        data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)",
            "        token[\"data\"] = data",
            "",
            "        # If there isn't a & in the data, we can return now",
            "        if \"&\" not in data:",
            "            return token",
            "",
            "        new_tokens = []",
            "",
            "        # For each possible entity that starts with a \"&\", we try to extract an",
            "        # actual entity and re-tokenize accordingly",
            "        for part in html5lib_shim.next_possible_entity(data):",
            "            if not part:",
            "                continue",
            "",
            "            if part.startswith(\"&\"):",
            "                entity = html5lib_shim.match_entity(part)",
            "                if entity is not None:",
            "                    if entity == \"amp\":",
            "                        # LinkifyFilter can't match urls across token boundaries",
            "                        # which is problematic with &amp; since that shows up in",
            "                        # querystrings all the time. This special-cases &amp;",
            "                        # and converts it to a & and sticks it in as a",
            "                        # Characters token. It'll get merged with surrounding",
            "                        # tokens in the BleachSanitizerfilter.__iter__ and",
            "                        # escaped in the serializer.",
            "                        new_tokens.append({\"type\": \"Characters\", \"data\": \"&\"})",
            "                    else:",
            "                        new_tokens.append({\"type\": \"Entity\", \"name\": entity})",
            "",
            "                    # Length of the entity plus 2--one for & at the beginning",
            "                    # and one for ; at the end",
            "                    remainder = part[len(entity) + 2 :]",
            "                    if remainder:",
            "                        new_tokens.append({\"type\": \"Characters\", \"data\": remainder})",
            "                    continue",
            "",
            "            new_tokens.append({\"type\": \"Characters\", \"data\": part})",
            "",
            "        return new_tokens",
            "",
            "    def sanitize_uri_value(self, value, allowed_protocols):",
            "        \"\"\"Checks a uri value to see if it's allowed",
            "",
            "        :arg value: the uri value to sanitize",
            "        :arg allowed_protocols: list of allowed protocols",
            "",
            "        :returns: allowed value or None",
            "",
            "        \"\"\"",
            "        # NOTE(willkg): This transforms the value into one that's easier to",
            "        # match and verify, but shouldn't get returned since it's vastly",
            "        # different than the original value.",
            "",
            "        # Convert all character entities in the value",
            "        new_value = html5lib_shim.convert_entities(value)",
            "",
            "        # Nix backtick, space characters, and control characters",
            "        new_value = re.sub(r\"[`\\000-\\040\\177-\\240\\s]+\", \"\", new_value)",
            "",
            "        # Remove REPLACEMENT characters",
            "        new_value = new_value.replace(\"\\ufffd\", \"\")",
            "",
            "        # Lowercase it--this breaks the value, but makes it easier to match",
            "        # against",
            "        new_value = new_value.lower()",
            "",
            "        try:",
            "            # Drop attributes with uri values that have protocols that aren't",
            "            # allowed",
            "            parsed = urlparse(new_value)",
            "        except ValueError:",
            "            # URI is impossible to parse, therefore it's not allowed",
            "            return None",
            "",
            "        if parsed.scheme:",
            "            # If urlparse found a scheme, check that",
            "            if parsed.scheme in allowed_protocols:",
            "                return value",
            "",
            "        else:",
            "            # Allow uris that are just an anchor",
            "            if new_value.startswith(\"#\"):",
            "                return value",
            "",
            "            # Handle protocols that urlparse doesn't recognize like \"myprotocol\"",
            "            if \":\" in new_value and new_value.split(\":\")[0] in allowed_protocols:",
            "                return value",
            "",
            "            # If there's no protocol/scheme specified, then assume it's \"http\"",
            "            # and see if that's allowed",
            "            if \"http\" in allowed_protocols:",
            "                return value",
            "",
            "        return None",
            "",
            "    def allow_token(self, token):",
            "        \"\"\"Handles the case where we're allowing the tag\"\"\"",
            "        if \"data\" in token:",
            "            # Loop through all the attributes and drop the ones that are not",
            "            # allowed, are unsafe or break other rules. Additionally, fix",
            "            # attribute values that need fixing.",
            "            #",
            "            # At the end of this loop, we have the final set of attributes",
            "            # we're keeping.",
            "            attrs = {}",
            "            for namespaced_name, val in token[\"data\"].items():",
            "                namespace, name = namespaced_name",
            "",
            "                # Drop attributes that are not explicitly allowed",
            "                #",
            "                # NOTE(willkg): We pass in the attribute name--not a namespaced",
            "                # name.",
            "                if not self.attr_filter(token[\"name\"], name, val):",
            "                    continue",
            "",
            "                # Drop attributes with uri values that use a disallowed protocol",
            "                # Sanitize attributes with uri values",
            "                if namespaced_name in self.attr_val_is_uri:",
            "                    new_value = self.sanitize_uri_value(val, self.allowed_protocols)",
            "                    if new_value is None:",
            "                        continue",
            "                    val = new_value",
            "",
            "                # Drop values in svg attrs with non-local IRIs",
            "                if namespaced_name in self.svg_attr_val_allows_ref:",
            "                    new_val = re.sub(r\"url\\s*\\(\\s*[^#\\s][^)]+?\\)\", \" \", unescape(val))",
            "                    new_val = new_val.strip()",
            "                    if not new_val:",
            "                        continue",
            "",
            "                    else:",
            "                        # Replace the val with the unescaped version because",
            "                        # it's a iri",
            "                        val = new_val",
            "",
            "                # Drop href and xlink:href attr for svg elements with non-local IRIs",
            "                if (None, token[\"name\"]) in self.svg_allow_local_href:",
            "                    if namespaced_name in [",
            "                        (None, \"href\"),",
            "                        (html5lib_shim.namespaces[\"xlink\"], \"href\"),",
            "                    ]:",
            "                        if re.search(r\"^\\s*[^#\\s]\", val):",
            "                            continue",
            "",
            "                # If it's a style attribute, sanitize it",
            "                if namespaced_name == (None, \"style\"):",
            "                    val = self.sanitize_css(val)",
            "",
            "                # At this point, we want to keep the attribute, so add it in",
            "                attrs[namespaced_name] = val",
            "",
            "            token[\"data\"] = alphabetize_attributes(attrs)",
            "",
            "        return token",
            "",
            "    def disallowed_token(self, token):",
            "        token_type = token[\"type\"]",
            "        if token_type == \"EndTag\":",
            "            token[\"data\"] = \"</%s>\" % token[\"name\"]",
            "",
            "        elif token[\"data\"]:",
            "            assert token_type in (\"StartTag\", \"EmptyTag\")",
            "            attrs = []",
            "            for (ns, name), v in token[\"data\"].items():",
            "                # If we end up with a namespace, but no name, switch them so we",
            "                # have a valid name to use.",
            "                if ns and not name:",
            "                    ns, name = name, ns",
            "",
            "                # Figure out namespaced name if the namespace is appropriate",
            "                # and exists; if the ns isn't in prefixes, then drop it.",
            "                if ns is None or ns not in html5lib_shim.prefixes:",
            "                    namespaced_name = name",
            "                else:",
            "                    namespaced_name = \"%s:%s\" % (html5lib_shim.prefixes[ns], name)",
            "",
            "                attrs.append(",
            "                    ' %s=\"%s\"'",
            "                    % (",
            "                        namespaced_name,",
            "                        # NOTE(willkg): HTMLSerializer escapes attribute values",
            "                        # already, so if we do it here (like HTMLSerializer does),",
            "                        # then we end up double-escaping.",
            "                        v,",
            "                    )",
            "                )",
            "            token[\"data\"] = \"<%s%s>\" % (token[\"name\"], \"\".join(attrs))",
            "",
            "        else:",
            "            token[\"data\"] = \"<%s>\" % token[\"name\"]",
            "",
            "        if token.get(\"selfClosing\"):",
            "            token[\"data\"] = token[\"data\"][:-1] + \"/>\"",
            "",
            "        token[\"type\"] = \"Characters\"",
            "",
            "        del token[\"name\"]",
            "        return token",
            "",
            "    def sanitize_css(self, style):",
            "        \"\"\"Sanitizes css in style tags\"\"\"",
            "        # Convert entities in the style so that it can be parsed as CSS",
            "        style = html5lib_shim.convert_entities(style)",
            "",
            "        # Drop any url values before we do anything else",
            "        style = re.compile(r\"url\\s*\\(\\s*[^\\s)]+?\\s*\\)\\s*\").sub(\" \", style)",
            "",
            "        # The gauntlet of sanitization",
            "",
            "        # Validate the css in the style tag and if it's not valid, then drop",
            "        # the whole thing.",
            "        parts = style.split(\";\")",
            "        gauntlet = re.compile(",
            "            r\"\"\"^(  # consider a style attribute value as composed of:",
            "[/:,#%!.\\s\\w]    # a non-newline character",
            "|\\w-\\w           # 3 characters in the form \\w-\\w",
            "|'[\\s\\w]+'\\s*    # a single quoted string of [\\s\\w]+ with trailing space",
            "|\"[\\s\\w]+\"       # a double quoted string of [\\s\\w]+",
            "|\\([\\d,%\\.\\s]+\\) # a parenthesized string of one or more digits, commas, periods, ...",
            ")*$\"\"\",  # ... percent signs, or whitespace e.g. from 'color: hsl(30,100%,50%)'",
            "            flags=re.U | re.VERBOSE,",
            "        )",
            "",
            "        for part in parts:",
            "            if not gauntlet.match(part):",
            "                return \"\"",
            "",
            "        if not re.match(r\"^\\s*([-\\w]+\\s*:[^:;]*(;\\s*|$))*$\", style):",
            "            return \"\"",
            "",
            "        clean = []",
            "        for prop, value in re.findall(r\"([-\\w]+)\\s*:\\s*([^:;]*)\", style):",
            "            if not value:",
            "                continue",
            "",
            "            if prop.lower() in self.allowed_css_properties:",
            "                clean.append(prop + \": \" + value + \";\")",
            "",
            "            elif prop.lower() in self.allowed_svg_properties:",
            "                clean.append(prop + \": \" + value + \";\")",
            "",
            "        return \" \".join(clean)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "omeroweb.webgateway.views",
            "bleach.sanitizer.BleachSanitizerFilter.sanitize_stream"
        ]
    }
}