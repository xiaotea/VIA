{
    "src/zenml/integrations/langchain/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": "     \"\"\"Definition of langchain integration for ZenML.\"\"\""
            },
            "1": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": "     NAME = LANGCHAIN"
            },
            "3": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    REQUIREMENTS = [\"langchain>=0.0.116\"]"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+    REQUIREMENTS = [\"langchain>=0.0.116\", \"pyyaml>=6.0.1\"]"
            },
            "5": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": "     @classmethod"
            },
            "7": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": "     def activate(cls) -> None:"
            }
        },
        "frontPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Initialization of the langchain integration.\"\"\"",
            "",
            "from zenml.integrations.constants import LANGCHAIN",
            "from zenml.integrations.integration import Integration",
            "",
            "from zenml.logger import get_logger",
            "",
            "logger = get_logger(__name__)",
            "",
            "",
            "class LangchainIntegration(Integration):",
            "    \"\"\"Definition of langchain integration for ZenML.\"\"\"",
            "",
            "    NAME = LANGCHAIN",
            "    REQUIREMENTS = [\"langchain>=0.0.116\"]",
            "",
            "    @classmethod",
            "    def activate(cls) -> None:",
            "        \"\"\"Activates the integration.\"\"\"",
            "        from zenml.integrations.langchain import materializers  # noqa",
            "",
            "",
            "LangchainIntegration.check_installation()"
        ],
        "afterPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Initialization of the langchain integration.\"\"\"",
            "",
            "from zenml.integrations.constants import LANGCHAIN",
            "from zenml.integrations.integration import Integration",
            "",
            "from zenml.logger import get_logger",
            "",
            "logger = get_logger(__name__)",
            "",
            "",
            "class LangchainIntegration(Integration):",
            "    \"\"\"Definition of langchain integration for ZenML.\"\"\"",
            "",
            "    NAME = LANGCHAIN",
            "    REQUIREMENTS = [\"langchain>=0.0.116\", \"pyyaml>=6.0.1\"]",
            "",
            "    @classmethod",
            "    def activate(cls) -> None:",
            "        \"\"\"Activates the integration.\"\"\"",
            "        from zenml.integrations.langchain import materializers  # noqa",
            "",
            "",
            "LangchainIntegration.check_installation()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "28": [
                "LangchainIntegration"
            ]
        },
        "addLocation": []
    },
    "src/zenml/integrations/llama_index/materializers/document_materializer.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": "     LCDocument = Any"
            },
            "1": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 31,
                "PatchRowcode": " else:"
            },
            "2": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 32,
                "PatchRowcode": "     from langchain.docstore.document import Document as LCDocument"
            },
            "3": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    from llama_index.readers.schema.base import Document"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+    from llama_index.readers.schema.base import (  # type: ignore[import]"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+        Document,"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+    )"
            },
            "7": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 37,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 38,
                "PatchRowcode": " class LlamaIndexDocumentMaterializer(LangchainDocumentMaterializer):"
            }
        },
        "frontPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Implementation of the llama-index document materializer.\"\"\"",
            "",
            "import sys",
            "from typing import TYPE_CHECKING, Any, ClassVar, Dict, Tuple, Type",
            "",
            "from zenml.enums import ArtifactType",
            "from zenml.integrations.langchain.materializers.document_materializer import (",
            "    LangchainDocumentMaterializer,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from zenml.metadata.metadata_types import MetadataType",
            "",
            "",
            "if TYPE_CHECKING and sys.version_info < (3, 8):",
            "    Document = Any",
            "    LCDocument = Any",
            "else:",
            "    from langchain.docstore.document import Document as LCDocument",
            "    from llama_index.readers.schema.base import Document",
            "",
            "",
            "class LlamaIndexDocumentMaterializer(LangchainDocumentMaterializer):",
            "    \"\"\"Handle serialization and deserialization of llama-index documents.\"\"\"",
            "",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.DATA",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (Document,)",
            "",
            "    def load(self, data_type: Type[Any]) -> Any:",
            "        \"\"\"Reads a llama-index document from JSON.",
            "",
            "        Args:",
            "            data_type: The type of the data to read.",
            "",
            "        Returns:",
            "            The data read.",
            "        \"\"\"",
            "        return Document.from_langchain_format(super().load(LCDocument))",
            "",
            "    def save(self, data: Any) -> None:",
            "        \"\"\"Serialize a llama-index document as a Langchain document.",
            "",
            "        Args:",
            "            data: The data to store.",
            "        \"\"\"",
            "        super().save(data.to_langchain_format())",
            "",
            "    def extract_metadata(self, data: Any) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extract metadata from the given Llama Index document.",
            "",
            "        Args:",
            "            data: The BaseModel object to extract metadata from.",
            "",
            "        Returns:",
            "            The extracted metadata as a dictionary.",
            "        \"\"\"",
            "        return super().extract_metadata(data.to_langchain_format())"
        ],
        "afterPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Implementation of the llama-index document materializer.\"\"\"",
            "",
            "import sys",
            "from typing import TYPE_CHECKING, Any, ClassVar, Dict, Tuple, Type",
            "",
            "from zenml.enums import ArtifactType",
            "from zenml.integrations.langchain.materializers.document_materializer import (",
            "    LangchainDocumentMaterializer,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from zenml.metadata.metadata_types import MetadataType",
            "",
            "",
            "if TYPE_CHECKING and sys.version_info < (3, 8):",
            "    Document = Any",
            "    LCDocument = Any",
            "else:",
            "    from langchain.docstore.document import Document as LCDocument",
            "    from llama_index.readers.schema.base import (  # type: ignore[import]",
            "        Document,",
            "    )",
            "",
            "",
            "class LlamaIndexDocumentMaterializer(LangchainDocumentMaterializer):",
            "    \"\"\"Handle serialization and deserialization of llama-index documents.\"\"\"",
            "",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.DATA",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (Document,)",
            "",
            "    def load(self, data_type: Type[Any]) -> Any:",
            "        \"\"\"Reads a llama-index document from JSON.",
            "",
            "        Args:",
            "            data_type: The type of the data to read.",
            "",
            "        Returns:",
            "            The data read.",
            "        \"\"\"",
            "        return Document.from_langchain_format(super().load(LCDocument))",
            "",
            "    def save(self, data: Any) -> None:",
            "        \"\"\"Serialize a llama-index document as a Langchain document.",
            "",
            "        Args:",
            "            data: The data to store.",
            "        \"\"\"",
            "        super().save(data.to_langchain_format())",
            "",
            "    def extract_metadata(self, data: Any) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extract metadata from the given Llama Index document.",
            "",
            "        Args:",
            "            data: The BaseModel object to extract metadata from.",
            "",
            "        Returns:",
            "            The extracted metadata as a dictionary.",
            "        \"\"\"",
            "        return super().extract_metadata(data.to_langchain_format())"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "33": []
        },
        "addLocation": []
    },
    "src/zenml/integrations/llama_index/materializers/gpt_index_materializer.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     GPTFaissIndex = Any"
            },
            "1": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     T = TypeVar(\"T\", bound=Any)"
            },
            "2": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " else:"
            },
            "3": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    from llama_index.indices.base import BaseGPTIndex"
            },
            "4": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    from llama_index.indices.vector_store import GPTFaissIndex"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+    from llama_index.indices.base import (  # type: ignore[import]"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+        BaseGPTIndex,"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+    )"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+    from llama_index.indices.vector_store import (  # type: ignore[import]"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        GPTFaissIndex,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+    )"
            },
            "11": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     T = TypeVar(\"T\", bound=BaseGPTIndex[Any])"
            },
            "13": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Implementation of the llama-index GPT index materializer.\"\"\"",
            "",
            "import os",
            "import sys",
            "import tempfile",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    ClassVar,",
            "    Generic,",
            "    Tuple,",
            "    Type,",
            "    TypeVar,",
            "    cast,",
            ")",
            "",
            "from zenml.enums import ArtifactType",
            "from zenml.io import fileio",
            "from zenml.materializers.base_materializer import BaseMaterializer",
            "",
            "DEFAULT_FILENAME = \"index.json\"",
            "DEFAULT_FAISS_FILENAME = \"faiss_index.json\"",
            "",
            "if TYPE_CHECKING and sys.version_info < (3, 8):",
            "    BaseGPTIndex = Any",
            "    GPTFaissIndex = Any",
            "    T = TypeVar(\"T\", bound=Any)",
            "else:",
            "    from llama_index.indices.base import BaseGPTIndex",
            "    from llama_index.indices.vector_store import GPTFaissIndex",
            "",
            "    T = TypeVar(\"T\", bound=BaseGPTIndex[Any])",
            "",
            "",
            "class LlamaIndexGPTIndexMaterializer(Generic[T], BaseMaterializer):",
            "    \"\"\"Materializer for llama_index GPT indices.\"\"\"",
            "",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.MODEL",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (BaseGPTIndex,)",
            "",
            "    def load(self, data_type: Type[T]) -> T:",
            "        \"\"\"Loads a llama-index GPT index from disk.",
            "",
            "        Args:",
            "            data_type: The type of the index.",
            "",
            "        Returns:",
            "            The index.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "",
            "        # Create a temporary folder",
            "        temp_dir = tempfile.mkdtemp(prefix=\"zenml-temp-\")",
            "        temp_file = os.path.join(str(temp_dir), DEFAULT_FILENAME)",
            "",
            "        # Copy from artifact store to temporary file",
            "        fileio.copy(filepath, temp_file)",
            "",
            "        index = data_type.load_from_disk(save_path=filepath)",
            "        assert isinstance(index, data_type)",
            "",
            "        # Cleanup and return",
            "        fileio.rmtree(temp_dir)",
            "        return index",
            "",
            "    def save(self, index: T) -> None:",
            "        \"\"\"Save a llama-index GPT index to disk.",
            "",
            "        Args:",
            "            index: The index to save.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            mode=\"w\", suffix=\".json\", delete=False",
            "        ) as f:",
            "            index.save_to_disk(save_path=f.name)",
            "            # Copy it into artifact store",
            "            fileio.copy(f.name, filepath)",
            "",
            "        # Close and remove the temporary file",
            "        f.close()",
            "        fileio.remove(f.name)",
            "",
            "",
            "class LlamaIndexGPTFaissIndexMaterializer(BaseMaterializer):",
            "    \"\"\"Materializer for llama_index GPT faiss indices.\"\"\"",
            "",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.MODEL",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (GPTFaissIndex,)",
            "",
            "    def load(self, data_type: Type[GPTFaissIndex]) -> GPTFaissIndex:",
            "        \"\"\"Load a llama-index GPT faiss index from disk.",
            "",
            "        Args:",
            "            data_type: The type of the index.",
            "",
            "        Returns:",
            "            The index.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "        faiss_filepath = os.path.join(self.uri, DEFAULT_FAISS_FILENAME)",
            "",
            "        # Create a temporary folder",
            "        temp_dir = tempfile.mkdtemp(prefix=\"zenml-temp-\")",
            "        temp_file = os.path.join(str(temp_dir), DEFAULT_FILENAME)",
            "",
            "        # Copy from artifact store to temporary file",
            "        fileio.copy(filepath, temp_file)",
            "",
            "        index = data_type.load_from_disk(",
            "            save_path=filepath, faiss_index_save_path=faiss_filepath",
            "        )",
            "",
            "        # Cleanup and return",
            "        fileio.rmtree(temp_dir)",
            "        return cast(GPTFaissIndex, index)",
            "",
            "    def save(self, index: GPTFaissIndex) -> None:",
            "        \"\"\"Save a llama-index GPT faiss index to disk.",
            "",
            "        Args:",
            "            index: The index to save.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "        faiss_filepath = os.path.join(self.uri, DEFAULT_FAISS_FILENAME)",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            mode=\"w\", suffix=\".json\", delete=False",
            "        ) as f:",
            "            index.save_to_disk(",
            "                save_path=f.name, faiss_index_save_path=faiss_filepath",
            "            )",
            "            # Copy it into artifact store",
            "            fileio.copy(f.name, filepath)",
            "",
            "        # Close and remove the temporary file",
            "        f.close()",
            "        fileio.remove(f.name)"
        ],
        "afterPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Implementation of the llama-index GPT index materializer.\"\"\"",
            "",
            "import os",
            "import sys",
            "import tempfile",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    ClassVar,",
            "    Generic,",
            "    Tuple,",
            "    Type,",
            "    TypeVar,",
            "    cast,",
            ")",
            "",
            "from zenml.enums import ArtifactType",
            "from zenml.io import fileio",
            "from zenml.materializers.base_materializer import BaseMaterializer",
            "",
            "DEFAULT_FILENAME = \"index.json\"",
            "DEFAULT_FAISS_FILENAME = \"faiss_index.json\"",
            "",
            "if TYPE_CHECKING and sys.version_info < (3, 8):",
            "    BaseGPTIndex = Any",
            "    GPTFaissIndex = Any",
            "    T = TypeVar(\"T\", bound=Any)",
            "else:",
            "    from llama_index.indices.base import (  # type: ignore[import]",
            "        BaseGPTIndex,",
            "    )",
            "    from llama_index.indices.vector_store import (  # type: ignore[import]",
            "        GPTFaissIndex,",
            "    )",
            "",
            "    T = TypeVar(\"T\", bound=BaseGPTIndex[Any])",
            "",
            "",
            "class LlamaIndexGPTIndexMaterializer(Generic[T], BaseMaterializer):",
            "    \"\"\"Materializer for llama_index GPT indices.\"\"\"",
            "",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.MODEL",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (BaseGPTIndex,)",
            "",
            "    def load(self, data_type: Type[T]) -> T:",
            "        \"\"\"Loads a llama-index GPT index from disk.",
            "",
            "        Args:",
            "            data_type: The type of the index.",
            "",
            "        Returns:",
            "            The index.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "",
            "        # Create a temporary folder",
            "        temp_dir = tempfile.mkdtemp(prefix=\"zenml-temp-\")",
            "        temp_file = os.path.join(str(temp_dir), DEFAULT_FILENAME)",
            "",
            "        # Copy from artifact store to temporary file",
            "        fileio.copy(filepath, temp_file)",
            "",
            "        index = data_type.load_from_disk(save_path=filepath)",
            "        assert isinstance(index, data_type)",
            "",
            "        # Cleanup and return",
            "        fileio.rmtree(temp_dir)",
            "        return index",
            "",
            "    def save(self, index: T) -> None:",
            "        \"\"\"Save a llama-index GPT index to disk.",
            "",
            "        Args:",
            "            index: The index to save.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            mode=\"w\", suffix=\".json\", delete=False",
            "        ) as f:",
            "            index.save_to_disk(save_path=f.name)",
            "            # Copy it into artifact store",
            "            fileio.copy(f.name, filepath)",
            "",
            "        # Close and remove the temporary file",
            "        f.close()",
            "        fileio.remove(f.name)",
            "",
            "",
            "class LlamaIndexGPTFaissIndexMaterializer(BaseMaterializer):",
            "    \"\"\"Materializer for llama_index GPT faiss indices.\"\"\"",
            "",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.MODEL",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (GPTFaissIndex,)",
            "",
            "    def load(self, data_type: Type[GPTFaissIndex]) -> GPTFaissIndex:",
            "        \"\"\"Load a llama-index GPT faiss index from disk.",
            "",
            "        Args:",
            "            data_type: The type of the index.",
            "",
            "        Returns:",
            "            The index.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "        faiss_filepath = os.path.join(self.uri, DEFAULT_FAISS_FILENAME)",
            "",
            "        # Create a temporary folder",
            "        temp_dir = tempfile.mkdtemp(prefix=\"zenml-temp-\")",
            "        temp_file = os.path.join(str(temp_dir), DEFAULT_FILENAME)",
            "",
            "        # Copy from artifact store to temporary file",
            "        fileio.copy(filepath, temp_file)",
            "",
            "        index = data_type.load_from_disk(",
            "            save_path=filepath, faiss_index_save_path=faiss_filepath",
            "        )",
            "",
            "        # Cleanup and return",
            "        fileio.rmtree(temp_dir)",
            "        return cast(GPTFaissIndex, index)",
            "",
            "    def save(self, index: GPTFaissIndex) -> None:",
            "        \"\"\"Save a llama-index GPT faiss index to disk.",
            "",
            "        Args:",
            "            index: The index to save.",
            "        \"\"\"",
            "        filepath = os.path.join(self.uri, DEFAULT_FILENAME)",
            "        faiss_filepath = os.path.join(self.uri, DEFAULT_FAISS_FILENAME)",
            "",
            "        with tempfile.NamedTemporaryFile(",
            "            mode=\"w\", suffix=\".json\", delete=False",
            "        ) as f:",
            "            index.save_to_disk(",
            "                save_path=f.name, faiss_index_save_path=faiss_filepath",
            "            )",
            "            # Copy it into artifact store",
            "            fileio.copy(f.name, filepath)",
            "",
            "        # Close and remove the temporary file",
            "        f.close()",
            "        fileio.remove(f.name)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "42": [],
            "43": []
        },
        "addLocation": []
    },
    "src/zenml/integrations/openai/__init__.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": "     \"\"\"Definition of OpenAI integration for ZenML.\"\"\""
            },
            "1": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 23,
                "PatchRowcode": "     NAME = OPEN_AI"
            },
            "3": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    REQUIREMENTS = [\"openai>=0.27.0\"]"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+    REQUIREMENTS = [\"openai>=0.27.0,<1.0.0\"]"
            },
            "5": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " OpenAIIntegration.check_installation()"
            }
        },
        "frontPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Initialization of the OpenAI integration.\"\"\"",
            "",
            "from zenml.integrations.constants import OPEN_AI",
            "from zenml.integrations.integration import Integration",
            "",
            "",
            "class OpenAIIntegration(Integration):",
            "    \"\"\"Definition of OpenAI integration for ZenML.\"\"\"",
            "",
            "    NAME = OPEN_AI",
            "    REQUIREMENTS = [\"openai>=0.27.0\"]",
            "",
            "",
            "OpenAIIntegration.check_installation()"
        ],
        "afterPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Initialization of the OpenAI integration.\"\"\"",
            "",
            "from zenml.integrations.constants import OPEN_AI",
            "from zenml.integrations.integration import Integration",
            "",
            "",
            "class OpenAIIntegration(Integration):",
            "    \"\"\"Definition of OpenAI integration for ZenML.\"\"\"",
            "",
            "    NAME = OPEN_AI",
            "    REQUIREMENTS = [\"openai>=0.27.0,<1.0.0\"]",
            "",
            "",
            "OpenAIIntegration.check_installation()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "24": [
                "OpenAIIntegration"
            ]
        },
        "addLocation": []
    },
    "src/zenml/integrations/openai/hooks/open_ai_failure_hook.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "         sys.stdout = original_stdout"
            },
            "1": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "         rich_traceback = output_captured.getvalue()"
            },
            "2": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        response = openai.ChatCompletion.create(  # type: ignore[no-untyped-call]"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+        response = openai.ChatCompletion.create(  # type: ignore"
            },
            "5": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "             model=model_name,"
            },
            "6": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "             messages=["
            },
            "7": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "                 {"
            }
        },
        "frontPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Functionality for OpenAI standard hooks.\"\"\"",
            "",
            "import io",
            "import sys",
            "",
            "import openai",
            "from rich.console import Console",
            "",
            "from zenml import get_step_context",
            "from zenml.client import Client",
            "from zenml.logger import get_logger",
            "",
            "logger = get_logger(__name__)",
            "",
            "",
            "def openai_alerter_failure_hook_helper(",
            "    exception: BaseException,",
            "    model_name: str,",
            ") -> None:",
            "    \"\"\"Standard failure hook that sends a message to an Alerter.",
            "",
            "    Your OpenAI API key must be stored in the secret store under the name",
            "    \"openai\" and with the key \"api_key\".",
            "",
            "    Args:",
            "        exception: The exception that was raised.",
            "        model_name: The OpenAI model to use for the chatbot.",
            "    \"\"\"",
            "    client = Client()",
            "    context = get_step_context()",
            "",
            "    # get the api_key from the secret store",
            "    try:",
            "        openai_secret = client.get_secret(",
            "            \"openai\", allow_partial_name_match=False",
            "        )",
            "        openai_api_key = openai_secret.secret_values.get(\"api_key\")",
            "    except (KeyError, NotImplementedError):",
            "        openai_api_key = None",
            "",
            "    alerter = client.active_stack.alerter",
            "    if alerter and openai_api_key:",
            "        output_captured = io.StringIO()",
            "        original_stdout = sys.stdout",
            "        sys.stdout = output_captured",
            "        console = Console()",
            "        console.print_exception(show_locals=False)",
            "",
            "        sys.stdout = original_stdout",
            "        rich_traceback = output_captured.getvalue()",
            "",
            "        response = openai.ChatCompletion.create(  # type: ignore[no-untyped-call]",
            "            model=model_name,",
            "            messages=[",
            "                {",
            "                    \"role\": \"user\",",
            "                    \"content\": f\"This is an error message (following an exception of type '{type(exception)}') I encountered while executing a ZenML step. Please suggest ways I might fix the problem. Feel free to give code snippets as examples, and note that your response will be piped to a Slack bot so make sure the formatting is appropriate: {exception} -- {rich_traceback}. Thank you!\",",
            "                }",
            "            ],",
            "        )",
            "        suggestion = response[\"choices\"][0][\"message\"][\"content\"]",
            "        message = \"*Failure Hook Notification! Step failed!*\" + \"\\n\\n\"",
            "        message += f\"Run name: `{context.pipeline_run.name}`\" + \"\\n\"",
            "        message += f\"Step name: `{context.step_run.name}`\" + \"\\n\"",
            "        message += f\"Parameters: `{context.step_run.config.parameters}`\" + \"\\n\"",
            "        message += f\"Exception: `({type(exception)}) {exception}`\" + \"\\n\\n\"",
            "        message += (",
            "            f\"*OpenAI ChatGPT's suggestion (model = `{model_name}`) on how to fix it:*\\n `{suggestion}`\"",
            "            + \"\\n\"",
            "        )",
            "        alerter.post(message)",
            "    elif not openai_api_key:",
            "        logger.warning(",
            "            \"Specified OpenAI failure hook but no OpenAI API key found. Skipping...\"",
            "        )",
            "    else:",
            "        logger.warning(",
            "            \"Specified OpenAI failure hook but no alerter configured in the stack. Skipping...\"",
            "        )",
            "",
            "",
            "def openai_chatgpt_alerter_failure_hook(",
            "    exception: BaseException,",
            ") -> None:",
            "    \"\"\"Alerter hook that uses the OpenAI ChatGPT model.",
            "",
            "    Args:",
            "        exception: The exception that was raised.",
            "    \"\"\"",
            "    openai_alerter_failure_hook_helper(exception, \"gpt-3.5-turbo\")",
            "",
            "",
            "def openai_gpt4_alerter_failure_hook(",
            "    exception: BaseException,",
            ") -> None:",
            "    \"\"\"Alerter hook that uses the OpenAI GPT-4 model.",
            "",
            "    Args:",
            "        exception: The exception that was raised.",
            "    \"\"\"",
            "    openai_alerter_failure_hook_helper(exception, \"gpt-4\")"
        ],
        "afterPatchFile": [
            "#  Copyright (c) ZenML GmbH 2023. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Functionality for OpenAI standard hooks.\"\"\"",
            "",
            "import io",
            "import sys",
            "",
            "import openai",
            "from rich.console import Console",
            "",
            "from zenml import get_step_context",
            "from zenml.client import Client",
            "from zenml.logger import get_logger",
            "",
            "logger = get_logger(__name__)",
            "",
            "",
            "def openai_alerter_failure_hook_helper(",
            "    exception: BaseException,",
            "    model_name: str,",
            ") -> None:",
            "    \"\"\"Standard failure hook that sends a message to an Alerter.",
            "",
            "    Your OpenAI API key must be stored in the secret store under the name",
            "    \"openai\" and with the key \"api_key\".",
            "",
            "    Args:",
            "        exception: The exception that was raised.",
            "        model_name: The OpenAI model to use for the chatbot.",
            "    \"\"\"",
            "    client = Client()",
            "    context = get_step_context()",
            "",
            "    # get the api_key from the secret store",
            "    try:",
            "        openai_secret = client.get_secret(",
            "            \"openai\", allow_partial_name_match=False",
            "        )",
            "        openai_api_key = openai_secret.secret_values.get(\"api_key\")",
            "    except (KeyError, NotImplementedError):",
            "        openai_api_key = None",
            "",
            "    alerter = client.active_stack.alerter",
            "    if alerter and openai_api_key:",
            "        output_captured = io.StringIO()",
            "        original_stdout = sys.stdout",
            "        sys.stdout = output_captured",
            "        console = Console()",
            "        console.print_exception(show_locals=False)",
            "",
            "        sys.stdout = original_stdout",
            "        rich_traceback = output_captured.getvalue()",
            "",
            "        response = openai.ChatCompletion.create(  # type: ignore",
            "            model=model_name,",
            "            messages=[",
            "                {",
            "                    \"role\": \"user\",",
            "                    \"content\": f\"This is an error message (following an exception of type '{type(exception)}') I encountered while executing a ZenML step. Please suggest ways I might fix the problem. Feel free to give code snippets as examples, and note that your response will be piped to a Slack bot so make sure the formatting is appropriate: {exception} -- {rich_traceback}. Thank you!\",",
            "                }",
            "            ],",
            "        )",
            "        suggestion = response[\"choices\"][0][\"message\"][\"content\"]",
            "        message = \"*Failure Hook Notification! Step failed!*\" + \"\\n\\n\"",
            "        message += f\"Run name: `{context.pipeline_run.name}`\" + \"\\n\"",
            "        message += f\"Step name: `{context.step_run.name}`\" + \"\\n\"",
            "        message += f\"Parameters: `{context.step_run.config.parameters}`\" + \"\\n\"",
            "        message += f\"Exception: `({type(exception)}) {exception}`\" + \"\\n\\n\"",
            "        message += (",
            "            f\"*OpenAI ChatGPT's suggestion (model = `{model_name}`) on how to fix it:*\\n `{suggestion}`\"",
            "            + \"\\n\"",
            "        )",
            "        alerter.post(message)",
            "    elif not openai_api_key:",
            "        logger.warning(",
            "            \"Specified OpenAI failure hook but no OpenAI API key found. Skipping...\"",
            "        )",
            "    else:",
            "        logger.warning(",
            "            \"Specified OpenAI failure hook but no alerter configured in the stack. Skipping...\"",
            "        )",
            "",
            "",
            "def openai_chatgpt_alerter_failure_hook(",
            "    exception: BaseException,",
            ") -> None:",
            "    \"\"\"Alerter hook that uses the OpenAI ChatGPT model.",
            "",
            "    Args:",
            "        exception: The exception that was raised.",
            "    \"\"\"",
            "    openai_alerter_failure_hook_helper(exception, \"gpt-3.5-turbo\")",
            "",
            "",
            "def openai_gpt4_alerter_failure_hook(",
            "    exception: BaseException,",
            ") -> None:",
            "    \"\"\"Alerter hook that uses the OpenAI GPT-4 model.",
            "",
            "    Args:",
            "        exception: The exception that was raised.",
            "    \"\"\"",
            "    openai_alerter_failure_hook_helper(exception, \"gpt-4\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "65": [
                "openai_alerter_failure_hook_helper"
            ]
        },
        "addLocation": []
    },
    "src/zenml/materializers/numpy_materializer.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": 151,
                "PatchRowcode": "             output_path: The path to save the histogram to."
            },
            "1": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "             arr: The numpy array of which to save the histogram."
            },
            "2": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "         \"\"\""
            },
            "3": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        import matplotlib.pyplot as plt  # type: ignore"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+        import matplotlib.pyplot as plt"
            },
            "5": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 155,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "         plt.hist(arr)"
            },
            "7": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 157,
                "PatchRowcode": "         with fileio.open(output_path, \"wb\") as f:"
            },
            "8": {
                "beforePatchRowNumber": 183,
                "afterPatchRowNumber": 183,
                "PatchRowcode": "             output_path: The path to save the image to."
            },
            "9": {
                "beforePatchRowNumber": 184,
                "afterPatchRowNumber": 184,
                "PatchRowcode": "             arr: The numpy array to save."
            },
            "10": {
                "beforePatchRowNumber": 185,
                "afterPatchRowNumber": 185,
                "PatchRowcode": "         \"\"\""
            },
            "11": {
                "beforePatchRowNumber": 186,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        from matplotlib.image import imsave  # type: ignore"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 186,
                "PatchRowcode": "+        from matplotlib.image import imsave"
            },
            "13": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 187,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 188,
                "PatchRowcode": "         with fileio.open(output_path, \"wb\") as f:"
            },
            "15": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "             imsave(f, arr)"
            }
        },
        "frontPatchFile": [
            "#  Copyright (c) ZenML GmbH 2021. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Implementation of the ZenML NumPy materializer.\"\"\"",
            "",
            "import os",
            "from collections import Counter",
            "from typing import TYPE_CHECKING, Any, ClassVar, Dict, Tuple, Type",
            "",
            "import numpy as np",
            "",
            "from zenml.enums import ArtifactType, VisualizationType",
            "from zenml.io import fileio",
            "from zenml.logger import get_logger",
            "from zenml.materializers.base_materializer import BaseMaterializer",
            "from zenml.metadata.metadata_types import DType, MetadataType",
            "",
            "if TYPE_CHECKING:",
            "    from numpy.typing import NDArray",
            "",
            "logger = get_logger(__name__)",
            "",
            "",
            "NUMPY_FILENAME = \"data.npy\"",
            "",
            "DATA_FILENAME = \"data.parquet\"",
            "SHAPE_FILENAME = \"shape.json\"",
            "DATA_VAR = \"data_var\"",
            "",
            "",
            "class NumpyMaterializer(BaseMaterializer):",
            "    \"\"\"Materializer to read data to and from pandas.\"\"\"",
            "",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (np.ndarray,)",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.DATA",
            "",
            "    def load(self, data_type: Type[Any]) -> \"Any\":",
            "        \"\"\"Reads a numpy array from a `.npy` file.",
            "",
            "        Args:",
            "            data_type: The type of the data to read.",
            "",
            "",
            "        Raises:",
            "            ImportError: If pyarrow is not installed.",
            "",
            "        Returns:",
            "            The numpy array.",
            "        \"\"\"",
            "        numpy_file = os.path.join(self.uri, NUMPY_FILENAME)",
            "",
            "        if fileio.exists(numpy_file):",
            "            with fileio.open(numpy_file, \"rb\") as f:",
            "                return np.load(f, allow_pickle=True)",
            "        elif fileio.exists(os.path.join(self.uri, DATA_FILENAME)):",
            "            logger.warning(",
            "                \"A legacy artifact was found. \"",
            "                \"This artifact was created with an older version of \"",
            "                \"ZenML. You can still use it, but it will be \"",
            "                \"converted to the new format on the next materialization.\"",
            "            )",
            "            try:",
            "                # Import old materializer dependencies",
            "                import pyarrow as pa  # type: ignore",
            "                import pyarrow.parquet as pq  # type: ignore",
            "",
            "                from zenml.utils import yaml_utils",
            "",
            "                # Read numpy array from parquet file",
            "                shape_dict = yaml_utils.read_json(",
            "                    os.path.join(self.uri, SHAPE_FILENAME)",
            "                )",
            "                shape_tuple = tuple(shape_dict.values())",
            "                with fileio.open(",
            "                    os.path.join(self.uri, DATA_FILENAME), \"rb\"",
            "                ) as f:",
            "                    input_stream = pa.input_stream(f)",
            "                    data = pq.read_table(input_stream)",
            "                vals = getattr(data.to_pandas(), DATA_VAR).values",
            "                return np.reshape(vals, shape_tuple)",
            "            except ImportError:",
            "                raise ImportError(",
            "                    \"You have an old version of a `NumpyMaterializer` \",",
            "                    \"data artifact stored in the artifact store \",",
            "                    \"as a `.parquet` file, which requires `pyarrow` for reading. \",",
            "                    \"You can install `pyarrow` by running `pip install pyarrow`.\",",
            "                )",
            "",
            "    def save(self, arr: \"NDArray[Any]\") -> None:",
            "        \"\"\"Writes a np.ndarray to the artifact store as a `.npy` file.",
            "",
            "        Args:",
            "            arr: The numpy array to write.",
            "        \"\"\"",
            "        with fileio.open(os.path.join(self.uri, NUMPY_FILENAME), \"wb\") as f:",
            "            np.save(f, arr)",
            "",
            "    def save_visualizations(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, VisualizationType]:",
            "        \"\"\"Saves visualizations for a numpy array.",
            "",
            "        If the array is 1D, a histogram is saved. If the array is 2D or 3D with",
            "        3 or 4 channels, an image is saved.",
            "",
            "        Args:",
            "            arr: The numpy array to visualize.",
            "",
            "        Returns:",
            "            A dictionary of visualization URIs and their types.",
            "        \"\"\"",
            "        if not np.issubdtype(arr.dtype, np.number):",
            "            return {}",
            "",
            "        try:",
            "            # Save histogram for 1D arrays",
            "            if len(arr.shape) == 1:",
            "                histogram_path = os.path.join(self.uri, \"histogram.png\")",
            "                self._save_histogram(histogram_path, arr)",
            "                return {histogram_path: VisualizationType.IMAGE}",
            "",
            "            # Save as image for 2D or 3D arrays with 3 or 4 channels",
            "            if self._array_can_be_saved_as_image(arr):",
            "                image_path = os.path.join(self.uri, \"image.png\")",
            "                self._save_image(image_path, arr)",
            "                return {image_path: VisualizationType.IMAGE}",
            "",
            "        except ImportError:",
            "            logger.info(",
            "                \"Skipping visualization of numpy array because matplotlib \"",
            "                \"is not installed. To install matplotlib, run \"",
            "                \"`pip install matplotlib`.\"",
            "            )",
            "",
            "        return {}",
            "",
            "    def _save_histogram(self, output_path: str, arr: \"NDArray[Any]\") -> None:",
            "        \"\"\"Saves a histogram of a numpy array.",
            "",
            "        Args:",
            "            output_path: The path to save the histogram to.",
            "            arr: The numpy array of which to save the histogram.",
            "        \"\"\"",
            "        import matplotlib.pyplot as plt  # type: ignore",
            "",
            "        plt.hist(arr)",
            "        with fileio.open(output_path, \"wb\") as f:",
            "            plt.savefig(f)",
            "        plt.close()",
            "",
            "    @staticmethod",
            "    def _array_can_be_saved_as_image(arr: \"NDArray[Any]\") -> bool:",
            "        \"\"\"Checks if a numpy array can be saved as an image.",
            "",
            "        This is the case if the array is 2D or 3D with 3 or 4 channels.",
            "",
            "        Args:",
            "            arr: The numpy array to check.",
            "",
            "        Returns:",
            "            True if the array can be saved as an image, False otherwise.",
            "        \"\"\"",
            "        if len(arr.shape) == 2:",
            "            return True",
            "        if len(arr.shape) == 3 and arr.shape[2] in [3, 4]:",
            "            return True",
            "        return False",
            "",
            "    def _save_image(self, output_path: str, arr: \"NDArray[Any]\") -> None:",
            "        \"\"\"Saves a numpy array as an image.",
            "",
            "        Args:",
            "            output_path: The path to save the image to.",
            "            arr: The numpy array to save.",
            "        \"\"\"",
            "        from matplotlib.image import imsave  # type: ignore",
            "",
            "        with fileio.open(output_path, \"wb\") as f:",
            "            imsave(f, arr)",
            "",
            "    def extract_metadata(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extract metadata from the given numpy array.",
            "",
            "        Args:",
            "            arr: The numpy array to extract metadata from.",
            "",
            "        Returns:",
            "            The extracted metadata as a dictionary.",
            "        \"\"\"",
            "        if np.issubdtype(arr.dtype, np.number):",
            "            return self._extract_numeric_metadata(arr)",
            "        elif np.issubdtype(arr.dtype, np.unicode_) or np.issubdtype(",
            "            arr.dtype, np.object_",
            "        ):",
            "            return self._extract_text_metadata(arr)",
            "        else:",
            "            return {}",
            "",
            "    def _extract_numeric_metadata(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extracts numeric metadata from a numpy array.",
            "",
            "        Args:",
            "            arr: The numpy array to extract metadata from.",
            "",
            "        Returns:",
            "            A dictionary of metadata.",
            "        \"\"\"",
            "        min_val = np.min(arr).item()",
            "        max_val = np.max(arr).item()",
            "",
            "        numpy_metadata: Dict[str, \"MetadataType\"] = {",
            "            \"shape\": tuple(arr.shape),",
            "            \"dtype\": DType(arr.dtype.type),",
            "            \"mean\": np.mean(arr).item(),",
            "            \"std\": np.std(arr).item(),",
            "            \"min\": min_val,",
            "            \"max\": max_val,",
            "        }",
            "        return numpy_metadata",
            "",
            "    def _extract_text_metadata(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extracts text metadata from a numpy array.",
            "",
            "        Args:",
            "            arr: The numpy array to extract metadata from.",
            "",
            "        Returns:",
            "            A dictionary of metadata.",
            "        \"\"\"",
            "        text = \" \".join(arr)",
            "        words = text.split()",
            "        word_counts = Counter(words)",
            "        unique_words = len(word_counts)",
            "        total_words = len(words)",
            "        most_common_word, most_common_count = word_counts.most_common(1)[0]",
            "",
            "        text_metadata: Dict[str, \"MetadataType\"] = {",
            "            \"shape\": tuple(arr.shape),",
            "            \"dtype\": DType(arr.dtype.type),",
            "            \"unique_words\": unique_words,",
            "            \"total_words\": total_words,",
            "            \"most_common_word\": most_common_word,",
            "            \"most_common_count\": most_common_count,",
            "        }",
            "        return text_metadata"
        ],
        "afterPatchFile": [
            "#  Copyright (c) ZenML GmbH 2021. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Implementation of the ZenML NumPy materializer.\"\"\"",
            "",
            "import os",
            "from collections import Counter",
            "from typing import TYPE_CHECKING, Any, ClassVar, Dict, Tuple, Type",
            "",
            "import numpy as np",
            "",
            "from zenml.enums import ArtifactType, VisualizationType",
            "from zenml.io import fileio",
            "from zenml.logger import get_logger",
            "from zenml.materializers.base_materializer import BaseMaterializer",
            "from zenml.metadata.metadata_types import DType, MetadataType",
            "",
            "if TYPE_CHECKING:",
            "    from numpy.typing import NDArray",
            "",
            "logger = get_logger(__name__)",
            "",
            "",
            "NUMPY_FILENAME = \"data.npy\"",
            "",
            "DATA_FILENAME = \"data.parquet\"",
            "SHAPE_FILENAME = \"shape.json\"",
            "DATA_VAR = \"data_var\"",
            "",
            "",
            "class NumpyMaterializer(BaseMaterializer):",
            "    \"\"\"Materializer to read data to and from pandas.\"\"\"",
            "",
            "    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (np.ndarray,)",
            "    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.DATA",
            "",
            "    def load(self, data_type: Type[Any]) -> \"Any\":",
            "        \"\"\"Reads a numpy array from a `.npy` file.",
            "",
            "        Args:",
            "            data_type: The type of the data to read.",
            "",
            "",
            "        Raises:",
            "            ImportError: If pyarrow is not installed.",
            "",
            "        Returns:",
            "            The numpy array.",
            "        \"\"\"",
            "        numpy_file = os.path.join(self.uri, NUMPY_FILENAME)",
            "",
            "        if fileio.exists(numpy_file):",
            "            with fileio.open(numpy_file, \"rb\") as f:",
            "                return np.load(f, allow_pickle=True)",
            "        elif fileio.exists(os.path.join(self.uri, DATA_FILENAME)):",
            "            logger.warning(",
            "                \"A legacy artifact was found. \"",
            "                \"This artifact was created with an older version of \"",
            "                \"ZenML. You can still use it, but it will be \"",
            "                \"converted to the new format on the next materialization.\"",
            "            )",
            "            try:",
            "                # Import old materializer dependencies",
            "                import pyarrow as pa  # type: ignore",
            "                import pyarrow.parquet as pq  # type: ignore",
            "",
            "                from zenml.utils import yaml_utils",
            "",
            "                # Read numpy array from parquet file",
            "                shape_dict = yaml_utils.read_json(",
            "                    os.path.join(self.uri, SHAPE_FILENAME)",
            "                )",
            "                shape_tuple = tuple(shape_dict.values())",
            "                with fileio.open(",
            "                    os.path.join(self.uri, DATA_FILENAME), \"rb\"",
            "                ) as f:",
            "                    input_stream = pa.input_stream(f)",
            "                    data = pq.read_table(input_stream)",
            "                vals = getattr(data.to_pandas(), DATA_VAR).values",
            "                return np.reshape(vals, shape_tuple)",
            "            except ImportError:",
            "                raise ImportError(",
            "                    \"You have an old version of a `NumpyMaterializer` \",",
            "                    \"data artifact stored in the artifact store \",",
            "                    \"as a `.parquet` file, which requires `pyarrow` for reading. \",",
            "                    \"You can install `pyarrow` by running `pip install pyarrow`.\",",
            "                )",
            "",
            "    def save(self, arr: \"NDArray[Any]\") -> None:",
            "        \"\"\"Writes a np.ndarray to the artifact store as a `.npy` file.",
            "",
            "        Args:",
            "            arr: The numpy array to write.",
            "        \"\"\"",
            "        with fileio.open(os.path.join(self.uri, NUMPY_FILENAME), \"wb\") as f:",
            "            np.save(f, arr)",
            "",
            "    def save_visualizations(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, VisualizationType]:",
            "        \"\"\"Saves visualizations for a numpy array.",
            "",
            "        If the array is 1D, a histogram is saved. If the array is 2D or 3D with",
            "        3 or 4 channels, an image is saved.",
            "",
            "        Args:",
            "            arr: The numpy array to visualize.",
            "",
            "        Returns:",
            "            A dictionary of visualization URIs and their types.",
            "        \"\"\"",
            "        if not np.issubdtype(arr.dtype, np.number):",
            "            return {}",
            "",
            "        try:",
            "            # Save histogram for 1D arrays",
            "            if len(arr.shape) == 1:",
            "                histogram_path = os.path.join(self.uri, \"histogram.png\")",
            "                self._save_histogram(histogram_path, arr)",
            "                return {histogram_path: VisualizationType.IMAGE}",
            "",
            "            # Save as image for 2D or 3D arrays with 3 or 4 channels",
            "            if self._array_can_be_saved_as_image(arr):",
            "                image_path = os.path.join(self.uri, \"image.png\")",
            "                self._save_image(image_path, arr)",
            "                return {image_path: VisualizationType.IMAGE}",
            "",
            "        except ImportError:",
            "            logger.info(",
            "                \"Skipping visualization of numpy array because matplotlib \"",
            "                \"is not installed. To install matplotlib, run \"",
            "                \"`pip install matplotlib`.\"",
            "            )",
            "",
            "        return {}",
            "",
            "    def _save_histogram(self, output_path: str, arr: \"NDArray[Any]\") -> None:",
            "        \"\"\"Saves a histogram of a numpy array.",
            "",
            "        Args:",
            "            output_path: The path to save the histogram to.",
            "            arr: The numpy array of which to save the histogram.",
            "        \"\"\"",
            "        import matplotlib.pyplot as plt",
            "",
            "        plt.hist(arr)",
            "        with fileio.open(output_path, \"wb\") as f:",
            "            plt.savefig(f)",
            "        plt.close()",
            "",
            "    @staticmethod",
            "    def _array_can_be_saved_as_image(arr: \"NDArray[Any]\") -> bool:",
            "        \"\"\"Checks if a numpy array can be saved as an image.",
            "",
            "        This is the case if the array is 2D or 3D with 3 or 4 channels.",
            "",
            "        Args:",
            "            arr: The numpy array to check.",
            "",
            "        Returns:",
            "            True if the array can be saved as an image, False otherwise.",
            "        \"\"\"",
            "        if len(arr.shape) == 2:",
            "            return True",
            "        if len(arr.shape) == 3 and arr.shape[2] in [3, 4]:",
            "            return True",
            "        return False",
            "",
            "    def _save_image(self, output_path: str, arr: \"NDArray[Any]\") -> None:",
            "        \"\"\"Saves a numpy array as an image.",
            "",
            "        Args:",
            "            output_path: The path to save the image to.",
            "            arr: The numpy array to save.",
            "        \"\"\"",
            "        from matplotlib.image import imsave",
            "",
            "        with fileio.open(output_path, \"wb\") as f:",
            "            imsave(f, arr)",
            "",
            "    def extract_metadata(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extract metadata from the given numpy array.",
            "",
            "        Args:",
            "            arr: The numpy array to extract metadata from.",
            "",
            "        Returns:",
            "            The extracted metadata as a dictionary.",
            "        \"\"\"",
            "        if np.issubdtype(arr.dtype, np.number):",
            "            return self._extract_numeric_metadata(arr)",
            "        elif np.issubdtype(arr.dtype, np.unicode_) or np.issubdtype(",
            "            arr.dtype, np.object_",
            "        ):",
            "            return self._extract_text_metadata(arr)",
            "        else:",
            "            return {}",
            "",
            "    def _extract_numeric_metadata(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extracts numeric metadata from a numpy array.",
            "",
            "        Args:",
            "            arr: The numpy array to extract metadata from.",
            "",
            "        Returns:",
            "            A dictionary of metadata.",
            "        \"\"\"",
            "        min_val = np.min(arr).item()",
            "        max_val = np.max(arr).item()",
            "",
            "        numpy_metadata: Dict[str, \"MetadataType\"] = {",
            "            \"shape\": tuple(arr.shape),",
            "            \"dtype\": DType(arr.dtype.type),",
            "            \"mean\": np.mean(arr).item(),",
            "            \"std\": np.std(arr).item(),",
            "            \"min\": min_val,",
            "            \"max\": max_val,",
            "        }",
            "        return numpy_metadata",
            "",
            "    def _extract_text_metadata(",
            "        self, arr: \"NDArray[Any]\"",
            "    ) -> Dict[str, \"MetadataType\"]:",
            "        \"\"\"Extracts text metadata from a numpy array.",
            "",
            "        Args:",
            "            arr: The numpy array to extract metadata from.",
            "",
            "        Returns:",
            "            A dictionary of metadata.",
            "        \"\"\"",
            "        text = \" \".join(arr)",
            "        words = text.split()",
            "        word_counts = Counter(words)",
            "        unique_words = len(word_counts)",
            "        total_words = len(words)",
            "        most_common_word, most_common_count = word_counts.most_common(1)[0]",
            "",
            "        text_metadata: Dict[str, \"MetadataType\"] = {",
            "            \"shape\": tuple(arr.shape),",
            "            \"dtype\": DType(arr.dtype.type),",
            "            \"unique_words\": unique_words,",
            "            \"total_words\": total_words,",
            "            \"most_common_word\": most_common_word,",
            "            \"most_common_count\": most_common_count,",
            "        }",
            "        return text_metadata"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "154": [
                "NumpyMaterializer",
                "_save_histogram"
            ],
            "186": [
                "NumpyMaterializer",
                "_save_image"
            ]
        },
        "addLocation": []
    },
    "src/zenml/zen_server/auth.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 165,
                "afterPatchRowNumber": 165,
                "PatchRowcode": "     elif activation_token is not None:"
            },
            "1": {
                "beforePatchRowNumber": 166,
                "afterPatchRowNumber": 166,
                "PatchRowcode": "         if not UserAuthModel.verify_activation_token(activation_token, user):"
            },
            "2": {
                "beforePatchRowNumber": 167,
                "afterPatchRowNumber": 167,
                "PatchRowcode": "             return None"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 168,
                "PatchRowcode": "+    else:"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 169,
                "PatchRowcode": "+        if authentication_scheme() != AuthScheme.NO_AUTH:"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 170,
                "PatchRowcode": "+            return None"
            },
            "6": {
                "beforePatchRowNumber": 168,
                "afterPatchRowNumber": 171,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 169,
                "afterPatchRowNumber": 172,
                "PatchRowcode": "     return auth_context"
            },
            "8": {
                "beforePatchRowNumber": 170,
                "afterPatchRowNumber": 173,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "#  Copyright (c) ZenML GmbH 2022. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Authentication module for ZenML server.\"\"\"",
            "",
            "import os",
            "from contextvars import ContextVar",
            "from typing import Callable, List, Optional, Set, Union",
            "from uuid import UUID",
            "",
            "from fastapi import Depends, HTTPException, status",
            "from fastapi.security import (",
            "    HTTPBasic,",
            "    HTTPBasicCredentials,",
            "    OAuth2PasswordBearer,",
            "    SecurityScopes,",
            ")",
            "from pydantic import BaseModel",
            "",
            "from zenml.constants import API, ENV_ZENML_AUTH_TYPE, LOGIN, VERSION_1",
            "from zenml.enums import PermissionType",
            "from zenml.exceptions import AuthorizationException",
            "from zenml.logger import get_logger",
            "from zenml.models import UserResponseModel",
            "from zenml.models.user_models import JWTToken, JWTTokenType, UserAuthModel",
            "from zenml.utils.enum_utils import StrEnum",
            "from zenml.zen_server.utils import ROOT_URL_PATH, zen_store",
            "from zenml.zen_stores.base_zen_store import DEFAULT_USERNAME",
            "",
            "logger = get_logger(__name__)",
            "",
            "# create a context variable to store the authentication context",
            "_auth_context: ContextVar[Optional[\"AuthContext\"]] = ContextVar(",
            "    \"auth_context\", default=None",
            ")",
            "",
            "",
            "def get_auth_context() -> Optional[\"AuthContext\"]:",
            "    \"\"\"Returns the current authentication context.",
            "",
            "    Returns:",
            "        The authentication context.",
            "    \"\"\"",
            "    auth_context = _auth_context.get()",
            "    return auth_context",
            "",
            "",
            "def set_auth_context(auth_context: \"AuthContext\") -> \"AuthContext\":",
            "    \"\"\"Sets the current authentication context.",
            "",
            "    Args:",
            "        auth_context: The authentication context.",
            "",
            "    Returns:",
            "        The authentication context.",
            "    \"\"\"",
            "    _auth_context.set(auth_context)",
            "    return auth_context",
            "",
            "",
            "class AuthScheme(StrEnum):",
            "    \"\"\"The authentication scheme.\"\"\"",
            "",
            "    NO_AUTH = \"NO_AUTH\"",
            "    HTTP_BASIC = \"HTTP_BASIC\"",
            "    OAUTH2_PASSWORD_BEARER = \"OAUTH2_PASSWORD_BEARER\"",
            "",
            "",
            "class AuthContext(BaseModel):",
            "    \"\"\"The authentication context.\"\"\"",
            "",
            "    user: UserResponseModel",
            "",
            "    @property",
            "    def permissions(self) -> Set[PermissionType]:",
            "        \"\"\"Returns the permissions of the user.",
            "",
            "        Returns:",
            "            The permissions of the user.",
            "        \"\"\"",
            "        if self.user.roles:",
            "            # Merge permissions from all roles",
            "            permissions: List[PermissionType] = []",
            "            for role in self.user.roles:",
            "                permissions.extend(role.permissions)",
            "",
            "            # Remove duplicates",
            "            return set(permissions)",
            "",
            "        return set()",
            "",
            "",
            "def authentication_scheme() -> AuthScheme:",
            "    \"\"\"Returns the authentication type.",
            "",
            "    Returns:",
            "        The authentication type.",
            "    \"\"\"",
            "    auth_scheme = AuthScheme(",
            "        os.environ.get(ENV_ZENML_AUTH_TYPE, AuthScheme.OAUTH2_PASSWORD_BEARER)",
            "    )",
            "    return auth_scheme",
            "",
            "",
            "def authenticate_credentials(",
            "    user_name_or_id: Optional[Union[str, UUID]] = None,",
            "    password: Optional[str] = None,",
            "    access_token: Optional[str] = None,",
            "    activation_token: Optional[str] = None,",
            ") -> Optional[AuthContext]:",
            "    \"\"\"Verify if user authentication credentials are valid.",
            "",
            "    This function can be used to validate all supplied user credentials to",
            "    cover a range of possibilities:",
            "",
            "     * username+password",
            "     * access token (with embedded user id)",
            "     * username+activation token",
            "",
            "    Args:",
            "        user_name_or_id: The username or user ID.",
            "        password: The password.",
            "        access_token: The access token.",
            "        activation_token: The activation token.",
            "",
            "    Returns:",
            "        The authenticated account details, if the account is valid, otherwise",
            "        None.",
            "    \"\"\"",
            "    user: Optional[UserAuthModel] = None",
            "    auth_context: Optional[AuthContext] = None",
            "    if user_name_or_id:",
            "        try:",
            "            user = zen_store().get_auth_user(user_name_or_id)",
            "            user_model = zen_store().get_user(",
            "                user_name_or_id=user_name_or_id, include_private=True",
            "            )",
            "            auth_context = AuthContext(user=user_model)",
            "        except KeyError:",
            "            # even when the user does not exist, we still want to execute the",
            "            # password/token verification to protect against response discrepancy",
            "            # attacks (https://cwe.mitre.org/data/definitions/204.html)",
            "            pass",
            "    if password is not None:",
            "        if not UserAuthModel.verify_password(password, user):",
            "            return None",
            "    elif access_token is not None:",
            "        user = UserAuthModel.verify_access_token(access_token)",
            "        if not user:",
            "            return None",
            "        user_model = zen_store().get_user(",
            "            user_name_or_id=user.id, include_private=True",
            "        )",
            "        auth_context = AuthContext(user=user_model)",
            "    elif activation_token is not None:",
            "        if not UserAuthModel.verify_activation_token(activation_token, user):",
            "            return None",
            "",
            "    return auth_context",
            "",
            "",
            "def http_authentication(",
            "    security_scopes: SecurityScopes,",
            "    credentials: HTTPBasicCredentials = Depends(HTTPBasic()),",
            ") -> AuthContext:",
            "    \"\"\"Authenticates any request to the ZenML Server with basic HTTP authentication.",
            "",
            "    Args:",
            "        security_scopes: Security scope will be ignored for http_auth",
            "        credentials: HTTP basic auth credentials passed to the request.",
            "",
            "    Returns:",
            "        The authentication context reflecting the authenticated user.",
            "",
            "    Raises:",
            "        HTTPException: If the user credentials could not be authenticated.",
            "    \"\"\"",
            "    auth_context = authenticate_credentials(",
            "        user_name_or_id=credentials.username, password=credentials.password",
            "    )",
            "    if auth_context is None:",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "        )",
            "",
            "    return auth_context",
            "",
            "",
            "def oauth2_password_bearer_authentication(",
            "    security_scopes: SecurityScopes,",
            "    token: str = Depends(",
            "        OAuth2PasswordBearer(",
            "            tokenUrl=ROOT_URL_PATH + API + VERSION_1 + LOGIN,",
            "            scopes={",
            "                \"read\": \"Read permissions on all entities\",",
            "                \"write\": \"Write permissions on all entities\",",
            "                \"me\": \"Editing permissions to own user\",",
            "            },",
            "        )",
            "    ),",
            ") -> AuthContext:",
            "    \"\"\"Authenticates any request to the ZenML server with OAuth2 password bearer JWT tokens.",
            "",
            "    Args:",
            "        security_scopes: Security scope for this token",
            "        token: The JWT bearer token to be authenticated.",
            "",
            "    Returns:",
            "        The authentication context reflecting the authenticated user.",
            "",
            "    Raises:",
            "        HTTPException: If the JWT token could not be authorized.",
            "    \"\"\"",
            "    if security_scopes.scopes:",
            "        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'",
            "    else:",
            "        authenticate_value = \"Bearer\"",
            "    auth_context = authenticate_credentials(access_token=token)",
            "",
            "    try:",
            "        access_token = JWTToken.decode(",
            "            token_type=JWTTokenType.ACCESS_TOKEN, token=token",
            "        )",
            "    except AuthorizationException:",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "            headers={\"WWW-Authenticate\": \"Bearer\"},",
            "        )",
            "    for scope in security_scopes.scopes:",
            "        if scope not in access_token.permissions:",
            "            raise HTTPException(",
            "                status_code=status.HTTP_403_FORBIDDEN,",
            "                detail=\"Not enough permissions\",",
            "                headers={\"WWW-Authenticate\": authenticate_value},",
            "            )",
            "    if auth_context is None:",
            "        # We have to return an additional WWW-Authenticate header here with the",
            "        # value Bearer to be compliant with the OAuth2 spec.",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "            headers={\"WWW-Authenticate\": \"Bearer\"},",
            "        )",
            "    return auth_context",
            "",
            "",
            "def no_authentication(security_scopes: SecurityScopes) -> AuthContext:",
            "    \"\"\"Doesn't authenticate requests to the ZenML server.",
            "",
            "    Args:",
            "        security_scopes: Security scope will be ignored for http_auth",
            "",
            "    Returns:",
            "        The authentication context reflecting the default user.",
            "",
            "    Raises:",
            "        HTTPException: If the default user is not available.",
            "    \"\"\"",
            "    auth_context = authenticate_credentials(user_name_or_id=DEFAULT_USERNAME)",
            "",
            "    if auth_context is None:",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "        )",
            "",
            "    return auth_context",
            "",
            "",
            "def authentication_provider() -> Callable[..., AuthContext]:",
            "    \"\"\"Returns the authentication provider.",
            "",
            "    Returns:",
            "        The authentication provider.",
            "",
            "    Raises:",
            "        ValueError: If the authentication scheme is not supported.",
            "    \"\"\"",
            "    auth_scheme = authentication_scheme()",
            "    if auth_scheme == AuthScheme.NO_AUTH:",
            "        return no_authentication",
            "    elif auth_scheme == AuthScheme.HTTP_BASIC:",
            "        return http_authentication",
            "    elif auth_scheme == AuthScheme.OAUTH2_PASSWORD_BEARER:",
            "        return oauth2_password_bearer_authentication",
            "    else:",
            "        raise ValueError(f\"Unknown authentication scheme: {auth_scheme}\")",
            "",
            "",
            "authorize = authentication_provider()"
        ],
        "afterPatchFile": [
            "#  Copyright (c) ZenML GmbH 2022. All Rights Reserved.",
            "#",
            "#  Licensed under the Apache License, Version 2.0 (the \"License\");",
            "#  you may not use this file except in compliance with the License.",
            "#  You may obtain a copy of the License at:",
            "#",
            "#       https://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "#  Unless required by applicable law or agreed to in writing, software",
            "#  distributed under the License is distributed on an \"AS IS\" BASIS,",
            "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express",
            "#  or implied. See the License for the specific language governing",
            "#  permissions and limitations under the License.",
            "\"\"\"Authentication module for ZenML server.\"\"\"",
            "",
            "import os",
            "from contextvars import ContextVar",
            "from typing import Callable, List, Optional, Set, Union",
            "from uuid import UUID",
            "",
            "from fastapi import Depends, HTTPException, status",
            "from fastapi.security import (",
            "    HTTPBasic,",
            "    HTTPBasicCredentials,",
            "    OAuth2PasswordBearer,",
            "    SecurityScopes,",
            ")",
            "from pydantic import BaseModel",
            "",
            "from zenml.constants import API, ENV_ZENML_AUTH_TYPE, LOGIN, VERSION_1",
            "from zenml.enums import PermissionType",
            "from zenml.exceptions import AuthorizationException",
            "from zenml.logger import get_logger",
            "from zenml.models import UserResponseModel",
            "from zenml.models.user_models import JWTToken, JWTTokenType, UserAuthModel",
            "from zenml.utils.enum_utils import StrEnum",
            "from zenml.zen_server.utils import ROOT_URL_PATH, zen_store",
            "from zenml.zen_stores.base_zen_store import DEFAULT_USERNAME",
            "",
            "logger = get_logger(__name__)",
            "",
            "# create a context variable to store the authentication context",
            "_auth_context: ContextVar[Optional[\"AuthContext\"]] = ContextVar(",
            "    \"auth_context\", default=None",
            ")",
            "",
            "",
            "def get_auth_context() -> Optional[\"AuthContext\"]:",
            "    \"\"\"Returns the current authentication context.",
            "",
            "    Returns:",
            "        The authentication context.",
            "    \"\"\"",
            "    auth_context = _auth_context.get()",
            "    return auth_context",
            "",
            "",
            "def set_auth_context(auth_context: \"AuthContext\") -> \"AuthContext\":",
            "    \"\"\"Sets the current authentication context.",
            "",
            "    Args:",
            "        auth_context: The authentication context.",
            "",
            "    Returns:",
            "        The authentication context.",
            "    \"\"\"",
            "    _auth_context.set(auth_context)",
            "    return auth_context",
            "",
            "",
            "class AuthScheme(StrEnum):",
            "    \"\"\"The authentication scheme.\"\"\"",
            "",
            "    NO_AUTH = \"NO_AUTH\"",
            "    HTTP_BASIC = \"HTTP_BASIC\"",
            "    OAUTH2_PASSWORD_BEARER = \"OAUTH2_PASSWORD_BEARER\"",
            "",
            "",
            "class AuthContext(BaseModel):",
            "    \"\"\"The authentication context.\"\"\"",
            "",
            "    user: UserResponseModel",
            "",
            "    @property",
            "    def permissions(self) -> Set[PermissionType]:",
            "        \"\"\"Returns the permissions of the user.",
            "",
            "        Returns:",
            "            The permissions of the user.",
            "        \"\"\"",
            "        if self.user.roles:",
            "            # Merge permissions from all roles",
            "            permissions: List[PermissionType] = []",
            "            for role in self.user.roles:",
            "                permissions.extend(role.permissions)",
            "",
            "            # Remove duplicates",
            "            return set(permissions)",
            "",
            "        return set()",
            "",
            "",
            "def authentication_scheme() -> AuthScheme:",
            "    \"\"\"Returns the authentication type.",
            "",
            "    Returns:",
            "        The authentication type.",
            "    \"\"\"",
            "    auth_scheme = AuthScheme(",
            "        os.environ.get(ENV_ZENML_AUTH_TYPE, AuthScheme.OAUTH2_PASSWORD_BEARER)",
            "    )",
            "    return auth_scheme",
            "",
            "",
            "def authenticate_credentials(",
            "    user_name_or_id: Optional[Union[str, UUID]] = None,",
            "    password: Optional[str] = None,",
            "    access_token: Optional[str] = None,",
            "    activation_token: Optional[str] = None,",
            ") -> Optional[AuthContext]:",
            "    \"\"\"Verify if user authentication credentials are valid.",
            "",
            "    This function can be used to validate all supplied user credentials to",
            "    cover a range of possibilities:",
            "",
            "     * username+password",
            "     * access token (with embedded user id)",
            "     * username+activation token",
            "",
            "    Args:",
            "        user_name_or_id: The username or user ID.",
            "        password: The password.",
            "        access_token: The access token.",
            "        activation_token: The activation token.",
            "",
            "    Returns:",
            "        The authenticated account details, if the account is valid, otherwise",
            "        None.",
            "    \"\"\"",
            "    user: Optional[UserAuthModel] = None",
            "    auth_context: Optional[AuthContext] = None",
            "    if user_name_or_id:",
            "        try:",
            "            user = zen_store().get_auth_user(user_name_or_id)",
            "            user_model = zen_store().get_user(",
            "                user_name_or_id=user_name_or_id, include_private=True",
            "            )",
            "            auth_context = AuthContext(user=user_model)",
            "        except KeyError:",
            "            # even when the user does not exist, we still want to execute the",
            "            # password/token verification to protect against response discrepancy",
            "            # attacks (https://cwe.mitre.org/data/definitions/204.html)",
            "            pass",
            "    if password is not None:",
            "        if not UserAuthModel.verify_password(password, user):",
            "            return None",
            "    elif access_token is not None:",
            "        user = UserAuthModel.verify_access_token(access_token)",
            "        if not user:",
            "            return None",
            "        user_model = zen_store().get_user(",
            "            user_name_or_id=user.id, include_private=True",
            "        )",
            "        auth_context = AuthContext(user=user_model)",
            "    elif activation_token is not None:",
            "        if not UserAuthModel.verify_activation_token(activation_token, user):",
            "            return None",
            "    else:",
            "        if authentication_scheme() != AuthScheme.NO_AUTH:",
            "            return None",
            "",
            "    return auth_context",
            "",
            "",
            "def http_authentication(",
            "    security_scopes: SecurityScopes,",
            "    credentials: HTTPBasicCredentials = Depends(HTTPBasic()),",
            ") -> AuthContext:",
            "    \"\"\"Authenticates any request to the ZenML Server with basic HTTP authentication.",
            "",
            "    Args:",
            "        security_scopes: Security scope will be ignored for http_auth",
            "        credentials: HTTP basic auth credentials passed to the request.",
            "",
            "    Returns:",
            "        The authentication context reflecting the authenticated user.",
            "",
            "    Raises:",
            "        HTTPException: If the user credentials could not be authenticated.",
            "    \"\"\"",
            "    auth_context = authenticate_credentials(",
            "        user_name_or_id=credentials.username, password=credentials.password",
            "    )",
            "    if auth_context is None:",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "        )",
            "",
            "    return auth_context",
            "",
            "",
            "def oauth2_password_bearer_authentication(",
            "    security_scopes: SecurityScopes,",
            "    token: str = Depends(",
            "        OAuth2PasswordBearer(",
            "            tokenUrl=ROOT_URL_PATH + API + VERSION_1 + LOGIN,",
            "            scopes={",
            "                \"read\": \"Read permissions on all entities\",",
            "                \"write\": \"Write permissions on all entities\",",
            "                \"me\": \"Editing permissions to own user\",",
            "            },",
            "        )",
            "    ),",
            ") -> AuthContext:",
            "    \"\"\"Authenticates any request to the ZenML server with OAuth2 password bearer JWT tokens.",
            "",
            "    Args:",
            "        security_scopes: Security scope for this token",
            "        token: The JWT bearer token to be authenticated.",
            "",
            "    Returns:",
            "        The authentication context reflecting the authenticated user.",
            "",
            "    Raises:",
            "        HTTPException: If the JWT token could not be authorized.",
            "    \"\"\"",
            "    if security_scopes.scopes:",
            "        authenticate_value = f'Bearer scope=\"{security_scopes.scope_str}\"'",
            "    else:",
            "        authenticate_value = \"Bearer\"",
            "    auth_context = authenticate_credentials(access_token=token)",
            "",
            "    try:",
            "        access_token = JWTToken.decode(",
            "            token_type=JWTTokenType.ACCESS_TOKEN, token=token",
            "        )",
            "    except AuthorizationException:",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "            headers={\"WWW-Authenticate\": \"Bearer\"},",
            "        )",
            "    for scope in security_scopes.scopes:",
            "        if scope not in access_token.permissions:",
            "            raise HTTPException(",
            "                status_code=status.HTTP_403_FORBIDDEN,",
            "                detail=\"Not enough permissions\",",
            "                headers={\"WWW-Authenticate\": authenticate_value},",
            "            )",
            "    if auth_context is None:",
            "        # We have to return an additional WWW-Authenticate header here with the",
            "        # value Bearer to be compliant with the OAuth2 spec.",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "            headers={\"WWW-Authenticate\": \"Bearer\"},",
            "        )",
            "    return auth_context",
            "",
            "",
            "def no_authentication(security_scopes: SecurityScopes) -> AuthContext:",
            "    \"\"\"Doesn't authenticate requests to the ZenML server.",
            "",
            "    Args:",
            "        security_scopes: Security scope will be ignored for http_auth",
            "",
            "    Returns:",
            "        The authentication context reflecting the default user.",
            "",
            "    Raises:",
            "        HTTPException: If the default user is not available.",
            "    \"\"\"",
            "    auth_context = authenticate_credentials(user_name_or_id=DEFAULT_USERNAME)",
            "",
            "    if auth_context is None:",
            "        raise HTTPException(",
            "            status_code=status.HTTP_401_UNAUTHORIZED,",
            "            detail=\"Invalid authentication credentials\",",
            "        )",
            "",
            "    return auth_context",
            "",
            "",
            "def authentication_provider() -> Callable[..., AuthContext]:",
            "    \"\"\"Returns the authentication provider.",
            "",
            "    Returns:",
            "        The authentication provider.",
            "",
            "    Raises:",
            "        ValueError: If the authentication scheme is not supported.",
            "    \"\"\"",
            "    auth_scheme = authentication_scheme()",
            "    if auth_scheme == AuthScheme.NO_AUTH:",
            "        return no_authentication",
            "    elif auth_scheme == AuthScheme.HTTP_BASIC:",
            "        return http_authentication",
            "    elif auth_scheme == AuthScheme.OAUTH2_PASSWORD_BEARER:",
            "        return oauth2_password_bearer_authentication",
            "    else:",
            "        raise ValueError(f\"Unknown authentication scheme: {auth_scheme}\")",
            "",
            "",
            "authorize = authentication_provider()"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "src.zenml.zen_server.auth.oauth2_password_bearer_authentication",
            "src.zenml.zen_server.auth.no_authentication",
            "src.octoprint.server.api",
            "src.zenml.zen_server.auth.http_authentication"
        ]
    }
}