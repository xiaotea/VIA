{
    "scrapy/downloadermiddlewares/decompression.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " import tarfile"
            },
            "1": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " import logging"
            },
            "2": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from tempfile import mktemp"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 11,
                "PatchRowcode": "+from warnings import warn"
            },
            "4": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " import six"
            },
            "6": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " except ImportError:"
            },
            "8": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": "     from io import BytesIO"
            },
            "9": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+from scrapy.exceptions import ScrapyDeprecationWarning"
            },
            "11": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from scrapy.responsetypes import responsetypes"
            },
            "12": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 22,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+warn("
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+    \"Use of the scrapy.downloadermiddlewares.decompression module is \""
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+    \"discouraged, as it is susceptible to decompression bomb attacks. For \""
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+    \"details, see \""
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+    \"https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7\","
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+    ScrapyDeprecationWarning,"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+    stacklevel=2,"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+)"
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+"
            },
            "22": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " logger = logging.getLogger(__name__)"
            },
            "23": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "24": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "\"\"\" This module implements the DecompressionMiddleware which tries to recognise",
            "and extract the potentially compressed responses that may arrive.",
            "\"\"\"",
            "",
            "import bz2",
            "import gzip",
            "import zipfile",
            "import tarfile",
            "import logging",
            "from tempfile import mktemp",
            "",
            "import six",
            "",
            "try:",
            "    from cStringIO import StringIO as BytesIO",
            "except ImportError:",
            "    from io import BytesIO",
            "",
            "from scrapy.responsetypes import responsetypes",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DecompressionMiddleware(object):",
            "    \"\"\" This middleware tries to recognise and extract the possibly compressed",
            "    responses that may arrive. \"\"\"",
            "",
            "    def __init__(self):",
            "        self._formats = {",
            "            'tar': self._is_tar,",
            "            'zip': self._is_zip,",
            "            'gz': self._is_gzip,",
            "            'bz2': self._is_bzip2",
            "        }",
            "",
            "    def _is_tar(self, response):",
            "        archive = BytesIO(response.body)",
            "        try:",
            "            tar_file = tarfile.open(name=mktemp(), fileobj=archive)",
            "        except tarfile.ReadError:",
            "            return",
            "",
            "        body = tar_file.extractfile(tar_file.members[0]).read()",
            "        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def _is_zip(self, response):",
            "        archive = BytesIO(response.body)",
            "        try:",
            "            zip_file = zipfile.ZipFile(archive)",
            "        except zipfile.BadZipfile:",
            "            return",
            "",
            "        namelist = zip_file.namelist()",
            "        body = zip_file.read(namelist[0])",
            "        respcls = responsetypes.from_args(filename=namelist[0], body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def _is_gzip(self, response):",
            "        archive = BytesIO(response.body)",
            "        try:",
            "            body = gzip.GzipFile(fileobj=archive).read()",
            "        except IOError:",
            "            return",
            "",
            "        respcls = responsetypes.from_args(body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def _is_bzip2(self, response):",
            "        try:",
            "            body = bz2.decompress(response.body)",
            "        except IOError:",
            "            return",
            "",
            "        respcls = responsetypes.from_args(body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def process_response(self, request, response, spider):",
            "        if not response.body:",
            "            return response",
            "",
            "        for fmt, func in six.iteritems(self._formats):",
            "            new_response = func(response)",
            "            if new_response:",
            "                logger.debug('Decompressed response with format: %(responsefmt)s',",
            "                             {'responsefmt': fmt}, extra={'spider': spider})",
            "                return new_response",
            "        return response"
        ],
        "afterPatchFile": [
            "\"\"\" This module implements the DecompressionMiddleware which tries to recognise",
            "and extract the potentially compressed responses that may arrive.",
            "\"\"\"",
            "",
            "import bz2",
            "import gzip",
            "import zipfile",
            "import tarfile",
            "import logging",
            "from tempfile import mktemp",
            "from warnings import warn",
            "",
            "import six",
            "",
            "try:",
            "    from cStringIO import StringIO as BytesIO",
            "except ImportError:",
            "    from io import BytesIO",
            "",
            "from scrapy.exceptions import ScrapyDeprecationWarning",
            "from scrapy.responsetypes import responsetypes",
            "",
            "warn(",
            "    \"Use of the scrapy.downloadermiddlewares.decompression module is \"",
            "    \"discouraged, as it is susceptible to decompression bomb attacks. For \"",
            "    \"details, see \"",
            "    \"https://github.com/scrapy/scrapy/security/advisories/GHSA-7j7m-v7m3-jqm7\",",
            "    ScrapyDeprecationWarning,",
            "    stacklevel=2,",
            ")",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class DecompressionMiddleware(object):",
            "    \"\"\" This middleware tries to recognise and extract the possibly compressed",
            "    responses that may arrive. \"\"\"",
            "",
            "    def __init__(self):",
            "        self._formats = {",
            "            'tar': self._is_tar,",
            "            'zip': self._is_zip,",
            "            'gz': self._is_gzip,",
            "            'bz2': self._is_bzip2",
            "        }",
            "",
            "    def _is_tar(self, response):",
            "        archive = BytesIO(response.body)",
            "        try:",
            "            tar_file = tarfile.open(name=mktemp(), fileobj=archive)",
            "        except tarfile.ReadError:",
            "            return",
            "",
            "        body = tar_file.extractfile(tar_file.members[0]).read()",
            "        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def _is_zip(self, response):",
            "        archive = BytesIO(response.body)",
            "        try:",
            "            zip_file = zipfile.ZipFile(archive)",
            "        except zipfile.BadZipfile:",
            "            return",
            "",
            "        namelist = zip_file.namelist()",
            "        body = zip_file.read(namelist[0])",
            "        respcls = responsetypes.from_args(filename=namelist[0], body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def _is_gzip(self, response):",
            "        archive = BytesIO(response.body)",
            "        try:",
            "            body = gzip.GzipFile(fileobj=archive).read()",
            "        except IOError:",
            "            return",
            "",
            "        respcls = responsetypes.from_args(body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def _is_bzip2(self, response):",
            "        try:",
            "            body = bz2.decompress(response.body)",
            "        except IOError:",
            "            return",
            "",
            "        respcls = responsetypes.from_args(body=body)",
            "        return response.replace(body=body, cls=respcls)",
            "",
            "    def process_response(self, request, response, spider):",
            "        if not response.body:",
            "            return response",
            "",
            "        for fmt, func in six.iteritems(self._formats):",
            "            new_response = func(response)",
            "            if new_response:",
            "                logger.debug('Decompressed response with format: %(responsefmt)s',",
            "                             {'responsefmt': fmt}, extra={'spider': spider})",
            "                return new_response",
            "        return response"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": []
    },
    "scrapy/downloadermiddlewares/httpcompression.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import zlib"
            },
            "1": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1,
                "PatchRowcode": "+import warnings"
            },
            "2": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+from logging import getLogger"
            },
            "3": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " "
            },
            "4": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from scrapy.utils.gz import gunzip"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 4,
                "PatchRowcode": "+from scrapy import signals"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 5,
                "PatchRowcode": "+from scrapy.exceptions import IgnoreRequest, NotConfigured"
            },
            "7": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " from scrapy.http import Response, TextResponse"
            },
            "8": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " from scrapy.responsetypes import responsetypes"
            },
            "9": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from scrapy.exceptions import NotConfigured"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+from scrapy.utils._compression import ("
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 9,
                "PatchRowcode": "+    _DecompressionMaxSizeExceeded,"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 10,
                "PatchRowcode": "+    _inflate,"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 11,
                "PatchRowcode": "+    _unbrotli,"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 12,
                "PatchRowcode": "+    _unzstd,"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 13,
                "PatchRowcode": "+)"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 14,
                "PatchRowcode": "+from scrapy.utils.deprecate import ScrapyDeprecationWarning"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 15,
                "PatchRowcode": "+from scrapy.utils.gz import gunzip"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+logger = getLogger(__name__)"
            },
            "20": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]"
            },
            "22": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " "
            },
            "23": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-ACCEPTED_ENCODINGS = [b'gzip', b'deflate']"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+try:"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+    import brotli  # noqa: F401"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+except ImportError:"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+    pass"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+else:"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+    ACCEPTED_ENCODINGS.append(b\"br\")"
            },
            "30": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " "
            },
            "31": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " try:"
            },
            "32": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    import brotli"
            },
            "33": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    ACCEPTED_ENCODINGS.append(b'br')"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+    import zstandard  # noqa: F401"
            },
            "35": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " except ImportError:"
            },
            "36": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 31,
                "PatchRowcode": "     pass"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+else:"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+    ACCEPTED_ENCODINGS.append(b\"zstd\")"
            },
            "39": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " "
            },
            "41": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " class HttpCompressionMiddleware(object):"
            },
            "42": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be"
            },
            "43": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "     sent/received from web sites\"\"\""
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 39,
                "PatchRowcode": "+"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+    def __init__(self, crawler=None):"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+        if not crawler:"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+            self._max_size = 1073741824"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+            self._warn_size = 33554432"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 44,
                "PatchRowcode": "+            return"
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 45,
                "PatchRowcode": "+        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 46,
                "PatchRowcode": "+        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 47,
                "PatchRowcode": "+        crawler.signals.connect(self.open_spider, signals.spider_opened)"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+"
            },
            "54": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     @classmethod"
            },
            "55": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 50,
                "PatchRowcode": "     def from_crawler(cls, crawler):"
            },
            "56": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "         if not crawler.settings.getbool('COMPRESSION_ENABLED'):"
            },
            "57": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 52,
                "PatchRowcode": "             raise NotConfigured"
            },
            "58": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        return cls()"
            },
            "59": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+        try:"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 54,
                "PatchRowcode": "+            return cls(crawler=crawler)"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 55,
                "PatchRowcode": "+        except TypeError:"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 56,
                "PatchRowcode": "+            warnings.warn("
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 57,
                "PatchRowcode": "+                \"HttpCompressionMiddleware subclasses must either modify \""
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+                \"their '__init__' method to support a 'crawler' parameter or \""
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+                \"reimplement their 'from_crawler' method.\","
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 60,
                "PatchRowcode": "+                ScrapyDeprecationWarning,"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 61,
                "PatchRowcode": "+            )"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 62,
                "PatchRowcode": "+            mw = cls()"
            },
            "69": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")"
            },
            "70": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")"
            },
            "71": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+            crawler.signals.connect(mw.open_spider, signals.spider_opened)"
            },
            "72": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+            return mw"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+"
            },
            "74": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+    def open_spider(self, spider):"
            },
            "75": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+        if hasattr(spider, \"download_maxsize\"):"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 70,
                "PatchRowcode": "+            self._max_size = spider.download_maxsize"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 71,
                "PatchRowcode": "+        if hasattr(spider, \"download_warnsize\"):"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+            self._warn_size = spider.download_warnsize"
            },
            "79": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " "
            },
            "80": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "     def process_request(self, request, spider):"
            },
            "81": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         request.headers.setdefault('Accept-Encoding',"
            },
            "82": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "             content_encoding = response.headers.getlist('Content-Encoding')"
            },
            "83": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "             if content_encoding:"
            },
            "84": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 85,
                "PatchRowcode": "                 encoding = content_encoding.pop()"
            },
            "85": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                decoded_body = self._decode(response.body, encoding.lower())"
            },
            "86": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                respcls = responsetypes.from_args(headers=response.headers, \\"
            },
            "87": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    url=response.url, body=decoded_body)"
            },
            "88": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+                max_size = request.meta.get(\"download_maxsize\", self._max_size)"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+                try:"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+                    decoded_body = self._decode("
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+                        response.body, encoding.lower(), max_size"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+                    )"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+                except _DecompressionMaxSizeExceeded:"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+                    raise IgnoreRequest("
            },
            "96": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+                        \"Ignored response {response} because its body \""
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+                        \"({body_size} B) exceeded DOWNLOAD_MAXSIZE \""
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+                        \"({max_size} B) during decompression.\".format("
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+                            response=response,"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+                            body_size=len(response.body),"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                            max_size=max_size,"
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+                        )"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+                    )"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+                if len(response.body) < warn_size <= len(decoded_body):"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+                    logger.warning("
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+                        \"%(response)s body size after decompression \""
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+                        \"(%(body_size)s B) is larger than the \""
            },
            "108": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+                        \"download warning size (%(warn_size)s B).\","
            },
            "109": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+                        {"
            },
            "110": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+                            \"response\": response,"
            },
            "111": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+                            \"body_size\": len(decoded_body),"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+                            \"warn_size\": warn_size,"
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+                        },"
            },
            "114": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+                    )"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+                respcls = responsetypes.from_args("
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+                    headers=response.headers, url=response.url, body=decoded_body"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 115,
                "PatchRowcode": "+                )"
            },
            "118": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "                 kwargs = dict(cls=respcls, body=decoded_body)"
            },
            "119": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "                 if issubclass(respcls, TextResponse):"
            },
            "120": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": 118,
                "PatchRowcode": "                     # force recalculating the encoding until we make sure the"
            },
            "121": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 124,
                "PatchRowcode": " "
            },
            "122": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 125,
                "PatchRowcode": "         return response"
            },
            "123": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 126,
                "PatchRowcode": " "
            },
            "124": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def _decode(self, body, encoding):"
            },
            "125": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if encoding == b'gzip' or encoding == b'x-gzip':"
            },
            "126": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            body = gunzip(body)"
            },
            "127": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "128": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if encoding == b'deflate':"
            },
            "129": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            try:"
            },
            "130": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                body = zlib.decompress(body)"
            },
            "131": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            except zlib.error:"
            },
            "132": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # ugly hack to work with raw deflate content that may"
            },
            "133": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # be sent by microsoft servers. For more information, see:"
            },
            "134": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # http://carsten.codimi.de/gzip.yaws/"
            },
            "135": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx"
            },
            "136": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                # http://www.gzip.org/zlib/zlib_faq.html#faq38"
            },
            "137": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                body = zlib.decompress(body, -15)"
            },
            "138": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:"
            },
            "139": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            body = brotli.decompress(body)"
            },
            "140": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 127,
                "PatchRowcode": "+    def _decode(self, body, encoding, max_size):"
            },
            "141": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 128,
                "PatchRowcode": "+        if encoding == b\"gzip\" or encoding == b\"x-gzip\":"
            },
            "142": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 129,
                "PatchRowcode": "+            return gunzip(body, max_size=max_size)"
            },
            "143": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 130,
                "PatchRowcode": "+        if encoding == b\"deflate\":"
            },
            "144": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 131,
                "PatchRowcode": "+            return _inflate(body, max_size=max_size)"
            },
            "145": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+        if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:"
            },
            "146": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+            return _unbrotli(body, max_size=max_size)"
            },
            "147": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 134,
                "PatchRowcode": "+        if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:"
            },
            "148": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 135,
                "PatchRowcode": "+            return _unzstd(body, max_size=max_size)"
            },
            "149": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "         return body"
            }
        },
        "frontPatchFile": [
            "import zlib",
            "",
            "from scrapy.utils.gz import gunzip",
            "from scrapy.http import Response, TextResponse",
            "from scrapy.responsetypes import responsetypes",
            "from scrapy.exceptions import NotConfigured",
            "",
            "",
            "ACCEPTED_ENCODINGS = [b'gzip', b'deflate']",
            "",
            "try:",
            "    import brotli",
            "    ACCEPTED_ENCODINGS.append(b'br')",
            "except ImportError:",
            "    pass",
            "",
            "",
            "class HttpCompressionMiddleware(object):",
            "    \"\"\"This middleware allows compressed (gzip, deflate) traffic to be",
            "    sent/received from web sites\"\"\"",
            "    @classmethod",
            "    def from_crawler(cls, crawler):",
            "        if not crawler.settings.getbool('COMPRESSION_ENABLED'):",
            "            raise NotConfigured",
            "        return cls()",
            "",
            "    def process_request(self, request, spider):",
            "        request.headers.setdefault('Accept-Encoding',",
            "                                   b\",\".join(ACCEPTED_ENCODINGS))",
            "",
            "    def process_response(self, request, response, spider):",
            "",
            "        if request.method == 'HEAD':",
            "            return response",
            "        if isinstance(response, Response):",
            "            content_encoding = response.headers.getlist('Content-Encoding')",
            "            if content_encoding:",
            "                encoding = content_encoding.pop()",
            "                decoded_body = self._decode(response.body, encoding.lower())",
            "                respcls = responsetypes.from_args(headers=response.headers, \\",
            "                    url=response.url, body=decoded_body)",
            "                kwargs = dict(cls=respcls, body=decoded_body)",
            "                if issubclass(respcls, TextResponse):",
            "                    # force recalculating the encoding until we make sure the",
            "                    # responsetypes guessing is reliable",
            "                    kwargs['encoding'] = None",
            "                response = response.replace(**kwargs)",
            "                if not content_encoding:",
            "                    del response.headers['Content-Encoding']",
            "",
            "        return response",
            "",
            "    def _decode(self, body, encoding):",
            "        if encoding == b'gzip' or encoding == b'x-gzip':",
            "            body = gunzip(body)",
            "",
            "        if encoding == b'deflate':",
            "            try:",
            "                body = zlib.decompress(body)",
            "            except zlib.error:",
            "                # ugly hack to work with raw deflate content that may",
            "                # be sent by microsoft servers. For more information, see:",
            "                # http://carsten.codimi.de/gzip.yaws/",
            "                # http://www.port80software.com/200ok/archive/2005/10/31/868.aspx",
            "                # http://www.gzip.org/zlib/zlib_faq.html#faq38",
            "                body = zlib.decompress(body, -15)",
            "        if encoding == b'br' and b'br' in ACCEPTED_ENCODINGS:",
            "            body = brotli.decompress(body)",
            "        return body"
        ],
        "afterPatchFile": [
            "import warnings",
            "from logging import getLogger",
            "",
            "from scrapy import signals",
            "from scrapy.exceptions import IgnoreRequest, NotConfigured",
            "from scrapy.http import Response, TextResponse",
            "from scrapy.responsetypes import responsetypes",
            "from scrapy.utils._compression import (",
            "    _DecompressionMaxSizeExceeded,",
            "    _inflate,",
            "    _unbrotli,",
            "    _unzstd,",
            ")",
            "from scrapy.utils.deprecate import ScrapyDeprecationWarning",
            "from scrapy.utils.gz import gunzip",
            "",
            "logger = getLogger(__name__)",
            "",
            "ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]",
            "",
            "try:",
            "    import brotli  # noqa: F401",
            "except ImportError:",
            "    pass",
            "else:",
            "    ACCEPTED_ENCODINGS.append(b\"br\")",
            "",
            "try:",
            "    import zstandard  # noqa: F401",
            "except ImportError:",
            "    pass",
            "else:",
            "    ACCEPTED_ENCODINGS.append(b\"zstd\")",
            "",
            "",
            "class HttpCompressionMiddleware(object):",
            "    \"\"\"This middleware allows compressed (gzip, deflate) traffic to be",
            "    sent/received from web sites\"\"\"",
            "",
            "    def __init__(self, crawler=None):",
            "        if not crawler:",
            "            self._max_size = 1073741824",
            "            self._warn_size = 33554432",
            "            return",
            "        self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
            "        self._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
            "        crawler.signals.connect(self.open_spider, signals.spider_opened)",
            "",
            "    @classmethod",
            "    def from_crawler(cls, crawler):",
            "        if not crawler.settings.getbool('COMPRESSION_ENABLED'):",
            "            raise NotConfigured",
            "        try:",
            "            return cls(crawler=crawler)",
            "        except TypeError:",
            "            warnings.warn(",
            "                \"HttpCompressionMiddleware subclasses must either modify \"",
            "                \"their '__init__' method to support a 'crawler' parameter or \"",
            "                \"reimplement their 'from_crawler' method.\",",
            "                ScrapyDeprecationWarning,",
            "            )",
            "            mw = cls()",
            "            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")",
            "            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")",
            "            crawler.signals.connect(mw.open_spider, signals.spider_opened)",
            "            return mw",
            "",
            "    def open_spider(self, spider):",
            "        if hasattr(spider, \"download_maxsize\"):",
            "            self._max_size = spider.download_maxsize",
            "        if hasattr(spider, \"download_warnsize\"):",
            "            self._warn_size = spider.download_warnsize",
            "",
            "    def process_request(self, request, spider):",
            "        request.headers.setdefault('Accept-Encoding',",
            "                                   b\",\".join(ACCEPTED_ENCODINGS))",
            "",
            "    def process_response(self, request, response, spider):",
            "",
            "        if request.method == 'HEAD':",
            "            return response",
            "        if isinstance(response, Response):",
            "            content_encoding = response.headers.getlist('Content-Encoding')",
            "            if content_encoding:",
            "                encoding = content_encoding.pop()",
            "                max_size = request.meta.get(\"download_maxsize\", self._max_size)",
            "                warn_size = request.meta.get(\"download_warnsize\", self._warn_size)",
            "                try:",
            "                    decoded_body = self._decode(",
            "                        response.body, encoding.lower(), max_size",
            "                    )",
            "                except _DecompressionMaxSizeExceeded:",
            "                    raise IgnoreRequest(",
            "                        \"Ignored response {response} because its body \"",
            "                        \"({body_size} B) exceeded DOWNLOAD_MAXSIZE \"",
            "                        \"({max_size} B) during decompression.\".format(",
            "                            response=response,",
            "                            body_size=len(response.body),",
            "                            max_size=max_size,",
            "                        )",
            "                    )",
            "                if len(response.body) < warn_size <= len(decoded_body):",
            "                    logger.warning(",
            "                        \"%(response)s body size after decompression \"",
            "                        \"(%(body_size)s B) is larger than the \"",
            "                        \"download warning size (%(warn_size)s B).\",",
            "                        {",
            "                            \"response\": response,",
            "                            \"body_size\": len(decoded_body),",
            "                            \"warn_size\": warn_size,",
            "                        },",
            "                    )",
            "                respcls = responsetypes.from_args(",
            "                    headers=response.headers, url=response.url, body=decoded_body",
            "                )",
            "                kwargs = dict(cls=respcls, body=decoded_body)",
            "                if issubclass(respcls, TextResponse):",
            "                    # force recalculating the encoding until we make sure the",
            "                    # responsetypes guessing is reliable",
            "                    kwargs['encoding'] = None",
            "                response = response.replace(**kwargs)",
            "                if not content_encoding:",
            "                    del response.headers['Content-Encoding']",
            "",
            "        return response",
            "",
            "    def _decode(self, body, encoding, max_size):",
            "        if encoding == b\"gzip\" or encoding == b\"x-gzip\":",
            "            return gunzip(body, max_size=max_size)",
            "        if encoding == b\"deflate\":",
            "            return _inflate(body, max_size=max_size)",
            "        if encoding == b\"br\" and b\"br\" in ACCEPTED_ENCODINGS:",
            "            return _unbrotli(body, max_size=max_size)",
            "        if encoding == b\"zstd\" and b\"zstd\" in ACCEPTED_ENCODINGS:",
            "            return _unzstd(body, max_size=max_size)",
            "        return body"
        ],
        "action": [
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0"
        ],
        "dele_reviseLocation": {
            "1": [],
            "3": [],
            "6": [],
            "9": [
                "ACCEPTED_ENCODINGS"
            ],
            "12": [],
            "13": [],
            "25": [
                "HttpCompressionMiddleware",
                "from_crawler"
            ],
            "39": [
                "HttpCompressionMiddleware",
                "process_response"
            ],
            "40": [
                "HttpCompressionMiddleware",
                "process_response"
            ],
            "41": [
                "HttpCompressionMiddleware",
                "process_response"
            ],
            "53": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "54": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "55": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "56": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "57": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "58": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "59": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "60": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "61": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "62": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "63": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "64": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "65": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "66": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "67": [
                "HttpCompressionMiddleware",
                "_decode"
            ],
            "68": [
                "HttpCompressionMiddleware",
                "_decode"
            ]
        },
        "addLocation": []
    },
    "scrapy/spiders/sitemap.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import re"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " import logging"
            },
            "2": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2,
                "PatchRowcode": "+import re"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 3,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import six"
            },
            "5": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from scrapy.spiders import Spider"
            },
            "7": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from scrapy.http import Request, XmlResponse"
            },
            "8": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 6,
                "PatchRowcode": "+from scrapy.http.response.xml import XmlResponse"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+from scrapy.spiders import Request, Spider"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 8,
                "PatchRowcode": "+from scrapy.utils._compression import _DecompressionMaxSizeExceeded"
            },
            "12": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from scrapy.utils.gz import gunzip, gzip_magic_number"
            },
            "13": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 10,
                "PatchRowcode": "+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots"
            },
            "15": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " logger = logging.getLogger(__name__)"
            },
            "17": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 19,
                "PatchRowcode": "     sitemap_follow = ['']"
            },
            "19": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 20,
                "PatchRowcode": "     sitemap_alternate_links = False"
            },
            "20": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " "
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+    @classmethod"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+    def from_crawler(cls, crawler, *args, **kwargs):"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+        spider = super(SitemapSpider, cls).from_crawler(crawler, *args, **kwargs)"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+        spider._max_size = getattr("
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+        )"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+        spider._warn_size = getattr("
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+        )"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+        return spider"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+"
            },
            "32": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "     def __init__(self, *a, **kw):"
            },
            "33": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "         super(SitemapSpider, self).__init__(*a, **kw)"
            },
            "34": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "         self._cbs = []"
            },
            "35": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "         \"\"\""
            },
            "36": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "         if isinstance(response, XmlResponse):"
            },
            "37": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "             return response.body"
            },
            "38": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        elif gzip_magic_number(response):"
            },
            "39": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            return gunzip(response.body)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+        if gzip_magic_number(response):"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+            uncompressed_size = len(response.body)"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+            max_size = response.meta.get(\"download_maxsize\", self._max_size)"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 89,
                "PatchRowcode": "+            try:"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+                body = gunzip(response.body, max_size=max_size)"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 91,
                "PatchRowcode": "+            except _DecompressionMaxSizeExceeded:"
            },
            "47": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 92,
                "PatchRowcode": "+                return None"
            },
            "48": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 93,
                "PatchRowcode": "+            if uncompressed_size < warn_size <= len(body):"
            },
            "49": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+                logger.warning("
            },
            "50": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 95,
                "PatchRowcode": "+                    \"%(response)s body size after decompression (%(body_length)s B) \""
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 96,
                "PatchRowcode": "+                    \"is larger than the download warning size (%(warn_size)s B).\","
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 97,
                "PatchRowcode": "+                    {"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 98,
                "PatchRowcode": "+                        \"response\": response,"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 99,
                "PatchRowcode": "+                        \"body_length\": len(body),"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 100,
                "PatchRowcode": "+                        \"warn_size\": warn_size,"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 101,
                "PatchRowcode": "+                    },"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 102,
                "PatchRowcode": "+                )"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 103,
                "PatchRowcode": "+            return body"
            },
            "59": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": 104,
                "PatchRowcode": "         # actual gzipped sitemap files are decompressed above ;"
            },
            "60": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 105,
                "PatchRowcode": "         # if we are here (response body is not gzipped)"
            },
            "61": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "         # and have a response for .xml.gz,"
            },
            "62": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "         # without actually being a .xml.gz file in the first place,"
            },
            "63": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "         # merely XML gzip-compressed on the fly,"
            },
            "64": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "         # in other word, here, we have plain XML"
            },
            "65": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+        if response.url.endswith('.xml') or response.url.endswith('.xml.gz'):"
            },
            "67": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 114,
                "PatchRowcode": "             return response.body"
            },
            "68": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 115,
                "PatchRowcode": " "
            },
            "69": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 116,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "import re",
            "import logging",
            "import six",
            "",
            "from scrapy.spiders import Spider",
            "from scrapy.http import Request, XmlResponse",
            "from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots",
            "from scrapy.utils.gz import gunzip, gzip_magic_number",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SitemapSpider(Spider):",
            "",
            "    sitemap_urls = ()",
            "    sitemap_rules = [('', 'parse')]",
            "    sitemap_follow = ['']",
            "    sitemap_alternate_links = False",
            "",
            "    def __init__(self, *a, **kw):",
            "        super(SitemapSpider, self).__init__(*a, **kw)",
            "        self._cbs = []",
            "        for r, c in self.sitemap_rules:",
            "            if isinstance(c, six.string_types):",
            "                c = getattr(self, c)",
            "            self._cbs.append((regex(r), c))",
            "        self._follow = [regex(x) for x in self.sitemap_follow]",
            "",
            "    def start_requests(self):",
            "        for url in self.sitemap_urls:",
            "            yield Request(url, self._parse_sitemap)",
            "",
            "    def sitemap_filter(self, entries):",
            "        \"\"\"This method can be used to filter sitemap entries by their",
            "        attributes, for example, you can filter locs with lastmod greater",
            "        than a given date (see docs).",
            "        \"\"\"",
            "        for entry in entries:",
            "            yield entry",
            "",
            "    def _parse_sitemap(self, response):",
            "        if response.url.endswith('/robots.txt'):",
            "            for url in sitemap_urls_from_robots(response.text, base_url=response.url):",
            "                yield Request(url, callback=self._parse_sitemap)",
            "        else:",
            "            body = self._get_sitemap_body(response)",
            "            if body is None:",
            "                logger.warning(\"Ignoring invalid sitemap: %(response)s\",",
            "                               {'response': response}, extra={'spider': self})",
            "                return",
            "",
            "            s = Sitemap(body)",
            "            it = self.sitemap_filter(s)",
            "",
            "            if s.type == 'sitemapindex':",
            "                for loc in iterloc(it, self.sitemap_alternate_links):",
            "                    if any(x.search(loc) for x in self._follow):",
            "                        yield Request(loc, callback=self._parse_sitemap)",
            "            elif s.type == 'urlset':",
            "                for loc in iterloc(it, self.sitemap_alternate_links):",
            "                    for r, c in self._cbs:",
            "                        if r.search(loc):",
            "                            yield Request(loc, callback=c)",
            "                            break",
            "",
            "    def _get_sitemap_body(self, response):",
            "        \"\"\"Return the sitemap body contained in the given response,",
            "        or None if the response is not a sitemap.",
            "        \"\"\"",
            "        if isinstance(response, XmlResponse):",
            "            return response.body",
            "        elif gzip_magic_number(response):",
            "            return gunzip(response.body)",
            "        # actual gzipped sitemap files are decompressed above ;",
            "        # if we are here (response body is not gzipped)",
            "        # and have a response for .xml.gz,",
            "        # it usually means that it was already gunzipped",
            "        # by HttpCompression middleware,",
            "        # the HTTP response being sent with \"Content-Encoding: gzip\"",
            "        # without actually being a .xml.gz file in the first place,",
            "        # merely XML gzip-compressed on the fly,",
            "        # in other word, here, we have plain XML",
            "        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):",
            "            return response.body",
            "",
            "",
            "def regex(x):",
            "    if isinstance(x, six.string_types):",
            "        return re.compile(x)",
            "    return x",
            "",
            "",
            "def iterloc(it, alt=False):",
            "    for d in it:",
            "        yield d['loc']",
            "",
            "        # Also consider alternate URLs (xhtml:link rel=\"alternate\")",
            "        if alt and 'alternate' in d:",
            "            for l in d['alternate']:",
            "                yield l"
        ],
        "afterPatchFile": [
            "import logging",
            "import re",
            "",
            "import six",
            "",
            "from scrapy.http.response.xml import XmlResponse",
            "from scrapy.spiders import Request, Spider",
            "from scrapy.utils._compression import _DecompressionMaxSizeExceeded",
            "from scrapy.utils.gz import gunzip, gzip_magic_number",
            "from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class SitemapSpider(Spider):",
            "",
            "    sitemap_urls = ()",
            "    sitemap_rules = [('', 'parse')]",
            "    sitemap_follow = ['']",
            "    sitemap_alternate_links = False",
            "",
            "    @classmethod",
            "    def from_crawler(cls, crawler, *args, **kwargs):",
            "        spider = super(SitemapSpider, cls).from_crawler(crawler, *args, **kwargs)",
            "        spider._max_size = getattr(",
            "            spider, \"download_maxsize\", spider.settings.getint(\"DOWNLOAD_MAXSIZE\")",
            "        )",
            "        spider._warn_size = getattr(",
            "            spider, \"download_warnsize\", spider.settings.getint(\"DOWNLOAD_WARNSIZE\")",
            "        )",
            "        return spider",
            "",
            "    def __init__(self, *a, **kw):",
            "        super(SitemapSpider, self).__init__(*a, **kw)",
            "        self._cbs = []",
            "        for r, c in self.sitemap_rules:",
            "            if isinstance(c, six.string_types):",
            "                c = getattr(self, c)",
            "            self._cbs.append((regex(r), c))",
            "        self._follow = [regex(x) for x in self.sitemap_follow]",
            "",
            "    def start_requests(self):",
            "        for url in self.sitemap_urls:",
            "            yield Request(url, self._parse_sitemap)",
            "",
            "    def sitemap_filter(self, entries):",
            "        \"\"\"This method can be used to filter sitemap entries by their",
            "        attributes, for example, you can filter locs with lastmod greater",
            "        than a given date (see docs).",
            "        \"\"\"",
            "        for entry in entries:",
            "            yield entry",
            "",
            "    def _parse_sitemap(self, response):",
            "        if response.url.endswith('/robots.txt'):",
            "            for url in sitemap_urls_from_robots(response.text, base_url=response.url):",
            "                yield Request(url, callback=self._parse_sitemap)",
            "        else:",
            "            body = self._get_sitemap_body(response)",
            "            if body is None:",
            "                logger.warning(\"Ignoring invalid sitemap: %(response)s\",",
            "                               {'response': response}, extra={'spider': self})",
            "                return",
            "",
            "            s = Sitemap(body)",
            "            it = self.sitemap_filter(s)",
            "",
            "            if s.type == 'sitemapindex':",
            "                for loc in iterloc(it, self.sitemap_alternate_links):",
            "                    if any(x.search(loc) for x in self._follow):",
            "                        yield Request(loc, callback=self._parse_sitemap)",
            "            elif s.type == 'urlset':",
            "                for loc in iterloc(it, self.sitemap_alternate_links):",
            "                    for r, c in self._cbs:",
            "                        if r.search(loc):",
            "                            yield Request(loc, callback=c)",
            "                            break",
            "",
            "    def _get_sitemap_body(self, response):",
            "        \"\"\"Return the sitemap body contained in the given response,",
            "        or None if the response is not a sitemap.",
            "        \"\"\"",
            "        if isinstance(response, XmlResponse):",
            "            return response.body",
            "        if gzip_magic_number(response):",
            "            uncompressed_size = len(response.body)",
            "            max_size = response.meta.get(\"download_maxsize\", self._max_size)",
            "            warn_size = response.meta.get(\"download_warnsize\", self._warn_size)",
            "            try:",
            "                body = gunzip(response.body, max_size=max_size)",
            "            except _DecompressionMaxSizeExceeded:",
            "                return None",
            "            if uncompressed_size < warn_size <= len(body):",
            "                logger.warning(",
            "                    \"%(response)s body size after decompression (%(body_length)s B) \"",
            "                    \"is larger than the download warning size (%(warn_size)s B).\",",
            "                    {",
            "                        \"response\": response,",
            "                        \"body_length\": len(body),",
            "                        \"warn_size\": warn_size,",
            "                    },",
            "                )",
            "            return body",
            "        # actual gzipped sitemap files are decompressed above ;",
            "        # if we are here (response body is not gzipped)",
            "        # and have a response for .xml.gz,",
            "        # it usually means that it was already gunzipped",
            "        # by HttpCompression middleware,",
            "        # the HTTP response being sent with \"Content-Encoding: gzip\"",
            "        # without actually being a .xml.gz file in the first place,",
            "        # merely XML gzip-compressed on the fly,",
            "        # in other word, here, we have plain XML",
            "        if response.url.endswith('.xml') or response.url.endswith('.xml.gz'):",
            "            return response.body",
            "",
            "",
            "def regex(x):",
            "    if isinstance(x, six.string_types):",
            "        return re.compile(x)",
            "    return x",
            "",
            "",
            "def iterloc(it, alt=False):",
            "    for d in it:",
            "        yield d['loc']",
            "",
            "        # Also consider alternate URLs (xhtml:link rel=\"alternate\")",
            "        if alt and 'alternate' in d:",
            "            for l in d['alternate']:",
            "                yield l"
        ],
        "action": [
            "1",
            "0",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1": [],
            "5": [],
            "6": [],
            "7": [],
            "9": [],
            "73": [
                "SitemapSpider",
                "_get_sitemap_body"
            ],
            "74": [
                "SitemapSpider",
                "_get_sitemap_body"
            ],
            "84": [
                "SitemapSpider",
                "_get_sitemap_body"
            ]
        },
        "addLocation": []
    }
}