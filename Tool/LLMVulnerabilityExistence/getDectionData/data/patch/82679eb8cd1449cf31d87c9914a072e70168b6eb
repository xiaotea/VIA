{
    "piccolo/engine/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " import contextvars"
            },
            "1": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import logging"
            },
            "2": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " import pprint"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 6,
                "PatchRowcode": "+import string"
            },
            "4": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 7,
                "PatchRowcode": " import typing as t"
            },
            "5": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from abc import ABCMeta, abstractmethod"
            },
            "6": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " "
            },
            "7": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " logger = logging.getLogger(__name__)"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+# This is a set to speed up lookups from O(n) when"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+# using str vs O(1) when using set[str]"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+VALID_SAVEPOINT_CHARACTERS: t.Final[set[str]] = set("
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+    string.ascii_letters + string.digits + \"-\" + \"_\""
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 23,
                "PatchRowcode": "+)"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 24,
                "PatchRowcode": "+"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 25,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 26,
                "PatchRowcode": "+def validate_savepoint_name(savepoint_name: str) -> None:"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 27,
                "PatchRowcode": "+    \"\"\"Validates a save point's name meets the required character set.\"\"\""
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 28,
                "PatchRowcode": "+    if not all(i in VALID_SAVEPOINT_CHARACTERS for i in savepoint_name):"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 29,
                "PatchRowcode": "+        raise ValueError("
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+            \"Savepoint names can only contain the following characters:\""
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+            f\" {VALID_SAVEPOINT_CHARACTERS}\""
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+        )"
            },
            "24": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 33,
                "PatchRowcode": " "
            },
            "25": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 34,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 35,
                "PatchRowcode": " class Batch:"
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextvars",
            "import logging",
            "import pprint",
            "import typing as t",
            "from abc import ABCMeta, abstractmethod",
            "",
            "from piccolo.querystring import QueryString",
            "from piccolo.utils.sync import run_sync",
            "from piccolo.utils.warnings import Level, colored_string, colored_warning",
            "",
            "if t.TYPE_CHECKING:  # pragma: no cover",
            "    from piccolo.query.base import Query",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class Batch:",
            "    pass",
            "",
            "",
            "TransactionClass = t.TypeVar(\"TransactionClass\")",
            "",
            "",
            "class Engine(t.Generic[TransactionClass], metaclass=ABCMeta):",
            "",
            "    __slots__ = (\"query_id\",)",
            "",
            "    def __init__(self):",
            "        run_sync(self.check_version())",
            "        run_sync(self.prep_database())",
            "        self.query_id = 0",
            "",
            "    @property",
            "    @abstractmethod",
            "    def engine_type(self) -> str:",
            "        pass",
            "",
            "    @property",
            "    @abstractmethod",
            "    def min_version_number(self) -> float:",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def get_version(self) -> float:",
            "        pass",
            "",
            "    @abstractmethod",
            "    def get_version_sync(self) -> float:",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def prep_database(self):",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def batch(",
            "        self,",
            "        query: Query,",
            "        batch_size: int = 100,",
            "        node: t.Optional[str] = None,",
            "    ) -> Batch:",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def run_querystring(self, querystring: QueryString, in_pool: bool):",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def run_ddl(self, ddl: str, in_pool: bool = True):",
            "        pass",
            "",
            "    @abstractmethod",
            "    def transaction(self):",
            "        pass",
            "",
            "    @abstractmethod",
            "    def atomic(self):",
            "        pass",
            "",
            "    async def check_version(self):",
            "        \"\"\"",
            "        Warn if the database version isn't supported.",
            "        \"\"\"",
            "        try:",
            "            version_number = await self.get_version()",
            "        except Exception as exception:",
            "            colored_warning(",
            "                f\"Unable to fetch server version: {exception}\",",
            "                level=Level.high,",
            "            )",
            "            return",
            "",
            "        engine_type = self.engine_type.capitalize()",
            "        logger.info(f\"Running {engine_type} version {version_number}\")",
            "        if version_number and (version_number < self.min_version_number):",
            "            message = (",
            "                f\"This version of {self.engine_type} isn't supported \"",
            "                f\"(< {self.min_version_number}) - some features might not be \"",
            "                \"available. For instructions on installing databases, see the \"",
            "                \"Piccolo docs.\"",
            "            )",
            "            colored_warning(message, stacklevel=3)",
            "",
            "    def _connection_pool_warning(self):",
            "        message = (",
            "            f\"Connection pooling is not supported for {self.engine_type}.\"",
            "        )",
            "        logger.warning(message)",
            "        colored_warning(message, stacklevel=3)",
            "",
            "    async def start_connection_pool(self):",
            "        \"\"\"",
            "        The database driver doesn't implement connection pooling.",
            "        \"\"\"",
            "        self._connection_pool_warning()",
            "",
            "    async def close_connection_pool(self):",
            "        \"\"\"",
            "        The database driver doesn't implement connection pooling.",
            "        \"\"\"",
            "        self._connection_pool_warning()",
            "",
            "    ###########################################################################",
            "",
            "    current_transaction: contextvars.ContextVar[t.Optional[TransactionClass]]",
            "",
            "    def transaction_exists(self) -> bool:",
            "        \"\"\"",
            "        Find out if a transaction is currently active.",
            "",
            "        :returns:",
            "            ``True`` if a transaction is already active for the current",
            "            asyncio task. This is useful to know, because nested transactions",
            "            aren't currently supported, so you can check if an existing",
            "            transaction is already active, before creating a new one.",
            "",
            "        \"\"\"",
            "        return self.current_transaction.get() is not None",
            "",
            "    ###########################################################################",
            "    # Logging queries and responses",
            "",
            "    def get_query_id(self) -> int:",
            "        self.query_id += 1",
            "        return self.query_id",
            "",
            "    def print_query(self, query_id: int, query: str):",
            "        print(colored_string(f\"\\nQuery {query_id}:\"))",
            "        print(query)",
            "",
            "    def print_response(self, query_id: int, response: t.List):",
            "        print(",
            "            colored_string(f\"\\nQuery {query_id} response:\", level=Level.high)",
            "        )",
            "        pprint.pprint(response)"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextvars",
            "import logging",
            "import pprint",
            "import string",
            "import typing as t",
            "from abc import ABCMeta, abstractmethod",
            "",
            "from piccolo.querystring import QueryString",
            "from piccolo.utils.sync import run_sync",
            "from piccolo.utils.warnings import Level, colored_string, colored_warning",
            "",
            "if t.TYPE_CHECKING:  # pragma: no cover",
            "    from piccolo.query.base import Query",
            "",
            "",
            "logger = logging.getLogger(__name__)",
            "# This is a set to speed up lookups from O(n) when",
            "# using str vs O(1) when using set[str]",
            "VALID_SAVEPOINT_CHARACTERS: t.Final[set[str]] = set(",
            "    string.ascii_letters + string.digits + \"-\" + \"_\"",
            ")",
            "",
            "",
            "def validate_savepoint_name(savepoint_name: str) -> None:",
            "    \"\"\"Validates a save point's name meets the required character set.\"\"\"",
            "    if not all(i in VALID_SAVEPOINT_CHARACTERS for i in savepoint_name):",
            "        raise ValueError(",
            "            \"Savepoint names can only contain the following characters:\"",
            "            f\" {VALID_SAVEPOINT_CHARACTERS}\"",
            "        )",
            "",
            "",
            "class Batch:",
            "    pass",
            "",
            "",
            "TransactionClass = t.TypeVar(\"TransactionClass\")",
            "",
            "",
            "class Engine(t.Generic[TransactionClass], metaclass=ABCMeta):",
            "",
            "    __slots__ = (\"query_id\",)",
            "",
            "    def __init__(self):",
            "        run_sync(self.check_version())",
            "        run_sync(self.prep_database())",
            "        self.query_id = 0",
            "",
            "    @property",
            "    @abstractmethod",
            "    def engine_type(self) -> str:",
            "        pass",
            "",
            "    @property",
            "    @abstractmethod",
            "    def min_version_number(self) -> float:",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def get_version(self) -> float:",
            "        pass",
            "",
            "    @abstractmethod",
            "    def get_version_sync(self) -> float:",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def prep_database(self):",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def batch(",
            "        self,",
            "        query: Query,",
            "        batch_size: int = 100,",
            "        node: t.Optional[str] = None,",
            "    ) -> Batch:",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def run_querystring(self, querystring: QueryString, in_pool: bool):",
            "        pass",
            "",
            "    @abstractmethod",
            "    async def run_ddl(self, ddl: str, in_pool: bool = True):",
            "        pass",
            "",
            "    @abstractmethod",
            "    def transaction(self):",
            "        pass",
            "",
            "    @abstractmethod",
            "    def atomic(self):",
            "        pass",
            "",
            "    async def check_version(self):",
            "        \"\"\"",
            "        Warn if the database version isn't supported.",
            "        \"\"\"",
            "        try:",
            "            version_number = await self.get_version()",
            "        except Exception as exception:",
            "            colored_warning(",
            "                f\"Unable to fetch server version: {exception}\",",
            "                level=Level.high,",
            "            )",
            "            return",
            "",
            "        engine_type = self.engine_type.capitalize()",
            "        logger.info(f\"Running {engine_type} version {version_number}\")",
            "        if version_number and (version_number < self.min_version_number):",
            "            message = (",
            "                f\"This version of {self.engine_type} isn't supported \"",
            "                f\"(< {self.min_version_number}) - some features might not be \"",
            "                \"available. For instructions on installing databases, see the \"",
            "                \"Piccolo docs.\"",
            "            )",
            "            colored_warning(message, stacklevel=3)",
            "",
            "    def _connection_pool_warning(self):",
            "        message = (",
            "            f\"Connection pooling is not supported for {self.engine_type}.\"",
            "        )",
            "        logger.warning(message)",
            "        colored_warning(message, stacklevel=3)",
            "",
            "    async def start_connection_pool(self):",
            "        \"\"\"",
            "        The database driver doesn't implement connection pooling.",
            "        \"\"\"",
            "        self._connection_pool_warning()",
            "",
            "    async def close_connection_pool(self):",
            "        \"\"\"",
            "        The database driver doesn't implement connection pooling.",
            "        \"\"\"",
            "        self._connection_pool_warning()",
            "",
            "    ###########################################################################",
            "",
            "    current_transaction: contextvars.ContextVar[t.Optional[TransactionClass]]",
            "",
            "    def transaction_exists(self) -> bool:",
            "        \"\"\"",
            "        Find out if a transaction is currently active.",
            "",
            "        :returns:",
            "            ``True`` if a transaction is already active for the current",
            "            asyncio task. This is useful to know, because nested transactions",
            "            aren't currently supported, so you can check if an existing",
            "            transaction is already active, before creating a new one.",
            "",
            "        \"\"\"",
            "        return self.current_transaction.get() is not None",
            "",
            "    ###########################################################################",
            "    # Logging queries and responses",
            "",
            "    def get_query_id(self) -> int:",
            "        self.query_id += 1",
            "        return self.query_id",
            "",
            "    def print_query(self, query_id: int, query: str):",
            "        print(colored_string(f\"\\nQuery {query_id}:\"))",
            "        print(query)",
            "",
            "    def print_response(self, query_id: int, response: t.List):",
            "        print(",
            "            colored_string(f\"\\nQuery {query_id} response:\", level=Level.high)",
            "        )",
            "        pprint.pprint(response)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "tensorflow.python.kernel_tests.init_ops_test.RangeTest"
        ]
    },
    "piccolo/engine/postgres.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 4,
                "PatchRowcode": " import typing as t"
            },
            "1": {
                "beforePatchRowNumber": 5,
                "afterPatchRowNumber": 5,
                "PatchRowcode": " from dataclasses import dataclass"
            },
            "2": {
                "beforePatchRowNumber": 6,
                "afterPatchRowNumber": 6,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 7,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from piccolo.engine.base import Batch, Engine"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 7,
                "PatchRowcode": "+from piccolo.engine.base import Batch, Engine, validate_savepoint_name"
            },
            "5": {
                "beforePatchRowNumber": 8,
                "afterPatchRowNumber": 8,
                "PatchRowcode": " from piccolo.engine.exceptions import TransactionError"
            },
            "6": {
                "beforePatchRowNumber": 9,
                "afterPatchRowNumber": 9,
                "PatchRowcode": " from piccolo.query.base import DDL, Query"
            },
            "7": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from piccolo.querystring import QueryString"
            },
            "8": {
                "beforePatchRowNumber": 129,
                "afterPatchRowNumber": 129,
                "PatchRowcode": "         self.transaction = transaction"
            },
            "9": {
                "beforePatchRowNumber": 130,
                "afterPatchRowNumber": 130,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 131,
                "afterPatchRowNumber": 131,
                "PatchRowcode": "     async def rollback_to(self):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 132,
                "PatchRowcode": "+        validate_savepoint_name(self.name)"
            },
            "12": {
                "beforePatchRowNumber": 132,
                "afterPatchRowNumber": 133,
                "PatchRowcode": "         await self.transaction.connection.execute("
            },
            "13": {
                "beforePatchRowNumber": 133,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "             f\"ROLLBACK TO SAVEPOINT {self.name}\""
            },
            "14": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "         )"
            },
            "15": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": 136,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 137,
                "PatchRowcode": "     async def release(self):"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 138,
                "PatchRowcode": "+        validate_savepoint_name(self.name)"
            },
            "18": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": 139,
                "PatchRowcode": "         await self.transaction.connection.execute("
            },
            "19": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 140,
                "PatchRowcode": "             f\"RELEASE SAVEPOINT {self.name}\""
            },
            "20": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 141,
                "PatchRowcode": "         )"
            },
            "21": {
                "beforePatchRowNumber": 236,
                "afterPatchRowNumber": 238,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 237,
                "afterPatchRowNumber": 239,
                "PatchRowcode": "     async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:"
            },
            "23": {
                "beforePatchRowNumber": 238,
                "afterPatchRowNumber": 240,
                "PatchRowcode": "         name = name or f\"savepoint_{self.get_savepoint_id()}\""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 241,
                "PatchRowcode": "+        validate_savepoint_name(name)"
            },
            "25": {
                "beforePatchRowNumber": 239,
                "afterPatchRowNumber": 242,
                "PatchRowcode": "         await self.connection.execute(f\"SAVEPOINT {name}\")"
            },
            "26": {
                "beforePatchRowNumber": 240,
                "afterPatchRowNumber": 243,
                "PatchRowcode": "         return Savepoint(name=name, transaction=self)"
            },
            "27": {
                "beforePatchRowNumber": 241,
                "afterPatchRowNumber": 244,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextvars",
            "import typing as t",
            "from dataclasses import dataclass",
            "",
            "from piccolo.engine.base import Batch, Engine",
            "from piccolo.engine.exceptions import TransactionError",
            "from piccolo.query.base import DDL, Query",
            "from piccolo.querystring import QueryString",
            "from piccolo.utils.lazy_loader import LazyLoader",
            "from piccolo.utils.sync import run_sync",
            "from piccolo.utils.warnings import Level, colored_warning",
            "",
            "asyncpg = LazyLoader(\"asyncpg\", globals(), \"asyncpg\")",
            "",
            "if t.TYPE_CHECKING:  # pragma: no cover",
            "    from asyncpg.connection import Connection",
            "    from asyncpg.cursor import Cursor",
            "    from asyncpg.pool import Pool",
            "",
            "",
            "@dataclass",
            "class AsyncBatch(Batch):",
            "",
            "    connection: Connection",
            "    query: Query",
            "    batch_size: int",
            "",
            "    # Set internally",
            "    _transaction = None",
            "    _cursor: t.Optional[Cursor] = None",
            "",
            "    @property",
            "    def cursor(self) -> Cursor:",
            "        if not self._cursor:",
            "            raise ValueError(\"_cursor not set\")",
            "        return self._cursor",
            "",
            "    async def next(self) -> t.List[t.Dict]:",
            "        data = await self.cursor.fetch(self.batch_size)",
            "        return await self.query._process_results(data)",
            "",
            "    def __aiter__(self):",
            "        return self",
            "",
            "    async def __anext__(self):",
            "        response = await self.next()",
            "        if response == []:",
            "            raise StopAsyncIteration()",
            "        return response",
            "",
            "    async def __aenter__(self):",
            "        self._transaction = self.connection.transaction()",
            "        await self._transaction.start()",
            "        querystring = self.query.querystrings[0]",
            "        template, template_args = querystring.compile_string()",
            "",
            "        self._cursor = await self.connection.cursor(template, *template_args)",
            "        return self",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        if exception:",
            "            await self._transaction.rollback()",
            "        else:",
            "            await self._transaction.commit()",
            "",
            "        await self.connection.close()",
            "",
            "        return exception is not None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class Atomic:",
            "    \"\"\"",
            "    This is useful if you want to build up a transaction programatically, by",
            "    adding queries to it.",
            "",
            "    Usage::",
            "",
            "        transaction = engine.atomic()",
            "        transaction.add(Foo.create_table())",
            "",
            "        # Either:",
            "        transaction.run_sync()",
            "        await transaction.run()",
            "",
            "    \"\"\"",
            "",
            "    __slots__ = (\"engine\", \"queries\")",
            "",
            "    def __init__(self, engine: PostgresEngine):",
            "        self.engine = engine",
            "        self.queries: t.List[Query] = []",
            "",
            "    def add(self, *query: Query):",
            "        self.queries += list(query)",
            "",
            "    async def run(self):",
            "        from piccolo.query.methods.objects import Create, GetOrCreate",
            "",
            "        try:",
            "            async with self.engine.transaction():",
            "                for query in self.queries:",
            "                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):",
            "                        await query.run()",
            "                    else:",
            "                        raise ValueError(\"Unrecognised query\")",
            "            self.queries = []",
            "        except Exception as exception:",
            "            self.queries = []",
            "            raise exception from exception",
            "",
            "    def run_sync(self):",
            "        return run_sync(self.run())",
            "",
            "    def __await__(self):",
            "        return self.run().__await__()",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class Savepoint:",
            "    def __init__(self, name: str, transaction: PostgresTransaction):",
            "        self.name = name",
            "        self.transaction = transaction",
            "",
            "    async def rollback_to(self):",
            "        await self.transaction.connection.execute(",
            "            f\"ROLLBACK TO SAVEPOINT {self.name}\"",
            "        )",
            "",
            "    async def release(self):",
            "        await self.transaction.connection.execute(",
            "            f\"RELEASE SAVEPOINT {self.name}\"",
            "        )",
            "",
            "",
            "class PostgresTransaction:",
            "    \"\"\"",
            "    Used for wrapping queries in a transaction, using a context manager.",
            "    Currently it's async only.",
            "",
            "    Usage::",
            "",
            "        async with engine.transaction():",
            "            # Run some queries:",
            "            await Band.select().run()",
            "",
            "    \"\"\"",
            "",
            "    __slots__ = (",
            "        \"engine\",",
            "        \"transaction\",",
            "        \"context\",",
            "        \"connection\",",
            "        \"_savepoint_id\",",
            "        \"_parent\",",
            "        \"_committed\",",
            "        \"_rolled_back\",",
            "    )",
            "",
            "    def __init__(self, engine: PostgresEngine, allow_nested: bool = True):",
            "        \"\"\"",
            "        :param allow_nested:",
            "            If ``True`` then if we try creating a new transaction when another",
            "            is already active, we treat this as a no-op::",
            "",
            "                async with DB.transaction():",
            "                    async with DB.transaction():",
            "                        pass",
            "",
            "            If we want to disallow this behaviour, then setting",
            "            ``allow_nested=False`` will cause a ``TransactionError`` to be",
            "            raised.",
            "",
            "        \"\"\"",
            "        self.engine = engine",
            "        current_transaction = self.engine.current_transaction.get()",
            "",
            "        self._savepoint_id = 0",
            "        self._parent = None",
            "        self._committed = False",
            "        self._rolled_back = False",
            "",
            "        if current_transaction:",
            "            if allow_nested:",
            "                self._parent = current_transaction",
            "            else:",
            "                raise TransactionError(",
            "                    \"A transaction is already active - nested transactions \"",
            "                    \"aren't allowed.\"",
            "                )",
            "",
            "    async def __aenter__(self) -> PostgresTransaction:",
            "        if self._parent is not None:",
            "            return self._parent",
            "",
            "        self.connection = await self.get_connection()",
            "        self.transaction = self.connection.transaction()",
            "        await self.begin()",
            "        self.context = self.engine.current_transaction.set(self)",
            "        return self",
            "",
            "    async def get_connection(self):",
            "        if self.engine.pool:",
            "            return await self.engine.pool.acquire()",
            "        else:",
            "            return await self.engine.get_new_connection()",
            "",
            "    async def begin(self):",
            "        await self.transaction.start()",
            "",
            "    async def commit(self):",
            "        await self.transaction.commit()",
            "        self._committed = True",
            "",
            "    async def rollback(self):",
            "        await self.transaction.rollback()",
            "        self._rolled_back = True",
            "",
            "    async def rollback_to(self, savepoint_name: str):",
            "        \"\"\"",
            "        Used to rollback to a savepoint just using the name.",
            "        \"\"\"",
            "        await Savepoint(name=savepoint_name, transaction=self).rollback_to()",
            "",
            "    ###########################################################################",
            "",
            "    def get_savepoint_id(self) -> int:",
            "        self._savepoint_id += 1",
            "        return self._savepoint_id",
            "",
            "    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:",
            "        name = name or f\"savepoint_{self.get_savepoint_id()}\"",
            "        await self.connection.execute(f\"SAVEPOINT {name}\")",
            "        return Savepoint(name=name, transaction=self)",
            "",
            "    ###########################################################################",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        if self._parent:",
            "            return exception is None",
            "",
            "        if exception:",
            "            # The user may have manually rolled it back.",
            "            if not self._rolled_back:",
            "                await self.rollback()",
            "        else:",
            "            # The user may have manually committed it.",
            "            if not self._committed and not self._rolled_back:",
            "                await self.commit()",
            "",
            "        if self.engine.pool:",
            "            await self.engine.pool.release(self.connection)",
            "        else:",
            "            await self.connection.close()",
            "",
            "        self.engine.current_transaction.reset(self.context)",
            "",
            "        return exception is None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class PostgresEngine(Engine[t.Optional[PostgresTransaction]]):",
            "    \"\"\"",
            "    Used to connect to PostgreSQL.",
            "",
            "    :param config:",
            "        The config dictionary is passed to the underlying database adapter,",
            "        asyncpg. Common arguments you're likely to need are:",
            "",
            "        * host",
            "        * port",
            "        * user",
            "        * password",
            "        * database",
            "",
            "        For example, ``{'host': 'localhost', 'port': 5432}``.",
            "",
            "        See the `asyncpg docs <https://magicstack.github.io/asyncpg/current/api/index.html#connection>`_",
            "        for all available options.",
            "",
            "    :param extensions:",
            "        When the engine starts, it will try and create these extensions",
            "        in Postgres. If you're using a read only database, set this value to an",
            "        empty tuple ``()``.",
            "",
            "    :param log_queries:",
            "        If ``True``, all SQL and DDL statements are printed out before being",
            "        run. Useful for debugging.",
            "",
            "    :param log_responses:",
            "        If ``True``, the raw response from each query is printed out. Useful",
            "        for debugging.",
            "",
            "    :param extra_nodes:",
            "        If you have additional database nodes (e.g. read replicas) for the",
            "        server, you can specify them here. It's a mapping of a memorable name",
            "        to a ``PostgresEngine`` instance. For example::",
            "",
            "            DB = PostgresEngine(",
            "                config={'database': 'main_db'},",
            "                extra_nodes={",
            "                    'read_replica_1': PostgresEngine(",
            "                        config={",
            "                            'database': 'main_db',",
            "                            host: 'read_replicate.my_db.com'",
            "                        },",
            "                        extensions=()",
            "                    )",
            "                }",
            "            )",
            "",
            "        Note how we set ``extensions=()``, because it's a read only database.",
            "",
            "        When executing a query, you can specify one of these nodes instead",
            "        of the main database. For example::",
            "",
            "            >>> await MyTable.select().run(node=\"read_replica_1\")",
            "",
            "    \"\"\"  # noqa: E501",
            "",
            "    __slots__ = (",
            "        \"config\",",
            "        \"extensions\",",
            "        \"log_queries\",",
            "        \"log_responses\",",
            "        \"extra_nodes\",",
            "        \"pool\",",
            "        \"current_transaction\",",
            "    )",
            "",
            "    engine_type = \"postgres\"",
            "    min_version_number = 10",
            "",
            "    def __init__(",
            "        self,",
            "        config: t.Dict[str, t.Any],",
            "        extensions: t.Sequence[str] = (\"uuid-ossp\",),",
            "        log_queries: bool = False,",
            "        log_responses: bool = False,",
            "        extra_nodes: t.Mapping[str, PostgresEngine] = None,",
            "    ) -> None:",
            "        if extra_nodes is None:",
            "            extra_nodes = {}",
            "",
            "        self.config = config",
            "        self.extensions = extensions",
            "        self.log_queries = log_queries",
            "        self.log_responses = log_responses",
            "        self.extra_nodes = extra_nodes",
            "        self.pool: t.Optional[Pool] = None",
            "        database_name = config.get(\"database\", \"Unknown\")",
            "        self.current_transaction = contextvars.ContextVar(",
            "            f\"pg_current_transaction_{database_name}\", default=None",
            "        )",
            "        super().__init__()",
            "",
            "    @staticmethod",
            "    def _parse_raw_version_string(version_string: str) -> float:",
            "        \"\"\"",
            "        The format of the version string isn't always consistent. Sometimes",
            "        it's just the version number e.g. '9.6.18', and sometimes",
            "        it contains specific build information e.g.",
            "        '12.4 (Ubuntu 12.4-0ubuntu0.20.04.1)'. Just extract the major and",
            "        minor version numbers.",
            "        \"\"\"",
            "        version_segment = version_string.split(\" \")[0]",
            "        major, minor = version_segment.split(\".\")[:2]",
            "        return float(f\"{major}.{minor}\")",
            "",
            "    async def get_version(self) -> float:",
            "        \"\"\"",
            "        Returns the version of Postgres being run.",
            "        \"\"\"",
            "        try:",
            "            response: t.Sequence[t.Dict] = await self._run_in_new_connection(",
            "                \"SHOW server_version\"",
            "            )",
            "        except ConnectionRefusedError as exception:",
            "            # Suppressing the exception, otherwise importing piccolo_conf.py",
            "            # containing an engine will raise an ImportError.",
            "            colored_warning(f\"Unable to connect to database - {exception}\")",
            "            return 0.0",
            "        else:",
            "            version_string = response[0][\"server_version\"]",
            "            return self._parse_raw_version_string(",
            "                version_string=version_string",
            "            )",
            "",
            "    def get_version_sync(self) -> float:",
            "        return run_sync(self.get_version())",
            "",
            "    async def prep_database(self):",
            "        for extension in self.extensions:",
            "            try:",
            "                await self._run_in_new_connection(",
            "                    f'CREATE EXTENSION IF NOT EXISTS \"{extension}\"',",
            "                )",
            "            except asyncpg.exceptions.InsufficientPrivilegeError:",
            "                colored_warning(",
            "                    f\"=> Unable to create {extension} extension - some \"",
            "                    \"functionality may not behave as expected. Make sure \"",
            "                    \"your database user has permission to create \"",
            "                    \"extensions, or add it manually using \"",
            "                    f'`CREATE EXTENSION \"{extension}\";`',",
            "                    level=Level.medium,",
            "                )",
            "",
            "    ###########################################################################",
            "    # These typos existed in the codebase for a while, so leaving these proxy",
            "    # methods for now to ensure backwards compatibility.",
            "",
            "    async def start_connnection_pool(self, **kwargs) -> None:",
            "        colored_warning(",
            "            \"`start_connnection_pool` is a typo - please change it to \"",
            "            \"`start_connection_pool`.\",",
            "            category=DeprecationWarning,",
            "        )",
            "        return await self.start_connection_pool()",
            "",
            "    async def close_connnection_pool(self, **kwargs) -> None:",
            "        colored_warning(",
            "            \"`close_connnection_pool` is a typo - please change it to \"",
            "            \"`close_connection_pool`.\",",
            "            category=DeprecationWarning,",
            "        )",
            "        return await self.close_connection_pool()",
            "",
            "    ###########################################################################",
            "",
            "    async def start_connection_pool(self, **kwargs) -> None:",
            "        if self.pool:",
            "            colored_warning(",
            "                \"A pool already exists - close it first if you want to create \"",
            "                \"a new pool.\",",
            "            )",
            "        else:",
            "            config = dict(self.config)",
            "            config.update(**kwargs)",
            "            self.pool = await asyncpg.create_pool(**config)",
            "",
            "    async def close_connection_pool(self) -> None:",
            "        if self.pool:",
            "            await self.pool.close()",
            "            self.pool = None",
            "        else:",
            "            colored_warning(\"No pool is running.\")",
            "",
            "    ###########################################################################",
            "",
            "    async def get_new_connection(self) -> Connection:",
            "        \"\"\"",
            "        Returns a new connection - doesn't retrieve it from the pool.",
            "        \"\"\"",
            "        return await asyncpg.connect(**self.config)",
            "",
            "    ###########################################################################",
            "",
            "    async def batch(",
            "        self,",
            "        query: Query,",
            "        batch_size: int = 100,",
            "        node: t.Optional[str] = None,",
            "    ) -> AsyncBatch:",
            "        \"\"\"",
            "        :param query:",
            "            The database query to run.",
            "        :param batch_size:",
            "            How many rows to fetch on each iteration.",
            "        :param node:",
            "            Which node to run the query on (see ``extra_nodes``). If not",
            "            specified, it runs on the main Postgres node.",
            "        \"\"\"",
            "        engine: t.Any = self.extra_nodes.get(node) if node else self",
            "        connection = await engine.get_new_connection()",
            "        return AsyncBatch(",
            "            connection=connection, query=query, batch_size=batch_size",
            "        )",
            "",
            "    ###########################################################################",
            "",
            "    async def _run_in_pool(self, query: str, args: t.Sequence[t.Any] = None):",
            "        if args is None:",
            "            args = []",
            "        if not self.pool:",
            "            raise ValueError(\"A pool isn't currently running.\")",
            "",
            "        async with self.pool.acquire() as connection:",
            "            response = await connection.fetch(query, *args)",
            "",
            "        return response",
            "",
            "    async def _run_in_new_connection(",
            "        self, query: str, args: t.Sequence[t.Any] = None",
            "    ):",
            "        if args is None:",
            "            args = []",
            "        connection = await self.get_new_connection()",
            "",
            "        try:",
            "            results = await connection.fetch(query, *args)",
            "        except asyncpg.exceptions.PostgresError as exception:",
            "            await connection.close()",
            "            raise exception",
            "",
            "        await connection.close()",
            "        return results",
            "",
            "    async def run_querystring(",
            "        self, querystring: QueryString, in_pool: bool = True",
            "    ):",
            "        query, query_args = querystring.compile_string(",
            "            engine_type=self.engine_type",
            "        )",
            "",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=querystring.__str__())",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await current_transaction.connection.fetch(",
            "                query, *query_args",
            "            )",
            "        elif in_pool and self.pool:",
            "            response = await self._run_in_pool(query, query_args)",
            "        else:",
            "            response = await self._run_in_new_connection(query, query_args)",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    async def run_ddl(self, ddl: str, in_pool: bool = True):",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=ddl)",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await current_transaction.connection.fetch(ddl)",
            "        elif in_pool and self.pool:",
            "            response = await self._run_in_pool(ddl)",
            "        else:",
            "            response = await self._run_in_new_connection(ddl)",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    def atomic(self) -> Atomic:",
            "        return Atomic(engine=self)",
            "",
            "    def transaction(self, allow_nested: bool = True) -> PostgresTransaction:",
            "        return PostgresTransaction(engine=self, allow_nested=allow_nested)"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextvars",
            "import typing as t",
            "from dataclasses import dataclass",
            "",
            "from piccolo.engine.base import Batch, Engine, validate_savepoint_name",
            "from piccolo.engine.exceptions import TransactionError",
            "from piccolo.query.base import DDL, Query",
            "from piccolo.querystring import QueryString",
            "from piccolo.utils.lazy_loader import LazyLoader",
            "from piccolo.utils.sync import run_sync",
            "from piccolo.utils.warnings import Level, colored_warning",
            "",
            "asyncpg = LazyLoader(\"asyncpg\", globals(), \"asyncpg\")",
            "",
            "if t.TYPE_CHECKING:  # pragma: no cover",
            "    from asyncpg.connection import Connection",
            "    from asyncpg.cursor import Cursor",
            "    from asyncpg.pool import Pool",
            "",
            "",
            "@dataclass",
            "class AsyncBatch(Batch):",
            "",
            "    connection: Connection",
            "    query: Query",
            "    batch_size: int",
            "",
            "    # Set internally",
            "    _transaction = None",
            "    _cursor: t.Optional[Cursor] = None",
            "",
            "    @property",
            "    def cursor(self) -> Cursor:",
            "        if not self._cursor:",
            "            raise ValueError(\"_cursor not set\")",
            "        return self._cursor",
            "",
            "    async def next(self) -> t.List[t.Dict]:",
            "        data = await self.cursor.fetch(self.batch_size)",
            "        return await self.query._process_results(data)",
            "",
            "    def __aiter__(self):",
            "        return self",
            "",
            "    async def __anext__(self):",
            "        response = await self.next()",
            "        if response == []:",
            "            raise StopAsyncIteration()",
            "        return response",
            "",
            "    async def __aenter__(self):",
            "        self._transaction = self.connection.transaction()",
            "        await self._transaction.start()",
            "        querystring = self.query.querystrings[0]",
            "        template, template_args = querystring.compile_string()",
            "",
            "        self._cursor = await self.connection.cursor(template, *template_args)",
            "        return self",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        if exception:",
            "            await self._transaction.rollback()",
            "        else:",
            "            await self._transaction.commit()",
            "",
            "        await self.connection.close()",
            "",
            "        return exception is not None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class Atomic:",
            "    \"\"\"",
            "    This is useful if you want to build up a transaction programatically, by",
            "    adding queries to it.",
            "",
            "    Usage::",
            "",
            "        transaction = engine.atomic()",
            "        transaction.add(Foo.create_table())",
            "",
            "        # Either:",
            "        transaction.run_sync()",
            "        await transaction.run()",
            "",
            "    \"\"\"",
            "",
            "    __slots__ = (\"engine\", \"queries\")",
            "",
            "    def __init__(self, engine: PostgresEngine):",
            "        self.engine = engine",
            "        self.queries: t.List[Query] = []",
            "",
            "    def add(self, *query: Query):",
            "        self.queries += list(query)",
            "",
            "    async def run(self):",
            "        from piccolo.query.methods.objects import Create, GetOrCreate",
            "",
            "        try:",
            "            async with self.engine.transaction():",
            "                for query in self.queries:",
            "                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):",
            "                        await query.run()",
            "                    else:",
            "                        raise ValueError(\"Unrecognised query\")",
            "            self.queries = []",
            "        except Exception as exception:",
            "            self.queries = []",
            "            raise exception from exception",
            "",
            "    def run_sync(self):",
            "        return run_sync(self.run())",
            "",
            "    def __await__(self):",
            "        return self.run().__await__()",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class Savepoint:",
            "    def __init__(self, name: str, transaction: PostgresTransaction):",
            "        self.name = name",
            "        self.transaction = transaction",
            "",
            "    async def rollback_to(self):",
            "        validate_savepoint_name(self.name)",
            "        await self.transaction.connection.execute(",
            "            f\"ROLLBACK TO SAVEPOINT {self.name}\"",
            "        )",
            "",
            "    async def release(self):",
            "        validate_savepoint_name(self.name)",
            "        await self.transaction.connection.execute(",
            "            f\"RELEASE SAVEPOINT {self.name}\"",
            "        )",
            "",
            "",
            "class PostgresTransaction:",
            "    \"\"\"",
            "    Used for wrapping queries in a transaction, using a context manager.",
            "    Currently it's async only.",
            "",
            "    Usage::",
            "",
            "        async with engine.transaction():",
            "            # Run some queries:",
            "            await Band.select().run()",
            "",
            "    \"\"\"",
            "",
            "    __slots__ = (",
            "        \"engine\",",
            "        \"transaction\",",
            "        \"context\",",
            "        \"connection\",",
            "        \"_savepoint_id\",",
            "        \"_parent\",",
            "        \"_committed\",",
            "        \"_rolled_back\",",
            "    )",
            "",
            "    def __init__(self, engine: PostgresEngine, allow_nested: bool = True):",
            "        \"\"\"",
            "        :param allow_nested:",
            "            If ``True`` then if we try creating a new transaction when another",
            "            is already active, we treat this as a no-op::",
            "",
            "                async with DB.transaction():",
            "                    async with DB.transaction():",
            "                        pass",
            "",
            "            If we want to disallow this behaviour, then setting",
            "            ``allow_nested=False`` will cause a ``TransactionError`` to be",
            "            raised.",
            "",
            "        \"\"\"",
            "        self.engine = engine",
            "        current_transaction = self.engine.current_transaction.get()",
            "",
            "        self._savepoint_id = 0",
            "        self._parent = None",
            "        self._committed = False",
            "        self._rolled_back = False",
            "",
            "        if current_transaction:",
            "            if allow_nested:",
            "                self._parent = current_transaction",
            "            else:",
            "                raise TransactionError(",
            "                    \"A transaction is already active - nested transactions \"",
            "                    \"aren't allowed.\"",
            "                )",
            "",
            "    async def __aenter__(self) -> PostgresTransaction:",
            "        if self._parent is not None:",
            "            return self._parent",
            "",
            "        self.connection = await self.get_connection()",
            "        self.transaction = self.connection.transaction()",
            "        await self.begin()",
            "        self.context = self.engine.current_transaction.set(self)",
            "        return self",
            "",
            "    async def get_connection(self):",
            "        if self.engine.pool:",
            "            return await self.engine.pool.acquire()",
            "        else:",
            "            return await self.engine.get_new_connection()",
            "",
            "    async def begin(self):",
            "        await self.transaction.start()",
            "",
            "    async def commit(self):",
            "        await self.transaction.commit()",
            "        self._committed = True",
            "",
            "    async def rollback(self):",
            "        await self.transaction.rollback()",
            "        self._rolled_back = True",
            "",
            "    async def rollback_to(self, savepoint_name: str):",
            "        \"\"\"",
            "        Used to rollback to a savepoint just using the name.",
            "        \"\"\"",
            "        await Savepoint(name=savepoint_name, transaction=self).rollback_to()",
            "",
            "    ###########################################################################",
            "",
            "    def get_savepoint_id(self) -> int:",
            "        self._savepoint_id += 1",
            "        return self._savepoint_id",
            "",
            "    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:",
            "        name = name or f\"savepoint_{self.get_savepoint_id()}\"",
            "        validate_savepoint_name(name)",
            "        await self.connection.execute(f\"SAVEPOINT {name}\")",
            "        return Savepoint(name=name, transaction=self)",
            "",
            "    ###########################################################################",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        if self._parent:",
            "            return exception is None",
            "",
            "        if exception:",
            "            # The user may have manually rolled it back.",
            "            if not self._rolled_back:",
            "                await self.rollback()",
            "        else:",
            "            # The user may have manually committed it.",
            "            if not self._committed and not self._rolled_back:",
            "                await self.commit()",
            "",
            "        if self.engine.pool:",
            "            await self.engine.pool.release(self.connection)",
            "        else:",
            "            await self.connection.close()",
            "",
            "        self.engine.current_transaction.reset(self.context)",
            "",
            "        return exception is None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class PostgresEngine(Engine[t.Optional[PostgresTransaction]]):",
            "    \"\"\"",
            "    Used to connect to PostgreSQL.",
            "",
            "    :param config:",
            "        The config dictionary is passed to the underlying database adapter,",
            "        asyncpg. Common arguments you're likely to need are:",
            "",
            "        * host",
            "        * port",
            "        * user",
            "        * password",
            "        * database",
            "",
            "        For example, ``{'host': 'localhost', 'port': 5432}``.",
            "",
            "        See the `asyncpg docs <https://magicstack.github.io/asyncpg/current/api/index.html#connection>`_",
            "        for all available options.",
            "",
            "    :param extensions:",
            "        When the engine starts, it will try and create these extensions",
            "        in Postgres. If you're using a read only database, set this value to an",
            "        empty tuple ``()``.",
            "",
            "    :param log_queries:",
            "        If ``True``, all SQL and DDL statements are printed out before being",
            "        run. Useful for debugging.",
            "",
            "    :param log_responses:",
            "        If ``True``, the raw response from each query is printed out. Useful",
            "        for debugging.",
            "",
            "    :param extra_nodes:",
            "        If you have additional database nodes (e.g. read replicas) for the",
            "        server, you can specify them here. It's a mapping of a memorable name",
            "        to a ``PostgresEngine`` instance. For example::",
            "",
            "            DB = PostgresEngine(",
            "                config={'database': 'main_db'},",
            "                extra_nodes={",
            "                    'read_replica_1': PostgresEngine(",
            "                        config={",
            "                            'database': 'main_db',",
            "                            host: 'read_replicate.my_db.com'",
            "                        },",
            "                        extensions=()",
            "                    )",
            "                }",
            "            )",
            "",
            "        Note how we set ``extensions=()``, because it's a read only database.",
            "",
            "        When executing a query, you can specify one of these nodes instead",
            "        of the main database. For example::",
            "",
            "            >>> await MyTable.select().run(node=\"read_replica_1\")",
            "",
            "    \"\"\"  # noqa: E501",
            "",
            "    __slots__ = (",
            "        \"config\",",
            "        \"extensions\",",
            "        \"log_queries\",",
            "        \"log_responses\",",
            "        \"extra_nodes\",",
            "        \"pool\",",
            "        \"current_transaction\",",
            "    )",
            "",
            "    engine_type = \"postgres\"",
            "    min_version_number = 10",
            "",
            "    def __init__(",
            "        self,",
            "        config: t.Dict[str, t.Any],",
            "        extensions: t.Sequence[str] = (\"uuid-ossp\",),",
            "        log_queries: bool = False,",
            "        log_responses: bool = False,",
            "        extra_nodes: t.Mapping[str, PostgresEngine] = None,",
            "    ) -> None:",
            "        if extra_nodes is None:",
            "            extra_nodes = {}",
            "",
            "        self.config = config",
            "        self.extensions = extensions",
            "        self.log_queries = log_queries",
            "        self.log_responses = log_responses",
            "        self.extra_nodes = extra_nodes",
            "        self.pool: t.Optional[Pool] = None",
            "        database_name = config.get(\"database\", \"Unknown\")",
            "        self.current_transaction = contextvars.ContextVar(",
            "            f\"pg_current_transaction_{database_name}\", default=None",
            "        )",
            "        super().__init__()",
            "",
            "    @staticmethod",
            "    def _parse_raw_version_string(version_string: str) -> float:",
            "        \"\"\"",
            "        The format of the version string isn't always consistent. Sometimes",
            "        it's just the version number e.g. '9.6.18', and sometimes",
            "        it contains specific build information e.g.",
            "        '12.4 (Ubuntu 12.4-0ubuntu0.20.04.1)'. Just extract the major and",
            "        minor version numbers.",
            "        \"\"\"",
            "        version_segment = version_string.split(\" \")[0]",
            "        major, minor = version_segment.split(\".\")[:2]",
            "        return float(f\"{major}.{minor}\")",
            "",
            "    async def get_version(self) -> float:",
            "        \"\"\"",
            "        Returns the version of Postgres being run.",
            "        \"\"\"",
            "        try:",
            "            response: t.Sequence[t.Dict] = await self._run_in_new_connection(",
            "                \"SHOW server_version\"",
            "            )",
            "        except ConnectionRefusedError as exception:",
            "            # Suppressing the exception, otherwise importing piccolo_conf.py",
            "            # containing an engine will raise an ImportError.",
            "            colored_warning(f\"Unable to connect to database - {exception}\")",
            "            return 0.0",
            "        else:",
            "            version_string = response[0][\"server_version\"]",
            "            return self._parse_raw_version_string(",
            "                version_string=version_string",
            "            )",
            "",
            "    def get_version_sync(self) -> float:",
            "        return run_sync(self.get_version())",
            "",
            "    async def prep_database(self):",
            "        for extension in self.extensions:",
            "            try:",
            "                await self._run_in_new_connection(",
            "                    f'CREATE EXTENSION IF NOT EXISTS \"{extension}\"',",
            "                )",
            "            except asyncpg.exceptions.InsufficientPrivilegeError:",
            "                colored_warning(",
            "                    f\"=> Unable to create {extension} extension - some \"",
            "                    \"functionality may not behave as expected. Make sure \"",
            "                    \"your database user has permission to create \"",
            "                    \"extensions, or add it manually using \"",
            "                    f'`CREATE EXTENSION \"{extension}\";`',",
            "                    level=Level.medium,",
            "                )",
            "",
            "    ###########################################################################",
            "    # These typos existed in the codebase for a while, so leaving these proxy",
            "    # methods for now to ensure backwards compatibility.",
            "",
            "    async def start_connnection_pool(self, **kwargs) -> None:",
            "        colored_warning(",
            "            \"`start_connnection_pool` is a typo - please change it to \"",
            "            \"`start_connection_pool`.\",",
            "            category=DeprecationWarning,",
            "        )",
            "        return await self.start_connection_pool()",
            "",
            "    async def close_connnection_pool(self, **kwargs) -> None:",
            "        colored_warning(",
            "            \"`close_connnection_pool` is a typo - please change it to \"",
            "            \"`close_connection_pool`.\",",
            "            category=DeprecationWarning,",
            "        )",
            "        return await self.close_connection_pool()",
            "",
            "    ###########################################################################",
            "",
            "    async def start_connection_pool(self, **kwargs) -> None:",
            "        if self.pool:",
            "            colored_warning(",
            "                \"A pool already exists - close it first if you want to create \"",
            "                \"a new pool.\",",
            "            )",
            "        else:",
            "            config = dict(self.config)",
            "            config.update(**kwargs)",
            "            self.pool = await asyncpg.create_pool(**config)",
            "",
            "    async def close_connection_pool(self) -> None:",
            "        if self.pool:",
            "            await self.pool.close()",
            "            self.pool = None",
            "        else:",
            "            colored_warning(\"No pool is running.\")",
            "",
            "    ###########################################################################",
            "",
            "    async def get_new_connection(self) -> Connection:",
            "        \"\"\"",
            "        Returns a new connection - doesn't retrieve it from the pool.",
            "        \"\"\"",
            "        return await asyncpg.connect(**self.config)",
            "",
            "    ###########################################################################",
            "",
            "    async def batch(",
            "        self,",
            "        query: Query,",
            "        batch_size: int = 100,",
            "        node: t.Optional[str] = None,",
            "    ) -> AsyncBatch:",
            "        \"\"\"",
            "        :param query:",
            "            The database query to run.",
            "        :param batch_size:",
            "            How many rows to fetch on each iteration.",
            "        :param node:",
            "            Which node to run the query on (see ``extra_nodes``). If not",
            "            specified, it runs on the main Postgres node.",
            "        \"\"\"",
            "        engine: t.Any = self.extra_nodes.get(node) if node else self",
            "        connection = await engine.get_new_connection()",
            "        return AsyncBatch(",
            "            connection=connection, query=query, batch_size=batch_size",
            "        )",
            "",
            "    ###########################################################################",
            "",
            "    async def _run_in_pool(self, query: str, args: t.Sequence[t.Any] = None):",
            "        if args is None:",
            "            args = []",
            "        if not self.pool:",
            "            raise ValueError(\"A pool isn't currently running.\")",
            "",
            "        async with self.pool.acquire() as connection:",
            "            response = await connection.fetch(query, *args)",
            "",
            "        return response",
            "",
            "    async def _run_in_new_connection(",
            "        self, query: str, args: t.Sequence[t.Any] = None",
            "    ):",
            "        if args is None:",
            "            args = []",
            "        connection = await self.get_new_connection()",
            "",
            "        try:",
            "            results = await connection.fetch(query, *args)",
            "        except asyncpg.exceptions.PostgresError as exception:",
            "            await connection.close()",
            "            raise exception",
            "",
            "        await connection.close()",
            "        return results",
            "",
            "    async def run_querystring(",
            "        self, querystring: QueryString, in_pool: bool = True",
            "    ):",
            "        query, query_args = querystring.compile_string(",
            "            engine_type=self.engine_type",
            "        )",
            "",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=querystring.__str__())",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await current_transaction.connection.fetch(",
            "                query, *query_args",
            "            )",
            "        elif in_pool and self.pool:",
            "            response = await self._run_in_pool(query, query_args)",
            "        else:",
            "            response = await self._run_in_new_connection(query, query_args)",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    async def run_ddl(self, ddl: str, in_pool: bool = True):",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=ddl)",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await current_transaction.connection.fetch(ddl)",
            "        elif in_pool and self.pool:",
            "            response = await self._run_in_pool(ddl)",
            "        else:",
            "            response = await self._run_in_new_connection(ddl)",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    def atomic(self) -> Atomic:",
            "        return Atomic(engine=self)",
            "",
            "    def transaction(self, allow_nested: bool = True) -> PostgresTransaction:",
            "        return PostgresTransaction(engine=self, allow_nested=allow_nested)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "7": []
        },
        "addLocation": []
    },
    "piccolo/engine/sqlite.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from dataclasses import dataclass"
            },
            "1": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from decimal import Decimal"
            },
            "2": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from piccolo.engine.base import Batch, Engine"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 13,
                "PatchRowcode": "+from piccolo.engine.base import Batch, Engine, validate_savepoint_name"
            },
            "5": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from piccolo.engine.exceptions import TransactionError"
            },
            "6": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from piccolo.query.base import DDL, Query"
            },
            "7": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from piccolo.querystring import QueryString"
            },
            "8": {
                "beforePatchRowNumber": 309,
                "afterPatchRowNumber": 309,
                "PatchRowcode": "         self.transaction = transaction"
            },
            "9": {
                "beforePatchRowNumber": 310,
                "afterPatchRowNumber": 310,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 311,
                "afterPatchRowNumber": 311,
                "PatchRowcode": "     async def rollback_to(self):"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 312,
                "PatchRowcode": "+        validate_savepoint_name(self.name)"
            },
            "12": {
                "beforePatchRowNumber": 312,
                "afterPatchRowNumber": 313,
                "PatchRowcode": "         await self.transaction.connection.execute("
            },
            "13": {
                "beforePatchRowNumber": 313,
                "afterPatchRowNumber": 314,
                "PatchRowcode": "             f\"ROLLBACK TO SAVEPOINT {self.name}\""
            },
            "14": {
                "beforePatchRowNumber": 314,
                "afterPatchRowNumber": 315,
                "PatchRowcode": "         )"
            },
            "15": {
                "beforePatchRowNumber": 315,
                "afterPatchRowNumber": 316,
                "PatchRowcode": " "
            },
            "16": {
                "beforePatchRowNumber": 316,
                "afterPatchRowNumber": 317,
                "PatchRowcode": "     async def release(self):"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 318,
                "PatchRowcode": "+        validate_savepoint_name(self.name)"
            },
            "18": {
                "beforePatchRowNumber": 317,
                "afterPatchRowNumber": 319,
                "PatchRowcode": "         await self.transaction.connection.execute("
            },
            "19": {
                "beforePatchRowNumber": 318,
                "afterPatchRowNumber": 320,
                "PatchRowcode": "             f\"RELEASE SAVEPOINT {self.name}\""
            },
            "20": {
                "beforePatchRowNumber": 319,
                "afterPatchRowNumber": 321,
                "PatchRowcode": "         )"
            },
            "21": {
                "beforePatchRowNumber": 413,
                "afterPatchRowNumber": 415,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 414,
                "afterPatchRowNumber": 416,
                "PatchRowcode": "     async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:"
            },
            "23": {
                "beforePatchRowNumber": 415,
                "afterPatchRowNumber": 417,
                "PatchRowcode": "         name = name or f\"savepoint_{self.get_savepoint_id()}\""
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 418,
                "PatchRowcode": "+        validate_savepoint_name(name)"
            },
            "25": {
                "beforePatchRowNumber": 416,
                "afterPatchRowNumber": 419,
                "PatchRowcode": "         await self.connection.execute(f\"SAVEPOINT {name}\")"
            },
            "26": {
                "beforePatchRowNumber": 417,
                "afterPatchRowNumber": 420,
                "PatchRowcode": "         return Savepoint(name=name, transaction=self)"
            },
            "27": {
                "beforePatchRowNumber": 418,
                "afterPatchRowNumber": 421,
                "PatchRowcode": " "
            }
        },
        "frontPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextvars",
            "import datetime",
            "import enum",
            "import os",
            "import sqlite3",
            "import typing as t",
            "import uuid",
            "from dataclasses import dataclass",
            "from decimal import Decimal",
            "",
            "from piccolo.engine.base import Batch, Engine",
            "from piccolo.engine.exceptions import TransactionError",
            "from piccolo.query.base import DDL, Query",
            "from piccolo.querystring import QueryString",
            "from piccolo.utils.encoding import dump_json, load_json",
            "from piccolo.utils.lazy_loader import LazyLoader",
            "from piccolo.utils.sync import run_sync",
            "",
            "aiosqlite = LazyLoader(\"aiosqlite\", globals(), \"aiosqlite\")",
            "",
            "",
            "if t.TYPE_CHECKING:  # pragma: no cover",
            "    from aiosqlite import Connection, Cursor  # type: ignore",
            "",
            "    from piccolo.table import Table",
            "",
            "###############################################################################",
            "",
            "# We need to register some adapters so sqlite returns types which are more",
            "# consistent with the Postgres engine.",
            "",
            "",
            "# In",
            "",
            "",
            "def convert_numeric_in(value):",
            "    \"\"\"",
            "    Convert any Decimal values into floats.",
            "    \"\"\"",
            "    return float(value)",
            "",
            "",
            "def convert_uuid_in(value) -> str:",
            "    \"\"\"",
            "    Converts the UUID value being passed into sqlite.",
            "    \"\"\"",
            "    return str(value)",
            "",
            "",
            "def convert_time_in(value: datetime.time) -> str:",
            "    \"\"\"",
            "    Converts the time value being passed into sqlite.",
            "    \"\"\"",
            "    return value.isoformat()",
            "",
            "",
            "def convert_date_in(value: datetime.date):",
            "    \"\"\"",
            "    Converts the date value being passed into sqlite.",
            "    \"\"\"",
            "    return value.isoformat()",
            "",
            "",
            "def convert_datetime_in(value: datetime.datetime) -> str:",
            "    \"\"\"",
            "    Converts the datetime into a string. If it's timezone aware, we want to",
            "    convert it to UTC first. This is to replicate Postgres, which stores",
            "    timezone aware datetimes in UTC.",
            "    \"\"\"",
            "    if value.tzinfo is not None:",
            "        value = value.astimezone(datetime.timezone.utc)",
            "    return str(value)",
            "",
            "",
            "def convert_timedelta_in(value: datetime.timedelta):",
            "    \"\"\"",
            "    Converts the timedelta value being passed into sqlite.",
            "    \"\"\"",
            "    return value.total_seconds()",
            "",
            "",
            "def convert_array_in(value: list):",
            "    \"\"\"",
            "    Converts a list value into a string.",
            "    \"\"\"",
            "    if value and type(value[0]) not in [str, int, float]:",
            "        raise ValueError(\"Can only serialise str, int and float.\")",
            "",
            "    return dump_json(value)",
            "",
            "",
            "# Out",
            "",
            "",
            "def convert_numeric_out(value: bytes) -> Decimal:",
            "    \"\"\"",
            "    Convert float values into Decimals.",
            "    \"\"\"",
            "    return Decimal(value.decode(\"ascii\"))",
            "",
            "",
            "def convert_int_out(value: bytes) -> int:",
            "    \"\"\"",
            "    Make sure Integer values are actually of type int.",
            "    \"\"\"",
            "    return int(float(value))",
            "",
            "",
            "def convert_uuid_out(value: bytes) -> uuid.UUID:",
            "    \"\"\"",
            "    If the value is a uuid, convert it to a UUID instance.",
            "    \"\"\"",
            "    return uuid.UUID(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_date_out(value: bytes) -> datetime.date:",
            "    return datetime.date.fromisoformat(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_time_out(value: bytes) -> datetime.time:",
            "    \"\"\"",
            "    If the value is a time, convert it to a UUID instance.",
            "    \"\"\"",
            "    return datetime.time.fromisoformat(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_seconds_out(value: bytes) -> datetime.timedelta:",
            "    \"\"\"",
            "    If the value is from a seconds column, convert it to a timedelta instance.",
            "    \"\"\"",
            "    return datetime.timedelta(seconds=float(value.decode(\"utf8\")))",
            "",
            "",
            "def convert_boolean_out(value: bytes) -> bool:",
            "    \"\"\"",
            "    If the value is from a boolean column, convert it to a bool value.",
            "    \"\"\"",
            "    _value = value.decode(\"utf8\")",
            "    return _value == \"1\"",
            "",
            "",
            "def convert_timestamp_out(value: bytes) -> datetime.datetime:",
            "    \"\"\"",
            "    If the value is from a timestamp column, convert it to a datetime value.",
            "    \"\"\"",
            "    return datetime.datetime.fromisoformat(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_timestamptz_out(value: bytes) -> datetime.datetime:",
            "    \"\"\"",
            "    If the value is from a timestamptz column, convert it to a datetime value,",
            "    with a timezone of UTC.",
            "    \"\"\"",
            "    _value = datetime.datetime.fromisoformat(value.decode(\"utf8\"))",
            "    _value = _value.replace(tzinfo=datetime.timezone.utc)",
            "    return _value",
            "",
            "",
            "def convert_array_out(value: bytes) -> t.List:",
            "    \"\"\"",
            "    If the value if from an array column, deserialise the string back into a",
            "    list.",
            "    \"\"\"",
            "    return load_json(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_M2M_out(value: bytes) -> t.List:",
            "    _value = value.decode(\"utf8\")",
            "    return _value.split(\",\")",
            "",
            "",
            "sqlite3.register_converter(\"Numeric\", convert_numeric_out)",
            "sqlite3.register_converter(\"Integer\", convert_int_out)",
            "sqlite3.register_converter(\"UUID\", convert_uuid_out)",
            "sqlite3.register_converter(\"Date\", convert_date_out)",
            "sqlite3.register_converter(\"Time\", convert_time_out)",
            "sqlite3.register_converter(\"Seconds\", convert_seconds_out)",
            "sqlite3.register_converter(\"Boolean\", convert_boolean_out)",
            "sqlite3.register_converter(\"Timestamp\", convert_timestamp_out)",
            "sqlite3.register_converter(\"Timestamptz\", convert_timestamptz_out)",
            "sqlite3.register_converter(\"Array\", convert_array_out)",
            "sqlite3.register_converter(\"M2M\", convert_M2M_out)",
            "",
            "sqlite3.register_adapter(Decimal, convert_numeric_in)",
            "sqlite3.register_adapter(uuid.UUID, convert_uuid_in)",
            "sqlite3.register_adapter(datetime.time, convert_time_in)",
            "sqlite3.register_adapter(datetime.date, convert_date_in)",
            "sqlite3.register_adapter(datetime.datetime, convert_datetime_in)",
            "sqlite3.register_adapter(datetime.timedelta, convert_timedelta_in)",
            "sqlite3.register_adapter(list, convert_array_in)",
            "",
            "###############################################################################",
            "",
            "",
            "@dataclass",
            "class AsyncBatch(Batch):",
            "",
            "    connection: Connection",
            "    query: Query",
            "    batch_size: int",
            "",
            "    # Set internally",
            "    _cursor: t.Optional[Cursor] = None",
            "",
            "    @property",
            "    def cursor(self) -> Cursor:",
            "        if not self._cursor:",
            "            raise ValueError(\"_cursor not set\")",
            "        return self._cursor",
            "",
            "    async def next(self) -> t.List[t.Dict]:",
            "        data = await self.cursor.fetchmany(self.batch_size)",
            "        return await self.query._process_results(data)",
            "",
            "    def __aiter__(self):",
            "        return self",
            "",
            "    async def __anext__(self):",
            "        response = await self.next()",
            "        if response == []:",
            "            raise StopAsyncIteration()",
            "        return response",
            "",
            "    async def __aenter__(self):",
            "        querystring = self.query.querystrings[0]",
            "        template, template_args = querystring.compile_string()",
            "",
            "        self._cursor = await self.connection.execute(template, *template_args)",
            "        return self",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        await self._cursor.close()",
            "        await self.connection.close()",
            "        return exception is not None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class TransactionType(enum.Enum):",
            "    \"\"\"",
            "    See the `SQLite <https://www.sqlite.org/lang_transaction.html>`_ docs for",
            "    more info.",
            "    \"\"\"",
            "",
            "    deferred = \"DEFERRED\"",
            "    immediate = \"IMMEDIATE\"",
            "    exclusive = \"EXCLUSIVE\"",
            "",
            "",
            "class Atomic:",
            "    \"\"\"",
            "    Usage:",
            "",
            "    transaction = engine.atomic()",
            "    transaction.add(Foo.create_table())",
            "",
            "    # Either:",
            "    transaction.run_sync()",
            "    await transaction.run()",
            "    \"\"\"",
            "",
            "    __slots__ = (\"engine\", \"queries\", \"transaction_type\")",
            "",
            "    def __init__(",
            "        self,",
            "        engine: SQLiteEngine,",
            "        transaction_type: TransactionType = TransactionType.deferred,",
            "    ):",
            "        self.engine = engine",
            "        self.transaction_type = transaction_type",
            "        self.queries: t.List[Query] = []",
            "",
            "    def add(self, *query: Query):",
            "        self.queries += list(query)",
            "",
            "    async def run(self):",
            "        from piccolo.query.methods.objects import Create, GetOrCreate",
            "",
            "        try:",
            "            async with self.engine.transaction(",
            "                transaction_type=self.transaction_type",
            "            ):",
            "                for query in self.queries:",
            "                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):",
            "                        await query.run()",
            "                    else:",
            "                        raise ValueError(\"Unrecognised query\")",
            "            self.queries = []",
            "        except Exception as exception:",
            "            self.queries = []",
            "            raise exception from exception",
            "",
            "    def run_sync(self):",
            "        return run_sync(self.run())",
            "",
            "    def __await__(self):",
            "        return self.run().__await__()",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class Savepoint:",
            "    def __init__(self, name: str, transaction: SQLiteTransaction):",
            "        self.name = name",
            "        self.transaction = transaction",
            "",
            "    async def rollback_to(self):",
            "        await self.transaction.connection.execute(",
            "            f\"ROLLBACK TO SAVEPOINT {self.name}\"",
            "        )",
            "",
            "    async def release(self):",
            "        await self.transaction.connection.execute(",
            "            f\"RELEASE SAVEPOINT {self.name}\"",
            "        )",
            "",
            "",
            "class SQLiteTransaction:",
            "    \"\"\"",
            "    Used for wrapping queries in a transaction, using a context manager.",
            "    Currently it's async only.",
            "",
            "    Usage::",
            "",
            "        async with engine.transaction():",
            "            # Run some queries:",
            "            await Band.select().run()",
            "",
            "    \"\"\"",
            "",
            "    __slots__ = (",
            "        \"engine\",",
            "        \"context\",",
            "        \"connection\",",
            "        \"transaction_type\",",
            "        \"allow_nested\",",
            "        \"_savepoint_id\",",
            "        \"_parent\",",
            "        \"_committed\",",
            "        \"_rolled_back\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        engine: SQLiteEngine,",
            "        transaction_type: TransactionType = TransactionType.deferred,",
            "        allow_nested: bool = True,",
            "    ):",
            "        \"\"\"",
            "        :param transaction_type:",
            "            If your transaction just contains ``SELECT`` queries, then use",
            "            ``TransactionType.deferred``. This will give you the best",
            "            performance. When performing ``INSERT``, ``UPDATE``, ``DELETE``",
            "            queries, we recommend using ``TransactionType.immediate`` to",
            "            avoid database locks.",
            "        \"\"\"",
            "        self.engine = engine",
            "        self.transaction_type = transaction_type",
            "        current_transaction = self.engine.current_transaction.get()",
            "",
            "        self._savepoint_id = 0",
            "        self._parent = None",
            "        self._committed = False",
            "        self._rolled_back = False",
            "",
            "        if current_transaction:",
            "            if allow_nested:",
            "                self._parent = current_transaction",
            "            else:",
            "                raise TransactionError(",
            "                    \"A transaction is already active - nested transactions \"",
            "                    \"aren't allowed.\"",
            "                )",
            "",
            "    async def __aenter__(self) -> SQLiteTransaction:",
            "        if self._parent is not None:",
            "            return self._parent",
            "",
            "        self.connection = await self.get_connection()",
            "        await self.begin()",
            "        self.context = self.engine.current_transaction.set(self)",
            "        return self",
            "",
            "    async def get_connection(self):",
            "        return await self.engine.get_connection()",
            "",
            "    async def begin(self):",
            "        await self.connection.execute(f\"BEGIN {self.transaction_type.value}\")",
            "",
            "    async def commit(self):",
            "        await self.connection.execute(\"COMMIT\")",
            "        self._committed = True",
            "",
            "    async def rollback(self):",
            "        await self.connection.execute(\"ROLLBACK\")",
            "        self._rolled_back = True",
            "",
            "    async def rollback_to(self, savepoint_name: str):",
            "        \"\"\"",
            "        Used to rollback to a savepoint just using the name.",
            "        \"\"\"",
            "        await Savepoint(name=savepoint_name, transaction=self).rollback_to()",
            "",
            "    ###########################################################################",
            "",
            "    def get_savepoint_id(self) -> int:",
            "        self._savepoint_id += 1",
            "        return self._savepoint_id",
            "",
            "    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:",
            "        name = name or f\"savepoint_{self.get_savepoint_id()}\"",
            "        await self.connection.execute(f\"SAVEPOINT {name}\")",
            "        return Savepoint(name=name, transaction=self)",
            "",
            "    ###########################################################################",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        if self._parent:",
            "            return exception is None",
            "",
            "        if exception:",
            "            # The user may have manually rolled it back.",
            "            if not self._rolled_back:",
            "                await self.rollback()",
            "        else:",
            "            # The user may have manually committed it.",
            "            if not self._committed and not self._rolled_back:",
            "                await self.commit()",
            "",
            "        await self.connection.close()",
            "        self.engine.current_transaction.reset(self.context)",
            "",
            "        return exception is None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "def dict_factory(cursor, row) -> t.Dict:",
            "    return {col[0]: row[idx] for idx, col in enumerate(cursor.description)}",
            "",
            "",
            "class SQLiteEngine(Engine[t.Optional[SQLiteTransaction]]):",
            "",
            "    __slots__ = (",
            "        \"connection_kwargs\",",
            "        \"current_transaction\",",
            "        \"log_queries\",",
            "        \"log_responses\",",
            "    )",
            "",
            "    engine_type = \"sqlite\"",
            "    min_version_number = 3.25",
            "",
            "    def __init__(",
            "        self,",
            "        path: str = \"piccolo.sqlite\",",
            "        log_queries: bool = False,",
            "        log_responses: bool = False,",
            "        **connection_kwargs,",
            "    ) -> None:",
            "        \"\"\"",
            "        :param path:",
            "            A relative or absolute path to the the SQLite database file (it",
            "            will be created if it doesn't already exist).",
            "        :param log_queries:",
            "            If ``True``, all SQL and DDL statements are printed out before",
            "            being run. Useful for debugging.",
            "        :param log_responses:",
            "            If ``True``, the raw response from each query is printed out.",
            "            Useful for debugging.",
            "        :param connection_kwargs:",
            "            These are passed directly to the database adapter. We recommend",
            "            setting ``timeout`` if you expect your application to process a",
            "            large number of concurrent writes, to prevent queries timing out.",
            "            See Python's `sqlite3 docs <https://docs.python.org/3/library/sqlite3.html#sqlite3.connect>`_",
            "            for more info.",
            "",
            "        \"\"\"  # noqa: E501",
            "        default_connection_kwargs = {",
            "            \"database\": path,",
            "            \"detect_types\": sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,",
            "            \"isolation_level\": None,",
            "        }",
            "",
            "        self.log_queries = log_queries",
            "        self.log_responses = log_responses",
            "        self.connection_kwargs = {",
            "            **default_connection_kwargs,",
            "            **connection_kwargs,",
            "        }",
            "",
            "        self.current_transaction = contextvars.ContextVar(",
            "            f\"sqlite_current_transaction_{path}\", default=None",
            "        )",
            "",
            "        super().__init__()",
            "",
            "    @property",
            "    def path(self):",
            "        return self.connection_kwargs[\"database\"]",
            "",
            "    @path.setter",
            "    def path(self, value: str):",
            "        self.connection_kwargs[\"database\"] = value",
            "",
            "    async def get_version(self) -> float:",
            "        return self.get_version_sync()",
            "",
            "    def get_version_sync(self) -> float:",
            "        major, minor, _ = sqlite3.sqlite_version_info",
            "        return float(f\"{major}.{minor}\")",
            "",
            "    async def prep_database(self):",
            "        pass",
            "",
            "    ###########################################################################",
            "",
            "    def remove_db_file(self):",
            "        \"\"\"",
            "        Use with caution - removes the SQLite file. Useful for testing",
            "        purposes.",
            "        \"\"\"",
            "        if os.path.exists(self.path):",
            "            os.unlink(self.path)",
            "",
            "    def create_db_file(self):",
            "        \"\"\"",
            "        Create the database file. Useful for testing purposes.",
            "        \"\"\"",
            "        if os.path.exists(self.path):",
            "            raise Exception(f\"Database at {self.path} already exists\")",
            "        with open(self.path, \"w\"):",
            "            pass",
            "",
            "    ###########################################################################",
            "",
            "    async def batch(",
            "        self, query: Query, batch_size: int = 100, node: t.Optional[str] = None",
            "    ) -> AsyncBatch:",
            "        \"\"\"",
            "        :param query:",
            "            The database query to run.",
            "        :param batch_size:",
            "            How many rows to fetch on each iteration.",
            "        :param node:",
            "            This is ignored currently, as SQLite runs off a single node. The",
            "            value is here so the API is consistent with Postgres.",
            "        \"\"\"",
            "        connection = await self.get_connection()",
            "        return AsyncBatch(",
            "            connection=connection, query=query, batch_size=batch_size",
            "        )",
            "",
            "    ###########################################################################",
            "",
            "    async def get_connection(self) -> Connection:",
            "        connection = await aiosqlite.connect(**self.connection_kwargs)",
            "        connection.row_factory = dict_factory  # type: ignore",
            "        await connection.execute(\"PRAGMA foreign_keys = 1\")",
            "        return connection",
            "",
            "    ###########################################################################",
            "",
            "    async def _get_inserted_pk(self, cursor, table: t.Type[Table]) -> t.Any:",
            "        \"\"\"",
            "        If the `pk` column is a non-integer then `ROWID` and `pk` will return",
            "        different types. Need to query by `lastrowid` to get `pk`s in SQLite",
            "        prior to 3.35.0.",
            "        \"\"\"",
            "        await cursor.execute(",
            "            f\"SELECT {table._meta.primary_key._meta.db_column_name} FROM \"",
            "            f\"{table._meta.tablename} WHERE ROWID = {cursor.lastrowid}\"",
            "        )",
            "        response = await cursor.fetchone()",
            "        return response[table._meta.primary_key._meta.db_column_name]",
            "",
            "    async def _run_in_new_connection(",
            "        self,",
            "        query: str,",
            "        args: t.List[t.Any] = None,",
            "        query_type: str = \"generic\",",
            "        table: t.Optional[t.Type[Table]] = None,",
            "    ):",
            "        if args is None:",
            "            args = []",
            "        async with aiosqlite.connect(**self.connection_kwargs) as connection:",
            "            await connection.execute(\"PRAGMA foreign_keys = 1\")",
            "",
            "            connection.row_factory = dict_factory  # type: ignore",
            "            async with connection.execute(query, args) as cursor:",
            "                await connection.commit()",
            "",
            "                if query_type == \"insert\" and self.get_version_sync() < 3.35:",
            "                    # We can't use the RETURNING clause on older versions",
            "                    # of SQLite.",
            "                    assert table is not None",
            "                    pk = await self._get_inserted_pk(cursor, table)",
            "                    return [{table._meta.primary_key._meta.db_column_name: pk}]",
            "                else:",
            "                    return await cursor.fetchall()",
            "",
            "    async def _run_in_existing_connection(",
            "        self,",
            "        connection,",
            "        query: str,",
            "        args: t.List[t.Any] = None,",
            "        query_type: str = \"generic\",",
            "        table: t.Optional[t.Type[Table]] = None,",
            "    ):",
            "        \"\"\"",
            "        This is used when a transaction is currently active.",
            "        \"\"\"",
            "        if args is None:",
            "            args = []",
            "        await connection.execute(\"PRAGMA foreign_keys = 1\")",
            "",
            "        connection.row_factory = dict_factory",
            "        async with connection.execute(query, args) as cursor:",
            "            response = await cursor.fetchall()",
            "",
            "            if query_type == \"insert\" and self.get_version_sync() < 3.35:",
            "                # We can't use the RETURNING clause on older versions",
            "                # of SQLite.",
            "                assert table is not None",
            "                pk = await self._get_inserted_pk(cursor, table)",
            "                return [{table._meta.primary_key._meta.db_column_name: pk}]",
            "            else:",
            "                return response",
            "",
            "    async def run_querystring(",
            "        self, querystring: QueryString, in_pool: bool = False",
            "    ):",
            "        \"\"\"",
            "        Connection pools aren't currently supported - the argument is there",
            "        for consistency with other engines.",
            "        \"\"\"",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=querystring.__str__())",
            "",
            "        query, query_args = querystring.compile_string(",
            "            engine_type=self.engine_type",
            "        )",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await self._run_in_existing_connection(",
            "                connection=current_transaction.connection,",
            "                query=query,",
            "                args=query_args,",
            "                query_type=querystring.query_type,",
            "                table=querystring.table,",
            "            )",
            "        else:",
            "            response = await self._run_in_new_connection(",
            "                query=query,",
            "                args=query_args,",
            "                query_type=querystring.query_type,",
            "                table=querystring.table,",
            "            )",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    async def run_ddl(self, ddl: str, in_pool: bool = False):",
            "        \"\"\"",
            "        Connection pools aren't currently supported - the argument is there",
            "        for consistency with other engines.",
            "        \"\"\"",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=ddl)",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await self._run_in_existing_connection(",
            "                connection=current_transaction.connection,",
            "                query=ddl,",
            "            )",
            "        else:",
            "            response = await self._run_in_new_connection(",
            "                query=ddl,",
            "            )",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    def atomic(",
            "        self, transaction_type: TransactionType = TransactionType.deferred",
            "    ) -> Atomic:",
            "        return Atomic(engine=self, transaction_type=transaction_type)",
            "",
            "    def transaction(",
            "        self,",
            "        transaction_type: TransactionType = TransactionType.deferred,",
            "        allow_nested: bool = True,",
            "    ) -> SQLiteTransaction:",
            "        \"\"\"",
            "        Create a new database transaction. See :class:`Transaction`.",
            "        \"\"\"",
            "        return SQLiteTransaction(",
            "            engine=self,",
            "            transaction_type=transaction_type,",
            "            allow_nested=allow_nested,",
            "        )"
        ],
        "afterPatchFile": [
            "from __future__ import annotations",
            "",
            "import contextvars",
            "import datetime",
            "import enum",
            "import os",
            "import sqlite3",
            "import typing as t",
            "import uuid",
            "from dataclasses import dataclass",
            "from decimal import Decimal",
            "",
            "from piccolo.engine.base import Batch, Engine, validate_savepoint_name",
            "from piccolo.engine.exceptions import TransactionError",
            "from piccolo.query.base import DDL, Query",
            "from piccolo.querystring import QueryString",
            "from piccolo.utils.encoding import dump_json, load_json",
            "from piccolo.utils.lazy_loader import LazyLoader",
            "from piccolo.utils.sync import run_sync",
            "",
            "aiosqlite = LazyLoader(\"aiosqlite\", globals(), \"aiosqlite\")",
            "",
            "",
            "if t.TYPE_CHECKING:  # pragma: no cover",
            "    from aiosqlite import Connection, Cursor  # type: ignore",
            "",
            "    from piccolo.table import Table",
            "",
            "###############################################################################",
            "",
            "# We need to register some adapters so sqlite returns types which are more",
            "# consistent with the Postgres engine.",
            "",
            "",
            "# In",
            "",
            "",
            "def convert_numeric_in(value):",
            "    \"\"\"",
            "    Convert any Decimal values into floats.",
            "    \"\"\"",
            "    return float(value)",
            "",
            "",
            "def convert_uuid_in(value) -> str:",
            "    \"\"\"",
            "    Converts the UUID value being passed into sqlite.",
            "    \"\"\"",
            "    return str(value)",
            "",
            "",
            "def convert_time_in(value: datetime.time) -> str:",
            "    \"\"\"",
            "    Converts the time value being passed into sqlite.",
            "    \"\"\"",
            "    return value.isoformat()",
            "",
            "",
            "def convert_date_in(value: datetime.date):",
            "    \"\"\"",
            "    Converts the date value being passed into sqlite.",
            "    \"\"\"",
            "    return value.isoformat()",
            "",
            "",
            "def convert_datetime_in(value: datetime.datetime) -> str:",
            "    \"\"\"",
            "    Converts the datetime into a string. If it's timezone aware, we want to",
            "    convert it to UTC first. This is to replicate Postgres, which stores",
            "    timezone aware datetimes in UTC.",
            "    \"\"\"",
            "    if value.tzinfo is not None:",
            "        value = value.astimezone(datetime.timezone.utc)",
            "    return str(value)",
            "",
            "",
            "def convert_timedelta_in(value: datetime.timedelta):",
            "    \"\"\"",
            "    Converts the timedelta value being passed into sqlite.",
            "    \"\"\"",
            "    return value.total_seconds()",
            "",
            "",
            "def convert_array_in(value: list):",
            "    \"\"\"",
            "    Converts a list value into a string.",
            "    \"\"\"",
            "    if value and type(value[0]) not in [str, int, float]:",
            "        raise ValueError(\"Can only serialise str, int and float.\")",
            "",
            "    return dump_json(value)",
            "",
            "",
            "# Out",
            "",
            "",
            "def convert_numeric_out(value: bytes) -> Decimal:",
            "    \"\"\"",
            "    Convert float values into Decimals.",
            "    \"\"\"",
            "    return Decimal(value.decode(\"ascii\"))",
            "",
            "",
            "def convert_int_out(value: bytes) -> int:",
            "    \"\"\"",
            "    Make sure Integer values are actually of type int.",
            "    \"\"\"",
            "    return int(float(value))",
            "",
            "",
            "def convert_uuid_out(value: bytes) -> uuid.UUID:",
            "    \"\"\"",
            "    If the value is a uuid, convert it to a UUID instance.",
            "    \"\"\"",
            "    return uuid.UUID(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_date_out(value: bytes) -> datetime.date:",
            "    return datetime.date.fromisoformat(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_time_out(value: bytes) -> datetime.time:",
            "    \"\"\"",
            "    If the value is a time, convert it to a UUID instance.",
            "    \"\"\"",
            "    return datetime.time.fromisoformat(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_seconds_out(value: bytes) -> datetime.timedelta:",
            "    \"\"\"",
            "    If the value is from a seconds column, convert it to a timedelta instance.",
            "    \"\"\"",
            "    return datetime.timedelta(seconds=float(value.decode(\"utf8\")))",
            "",
            "",
            "def convert_boolean_out(value: bytes) -> bool:",
            "    \"\"\"",
            "    If the value is from a boolean column, convert it to a bool value.",
            "    \"\"\"",
            "    _value = value.decode(\"utf8\")",
            "    return _value == \"1\"",
            "",
            "",
            "def convert_timestamp_out(value: bytes) -> datetime.datetime:",
            "    \"\"\"",
            "    If the value is from a timestamp column, convert it to a datetime value.",
            "    \"\"\"",
            "    return datetime.datetime.fromisoformat(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_timestamptz_out(value: bytes) -> datetime.datetime:",
            "    \"\"\"",
            "    If the value is from a timestamptz column, convert it to a datetime value,",
            "    with a timezone of UTC.",
            "    \"\"\"",
            "    _value = datetime.datetime.fromisoformat(value.decode(\"utf8\"))",
            "    _value = _value.replace(tzinfo=datetime.timezone.utc)",
            "    return _value",
            "",
            "",
            "def convert_array_out(value: bytes) -> t.List:",
            "    \"\"\"",
            "    If the value if from an array column, deserialise the string back into a",
            "    list.",
            "    \"\"\"",
            "    return load_json(value.decode(\"utf8\"))",
            "",
            "",
            "def convert_M2M_out(value: bytes) -> t.List:",
            "    _value = value.decode(\"utf8\")",
            "    return _value.split(\",\")",
            "",
            "",
            "sqlite3.register_converter(\"Numeric\", convert_numeric_out)",
            "sqlite3.register_converter(\"Integer\", convert_int_out)",
            "sqlite3.register_converter(\"UUID\", convert_uuid_out)",
            "sqlite3.register_converter(\"Date\", convert_date_out)",
            "sqlite3.register_converter(\"Time\", convert_time_out)",
            "sqlite3.register_converter(\"Seconds\", convert_seconds_out)",
            "sqlite3.register_converter(\"Boolean\", convert_boolean_out)",
            "sqlite3.register_converter(\"Timestamp\", convert_timestamp_out)",
            "sqlite3.register_converter(\"Timestamptz\", convert_timestamptz_out)",
            "sqlite3.register_converter(\"Array\", convert_array_out)",
            "sqlite3.register_converter(\"M2M\", convert_M2M_out)",
            "",
            "sqlite3.register_adapter(Decimal, convert_numeric_in)",
            "sqlite3.register_adapter(uuid.UUID, convert_uuid_in)",
            "sqlite3.register_adapter(datetime.time, convert_time_in)",
            "sqlite3.register_adapter(datetime.date, convert_date_in)",
            "sqlite3.register_adapter(datetime.datetime, convert_datetime_in)",
            "sqlite3.register_adapter(datetime.timedelta, convert_timedelta_in)",
            "sqlite3.register_adapter(list, convert_array_in)",
            "",
            "###############################################################################",
            "",
            "",
            "@dataclass",
            "class AsyncBatch(Batch):",
            "",
            "    connection: Connection",
            "    query: Query",
            "    batch_size: int",
            "",
            "    # Set internally",
            "    _cursor: t.Optional[Cursor] = None",
            "",
            "    @property",
            "    def cursor(self) -> Cursor:",
            "        if not self._cursor:",
            "            raise ValueError(\"_cursor not set\")",
            "        return self._cursor",
            "",
            "    async def next(self) -> t.List[t.Dict]:",
            "        data = await self.cursor.fetchmany(self.batch_size)",
            "        return await self.query._process_results(data)",
            "",
            "    def __aiter__(self):",
            "        return self",
            "",
            "    async def __anext__(self):",
            "        response = await self.next()",
            "        if response == []:",
            "            raise StopAsyncIteration()",
            "        return response",
            "",
            "    async def __aenter__(self):",
            "        querystring = self.query.querystrings[0]",
            "        template, template_args = querystring.compile_string()",
            "",
            "        self._cursor = await self.connection.execute(template, *template_args)",
            "        return self",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        await self._cursor.close()",
            "        await self.connection.close()",
            "        return exception is not None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class TransactionType(enum.Enum):",
            "    \"\"\"",
            "    See the `SQLite <https://www.sqlite.org/lang_transaction.html>`_ docs for",
            "    more info.",
            "    \"\"\"",
            "",
            "    deferred = \"DEFERRED\"",
            "    immediate = \"IMMEDIATE\"",
            "    exclusive = \"EXCLUSIVE\"",
            "",
            "",
            "class Atomic:",
            "    \"\"\"",
            "    Usage:",
            "",
            "    transaction = engine.atomic()",
            "    transaction.add(Foo.create_table())",
            "",
            "    # Either:",
            "    transaction.run_sync()",
            "    await transaction.run()",
            "    \"\"\"",
            "",
            "    __slots__ = (\"engine\", \"queries\", \"transaction_type\")",
            "",
            "    def __init__(",
            "        self,",
            "        engine: SQLiteEngine,",
            "        transaction_type: TransactionType = TransactionType.deferred,",
            "    ):",
            "        self.engine = engine",
            "        self.transaction_type = transaction_type",
            "        self.queries: t.List[Query] = []",
            "",
            "    def add(self, *query: Query):",
            "        self.queries += list(query)",
            "",
            "    async def run(self):",
            "        from piccolo.query.methods.objects import Create, GetOrCreate",
            "",
            "        try:",
            "            async with self.engine.transaction(",
            "                transaction_type=self.transaction_type",
            "            ):",
            "                for query in self.queries:",
            "                    if isinstance(query, (Query, DDL, Create, GetOrCreate)):",
            "                        await query.run()",
            "                    else:",
            "                        raise ValueError(\"Unrecognised query\")",
            "            self.queries = []",
            "        except Exception as exception:",
            "            self.queries = []",
            "            raise exception from exception",
            "",
            "    def run_sync(self):",
            "        return run_sync(self.run())",
            "",
            "    def __await__(self):",
            "        return self.run().__await__()",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "class Savepoint:",
            "    def __init__(self, name: str, transaction: SQLiteTransaction):",
            "        self.name = name",
            "        self.transaction = transaction",
            "",
            "    async def rollback_to(self):",
            "        validate_savepoint_name(self.name)",
            "        await self.transaction.connection.execute(",
            "            f\"ROLLBACK TO SAVEPOINT {self.name}\"",
            "        )",
            "",
            "    async def release(self):",
            "        validate_savepoint_name(self.name)",
            "        await self.transaction.connection.execute(",
            "            f\"RELEASE SAVEPOINT {self.name}\"",
            "        )",
            "",
            "",
            "class SQLiteTransaction:",
            "    \"\"\"",
            "    Used for wrapping queries in a transaction, using a context manager.",
            "    Currently it's async only.",
            "",
            "    Usage::",
            "",
            "        async with engine.transaction():",
            "            # Run some queries:",
            "            await Band.select().run()",
            "",
            "    \"\"\"",
            "",
            "    __slots__ = (",
            "        \"engine\",",
            "        \"context\",",
            "        \"connection\",",
            "        \"transaction_type\",",
            "        \"allow_nested\",",
            "        \"_savepoint_id\",",
            "        \"_parent\",",
            "        \"_committed\",",
            "        \"_rolled_back\",",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        engine: SQLiteEngine,",
            "        transaction_type: TransactionType = TransactionType.deferred,",
            "        allow_nested: bool = True,",
            "    ):",
            "        \"\"\"",
            "        :param transaction_type:",
            "            If your transaction just contains ``SELECT`` queries, then use",
            "            ``TransactionType.deferred``. This will give you the best",
            "            performance. When performing ``INSERT``, ``UPDATE``, ``DELETE``",
            "            queries, we recommend using ``TransactionType.immediate`` to",
            "            avoid database locks.",
            "        \"\"\"",
            "        self.engine = engine",
            "        self.transaction_type = transaction_type",
            "        current_transaction = self.engine.current_transaction.get()",
            "",
            "        self._savepoint_id = 0",
            "        self._parent = None",
            "        self._committed = False",
            "        self._rolled_back = False",
            "",
            "        if current_transaction:",
            "            if allow_nested:",
            "                self._parent = current_transaction",
            "            else:",
            "                raise TransactionError(",
            "                    \"A transaction is already active - nested transactions \"",
            "                    \"aren't allowed.\"",
            "                )",
            "",
            "    async def __aenter__(self) -> SQLiteTransaction:",
            "        if self._parent is not None:",
            "            return self._parent",
            "",
            "        self.connection = await self.get_connection()",
            "        await self.begin()",
            "        self.context = self.engine.current_transaction.set(self)",
            "        return self",
            "",
            "    async def get_connection(self):",
            "        return await self.engine.get_connection()",
            "",
            "    async def begin(self):",
            "        await self.connection.execute(f\"BEGIN {self.transaction_type.value}\")",
            "",
            "    async def commit(self):",
            "        await self.connection.execute(\"COMMIT\")",
            "        self._committed = True",
            "",
            "    async def rollback(self):",
            "        await self.connection.execute(\"ROLLBACK\")",
            "        self._rolled_back = True",
            "",
            "    async def rollback_to(self, savepoint_name: str):",
            "        \"\"\"",
            "        Used to rollback to a savepoint just using the name.",
            "        \"\"\"",
            "        await Savepoint(name=savepoint_name, transaction=self).rollback_to()",
            "",
            "    ###########################################################################",
            "",
            "    def get_savepoint_id(self) -> int:",
            "        self._savepoint_id += 1",
            "        return self._savepoint_id",
            "",
            "    async def savepoint(self, name: t.Optional[str] = None) -> Savepoint:",
            "        name = name or f\"savepoint_{self.get_savepoint_id()}\"",
            "        validate_savepoint_name(name)",
            "        await self.connection.execute(f\"SAVEPOINT {name}\")",
            "        return Savepoint(name=name, transaction=self)",
            "",
            "    ###########################################################################",
            "",
            "    async def __aexit__(self, exception_type, exception, traceback):",
            "        if self._parent:",
            "            return exception is None",
            "",
            "        if exception:",
            "            # The user may have manually rolled it back.",
            "            if not self._rolled_back:",
            "                await self.rollback()",
            "        else:",
            "            # The user may have manually committed it.",
            "            if not self._committed and not self._rolled_back:",
            "                await self.commit()",
            "",
            "        await self.connection.close()",
            "        self.engine.current_transaction.reset(self.context)",
            "",
            "        return exception is None",
            "",
            "",
            "###############################################################################",
            "",
            "",
            "def dict_factory(cursor, row) -> t.Dict:",
            "    return {col[0]: row[idx] for idx, col in enumerate(cursor.description)}",
            "",
            "",
            "class SQLiteEngine(Engine[t.Optional[SQLiteTransaction]]):",
            "",
            "    __slots__ = (",
            "        \"connection_kwargs\",",
            "        \"current_transaction\",",
            "        \"log_queries\",",
            "        \"log_responses\",",
            "    )",
            "",
            "    engine_type = \"sqlite\"",
            "    min_version_number = 3.25",
            "",
            "    def __init__(",
            "        self,",
            "        path: str = \"piccolo.sqlite\",",
            "        log_queries: bool = False,",
            "        log_responses: bool = False,",
            "        **connection_kwargs,",
            "    ) -> None:",
            "        \"\"\"",
            "        :param path:",
            "            A relative or absolute path to the the SQLite database file (it",
            "            will be created if it doesn't already exist).",
            "        :param log_queries:",
            "            If ``True``, all SQL and DDL statements are printed out before",
            "            being run. Useful for debugging.",
            "        :param log_responses:",
            "            If ``True``, the raw response from each query is printed out.",
            "            Useful for debugging.",
            "        :param connection_kwargs:",
            "            These are passed directly to the database adapter. We recommend",
            "            setting ``timeout`` if you expect your application to process a",
            "            large number of concurrent writes, to prevent queries timing out.",
            "            See Python's `sqlite3 docs <https://docs.python.org/3/library/sqlite3.html#sqlite3.connect>`_",
            "            for more info.",
            "",
            "        \"\"\"  # noqa: E501",
            "        default_connection_kwargs = {",
            "            \"database\": path,",
            "            \"detect_types\": sqlite3.PARSE_DECLTYPES | sqlite3.PARSE_COLNAMES,",
            "            \"isolation_level\": None,",
            "        }",
            "",
            "        self.log_queries = log_queries",
            "        self.log_responses = log_responses",
            "        self.connection_kwargs = {",
            "            **default_connection_kwargs,",
            "            **connection_kwargs,",
            "        }",
            "",
            "        self.current_transaction = contextvars.ContextVar(",
            "            f\"sqlite_current_transaction_{path}\", default=None",
            "        )",
            "",
            "        super().__init__()",
            "",
            "    @property",
            "    def path(self):",
            "        return self.connection_kwargs[\"database\"]",
            "",
            "    @path.setter",
            "    def path(self, value: str):",
            "        self.connection_kwargs[\"database\"] = value",
            "",
            "    async def get_version(self) -> float:",
            "        return self.get_version_sync()",
            "",
            "    def get_version_sync(self) -> float:",
            "        major, minor, _ = sqlite3.sqlite_version_info",
            "        return float(f\"{major}.{minor}\")",
            "",
            "    async def prep_database(self):",
            "        pass",
            "",
            "    ###########################################################################",
            "",
            "    def remove_db_file(self):",
            "        \"\"\"",
            "        Use with caution - removes the SQLite file. Useful for testing",
            "        purposes.",
            "        \"\"\"",
            "        if os.path.exists(self.path):",
            "            os.unlink(self.path)",
            "",
            "    def create_db_file(self):",
            "        \"\"\"",
            "        Create the database file. Useful for testing purposes.",
            "        \"\"\"",
            "        if os.path.exists(self.path):",
            "            raise Exception(f\"Database at {self.path} already exists\")",
            "        with open(self.path, \"w\"):",
            "            pass",
            "",
            "    ###########################################################################",
            "",
            "    async def batch(",
            "        self, query: Query, batch_size: int = 100, node: t.Optional[str] = None",
            "    ) -> AsyncBatch:",
            "        \"\"\"",
            "        :param query:",
            "            The database query to run.",
            "        :param batch_size:",
            "            How many rows to fetch on each iteration.",
            "        :param node:",
            "            This is ignored currently, as SQLite runs off a single node. The",
            "            value is here so the API is consistent with Postgres.",
            "        \"\"\"",
            "        connection = await self.get_connection()",
            "        return AsyncBatch(",
            "            connection=connection, query=query, batch_size=batch_size",
            "        )",
            "",
            "    ###########################################################################",
            "",
            "    async def get_connection(self) -> Connection:",
            "        connection = await aiosqlite.connect(**self.connection_kwargs)",
            "        connection.row_factory = dict_factory  # type: ignore",
            "        await connection.execute(\"PRAGMA foreign_keys = 1\")",
            "        return connection",
            "",
            "    ###########################################################################",
            "",
            "    async def _get_inserted_pk(self, cursor, table: t.Type[Table]) -> t.Any:",
            "        \"\"\"",
            "        If the `pk` column is a non-integer then `ROWID` and `pk` will return",
            "        different types. Need to query by `lastrowid` to get `pk`s in SQLite",
            "        prior to 3.35.0.",
            "        \"\"\"",
            "        await cursor.execute(",
            "            f\"SELECT {table._meta.primary_key._meta.db_column_name} FROM \"",
            "            f\"{table._meta.tablename} WHERE ROWID = {cursor.lastrowid}\"",
            "        )",
            "        response = await cursor.fetchone()",
            "        return response[table._meta.primary_key._meta.db_column_name]",
            "",
            "    async def _run_in_new_connection(",
            "        self,",
            "        query: str,",
            "        args: t.List[t.Any] = None,",
            "        query_type: str = \"generic\",",
            "        table: t.Optional[t.Type[Table]] = None,",
            "    ):",
            "        if args is None:",
            "            args = []",
            "        async with aiosqlite.connect(**self.connection_kwargs) as connection:",
            "            await connection.execute(\"PRAGMA foreign_keys = 1\")",
            "",
            "            connection.row_factory = dict_factory  # type: ignore",
            "            async with connection.execute(query, args) as cursor:",
            "                await connection.commit()",
            "",
            "                if query_type == \"insert\" and self.get_version_sync() < 3.35:",
            "                    # We can't use the RETURNING clause on older versions",
            "                    # of SQLite.",
            "                    assert table is not None",
            "                    pk = await self._get_inserted_pk(cursor, table)",
            "                    return [{table._meta.primary_key._meta.db_column_name: pk}]",
            "                else:",
            "                    return await cursor.fetchall()",
            "",
            "    async def _run_in_existing_connection(",
            "        self,",
            "        connection,",
            "        query: str,",
            "        args: t.List[t.Any] = None,",
            "        query_type: str = \"generic\",",
            "        table: t.Optional[t.Type[Table]] = None,",
            "    ):",
            "        \"\"\"",
            "        This is used when a transaction is currently active.",
            "        \"\"\"",
            "        if args is None:",
            "            args = []",
            "        await connection.execute(\"PRAGMA foreign_keys = 1\")",
            "",
            "        connection.row_factory = dict_factory",
            "        async with connection.execute(query, args) as cursor:",
            "            response = await cursor.fetchall()",
            "",
            "            if query_type == \"insert\" and self.get_version_sync() < 3.35:",
            "                # We can't use the RETURNING clause on older versions",
            "                # of SQLite.",
            "                assert table is not None",
            "                pk = await self._get_inserted_pk(cursor, table)",
            "                return [{table._meta.primary_key._meta.db_column_name: pk}]",
            "            else:",
            "                return response",
            "",
            "    async def run_querystring(",
            "        self, querystring: QueryString, in_pool: bool = False",
            "    ):",
            "        \"\"\"",
            "        Connection pools aren't currently supported - the argument is there",
            "        for consistency with other engines.",
            "        \"\"\"",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=querystring.__str__())",
            "",
            "        query, query_args = querystring.compile_string(",
            "            engine_type=self.engine_type",
            "        )",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await self._run_in_existing_connection(",
            "                connection=current_transaction.connection,",
            "                query=query,",
            "                args=query_args,",
            "                query_type=querystring.query_type,",
            "                table=querystring.table,",
            "            )",
            "        else:",
            "            response = await self._run_in_new_connection(",
            "                query=query,",
            "                args=query_args,",
            "                query_type=querystring.query_type,",
            "                table=querystring.table,",
            "            )",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    async def run_ddl(self, ddl: str, in_pool: bool = False):",
            "        \"\"\"",
            "        Connection pools aren't currently supported - the argument is there",
            "        for consistency with other engines.",
            "        \"\"\"",
            "        query_id = self.get_query_id()",
            "",
            "        if self.log_queries:",
            "            self.print_query(query_id=query_id, query=ddl)",
            "",
            "        # If running inside a transaction:",
            "        current_transaction = self.current_transaction.get()",
            "        if current_transaction:",
            "            response = await self._run_in_existing_connection(",
            "                connection=current_transaction.connection,",
            "                query=ddl,",
            "            )",
            "        else:",
            "            response = await self._run_in_new_connection(",
            "                query=ddl,",
            "            )",
            "",
            "        if self.log_responses:",
            "            self.print_response(query_id=query_id, response=response)",
            "",
            "        return response",
            "",
            "    def atomic(",
            "        self, transaction_type: TransactionType = TransactionType.deferred",
            "    ) -> Atomic:",
            "        return Atomic(engine=self, transaction_type=transaction_type)",
            "",
            "    def transaction(",
            "        self,",
            "        transaction_type: TransactionType = TransactionType.deferred,",
            "        allow_nested: bool = True,",
            "    ) -> SQLiteTransaction:",
            "        \"\"\"",
            "        Create a new database transaction. See :class:`Transaction`.",
            "        \"\"\"",
            "        return SQLiteTransaction(",
            "            engine=self,",
            "            transaction_type=transaction_type,",
            "            allow_nested=allow_nested,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "13": []
        },
        "addLocation": []
    }
}