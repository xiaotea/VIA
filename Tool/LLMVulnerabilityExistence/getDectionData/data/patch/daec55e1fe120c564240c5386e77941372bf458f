{
    "synapse/federation/federation_server.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 84,
                "PatchRowcode": " from synapse.storage.databases.main.lock import Lock"
            },
            "1": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 85,
                "PatchRowcode": " from synapse.storage.databases.main.roommember import extract_heroes_from_room_summary"
            },
            "2": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 86,
                "PatchRowcode": " from synapse.storage.roommember import MemberSummary"
            },
            "3": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from synapse.types import JsonDict, StateMap, get_domain_from_id"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 87,
                "PatchRowcode": "+from synapse.types import JsonDict, StateMap, get_domain_from_id, UserID"
            },
            "5": {
                "beforePatchRowNumber": 88,
                "afterPatchRowNumber": 88,
                "PatchRowcode": " from synapse.util import unwrapFirstError"
            },
            "6": {
                "beforePatchRowNumber": 89,
                "afterPatchRowNumber": 89,
                "PatchRowcode": " from synapse.util.async_helpers import Linearizer, concurrently_execute, gather_results"
            },
            "7": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 90,
                "PatchRowcode": " from synapse.util.caches.response_cache import ResponseCache"
            },
            "8": {
                "beforePatchRowNumber": 999,
                "afterPatchRowNumber": 999,
                "PatchRowcode": "     async def on_claim_client_keys("
            },
            "9": {
                "beforePatchRowNumber": 1000,
                "afterPatchRowNumber": 1000,
                "PatchRowcode": "         self, query: List[Tuple[str, str, str, int]], always_include_fallback_keys: bool"
            },
            "10": {
                "beforePatchRowNumber": 1001,
                "afterPatchRowNumber": 1001,
                "PatchRowcode": "     ) -> Dict[str, Any]:"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1002,
                "PatchRowcode": "+        if any("
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1003,
                "PatchRowcode": "+            not self.hs.is_mine(UserID.from_string(user_id))"
            },
            "13": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1004,
                "PatchRowcode": "+            for user_id, _, _, _ in query"
            },
            "14": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1005,
                "PatchRowcode": "+        ):"
            },
            "15": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1006,
                "PatchRowcode": "+            raise SynapseError(400, \"User is not hosted on this homeserver\")"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1007,
                "PatchRowcode": "+"
            },
            "17": {
                "beforePatchRowNumber": 1002,
                "afterPatchRowNumber": 1008,
                "PatchRowcode": "         log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})"
            },
            "18": {
                "beforePatchRowNumber": 1003,
                "afterPatchRowNumber": 1009,
                "PatchRowcode": "         results = await self._e2e_keys_handler.claim_local_one_time_keys("
            },
            "19": {
                "beforePatchRowNumber": 1004,
                "afterPatchRowNumber": 1010,
                "PatchRowcode": "             query, always_include_fallback_keys=always_include_fallback_keys"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019-2021 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import (",
            "    Direction,",
            "    EduTypes,",
            "    EventContentFields,",
            "    EventTypes,",
            "    Membership,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_base import (",
            "    FederationBase,",
            "    InvalidEventSignatureError,",
            "    event_from_pdu_json,",
            ")",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.handlers.worker_lock import NEW_EVENT_DURING_PURGE_LOCK_NAME",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    log_kv,",
            "    set_tag,",
            "    start_active_span_from_edu,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import wrap_as_background_process",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.storage.databases.main.lock import Lock",
            "from synapse.storage.databases.main.roommember import extract_heroes_from_room_summary",
            "from synapse.storage.roommember import MemberSummary",
            "from synapse.types import JsonDict, StateMap, get_domain_from_id",
            "from synapse.util import unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute, gather_results",
            "from synapse.util.caches.response_cache import ResponseCache",
            "from synapse.util.stringutils import parse_server_name",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\",",
            "    \"Time taken to process an event\",",
            ")",
            "",
            "last_pdu_ts_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_time\",",
            "    \"The timestamp of the last PDU which was successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "# The name of the lock to use when process events in a room received over",
            "# federation.",
            "_INBOUND_EVENT_HANDLING_LOCK_NAME = \"federation_inbound_pdu\"",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.server_name = hs.hostname",
            "        self.handler = hs.get_federation_handler()",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self.state = hs.get_state_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._room_member_handler = hs.get_room_member_handler()",
            "        self._e2e_keys_handler = hs.get_e2e_keys_handler()",
            "        self._worker_lock_handler = hs.get_worker_locks_handler()",
            "",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "",
            "        # origins that we are currently processing a transaction from.",
            "        # a dict from origin to txn id.",
            "        self._active_transactions: Dict[str, str] = {}",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"fed_txn_handler\", timeout_ms=30000",
            "        )",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache: ResponseCache[",
            "            Tuple[str, Optional[str]]",
            "        ] = ResponseCache(hs.get_clock(), \"state_resp\", timeout_ms=30000)",
            "        self._state_ids_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"state_ids_resp\", timeout_ms=30000",
            "        )",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.config.federation.federation_metrics_domains",
            "        )",
            "",
            "        self._room_prejoin_state_types = hs.config.api.room_prejoin_state",
            "",
            "        # Whether we have started handling old events in the staging area.",
            "        self._started_handling_of_staged_events = False",
            "",
            "    @wrap_as_background_process(\"_handle_old_staged_events\")",
            "    async def _handle_old_staged_events(self) -> None:",
            "        \"\"\"Handle old staged events by fetching all rooms that have staged",
            "        events and start the processing of each of those rooms.",
            "        \"\"\"",
            "",
            "        # Get all the rooms IDs with staged events.",
            "        room_ids = await self.store.get_all_rooms_with_staged_incoming_events()",
            "",
            "        # We then shuffle them so that if there are multiple instances doing",
            "        # this work they're less likely to collide.",
            "        random.shuffle(room_ids)",
            "",
            "        for room_id in room_ids:",
            "            room_version = await self.store.get_room_version(room_id)",
            "",
            "            # Try and acquire the processing lock for the room, if we get it start a",
            "            # background process for handling the events in the room.",
            "            lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if lock:",
            "                logger.info(\"Handling old staged inbound events in %s\", room_id)",
            "                self._process_incoming_pdus_in_room_inner(",
            "                    room_id,",
            "                    room_version,",
            "                    lock,",
            "                )",
            "",
            "            # We pause a bit so that we don't start handling all rooms at once.",
            "            await self._clock.sleep(random.uniform(0, 0.1))",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_dict_from_pdus(pdus)",
            "",
            "        return 200, res",
            "",
            "    async def on_timestamp_to_event_request(",
            "        self, origin: str, room_id: str, timestamp: int, direction: Direction",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"When we receive a federated `/timestamp_to_event` request,",
            "        handle all of the logic for validating and fetching the event.",
            "",
            "        Args:",
            "            origin: The server we received the event from",
            "            room_id: Room to fetch the event from",
            "            timestamp: The point in time (inclusive) we should navigate from in",
            "                the given direction to find the closest event.",
            "            direction: indicates whether we should navigate forward",
            "                or backward from the given timestamp to find the closest event.",
            "",
            "        Returns:",
            "            Tuple indicating the response status code and dictionary response",
            "            body including `event_id`.",
            "        \"\"\"",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            # We only try to fetch data from the local database",
            "            event_id = await self.store.get_event_id_for_timestamp(",
            "                room_id, timestamp, direction",
            "            )",
            "            if event_id:",
            "                event = await self.store.get_event(",
            "                    event_id, allow_none=False, allow_rejected=False",
            "                )",
            "",
            "                return 200, {",
            "                    \"event_id\": event_id,",
            "                    \"origin_server_ts\": event.origin_server_ts,",
            "                }",
            "",
            "        raise SynapseError(",
            "            404,",
            "            \"Unable to find event from %s in direction %s\" % (timestamp, direction),",
            "            errcode=Codes.NOT_FOUND,",
            "        )",
            "",
            "    async def on_incoming_transaction(",
            "        self,",
            "        origin: str,",
            "        transaction_id: str,",
            "        destination: str,",
            "        transaction_data: JsonDict,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # If we receive a transaction we should make sure that kick off handling",
            "        # any old events in the staging area.",
            "        if not self._started_handling_of_staged_events:",
            "            self._started_handling_of_staged_events = True",
            "            self._handle_old_staged_events()",
            "",
            "            # Start a periodic check for old staged events. This is to handle",
            "            # the case where locks time out, e.g. if another process gets killed",
            "            # without dropping its locks.",
            "            self._clock.looping_call(self._handle_old_staged_events, 60 * 1000)",
            "",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(",
            "            transaction_id=transaction_id,",
            "            destination=destination,",
            "            origin=origin,",
            "            origin_server_ts=transaction_data.get(\"origin_server_ts\"),  # type: ignore[arg-type]",
            "            pdus=transaction_data.get(\"pdus\"),",
            "            edus=transaction_data.get(\"edus\"),",
            "        )",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # Reject malformed transactions early: reject if too many PDUs/EDUs",
            "        if len(transaction.pdus) > 50 or len(transaction.edus) > 100:",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "            return 400, {}",
            "",
            "        # we only process one transaction from each origin at a time. We need to do",
            "        # this check here, rather than in _on_incoming_transaction_inner so that we",
            "        # don't cache the rejection in _transaction_resp_cache (so that if the txn",
            "        # arrives again later, we can process it).",
            "        current_transaction = self._active_transactions.get(origin)",
            "        if current_transaction and current_transaction != transaction_id:",
            "            logger.warning(",
            "                \"Received another txn %s from %s while still processing %s\",",
            "                transaction_id,",
            "                origin,",
            "                current_transaction,",
            "            )",
            "            return 429, {",
            "                \"errcode\": Codes.UNKNOWN,",
            "                \"error\": \"Too many concurrent transactions\",",
            "            }",
            "",
            "        # CRITICAL SECTION: we must now not await until we populate _active_transactions",
            "        # in _on_incoming_transaction_inner.",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # CRITICAL SECTION: the first thing we must do (before awaiting) is",
            "        # add an entry to _active_transactions.",
            "        assert origin not in self._active_transactions",
            "        self._active_transactions[origin] = transaction.transaction_id",
            "",
            "        try:",
            "            result = await self._handle_incoming_transaction(",
            "                origin, transaction, request_time",
            "            )",
            "            return result",
            "        finally:",
            "            del self._active_transactions[origin]",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        existing_response = await self.transaction_actions.have_responded(",
            "            origin, transaction",
            "        )",
            "",
            "        if existing_response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,",
            "            )",
            "            return existing_response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            gather_results(",
            "                (",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ),",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room: Dict[str, List[EventBase]] = {}",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str) -> None:",
            "            with nested_logging_context(room_id):",
            "                logger.debug(\"Processing PDUs for %s\", room_id)",
            "",
            "                try:",
            "                    await self.check_server_matches_acl(origin_host, room_id)",
            "                except AuthError as e:",
            "                    logger.warning(",
            "                        \"Ignoring PDUs for room %s from banned server\", room_id",
            "                    )",
            "                    for pdu in pdus_by_room[room_id]:",
            "                        event_id = pdu.event_id",
            "                        pdu_results[event_id] = e.error_dict(self.hs.config)",
            "                    return",
            "",
            "                for pdu in pdus_by_room[room_id]:",
            "                    pdu_results[pdu.event_id] = await process_pdu(pdu)",
            "",
            "        async def process_pdu(pdu: EventBase) -> JsonDict:",
            "            \"\"\"",
            "            Processes a pushed PDU sent to us via a `/send` transaction",
            "",
            "            Returns:",
            "                JsonDict representing a \"PDU Processing Result\" that will be bundled up",
            "                with the other processed PDU's in the `/send` transaction and sent back",
            "                to remote homeserver.",
            "            \"\"\"",
            "            event_id = pdu.event_id",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    await self._handle_received_pdu(origin, pdu)",
            "                    return {}",
            "                except FederationError as e:",
            "                    logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                    return {\"error\": str(e)}",
            "                except Exception as e:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "                    return {\"error\": str(e)}",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            last_pdu_ts_metric.labels(server_name=origin).set(newest_pdu_ts / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction) -> None:",
            "        \"\"\"Process the EDUs in a received transaction.\"\"\"",
            "",
            "        async def _process_edu(edu_dict: JsonDict) -> None:",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            transaction.edus,",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            resp = await self._state_resp_cache.wrap(",
            "                (room_id, event_id),",
            "                self._on_context_state_request_compute,",
            "                room_id,",
            "                event_id,",
            "            )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id),",
            "            self._on_state_ids_request_compute,",
            "            room_id,",
            "            event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _on_state_ids_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> JsonDict:",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(room_id, state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": list(auth_chain_ids)}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        pdus: Collection[EventBase]",
            "        event_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        pdus = await self.store.get_events_as_list(event_ids)",
            "",
            "        auth_chain = await self.store.get_auth_chain(",
            "            room_id, [pdu.event_id for pdu in pdus]",
            "        )",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_dict_from_pdus([pdu])",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        # Refuse the request if that room has seen too many joins recently.",
            "        # This is in addition to the HS-level rate limiting applied by",
            "        # BaseFederationServlet.",
            "        # type-ignore: mypy doesn't seem able to deduce the type of the limiter(!?)",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {pdu.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "        caller_supports_partial_state: bool = False,",
            "    ) -> Dict[str, Any]:",
            "        set_tag(",
            "            SynapseTags.SEND_JOIN_RESPONSE_IS_PARTIAL_STATE,",
            "            caller_supports_partial_state,",
            "        )",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "",
            "        event, context = await self._on_send_membership_event(",
            "            origin, content, Membership.JOIN, room_id",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_event_ids: Collection[str]",
            "        servers_in_room: Optional[Collection[str]]",
            "        if caller_supports_partial_state:",
            "            summary = await self.store.get_room_summary(room_id)",
            "            state_event_ids = _get_event_ids_for_partial_state_join(",
            "                event, prev_state_ids, summary",
            "            )",
            "            servers_in_room = await self.state.get_hosts_in_room_at_events(",
            "                room_id, event_ids=event.prev_event_ids()",
            "            )",
            "        else:",
            "            state_event_ids = prev_state_ids.values()",
            "            servers_in_room = None",
            "",
            "        auth_chain_event_ids = await self.store.get_auth_chain_ids(",
            "            room_id, state_event_ids",
            "        )",
            "",
            "        # if the caller has opted in, we can omit any auth_chain events which are",
            "        # already in state_event_ids",
            "        if caller_supports_partial_state:",
            "            auth_chain_event_ids.difference_update(state_event_ids)",
            "",
            "        auth_chain_events = await self.store.get_events_as_list(auth_chain_event_ids)",
            "        state_events = await self.store.get_events_as_list(state_event_ids)",
            "",
            "        # we try to do all the async stuff before this point, so that time_now is as",
            "        # accurate as possible.",
            "        time_now = self._clock.time_msec()",
            "        event_json = event.get_pdu_json(time_now)",
            "        resp = {",
            "            \"event\": event_json,",
            "            \"state\": [p.get_pdu_json(time_now) for p in state_events],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in auth_chain_events],",
            "            \"members_omitted\": caller_supports_partial_state,",
            "        }",
            "",
            "        if servers_in_room is not None:",
            "            resp[\"servers_in_room\"] = list(servers_in_room)",
            "",
            "        return resp",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(",
            "        self, origin: str, content: JsonDict, room_id: str",
            "    ) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "        await self._on_send_membership_event(origin, content, Membership.LEAVE, room_id)",
            "        return {}",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> JsonDict:",
            "        \"\"\"We've received a /make_knock/ request, so we create a partial knock",
            "        event for the room and hand that back, along with the room version, to the knocking",
            "        homeserver. We do *not* persist or process this event until the other server has",
            "        signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "            supported_versions: The room versions supported by the requesting server.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # Before we do anything: check if the room is partial-stated.",
            "            # Note that at the time this check was added, `on_make_knock_request` would",
            "            # block due to https://github.com/matrix-org/synapse/issues/12997.",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_knock right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        # Check that this room version is supported by the remote homeserver",
            "        if room_version.identifier not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version.identifier, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version.identifier)",
            "",
            "        # Check that this room supports knocking as defined by its room version",
            "        if not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        pdu = await self.handler.on_make_knock_request(origin, room_id, user_id)",
            "        return {",
            "            \"event\": pdu.get_templated_pdu_json(),",
            "            \"room_version\": room_version.identifier,",
            "        }",
            "",
            "    async def on_send_knock_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "    ) -> Dict[str, List[JsonDict]]:",
            "        \"\"\"",
            "        We have received a knock event for a room. Verify and send the event into the room",
            "        on the knocking homeserver's behalf. Then reply with some stripped state from the",
            "        room for the knockee.",
            "",
            "        Args:",
            "            origin: The remote homeserver of the knocking user.",
            "            content: The content of the request.",
            "            room_id: The ID of the room to knock on.",
            "",
            "        Returns:",
            "            The stripped room state.",
            "        \"\"\"",
            "        _, context = await self._on_send_membership_event(",
            "            origin, content, Membership.KNOCK, room_id",
            "        )",
            "",
            "        # Retrieve stripped state events from the room and send them back to the remote",
            "        # server. This will allow the remote server's clients to display information",
            "        # related to the room while the knock request is pending.",
            "        stripped_room_state = (",
            "            await self.store.get_stripped_room_state_from_event_context(",
            "                context, self._room_prejoin_state_types",
            "            )",
            "        )",
            "        return {\"knock_room_state\": stripped_room_state}",
            "",
            "    async def _on_send_membership_event(",
            "        self, origin: str, content: JsonDict, membership_type: str, room_id: str",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Handle an on_send_{join,leave,knock} request",
            "",
            "        Does some preliminary validation before passing the request on to the",
            "        federation handler.",
            "",
            "        Args:",
            "            origin: The (authenticated) requesting server",
            "            content: The body of the send_* request - a complete membership event",
            "            membership_type: The expected membership type (join or leave, depending",
            "                on the endpoint)",
            "            room_id: The room_id from the request, to be validated against the room_id",
            "                in the event",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            SynapseError if there is a problem with the request, including things like",
            "               the room_id not matching or the event not being authorized.",
            "        \"\"\"",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        if content[\"room_id\"] != room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room ID in body does not match that in request path\",",
            "                Codes.BAD_JSON,",
            "            )",
            "",
            "        # Note that get_room_version throws if the room does not exist here.",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /send_join, /send_knock or /send_leave.",
            "            # This is because we will not be able to provide the server list (for partial",
            "            # joins) or the full state (for full joins).",
            "            # Return a 404 as we would if we weren't in the room at all.",
            "            logger.info(",
            "                f\"Rejecting /send_{membership_type} to %s because it's a partial state room\",",
            "                room_id,",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                f\"Unable to handle /send_{membership_type} right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        if membership_type == Membership.KNOCK and not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        event = event_from_pdu_json(content, room_version)",
            "",
            "        if event.type != EventTypes.Member or not event.is_state():",
            "            raise SynapseError(400, \"Not an m.room.member event\", Codes.BAD_JSON)",
            "",
            "        if event.content.get(\"membership\") != membership_type:",
            "            raise SynapseError(400, \"Not a %s event\" % membership_type, Codes.BAD_JSON)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, event.room_id)",
            "",
            "        logger.debug(\"_on_send_membership_event: pdu sigs: %s\", event.signatures)",
            "",
            "        # Sign the event since we're vouching on behalf of the remote server that",
            "        # the event is valid to be sent into the room. Currently this is only done",
            "        # if the user is being joined via restricted join rules.",
            "        if (",
            "            room_version.restricted_join_rule",
            "            and event.membership == Membership.JOIN",
            "            and EventContentFields.AUTHORISING_USER in event.content",
            "        ):",
            "            # We can only authorise our own users.",
            "            authorising_server = get_domain_from_id(",
            "                event.content[EventContentFields.AUTHORISING_USER]",
            "            )",
            "            if not self._is_mine_server_name(authorising_server):",
            "                raise SynapseError(",
            "                    400,",
            "                    f\"Cannot authorise membership event for {authorising_server}. We can only authorise requests from our own homeserver\",",
            "                )",
            "",
            "            event.signatures.update(",
            "                compute_event_signature(",
            "                    room_version,",
            "                    event.get_pdu_json(),",
            "                    self.hs.hostname,",
            "                    self.hs.signing_key,",
            "                )",
            "            )",
            "",
            "        try:",
            "            event = await self._check_sigs_and_hash(room_version, event)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {event.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "",
            "        try:",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were persisting the event.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated during `on_send_membership_event`, trying again.\",",
            "                room_id,",
            "            )",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, query: List[Tuple[str, str, str, int]], always_include_fallback_keys: bool",
            "    ) -> Dict[str, Any]:",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self._e2e_keys_handler.claim_local_one_time_keys(",
            "            query, always_include_fallback_keys=always_include_fallback_keys",
            "        )",
            "",
            "        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}",
            "        for result in results:",
            "            for user_id, device_keys in result.items():",
            "                for device_id, keys in device_keys.items():",
            "                    for key_id, key in keys.items():",
            "                        json_result.setdefault(user_id, {}).setdefault(device_id, {})[",
            "                            key_id",
            "                        ] = key",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_dict_from_pdus(self, pdu_list: List[EventBase]) -> JsonDict:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            # Just need a dummy transaction ID and destination since it won't be used.",
            "            transaction_id=\"\",",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=\"\",",
            "        ).get_dict()",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            logger.warning(\"event id %s: %s\", pdu.event_id, e)",
            "            raise FederationError(\"ERROR\", 403, str(e), affected=pdu.event_id)",
            "",
            "        if await self._spam_checker_module_callbacks.should_drop_federated_event(pdu):",
            "            logger.warning(",
            "                \"Unstaged federated event contains spam, dropping %s\", pdu.event_id",
            "            )",
            "            return",
            "",
            "        # Add the event to our staging area",
            "        await self.store.insert_received_event_to_staging(origin, pdu)",
            "",
            "        # Try and acquire the processing lock for the room, if we get it start a",
            "        # background process for handling the events in the room.",
            "        lock = await self.store.try_acquire_lock(",
            "            _INBOUND_EVENT_HANDLING_LOCK_NAME, pdu.room_id",
            "        )",
            "        if lock:",
            "            self._process_incoming_pdus_in_room_inner(",
            "                pdu.room_id, room_version, lock, origin, pdu",
            "            )",
            "",
            "    async def _get_next_nonspam_staged_event_for_room(",
            "        self, room_id: str, room_version: RoomVersion",
            "    ) -> Optional[Tuple[str, EventBase]]:",
            "        \"\"\"Fetch the first non-spam event from staging queue.",
            "",
            "        Args:",
            "            room_id: the room to fetch the first non-spam event in.",
            "            room_version: the version of the room.",
            "",
            "        Returns:",
            "            The first non-spam event in that room.",
            "        \"\"\"",
            "",
            "        while True:",
            "            # We need to do this check outside the lock to avoid a race between",
            "            # a new event being inserted by another instance and it attempting",
            "            # to acquire the lock.",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if next is None:",
            "                return None",
            "",
            "            origin, event = next",
            "",
            "            if await self._spam_checker_module_callbacks.should_drop_federated_event(",
            "                event",
            "            ):",
            "                logger.warning(",
            "                    \"Staged federated event contains spam, dropping %s\",",
            "                    event.event_id,",
            "                )",
            "                continue",
            "",
            "            return next",
            "",
            "    @wrap_as_background_process(\"_process_incoming_pdus_in_room_inner\")",
            "    async def _process_incoming_pdus_in_room_inner(",
            "        self,",
            "        room_id: str,",
            "        room_version: RoomVersion,",
            "        lock: Lock,",
            "        latest_origin: Optional[str] = None,",
            "        latest_event: Optional[EventBase] = None,",
            "    ) -> None:",
            "        \"\"\"Process events in the staging area for the given room.",
            "",
            "        The latest_origin and latest_event args are the latest origin and event",
            "        received (or None to simply pull the next event from the database).",
            "        \"\"\"",
            "",
            "        # The common path is for the event we just received be the only event in",
            "        # the room, so instead of pulling the event out of the DB and parsing",
            "        # the event we just pull out the next event ID and check if that matches.",
            "        if latest_event is not None and latest_origin is not None:",
            "            result = await self.store.get_next_staged_event_id_for_room(room_id)",
            "            if result is None:",
            "                latest_origin = None",
            "                latest_event = None",
            "            else:",
            "                next_origin, next_event_id = result",
            "                if (",
            "                    next_origin != latest_origin",
            "                    or next_event_id != latest_event.event_id",
            "                ):",
            "                    latest_origin = None",
            "                    latest_event = None",
            "",
            "        if latest_origin is None or latest_event is None:",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "            if not next:",
            "                await lock.release()",
            "                return",
            "",
            "            origin, event = next",
            "        else:",
            "            origin = latest_origin",
            "            event = latest_event",
            "",
            "        # We loop round until there are no more events in the room in the",
            "        # staging area, or we fail to get the lock (which means another process",
            "        # has started processing).",
            "        while True:",
            "            async with lock:",
            "                logger.info(\"handling received PDU in room %s: %s\", room_id, event)",
            "                try:",
            "                    with nested_logging_context(event.event_id):",
            "                        # We're taking out a lock within a lock, which could",
            "                        # lead to deadlocks if we're not careful. However, it is",
            "                        # safe on this occasion as we only ever take a write",
            "                        # lock when deleting a room, which we would never do",
            "                        # while holding the `_INBOUND_EVENT_HANDLING_LOCK_NAME`",
            "                        # lock.",
            "                        async with self._worker_lock_handler.acquire_read_write_lock(",
            "                            NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "                        ):",
            "                            await self._federation_event_handler.on_receive_pdu(",
            "                                origin, event",
            "                            )",
            "                except FederationError as e:",
            "                    # XXX: Ideally we'd inform the remote we failed to process",
            "                    # the event, but we can't return an error in the transaction",
            "                    # response (as we've already responded).",
            "                    logger.warning(\"Error handling PDU %s: %s\", event.event_id, e)",
            "                except Exception:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event.event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "",
            "                received_ts = await self.store.remove_received_event_from_staging(",
            "                    origin, event.event_id",
            "                )",
            "                if received_ts is not None:",
            "                    pdu_process_time.observe(",
            "                        (self._clock.time_msec() - received_ts) / 1000",
            "                    )",
            "",
            "            next = await self._get_next_nonspam_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if not next:",
            "                break",
            "",
            "            origin, event = next",
            "",
            "            # Prune the event queue if it's getting large.",
            "            #",
            "            # We do this *after* handling the first event as the common case is",
            "            # that the queue is empty (/has the single event in), and so there's",
            "            # no need to do this check.",
            "            pruned = await self.store.prune_staged_events_in_room(room_id, room_version)",
            "            if pruned:",
            "                # If we have pruned the queue check we need to refetch the next",
            "                # event to handle.",
            "                next = await self.store.get_next_staged_event_for_room(",
            "                    room_id, room_version",
            "                )",
            "                if not next:",
            "                    break",
            "",
            "                origin, event = next",
            "",
            "            new_lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if not new_lock:",
            "                return",
            "            lock = new_lock",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ) -> None:",
            "        await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict) -> None:",
            "        await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str) -> None:",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        server_acl_evaluator = (",
            "            await self._storage_controllers.state.get_server_acl_for_room(room_id)",
            "        )",
            "        if server_acl_evaluator and not server_acl_evaluator.server_matches_acl_event(",
            "            server_name",
            "        ):",
            "            raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers: Dict[str, Callable[[str, dict], Awaitable[None]]] = {}",
            "        self.query_handlers: Dict[str, Callable[[dict], Awaitable[JsonDict]]] = {}",
            "",
            "        # Map from type to instance names that we should route EDU handling to.",
            "        # We randomly choose one instance from the list to route to for each new",
            "        # EDU received.",
            "        self._edu_type_to_instance: Dict[str, List[str]] = {}",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], Awaitable[JsonDict]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instances_for_edu(",
            "        self, edu_type: str, instance_names: List[str]",
            "    ) -> None:",
            "        \"\"\"Register that the EDU handler is on multiple instances.\"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_names",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict) -> None:",
            "        if not self.config.server.use_presence and edu_type == EduTypes.PRESENCE:",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        instances = self._edu_type_to_instance.get(edu_type, [\"master\"])",
            "        if self._instance_name not in instances:",
            "            # Pick an instance randomly so that we don't overload one.",
            "            route_to = random.choice(instances)",
            "",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict) -> JsonDict:",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))",
            "",
            "",
            "def _get_event_ids_for_partial_state_join(",
            "    join_event: EventBase,",
            "    prev_state_ids: StateMap[str],",
            "    summary: Mapping[str, MemberSummary],",
            ") -> Collection[str]:",
            "    \"\"\"Calculate state to be returned in a partial_state send_join",
            "",
            "    Args:",
            "        join_event: the join event being send_joined",
            "        prev_state_ids: the event ids of the state before the join",
            "",
            "    Returns:",
            "        the event ids to be returned",
            "    \"\"\"",
            "",
            "    # return all non-member events",
            "    state_event_ids = {",
            "        event_id",
            "        for (event_type, state_key), event_id in prev_state_ids.items()",
            "        if event_type != EventTypes.Member",
            "    }",
            "",
            "    # we also need the current state of the current user (it's going to",
            "    # be an auth event for the new join, so we may as well return it)",
            "    current_membership_event_id = prev_state_ids.get(",
            "        (EventTypes.Member, join_event.state_key)",
            "    )",
            "    if current_membership_event_id is not None:",
            "        state_event_ids.add(current_membership_event_id)",
            "",
            "    name_id = prev_state_ids.get((EventTypes.Name, \"\"))",
            "    canonical_alias_id = prev_state_ids.get((EventTypes.CanonicalAlias, \"\"))",
            "    if not name_id and not canonical_alias_id:",
            "        # Also include the hero members of the room (for DM rooms without a title).",
            "        # To do this properly, we should select the correct subset of membership events",
            "        # from `prev_state_ids`. Instead, we are lazier and use the (cached)",
            "        # `get_room_summary` function, which is based on the current state of the room.",
            "        # This introduces races; we choose to ignore them because a) they should be rare",
            "        # and b) even if it's wrong, joining servers will get the full state eventually.",
            "        heroes = extract_heroes_from_room_summary(summary, join_event.state_key)",
            "        for hero in heroes:",
            "            membership_event_id = prev_state_ids.get((EventTypes.Member, hero))",
            "            if membership_event_id:",
            "                state_event_ids.add(membership_event_id)",
            "",
            "    return state_event_ids"
        ],
        "afterPatchFile": [
            "# Copyright 2015, 2016 OpenMarket Ltd",
            "# Copyright 2018 New Vector Ltd",
            "# Copyright 2019-2021 Matrix.org Federation C.I.C",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Awaitable,",
            "    Callable,",
            "    Collection,",
            "    Dict,",
            "    List,",
            "    Mapping,",
            "    Optional,",
            "    Tuple,",
            "    Union,",
            ")",
            "",
            "from prometheus_client import Counter, Gauge, Histogram",
            "",
            "from twisted.python import failure",
            "",
            "from synapse.api.constants import (",
            "    Direction,",
            "    EduTypes,",
            "    EventContentFields,",
            "    EventTypes,",
            "    Membership,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    FederationError,",
            "    IncompatibleRoomVersionError,",
            "    NotFoundError,",
            "    PartialStateConflictError,",
            "    SynapseError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS, RoomVersion",
            "from synapse.crypto.event_signing import compute_event_signature",
            "from synapse.events import EventBase",
            "from synapse.events.snapshot import EventContext",
            "from synapse.federation.federation_base import (",
            "    FederationBase,",
            "    InvalidEventSignatureError,",
            "    event_from_pdu_json,",
            ")",
            "from synapse.federation.persistence import TransactionActions",
            "from synapse.federation.units import Edu, Transaction",
            "from synapse.handlers.worker_lock import NEW_EVENT_DURING_PURGE_LOCK_NAME",
            "from synapse.http.servlet import assert_params_in_dict",
            "from synapse.logging.context import (",
            "    make_deferred_yieldable,",
            "    nested_logging_context,",
            "    run_in_background,",
            ")",
            "from synapse.logging.opentracing import (",
            "    SynapseTags,",
            "    log_kv,",
            "    set_tag,",
            "    start_active_span_from_edu,",
            "    tag_args,",
            "    trace,",
            ")",
            "from synapse.metrics.background_process_metrics import wrap_as_background_process",
            "from synapse.replication.http.federation import (",
            "    ReplicationFederationSendEduRestServlet,",
            "    ReplicationGetQueryRestServlet,",
            ")",
            "from synapse.storage.databases.main.lock import Lock",
            "from synapse.storage.databases.main.roommember import extract_heroes_from_room_summary",
            "from synapse.storage.roommember import MemberSummary",
            "from synapse.types import JsonDict, StateMap, get_domain_from_id, UserID",
            "from synapse.util import unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute, gather_results",
            "from synapse.util.caches.response_cache import ResponseCache",
            "from synapse.util.stringutils import parse_server_name",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "# when processing incoming transactions, we try to handle multiple rooms in",
            "# parallel, up to this limit.",
            "TRANSACTION_CONCURRENCY_LIMIT = 10",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "received_pdus_counter = Counter(\"synapse_federation_server_received_pdus\", \"\")",
            "",
            "received_edus_counter = Counter(\"synapse_federation_server_received_edus\", \"\")",
            "",
            "received_queries_counter = Counter(",
            "    \"synapse_federation_server_received_queries\", \"\", [\"type\"]",
            ")",
            "",
            "pdu_process_time = Histogram(",
            "    \"synapse_federation_server_pdu_process_time\",",
            "    \"Time taken to process an event\",",
            ")",
            "",
            "last_pdu_ts_metric = Gauge(",
            "    \"synapse_federation_last_received_pdu_time\",",
            "    \"The timestamp of the last PDU which was successfully received from the given domain\",",
            "    labelnames=(\"server_name\",),",
            ")",
            "",
            "",
            "# The name of the lock to use when process events in a room received over",
            "# federation.",
            "_INBOUND_EVENT_HANDLING_LOCK_NAME = \"federation_inbound_pdu\"",
            "",
            "",
            "class FederationServer(FederationBase):",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.server_name = hs.hostname",
            "        self.handler = hs.get_federation_handler()",
            "        self._spam_checker_module_callbacks = hs.get_module_api_callbacks().spam_checker",
            "        self._federation_event_handler = hs.get_federation_event_handler()",
            "        self.state = hs.get_state_handler()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self._room_member_handler = hs.get_room_member_handler()",
            "        self._e2e_keys_handler = hs.get_e2e_keys_handler()",
            "        self._worker_lock_handler = hs.get_worker_locks_handler()",
            "",
            "        self._state_storage_controller = hs.get_storage_controllers().state",
            "",
            "        self.device_handler = hs.get_device_handler()",
            "",
            "        # Ensure the following handlers are loaded since they register callbacks",
            "        # with FederationHandlerRegistry.",
            "        hs.get_directory_handler()",
            "",
            "        self._server_linearizer = Linearizer(\"fed_server\")",
            "",
            "        # origins that we are currently processing a transaction from.",
            "        # a dict from origin to txn id.",
            "        self._active_transactions: Dict[str, str] = {}",
            "",
            "        # We cache results for transaction with the same ID",
            "        self._transaction_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"fed_txn_handler\", timeout_ms=30000",
            "        )",
            "",
            "        self.transaction_actions = TransactionActions(self.store)",
            "",
            "        self.registry = hs.get_federation_registry()",
            "",
            "        # We cache responses to state queries, as they take a while and often",
            "        # come in waves.",
            "        self._state_resp_cache: ResponseCache[",
            "            Tuple[str, Optional[str]]",
            "        ] = ResponseCache(hs.get_clock(), \"state_resp\", timeout_ms=30000)",
            "        self._state_ids_resp_cache: ResponseCache[Tuple[str, str]] = ResponseCache(",
            "            hs.get_clock(), \"state_ids_resp\", timeout_ms=30000",
            "        )",
            "",
            "        self._federation_metrics_domains = (",
            "            hs.config.federation.federation_metrics_domains",
            "        )",
            "",
            "        self._room_prejoin_state_types = hs.config.api.room_prejoin_state",
            "",
            "        # Whether we have started handling old events in the staging area.",
            "        self._started_handling_of_staged_events = False",
            "",
            "    @wrap_as_background_process(\"_handle_old_staged_events\")",
            "    async def _handle_old_staged_events(self) -> None:",
            "        \"\"\"Handle old staged events by fetching all rooms that have staged",
            "        events and start the processing of each of those rooms.",
            "        \"\"\"",
            "",
            "        # Get all the rooms IDs with staged events.",
            "        room_ids = await self.store.get_all_rooms_with_staged_incoming_events()",
            "",
            "        # We then shuffle them so that if there are multiple instances doing",
            "        # this work they're less likely to collide.",
            "        random.shuffle(room_ids)",
            "",
            "        for room_id in room_ids:",
            "            room_version = await self.store.get_room_version(room_id)",
            "",
            "            # Try and acquire the processing lock for the room, if we get it start a",
            "            # background process for handling the events in the room.",
            "            lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if lock:",
            "                logger.info(\"Handling old staged inbound events in %s\", room_id)",
            "                self._process_incoming_pdus_in_room_inner(",
            "                    room_id,",
            "                    room_version,",
            "                    lock,",
            "                )",
            "",
            "            # We pause a bit so that we don't start handling all rooms at once.",
            "            await self._clock.sleep(random.uniform(0, 0.1))",
            "",
            "    async def on_backfill_request(",
            "        self, origin: str, room_id: str, versions: List[str], limit: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            pdus = await self.handler.on_backfill_request(",
            "                origin, room_id, versions, limit",
            "            )",
            "",
            "            res = self._transaction_dict_from_pdus(pdus)",
            "",
            "        return 200, res",
            "",
            "    async def on_timestamp_to_event_request(",
            "        self, origin: str, room_id: str, timestamp: int, direction: Direction",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"When we receive a federated `/timestamp_to_event` request,",
            "        handle all of the logic for validating and fetching the event.",
            "",
            "        Args:",
            "            origin: The server we received the event from",
            "            room_id: Room to fetch the event from",
            "            timestamp: The point in time (inclusive) we should navigate from in",
            "                the given direction to find the closest event.",
            "            direction: indicates whether we should navigate forward",
            "                or backward from the given timestamp to find the closest event.",
            "",
            "        Returns:",
            "            Tuple indicating the response status code and dictionary response",
            "            body including `event_id`.",
            "        \"\"\"",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            # We only try to fetch data from the local database",
            "            event_id = await self.store.get_event_id_for_timestamp(",
            "                room_id, timestamp, direction",
            "            )",
            "            if event_id:",
            "                event = await self.store.get_event(",
            "                    event_id, allow_none=False, allow_rejected=False",
            "                )",
            "",
            "                return 200, {",
            "                    \"event_id\": event_id,",
            "                    \"origin_server_ts\": event.origin_server_ts,",
            "                }",
            "",
            "        raise SynapseError(",
            "            404,",
            "            \"Unable to find event from %s in direction %s\" % (timestamp, direction),",
            "            errcode=Codes.NOT_FOUND,",
            "        )",
            "",
            "    async def on_incoming_transaction(",
            "        self,",
            "        origin: str,",
            "        transaction_id: str,",
            "        destination: str,",
            "        transaction_data: JsonDict,",
            "    ) -> Tuple[int, JsonDict]:",
            "        # If we receive a transaction we should make sure that kick off handling",
            "        # any old events in the staging area.",
            "        if not self._started_handling_of_staged_events:",
            "            self._started_handling_of_staged_events = True",
            "            self._handle_old_staged_events()",
            "",
            "            # Start a periodic check for old staged events. This is to handle",
            "            # the case where locks time out, e.g. if another process gets killed",
            "            # without dropping its locks.",
            "            self._clock.looping_call(self._handle_old_staged_events, 60 * 1000)",
            "",
            "        # keep this as early as possible to make the calculated origin ts as",
            "        # accurate as possible.",
            "        request_time = self._clock.time_msec()",
            "",
            "        transaction = Transaction(",
            "            transaction_id=transaction_id,",
            "            destination=destination,",
            "            origin=origin,",
            "            origin_server_ts=transaction_data.get(\"origin_server_ts\"),  # type: ignore[arg-type]",
            "            pdus=transaction_data.get(\"pdus\"),",
            "            edus=transaction_data.get(\"edus\"),",
            "        )",
            "",
            "        if not transaction_id:",
            "            raise Exception(\"Transaction missing transaction_id\")",
            "",
            "        logger.debug(\"[%s] Got transaction\", transaction_id)",
            "",
            "        # Reject malformed transactions early: reject if too many PDUs/EDUs",
            "        if len(transaction.pdus) > 50 or len(transaction.edus) > 100:",
            "            logger.info(\"Transaction PDU or EDU count too large. Returning 400\")",
            "            return 400, {}",
            "",
            "        # we only process one transaction from each origin at a time. We need to do",
            "        # this check here, rather than in _on_incoming_transaction_inner so that we",
            "        # don't cache the rejection in _transaction_resp_cache (so that if the txn",
            "        # arrives again later, we can process it).",
            "        current_transaction = self._active_transactions.get(origin)",
            "        if current_transaction and current_transaction != transaction_id:",
            "            logger.warning(",
            "                \"Received another txn %s from %s while still processing %s\",",
            "                transaction_id,",
            "                origin,",
            "                current_transaction,",
            "            )",
            "            return 429, {",
            "                \"errcode\": Codes.UNKNOWN,",
            "                \"error\": \"Too many concurrent transactions\",",
            "            }",
            "",
            "        # CRITICAL SECTION: we must now not await until we populate _active_transactions",
            "        # in _on_incoming_transaction_inner.",
            "",
            "        # We wrap in a ResponseCache so that we de-duplicate retried",
            "        # transactions.",
            "        return await self._transaction_resp_cache.wrap(",
            "            (origin, transaction_id),",
            "            self._on_incoming_transaction_inner,",
            "            origin,",
            "            transaction,",
            "            request_time,",
            "        )",
            "",
            "    async def _on_incoming_transaction_inner(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        # CRITICAL SECTION: the first thing we must do (before awaiting) is",
            "        # add an entry to _active_transactions.",
            "        assert origin not in self._active_transactions",
            "        self._active_transactions[origin] = transaction.transaction_id",
            "",
            "        try:",
            "            result = await self._handle_incoming_transaction(",
            "                origin, transaction, request_time",
            "            )",
            "            return result",
            "        finally:",
            "            del self._active_transactions[origin]",
            "",
            "    async def _handle_incoming_transaction(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        \"\"\"Process an incoming transaction and return the HTTP response",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            HTTP response code and body",
            "        \"\"\"",
            "        existing_response = await self.transaction_actions.have_responded(",
            "            origin, transaction",
            "        )",
            "",
            "        if existing_response:",
            "            logger.debug(",
            "                \"[%s] We've already responded to this request\",",
            "                transaction.transaction_id,",
            "            )",
            "            return existing_response",
            "",
            "        logger.debug(\"[%s] Transaction is new\", transaction.transaction_id)",
            "",
            "        # We process PDUs and EDUs in parallel. This is important as we don't",
            "        # want to block things like to device messages from reaching clients",
            "        # behind the potentially expensive handling of PDUs.",
            "        pdu_results, _ = await make_deferred_yieldable(",
            "            gather_results(",
            "                (",
            "                    run_in_background(",
            "                        self._handle_pdus_in_txn, origin, transaction, request_time",
            "                    ),",
            "                    run_in_background(self._handle_edus_in_txn, origin, transaction),",
            "                ),",
            "                consumeErrors=True,",
            "            ).addErrback(unwrapFirstError)",
            "        )",
            "",
            "        response = {\"pdus\": pdu_results}",
            "",
            "        logger.debug(\"Returning: %s\", str(response))",
            "",
            "        await self.transaction_actions.set_response(origin, transaction, 200, response)",
            "        return 200, response",
            "",
            "    async def _handle_pdus_in_txn(",
            "        self, origin: str, transaction: Transaction, request_time: int",
            "    ) -> Dict[str, dict]:",
            "        \"\"\"Process the PDUs in a received transaction.",
            "",
            "        Args:",
            "            origin: the server making the request",
            "            transaction: incoming transaction",
            "            request_time: timestamp that the HTTP request arrived at",
            "",
            "        Returns:",
            "            A map from event ID of a processed PDU to any errors we should",
            "            report back to the sending server.",
            "        \"\"\"",
            "",
            "        received_pdus_counter.inc(len(transaction.pdus))",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        pdus_by_room: Dict[str, List[EventBase]] = {}",
            "",
            "        newest_pdu_ts = 0",
            "",
            "        for p in transaction.pdus:",
            "            # FIXME (richardv): I don't think this works:",
            "            #  https://github.com/matrix-org/synapse/issues/8429",
            "            if \"unsigned\" in p:",
            "                unsigned = p[\"unsigned\"]",
            "                if \"age\" in unsigned:",
            "                    p[\"age\"] = unsigned[\"age\"]",
            "            if \"age\" in p:",
            "                p[\"age_ts\"] = request_time - int(p[\"age\"])",
            "                del p[\"age\"]",
            "",
            "            # We try and pull out an event ID so that if later checks fail we",
            "            # can log something sensible. We don't mandate an event ID here in",
            "            # case future event formats get rid of the key.",
            "            possible_event_id = p.get(\"event_id\", \"<Unknown>\")",
            "",
            "            # Now we get the room ID so that we can check that we know the",
            "            # version of the room.",
            "            room_id = p.get(\"room_id\")",
            "            if not room_id:",
            "                logger.info(",
            "                    \"Ignoring PDU as does not have a room_id. Event ID: %s\",",
            "                    possible_event_id,",
            "                )",
            "                continue",
            "",
            "            try:",
            "                room_version = await self.store.get_room_version(room_id)",
            "            except NotFoundError:",
            "                logger.info(\"Ignoring PDU for unknown room_id: %s\", room_id)",
            "                continue",
            "            except UnsupportedRoomVersionError as e:",
            "                # this can happen if support for a given room version is withdrawn,",
            "                # so that we still get events for said room.",
            "                logger.info(\"Ignoring PDU: %s\", e)",
            "                continue",
            "",
            "            event = event_from_pdu_json(p, room_version)",
            "            pdus_by_room.setdefault(room_id, []).append(event)",
            "",
            "            if event.origin_server_ts > newest_pdu_ts:",
            "                newest_pdu_ts = event.origin_server_ts",
            "",
            "        pdu_results = {}",
            "",
            "        # we can process different rooms in parallel (which is useful if they",
            "        # require callouts to other servers to fetch missing events), but",
            "        # impose a limit to avoid going too crazy with ram/cpu.",
            "",
            "        async def process_pdus_for_room(room_id: str) -> None:",
            "            with nested_logging_context(room_id):",
            "                logger.debug(\"Processing PDUs for %s\", room_id)",
            "",
            "                try:",
            "                    await self.check_server_matches_acl(origin_host, room_id)",
            "                except AuthError as e:",
            "                    logger.warning(",
            "                        \"Ignoring PDUs for room %s from banned server\", room_id",
            "                    )",
            "                    for pdu in pdus_by_room[room_id]:",
            "                        event_id = pdu.event_id",
            "                        pdu_results[event_id] = e.error_dict(self.hs.config)",
            "                    return",
            "",
            "                for pdu in pdus_by_room[room_id]:",
            "                    pdu_results[pdu.event_id] = await process_pdu(pdu)",
            "",
            "        async def process_pdu(pdu: EventBase) -> JsonDict:",
            "            \"\"\"",
            "            Processes a pushed PDU sent to us via a `/send` transaction",
            "",
            "            Returns:",
            "                JsonDict representing a \"PDU Processing Result\" that will be bundled up",
            "                with the other processed PDU's in the `/send` transaction and sent back",
            "                to remote homeserver.",
            "            \"\"\"",
            "            event_id = pdu.event_id",
            "            with nested_logging_context(event_id):",
            "                try:",
            "                    await self._handle_received_pdu(origin, pdu)",
            "                    return {}",
            "                except FederationError as e:",
            "                    logger.warning(\"Error handling PDU %s: %s\", event_id, e)",
            "                    return {\"error\": str(e)}",
            "                except Exception as e:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "                    return {\"error\": str(e)}",
            "",
            "        await concurrently_execute(",
            "            process_pdus_for_room, pdus_by_room.keys(), TRANSACTION_CONCURRENCY_LIMIT",
            "        )",
            "",
            "        if newest_pdu_ts and origin in self._federation_metrics_domains:",
            "            last_pdu_ts_metric.labels(server_name=origin).set(newest_pdu_ts / 1000)",
            "",
            "        return pdu_results",
            "",
            "    async def _handle_edus_in_txn(self, origin: str, transaction: Transaction) -> None:",
            "        \"\"\"Process the EDUs in a received transaction.\"\"\"",
            "",
            "        async def _process_edu(edu_dict: JsonDict) -> None:",
            "            received_edus_counter.inc()",
            "",
            "            edu = Edu(",
            "                origin=origin,",
            "                destination=self.server_name,",
            "                edu_type=edu_dict[\"edu_type\"],",
            "                content=edu_dict[\"content\"],",
            "            )",
            "            await self.registry.on_edu(edu.edu_type, origin, edu.content)",
            "",
            "        await concurrently_execute(",
            "            _process_edu,",
            "            transaction.edus,",
            "            TRANSACTION_CONCURRENCY_LIMIT,",
            "        )",
            "",
            "    async def on_room_state_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        # we grab the linearizer to protect ourselves from servers which hammer",
            "        # us. In theory we might already have the response to this query",
            "        # in the cache so we could return it without waiting for the linearizer",
            "        # - but that's non-trivial to get right, and anyway somewhat defeats",
            "        # the point of the linearizer.",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            resp = await self._state_resp_cache.wrap(",
            "                (room_id, event_id),",
            "                self._on_context_state_request_compute,",
            "                room_id,",
            "                event_id,",
            "            )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def on_state_ids_request(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, JsonDict]:",
            "        if not event_id:",
            "            raise NotImplementedError(\"Specify an event\")",
            "",
            "        await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        resp = await self._state_ids_resp_cache.wrap(",
            "            (room_id, event_id),",
            "            self._on_state_ids_request_compute,",
            "            room_id,",
            "            event_id,",
            "        )",
            "",
            "        return 200, resp",
            "",
            "    @trace",
            "    @tag_args",
            "    async def _on_state_ids_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> JsonDict:",
            "        state_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        auth_chain_ids = await self.store.get_auth_chain_ids(room_id, state_ids)",
            "        return {\"pdu_ids\": state_ids, \"auth_chain_ids\": list(auth_chain_ids)}",
            "",
            "    async def _on_context_state_request_compute(",
            "        self, room_id: str, event_id: str",
            "    ) -> Dict[str, list]:",
            "        pdus: Collection[EventBase]",
            "        event_ids = await self.handler.get_state_ids_for_pdu(room_id, event_id)",
            "        pdus = await self.store.get_events_as_list(event_ids)",
            "",
            "        auth_chain = await self.store.get_auth_chain(",
            "            room_id, [pdu.event_id for pdu in pdus]",
            "        )",
            "",
            "        return {",
            "            \"pdus\": [pdu.get_pdu_json() for pdu in pdus],",
            "            \"auth_chain\": [pdu.get_pdu_json() for pdu in auth_chain],",
            "        }",
            "",
            "    async def on_pdu_request(",
            "        self, origin: str, event_id: str",
            "    ) -> Tuple[int, Union[JsonDict, str]]:",
            "        pdu = await self.handler.get_persisted_pdu(origin, event_id)",
            "",
            "        if pdu:",
            "            return 200, self._transaction_dict_from_pdus([pdu])",
            "        else:",
            "            return 404, \"\"",
            "",
            "    async def on_query_request(",
            "        self, query_type: str, args: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        received_queries_counter.labels(query_type).inc()",
            "        resp = await self.registry.on_query(query_type, args)",
            "        return 200, resp",
            "",
            "    async def on_make_join_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "        if room_version not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version)",
            "",
            "        # Refuse the request if that room has seen too many joins recently.",
            "        # This is in addition to the HS-level rate limiting applied by",
            "        # BaseFederationServlet.",
            "        # type-ignore: mypy doesn't seem able to deduce the type of the limiter(!?)",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "        pdu = await self.handler.on_make_join_request(origin, room_id, user_id)",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_invite_request(",
            "        self, origin: str, content: JsonDict, room_version_id: str",
            "    ) -> Dict[str, Any]:",
            "        room_version = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "        if not room_version:",
            "            raise SynapseError(",
            "                400,",
            "                \"Homeserver does not support this room version\",",
            "                Codes.UNSUPPORTED_ROOM_VERSION,",
            "            )",
            "",
            "        pdu = event_from_pdu_json(content, room_version)",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, pdu.room_id)",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {pdu.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "        ret_pdu = await self.handler.on_invite_request(origin, pdu, room_version)",
            "        time_now = self._clock.time_msec()",
            "        return {\"event\": ret_pdu.get_pdu_json(time_now)}",
            "",
            "    async def on_send_join_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "        caller_supports_partial_state: bool = False,",
            "    ) -> Dict[str, Any]:",
            "        set_tag(",
            "            SynapseTags.SEND_JOIN_RESPONSE_IS_PARTIAL_STATE,",
            "            caller_supports_partial_state,",
            "        )",
            "        await self._room_member_handler._join_rate_per_room_limiter.ratelimit(  # type: ignore[has-type]",
            "            requester=None,",
            "            key=room_id,",
            "            update=False,",
            "        )",
            "",
            "        event, context = await self._on_send_membership_event(",
            "            origin, content, Membership.JOIN, room_id",
            "        )",
            "",
            "        prev_state_ids = await context.get_prev_state_ids()",
            "",
            "        state_event_ids: Collection[str]",
            "        servers_in_room: Optional[Collection[str]]",
            "        if caller_supports_partial_state:",
            "            summary = await self.store.get_room_summary(room_id)",
            "            state_event_ids = _get_event_ids_for_partial_state_join(",
            "                event, prev_state_ids, summary",
            "            )",
            "            servers_in_room = await self.state.get_hosts_in_room_at_events(",
            "                room_id, event_ids=event.prev_event_ids()",
            "            )",
            "        else:",
            "            state_event_ids = prev_state_ids.values()",
            "            servers_in_room = None",
            "",
            "        auth_chain_event_ids = await self.store.get_auth_chain_ids(",
            "            room_id, state_event_ids",
            "        )",
            "",
            "        # if the caller has opted in, we can omit any auth_chain events which are",
            "        # already in state_event_ids",
            "        if caller_supports_partial_state:",
            "            auth_chain_event_ids.difference_update(state_event_ids)",
            "",
            "        auth_chain_events = await self.store.get_events_as_list(auth_chain_event_ids)",
            "        state_events = await self.store.get_events_as_list(state_event_ids)",
            "",
            "        # we try to do all the async stuff before this point, so that time_now is as",
            "        # accurate as possible.",
            "        time_now = self._clock.time_msec()",
            "        event_json = event.get_pdu_json(time_now)",
            "        resp = {",
            "            \"event\": event_json,",
            "            \"state\": [p.get_pdu_json(time_now) for p in state_events],",
            "            \"auth_chain\": [p.get_pdu_json(time_now) for p in auth_chain_events],",
            "            \"members_omitted\": caller_supports_partial_state,",
            "        }",
            "",
            "        if servers_in_room is not None:",
            "            resp[\"servers_in_room\"] = list(servers_in_room)",
            "",
            "        return resp",
            "",
            "    async def on_make_leave_request(",
            "        self, origin: str, room_id: str, user_id: str",
            "    ) -> Dict[str, Any]:",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "        pdu = await self.handler.on_make_leave_request(origin, room_id, user_id)",
            "",
            "        room_version = await self.store.get_room_version_id(room_id)",
            "",
            "        return {\"event\": pdu.get_templated_pdu_json(), \"room_version\": room_version}",
            "",
            "    async def on_send_leave_request(",
            "        self, origin: str, content: JsonDict, room_id: str",
            "    ) -> dict:",
            "        logger.debug(\"on_send_leave_request: content: %s\", content)",
            "        await self._on_send_membership_event(origin, content, Membership.LEAVE, room_id)",
            "        return {}",
            "",
            "    async def on_make_knock_request(",
            "        self, origin: str, room_id: str, user_id: str, supported_versions: List[str]",
            "    ) -> JsonDict:",
            "        \"\"\"We've received a /make_knock/ request, so we create a partial knock",
            "        event for the room and hand that back, along with the room version, to the knocking",
            "        homeserver. We do *not* persist or process this event until the other server has",
            "        signed it and sent it back.",
            "",
            "        Args:",
            "            origin: The (verified) server name of the requesting server.",
            "            room_id: The room to create the knock event in.",
            "            user_id: The user to create the knock for.",
            "            supported_versions: The room versions supported by the requesting server.",
            "",
            "        Returns:",
            "            The partial knock event.",
            "        \"\"\"",
            "        origin_host, _ = parse_server_name(origin)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # Before we do anything: check if the room is partial-stated.",
            "            # Note that at the time this check was added, `on_make_knock_request` would",
            "            # block due to https://github.com/matrix-org/synapse/issues/12997.",
            "            raise SynapseError(",
            "                404,",
            "                \"Unable to handle /make_knock right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        # Check that this room version is supported by the remote homeserver",
            "        if room_version.identifier not in supported_versions:",
            "            logger.warning(",
            "                \"Room version %s not in %s\", room_version.identifier, supported_versions",
            "            )",
            "            raise IncompatibleRoomVersionError(room_version=room_version.identifier)",
            "",
            "        # Check that this room supports knocking as defined by its room version",
            "        if not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        pdu = await self.handler.on_make_knock_request(origin, room_id, user_id)",
            "        return {",
            "            \"event\": pdu.get_templated_pdu_json(),",
            "            \"room_version\": room_version.identifier,",
            "        }",
            "",
            "    async def on_send_knock_request(",
            "        self,",
            "        origin: str,",
            "        content: JsonDict,",
            "        room_id: str,",
            "    ) -> Dict[str, List[JsonDict]]:",
            "        \"\"\"",
            "        We have received a knock event for a room. Verify and send the event into the room",
            "        on the knocking homeserver's behalf. Then reply with some stripped state from the",
            "        room for the knockee.",
            "",
            "        Args:",
            "            origin: The remote homeserver of the knocking user.",
            "            content: The content of the request.",
            "            room_id: The ID of the room to knock on.",
            "",
            "        Returns:",
            "            The stripped room state.",
            "        \"\"\"",
            "        _, context = await self._on_send_membership_event(",
            "            origin, content, Membership.KNOCK, room_id",
            "        )",
            "",
            "        # Retrieve stripped state events from the room and send them back to the remote",
            "        # server. This will allow the remote server's clients to display information",
            "        # related to the room while the knock request is pending.",
            "        stripped_room_state = (",
            "            await self.store.get_stripped_room_state_from_event_context(",
            "                context, self._room_prejoin_state_types",
            "            )",
            "        )",
            "        return {\"knock_room_state\": stripped_room_state}",
            "",
            "    async def _on_send_membership_event(",
            "        self, origin: str, content: JsonDict, membership_type: str, room_id: str",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Handle an on_send_{join,leave,knock} request",
            "",
            "        Does some preliminary validation before passing the request on to the",
            "        federation handler.",
            "",
            "        Args:",
            "            origin: The (authenticated) requesting server",
            "            content: The body of the send_* request - a complete membership event",
            "            membership_type: The expected membership type (join or leave, depending",
            "                on the endpoint)",
            "            room_id: The room_id from the request, to be validated against the room_id",
            "                in the event",
            "",
            "        Returns:",
            "            The event and context of the event after inserting it into the room graph.",
            "",
            "        Raises:",
            "            SynapseError if there is a problem with the request, including things like",
            "               the room_id not matching or the event not being authorized.",
            "        \"\"\"",
            "        assert_params_in_dict(content, [\"room_id\"])",
            "        if content[\"room_id\"] != room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room ID in body does not match that in request path\",",
            "                Codes.BAD_JSON,",
            "            )",
            "",
            "        # Note that get_room_version throws if the room does not exist here.",
            "        room_version = await self.store.get_room_version(room_id)",
            "",
            "        if await self.store.is_partial_state_room(room_id):",
            "            # If our server is still only partially joined, we can't give a complete",
            "            # response to /send_join, /send_knock or /send_leave.",
            "            # This is because we will not be able to provide the server list (for partial",
            "            # joins) or the full state (for full joins).",
            "            # Return a 404 as we would if we weren't in the room at all.",
            "            logger.info(",
            "                f\"Rejecting /send_{membership_type} to %s because it's a partial state room\",",
            "                room_id,",
            "            )",
            "            raise SynapseError(",
            "                404,",
            "                f\"Unable to handle /send_{membership_type} right now; this server is not fully joined.\",",
            "                errcode=Codes.NOT_FOUND,",
            "            )",
            "",
            "        if membership_type == Membership.KNOCK and not room_version.knock_join_rule:",
            "            raise SynapseError(",
            "                403,",
            "                \"This room version does not support knocking\",",
            "                errcode=Codes.FORBIDDEN,",
            "            )",
            "",
            "        event = event_from_pdu_json(content, room_version)",
            "",
            "        if event.type != EventTypes.Member or not event.is_state():",
            "            raise SynapseError(400, \"Not an m.room.member event\", Codes.BAD_JSON)",
            "",
            "        if event.content.get(\"membership\") != membership_type:",
            "            raise SynapseError(400, \"Not a %s event\" % membership_type, Codes.BAD_JSON)",
            "",
            "        origin_host, _ = parse_server_name(origin)",
            "        await self.check_server_matches_acl(origin_host, event.room_id)",
            "",
            "        logger.debug(\"_on_send_membership_event: pdu sigs: %s\", event.signatures)",
            "",
            "        # Sign the event since we're vouching on behalf of the remote server that",
            "        # the event is valid to be sent into the room. Currently this is only done",
            "        # if the user is being joined via restricted join rules.",
            "        if (",
            "            room_version.restricted_join_rule",
            "            and event.membership == Membership.JOIN",
            "            and EventContentFields.AUTHORISING_USER in event.content",
            "        ):",
            "            # We can only authorise our own users.",
            "            authorising_server = get_domain_from_id(",
            "                event.content[EventContentFields.AUTHORISING_USER]",
            "            )",
            "            if not self._is_mine_server_name(authorising_server):",
            "                raise SynapseError(",
            "                    400,",
            "                    f\"Cannot authorise membership event for {authorising_server}. We can only authorise requests from our own homeserver\",",
            "                )",
            "",
            "            event.signatures.update(",
            "                compute_event_signature(",
            "                    room_version,",
            "                    event.get_pdu_json(),",
            "                    self.hs.hostname,",
            "                    self.hs.signing_key,",
            "                )",
            "            )",
            "",
            "        try:",
            "            event = await self._check_sigs_and_hash(room_version, event)",
            "        except InvalidEventSignatureError as e:",
            "            errmsg = f\"event id {event.event_id}: {e}\"",
            "            logger.warning(\"%s\", errmsg)",
            "            raise SynapseError(403, errmsg, Codes.FORBIDDEN)",
            "",
            "        try:",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "        except PartialStateConflictError:",
            "            # The room was un-partial stated while we were persisting the event.",
            "            # Try once more, with full state this time.",
            "            logger.info(",
            "                \"Room %s was un-partial stated during `on_send_membership_event`, trying again.\",",
            "                room_id,",
            "            )",
            "            return await self._federation_event_handler.on_send_membership_event(",
            "                origin, event",
            "            )",
            "",
            "    async def on_event_auth(",
            "        self, origin: str, room_id: str, event_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            await self._event_auth_handler.assert_host_in_room(room_id, origin)",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            time_now = self._clock.time_msec()",
            "            auth_pdus = await self.handler.on_event_auth(event_id)",
            "            res = {\"auth_chain\": [a.get_pdu_json(time_now) for a in auth_pdus]}",
            "        return 200, res",
            "",
            "    async def on_query_client_keys(",
            "        self, origin: str, content: Dict[str, str]",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        return await self.on_query_request(\"client_keys\", content)",
            "",
            "    async def on_query_user_devices(",
            "        self, origin: str, user_id: str",
            "    ) -> Tuple[int, Dict[str, Any]]:",
            "        keys = await self.device_handler.on_federation_query_user_devices(user_id)",
            "        return 200, keys",
            "",
            "    @trace",
            "    async def on_claim_client_keys(",
            "        self, query: List[Tuple[str, str, str, int]], always_include_fallback_keys: bool",
            "    ) -> Dict[str, Any]:",
            "        if any(",
            "            not self.hs.is_mine(UserID.from_string(user_id))",
            "            for user_id, _, _, _ in query",
            "        ):",
            "            raise SynapseError(400, \"User is not hosted on this homeserver\")",
            "",
            "        log_kv({\"message\": \"Claiming one time keys.\", \"user, device pairs\": query})",
            "        results = await self._e2e_keys_handler.claim_local_one_time_keys(",
            "            query, always_include_fallback_keys=always_include_fallback_keys",
            "        )",
            "",
            "        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}",
            "        for result in results:",
            "            for user_id, device_keys in result.items():",
            "                for device_id, keys in device_keys.items():",
            "                    for key_id, key in keys.items():",
            "                        json_result.setdefault(user_id, {}).setdefault(device_id, {})[",
            "                            key_id",
            "                        ] = key",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        return {\"one_time_keys\": json_result}",
            "",
            "    async def on_get_missing_events(",
            "        self,",
            "        origin: str,",
            "        room_id: str,",
            "        earliest_events: List[str],",
            "        latest_events: List[str],",
            "        limit: int,",
            "    ) -> Dict[str, list]:",
            "        async with self._server_linearizer.queue((origin, room_id)):",
            "            origin_host, _ = parse_server_name(origin)",
            "            await self.check_server_matches_acl(origin_host, room_id)",
            "",
            "            logger.debug(",
            "                \"on_get_missing_events: earliest_events: %r, latest_events: %r,\"",
            "                \" limit: %d\",",
            "                earliest_events,",
            "                latest_events,",
            "                limit,",
            "            )",
            "",
            "            missing_events = await self.handler.on_get_missing_events(",
            "                origin, room_id, earliest_events, latest_events, limit",
            "            )",
            "",
            "            if len(missing_events) < 5:",
            "                logger.debug(",
            "                    \"Returning %d events: %r\", len(missing_events), missing_events",
            "                )",
            "            else:",
            "                logger.debug(\"Returning %d events\", len(missing_events))",
            "",
            "            time_now = self._clock.time_msec()",
            "",
            "        return {\"events\": [ev.get_pdu_json(time_now) for ev in missing_events]}",
            "",
            "    async def on_openid_userinfo(self, token: str) -> Optional[str]:",
            "        ts_now_ms = self._clock.time_msec()",
            "        return await self.store.get_user_id_for_open_id_token(token, ts_now_ms)",
            "",
            "    def _transaction_dict_from_pdus(self, pdu_list: List[EventBase]) -> JsonDict:",
            "        \"\"\"Returns a new Transaction containing the given PDUs suitable for",
            "        transmission.",
            "        \"\"\"",
            "        time_now = self._clock.time_msec()",
            "        pdus = [p.get_pdu_json(time_now) for p in pdu_list]",
            "        return Transaction(",
            "            # Just need a dummy transaction ID and destination since it won't be used.",
            "            transaction_id=\"\",",
            "            origin=self.server_name,",
            "            pdus=pdus,",
            "            origin_server_ts=int(time_now),",
            "            destination=\"\",",
            "        ).get_dict()",
            "",
            "    async def _handle_received_pdu(self, origin: str, pdu: EventBase) -> None:",
            "        \"\"\"Process a PDU received in a federation /send/ transaction.",
            "",
            "        If the event is invalid, then this method throws a FederationError.",
            "        (The error will then be logged and sent back to the sender (which",
            "        probably won't do anything with it), and other events in the",
            "        transaction will be processed as normal).",
            "",
            "        It is likely that we'll then receive other events which refer to",
            "        this rejected_event in their prev_events, etc.  When that happens,",
            "        we'll attempt to fetch the rejected event again, which will presumably",
            "        fail, so those second-generation events will also get rejected.",
            "",
            "        Eventually, we get to the point where there are more than 10 events",
            "        between any new events and the original rejected event. Since we",
            "        only try to backfill 10 events deep on received pdu, we then accept the",
            "        new event, possibly introducing a discontinuity in the DAG, with new",
            "        forward extremities, so normal service is approximately returned,",
            "        until we try to backfill across the discontinuity.",
            "",
            "        Args:",
            "            origin: server which sent the pdu",
            "            pdu: received pdu",
            "",
            "        Raises: FederationError if the signatures / hash do not match, or",
            "            if the event was unacceptable for any other reason (eg, too large,",
            "            too many prev_events, couldn't find the prev_events)",
            "        \"\"\"",
            "",
            "        # We've already checked that we know the room version by this point",
            "        room_version = await self.store.get_room_version(pdu.room_id)",
            "",
            "        # Check signature.",
            "        try:",
            "            pdu = await self._check_sigs_and_hash(room_version, pdu)",
            "        except InvalidEventSignatureError as e:",
            "            logger.warning(\"event id %s: %s\", pdu.event_id, e)",
            "            raise FederationError(\"ERROR\", 403, str(e), affected=pdu.event_id)",
            "",
            "        if await self._spam_checker_module_callbacks.should_drop_federated_event(pdu):",
            "            logger.warning(",
            "                \"Unstaged federated event contains spam, dropping %s\", pdu.event_id",
            "            )",
            "            return",
            "",
            "        # Add the event to our staging area",
            "        await self.store.insert_received_event_to_staging(origin, pdu)",
            "",
            "        # Try and acquire the processing lock for the room, if we get it start a",
            "        # background process for handling the events in the room.",
            "        lock = await self.store.try_acquire_lock(",
            "            _INBOUND_EVENT_HANDLING_LOCK_NAME, pdu.room_id",
            "        )",
            "        if lock:",
            "            self._process_incoming_pdus_in_room_inner(",
            "                pdu.room_id, room_version, lock, origin, pdu",
            "            )",
            "",
            "    async def _get_next_nonspam_staged_event_for_room(",
            "        self, room_id: str, room_version: RoomVersion",
            "    ) -> Optional[Tuple[str, EventBase]]:",
            "        \"\"\"Fetch the first non-spam event from staging queue.",
            "",
            "        Args:",
            "            room_id: the room to fetch the first non-spam event in.",
            "            room_version: the version of the room.",
            "",
            "        Returns:",
            "            The first non-spam event in that room.",
            "        \"\"\"",
            "",
            "        while True:",
            "            # We need to do this check outside the lock to avoid a race between",
            "            # a new event being inserted by another instance and it attempting",
            "            # to acquire the lock.",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if next is None:",
            "                return None",
            "",
            "            origin, event = next",
            "",
            "            if await self._spam_checker_module_callbacks.should_drop_federated_event(",
            "                event",
            "            ):",
            "                logger.warning(",
            "                    \"Staged federated event contains spam, dropping %s\",",
            "                    event.event_id,",
            "                )",
            "                continue",
            "",
            "            return next",
            "",
            "    @wrap_as_background_process(\"_process_incoming_pdus_in_room_inner\")",
            "    async def _process_incoming_pdus_in_room_inner(",
            "        self,",
            "        room_id: str,",
            "        room_version: RoomVersion,",
            "        lock: Lock,",
            "        latest_origin: Optional[str] = None,",
            "        latest_event: Optional[EventBase] = None,",
            "    ) -> None:",
            "        \"\"\"Process events in the staging area for the given room.",
            "",
            "        The latest_origin and latest_event args are the latest origin and event",
            "        received (or None to simply pull the next event from the database).",
            "        \"\"\"",
            "",
            "        # The common path is for the event we just received be the only event in",
            "        # the room, so instead of pulling the event out of the DB and parsing",
            "        # the event we just pull out the next event ID and check if that matches.",
            "        if latest_event is not None and latest_origin is not None:",
            "            result = await self.store.get_next_staged_event_id_for_room(room_id)",
            "            if result is None:",
            "                latest_origin = None",
            "                latest_event = None",
            "            else:",
            "                next_origin, next_event_id = result",
            "                if (",
            "                    next_origin != latest_origin",
            "                    or next_event_id != latest_event.event_id",
            "                ):",
            "                    latest_origin = None",
            "                    latest_event = None",
            "",
            "        if latest_origin is None or latest_event is None:",
            "            next = await self.store.get_next_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "            if not next:",
            "                await lock.release()",
            "                return",
            "",
            "            origin, event = next",
            "        else:",
            "            origin = latest_origin",
            "            event = latest_event",
            "",
            "        # We loop round until there are no more events in the room in the",
            "        # staging area, or we fail to get the lock (which means another process",
            "        # has started processing).",
            "        while True:",
            "            async with lock:",
            "                logger.info(\"handling received PDU in room %s: %s\", room_id, event)",
            "                try:",
            "                    with nested_logging_context(event.event_id):",
            "                        # We're taking out a lock within a lock, which could",
            "                        # lead to deadlocks if we're not careful. However, it is",
            "                        # safe on this occasion as we only ever take a write",
            "                        # lock when deleting a room, which we would never do",
            "                        # while holding the `_INBOUND_EVENT_HANDLING_LOCK_NAME`",
            "                        # lock.",
            "                        async with self._worker_lock_handler.acquire_read_write_lock(",
            "                            NEW_EVENT_DURING_PURGE_LOCK_NAME, room_id, write=False",
            "                        ):",
            "                            await self._federation_event_handler.on_receive_pdu(",
            "                                origin, event",
            "                            )",
            "                except FederationError as e:",
            "                    # XXX: Ideally we'd inform the remote we failed to process",
            "                    # the event, but we can't return an error in the transaction",
            "                    # response (as we've already responded).",
            "                    logger.warning(\"Error handling PDU %s: %s\", event.event_id, e)",
            "                except Exception:",
            "                    f = failure.Failure()",
            "                    logger.error(",
            "                        \"Failed to handle PDU %s\",",
            "                        event.event_id,",
            "                        exc_info=(f.type, f.value, f.getTracebackObject()),",
            "                    )",
            "",
            "                received_ts = await self.store.remove_received_event_from_staging(",
            "                    origin, event.event_id",
            "                )",
            "                if received_ts is not None:",
            "                    pdu_process_time.observe(",
            "                        (self._clock.time_msec() - received_ts) / 1000",
            "                    )",
            "",
            "            next = await self._get_next_nonspam_staged_event_for_room(",
            "                room_id, room_version",
            "            )",
            "",
            "            if not next:",
            "                break",
            "",
            "            origin, event = next",
            "",
            "            # Prune the event queue if it's getting large.",
            "            #",
            "            # We do this *after* handling the first event as the common case is",
            "            # that the queue is empty (/has the single event in), and so there's",
            "            # no need to do this check.",
            "            pruned = await self.store.prune_staged_events_in_room(room_id, room_version)",
            "            if pruned:",
            "                # If we have pruned the queue check we need to refetch the next",
            "                # event to handle.",
            "                next = await self.store.get_next_staged_event_for_room(",
            "                    room_id, room_version",
            "                )",
            "                if not next:",
            "                    break",
            "",
            "                origin, event = next",
            "",
            "            new_lock = await self.store.try_acquire_lock(",
            "                _INBOUND_EVENT_HANDLING_LOCK_NAME, room_id",
            "            )",
            "            if not new_lock:",
            "                return",
            "            lock = new_lock",
            "",
            "    async def exchange_third_party_invite(",
            "        self, sender_user_id: str, target_user_id: str, room_id: str, signed: Dict",
            "    ) -> None:",
            "        await self.handler.exchange_third_party_invite(",
            "            sender_user_id, target_user_id, room_id, signed",
            "        )",
            "",
            "    async def on_exchange_third_party_invite_request(self, event_dict: Dict) -> None:",
            "        await self.handler.on_exchange_third_party_invite_request(event_dict)",
            "",
            "    async def check_server_matches_acl(self, server_name: str, room_id: str) -> None:",
            "        \"\"\"Check if the given server is allowed by the server ACLs in the room",
            "",
            "        Args:",
            "            server_name: name of server, *without any port part*",
            "            room_id: ID of the room to check",
            "",
            "        Raises:",
            "            AuthError if the server does not match the ACL",
            "        \"\"\"",
            "        server_acl_evaluator = (",
            "            await self._storage_controllers.state.get_server_acl_for_room(room_id)",
            "        )",
            "        if server_acl_evaluator and not server_acl_evaluator.server_matches_acl_event(",
            "            server_name",
            "        ):",
            "            raise AuthError(code=403, msg=\"Server is banned from room\")",
            "",
            "",
            "class FederationHandlerRegistry:",
            "    \"\"\"Allows classes to register themselves as handlers for a given EDU or",
            "    query type for incoming federation traffic.",
            "    \"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.clock = hs.get_clock()",
            "        self._instance_name = hs.get_instance_name()",
            "",
            "        # These are safe to load in monolith mode, but will explode if we try",
            "        # and use them. However we have guards before we use them to ensure that",
            "        # we don't route to ourselves, and in monolith mode that will always be",
            "        # the case.",
            "        self._get_query_client = ReplicationGetQueryRestServlet.make_client(hs)",
            "        self._send_edu = ReplicationFederationSendEduRestServlet.make_client(hs)",
            "",
            "        self.edu_handlers: Dict[str, Callable[[str, dict], Awaitable[None]]] = {}",
            "        self.query_handlers: Dict[str, Callable[[dict], Awaitable[JsonDict]]] = {}",
            "",
            "        # Map from type to instance names that we should route EDU handling to.",
            "        # We randomly choose one instance from the list to route to for each new",
            "        # EDU received.",
            "        self._edu_type_to_instance: Dict[str, List[str]] = {}",
            "",
            "    def register_edu_handler(",
            "        self, edu_type: str, handler: Callable[[str, JsonDict], Awaitable[None]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation EDU of the given type.",
            "",
            "        Args:",
            "            edu_type: The type of the incoming EDU to register handler for",
            "            handler: A callable invoked on incoming EDU",
            "                of the given type. The arguments are the origin server name and",
            "                the EDU contents.",
            "        \"\"\"",
            "        if edu_type in self.edu_handlers:",
            "            raise KeyError(\"Already have an EDU handler for %s\" % (edu_type,))",
            "",
            "        logger.info(\"Registering federation EDU handler for %r\", edu_type)",
            "",
            "        self.edu_handlers[edu_type] = handler",
            "",
            "    def register_query_handler(",
            "        self, query_type: str, handler: Callable[[dict], Awaitable[JsonDict]]",
            "    ) -> None:",
            "        \"\"\"Sets the handler callable that will be used to handle an incoming",
            "        federation query of the given type.",
            "",
            "        Args:",
            "            query_type: Category name of the query, which should match",
            "                the string used by make_query.",
            "            handler: Invoked to handle",
            "                incoming queries of this type. The return will be yielded",
            "                on and the result used as the response to the query request.",
            "        \"\"\"",
            "        if query_type in self.query_handlers:",
            "            raise KeyError(\"Already have a Query handler for %s\" % (query_type,))",
            "",
            "        logger.info(\"Registering federation query handler for %r\", query_type)",
            "",
            "        self.query_handlers[query_type] = handler",
            "",
            "    def register_instances_for_edu(",
            "        self, edu_type: str, instance_names: List[str]",
            "    ) -> None:",
            "        \"\"\"Register that the EDU handler is on multiple instances.\"\"\"",
            "        self._edu_type_to_instance[edu_type] = instance_names",
            "",
            "    async def on_edu(self, edu_type: str, origin: str, content: dict) -> None:",
            "        if not self.config.server.use_presence and edu_type == EduTypes.PRESENCE:",
            "            return",
            "",
            "        # Check if we have a handler on this instance",
            "        handler = self.edu_handlers.get(edu_type)",
            "        if handler:",
            "            with start_active_span_from_edu(content, \"handle_edu\"):",
            "                try:",
            "                    await handler(origin, content)",
            "                except SynapseError as e:",
            "                    logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "                except Exception:",
            "                    logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        instances = self._edu_type_to_instance.get(edu_type, [\"master\"])",
            "        if self._instance_name not in instances:",
            "            # Pick an instance randomly so that we don't overload one.",
            "            route_to = random.choice(instances)",
            "",
            "            try:",
            "                await self._send_edu(",
            "                    instance_name=route_to,",
            "                    edu_type=edu_type,",
            "                    origin=origin,",
            "                    content=content,",
            "                )",
            "            except SynapseError as e:",
            "                logger.info(\"Failed to handle edu %r: %r\", edu_type, e)",
            "            except Exception:",
            "                logger.exception(\"Failed to handle edu %r\", edu_type)",
            "            return",
            "",
            "        # Oh well, let's just log and move on.",
            "        logger.warning(\"No handler registered for EDU type %s\", edu_type)",
            "",
            "    async def on_query(self, query_type: str, args: dict) -> JsonDict:",
            "        handler = self.query_handlers.get(query_type)",
            "        if handler:",
            "            return await handler(args)",
            "",
            "        # Check if we can route it somewhere else that isn't us",
            "        if self._instance_name == \"master\":",
            "            return await self._get_query_client(query_type=query_type, args=args)",
            "",
            "        # Uh oh, no handler! Let's raise an exception so the request returns an",
            "        # error.",
            "        logger.warning(\"No handler registered for query type %s\", query_type)",
            "        raise NotFoundError(\"No handler for Query type '%s'\" % (query_type,))",
            "",
            "",
            "def _get_event_ids_for_partial_state_join(",
            "    join_event: EventBase,",
            "    prev_state_ids: StateMap[str],",
            "    summary: Mapping[str, MemberSummary],",
            ") -> Collection[str]:",
            "    \"\"\"Calculate state to be returned in a partial_state send_join",
            "",
            "    Args:",
            "        join_event: the join event being send_joined",
            "        prev_state_ids: the event ids of the state before the join",
            "",
            "    Returns:",
            "        the event ids to be returned",
            "    \"\"\"",
            "",
            "    # return all non-member events",
            "    state_event_ids = {",
            "        event_id",
            "        for (event_type, state_key), event_id in prev_state_ids.items()",
            "        if event_type != EventTypes.Member",
            "    }",
            "",
            "    # we also need the current state of the current user (it's going to",
            "    # be an auth event for the new join, so we may as well return it)",
            "    current_membership_event_id = prev_state_ids.get(",
            "        (EventTypes.Member, join_event.state_key)",
            "    )",
            "    if current_membership_event_id is not None:",
            "        state_event_ids.add(current_membership_event_id)",
            "",
            "    name_id = prev_state_ids.get((EventTypes.Name, \"\"))",
            "    canonical_alias_id = prev_state_ids.get((EventTypes.CanonicalAlias, \"\"))",
            "    if not name_id and not canonical_alias_id:",
            "        # Also include the hero members of the room (for DM rooms without a title).",
            "        # To do this properly, we should select the correct subset of membership events",
            "        # from `prev_state_ids`. Instead, we are lazier and use the (cached)",
            "        # `get_room_summary` function, which is based on the current state of the room.",
            "        # This introduces races; we choose to ignore them because a) they should be rare",
            "        # and b) even if it's wrong, joining servers will get the full state eventually.",
            "        heroes = extract_heroes_from_room_summary(summary, join_event.state_key)",
            "        for hero in heroes:",
            "            membership_event_id = prev_state_ids.get((EventTypes.Member, hero))",
            "            if membership_event_id:",
            "                state_event_ids.add(membership_event_id)",
            "",
            "    return state_event_ids"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "87": []
        },
        "addLocation": []
    },
    "synapse/handlers/device.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 328,
                "afterPatchRowNumber": 328,
                "PatchRowcode": "         return result"
            },
            "1": {
                "beforePatchRowNumber": 329,
                "afterPatchRowNumber": 329,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 330,
                "afterPatchRowNumber": 330,
                "PatchRowcode": "     async def on_federation_query_user_devices(self, user_id: str) -> JsonDict:"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 331,
                "PatchRowcode": "+        if not self.hs.is_mine(UserID.from_string(user_id)):"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 332,
                "PatchRowcode": "+            raise SynapseError(400, \"User is not hosted on this homeserver\")"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 333,
                "PatchRowcode": "+"
            },
            "6": {
                "beforePatchRowNumber": 331,
                "afterPatchRowNumber": 334,
                "PatchRowcode": "         stream_id, devices = await self.store.get_e2e_device_keys_for_federation_query("
            },
            "7": {
                "beforePatchRowNumber": 332,
                "afterPatchRowNumber": 335,
                "PatchRowcode": "             user_id"
            },
            "8": {
                "beforePatchRowNumber": 333,
                "afterPatchRowNumber": 336,
                "PatchRowcode": "         )"
            }
        },
        "frontPatchFile": [
            "# Copyright 2016 OpenMarket Ltd",
            "# Copyright 2019 New Vector Ltd",
            "# Copyright 2019,2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Mapping, Optional, Set, Tuple",
            "",
            "from synapse.api import errors",
            "from synapse.api.constants import EduTypes, EventTypes",
            "from synapse.api.errors import (",
            "    Codes,",
            "    FederationDeniedError,",
            "    HttpResponseException,",
            "    InvalidAPICallError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.logging.opentracing import log_kv, set_tag, trace",
            "from synapse.metrics.background_process_metrics import (",
            "    run_as_background_process,",
            "    wrap_as_background_process,",
            ")",
            "from synapse.storage.databases.main.client_ips import DeviceLastConnectionInfo",
            "from synapse.types import (",
            "    JsonDict,",
            "    JsonMapping,",
            "    ScheduledTask,",
            "    StrCollection,",
            "    StreamKeyType,",
            "    StreamToken,",
            "    TaskStatus,",
            "    UserID,",
            "    get_domain_from_id,",
            "    get_verify_key_from_cross_signing_key,",
            ")",
            "from synapse.util import stringutils",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.caches.expiringcache import ExpiringCache",
            "from synapse.util.cancellation import cancellable",
            "from synapse.util.metrics import measure_func",
            "from synapse.util.retryutils import (",
            "    NotRetryingDestination,",
            "    filter_destinations_by_retry_limiter,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "DELETE_DEVICE_MSGS_TASK_NAME = \"delete_device_messages\"",
            "MAX_DEVICE_DISPLAY_NAME_LEN = 100",
            "DELETE_STALE_DEVICES_INTERVAL_MS = 24 * 60 * 60 * 1000",
            "",
            "",
            "class DeviceWorkerHandler:",
            "    device_list_updater: \"DeviceListWorkerUpdater\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.clock = hs.get_clock()",
            "        self.hs = hs",
            "        self.store = hs.get_datastores().main",
            "        self.notifier = hs.get_notifier()",
            "        self.state = hs.get_state_handler()",
            "        self._appservice_handler = hs.get_application_service_handler()",
            "        self._state_storage = hs.get_storage_controllers().state",
            "        self._auth_handler = hs.get_auth_handler()",
            "        self._event_sources = hs.get_event_sources()",
            "        self.server_name = hs.hostname",
            "        self._msc3852_enabled = hs.config.experimental.msc3852_enabled",
            "        self._query_appservices_for_keys = (",
            "            hs.config.experimental.msc3984_appservice_key_query",
            "        )",
            "        self._task_scheduler = hs.get_task_scheduler()",
            "",
            "        self.device_list_updater = DeviceListWorkerUpdater(hs)",
            "",
            "        self._task_scheduler.register_action(",
            "            self._delete_device_messages, DELETE_DEVICE_MSGS_TASK_NAME",
            "        )",
            "",
            "    @trace",
            "    async def get_devices_by_user(self, user_id: str) -> List[JsonDict]:",
            "        \"\"\"",
            "        Retrieve the given user's devices",
            "",
            "        Args:",
            "            user_id: The user ID to query for devices.",
            "        Returns:",
            "            info on each device",
            "        \"\"\"",
            "",
            "        set_tag(\"user_id\", user_id)",
            "        device_map = await self.store.get_devices_by_user(user_id)",
            "",
            "        ips = await self.store.get_last_client_ip_by_device(user_id, device_id=None)",
            "",
            "        devices = list(device_map.values())",
            "        for device in devices:",
            "            _update_device_from_client_ips(device, ips)",
            "",
            "        log_kv(device_map)",
            "        return devices",
            "",
            "    async def get_dehydrated_device(",
            "        self, user_id: str",
            "    ) -> Optional[Tuple[str, JsonDict]]:",
            "        \"\"\"Retrieve the information for a dehydrated device.",
            "",
            "        Args:",
            "            user_id: the user whose dehydrated device we are looking for",
            "        Returns:",
            "            a tuple whose first item is the device ID, and the second item is",
            "            the dehydrated device information",
            "        \"\"\"",
            "        return await self.store.get_dehydrated_device(user_id)",
            "",
            "    @trace",
            "    async def get_device(self, user_id: str, device_id: str) -> JsonDict:",
            "        \"\"\"Retrieve the given device",
            "",
            "        Args:",
            "            user_id: The user to get the device from",
            "            device_id: The device to fetch.",
            "",
            "        Returns:",
            "            info on the device",
            "        Raises:",
            "            errors.NotFoundError: if the device was not found",
            "        \"\"\"",
            "        device = await self.store.get_device(user_id, device_id)",
            "        if device is None:",
            "            raise errors.NotFoundError()",
            "",
            "        ips = await self.store.get_last_client_ip_by_device(user_id, device_id)",
            "        _update_device_from_client_ips(device, ips)",
            "",
            "        set_tag(\"device\", str(device))",
            "        set_tag(\"ips\", str(ips))",
            "",
            "        return device",
            "",
            "    @cancellable",
            "    async def get_device_changes_in_shared_rooms(",
            "        self, user_id: str, room_ids: StrCollection, from_token: StreamToken",
            "    ) -> Set[str]:",
            "        \"\"\"Get the set of users whose devices have changed who share a room with",
            "        the given user.",
            "        \"\"\"",
            "        changed_users = await self.store.get_device_list_changes_in_rooms(",
            "            room_ids, from_token.device_list_key",
            "        )",
            "",
            "        if changed_users is not None:",
            "            # We also check if the given user has changed their device. If",
            "            # they're in no rooms then the above query won't include them.",
            "            changed = await self.store.get_users_whose_devices_changed(",
            "                from_token.device_list_key, [user_id]",
            "            )",
            "            changed_users.update(changed)",
            "            return changed_users",
            "",
            "        # If the DB returned None then the `from_token` is too old, so we fall",
            "        # back on looking for device updates for all users.",
            "",
            "        users_who_share_room = await self.store.get_users_who_share_room_with_user(",
            "            user_id",
            "        )",
            "",
            "        tracked_users = set(users_who_share_room)",
            "",
            "        # Always tell the user about their own devices",
            "        tracked_users.add(user_id)",
            "",
            "        changed = await self.store.get_users_whose_devices_changed(",
            "            from_token.device_list_key, tracked_users",
            "        )",
            "",
            "        return changed",
            "",
            "    @trace",
            "    @measure_func(\"device.get_user_ids_changed\")",
            "    @cancellable",
            "    async def get_user_ids_changed(",
            "        self, user_id: str, from_token: StreamToken",
            "    ) -> JsonDict:",
            "        \"\"\"Get list of users that have had the devices updated, or have newly",
            "        joined a room, that `user_id` may be interested in.",
            "        \"\"\"",
            "",
            "        set_tag(\"user_id\", user_id)",
            "        set_tag(\"from_token\", str(from_token))",
            "        now_room_key = self.store.get_room_max_token()",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "",
            "        changed = await self.get_device_changes_in_shared_rooms(",
            "            user_id, room_ids, from_token",
            "        )",
            "",
            "        # Then work out if any users have since joined",
            "        rooms_changed = self.store.get_rooms_that_changed(room_ids, from_token.room_key)",
            "",
            "        member_events = await self.store.get_membership_changes_for_user(",
            "            user_id, from_token.room_key, now_room_key",
            "        )",
            "        rooms_changed.update(event.room_id for event in member_events)",
            "",
            "        stream_ordering = from_token.room_key.stream",
            "",
            "        possibly_changed = set(changed)",
            "        possibly_left = set()",
            "        for room_id in rooms_changed:",
            "            # Check if the forward extremities have changed. If not then we know",
            "            # the current state won't have changed, and so we can skip this room.",
            "            try:",
            "                if not await self.store.have_room_forward_extremities_changed_since(",
            "                    room_id, stream_ordering",
            "                ):",
            "                    continue",
            "            except errors.StoreError:",
            "                pass",
            "",
            "            current_state_ids = await self._state_storage.get_current_state_ids(",
            "                room_id, await_full_state=False",
            "            )",
            "",
            "            # The user may have left the room",
            "            # TODO: Check if they actually did or if we were just invited.",
            "            if room_id not in room_ids:",
            "                for etype, state_key in current_state_ids.keys():",
            "                    if etype != EventTypes.Member:",
            "                        continue",
            "                    possibly_left.add(state_key)",
            "                continue",
            "",
            "            # Fetch the current state at the time.",
            "            try:",
            "                event_ids = await self.store.get_forward_extremities_for_room_at_stream_ordering(",
            "                    room_id, stream_ordering=stream_ordering",
            "                )",
            "            except errors.StoreError:",
            "                # we have purged the stream_ordering index since the stream",
            "                # ordering: treat it the same as a new room",
            "                event_ids = []",
            "",
            "            # special-case for an empty prev state: include all members",
            "            # in the changed list",
            "            if not event_ids:",
            "                log_kv(",
            "                    {\"event\": \"encountered empty previous state\", \"room_id\": room_id}",
            "                )",
            "                for etype, state_key in current_state_ids.keys():",
            "                    if etype != EventTypes.Member:",
            "                        continue",
            "                    possibly_changed.add(state_key)",
            "                continue",
            "",
            "            current_member_id = current_state_ids.get((EventTypes.Member, user_id))",
            "            if not current_member_id:",
            "                continue",
            "",
            "            # mapping from event_id -> state_dict",
            "            prev_state_ids = await self._state_storage.get_state_ids_for_events(",
            "                event_ids,",
            "                await_full_state=False,",
            "            )",
            "",
            "            # Check if we've joined the room? If so we just blindly add all the users to",
            "            # the \"possibly changed\" users.",
            "            for state_dict in prev_state_ids.values():",
            "                member_event = state_dict.get((EventTypes.Member, user_id), None)",
            "                if not member_event or member_event != current_member_id:",
            "                    for etype, state_key in current_state_ids.keys():",
            "                        if etype != EventTypes.Member:",
            "                            continue",
            "                        possibly_changed.add(state_key)",
            "                    break",
            "",
            "            # If there has been any change in membership, include them in the",
            "            # possibly changed list. We'll check if they are joined below,",
            "            # and we're not toooo worried about spuriously adding users.",
            "            for key, event_id in current_state_ids.items():",
            "                etype, state_key = key",
            "                if etype != EventTypes.Member:",
            "                    continue",
            "",
            "                # check if this member has changed since any of the extremities",
            "                # at the stream_ordering, and add them to the list if so.",
            "                for state_dict in prev_state_ids.values():",
            "                    prev_event_id = state_dict.get(key, None)",
            "                    if not prev_event_id or prev_event_id != event_id:",
            "                        if state_key != user_id:",
            "                            possibly_changed.add(state_key)",
            "                        break",
            "",
            "        if possibly_changed or possibly_left:",
            "            possibly_joined = possibly_changed",
            "            possibly_left = possibly_changed | possibly_left",
            "",
            "            # Double check if we still share rooms with the given user.",
            "            users_rooms = await self.store.get_rooms_for_users(possibly_left)",
            "            for changed_user_id, entries in users_rooms.items():",
            "                if any(rid in room_ids for rid in entries):",
            "                    possibly_left.discard(changed_user_id)",
            "                else:",
            "                    possibly_joined.discard(changed_user_id)",
            "",
            "        else:",
            "            possibly_joined = set()",
            "            possibly_left = set()",
            "",
            "        result = {\"changed\": list(possibly_joined), \"left\": list(possibly_left)}",
            "",
            "        log_kv(result)",
            "",
            "        return result",
            "",
            "    async def on_federation_query_user_devices(self, user_id: str) -> JsonDict:",
            "        stream_id, devices = await self.store.get_e2e_device_keys_for_federation_query(",
            "            user_id",
            "        )",
            "        master_key = await self.store.get_e2e_cross_signing_key(user_id, \"master\")",
            "        self_signing_key = await self.store.get_e2e_cross_signing_key(",
            "            user_id, \"self_signing\"",
            "        )",
            "",
            "        # Check if the application services have any results.",
            "        if self._query_appservices_for_keys:",
            "            # Query the appservice for all devices for this user.",
            "            query: Dict[str, Optional[List[str]]] = {user_id: None}",
            "",
            "            # Query the appservices for any keys.",
            "            appservice_results = await self._appservice_handler.query_keys(query)",
            "",
            "            # Merge results, overriding anything from the database.",
            "            appservice_devices = appservice_results.get(\"device_keys\", {}).get(",
            "                user_id, {}",
            "            )",
            "",
            "            # Filter the database results to only those devices that the appservice has",
            "            # *not* responded with.",
            "            devices = [d for d in devices if d[\"device_id\"] not in appservice_devices]",
            "            # Append the appservice response by wrapping each result in another dictionary.",
            "            devices.extend(",
            "                {\"device_id\": device_id, \"keys\": device}",
            "                for device_id, device in appservice_devices.items()",
            "            )",
            "",
            "            # TODO Handle cross-signing keys.",
            "",
            "        return {",
            "            \"user_id\": user_id,",
            "            \"stream_id\": stream_id,",
            "            \"devices\": devices,",
            "            \"master_key\": master_key,",
            "            \"self_signing_key\": self_signing_key,",
            "        }",
            "",
            "    async def handle_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Handles sending appropriate device list updates in a room that has",
            "        gone from partial to full state.",
            "        \"\"\"",
            "",
            "        # TODO(faster_joins): worker mode support",
            "        #   https://github.com/matrix-org/synapse/issues/12994",
            "        logger.error(",
            "            \"Trying handling device list state for partial join: not supported on workers.\"",
            "        )",
            "",
            "    DEVICE_MSGS_DELETE_BATCH_LIMIT = 1000",
            "    DEVICE_MSGS_DELETE_SLEEP_MS = 1000",
            "",
            "    async def _delete_device_messages(",
            "        self,",
            "        task: ScheduledTask,",
            "    ) -> Tuple[TaskStatus, Optional[JsonMapping], Optional[str]]:",
            "        \"\"\"Scheduler task to delete device messages in batch of `DEVICE_MSGS_DELETE_BATCH_LIMIT`.\"\"\"",
            "        assert task.params is not None",
            "        user_id = task.params[\"user_id\"]",
            "        device_id = task.params[\"device_id\"]",
            "        up_to_stream_id = task.params[\"up_to_stream_id\"]",
            "",
            "        # Delete the messages in batches to avoid too much DB load.",
            "        while True:",
            "            res = await self.store.delete_messages_for_device(",
            "                user_id=user_id,",
            "                device_id=device_id,",
            "                up_to_stream_id=up_to_stream_id,",
            "                limit=DeviceHandler.DEVICE_MSGS_DELETE_BATCH_LIMIT,",
            "            )",
            "",
            "            if res < DeviceHandler.DEVICE_MSGS_DELETE_BATCH_LIMIT:",
            "                return TaskStatus.COMPLETE, None, None",
            "",
            "            await self.clock.sleep(DeviceHandler.DEVICE_MSGS_DELETE_SLEEP_MS / 1000.0)",
            "",
            "",
            "class DeviceHandler(DeviceWorkerHandler):",
            "    device_list_updater: \"DeviceListUpdater\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.federation_sender = hs.get_federation_sender()",
            "        self._account_data_handler = hs.get_account_data_handler()",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.db_pool = hs.get_datastores().main.db_pool",
            "",
            "        self.device_list_updater = DeviceListUpdater(hs, self)",
            "",
            "        federation_registry = hs.get_federation_registry()",
            "",
            "        federation_registry.register_edu_handler(",
            "            EduTypes.DEVICE_LIST_UPDATE,",
            "            self.device_list_updater.incoming_device_list_update,",
            "        )",
            "",
            "        # Whether `_handle_new_device_update_async` is currently processing.",
            "        self._handle_new_device_update_is_processing = False",
            "",
            "        # If a new device update may have happened while the loop was",
            "        # processing.",
            "        self._handle_new_device_update_new_data = False",
            "",
            "        # On start up check if there are any updates pending.",
            "        hs.get_reactor().callWhenRunning(self._handle_new_device_update_async)",
            "",
            "        self._delete_stale_devices_after = hs.config.server.delete_stale_devices_after",
            "",
            "        # Ideally we would run this on a worker and condition this on the",
            "        # \"run_background_tasks_on\" setting, but this would mean making the notification",
            "        # of device list changes over federation work on workers, which is nontrivial.",
            "        if self._delete_stale_devices_after is not None:",
            "            self.clock.looping_call(",
            "                run_as_background_process,",
            "                DELETE_STALE_DEVICES_INTERVAL_MS,",
            "                \"delete_stale_devices\",",
            "                self._delete_stale_devices,",
            "            )",
            "",
            "    def _check_device_name_length(self, name: Optional[str]) -> None:",
            "        \"\"\"",
            "        Checks whether a device name is longer than the maximum allowed length.",
            "",
            "        Args:",
            "            name: The name of the device.",
            "",
            "        Raises:",
            "            SynapseError: if the device name is too long.",
            "        \"\"\"",
            "        if name and len(name) > MAX_DEVICE_DISPLAY_NAME_LEN:",
            "            raise SynapseError(",
            "                400,",
            "                \"Device display name is too long (max %i)\"",
            "                % (MAX_DEVICE_DISPLAY_NAME_LEN,),",
            "                errcode=Codes.TOO_LARGE,",
            "            )",
            "",
            "    async def check_device_registered(",
            "        self,",
            "        user_id: str,",
            "        device_id: Optional[str],",
            "        initial_device_display_name: Optional[str] = None,",
            "        auth_provider_id: Optional[str] = None,",
            "        auth_provider_session_id: Optional[str] = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        If the given device has not been registered, register it with the",
            "        supplied display name.",
            "",
            "        If no device_id is supplied, we make one up.",
            "",
            "        Args:",
            "            user_id:  @user:id",
            "            device_id: device id supplied by client",
            "            initial_device_display_name: device display name from client",
            "            auth_provider_id: The SSO IdP the user used, if any.",
            "            auth_provider_session_id: The session ID (sid) got from the SSO IdP.",
            "        Returns:",
            "            device id (generated if none was supplied)",
            "        \"\"\"",
            "",
            "        self._check_device_name_length(initial_device_display_name)",
            "",
            "        if device_id is not None:",
            "            new_device = await self.store.store_device(",
            "                user_id=user_id,",
            "                device_id=device_id,",
            "                initial_device_display_name=initial_device_display_name,",
            "                auth_provider_id=auth_provider_id,",
            "                auth_provider_session_id=auth_provider_session_id,",
            "            )",
            "            if new_device:",
            "                await self.notify_device_update(user_id, [device_id])",
            "            return device_id",
            "",
            "        # if the device id is not specified, we'll autogen one, but loop a few",
            "        # times in case of a clash.",
            "        attempts = 0",
            "        while attempts < 5:",
            "            new_device_id = stringutils.random_string(10).upper()",
            "            new_device = await self.store.store_device(",
            "                user_id=user_id,",
            "                device_id=new_device_id,",
            "                initial_device_display_name=initial_device_display_name,",
            "                auth_provider_id=auth_provider_id,",
            "                auth_provider_session_id=auth_provider_session_id,",
            "            )",
            "            if new_device:",
            "                await self.notify_device_update(user_id, [new_device_id])",
            "                return new_device_id",
            "            attempts += 1",
            "",
            "        raise errors.StoreError(500, \"Couldn't generate a device ID.\")",
            "",
            "    async def _delete_stale_devices(self) -> None:",
            "        \"\"\"Background task that deletes devices which haven't been accessed for more than",
            "        a configured time period.",
            "        \"\"\"",
            "        # We should only be running this job if the config option is defined.",
            "        assert self._delete_stale_devices_after is not None",
            "        now_ms = self.clock.time_msec()",
            "        since_ms = now_ms - self._delete_stale_devices_after",
            "        devices = await self.store.get_local_devices_not_accessed_since(since_ms)",
            "",
            "        for user_id, user_devices in devices.items():",
            "            await self.delete_devices(user_id, user_devices)",
            "",
            "    @trace",
            "    async def delete_all_devices_for_user(",
            "        self, user_id: str, except_device_id: Optional[str] = None",
            "    ) -> None:",
            "        \"\"\"Delete all of the user's devices",
            "",
            "        Args:",
            "            user_id: The user to remove all devices from",
            "            except_device_id: optional device id which should not be deleted",
            "        \"\"\"",
            "        device_map = await self.store.get_devices_by_user(user_id)",
            "        device_ids = list(device_map)",
            "        if except_device_id is not None:",
            "            device_ids = [d for d in device_ids if d != except_device_id]",
            "        await self.delete_devices(user_id, device_ids)",
            "",
            "    async def delete_devices(self, user_id: str, device_ids: List[str]) -> None:",
            "        \"\"\"Delete several devices",
            "",
            "        Args:",
            "            user_id: The user to delete devices from.",
            "            device_ids: The list of device IDs to delete",
            "        \"\"\"",
            "        to_device_stream_id = self._event_sources.get_current_token().to_device_key",
            "",
            "        try:",
            "            await self.store.delete_devices(user_id, device_ids)",
            "        except errors.StoreError as e:",
            "            if e.code == 404:",
            "                # no match",
            "                set_tag(\"error\", True)",
            "                set_tag(\"reason\", \"User doesn't have that device id.\")",
            "            else:",
            "                raise",
            "",
            "        # Delete data specific to each device. Not optimised as it is not",
            "        # considered as part of a critical path.",
            "        for device_id in device_ids:",
            "            await self._auth_handler.delete_access_tokens_for_user(",
            "                user_id, device_id=device_id",
            "            )",
            "            await self.store.delete_e2e_keys_by_device(",
            "                user_id=user_id, device_id=device_id",
            "            )",
            "",
            "            if self.hs.config.experimental.msc3890_enabled:",
            "                # Remove any local notification settings for this device in accordance",
            "                # with MSC3890.",
            "                await self._account_data_handler.remove_account_data_for_user(",
            "                    user_id,",
            "                    f\"org.matrix.msc3890.local_notification_settings.{device_id}\",",
            "                )",
            "",
            "            # Delete device messages asynchronously and in batches using the task scheduler",
            "            await self._task_scheduler.schedule_task(",
            "                DELETE_DEVICE_MSGS_TASK_NAME,",
            "                resource_id=device_id,",
            "                params={",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                    \"up_to_stream_id\": to_device_stream_id,",
            "                },",
            "            )",
            "",
            "        # Pushers are deleted after `delete_access_tokens_for_user` is called so that",
            "        # modules using `on_logged_out` hook can use them if needed.",
            "        await self.hs.get_pusherpool().remove_pushers_by_devices(user_id, device_ids)",
            "",
            "        await self.notify_device_update(user_id, device_ids)",
            "",
            "    async def update_device(self, user_id: str, device_id: str, content: dict) -> None:",
            "        \"\"\"Update the given device",
            "",
            "        Args:",
            "            user_id: The user to update devices of.",
            "            device_id: The device to update.",
            "            content: body of update request",
            "        \"\"\"",
            "",
            "        # Reject a new displayname which is too long.",
            "        new_display_name = content.get(\"display_name\")",
            "",
            "        self._check_device_name_length(new_display_name)",
            "",
            "        try:",
            "            await self.store.update_device(",
            "                user_id, device_id, new_display_name=new_display_name",
            "            )",
            "            await self.notify_device_update(user_id, [device_id])",
            "        except errors.StoreError as e:",
            "            if e.code == 404:",
            "                raise errors.NotFoundError()",
            "            else:",
            "                raise",
            "",
            "    @trace",
            "    @measure_func(\"notify_device_update\")",
            "    async def notify_device_update(",
            "        self, user_id: str, device_ids: StrCollection",
            "    ) -> None:",
            "        \"\"\"Notify that a user's device(s) has changed. Pokes the notifier, and",
            "        remote servers if the user is local.",
            "",
            "        Args:",
            "            user_id: The Matrix ID of the user who's device list has been updated.",
            "            device_ids: The device IDs that have changed.",
            "        \"\"\"",
            "        if not device_ids:",
            "            # No changes to notify about, so this is a no-op.",
            "            return",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "",
            "        position = await self.store.add_device_change_to_streams(",
            "            user_id,",
            "            device_ids,",
            "            room_ids=room_ids,",
            "        )",
            "",
            "        if not position:",
            "            # This should only happen if there are no updates, so we bail.",
            "            return",
            "",
            "        for device_id in device_ids:",
            "            logger.debug(",
            "                \"Notifying about update %r/%r, ID: %r\", user_id, device_id, position",
            "            )",
            "",
            "        # specify the user ID too since the user should always get their own device list",
            "        # updates, even if they aren't in any rooms.",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.DEVICE_LIST, position, users={user_id}, rooms=room_ids",
            "        )",
            "",
            "        # We may need to do some processing asynchronously for local user IDs.",
            "        if self.hs.is_mine_id(user_id):",
            "            self._handle_new_device_update_async()",
            "",
            "    async def notify_user_signature_update(",
            "        self, from_user_id: str, user_ids: List[str]",
            "    ) -> None:",
            "        \"\"\"Notify a user that they have made new signatures of other users.",
            "",
            "        Args:",
            "            from_user_id: the user who made the signature",
            "            user_ids: the users IDs that have new signatures",
            "        \"\"\"",
            "",
            "        position = await self.store.add_user_signature_change_to_streams(",
            "            from_user_id, user_ids",
            "        )",
            "",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.DEVICE_LIST, position, users=[from_user_id]",
            "        )",
            "",
            "    async def store_dehydrated_device(",
            "        self,",
            "        user_id: str,",
            "        device_id: Optional[str],",
            "        device_data: JsonDict,",
            "        initial_device_display_name: Optional[str] = None,",
            "        keys_for_device: Optional[JsonDict] = None,",
            "    ) -> str:",
            "        \"\"\"Store a dehydrated device for a user, optionally storing the keys associated with",
            "        it as well.  If the user had a previous dehydrated device, it is removed.",
            "",
            "        Args:",
            "            user_id: the user that we are storing the device for",
            "            device_id: device id supplied by client",
            "            device_data: the dehydrated device information",
            "            initial_device_display_name: The display name to use for the device",
            "            keys_for_device: keys for the dehydrated device",
            "        Returns:",
            "            device id of the dehydrated device",
            "        \"\"\"",
            "        device_id = await self.check_device_registered(",
            "            user_id,",
            "            device_id,",
            "            initial_device_display_name,",
            "        )",
            "",
            "        time_now = self.clock.time_msec()",
            "",
            "        old_device_id = await self.store.store_dehydrated_device(",
            "            user_id, device_id, device_data, time_now, keys_for_device",
            "        )",
            "",
            "        if old_device_id is not None:",
            "            await self.delete_devices(user_id, [old_device_id])",
            "",
            "        return device_id",
            "",
            "    async def rehydrate_device(",
            "        self, user_id: str, access_token: str, device_id: str",
            "    ) -> dict:",
            "        \"\"\"Process a rehydration request from the user.",
            "",
            "        Args:",
            "            user_id: the user who is rehydrating the device",
            "            access_token: the access token used for the request",
            "            device_id: the ID of the device that will be rehydrated",
            "        Returns:",
            "            a dict containing {\"success\": True}",
            "        \"\"\"",
            "        success = await self.store.remove_dehydrated_device(user_id, device_id)",
            "",
            "        if not success:",
            "            raise errors.NotFoundError()",
            "",
            "        # If the dehydrated device was successfully deleted (the device ID",
            "        # matched the stored dehydrated device), then modify the access",
            "        # token and refresh token to use the dehydrated device's ID and",
            "        # copy the old device display name to the dehydrated device,",
            "        # and destroy the old device ID",
            "        old_device_id = await self.store.set_device_for_access_token(",
            "            access_token, device_id",
            "        )",
            "        await self.store.set_device_for_refresh_token(user_id, old_device_id, device_id)",
            "        old_device = await self.store.get_device(user_id, old_device_id)",
            "        if old_device is None:",
            "            raise errors.NotFoundError()",
            "        await self.store.update_device(user_id, device_id, old_device[\"display_name\"])",
            "        # can't call self.delete_device because that will clobber the",
            "        # access token so call the storage layer directly",
            "        await self.store.delete_devices(user_id, [old_device_id])",
            "        await self.store.delete_e2e_keys_by_device(",
            "            user_id=user_id, device_id=old_device_id",
            "        )",
            "",
            "        # tell everyone that the old device is gone and that the dehydrated",
            "        # device has a new display name",
            "        await self.notify_device_update(user_id, [old_device_id, device_id])",
            "",
            "        return {\"success\": True}",
            "",
            "    async def delete_dehydrated_device(self, user_id: str, device_id: str) -> None:",
            "        \"\"\"",
            "        Delete a stored dehydrated device.",
            "",
            "        Args:",
            "            user_id: the user_id to delete the device from",
            "            device_id: id of the dehydrated device to delete",
            "        \"\"\"",
            "        success = await self.store.remove_dehydrated_device(user_id, device_id)",
            "",
            "        if not success:",
            "            raise errors.NotFoundError()",
            "",
            "        await self.delete_devices(user_id, [device_id])",
            "        await self.store.delete_e2e_keys_by_device(user_id=user_id, device_id=device_id)",
            "",
            "    @wrap_as_background_process(\"_handle_new_device_update_async\")",
            "    async def _handle_new_device_update_async(self) -> None:",
            "        \"\"\"Called when we have a new local device list update that we need to",
            "        send out over federation.",
            "",
            "        This happens in the background so as not to block the original request",
            "        that generated the device update.",
            "        \"\"\"",
            "        if self._handle_new_device_update_is_processing:",
            "            self._handle_new_device_update_new_data = True",
            "            return",
            "",
            "        self._handle_new_device_update_is_processing = True",
            "",
            "        # The stream ID we processed previous iteration (if any), and the set of",
            "        # hosts we've already poked about for this update. This is so that we",
            "        # don't poke the same remote server about the same update repeatedly.",
            "        current_stream_id = None",
            "        hosts_already_sent_to: Set[str] = set()",
            "",
            "        try:",
            "            stream_id, room_id = await self.store.get_device_change_last_converted_pos()",
            "",
            "            while True:",
            "                self._handle_new_device_update_new_data = False",
            "                max_stream_id = self.store.get_device_stream_token()",
            "                rows = await self.store.get_uncoverted_outbound_room_pokes(",
            "                    stream_id, room_id",
            "                )",
            "                if not rows:",
            "                    # If the DB returned nothing then there is nothing left to",
            "                    # do, *unless* a new device list update happened during the",
            "                    # DB query.",
            "",
            "                    # Advance `(stream_id, room_id)`.",
            "                    # `max_stream_id` comes from *before* the query for unconverted",
            "                    # rows, which means that any unconverted rows must have a larger",
            "                    # stream ID.",
            "                    if max_stream_id > stream_id:",
            "                        stream_id, room_id = max_stream_id, \"\"",
            "                        await self.store.set_device_change_last_converted_pos(",
            "                            stream_id, room_id",
            "                        )",
            "                    else:",
            "                        assert max_stream_id == stream_id",
            "                        # Avoid moving `room_id` backwards.",
            "",
            "                    if self._handle_new_device_update_new_data:",
            "                        continue",
            "                    else:",
            "                        return",
            "",
            "                for user_id, device_id, room_id, stream_id, opentracing_context in rows:",
            "                    hosts = set()",
            "",
            "                    # Ignore any users that aren't ours",
            "                    if self.hs.is_mine_id(user_id):",
            "                        hosts = set(",
            "                            await self._storage_controllers.state.get_current_hosts_in_room_or_partial_state_approximation(",
            "                                room_id",
            "                            )",
            "                        )",
            "                        hosts.discard(self.server_name)",
            "                        # For rooms with partial state, `hosts` is merely an",
            "                        # approximation. When we transition to a full state room, we",
            "                        # will have to send out device list updates to any servers we",
            "                        # missed.",
            "",
            "                    # Check if we've already sent this update to some hosts",
            "                    if current_stream_id == stream_id:",
            "                        hosts -= hosts_already_sent_to",
            "",
            "                    await self.store.add_device_list_outbound_pokes(",
            "                        user_id=user_id,",
            "                        device_id=device_id,",
            "                        room_id=room_id,",
            "                        hosts=hosts,",
            "                        context=opentracing_context,",
            "                    )",
            "",
            "                    # Notify replication that we've updated the device list stream.",
            "                    self.notifier.notify_replication()",
            "",
            "                    if hosts:",
            "                        logger.info(",
            "                            \"Sending device list update notif for %r to: %r\",",
            "                            user_id,",
            "                            hosts,",
            "                        )",
            "                        await self.federation_sender.send_device_messages(",
            "                            hosts, immediate=False",
            "                        )",
            "                        # TODO: when called, this isn't in a logging context.",
            "                        # This leads to log spam, sentry event spam, and massive",
            "                        # memory usage.",
            "                        # See https://github.com/matrix-org/synapse/issues/12552.",
            "                        # log_kv(",
            "                        #     {\"message\": \"sent device update to host\", \"host\": host}",
            "                        # )",
            "",
            "                    if current_stream_id != stream_id:",
            "                        # Clear the set of hosts we've already sent to as we're",
            "                        # processing a new update.",
            "                        hosts_already_sent_to.clear()",
            "",
            "                    hosts_already_sent_to.update(hosts)",
            "                    current_stream_id = stream_id",
            "",
            "                # Advance `(stream_id, room_id)`.",
            "                _, _, room_id, stream_id, _ = rows[-1]",
            "                await self.store.set_device_change_last_converted_pos(",
            "                    stream_id, room_id",
            "                )",
            "",
            "        finally:",
            "            self._handle_new_device_update_is_processing = False",
            "",
            "    async def handle_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Handles sending appropriate device list updates in a room that has",
            "        gone from partial to full state.",
            "        \"\"\"",
            "",
            "        # We defer to the device list updater to handle pending remote device",
            "        # list updates.",
            "        await self.device_list_updater.handle_room_un_partial_stated(room_id)",
            "",
            "        # Replay local updates.",
            "        (",
            "            join_event_id,",
            "            device_lists_stream_id,",
            "        ) = await self.store.get_join_event_id_and_device_lists_stream_id_for_partial_state(",
            "            room_id",
            "        )",
            "",
            "        # Get the local device list changes that have happened in the room since",
            "        # we started joining. If there are no updates there's nothing left to do.",
            "        changes = await self.store.get_device_list_changes_in_room(",
            "            room_id, device_lists_stream_id",
            "        )",
            "        local_changes = {(u, d) for u, d in changes if self.hs.is_mine_id(u)}",
            "        if not local_changes:",
            "            return",
            "",
            "        # Note: We have persisted the full state at this point, we just haven't",
            "        # cleared the `partial_room` flag.",
            "        join_state_ids = await self._state_storage.get_state_ids_for_event(",
            "            join_event_id, await_full_state=False",
            "        )",
            "        current_state_ids = await self.store.get_partial_current_state_ids(room_id)",
            "",
            "        # Now we need to work out all servers that might have been in the room",
            "        # at any point during our join.",
            "",
            "        # First we look for any membership states that have changed between the",
            "        # initial join and now...",
            "        all_keys = set(join_state_ids)",
            "        all_keys.update(current_state_ids)",
            "",
            "        potentially_changed_hosts = set()",
            "        for etype, state_key in all_keys:",
            "            if etype != EventTypes.Member:",
            "                continue",
            "",
            "            prev = join_state_ids.get((etype, state_key))",
            "            current = current_state_ids.get((etype, state_key))",
            "",
            "            if prev != current:",
            "                potentially_changed_hosts.add(get_domain_from_id(state_key))",
            "",
            "        # ... then we add all the hosts that are currently joined to the room...",
            "        current_hosts_in_room = await self.store.get_current_hosts_in_room(room_id)",
            "        potentially_changed_hosts.update(current_hosts_in_room)",
            "",
            "        # ... and finally we remove any hosts that we were told about, as we",
            "        # will have sent device list updates to those hosts when they happened.",
            "        known_hosts_at_join = await self.store.get_partial_state_servers_at_join(",
            "            room_id",
            "        )",
            "        assert known_hosts_at_join is not None",
            "        potentially_changed_hosts.difference_update(known_hosts_at_join)",
            "",
            "        potentially_changed_hosts.discard(self.server_name)",
            "",
            "        if not potentially_changed_hosts:",
            "            # Nothing to do.",
            "            return",
            "",
            "        logger.info(",
            "            \"Found %d changed hosts to send device list updates to\",",
            "            len(potentially_changed_hosts),",
            "        )",
            "",
            "        for user_id, device_id in local_changes:",
            "            await self.store.add_device_list_outbound_pokes(",
            "                user_id=user_id,",
            "                device_id=device_id,",
            "                room_id=room_id,",
            "                hosts=potentially_changed_hosts,",
            "                context=None,",
            "            )",
            "",
            "        # Notify things that device lists need to be sent out.",
            "        self.notifier.notify_replication()",
            "        await self.federation_sender.send_device_messages(",
            "            potentially_changed_hosts, immediate=False",
            "        )",
            "",
            "",
            "def _update_device_from_client_ips(",
            "    device: JsonDict, client_ips: Mapping[Tuple[str, str], DeviceLastConnectionInfo]",
            ") -> None:",
            "    ip = client_ips.get((device[\"user_id\"], device[\"device_id\"]))",
            "    device.update(",
            "        {",
            "            \"last_seen_user_agent\": ip.user_agent if ip else None,",
            "            \"last_seen_ts\": ip.last_seen if ip else None,",
            "            \"last_seen_ip\": ip.ip if ip else None,",
            "        }",
            "    )",
            "",
            "",
            "class DeviceListWorkerUpdater:",
            "    \"Handles incoming device list updates from federation and contacts the main process over replication\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        from synapse.replication.http.devices import (",
            "            ReplicationMultiUserDevicesResyncRestServlet,",
            "        )",
            "",
            "        self._multi_user_device_resync_client = (",
            "            ReplicationMultiUserDevicesResyncRestServlet.make_client(hs)",
            "        )",
            "",
            "    async def multi_user_device_resync(",
            "        self, user_ids: List[str], mark_failed_as_stale: bool = True",
            "    ) -> Dict[str, Optional[JsonMapping]]:",
            "        \"\"\"",
            "        Like `user_device_resync` but operates on multiple users **from the same origin**",
            "        at once.",
            "",
            "        Returns:",
            "            Dict from User ID to the same Dict as `user_device_resync`.",
            "        \"\"\"",
            "        # mark_failed_as_stale is not sent. Ensure this doesn't break expectations.",
            "        assert mark_failed_as_stale",
            "",
            "        if not user_ids:",
            "            # Shortcut empty requests",
            "            return {}",
            "",
            "        return await self._multi_user_device_resync_client(user_ids=user_ids)",
            "",
            "",
            "class DeviceListUpdater(DeviceListWorkerUpdater):",
            "    \"Handles incoming device list updates from federation and updates the DB\"",
            "",
            "    def __init__(self, hs: \"HomeServer\", device_handler: DeviceHandler):",
            "        self.store = hs.get_datastores().main",
            "        self.federation = hs.get_federation_client()",
            "        self.clock = hs.get_clock()",
            "        self.device_handler = device_handler",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self._remote_edu_linearizer = Linearizer(name=\"remote_device_list\")",
            "        self._resync_linearizer = Linearizer(name=\"remote_device_resync\")",
            "",
            "        # user_id -> list of updates waiting to be handled.",
            "        self._pending_updates: Dict[",
            "            str, List[Tuple[str, str, Iterable[str], JsonDict]]",
            "        ] = {}",
            "",
            "        # Recently seen stream ids. We don't bother keeping these in the DB,",
            "        # but they're useful to have them about to reduce the number of spurious",
            "        # resyncs.",
            "        self._seen_updates: ExpiringCache[str, Set[str]] = ExpiringCache(",
            "            cache_name=\"device_update_edu\",",
            "            clock=self.clock,",
            "            max_len=10000,",
            "            expiry_ms=30 * 60 * 1000,",
            "            iterable=True,",
            "        )",
            "",
            "        # Attempt to resync out of sync device lists every 30s.",
            "        self._resync_retry_in_progress = False",
            "        self.clock.looping_call(",
            "            run_as_background_process,",
            "            30 * 1000,",
            "            func=self._maybe_retry_device_resync,",
            "            desc=\"_maybe_retry_device_resync\",",
            "        )",
            "",
            "    @trace",
            "    async def incoming_device_list_update(",
            "        self, origin: str, edu_content: JsonDict",
            "    ) -> None:",
            "        \"\"\"Called on incoming device list update from federation. Responsible",
            "        for parsing the EDU and adding to pending updates list.",
            "        \"\"\"",
            "",
            "        set_tag(\"origin\", origin)",
            "        set_tag(\"edu_content\", str(edu_content))",
            "        user_id = edu_content.pop(\"user_id\")",
            "        device_id = edu_content.pop(\"device_id\")",
            "        stream_id = str(edu_content.pop(\"stream_id\"))  # They may come as ints",
            "        prev_ids = edu_content.pop(\"prev_id\", [])",
            "        if not isinstance(prev_ids, list):",
            "            raise SynapseError(",
            "                400, \"Device list update had an invalid 'prev_ids' field\"",
            "            )",
            "        prev_ids = [str(p) for p in prev_ids]  # They may come as ints",
            "",
            "        if get_domain_from_id(user_id) != origin:",
            "            # TODO: Raise?",
            "            logger.warning(",
            "                \"Got device list update edu for %r/%r from %r\",",
            "                user_id,",
            "                device_id,",
            "                origin,",
            "            )",
            "",
            "            set_tag(\"error\", True)",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Got a device list update edu from a user and \"",
            "                    \"device which does not match the origin of the request.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            return",
            "",
            "        # Check if we are partially joining any rooms. If so we need to store",
            "        # all device list updates so that we can handle them correctly once we",
            "        # know who is in the room.",
            "        # TODO(faster_joins): this fetches and processes a bunch of data that we don't",
            "        # use. Could be replaced by a tighter query e.g.",
            "        #   SELECT EXISTS(SELECT 1 FROM partial_state_rooms)",
            "        partial_rooms = await self.store.get_partial_state_room_resync_info()",
            "        if partial_rooms:",
            "            await self.store.add_remote_device_list_to_pending(",
            "                user_id,",
            "                device_id,",
            "            )",
            "            self._notifier.notify_replication()",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "        if not room_ids:",
            "            # We don't share any rooms with this user. Ignore update, as we",
            "            # probably won't get any further updates.",
            "            set_tag(\"error\", True)",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Got an update from a user for which \"",
            "                    \"we don't share any rooms\",",
            "                    \"other user_id\": user_id,",
            "                }",
            "            )",
            "            logger.warning(",
            "                \"Got device list update edu for %r/%r, but don't share a room\",",
            "                user_id,",
            "                device_id,",
            "            )",
            "            return",
            "",
            "        logger.debug(\"Received device list update for %r/%r\", user_id, device_id)",
            "",
            "        self._pending_updates.setdefault(user_id, []).append(",
            "            (device_id, stream_id, prev_ids, edu_content)",
            "        )",
            "",
            "        await self._handle_device_updates(user_id)",
            "",
            "    @measure_func(\"_incoming_device_list_update\")",
            "    async def _handle_device_updates(self, user_id: str) -> None:",
            "        \"Actually handle pending updates.\"",
            "",
            "        async with self._remote_edu_linearizer.queue(user_id):",
            "            pending_updates = self._pending_updates.pop(user_id, [])",
            "            if not pending_updates:",
            "                # This can happen since we batch updates",
            "                return",
            "",
            "            for device_id, stream_id, prev_ids, _ in pending_updates:",
            "                logger.debug(",
            "                    \"Handling update %r/%r, ID: %r, prev: %r \",",
            "                    user_id,",
            "                    device_id,",
            "                    stream_id,",
            "                    prev_ids,",
            "                )",
            "",
            "            # Given a list of updates we check if we need to resync. This",
            "            # happens if we've missed updates.",
            "            resync = await self._need_to_do_resync(user_id, pending_updates)",
            "",
            "            if logger.isEnabledFor(logging.INFO):",
            "                logger.info(",
            "                    \"Received device list update for %s, requiring resync: %s. Devices: %s\",",
            "                    user_id,",
            "                    resync,",
            "                    \", \".join(u[0] for u in pending_updates),",
            "                )",
            "",
            "            if resync:",
            "                # We mark as stale up front in case we get restarted.",
            "                await self.store.mark_remote_users_device_caches_as_stale([user_id])",
            "                run_as_background_process(",
            "                    \"_maybe_retry_device_resync\",",
            "                    self.multi_user_device_resync,",
            "                    [user_id],",
            "                    False,",
            "                )",
            "            else:",
            "                # Simply update the single device, since we know that is the only",
            "                # change (because of the single prev_id matching the current cache)",
            "                for device_id, stream_id, _, content in pending_updates:",
            "                    await self.store.update_remote_device_list_cache_entry(",
            "                        user_id, device_id, content, stream_id",
            "                    )",
            "",
            "                await self.device_handler.notify_device_update(",
            "                    user_id, [device_id for device_id, _, _, _ in pending_updates]",
            "                )",
            "",
            "                self._seen_updates.setdefault(user_id, set()).update(",
            "                    stream_id for _, stream_id, _, _ in pending_updates",
            "                )",
            "",
            "    async def _need_to_do_resync(",
            "        self, user_id: str, updates: Iterable[Tuple[str, str, Iterable[str], JsonDict]]",
            "    ) -> bool:",
            "        \"\"\"Given a list of updates for a user figure out if we need to do a full",
            "        resync, or whether we have enough data that we can just apply the delta.",
            "        \"\"\"",
            "        seen_updates: Set[str] = self._seen_updates.get(user_id, set())",
            "",
            "        extremity = await self.store.get_device_list_last_stream_id_for_remote(user_id)",
            "",
            "        logger.debug(\"Current extremity for %r: %r\", user_id, extremity)",
            "",
            "        stream_id_in_updates = set()  # stream_ids in updates list",
            "        for _, stream_id, prev_ids, _ in updates:",
            "            if not prev_ids:",
            "                # We always do a resync if there are no previous IDs",
            "                return True",
            "",
            "            for prev_id in prev_ids:",
            "                if prev_id == extremity:",
            "                    continue",
            "                elif prev_id in seen_updates:",
            "                    continue",
            "                elif prev_id in stream_id_in_updates:",
            "                    continue",
            "                else:",
            "                    return True",
            "",
            "            stream_id_in_updates.add(stream_id)",
            "",
            "        return False",
            "",
            "    @trace",
            "    async def _maybe_retry_device_resync(self) -> None:",
            "        \"\"\"Retry to resync device lists that are out of sync, except if another retry is",
            "        in progress.",
            "        \"\"\"",
            "        if self._resync_retry_in_progress:",
            "            return",
            "",
            "        try:",
            "            # Prevent another call of this function to retry resyncing device lists so",
            "            # we don't send too many requests.",
            "            self._resync_retry_in_progress = True",
            "            # Get all of the users that need resyncing.",
            "            need_resync = await self.store.get_user_ids_requiring_device_list_resync()",
            "",
            "            # Filter out users whose host is marked as \"down\" up front.",
            "            hosts = await filter_destinations_by_retry_limiter(",
            "                {get_domain_from_id(u) for u in need_resync}, self.clock, self.store",
            "            )",
            "            hosts = set(hosts)",
            "",
            "            # Iterate over the set of user IDs.",
            "            for user_id in need_resync:",
            "                if get_domain_from_id(user_id) not in hosts:",
            "                    continue",
            "",
            "                try:",
            "                    # Try to resync the current user's devices list.",
            "                    result = (await self.multi_user_device_resync([user_id], False))[",
            "                        user_id",
            "                    ]",
            "",
            "                    # user_device_resync only returns a result if it managed to",
            "                    # successfully resync and update the database. Updating the table",
            "                    # of users requiring resync isn't necessary here as",
            "                    # user_device_resync already does it (through",
            "                    # self.store.update_remote_device_list_cache).",
            "                    if result:",
            "                        logger.debug(",
            "                            \"Successfully resynced the device list for %s\",",
            "                            user_id,",
            "                        )",
            "                except Exception as e:",
            "                    # If there was an issue resyncing this user, e.g. if the remote",
            "                    # server sent a malformed result, just log the error instead of",
            "                    # aborting all the subsequent resyncs.",
            "                    logger.debug(",
            "                        \"Could not resync the device list for %s: %s\",",
            "                        user_id,",
            "                        e,",
            "                    )",
            "        finally:",
            "            # Allow future calls to retry resyncinc out of sync device lists.",
            "            self._resync_retry_in_progress = False",
            "",
            "    async def multi_user_device_resync(",
            "        self, user_ids: List[str], mark_failed_as_stale: bool = True",
            "    ) -> Dict[str, Optional[JsonMapping]]:",
            "        \"\"\"",
            "        Like `user_device_resync` but operates on multiple users **from the same origin**",
            "        at once.",
            "",
            "        Returns:",
            "            Dict from User ID to the same Dict as `user_device_resync`.",
            "        \"\"\"",
            "        if not user_ids:",
            "            return {}",
            "",
            "        origins = {UserID.from_string(user_id).domain for user_id in user_ids}",
            "",
            "        if len(origins) != 1:",
            "            raise InvalidAPICallError(f\"Only one origin permitted, got {origins!r}\")",
            "",
            "        result = {}",
            "        failed = set()",
            "        # TODO(Perf): Actually batch these up",
            "        for user_id in user_ids:",
            "            async with self._resync_linearizer.queue(user_id):",
            "                (",
            "                    user_result,",
            "                    user_failed,",
            "                ) = await self._user_device_resync_returning_failed(user_id)",
            "            result[user_id] = user_result",
            "            if user_failed:",
            "                failed.add(user_id)",
            "",
            "        if mark_failed_as_stale:",
            "            await self.store.mark_remote_users_device_caches_as_stale(failed)",
            "",
            "        return result",
            "",
            "    async def _user_device_resync_returning_failed(",
            "        self, user_id: str",
            "    ) -> Tuple[Optional[JsonMapping], bool]:",
            "        \"\"\"Fetches all devices for a user and updates the device cache with them.",
            "",
            "        Args:",
            "            user_id: The user's id whose device_list will be updated.",
            "        Returns:",
            "            - A dict with device info as under the \"devices\" in the result of this",
            "              request:",
            "              https://matrix.org/docs/spec/server_server/r0.1.2#get-matrix-federation-v1-user-devices-userid",
            "              None when we weren't able to fetch the device info for some reason,",
            "              e.g. due to a connection problem.",
            "            - True iff the resync failed and the device list should be marked as stale.",
            "        \"\"\"",
            "        # Check that we haven't gone and fetched the devices since we last",
            "        # checked if we needed to resync these device lists.",
            "        if await self.store.get_users_whose_devices_are_cached([user_id]):",
            "            cached = await self.store.get_cached_devices_for_user(user_id)",
            "            return cached, False",
            "",
            "        logger.debug(\"Attempting to resync the device list for %s\", user_id)",
            "        log_kv({\"message\": \"Doing resync to update device list.\"})",
            "        # Fetch all devices for the user.",
            "        origin = get_domain_from_id(user_id)",
            "        try:",
            "            result = await self.federation.query_user_devices(origin, user_id)",
            "        except NotRetryingDestination:",
            "            return None, True",
            "        except (RequestSendFailed, HttpResponseException) as e:",
            "            logger.warning(",
            "                \"Failed to handle device list update for %s: %s\",",
            "                user_id,",
            "                e,",
            "            )",
            "",
            "            # We abort on exceptions rather than accepting the update",
            "            # as otherwise synapse will 'forget' that its device list",
            "            # is out of date. If we bail then we will retry the resync",
            "            # next time we get a device list update for this user_id.",
            "            # This makes it more likely that the device lists will",
            "            # eventually become consistent.",
            "            return None, True",
            "        except FederationDeniedError as e:",
            "            set_tag(\"error\", True)",
            "            log_kv({\"reason\": \"FederationDeniedError\"})",
            "            logger.info(e)",
            "            return None, False",
            "        except Exception as e:",
            "            set_tag(\"error\", True)",
            "            log_kv(",
            "                {\"message\": \"Exception raised by federation request\", \"exception\": e}",
            "            )",
            "            logger.exception(\"Failed to handle device list update for %s\", user_id)",
            "",
            "            return None, True",
            "        log_kv({\"result\": result})",
            "        stream_id = result[\"stream_id\"]",
            "        devices = result[\"devices\"]",
            "",
            "        # Get the master key and the self-signing key for this user if provided in the",
            "        # response (None if not in the response).",
            "        # The response will not contain the user signing key, as this key is only used by",
            "        # its owner, thus it doesn't make sense to send it over federation.",
            "        master_key = result.get(\"master_key\")",
            "        self_signing_key = result.get(\"self_signing_key\")",
            "",
            "        ignore_devices = False",
            "        # If the remote server has more than ~1000 devices for this user",
            "        # we assume that something is going horribly wrong (e.g. a bot",
            "        # that logs in and creates a new device every time it tries to",
            "        # send a message).  Maintaining lots of devices per user in the",
            "        # cache can cause serious performance issues as if this request",
            "        # takes more than 60s to complete, internal replication from the",
            "        # inbound federation worker to the synapse master may time out",
            "        # causing the inbound federation to fail and causing the remote",
            "        # server to retry, causing a DoS.  So in this scenario we give",
            "        # up on storing the total list of devices and only handle the",
            "        # delta instead.",
            "        if len(devices) > 1000:",
            "            logger.warning(",
            "                \"Ignoring device list snapshot for %s as it has >1K devs (%d)\",",
            "                user_id,",
            "                len(devices),",
            "            )",
            "            devices = []",
            "            ignore_devices = True",
            "        else:",
            "            prev_stream_id = await self.store.get_device_list_last_stream_id_for_remote(",
            "                user_id",
            "            )",
            "            cached_devices = await self.store.get_cached_devices_for_user(user_id)",
            "",
            "            # To ensure that a user with no devices is cached, we skip the resync only",
            "            # if we have a stream_id from previously writing a cache entry.",
            "            if prev_stream_id is not None and cached_devices == {",
            "                d[\"device_id\"]: d for d in devices",
            "            }:",
            "                logging.info(",
            "                    \"Skipping device list resync for %s, as our cache matches already\",",
            "                    user_id,",
            "                )",
            "                devices = []",
            "                ignore_devices = True",
            "",
            "        for device in devices:",
            "            logger.debug(",
            "                \"Handling resync update %r/%r, ID: %r\",",
            "                user_id,",
            "                device[\"device_id\"],",
            "                stream_id,",
            "            )",
            "",
            "        if not ignore_devices:",
            "            await self.store.update_remote_device_list_cache(",
            "                user_id, devices, stream_id",
            "            )",
            "        # mark the cache as valid, whether or not we actually processed any device",
            "        # list updates.",
            "        await self.store.mark_remote_user_device_cache_as_valid(user_id)",
            "        device_ids = [device[\"device_id\"] for device in devices]",
            "",
            "        # Handle cross-signing keys.",
            "        cross_signing_device_ids = await self.process_cross_signing_key_update(",
            "            user_id,",
            "            master_key,",
            "            self_signing_key,",
            "        )",
            "        device_ids = device_ids + cross_signing_device_ids",
            "",
            "        if device_ids:",
            "            await self.device_handler.notify_device_update(user_id, device_ids)",
            "",
            "        # We clobber the seen updates since we've re-synced from a given",
            "        # point.",
            "        self._seen_updates[user_id] = {stream_id}",
            "",
            "        return result, False",
            "",
            "    async def process_cross_signing_key_update(",
            "        self,",
            "        user_id: str,",
            "        master_key: Optional[JsonDict],",
            "        self_signing_key: Optional[JsonDict],",
            "    ) -> List[str]:",
            "        \"\"\"Process the given new master and self-signing key for the given remote user.",
            "",
            "        Args:",
            "            user_id: The ID of the user these keys are for.",
            "            master_key: The dict of the cross-signing master key as returned by the",
            "                remote server.",
            "            self_signing_key: The dict of the cross-signing self-signing key as returned",
            "                by the remote server.",
            "",
            "        Return:",
            "            The device IDs for the given keys.",
            "        \"\"\"",
            "        device_ids = []",
            "",
            "        current_keys_map = await self.store.get_e2e_cross_signing_keys_bulk([user_id])",
            "        current_keys = current_keys_map.get(user_id) or {}",
            "",
            "        if master_key and master_key != current_keys.get(\"master\"):",
            "            await self.store.set_e2e_cross_signing_key(user_id, \"master\", master_key)",
            "            _, verify_key = get_verify_key_from_cross_signing_key(master_key)",
            "            # verify_key is a VerifyKey from signedjson, which uses",
            "            # .version to denote the portion of the key ID after the",
            "            # algorithm and colon, which is the device ID",
            "            device_ids.append(verify_key.version)",
            "        if self_signing_key and self_signing_key != current_keys.get(\"self_signing\"):",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user_id, \"self_signing\", self_signing_key",
            "            )",
            "            _, verify_key = get_verify_key_from_cross_signing_key(self_signing_key)",
            "            device_ids.append(verify_key.version)",
            "",
            "        return device_ids",
            "",
            "    async def handle_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Handles sending appropriate device list updates in a room that has",
            "        gone from partial to full state.",
            "        \"\"\"",
            "",
            "        pending_updates = (",
            "            await self.store.get_pending_remote_device_list_updates_for_room(room_id)",
            "        )",
            "",
            "        for user_id, device_id in pending_updates:",
            "            logger.info(",
            "                \"Got pending device list update in room %s: %s / %s\",",
            "                room_id,",
            "                user_id,",
            "                device_id,",
            "            )",
            "            position = await self.store.add_device_change_to_streams(",
            "                user_id,",
            "                [device_id],",
            "                room_ids=[room_id],",
            "            )",
            "",
            "            if not position:",
            "                # This should only happen if there are no updates, which",
            "                # shouldn't happen when we've passed in a non-empty set of",
            "                # device IDs.",
            "                continue",
            "",
            "            self.device_handler.notifier.on_new_event(",
            "                StreamKeyType.DEVICE_LIST, position, rooms=[room_id]",
            "            )"
        ],
        "afterPatchFile": [
            "# Copyright 2016 OpenMarket Ltd",
            "# Copyright 2019 New Vector Ltd",
            "# Copyright 2019,2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Mapping, Optional, Set, Tuple",
            "",
            "from synapse.api import errors",
            "from synapse.api.constants import EduTypes, EventTypes",
            "from synapse.api.errors import (",
            "    Codes,",
            "    FederationDeniedError,",
            "    HttpResponseException,",
            "    InvalidAPICallError,",
            "    RequestSendFailed,",
            "    SynapseError,",
            ")",
            "from synapse.logging.opentracing import log_kv, set_tag, trace",
            "from synapse.metrics.background_process_metrics import (",
            "    run_as_background_process,",
            "    wrap_as_background_process,",
            ")",
            "from synapse.storage.databases.main.client_ips import DeviceLastConnectionInfo",
            "from synapse.types import (",
            "    JsonDict,",
            "    JsonMapping,",
            "    ScheduledTask,",
            "    StrCollection,",
            "    StreamKeyType,",
            "    StreamToken,",
            "    TaskStatus,",
            "    UserID,",
            "    get_domain_from_id,",
            "    get_verify_key_from_cross_signing_key,",
            ")",
            "from synapse.util import stringutils",
            "from synapse.util.async_helpers import Linearizer",
            "from synapse.util.caches.expiringcache import ExpiringCache",
            "from synapse.util.cancellation import cancellable",
            "from synapse.util.metrics import measure_func",
            "from synapse.util.retryutils import (",
            "    NotRetryingDestination,",
            "    filter_destinations_by_retry_limiter,",
            ")",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "DELETE_DEVICE_MSGS_TASK_NAME = \"delete_device_messages\"",
            "MAX_DEVICE_DISPLAY_NAME_LEN = 100",
            "DELETE_STALE_DEVICES_INTERVAL_MS = 24 * 60 * 60 * 1000",
            "",
            "",
            "class DeviceWorkerHandler:",
            "    device_list_updater: \"DeviceListWorkerUpdater\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.clock = hs.get_clock()",
            "        self.hs = hs",
            "        self.store = hs.get_datastores().main",
            "        self.notifier = hs.get_notifier()",
            "        self.state = hs.get_state_handler()",
            "        self._appservice_handler = hs.get_application_service_handler()",
            "        self._state_storage = hs.get_storage_controllers().state",
            "        self._auth_handler = hs.get_auth_handler()",
            "        self._event_sources = hs.get_event_sources()",
            "        self.server_name = hs.hostname",
            "        self._msc3852_enabled = hs.config.experimental.msc3852_enabled",
            "        self._query_appservices_for_keys = (",
            "            hs.config.experimental.msc3984_appservice_key_query",
            "        )",
            "        self._task_scheduler = hs.get_task_scheduler()",
            "",
            "        self.device_list_updater = DeviceListWorkerUpdater(hs)",
            "",
            "        self._task_scheduler.register_action(",
            "            self._delete_device_messages, DELETE_DEVICE_MSGS_TASK_NAME",
            "        )",
            "",
            "    @trace",
            "    async def get_devices_by_user(self, user_id: str) -> List[JsonDict]:",
            "        \"\"\"",
            "        Retrieve the given user's devices",
            "",
            "        Args:",
            "            user_id: The user ID to query for devices.",
            "        Returns:",
            "            info on each device",
            "        \"\"\"",
            "",
            "        set_tag(\"user_id\", user_id)",
            "        device_map = await self.store.get_devices_by_user(user_id)",
            "",
            "        ips = await self.store.get_last_client_ip_by_device(user_id, device_id=None)",
            "",
            "        devices = list(device_map.values())",
            "        for device in devices:",
            "            _update_device_from_client_ips(device, ips)",
            "",
            "        log_kv(device_map)",
            "        return devices",
            "",
            "    async def get_dehydrated_device(",
            "        self, user_id: str",
            "    ) -> Optional[Tuple[str, JsonDict]]:",
            "        \"\"\"Retrieve the information for a dehydrated device.",
            "",
            "        Args:",
            "            user_id: the user whose dehydrated device we are looking for",
            "        Returns:",
            "            a tuple whose first item is the device ID, and the second item is",
            "            the dehydrated device information",
            "        \"\"\"",
            "        return await self.store.get_dehydrated_device(user_id)",
            "",
            "    @trace",
            "    async def get_device(self, user_id: str, device_id: str) -> JsonDict:",
            "        \"\"\"Retrieve the given device",
            "",
            "        Args:",
            "            user_id: The user to get the device from",
            "            device_id: The device to fetch.",
            "",
            "        Returns:",
            "            info on the device",
            "        Raises:",
            "            errors.NotFoundError: if the device was not found",
            "        \"\"\"",
            "        device = await self.store.get_device(user_id, device_id)",
            "        if device is None:",
            "            raise errors.NotFoundError()",
            "",
            "        ips = await self.store.get_last_client_ip_by_device(user_id, device_id)",
            "        _update_device_from_client_ips(device, ips)",
            "",
            "        set_tag(\"device\", str(device))",
            "        set_tag(\"ips\", str(ips))",
            "",
            "        return device",
            "",
            "    @cancellable",
            "    async def get_device_changes_in_shared_rooms(",
            "        self, user_id: str, room_ids: StrCollection, from_token: StreamToken",
            "    ) -> Set[str]:",
            "        \"\"\"Get the set of users whose devices have changed who share a room with",
            "        the given user.",
            "        \"\"\"",
            "        changed_users = await self.store.get_device_list_changes_in_rooms(",
            "            room_ids, from_token.device_list_key",
            "        )",
            "",
            "        if changed_users is not None:",
            "            # We also check if the given user has changed their device. If",
            "            # they're in no rooms then the above query won't include them.",
            "            changed = await self.store.get_users_whose_devices_changed(",
            "                from_token.device_list_key, [user_id]",
            "            )",
            "            changed_users.update(changed)",
            "            return changed_users",
            "",
            "        # If the DB returned None then the `from_token` is too old, so we fall",
            "        # back on looking for device updates for all users.",
            "",
            "        users_who_share_room = await self.store.get_users_who_share_room_with_user(",
            "            user_id",
            "        )",
            "",
            "        tracked_users = set(users_who_share_room)",
            "",
            "        # Always tell the user about their own devices",
            "        tracked_users.add(user_id)",
            "",
            "        changed = await self.store.get_users_whose_devices_changed(",
            "            from_token.device_list_key, tracked_users",
            "        )",
            "",
            "        return changed",
            "",
            "    @trace",
            "    @measure_func(\"device.get_user_ids_changed\")",
            "    @cancellable",
            "    async def get_user_ids_changed(",
            "        self, user_id: str, from_token: StreamToken",
            "    ) -> JsonDict:",
            "        \"\"\"Get list of users that have had the devices updated, or have newly",
            "        joined a room, that `user_id` may be interested in.",
            "        \"\"\"",
            "",
            "        set_tag(\"user_id\", user_id)",
            "        set_tag(\"from_token\", str(from_token))",
            "        now_room_key = self.store.get_room_max_token()",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "",
            "        changed = await self.get_device_changes_in_shared_rooms(",
            "            user_id, room_ids, from_token",
            "        )",
            "",
            "        # Then work out if any users have since joined",
            "        rooms_changed = self.store.get_rooms_that_changed(room_ids, from_token.room_key)",
            "",
            "        member_events = await self.store.get_membership_changes_for_user(",
            "            user_id, from_token.room_key, now_room_key",
            "        )",
            "        rooms_changed.update(event.room_id for event in member_events)",
            "",
            "        stream_ordering = from_token.room_key.stream",
            "",
            "        possibly_changed = set(changed)",
            "        possibly_left = set()",
            "        for room_id in rooms_changed:",
            "            # Check if the forward extremities have changed. If not then we know",
            "            # the current state won't have changed, and so we can skip this room.",
            "            try:",
            "                if not await self.store.have_room_forward_extremities_changed_since(",
            "                    room_id, stream_ordering",
            "                ):",
            "                    continue",
            "            except errors.StoreError:",
            "                pass",
            "",
            "            current_state_ids = await self._state_storage.get_current_state_ids(",
            "                room_id, await_full_state=False",
            "            )",
            "",
            "            # The user may have left the room",
            "            # TODO: Check if they actually did or if we were just invited.",
            "            if room_id not in room_ids:",
            "                for etype, state_key in current_state_ids.keys():",
            "                    if etype != EventTypes.Member:",
            "                        continue",
            "                    possibly_left.add(state_key)",
            "                continue",
            "",
            "            # Fetch the current state at the time.",
            "            try:",
            "                event_ids = await self.store.get_forward_extremities_for_room_at_stream_ordering(",
            "                    room_id, stream_ordering=stream_ordering",
            "                )",
            "            except errors.StoreError:",
            "                # we have purged the stream_ordering index since the stream",
            "                # ordering: treat it the same as a new room",
            "                event_ids = []",
            "",
            "            # special-case for an empty prev state: include all members",
            "            # in the changed list",
            "            if not event_ids:",
            "                log_kv(",
            "                    {\"event\": \"encountered empty previous state\", \"room_id\": room_id}",
            "                )",
            "                for etype, state_key in current_state_ids.keys():",
            "                    if etype != EventTypes.Member:",
            "                        continue",
            "                    possibly_changed.add(state_key)",
            "                continue",
            "",
            "            current_member_id = current_state_ids.get((EventTypes.Member, user_id))",
            "            if not current_member_id:",
            "                continue",
            "",
            "            # mapping from event_id -> state_dict",
            "            prev_state_ids = await self._state_storage.get_state_ids_for_events(",
            "                event_ids,",
            "                await_full_state=False,",
            "            )",
            "",
            "            # Check if we've joined the room? If so we just blindly add all the users to",
            "            # the \"possibly changed\" users.",
            "            for state_dict in prev_state_ids.values():",
            "                member_event = state_dict.get((EventTypes.Member, user_id), None)",
            "                if not member_event or member_event != current_member_id:",
            "                    for etype, state_key in current_state_ids.keys():",
            "                        if etype != EventTypes.Member:",
            "                            continue",
            "                        possibly_changed.add(state_key)",
            "                    break",
            "",
            "            # If there has been any change in membership, include them in the",
            "            # possibly changed list. We'll check if they are joined below,",
            "            # and we're not toooo worried about spuriously adding users.",
            "            for key, event_id in current_state_ids.items():",
            "                etype, state_key = key",
            "                if etype != EventTypes.Member:",
            "                    continue",
            "",
            "                # check if this member has changed since any of the extremities",
            "                # at the stream_ordering, and add them to the list if so.",
            "                for state_dict in prev_state_ids.values():",
            "                    prev_event_id = state_dict.get(key, None)",
            "                    if not prev_event_id or prev_event_id != event_id:",
            "                        if state_key != user_id:",
            "                            possibly_changed.add(state_key)",
            "                        break",
            "",
            "        if possibly_changed or possibly_left:",
            "            possibly_joined = possibly_changed",
            "            possibly_left = possibly_changed | possibly_left",
            "",
            "            # Double check if we still share rooms with the given user.",
            "            users_rooms = await self.store.get_rooms_for_users(possibly_left)",
            "            for changed_user_id, entries in users_rooms.items():",
            "                if any(rid in room_ids for rid in entries):",
            "                    possibly_left.discard(changed_user_id)",
            "                else:",
            "                    possibly_joined.discard(changed_user_id)",
            "",
            "        else:",
            "            possibly_joined = set()",
            "            possibly_left = set()",
            "",
            "        result = {\"changed\": list(possibly_joined), \"left\": list(possibly_left)}",
            "",
            "        log_kv(result)",
            "",
            "        return result",
            "",
            "    async def on_federation_query_user_devices(self, user_id: str) -> JsonDict:",
            "        if not self.hs.is_mine(UserID.from_string(user_id)):",
            "            raise SynapseError(400, \"User is not hosted on this homeserver\")",
            "",
            "        stream_id, devices = await self.store.get_e2e_device_keys_for_federation_query(",
            "            user_id",
            "        )",
            "        master_key = await self.store.get_e2e_cross_signing_key(user_id, \"master\")",
            "        self_signing_key = await self.store.get_e2e_cross_signing_key(",
            "            user_id, \"self_signing\"",
            "        )",
            "",
            "        # Check if the application services have any results.",
            "        if self._query_appservices_for_keys:",
            "            # Query the appservice for all devices for this user.",
            "            query: Dict[str, Optional[List[str]]] = {user_id: None}",
            "",
            "            # Query the appservices for any keys.",
            "            appservice_results = await self._appservice_handler.query_keys(query)",
            "",
            "            # Merge results, overriding anything from the database.",
            "            appservice_devices = appservice_results.get(\"device_keys\", {}).get(",
            "                user_id, {}",
            "            )",
            "",
            "            # Filter the database results to only those devices that the appservice has",
            "            # *not* responded with.",
            "            devices = [d for d in devices if d[\"device_id\"] not in appservice_devices]",
            "            # Append the appservice response by wrapping each result in another dictionary.",
            "            devices.extend(",
            "                {\"device_id\": device_id, \"keys\": device}",
            "                for device_id, device in appservice_devices.items()",
            "            )",
            "",
            "            # TODO Handle cross-signing keys.",
            "",
            "        return {",
            "            \"user_id\": user_id,",
            "            \"stream_id\": stream_id,",
            "            \"devices\": devices,",
            "            \"master_key\": master_key,",
            "            \"self_signing_key\": self_signing_key,",
            "        }",
            "",
            "    async def handle_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Handles sending appropriate device list updates in a room that has",
            "        gone from partial to full state.",
            "        \"\"\"",
            "",
            "        # TODO(faster_joins): worker mode support",
            "        #   https://github.com/matrix-org/synapse/issues/12994",
            "        logger.error(",
            "            \"Trying handling device list state for partial join: not supported on workers.\"",
            "        )",
            "",
            "    DEVICE_MSGS_DELETE_BATCH_LIMIT = 1000",
            "    DEVICE_MSGS_DELETE_SLEEP_MS = 1000",
            "",
            "    async def _delete_device_messages(",
            "        self,",
            "        task: ScheduledTask,",
            "    ) -> Tuple[TaskStatus, Optional[JsonMapping], Optional[str]]:",
            "        \"\"\"Scheduler task to delete device messages in batch of `DEVICE_MSGS_DELETE_BATCH_LIMIT`.\"\"\"",
            "        assert task.params is not None",
            "        user_id = task.params[\"user_id\"]",
            "        device_id = task.params[\"device_id\"]",
            "        up_to_stream_id = task.params[\"up_to_stream_id\"]",
            "",
            "        # Delete the messages in batches to avoid too much DB load.",
            "        while True:",
            "            res = await self.store.delete_messages_for_device(",
            "                user_id=user_id,",
            "                device_id=device_id,",
            "                up_to_stream_id=up_to_stream_id,",
            "                limit=DeviceHandler.DEVICE_MSGS_DELETE_BATCH_LIMIT,",
            "            )",
            "",
            "            if res < DeviceHandler.DEVICE_MSGS_DELETE_BATCH_LIMIT:",
            "                return TaskStatus.COMPLETE, None, None",
            "",
            "            await self.clock.sleep(DeviceHandler.DEVICE_MSGS_DELETE_SLEEP_MS / 1000.0)",
            "",
            "",
            "class DeviceHandler(DeviceWorkerHandler):",
            "    device_list_updater: \"DeviceListUpdater\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        super().__init__(hs)",
            "",
            "        self.federation_sender = hs.get_federation_sender()",
            "        self._account_data_handler = hs.get_account_data_handler()",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.db_pool = hs.get_datastores().main.db_pool",
            "",
            "        self.device_list_updater = DeviceListUpdater(hs, self)",
            "",
            "        federation_registry = hs.get_federation_registry()",
            "",
            "        federation_registry.register_edu_handler(",
            "            EduTypes.DEVICE_LIST_UPDATE,",
            "            self.device_list_updater.incoming_device_list_update,",
            "        )",
            "",
            "        # Whether `_handle_new_device_update_async` is currently processing.",
            "        self._handle_new_device_update_is_processing = False",
            "",
            "        # If a new device update may have happened while the loop was",
            "        # processing.",
            "        self._handle_new_device_update_new_data = False",
            "",
            "        # On start up check if there are any updates pending.",
            "        hs.get_reactor().callWhenRunning(self._handle_new_device_update_async)",
            "",
            "        self._delete_stale_devices_after = hs.config.server.delete_stale_devices_after",
            "",
            "        # Ideally we would run this on a worker and condition this on the",
            "        # \"run_background_tasks_on\" setting, but this would mean making the notification",
            "        # of device list changes over federation work on workers, which is nontrivial.",
            "        if self._delete_stale_devices_after is not None:",
            "            self.clock.looping_call(",
            "                run_as_background_process,",
            "                DELETE_STALE_DEVICES_INTERVAL_MS,",
            "                \"delete_stale_devices\",",
            "                self._delete_stale_devices,",
            "            )",
            "",
            "    def _check_device_name_length(self, name: Optional[str]) -> None:",
            "        \"\"\"",
            "        Checks whether a device name is longer than the maximum allowed length.",
            "",
            "        Args:",
            "            name: The name of the device.",
            "",
            "        Raises:",
            "            SynapseError: if the device name is too long.",
            "        \"\"\"",
            "        if name and len(name) > MAX_DEVICE_DISPLAY_NAME_LEN:",
            "            raise SynapseError(",
            "                400,",
            "                \"Device display name is too long (max %i)\"",
            "                % (MAX_DEVICE_DISPLAY_NAME_LEN,),",
            "                errcode=Codes.TOO_LARGE,",
            "            )",
            "",
            "    async def check_device_registered(",
            "        self,",
            "        user_id: str,",
            "        device_id: Optional[str],",
            "        initial_device_display_name: Optional[str] = None,",
            "        auth_provider_id: Optional[str] = None,",
            "        auth_provider_session_id: Optional[str] = None,",
            "    ) -> str:",
            "        \"\"\"",
            "        If the given device has not been registered, register it with the",
            "        supplied display name.",
            "",
            "        If no device_id is supplied, we make one up.",
            "",
            "        Args:",
            "            user_id:  @user:id",
            "            device_id: device id supplied by client",
            "            initial_device_display_name: device display name from client",
            "            auth_provider_id: The SSO IdP the user used, if any.",
            "            auth_provider_session_id: The session ID (sid) got from the SSO IdP.",
            "        Returns:",
            "            device id (generated if none was supplied)",
            "        \"\"\"",
            "",
            "        self._check_device_name_length(initial_device_display_name)",
            "",
            "        if device_id is not None:",
            "            new_device = await self.store.store_device(",
            "                user_id=user_id,",
            "                device_id=device_id,",
            "                initial_device_display_name=initial_device_display_name,",
            "                auth_provider_id=auth_provider_id,",
            "                auth_provider_session_id=auth_provider_session_id,",
            "            )",
            "            if new_device:",
            "                await self.notify_device_update(user_id, [device_id])",
            "            return device_id",
            "",
            "        # if the device id is not specified, we'll autogen one, but loop a few",
            "        # times in case of a clash.",
            "        attempts = 0",
            "        while attempts < 5:",
            "            new_device_id = stringutils.random_string(10).upper()",
            "            new_device = await self.store.store_device(",
            "                user_id=user_id,",
            "                device_id=new_device_id,",
            "                initial_device_display_name=initial_device_display_name,",
            "                auth_provider_id=auth_provider_id,",
            "                auth_provider_session_id=auth_provider_session_id,",
            "            )",
            "            if new_device:",
            "                await self.notify_device_update(user_id, [new_device_id])",
            "                return new_device_id",
            "            attempts += 1",
            "",
            "        raise errors.StoreError(500, \"Couldn't generate a device ID.\")",
            "",
            "    async def _delete_stale_devices(self) -> None:",
            "        \"\"\"Background task that deletes devices which haven't been accessed for more than",
            "        a configured time period.",
            "        \"\"\"",
            "        # We should only be running this job if the config option is defined.",
            "        assert self._delete_stale_devices_after is not None",
            "        now_ms = self.clock.time_msec()",
            "        since_ms = now_ms - self._delete_stale_devices_after",
            "        devices = await self.store.get_local_devices_not_accessed_since(since_ms)",
            "",
            "        for user_id, user_devices in devices.items():",
            "            await self.delete_devices(user_id, user_devices)",
            "",
            "    @trace",
            "    async def delete_all_devices_for_user(",
            "        self, user_id: str, except_device_id: Optional[str] = None",
            "    ) -> None:",
            "        \"\"\"Delete all of the user's devices",
            "",
            "        Args:",
            "            user_id: The user to remove all devices from",
            "            except_device_id: optional device id which should not be deleted",
            "        \"\"\"",
            "        device_map = await self.store.get_devices_by_user(user_id)",
            "        device_ids = list(device_map)",
            "        if except_device_id is not None:",
            "            device_ids = [d for d in device_ids if d != except_device_id]",
            "        await self.delete_devices(user_id, device_ids)",
            "",
            "    async def delete_devices(self, user_id: str, device_ids: List[str]) -> None:",
            "        \"\"\"Delete several devices",
            "",
            "        Args:",
            "            user_id: The user to delete devices from.",
            "            device_ids: The list of device IDs to delete",
            "        \"\"\"",
            "        to_device_stream_id = self._event_sources.get_current_token().to_device_key",
            "",
            "        try:",
            "            await self.store.delete_devices(user_id, device_ids)",
            "        except errors.StoreError as e:",
            "            if e.code == 404:",
            "                # no match",
            "                set_tag(\"error\", True)",
            "                set_tag(\"reason\", \"User doesn't have that device id.\")",
            "            else:",
            "                raise",
            "",
            "        # Delete data specific to each device. Not optimised as it is not",
            "        # considered as part of a critical path.",
            "        for device_id in device_ids:",
            "            await self._auth_handler.delete_access_tokens_for_user(",
            "                user_id, device_id=device_id",
            "            )",
            "            await self.store.delete_e2e_keys_by_device(",
            "                user_id=user_id, device_id=device_id",
            "            )",
            "",
            "            if self.hs.config.experimental.msc3890_enabled:",
            "                # Remove any local notification settings for this device in accordance",
            "                # with MSC3890.",
            "                await self._account_data_handler.remove_account_data_for_user(",
            "                    user_id,",
            "                    f\"org.matrix.msc3890.local_notification_settings.{device_id}\",",
            "                )",
            "",
            "            # Delete device messages asynchronously and in batches using the task scheduler",
            "            await self._task_scheduler.schedule_task(",
            "                DELETE_DEVICE_MSGS_TASK_NAME,",
            "                resource_id=device_id,",
            "                params={",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                    \"up_to_stream_id\": to_device_stream_id,",
            "                },",
            "            )",
            "",
            "        # Pushers are deleted after `delete_access_tokens_for_user` is called so that",
            "        # modules using `on_logged_out` hook can use them if needed.",
            "        await self.hs.get_pusherpool().remove_pushers_by_devices(user_id, device_ids)",
            "",
            "        await self.notify_device_update(user_id, device_ids)",
            "",
            "    async def update_device(self, user_id: str, device_id: str, content: dict) -> None:",
            "        \"\"\"Update the given device",
            "",
            "        Args:",
            "            user_id: The user to update devices of.",
            "            device_id: The device to update.",
            "            content: body of update request",
            "        \"\"\"",
            "",
            "        # Reject a new displayname which is too long.",
            "        new_display_name = content.get(\"display_name\")",
            "",
            "        self._check_device_name_length(new_display_name)",
            "",
            "        try:",
            "            await self.store.update_device(",
            "                user_id, device_id, new_display_name=new_display_name",
            "            )",
            "            await self.notify_device_update(user_id, [device_id])",
            "        except errors.StoreError as e:",
            "            if e.code == 404:",
            "                raise errors.NotFoundError()",
            "            else:",
            "                raise",
            "",
            "    @trace",
            "    @measure_func(\"notify_device_update\")",
            "    async def notify_device_update(",
            "        self, user_id: str, device_ids: StrCollection",
            "    ) -> None:",
            "        \"\"\"Notify that a user's device(s) has changed. Pokes the notifier, and",
            "        remote servers if the user is local.",
            "",
            "        Args:",
            "            user_id: The Matrix ID of the user who's device list has been updated.",
            "            device_ids: The device IDs that have changed.",
            "        \"\"\"",
            "        if not device_ids:",
            "            # No changes to notify about, so this is a no-op.",
            "            return",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "",
            "        position = await self.store.add_device_change_to_streams(",
            "            user_id,",
            "            device_ids,",
            "            room_ids=room_ids,",
            "        )",
            "",
            "        if not position:",
            "            # This should only happen if there are no updates, so we bail.",
            "            return",
            "",
            "        for device_id in device_ids:",
            "            logger.debug(",
            "                \"Notifying about update %r/%r, ID: %r\", user_id, device_id, position",
            "            )",
            "",
            "        # specify the user ID too since the user should always get their own device list",
            "        # updates, even if they aren't in any rooms.",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.DEVICE_LIST, position, users={user_id}, rooms=room_ids",
            "        )",
            "",
            "        # We may need to do some processing asynchronously for local user IDs.",
            "        if self.hs.is_mine_id(user_id):",
            "            self._handle_new_device_update_async()",
            "",
            "    async def notify_user_signature_update(",
            "        self, from_user_id: str, user_ids: List[str]",
            "    ) -> None:",
            "        \"\"\"Notify a user that they have made new signatures of other users.",
            "",
            "        Args:",
            "            from_user_id: the user who made the signature",
            "            user_ids: the users IDs that have new signatures",
            "        \"\"\"",
            "",
            "        position = await self.store.add_user_signature_change_to_streams(",
            "            from_user_id, user_ids",
            "        )",
            "",
            "        self.notifier.on_new_event(",
            "            StreamKeyType.DEVICE_LIST, position, users=[from_user_id]",
            "        )",
            "",
            "    async def store_dehydrated_device(",
            "        self,",
            "        user_id: str,",
            "        device_id: Optional[str],",
            "        device_data: JsonDict,",
            "        initial_device_display_name: Optional[str] = None,",
            "        keys_for_device: Optional[JsonDict] = None,",
            "    ) -> str:",
            "        \"\"\"Store a dehydrated device for a user, optionally storing the keys associated with",
            "        it as well.  If the user had a previous dehydrated device, it is removed.",
            "",
            "        Args:",
            "            user_id: the user that we are storing the device for",
            "            device_id: device id supplied by client",
            "            device_data: the dehydrated device information",
            "            initial_device_display_name: The display name to use for the device",
            "            keys_for_device: keys for the dehydrated device",
            "        Returns:",
            "            device id of the dehydrated device",
            "        \"\"\"",
            "        device_id = await self.check_device_registered(",
            "            user_id,",
            "            device_id,",
            "            initial_device_display_name,",
            "        )",
            "",
            "        time_now = self.clock.time_msec()",
            "",
            "        old_device_id = await self.store.store_dehydrated_device(",
            "            user_id, device_id, device_data, time_now, keys_for_device",
            "        )",
            "",
            "        if old_device_id is not None:",
            "            await self.delete_devices(user_id, [old_device_id])",
            "",
            "        return device_id",
            "",
            "    async def rehydrate_device(",
            "        self, user_id: str, access_token: str, device_id: str",
            "    ) -> dict:",
            "        \"\"\"Process a rehydration request from the user.",
            "",
            "        Args:",
            "            user_id: the user who is rehydrating the device",
            "            access_token: the access token used for the request",
            "            device_id: the ID of the device that will be rehydrated",
            "        Returns:",
            "            a dict containing {\"success\": True}",
            "        \"\"\"",
            "        success = await self.store.remove_dehydrated_device(user_id, device_id)",
            "",
            "        if not success:",
            "            raise errors.NotFoundError()",
            "",
            "        # If the dehydrated device was successfully deleted (the device ID",
            "        # matched the stored dehydrated device), then modify the access",
            "        # token and refresh token to use the dehydrated device's ID and",
            "        # copy the old device display name to the dehydrated device,",
            "        # and destroy the old device ID",
            "        old_device_id = await self.store.set_device_for_access_token(",
            "            access_token, device_id",
            "        )",
            "        await self.store.set_device_for_refresh_token(user_id, old_device_id, device_id)",
            "        old_device = await self.store.get_device(user_id, old_device_id)",
            "        if old_device is None:",
            "            raise errors.NotFoundError()",
            "        await self.store.update_device(user_id, device_id, old_device[\"display_name\"])",
            "        # can't call self.delete_device because that will clobber the",
            "        # access token so call the storage layer directly",
            "        await self.store.delete_devices(user_id, [old_device_id])",
            "        await self.store.delete_e2e_keys_by_device(",
            "            user_id=user_id, device_id=old_device_id",
            "        )",
            "",
            "        # tell everyone that the old device is gone and that the dehydrated",
            "        # device has a new display name",
            "        await self.notify_device_update(user_id, [old_device_id, device_id])",
            "",
            "        return {\"success\": True}",
            "",
            "    async def delete_dehydrated_device(self, user_id: str, device_id: str) -> None:",
            "        \"\"\"",
            "        Delete a stored dehydrated device.",
            "",
            "        Args:",
            "            user_id: the user_id to delete the device from",
            "            device_id: id of the dehydrated device to delete",
            "        \"\"\"",
            "        success = await self.store.remove_dehydrated_device(user_id, device_id)",
            "",
            "        if not success:",
            "            raise errors.NotFoundError()",
            "",
            "        await self.delete_devices(user_id, [device_id])",
            "        await self.store.delete_e2e_keys_by_device(user_id=user_id, device_id=device_id)",
            "",
            "    @wrap_as_background_process(\"_handle_new_device_update_async\")",
            "    async def _handle_new_device_update_async(self) -> None:",
            "        \"\"\"Called when we have a new local device list update that we need to",
            "        send out over federation.",
            "",
            "        This happens in the background so as not to block the original request",
            "        that generated the device update.",
            "        \"\"\"",
            "        if self._handle_new_device_update_is_processing:",
            "            self._handle_new_device_update_new_data = True",
            "            return",
            "",
            "        self._handle_new_device_update_is_processing = True",
            "",
            "        # The stream ID we processed previous iteration (if any), and the set of",
            "        # hosts we've already poked about for this update. This is so that we",
            "        # don't poke the same remote server about the same update repeatedly.",
            "        current_stream_id = None",
            "        hosts_already_sent_to: Set[str] = set()",
            "",
            "        try:",
            "            stream_id, room_id = await self.store.get_device_change_last_converted_pos()",
            "",
            "            while True:",
            "                self._handle_new_device_update_new_data = False",
            "                max_stream_id = self.store.get_device_stream_token()",
            "                rows = await self.store.get_uncoverted_outbound_room_pokes(",
            "                    stream_id, room_id",
            "                )",
            "                if not rows:",
            "                    # If the DB returned nothing then there is nothing left to",
            "                    # do, *unless* a new device list update happened during the",
            "                    # DB query.",
            "",
            "                    # Advance `(stream_id, room_id)`.",
            "                    # `max_stream_id` comes from *before* the query for unconverted",
            "                    # rows, which means that any unconverted rows must have a larger",
            "                    # stream ID.",
            "                    if max_stream_id > stream_id:",
            "                        stream_id, room_id = max_stream_id, \"\"",
            "                        await self.store.set_device_change_last_converted_pos(",
            "                            stream_id, room_id",
            "                        )",
            "                    else:",
            "                        assert max_stream_id == stream_id",
            "                        # Avoid moving `room_id` backwards.",
            "",
            "                    if self._handle_new_device_update_new_data:",
            "                        continue",
            "                    else:",
            "                        return",
            "",
            "                for user_id, device_id, room_id, stream_id, opentracing_context in rows:",
            "                    hosts = set()",
            "",
            "                    # Ignore any users that aren't ours",
            "                    if self.hs.is_mine_id(user_id):",
            "                        hosts = set(",
            "                            await self._storage_controllers.state.get_current_hosts_in_room_or_partial_state_approximation(",
            "                                room_id",
            "                            )",
            "                        )",
            "                        hosts.discard(self.server_name)",
            "                        # For rooms with partial state, `hosts` is merely an",
            "                        # approximation. When we transition to a full state room, we",
            "                        # will have to send out device list updates to any servers we",
            "                        # missed.",
            "",
            "                    # Check if we've already sent this update to some hosts",
            "                    if current_stream_id == stream_id:",
            "                        hosts -= hosts_already_sent_to",
            "",
            "                    await self.store.add_device_list_outbound_pokes(",
            "                        user_id=user_id,",
            "                        device_id=device_id,",
            "                        room_id=room_id,",
            "                        hosts=hosts,",
            "                        context=opentracing_context,",
            "                    )",
            "",
            "                    # Notify replication that we've updated the device list stream.",
            "                    self.notifier.notify_replication()",
            "",
            "                    if hosts:",
            "                        logger.info(",
            "                            \"Sending device list update notif for %r to: %r\",",
            "                            user_id,",
            "                            hosts,",
            "                        )",
            "                        await self.federation_sender.send_device_messages(",
            "                            hosts, immediate=False",
            "                        )",
            "                        # TODO: when called, this isn't in a logging context.",
            "                        # This leads to log spam, sentry event spam, and massive",
            "                        # memory usage.",
            "                        # See https://github.com/matrix-org/synapse/issues/12552.",
            "                        # log_kv(",
            "                        #     {\"message\": \"sent device update to host\", \"host\": host}",
            "                        # )",
            "",
            "                    if current_stream_id != stream_id:",
            "                        # Clear the set of hosts we've already sent to as we're",
            "                        # processing a new update.",
            "                        hosts_already_sent_to.clear()",
            "",
            "                    hosts_already_sent_to.update(hosts)",
            "                    current_stream_id = stream_id",
            "",
            "                # Advance `(stream_id, room_id)`.",
            "                _, _, room_id, stream_id, _ = rows[-1]",
            "                await self.store.set_device_change_last_converted_pos(",
            "                    stream_id, room_id",
            "                )",
            "",
            "        finally:",
            "            self._handle_new_device_update_is_processing = False",
            "",
            "    async def handle_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Handles sending appropriate device list updates in a room that has",
            "        gone from partial to full state.",
            "        \"\"\"",
            "",
            "        # We defer to the device list updater to handle pending remote device",
            "        # list updates.",
            "        await self.device_list_updater.handle_room_un_partial_stated(room_id)",
            "",
            "        # Replay local updates.",
            "        (",
            "            join_event_id,",
            "            device_lists_stream_id,",
            "        ) = await self.store.get_join_event_id_and_device_lists_stream_id_for_partial_state(",
            "            room_id",
            "        )",
            "",
            "        # Get the local device list changes that have happened in the room since",
            "        # we started joining. If there are no updates there's nothing left to do.",
            "        changes = await self.store.get_device_list_changes_in_room(",
            "            room_id, device_lists_stream_id",
            "        )",
            "        local_changes = {(u, d) for u, d in changes if self.hs.is_mine_id(u)}",
            "        if not local_changes:",
            "            return",
            "",
            "        # Note: We have persisted the full state at this point, we just haven't",
            "        # cleared the `partial_room` flag.",
            "        join_state_ids = await self._state_storage.get_state_ids_for_event(",
            "            join_event_id, await_full_state=False",
            "        )",
            "        current_state_ids = await self.store.get_partial_current_state_ids(room_id)",
            "",
            "        # Now we need to work out all servers that might have been in the room",
            "        # at any point during our join.",
            "",
            "        # First we look for any membership states that have changed between the",
            "        # initial join and now...",
            "        all_keys = set(join_state_ids)",
            "        all_keys.update(current_state_ids)",
            "",
            "        potentially_changed_hosts = set()",
            "        for etype, state_key in all_keys:",
            "            if etype != EventTypes.Member:",
            "                continue",
            "",
            "            prev = join_state_ids.get((etype, state_key))",
            "            current = current_state_ids.get((etype, state_key))",
            "",
            "            if prev != current:",
            "                potentially_changed_hosts.add(get_domain_from_id(state_key))",
            "",
            "        # ... then we add all the hosts that are currently joined to the room...",
            "        current_hosts_in_room = await self.store.get_current_hosts_in_room(room_id)",
            "        potentially_changed_hosts.update(current_hosts_in_room)",
            "",
            "        # ... and finally we remove any hosts that we were told about, as we",
            "        # will have sent device list updates to those hosts when they happened.",
            "        known_hosts_at_join = await self.store.get_partial_state_servers_at_join(",
            "            room_id",
            "        )",
            "        assert known_hosts_at_join is not None",
            "        potentially_changed_hosts.difference_update(known_hosts_at_join)",
            "",
            "        potentially_changed_hosts.discard(self.server_name)",
            "",
            "        if not potentially_changed_hosts:",
            "            # Nothing to do.",
            "            return",
            "",
            "        logger.info(",
            "            \"Found %d changed hosts to send device list updates to\",",
            "            len(potentially_changed_hosts),",
            "        )",
            "",
            "        for user_id, device_id in local_changes:",
            "            await self.store.add_device_list_outbound_pokes(",
            "                user_id=user_id,",
            "                device_id=device_id,",
            "                room_id=room_id,",
            "                hosts=potentially_changed_hosts,",
            "                context=None,",
            "            )",
            "",
            "        # Notify things that device lists need to be sent out.",
            "        self.notifier.notify_replication()",
            "        await self.federation_sender.send_device_messages(",
            "            potentially_changed_hosts, immediate=False",
            "        )",
            "",
            "",
            "def _update_device_from_client_ips(",
            "    device: JsonDict, client_ips: Mapping[Tuple[str, str], DeviceLastConnectionInfo]",
            ") -> None:",
            "    ip = client_ips.get((device[\"user_id\"], device[\"device_id\"]))",
            "    device.update(",
            "        {",
            "            \"last_seen_user_agent\": ip.user_agent if ip else None,",
            "            \"last_seen_ts\": ip.last_seen if ip else None,",
            "            \"last_seen_ip\": ip.ip if ip else None,",
            "        }",
            "    )",
            "",
            "",
            "class DeviceListWorkerUpdater:",
            "    \"Handles incoming device list updates from federation and contacts the main process over replication\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        from synapse.replication.http.devices import (",
            "            ReplicationMultiUserDevicesResyncRestServlet,",
            "        )",
            "",
            "        self._multi_user_device_resync_client = (",
            "            ReplicationMultiUserDevicesResyncRestServlet.make_client(hs)",
            "        )",
            "",
            "    async def multi_user_device_resync(",
            "        self, user_ids: List[str], mark_failed_as_stale: bool = True",
            "    ) -> Dict[str, Optional[JsonMapping]]:",
            "        \"\"\"",
            "        Like `user_device_resync` but operates on multiple users **from the same origin**",
            "        at once.",
            "",
            "        Returns:",
            "            Dict from User ID to the same Dict as `user_device_resync`.",
            "        \"\"\"",
            "        # mark_failed_as_stale is not sent. Ensure this doesn't break expectations.",
            "        assert mark_failed_as_stale",
            "",
            "        if not user_ids:",
            "            # Shortcut empty requests",
            "            return {}",
            "",
            "        return await self._multi_user_device_resync_client(user_ids=user_ids)",
            "",
            "",
            "class DeviceListUpdater(DeviceListWorkerUpdater):",
            "    \"Handles incoming device list updates from federation and updates the DB\"",
            "",
            "    def __init__(self, hs: \"HomeServer\", device_handler: DeviceHandler):",
            "        self.store = hs.get_datastores().main",
            "        self.federation = hs.get_federation_client()",
            "        self.clock = hs.get_clock()",
            "        self.device_handler = device_handler",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self._remote_edu_linearizer = Linearizer(name=\"remote_device_list\")",
            "        self._resync_linearizer = Linearizer(name=\"remote_device_resync\")",
            "",
            "        # user_id -> list of updates waiting to be handled.",
            "        self._pending_updates: Dict[",
            "            str, List[Tuple[str, str, Iterable[str], JsonDict]]",
            "        ] = {}",
            "",
            "        # Recently seen stream ids. We don't bother keeping these in the DB,",
            "        # but they're useful to have them about to reduce the number of spurious",
            "        # resyncs.",
            "        self._seen_updates: ExpiringCache[str, Set[str]] = ExpiringCache(",
            "            cache_name=\"device_update_edu\",",
            "            clock=self.clock,",
            "            max_len=10000,",
            "            expiry_ms=30 * 60 * 1000,",
            "            iterable=True,",
            "        )",
            "",
            "        # Attempt to resync out of sync device lists every 30s.",
            "        self._resync_retry_in_progress = False",
            "        self.clock.looping_call(",
            "            run_as_background_process,",
            "            30 * 1000,",
            "            func=self._maybe_retry_device_resync,",
            "            desc=\"_maybe_retry_device_resync\",",
            "        )",
            "",
            "    @trace",
            "    async def incoming_device_list_update(",
            "        self, origin: str, edu_content: JsonDict",
            "    ) -> None:",
            "        \"\"\"Called on incoming device list update from federation. Responsible",
            "        for parsing the EDU and adding to pending updates list.",
            "        \"\"\"",
            "",
            "        set_tag(\"origin\", origin)",
            "        set_tag(\"edu_content\", str(edu_content))",
            "        user_id = edu_content.pop(\"user_id\")",
            "        device_id = edu_content.pop(\"device_id\")",
            "        stream_id = str(edu_content.pop(\"stream_id\"))  # They may come as ints",
            "        prev_ids = edu_content.pop(\"prev_id\", [])",
            "        if not isinstance(prev_ids, list):",
            "            raise SynapseError(",
            "                400, \"Device list update had an invalid 'prev_ids' field\"",
            "            )",
            "        prev_ids = [str(p) for p in prev_ids]  # They may come as ints",
            "",
            "        if get_domain_from_id(user_id) != origin:",
            "            # TODO: Raise?",
            "            logger.warning(",
            "                \"Got device list update edu for %r/%r from %r\",",
            "                user_id,",
            "                device_id,",
            "                origin,",
            "            )",
            "",
            "            set_tag(\"error\", True)",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Got a device list update edu from a user and \"",
            "                    \"device which does not match the origin of the request.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            return",
            "",
            "        # Check if we are partially joining any rooms. If so we need to store",
            "        # all device list updates so that we can handle them correctly once we",
            "        # know who is in the room.",
            "        # TODO(faster_joins): this fetches and processes a bunch of data that we don't",
            "        # use. Could be replaced by a tighter query e.g.",
            "        #   SELECT EXISTS(SELECT 1 FROM partial_state_rooms)",
            "        partial_rooms = await self.store.get_partial_state_room_resync_info()",
            "        if partial_rooms:",
            "            await self.store.add_remote_device_list_to_pending(",
            "                user_id,",
            "                device_id,",
            "            )",
            "            self._notifier.notify_replication()",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "        if not room_ids:",
            "            # We don't share any rooms with this user. Ignore update, as we",
            "            # probably won't get any further updates.",
            "            set_tag(\"error\", True)",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Got an update from a user for which \"",
            "                    \"we don't share any rooms\",",
            "                    \"other user_id\": user_id,",
            "                }",
            "            )",
            "            logger.warning(",
            "                \"Got device list update edu for %r/%r, but don't share a room\",",
            "                user_id,",
            "                device_id,",
            "            )",
            "            return",
            "",
            "        logger.debug(\"Received device list update for %r/%r\", user_id, device_id)",
            "",
            "        self._pending_updates.setdefault(user_id, []).append(",
            "            (device_id, stream_id, prev_ids, edu_content)",
            "        )",
            "",
            "        await self._handle_device_updates(user_id)",
            "",
            "    @measure_func(\"_incoming_device_list_update\")",
            "    async def _handle_device_updates(self, user_id: str) -> None:",
            "        \"Actually handle pending updates.\"",
            "",
            "        async with self._remote_edu_linearizer.queue(user_id):",
            "            pending_updates = self._pending_updates.pop(user_id, [])",
            "            if not pending_updates:",
            "                # This can happen since we batch updates",
            "                return",
            "",
            "            for device_id, stream_id, prev_ids, _ in pending_updates:",
            "                logger.debug(",
            "                    \"Handling update %r/%r, ID: %r, prev: %r \",",
            "                    user_id,",
            "                    device_id,",
            "                    stream_id,",
            "                    prev_ids,",
            "                )",
            "",
            "            # Given a list of updates we check if we need to resync. This",
            "            # happens if we've missed updates.",
            "            resync = await self._need_to_do_resync(user_id, pending_updates)",
            "",
            "            if logger.isEnabledFor(logging.INFO):",
            "                logger.info(",
            "                    \"Received device list update for %s, requiring resync: %s. Devices: %s\",",
            "                    user_id,",
            "                    resync,",
            "                    \", \".join(u[0] for u in pending_updates),",
            "                )",
            "",
            "            if resync:",
            "                # We mark as stale up front in case we get restarted.",
            "                await self.store.mark_remote_users_device_caches_as_stale([user_id])",
            "                run_as_background_process(",
            "                    \"_maybe_retry_device_resync\",",
            "                    self.multi_user_device_resync,",
            "                    [user_id],",
            "                    False,",
            "                )",
            "            else:",
            "                # Simply update the single device, since we know that is the only",
            "                # change (because of the single prev_id matching the current cache)",
            "                for device_id, stream_id, _, content in pending_updates:",
            "                    await self.store.update_remote_device_list_cache_entry(",
            "                        user_id, device_id, content, stream_id",
            "                    )",
            "",
            "                await self.device_handler.notify_device_update(",
            "                    user_id, [device_id for device_id, _, _, _ in pending_updates]",
            "                )",
            "",
            "                self._seen_updates.setdefault(user_id, set()).update(",
            "                    stream_id for _, stream_id, _, _ in pending_updates",
            "                )",
            "",
            "    async def _need_to_do_resync(",
            "        self, user_id: str, updates: Iterable[Tuple[str, str, Iterable[str], JsonDict]]",
            "    ) -> bool:",
            "        \"\"\"Given a list of updates for a user figure out if we need to do a full",
            "        resync, or whether we have enough data that we can just apply the delta.",
            "        \"\"\"",
            "        seen_updates: Set[str] = self._seen_updates.get(user_id, set())",
            "",
            "        extremity = await self.store.get_device_list_last_stream_id_for_remote(user_id)",
            "",
            "        logger.debug(\"Current extremity for %r: %r\", user_id, extremity)",
            "",
            "        stream_id_in_updates = set()  # stream_ids in updates list",
            "        for _, stream_id, prev_ids, _ in updates:",
            "            if not prev_ids:",
            "                # We always do a resync if there are no previous IDs",
            "                return True",
            "",
            "            for prev_id in prev_ids:",
            "                if prev_id == extremity:",
            "                    continue",
            "                elif prev_id in seen_updates:",
            "                    continue",
            "                elif prev_id in stream_id_in_updates:",
            "                    continue",
            "                else:",
            "                    return True",
            "",
            "            stream_id_in_updates.add(stream_id)",
            "",
            "        return False",
            "",
            "    @trace",
            "    async def _maybe_retry_device_resync(self) -> None:",
            "        \"\"\"Retry to resync device lists that are out of sync, except if another retry is",
            "        in progress.",
            "        \"\"\"",
            "        if self._resync_retry_in_progress:",
            "            return",
            "",
            "        try:",
            "            # Prevent another call of this function to retry resyncing device lists so",
            "            # we don't send too many requests.",
            "            self._resync_retry_in_progress = True",
            "            # Get all of the users that need resyncing.",
            "            need_resync = await self.store.get_user_ids_requiring_device_list_resync()",
            "",
            "            # Filter out users whose host is marked as \"down\" up front.",
            "            hosts = await filter_destinations_by_retry_limiter(",
            "                {get_domain_from_id(u) for u in need_resync}, self.clock, self.store",
            "            )",
            "            hosts = set(hosts)",
            "",
            "            # Iterate over the set of user IDs.",
            "            for user_id in need_resync:",
            "                if get_domain_from_id(user_id) not in hosts:",
            "                    continue",
            "",
            "                try:",
            "                    # Try to resync the current user's devices list.",
            "                    result = (await self.multi_user_device_resync([user_id], False))[",
            "                        user_id",
            "                    ]",
            "",
            "                    # user_device_resync only returns a result if it managed to",
            "                    # successfully resync and update the database. Updating the table",
            "                    # of users requiring resync isn't necessary here as",
            "                    # user_device_resync already does it (through",
            "                    # self.store.update_remote_device_list_cache).",
            "                    if result:",
            "                        logger.debug(",
            "                            \"Successfully resynced the device list for %s\",",
            "                            user_id,",
            "                        )",
            "                except Exception as e:",
            "                    # If there was an issue resyncing this user, e.g. if the remote",
            "                    # server sent a malformed result, just log the error instead of",
            "                    # aborting all the subsequent resyncs.",
            "                    logger.debug(",
            "                        \"Could not resync the device list for %s: %s\",",
            "                        user_id,",
            "                        e,",
            "                    )",
            "        finally:",
            "            # Allow future calls to retry resyncinc out of sync device lists.",
            "            self._resync_retry_in_progress = False",
            "",
            "    async def multi_user_device_resync(",
            "        self, user_ids: List[str], mark_failed_as_stale: bool = True",
            "    ) -> Dict[str, Optional[JsonMapping]]:",
            "        \"\"\"",
            "        Like `user_device_resync` but operates on multiple users **from the same origin**",
            "        at once.",
            "",
            "        Returns:",
            "            Dict from User ID to the same Dict as `user_device_resync`.",
            "        \"\"\"",
            "        if not user_ids:",
            "            return {}",
            "",
            "        origins = {UserID.from_string(user_id).domain for user_id in user_ids}",
            "",
            "        if len(origins) != 1:",
            "            raise InvalidAPICallError(f\"Only one origin permitted, got {origins!r}\")",
            "",
            "        result = {}",
            "        failed = set()",
            "        # TODO(Perf): Actually batch these up",
            "        for user_id in user_ids:",
            "            async with self._resync_linearizer.queue(user_id):",
            "                (",
            "                    user_result,",
            "                    user_failed,",
            "                ) = await self._user_device_resync_returning_failed(user_id)",
            "            result[user_id] = user_result",
            "            if user_failed:",
            "                failed.add(user_id)",
            "",
            "        if mark_failed_as_stale:",
            "            await self.store.mark_remote_users_device_caches_as_stale(failed)",
            "",
            "        return result",
            "",
            "    async def _user_device_resync_returning_failed(",
            "        self, user_id: str",
            "    ) -> Tuple[Optional[JsonMapping], bool]:",
            "        \"\"\"Fetches all devices for a user and updates the device cache with them.",
            "",
            "        Args:",
            "            user_id: The user's id whose device_list will be updated.",
            "        Returns:",
            "            - A dict with device info as under the \"devices\" in the result of this",
            "              request:",
            "              https://matrix.org/docs/spec/server_server/r0.1.2#get-matrix-federation-v1-user-devices-userid",
            "              None when we weren't able to fetch the device info for some reason,",
            "              e.g. due to a connection problem.",
            "            - True iff the resync failed and the device list should be marked as stale.",
            "        \"\"\"",
            "        # Check that we haven't gone and fetched the devices since we last",
            "        # checked if we needed to resync these device lists.",
            "        if await self.store.get_users_whose_devices_are_cached([user_id]):",
            "            cached = await self.store.get_cached_devices_for_user(user_id)",
            "            return cached, False",
            "",
            "        logger.debug(\"Attempting to resync the device list for %s\", user_id)",
            "        log_kv({\"message\": \"Doing resync to update device list.\"})",
            "        # Fetch all devices for the user.",
            "        origin = get_domain_from_id(user_id)",
            "        try:",
            "            result = await self.federation.query_user_devices(origin, user_id)",
            "        except NotRetryingDestination:",
            "            return None, True",
            "        except (RequestSendFailed, HttpResponseException) as e:",
            "            logger.warning(",
            "                \"Failed to handle device list update for %s: %s\",",
            "                user_id,",
            "                e,",
            "            )",
            "",
            "            # We abort on exceptions rather than accepting the update",
            "            # as otherwise synapse will 'forget' that its device list",
            "            # is out of date. If we bail then we will retry the resync",
            "            # next time we get a device list update for this user_id.",
            "            # This makes it more likely that the device lists will",
            "            # eventually become consistent.",
            "            return None, True",
            "        except FederationDeniedError as e:",
            "            set_tag(\"error\", True)",
            "            log_kv({\"reason\": \"FederationDeniedError\"})",
            "            logger.info(e)",
            "            return None, False",
            "        except Exception as e:",
            "            set_tag(\"error\", True)",
            "            log_kv(",
            "                {\"message\": \"Exception raised by federation request\", \"exception\": e}",
            "            )",
            "            logger.exception(\"Failed to handle device list update for %s\", user_id)",
            "",
            "            return None, True",
            "        log_kv({\"result\": result})",
            "        stream_id = result[\"stream_id\"]",
            "        devices = result[\"devices\"]",
            "",
            "        # Get the master key and the self-signing key for this user if provided in the",
            "        # response (None if not in the response).",
            "        # The response will not contain the user signing key, as this key is only used by",
            "        # its owner, thus it doesn't make sense to send it over federation.",
            "        master_key = result.get(\"master_key\")",
            "        self_signing_key = result.get(\"self_signing_key\")",
            "",
            "        ignore_devices = False",
            "        # If the remote server has more than ~1000 devices for this user",
            "        # we assume that something is going horribly wrong (e.g. a bot",
            "        # that logs in and creates a new device every time it tries to",
            "        # send a message).  Maintaining lots of devices per user in the",
            "        # cache can cause serious performance issues as if this request",
            "        # takes more than 60s to complete, internal replication from the",
            "        # inbound federation worker to the synapse master may time out",
            "        # causing the inbound federation to fail and causing the remote",
            "        # server to retry, causing a DoS.  So in this scenario we give",
            "        # up on storing the total list of devices and only handle the",
            "        # delta instead.",
            "        if len(devices) > 1000:",
            "            logger.warning(",
            "                \"Ignoring device list snapshot for %s as it has >1K devs (%d)\",",
            "                user_id,",
            "                len(devices),",
            "            )",
            "            devices = []",
            "            ignore_devices = True",
            "        else:",
            "            prev_stream_id = await self.store.get_device_list_last_stream_id_for_remote(",
            "                user_id",
            "            )",
            "            cached_devices = await self.store.get_cached_devices_for_user(user_id)",
            "",
            "            # To ensure that a user with no devices is cached, we skip the resync only",
            "            # if we have a stream_id from previously writing a cache entry.",
            "            if prev_stream_id is not None and cached_devices == {",
            "                d[\"device_id\"]: d for d in devices",
            "            }:",
            "                logging.info(",
            "                    \"Skipping device list resync for %s, as our cache matches already\",",
            "                    user_id,",
            "                )",
            "                devices = []",
            "                ignore_devices = True",
            "",
            "        for device in devices:",
            "            logger.debug(",
            "                \"Handling resync update %r/%r, ID: %r\",",
            "                user_id,",
            "                device[\"device_id\"],",
            "                stream_id,",
            "            )",
            "",
            "        if not ignore_devices:",
            "            await self.store.update_remote_device_list_cache(",
            "                user_id, devices, stream_id",
            "            )",
            "        # mark the cache as valid, whether or not we actually processed any device",
            "        # list updates.",
            "        await self.store.mark_remote_user_device_cache_as_valid(user_id)",
            "        device_ids = [device[\"device_id\"] for device in devices]",
            "",
            "        # Handle cross-signing keys.",
            "        cross_signing_device_ids = await self.process_cross_signing_key_update(",
            "            user_id,",
            "            master_key,",
            "            self_signing_key,",
            "        )",
            "        device_ids = device_ids + cross_signing_device_ids",
            "",
            "        if device_ids:",
            "            await self.device_handler.notify_device_update(user_id, device_ids)",
            "",
            "        # We clobber the seen updates since we've re-synced from a given",
            "        # point.",
            "        self._seen_updates[user_id] = {stream_id}",
            "",
            "        return result, False",
            "",
            "    async def process_cross_signing_key_update(",
            "        self,",
            "        user_id: str,",
            "        master_key: Optional[JsonDict],",
            "        self_signing_key: Optional[JsonDict],",
            "    ) -> List[str]:",
            "        \"\"\"Process the given new master and self-signing key for the given remote user.",
            "",
            "        Args:",
            "            user_id: The ID of the user these keys are for.",
            "            master_key: The dict of the cross-signing master key as returned by the",
            "                remote server.",
            "            self_signing_key: The dict of the cross-signing self-signing key as returned",
            "                by the remote server.",
            "",
            "        Return:",
            "            The device IDs for the given keys.",
            "        \"\"\"",
            "        device_ids = []",
            "",
            "        current_keys_map = await self.store.get_e2e_cross_signing_keys_bulk([user_id])",
            "        current_keys = current_keys_map.get(user_id) or {}",
            "",
            "        if master_key and master_key != current_keys.get(\"master\"):",
            "            await self.store.set_e2e_cross_signing_key(user_id, \"master\", master_key)",
            "            _, verify_key = get_verify_key_from_cross_signing_key(master_key)",
            "            # verify_key is a VerifyKey from signedjson, which uses",
            "            # .version to denote the portion of the key ID after the",
            "            # algorithm and colon, which is the device ID",
            "            device_ids.append(verify_key.version)",
            "        if self_signing_key and self_signing_key != current_keys.get(\"self_signing\"):",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user_id, \"self_signing\", self_signing_key",
            "            )",
            "            _, verify_key = get_verify_key_from_cross_signing_key(self_signing_key)",
            "            device_ids.append(verify_key.version)",
            "",
            "        return device_ids",
            "",
            "    async def handle_room_un_partial_stated(self, room_id: str) -> None:",
            "        \"\"\"Handles sending appropriate device list updates in a room that has",
            "        gone from partial to full state.",
            "        \"\"\"",
            "",
            "        pending_updates = (",
            "            await self.store.get_pending_remote_device_list_updates_for_room(room_id)",
            "        )",
            "",
            "        for user_id, device_id in pending_updates:",
            "            logger.info(",
            "                \"Got pending device list update in room %s: %s / %s\",",
            "                room_id,",
            "                user_id,",
            "                device_id,",
            "            )",
            "            position = await self.store.add_device_change_to_streams(",
            "                user_id,",
            "                [device_id],",
            "                room_ids=[room_id],",
            "            )",
            "",
            "            if not position:",
            "                # This should only happen if there are no updates, which",
            "                # shouldn't happen when we've passed in a non-empty set of",
            "                # device IDs.",
            "                continue",
            "",
            "            self.device_handler.notifier.on_new_event(",
            "                StreamKeyType.DEVICE_LIST, position, rooms=[room_id]",
            "            )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.handlers.device.DeviceWorkerHandler.self"
        ]
    },
    "synapse/handlers/e2e_keys.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 542,
                "afterPatchRowNumber": 542,
                "PatchRowcode": "         device_keys_query: Dict[str, Optional[List[str]]] = query_body.get("
            },
            "1": {
                "beforePatchRowNumber": 543,
                "afterPatchRowNumber": 543,
                "PatchRowcode": "             \"device_keys\", {}"
            },
            "2": {
                "beforePatchRowNumber": 544,
                "afterPatchRowNumber": 544,
                "PatchRowcode": "         )"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 545,
                "PatchRowcode": "+        if any("
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 546,
                "PatchRowcode": "+            not self.is_mine(UserID.from_string(user_id))"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 547,
                "PatchRowcode": "+            for user_id in device_keys_query"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 548,
                "PatchRowcode": "+        ):"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 549,
                "PatchRowcode": "+            raise SynapseError(400, \"User is not hosted on this homeserver\")"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 550,
                "PatchRowcode": "+"
            },
            "9": {
                "beforePatchRowNumber": 545,
                "afterPatchRowNumber": 551,
                "PatchRowcode": "         res = await self.query_local_devices("
            },
            "10": {
                "beforePatchRowNumber": 546,
                "afterPatchRowNumber": 552,
                "PatchRowcode": "             device_keys_query,"
            },
            "11": {
                "beforePatchRowNumber": 547,
                "afterPatchRowNumber": 553,
                "PatchRowcode": "             include_displaynames=("
            }
        },
        "frontPatchFile": [
            "# Copyright 2016 OpenMarket Ltd",
            "# Copyright 2018-2019 New Vector Ltd",
            "# Copyright 2019 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Mapping, Optional, Tuple",
            "",
            "import attr",
            "from canonicaljson import encode_canonical_json",
            "from signedjson.key import VerifyKey, decode_verify_key_bytes",
            "from signedjson.sign import SignatureVerifyException, verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from twisted.internet import defer",
            "",
            "from synapse.api.constants import EduTypes",
            "from synapse.api.errors import CodeMessageException, Codes, NotFoundError, SynapseError",
            "from synapse.handlers.device import DeviceHandler",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.logging.opentracing import log_kv, set_tag, tag_args, trace",
            "from synapse.types import (",
            "    JsonDict,",
            "    JsonMapping,",
            "    UserID,",
            "    get_domain_from_id,",
            "    get_verify_key_from_cross_signing_key,",
            ")",
            "from synapse.util import json_decoder",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.cancellation import cancellable",
            "from synapse.util.retryutils import NotRetryingDestination",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class E2eKeysHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.store = hs.get_datastores().main",
            "        self.federation = hs.get_federation_client()",
            "        self.device_handler = hs.get_device_handler()",
            "        self._appservice_handler = hs.get_application_service_handler()",
            "        self.is_mine = hs.is_mine",
            "        self.clock = hs.get_clock()",
            "",
            "        federation_registry = hs.get_federation_registry()",
            "",
            "        is_master = hs.config.worker.worker_app is None",
            "        if is_master:",
            "            edu_updater = SigningKeyEduUpdater(hs)",
            "",
            "            # Only register this edu handler on master as it requires writing",
            "            # device updates to the db",
            "            federation_registry.register_edu_handler(",
            "                EduTypes.SIGNING_KEY_UPDATE,",
            "                edu_updater.incoming_signing_key_update,",
            "            )",
            "            # also handle the unstable version",
            "            # FIXME: remove this when enough servers have upgraded",
            "            federation_registry.register_edu_handler(",
            "                EduTypes.UNSTABLE_SIGNING_KEY_UPDATE,",
            "                edu_updater.incoming_signing_key_update,",
            "            )",
            "",
            "        # doesn't really work as part of the generic query API, because the",
            "        # query request requires an object POST, but we abuse the",
            "        # \"query handler\" interface.",
            "        federation_registry.register_query_handler(",
            "            \"client_keys\", self.on_federation_query_client_keys",
            "        )",
            "",
            "        # Limit the number of in-flight requests from a single device.",
            "        self._query_devices_linearizer = Linearizer(",
            "            name=\"query_devices\",",
            "            max_count=10,",
            "        )",
            "",
            "        self._query_appservices_for_otks = (",
            "            hs.config.experimental.msc3983_appservice_otk_claims",
            "        )",
            "        self._query_appservices_for_keys = (",
            "            hs.config.experimental.msc3984_appservice_key_query",
            "        )",
            "",
            "    @trace",
            "    @cancellable",
            "    async def query_devices(",
            "        self,",
            "        query_body: JsonDict,",
            "        timeout: int,",
            "        from_user_id: str,",
            "        from_device_id: Optional[str],",
            "    ) -> JsonDict:",
            "        \"\"\"Handle a device key query from a client",
            "",
            "        {",
            "            \"device_keys\": {",
            "                \"<user_id>\": [\"<device_id>\"]",
            "            }",
            "        }",
            "        ->",
            "        {",
            "            \"device_keys\": {",
            "                \"<user_id>\": {",
            "                    \"<device_id>\": {",
            "                        ...",
            "                    }",
            "                }",
            "            }",
            "        }",
            "",
            "        Args:",
            "            from_user_id: the user making the query.  This is used when",
            "                adding cross-signing signatures to limit what signatures users",
            "                can see.",
            "            from_device_id: the device making the query. This is used to limit",
            "                the number of in-flight queries at a time.",
            "        \"\"\"",
            "        async with self._query_devices_linearizer.queue((from_user_id, from_device_id)):",
            "            device_keys_query: Dict[str, List[str]] = query_body.get(\"device_keys\", {})",
            "",
            "            # separate users by domain.",
            "            # make a map from domain to user_id to device_ids",
            "            local_query = {}",
            "            remote_queries = {}",
            "",
            "            for user_id, device_ids in device_keys_query.items():",
            "                # we use UserID.from_string to catch invalid user ids",
            "                if self.is_mine(UserID.from_string(user_id)):",
            "                    local_query[user_id] = device_ids",
            "                else:",
            "                    remote_queries[user_id] = device_ids",
            "",
            "            set_tag(\"local_key_query\", str(local_query))",
            "            set_tag(\"remote_key_query\", str(remote_queries))",
            "",
            "            # First get local devices.",
            "            # A map of destination -> failure response.",
            "            failures: Dict[str, JsonDict] = {}",
            "            results = {}",
            "            if local_query:",
            "                local_result = await self.query_local_devices(local_query)",
            "                for user_id, keys in local_result.items():",
            "                    if user_id in local_query:",
            "                        results[user_id] = keys",
            "",
            "            # Get cached cross-signing keys",
            "            cross_signing_keys = await self.get_cross_signing_keys_from_cache(",
            "                device_keys_query, from_user_id",
            "            )",
            "",
            "            # Now attempt to get any remote devices from our local cache.",
            "            # A map of destination -> user ID -> device IDs.",
            "            remote_queries_not_in_cache: Dict[str, Dict[str, Iterable[str]]] = {}",
            "            if remote_queries:",
            "                user_ids = set()",
            "                user_and_device_ids: List[Tuple[str, str]] = []",
            "                for user_id, device_ids in remote_queries.items():",
            "                    if device_ids:",
            "                        user_and_device_ids.extend(",
            "                            (user_id, device_id) for device_id in device_ids",
            "                        )",
            "                    else:",
            "                        user_ids.add(user_id)",
            "",
            "                (",
            "                    user_ids_not_in_cache,",
            "                    remote_results,",
            "                ) = await self.store.get_user_devices_from_cache(",
            "                    user_ids, user_and_device_ids",
            "                )",
            "",
            "                # Check that the homeserver still shares a room with all cached users.",
            "                # Note that this check may be slightly racy when a remote user leaves a",
            "                # room after we have fetched their cached device list. In the worst case",
            "                # we will do extra federation queries for devices that we had cached.",
            "                cached_users = set(remote_results.keys())",
            "                valid_cached_users = (",
            "                    await self.store.get_users_server_still_shares_room_with(",
            "                        remote_results.keys()",
            "                    )",
            "                )",
            "                invalid_cached_users = cached_users - valid_cached_users",
            "                if invalid_cached_users:",
            "                    # Fix up results. If we get here, it means there was either a bug in",
            "                    # device list tracking, or we hit the race mentioned above.",
            "                    # TODO: In practice, this path is hit fairly often in existing",
            "                    #       deployments when clients query the keys of departed remote",
            "                    #       users. A background update to mark the appropriate device",
            "                    #       lists as unsubscribed is needed.",
            "                    #       https://github.com/matrix-org/synapse/issues/13651",
            "                    # Note that this currently introduces a failure mode when clients",
            "                    # are trying to decrypt old messages from a remote user whose",
            "                    # homeserver is no longer available. We may want to consider falling",
            "                    # back to the cached data when we fail to retrieve a device list",
            "                    # over federation for such remote users.",
            "                    user_ids_not_in_cache.update(invalid_cached_users)",
            "                    for invalid_user_id in invalid_cached_users:",
            "                        remote_results.pop(invalid_user_id)",
            "",
            "                for user_id, devices in remote_results.items():",
            "                    user_devices = results.setdefault(user_id, {})",
            "                    for device_id, device in devices.items():",
            "                        keys = device.get(\"keys\", None)",
            "                        device_display_name = device.get(\"device_display_name\", None)",
            "                        if keys:",
            "                            result = dict(keys)",
            "                            unsigned = result.setdefault(\"unsigned\", {})",
            "                            if device_display_name:",
            "                                unsigned[\"device_display_name\"] = device_display_name",
            "                            user_devices[device_id] = result",
            "",
            "                # check for missing cross-signing keys.",
            "                for user_id in remote_queries.keys():",
            "                    cached_cross_master = user_id in cross_signing_keys[\"master_keys\"]",
            "                    cached_cross_selfsigning = (",
            "                        user_id in cross_signing_keys[\"self_signing_keys\"]",
            "                    )",
            "",
            "                    # check if we are missing only one of cross-signing master or",
            "                    # self-signing key, but the other one is cached.",
            "                    # as we need both, this will issue a federation request.",
            "                    # if we don't have any of the keys, either the user doesn't have",
            "                    # cross-signing set up, or the cached device list",
            "                    # is not (yet) updated.",
            "                    if cached_cross_master ^ cached_cross_selfsigning:",
            "                        user_ids_not_in_cache.add(user_id)",
            "",
            "                # add those users to the list to fetch over federation.",
            "                for user_id in user_ids_not_in_cache:",
            "                    domain = get_domain_from_id(user_id)",
            "                    r = remote_queries_not_in_cache.setdefault(domain, {})",
            "                    r[user_id] = remote_queries[user_id]",
            "",
            "            # Now fetch any devices that we don't have in our cache",
            "            # TODO It might make sense to propagate cancellations into the",
            "            #      deferreds which are querying remote homeservers.",
            "            logger.debug(",
            "                \"%d destinations to query devices for\", len(remote_queries_not_in_cache)",
            "            )",
            "",
            "            async def _query(",
            "                destination_queries: Tuple[str, Dict[str, Iterable[str]]]",
            "            ) -> None:",
            "                destination, queries = destination_queries",
            "                return await self._query_devices_for_destination(",
            "                    results,",
            "                    cross_signing_keys,",
            "                    failures,",
            "                    destination,",
            "                    queries,",
            "                    timeout,",
            "                )",
            "",
            "            await concurrently_execute(",
            "                _query,",
            "                remote_queries_not_in_cache.items(),",
            "                10,",
            "                delay_cancellation=True,",
            "            )",
            "",
            "            return {\"device_keys\": results, \"failures\": failures, **cross_signing_keys}",
            "",
            "    @trace",
            "    async def _query_devices_for_destination(",
            "        self,",
            "        results: JsonDict,",
            "        cross_signing_keys: JsonDict,",
            "        failures: Dict[str, JsonDict],",
            "        destination: str,",
            "        destination_query: Dict[str, Iterable[str]],",
            "        timeout: int,",
            "    ) -> None:",
            "        \"\"\"This is called when we are querying the device list of a user on",
            "        a remote homeserver and their device list is not in the device list",
            "        cache. If we share a room with this user and we're not querying for",
            "        specific user we will update the cache with their device list.",
            "",
            "        Args:",
            "            results: A map from user ID to their device keys, which gets",
            "                updated with the newly fetched keys.",
            "            cross_signing_keys: Map from user ID to their cross signing keys,",
            "                which gets updated with the newly fetched keys.",
            "            failures: Map of destinations to failures that have occurred while",
            "                attempting to fetch keys.",
            "            destination: The remote server to query",
            "            destination_query: The query dict of devices to query the remote",
            "                server for.",
            "            timeout: The timeout for remote HTTP requests.",
            "        \"\"\"",
            "",
            "        # We first consider whether we wish to update the device list cache with",
            "        # the users device list. We want to track a user's devices when the",
            "        # authenticated user shares a room with the queried user and the query",
            "        # has not specified a particular device.",
            "        # If we update the cache for the queried user we remove them from further",
            "        # queries. We use the more efficient batched query_client_keys for all",
            "        # remaining users",
            "        user_ids_updated = []",
            "",
            "        # Perform a user device resync for each user only once and only as long as:",
            "        # - they have an empty device_list",
            "        # - they are in some rooms that this server can see",
            "        users_to_resync_devices = {",
            "            user_id",
            "            for (user_id, device_list) in destination_query.items()",
            "            if (not device_list) and (await self.store.get_rooms_for_user(user_id))",
            "        }",
            "",
            "        logger.debug(",
            "            \"%d users to resync devices for from destination %s\",",
            "            len(users_to_resync_devices),",
            "            destination,",
            "        )",
            "",
            "        try:",
            "            user_resync_results = (",
            "                await self.device_handler.device_list_updater.multi_user_device_resync(",
            "                    list(users_to_resync_devices)",
            "                )",
            "            )",
            "            for user_id in users_to_resync_devices:",
            "                resync_results = user_resync_results[user_id]",
            "",
            "                if resync_results is None:",
            "                    # TODO: It's weird that we'll store a failure against a",
            "                    #       destination, yet continue processing users from that",
            "                    #       destination.",
            "                    #       We might want to consider changing this, but for now",
            "                    #       I'm leaving it as I found it.",
            "                    failures[destination] = _exception_to_failure(",
            "                        ValueError(f\"Device resync failed for {user_id!r}\")",
            "                    )",
            "                    continue",
            "",
            "                # Add the device keys to the results.",
            "                user_devices = resync_results[\"devices\"]",
            "                user_results = results.setdefault(user_id, {})",
            "                for device in user_devices:",
            "                    user_results[device[\"device_id\"]] = device[\"keys\"]",
            "                user_ids_updated.append(user_id)",
            "",
            "                # Add any cross signing keys to the results.",
            "                master_key = resync_results.get(\"master_key\")",
            "                self_signing_key = resync_results.get(\"self_signing_key\")",
            "",
            "                if master_key:",
            "                    cross_signing_keys[\"master_keys\"][user_id] = master_key",
            "",
            "                if self_signing_key:",
            "                    cross_signing_keys[\"self_signing_keys\"][user_id] = self_signing_key",
            "        except Exception as e:",
            "            failures[destination] = _exception_to_failure(e)",
            "",
            "        if len(destination_query) == len(user_ids_updated):",
            "            # We've updated all the users in the query and we do not need to",
            "            # make any further remote calls.",
            "            return",
            "",
            "        # Remove all the users from the query which we have updated",
            "        for user_id in user_ids_updated:",
            "            destination_query.pop(user_id)",
            "",
            "        try:",
            "            remote_result = await self.federation.query_client_keys(",
            "                destination, {\"device_keys\": destination_query}, timeout=timeout",
            "            )",
            "",
            "            for user_id, keys in remote_result[\"device_keys\"].items():",
            "                if user_id in destination_query:",
            "                    results[user_id] = keys",
            "",
            "            if \"master_keys\" in remote_result:",
            "                for user_id, key in remote_result[\"master_keys\"].items():",
            "                    if user_id in destination_query:",
            "                        cross_signing_keys[\"master_keys\"][user_id] = key",
            "",
            "            if \"self_signing_keys\" in remote_result:",
            "                for user_id, key in remote_result[\"self_signing_keys\"].items():",
            "                    if user_id in destination_query:",
            "                        cross_signing_keys[\"self_signing_keys\"][user_id] = key",
            "",
            "        except Exception as e:",
            "            failure = _exception_to_failure(e)",
            "            failures[destination] = failure",
            "            set_tag(\"error\", True)",
            "            set_tag(\"reason\", str(failure))",
            "",
            "        return",
            "",
            "    @cancellable",
            "    async def get_cross_signing_keys_from_cache(",
            "        self, query: Iterable[str], from_user_id: Optional[str]",
            "    ) -> Dict[str, Dict[str, JsonMapping]]:",
            "        \"\"\"Get cross-signing keys for users from the database",
            "",
            "        Args:",
            "            query: an iterable of user IDs.  A dict whose keys",
            "                are user IDs satisfies this, so the query format used for",
            "                query_devices can be used here.",
            "            from_user_id: the user making the query.  This is used when",
            "                adding cross-signing signatures to limit what signatures users",
            "                can see.",
            "",
            "        Returns:",
            "            A map from (master_keys|self_signing_keys|user_signing_keys) -> user_id -> key",
            "        \"\"\"",
            "        master_keys = {}",
            "        self_signing_keys = {}",
            "        user_signing_keys = {}",
            "",
            "        user_ids = list(query)",
            "",
            "        keys = await self.store.get_e2e_cross_signing_keys_bulk(user_ids, from_user_id)",
            "",
            "        for user_id, user_info in keys.items():",
            "            if user_info is None:",
            "                continue",
            "            if \"master\" in user_info:",
            "                master_keys[user_id] = user_info[\"master\"]",
            "            if \"self_signing\" in user_info:",
            "                self_signing_keys[user_id] = user_info[\"self_signing\"]",
            "",
            "        # users can see other users' master and self-signing keys, but can",
            "        # only see their own user-signing keys",
            "        if from_user_id:",
            "            from_user_key = keys.get(from_user_id)",
            "            if from_user_key and \"user_signing\" in from_user_key:",
            "                user_signing_keys[from_user_id] = from_user_key[\"user_signing\"]",
            "",
            "        return {",
            "            \"master_keys\": master_keys,",
            "            \"self_signing_keys\": self_signing_keys,",
            "            \"user_signing_keys\": user_signing_keys,",
            "        }",
            "",
            "    @trace",
            "    @cancellable",
            "    async def query_local_devices(",
            "        self,",
            "        query: Mapping[str, Optional[List[str]]],",
            "        include_displaynames: bool = True,",
            "    ) -> Dict[str, Dict[str, dict]]:",
            "        \"\"\"Get E2E device keys for local users",
            "",
            "        Args:",
            "            query: map from user_id to a list",
            "                 of devices to query (None for all devices)",
            "            include_displaynames: Whether to include device displaynames in the returned",
            "                device details.",
            "",
            "        Returns:",
            "            A map from user_id -> device_id -> device details",
            "        \"\"\"",
            "        set_tag(\"local_query\", str(query))",
            "        local_query: List[Tuple[str, Optional[str]]] = []",
            "",
            "        result_dict: Dict[str, Dict[str, dict]] = {}",
            "        for user_id, device_ids in query.items():",
            "            # we use UserID.from_string to catch invalid user ids",
            "            if not self.is_mine(UserID.from_string(user_id)):",
            "                logger.warning(\"Request for keys for non-local user %s\", user_id)",
            "                log_kv(",
            "                    {",
            "                        \"message\": \"Requested a local key for a user which\"",
            "                        \" was not local to the homeserver\",",
            "                        \"user_id\": user_id,",
            "                    }",
            "                )",
            "                set_tag(\"error\", True)",
            "                raise SynapseError(400, \"Not a user here\")",
            "",
            "            if not device_ids:",
            "                local_query.append((user_id, None))",
            "            else:",
            "                for device_id in device_ids:",
            "                    local_query.append((user_id, device_id))",
            "",
            "            # make sure that each queried user appears in the result dict",
            "            result_dict[user_id] = {}",
            "",
            "        results = await self.store.get_e2e_device_keys_for_cs_api(",
            "            local_query, include_displaynames",
            "        )",
            "",
            "        # Check if the application services have any additional results.",
            "        if self._query_appservices_for_keys:",
            "            # Query the appservices for any keys.",
            "            appservice_results = await self._appservice_handler.query_keys(query)",
            "",
            "            # Merge results, overriding with what the appservice returned.",
            "            for user_id, devices in appservice_results.get(\"device_keys\", {}).items():",
            "                # Copy the appservice device info over the homeserver device info, but",
            "                # don't completely overwrite it.",
            "                results.setdefault(user_id, {}).update(devices)",
            "",
            "            # TODO Handle cross-signing keys.",
            "",
            "        # Build the result structure",
            "        for user_id, device_keys in results.items():",
            "            for device_id, device_info in device_keys.items():",
            "                result_dict[user_id][device_id] = device_info",
            "",
            "        log_kv(results)",
            "        return result_dict",
            "",
            "    async def on_federation_query_client_keys(",
            "        self, query_body: Dict[str, Dict[str, Optional[List[str]]]]",
            "    ) -> JsonDict:",
            "        \"\"\"Handle a device key query from a federated server:",
            "",
            "        Handles the path: GET /_matrix/federation/v1/users/keys/query",
            "",
            "        Args:",
            "            query_body: The body of the query request. Should contain a key",
            "                \"device_keys\" that map to a dictionary of user ID's -> list of",
            "                device IDs. If the list of device IDs is empty, all devices of",
            "                that user will be queried.",
            "",
            "        Returns:",
            "            A json dictionary containing the following:",
            "                - device_keys: A dictionary containing the requested device information.",
            "                - master_keys: An optional dictionary of user ID -> master cross-signing",
            "                   key info.",
            "                - self_signing_key: An optional dictionary of user ID -> self-signing",
            "                    key info.",
            "        \"\"\"",
            "        device_keys_query: Dict[str, Optional[List[str]]] = query_body.get(",
            "            \"device_keys\", {}",
            "        )",
            "        res = await self.query_local_devices(",
            "            device_keys_query,",
            "            include_displaynames=(",
            "                self.config.federation.allow_device_name_lookup_over_federation",
            "            ),",
            "        )",
            "",
            "        # add in the cross-signing keys",
            "        cross_signing_keys = await self.get_cross_signing_keys_from_cache(",
            "            device_keys_query, None",
            "        )",
            "",
            "        return {\"device_keys\": res, **cross_signing_keys}",
            "",
            "    async def claim_local_one_time_keys(",
            "        self,",
            "        local_query: List[Tuple[str, str, str, int]],",
            "        always_include_fallback_keys: bool,",
            "    ) -> Iterable[Dict[str, Dict[str, Dict[str, JsonDict]]]]:",
            "        \"\"\"Claim one time keys for local users.",
            "",
            "        1. Attempt to claim OTKs from the database.",
            "        2. Ask application services if they provide OTKs.",
            "        3. Attempt to fetch fallback keys from the database.",
            "",
            "        Args:",
            "            local_query: An iterable of tuples of (user ID, device ID, algorithm).",
            "            always_include_fallback_keys: True to always include fallback keys.",
            "",
            "        Returns:",
            "            An iterable of maps of user ID -> a map device ID -> a map of key ID -> JSON bytes.",
            "        \"\"\"",
            "",
            "        # Cap the number of OTKs that can be claimed at once to avoid abuse.",
            "        local_query = [",
            "            (user_id, device_id, algorithm, min(count, 5))",
            "            for user_id, device_id, algorithm, count in local_query",
            "        ]",
            "",
            "        otk_results, not_found = await self.store.claim_e2e_one_time_keys(local_query)",
            "",
            "        # If the application services have not provided any keys via the C-S",
            "        # API, query it directly for one-time keys.",
            "        if self._query_appservices_for_otks:",
            "            # TODO Should this query for fallback keys of uploaded OTKs if",
            "            #      always_include_fallback_keys is True? The MSC is ambiguous.",
            "            (",
            "                appservice_results,",
            "                not_found,",
            "            ) = await self._appservice_handler.claim_e2e_one_time_keys(not_found)",
            "        else:",
            "            appservice_results = {}",
            "",
            "        # Calculate which user ID / device ID / algorithm tuples to get fallback",
            "        # keys for. This can be either only missing results *or* all results",
            "        # (which don't already have a fallback key).",
            "        if always_include_fallback_keys:",
            "            # Build the fallback query as any part of the original query where",
            "            # the appservice didn't respond with a fallback key.",
            "            fallback_query = []",
            "",
            "            # Iterate each item in the original query and search the results",
            "            # from the appservice for that user ID / device ID. If it is found,",
            "            # check if any of the keys match the requested algorithm & are a",
            "            # fallback key.",
            "            for user_id, device_id, algorithm, _count in local_query:",
            "                # Check if the appservice responded for this query.",
            "                as_result = appservice_results.get(user_id, {}).get(device_id, {})",
            "                found_otk = False",
            "                for key_id, key_json in as_result.items():",
            "                    if key_id.startswith(f\"{algorithm}:\"):",
            "                        # A OTK or fallback key was found for this query.",
            "                        found_otk = True",
            "                        # A fallback key was found for this query, no need to",
            "                        # query further.",
            "                        if key_json.get(\"fallback\", False):",
            "                            break",
            "",
            "                else:",
            "                    # No fallback key was found from appservices, query for it.",
            "                    # Only mark the fallback key as used if no OTK was found",
            "                    # (from either the database or appservices).",
            "                    mark_as_used = not found_otk and not any(",
            "                        key_id.startswith(f\"{algorithm}:\")",
            "                        for key_id in otk_results.get(user_id, {})",
            "                        .get(device_id, {})",
            "                        .keys()",
            "                    )",
            "                    # Note that it doesn't make sense to request more than 1 fallback key",
            "                    # per (user_id, device_id, algorithm).",
            "                    fallback_query.append((user_id, device_id, algorithm, mark_as_used))",
            "",
            "        else:",
            "            # All fallback keys get marked as used.",
            "            fallback_query = [",
            "                # Note that it doesn't make sense to request more than 1 fallback key",
            "                # per (user_id, device_id, algorithm).",
            "                (user_id, device_id, algorithm, True)",
            "                for user_id, device_id, algorithm, count in not_found",
            "            ]",
            "",
            "        # For each user that does not have a one-time keys available, see if",
            "        # there is a fallback key.",
            "        fallback_results = await self.store.claim_e2e_fallback_keys(fallback_query)",
            "",
            "        # Return the results in order, each item from the input query should",
            "        # only appear once in the combined list.",
            "        return (otk_results, appservice_results, fallback_results)",
            "",
            "    @trace",
            "    async def claim_one_time_keys(",
            "        self,",
            "        query: Dict[str, Dict[str, Dict[str, int]]],",
            "        user: UserID,",
            "        timeout: Optional[int],",
            "        always_include_fallback_keys: bool,",
            "    ) -> JsonDict:",
            "        local_query: List[Tuple[str, str, str, int]] = []",
            "        remote_queries: Dict[str, Dict[str, Dict[str, Dict[str, int]]]] = {}",
            "",
            "        for user_id, one_time_keys in query.items():",
            "            # we use UserID.from_string to catch invalid user ids",
            "            if self.is_mine(UserID.from_string(user_id)):",
            "                for device_id, algorithms in one_time_keys.items():",
            "                    for algorithm, count in algorithms.items():",
            "                        local_query.append((user_id, device_id, algorithm, count))",
            "            else:",
            "                domain = get_domain_from_id(user_id)",
            "                remote_queries.setdefault(domain, {})[user_id] = one_time_keys",
            "",
            "        set_tag(\"local_key_query\", str(local_query))",
            "        set_tag(\"remote_key_query\", str(remote_queries))",
            "",
            "        results = await self.claim_local_one_time_keys(",
            "            local_query, always_include_fallback_keys",
            "        )",
            "",
            "        # A map of user ID -> device ID -> key ID -> key.",
            "        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}",
            "        for result in results:",
            "            for user_id, device_keys in result.items():",
            "                for device_id, keys in device_keys.items():",
            "                    for key_id, key in keys.items():",
            "                        json_result.setdefault(user_id, {}).setdefault(",
            "                            device_id, {}",
            "                        ).update({key_id: key})",
            "",
            "        # Remote failures.",
            "        failures: Dict[str, JsonDict] = {}",
            "",
            "        @trace",
            "        async def claim_client_keys(destination: str) -> None:",
            "            set_tag(\"destination\", destination)",
            "            device_keys = remote_queries[destination]",
            "            try:",
            "                remote_result = await self.federation.claim_client_keys(",
            "                    user, destination, device_keys, timeout=timeout",
            "                )",
            "                for user_id, keys in remote_result[\"one_time_keys\"].items():",
            "                    if user_id in device_keys:",
            "                        json_result[user_id] = keys",
            "",
            "            except Exception as e:",
            "                failure = _exception_to_failure(e)",
            "                failures[destination] = failure",
            "                set_tag(\"error\", True)",
            "                set_tag(\"reason\", str(failure))",
            "",
            "        await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [",
            "                    run_in_background(claim_client_keys, destination)",
            "                    for destination in remote_queries",
            "                ],",
            "                consumeErrors=True,",
            "            )",
            "        )",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        log_kv({\"one_time_keys\": json_result, \"failures\": failures})",
            "        return {\"one_time_keys\": json_result, \"failures\": failures}",
            "",
            "    @tag_args",
            "    async def upload_keys_for_user(",
            "        self, user_id: str, device_id: str, keys: JsonDict",
            "    ) -> JsonDict:",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        time_now = self.clock.time_msec()",
            "",
            "        # TODO: Validate the JSON to make sure it has the right keys.",
            "        device_keys = keys.get(\"device_keys\", None)",
            "        if device_keys:",
            "            logger.info(",
            "                \"Updating device_keys for device %r for user %s at %d\",",
            "                device_id,",
            "                user_id,",
            "                time_now,",
            "            )",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Updating device_keys for user.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            # TODO: Sign the JSON with the server key",
            "            changed = await self.store.set_e2e_device_keys(",
            "                user_id, device_id, time_now, device_keys",
            "            )",
            "            if changed:",
            "                # Only notify about device updates *if* the keys actually changed",
            "                await self.device_handler.notify_device_update(user_id, [device_id])",
            "        else:",
            "            log_kv({\"message\": \"Not updating device_keys for user\", \"user_id\": user_id})",
            "        one_time_keys = keys.get(\"one_time_keys\", None)",
            "        if one_time_keys:",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Updating one_time_keys for device.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            await self._upload_one_time_keys_for_user(",
            "                user_id, device_id, time_now, one_time_keys",
            "            )",
            "        else:",
            "            log_kv(",
            "                {\"message\": \"Did not update one_time_keys\", \"reason\": \"no keys given\"}",
            "            )",
            "        fallback_keys = keys.get(\"fallback_keys\") or keys.get(",
            "            \"org.matrix.msc2732.fallback_keys\"",
            "        )",
            "        if fallback_keys and isinstance(fallback_keys, dict):",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Updating fallback_keys for device.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            await self.store.set_e2e_fallback_keys(user_id, device_id, fallback_keys)",
            "        elif fallback_keys:",
            "            log_kv({\"message\": \"Did not update fallback_keys\", \"reason\": \"not a dict\"})",
            "        else:",
            "            log_kv(",
            "                {\"message\": \"Did not update fallback_keys\", \"reason\": \"no keys given\"}",
            "            )",
            "",
            "        # the device should have been registered already, but it may have been",
            "        # deleted due to a race with a DELETE request. Or we may be using an",
            "        # old access_token without an associated device_id. Either way, we",
            "        # need to double-check the device is registered to avoid ending up with",
            "        # keys without a corresponding device.",
            "        await self.device_handler.check_device_registered(user_id, device_id)",
            "",
            "        result = await self.store.count_e2e_one_time_keys(user_id, device_id)",
            "",
            "        set_tag(\"one_time_key_counts\", str(result))",
            "        return {\"one_time_key_counts\": result}",
            "",
            "    async def _upload_one_time_keys_for_user(",
            "        self, user_id: str, device_id: str, time_now: int, one_time_keys: JsonDict",
            "    ) -> None:",
            "        logger.info(",
            "            \"Adding one_time_keys %r for device %r for user %r at %d\",",
            "            one_time_keys.keys(),",
            "            device_id,",
            "            user_id,",
            "            time_now,",
            "        )",
            "",
            "        # make a list of (alg, id, key) tuples",
            "        key_list = []",
            "        for key_id, key_obj in one_time_keys.items():",
            "            algorithm, key_id = key_id.split(\":\")",
            "            key_list.append((algorithm, key_id, key_obj))",
            "",
            "        # First we check if we have already persisted any of the keys.",
            "        existing_key_map = await self.store.get_e2e_one_time_keys(",
            "            user_id, device_id, [k_id for _, k_id, _ in key_list]",
            "        )",
            "",
            "        new_keys = []  # Keys that we need to insert. (alg, id, json) tuples.",
            "        for algorithm, key_id, key in key_list:",
            "            ex_json = existing_key_map.get((algorithm, key_id), None)",
            "            if ex_json:",
            "                if not _one_time_keys_match(ex_json, key):",
            "                    raise SynapseError(",
            "                        400,",
            "                        (",
            "                            \"One time key %s:%s already exists. \"",
            "                            \"Old key: %s; new key: %r\"",
            "                        )",
            "                        % (algorithm, key_id, ex_json, key),",
            "                    )",
            "            else:",
            "                new_keys.append(",
            "                    (algorithm, key_id, encode_canonical_json(key).decode(\"ascii\"))",
            "                )",
            "",
            "        log_kv({\"message\": \"Inserting new one_time_keys.\", \"keys\": new_keys})",
            "        await self.store.add_e2e_one_time_keys(user_id, device_id, time_now, new_keys)",
            "",
            "    async def upload_signing_keys_for_user(",
            "        self, user_id: str, keys: JsonDict",
            "    ) -> JsonDict:",
            "        \"\"\"Upload signing keys for cross-signing",
            "",
            "        Args:",
            "            user_id: the user uploading the keys",
            "            keys: the signing keys",
            "        \"\"\"",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        # if a master key is uploaded, then check it.  Otherwise, load the",
            "        # stored master key, to check signatures on other keys",
            "        if \"master_key\" in keys:",
            "            master_key = keys[\"master_key\"]",
            "",
            "            _check_cross_signing_key(master_key, user_id, \"master\")",
            "        else:",
            "            master_key = await self.store.get_e2e_cross_signing_key(user_id, \"master\")",
            "",
            "        # if there is no master key, then we can't do anything, because all the",
            "        # other cross-signing keys need to be signed by the master key",
            "        if not master_key:",
            "            raise SynapseError(400, \"No master key available\", Codes.MISSING_PARAM)",
            "",
            "        try:",
            "            master_key_id, master_verify_key = get_verify_key_from_cross_signing_key(",
            "                master_key",
            "            )",
            "        except ValueError:",
            "            if \"master_key\" in keys:",
            "                # the invalid key came from the request",
            "                raise SynapseError(400, \"Invalid master key\", Codes.INVALID_PARAM)",
            "            else:",
            "                # the invalid key came from the database",
            "                logger.error(\"Invalid master key found for user %s\", user_id)",
            "                raise SynapseError(500, \"Invalid master key\")",
            "",
            "        # for the other cross-signing keys, make sure that they have valid",
            "        # signatures from the master key",
            "        if \"self_signing_key\" in keys:",
            "            self_signing_key = keys[\"self_signing_key\"]",
            "",
            "            _check_cross_signing_key(",
            "                self_signing_key, user_id, \"self_signing\", master_verify_key",
            "            )",
            "",
            "        if \"user_signing_key\" in keys:",
            "            user_signing_key = keys[\"user_signing_key\"]",
            "",
            "            _check_cross_signing_key(",
            "                user_signing_key, user_id, \"user_signing\", master_verify_key",
            "            )",
            "",
            "        # if everything checks out, then store the keys and send notifications",
            "        deviceids = []",
            "        if \"master_key\" in keys:",
            "            await self.store.set_e2e_cross_signing_key(user_id, \"master\", master_key)",
            "            deviceids.append(master_verify_key.version)",
            "        if \"self_signing_key\" in keys:",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user_id, \"self_signing\", self_signing_key",
            "            )",
            "            try:",
            "                deviceids.append(",
            "                    get_verify_key_from_cross_signing_key(self_signing_key)[1].version",
            "                )",
            "            except ValueError:",
            "                raise SynapseError(400, \"Invalid self-signing key\", Codes.INVALID_PARAM)",
            "        if \"user_signing_key\" in keys:",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user_id, \"user_signing\", user_signing_key",
            "            )",
            "            # the signature stream matches the semantics that we want for",
            "            # user-signing key updates: only the user themselves is notified of",
            "            # their own user-signing key updates",
            "            await self.device_handler.notify_user_signature_update(user_id, [user_id])",
            "",
            "        # master key and self-signing key updates match the semantics of device",
            "        # list updates: all users who share an encrypted room are notified",
            "        if len(deviceids):",
            "            await self.device_handler.notify_device_update(user_id, deviceids)",
            "",
            "        return {}",
            "",
            "    async def upload_signatures_for_device_keys(",
            "        self, user_id: str, signatures: JsonDict",
            "    ) -> JsonDict:",
            "        \"\"\"Upload device signatures for cross-signing",
            "",
            "        Args:",
            "            user_id: the user uploading the signatures",
            "            signatures: map of users to devices to signed keys. This is the submission",
            "            from the user; an exception will be raised if it is malformed.",
            "        Returns:",
            "            The response to be sent back to the client.  The response will have",
            "                a \"failures\" key, which will be a dict mapping users to devices",
            "                to errors for the signatures that failed.",
            "        Raises:",
            "            SynapseError: if the signatures dict is not valid.",
            "        \"\"\"",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        failures = {}",
            "",
            "        # signatures to be stored.  Each item will be a SignatureListItem",
            "        signature_list = []",
            "",
            "        # split between checking signatures for own user and signatures for",
            "        # other users, since we verify them with different keys",
            "        self_signatures = signatures.get(user_id, {})",
            "        other_signatures = {k: v for k, v in signatures.items() if k != user_id}",
            "",
            "        self_signature_list, self_failures = await self._process_self_signatures(",
            "            user_id, self_signatures",
            "        )",
            "        signature_list.extend(self_signature_list)",
            "        failures.update(self_failures)",
            "",
            "        other_signature_list, other_failures = await self._process_other_signatures(",
            "            user_id, other_signatures",
            "        )",
            "        signature_list.extend(other_signature_list)",
            "        failures.update(other_failures)",
            "",
            "        # store the signature, and send the appropriate notifications for sync",
            "        logger.debug(\"upload signature failures: %r\", failures)",
            "        await self.store.store_e2e_cross_signing_signatures(user_id, signature_list)",
            "",
            "        self_device_ids = [item.target_device_id for item in self_signature_list]",
            "        if self_device_ids:",
            "            await self.device_handler.notify_device_update(user_id, self_device_ids)",
            "        signed_users = [item.target_user_id for item in other_signature_list]",
            "        if signed_users:",
            "            await self.device_handler.notify_user_signature_update(",
            "                user_id, signed_users",
            "            )",
            "",
            "        return {\"failures\": failures}",
            "",
            "    async def _process_self_signatures(",
            "        self, user_id: str, signatures: JsonDict",
            "    ) -> Tuple[List[\"SignatureListItem\"], Dict[str, Dict[str, dict]]]:",
            "        \"\"\"Process uploaded signatures of the user's own keys.",
            "",
            "        Signatures of the user's own keys from this API come in two forms:",
            "        - signatures of the user's devices by the user's self-signing key,",
            "        - signatures of the user's master key by the user's devices.",
            "",
            "        Args:",
            "            user_id: the user uploading the keys",
            "            signatures (dict[string, dict]): map of devices to signed keys",
            "",
            "        Returns:",
            "            A tuple of a list of signatures to store, and a map of users to",
            "            devices to failure reasons",
            "",
            "        Raises:",
            "            SynapseError: if the input is malformed",
            "        \"\"\"",
            "        signature_list: List[\"SignatureListItem\"] = []",
            "        failures: Dict[str, Dict[str, JsonDict]] = {}",
            "        if not signatures:",
            "            return signature_list, failures",
            "",
            "        if not isinstance(signatures, dict):",
            "            raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "",
            "        try:",
            "            # get our self-signing key to verify the signatures",
            "            (",
            "                _,",
            "                self_signing_key_id,",
            "                self_signing_verify_key,",
            "            ) = await self._get_e2e_cross_signing_verify_key(user_id, \"self_signing\")",
            "",
            "            # get our master key, since we may have received a signature of it.",
            "            # We need to fetch it here so that we know what its key ID is, so",
            "            # that we can check if a signature that was sent is a signature of",
            "            # the master key or of a device",
            "            (",
            "                master_key,",
            "                _,",
            "                master_verify_key,",
            "            ) = await self._get_e2e_cross_signing_verify_key(user_id, \"master\")",
            "",
            "            # fetch our stored devices.  This is used to 1. verify",
            "            # signatures on the master key, and 2. to compare with what",
            "            # was sent if the device was signed",
            "            devices = await self.store.get_e2e_device_keys_for_cs_api([(user_id, None)])",
            "",
            "            if user_id not in devices:",
            "                raise NotFoundError(\"No device keys found\")",
            "",
            "            devices = devices[user_id]",
            "        except SynapseError as e:",
            "            failure = _exception_to_failure(e)",
            "            failures[user_id] = {device: failure for device in signatures.keys()}",
            "            return signature_list, failures",
            "",
            "        for device_id, device in signatures.items():",
            "            # make sure submitted data is in the right form",
            "            if not isinstance(device, dict):",
            "                raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "",
            "            try:",
            "                if \"signatures\" not in device or user_id not in device[\"signatures\"]:",
            "                    # no signature was sent",
            "                    raise SynapseError(",
            "                        400, \"Invalid signature\", Codes.INVALID_SIGNATURE",
            "                    )",
            "",
            "                if device_id == master_verify_key.version:",
            "                    # The signature is of the master key. This needs to be",
            "                    # handled differently from signatures of normal devices.",
            "                    master_key_signature_list = self._check_master_key_signature(",
            "                        user_id, device_id, device, master_key, devices",
            "                    )",
            "                    signature_list.extend(master_key_signature_list)",
            "                    continue",
            "",
            "                # at this point, we have a device that should be signed",
            "                # by the self-signing key",
            "                if self_signing_key_id not in device[\"signatures\"][user_id]:",
            "                    # no signature was sent",
            "                    raise SynapseError(",
            "                        400, \"Invalid signature\", Codes.INVALID_SIGNATURE",
            "                    )",
            "",
            "                try:",
            "                    stored_device = devices[device_id]",
            "                except KeyError:",
            "                    raise NotFoundError(\"Unknown device\")",
            "                if self_signing_key_id in stored_device.get(\"signatures\", {}).get(",
            "                    user_id, {}",
            "                ):",
            "                    # we already have a signature on this device, so we",
            "                    # can skip it, since it should be exactly the same",
            "                    continue",
            "",
            "                _check_device_signature(",
            "                    user_id, self_signing_verify_key, device, stored_device",
            "                )",
            "",
            "                signature = device[\"signatures\"][user_id][self_signing_key_id]",
            "                signature_list.append(",
            "                    SignatureListItem(",
            "                        self_signing_key_id, user_id, device_id, signature",
            "                    )",
            "                )",
            "            except SynapseError as e:",
            "                failures.setdefault(user_id, {})[device_id] = _exception_to_failure(e)",
            "",
            "        return signature_list, failures",
            "",
            "    def _check_master_key_signature(",
            "        self,",
            "        user_id: str,",
            "        master_key_id: str,",
            "        signed_master_key: JsonDict,",
            "        stored_master_key: JsonMapping,",
            "        devices: Dict[str, Dict[str, JsonDict]],",
            "    ) -> List[\"SignatureListItem\"]:",
            "        \"\"\"Check signatures of a user's master key made by their devices.",
            "",
            "        Args:",
            "            user_id: the user whose master key is being checked",
            "            master_key_id: the ID of the user's master key",
            "            signed_master_key: the user's signed master key that was uploaded",
            "            stored_master_key: our previously-stored copy of the user's master key",
            "            devices: the user's devices",
            "",
            "        Returns:",
            "            A list of signatures to store",
            "",
            "        Raises:",
            "            SynapseError: if a signature is invalid",
            "        \"\"\"",
            "        # for each device that signed the master key, check the signature.",
            "        master_key_signature_list = []",
            "        sigs = signed_master_key[\"signatures\"]",
            "        for signing_key_id, signature in sigs[user_id].items():",
            "            _, signing_device_id = signing_key_id.split(\":\", 1)",
            "            if (",
            "                signing_device_id not in devices",
            "                or signing_key_id not in devices[signing_device_id][\"keys\"]",
            "            ):",
            "                # signed by an unknown device, or the",
            "                # device does not have the key",
            "                raise SynapseError(400, \"Invalid signature\", Codes.INVALID_SIGNATURE)",
            "",
            "            # get the key and check the signature",
            "            pubkey = devices[signing_device_id][\"keys\"][signing_key_id]",
            "            verify_key = decode_verify_key_bytes(signing_key_id, decode_base64(pubkey))",
            "            _check_device_signature(",
            "                user_id, verify_key, signed_master_key, stored_master_key",
            "            )",
            "",
            "            master_key_signature_list.append(",
            "                SignatureListItem(signing_key_id, user_id, master_key_id, signature)",
            "            )",
            "",
            "        return master_key_signature_list",
            "",
            "    async def _process_other_signatures(",
            "        self, user_id: str, signatures: Dict[str, dict]",
            "    ) -> Tuple[List[\"SignatureListItem\"], Dict[str, Dict[str, dict]]]:",
            "        \"\"\"Process uploaded signatures of other users' keys.  These will be the",
            "        target user's master keys, signed by the uploading user's user-signing",
            "        key.",
            "",
            "        Args:",
            "            user_id: the user uploading the keys",
            "            signatures: map of users to devices to signed keys",
            "",
            "        Returns:",
            "            A list of signatures to store, and a map of users to devices to failure",
            "            reasons",
            "",
            "        Raises:",
            "            SynapseError: if the input is malformed",
            "        \"\"\"",
            "        signature_list: List[\"SignatureListItem\"] = []",
            "        failures: Dict[str, Dict[str, JsonDict]] = {}",
            "        if not signatures:",
            "            return signature_list, failures",
            "",
            "        try:",
            "            # get our user-signing key to verify the signatures",
            "            (",
            "                user_signing_key,",
            "                user_signing_key_id,",
            "                user_signing_verify_key,",
            "            ) = await self._get_e2e_cross_signing_verify_key(user_id, \"user_signing\")",
            "        except SynapseError as e:",
            "            failure = _exception_to_failure(e)",
            "            for user, devicemap in signatures.items():",
            "                failures[user] = {device_id: failure for device_id in devicemap.keys()}",
            "            return signature_list, failures",
            "",
            "        for target_user, devicemap in signatures.items():",
            "            # make sure submitted data is in the right form",
            "            if not isinstance(devicemap, dict):",
            "                raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "            for device in devicemap.values():",
            "                if not isinstance(device, dict):",
            "                    raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "",
            "            device_id = None",
            "            try:",
            "                # get the target user's master key, to make sure it matches",
            "                # what was sent",
            "                (",
            "                    master_key,",
            "                    master_key_id,",
            "                    _,",
            "                ) = await self._get_e2e_cross_signing_verify_key(",
            "                    target_user, \"master\", user_id",
            "                )",
            "",
            "                # make sure that the target user's master key is the one that",
            "                # was signed (and no others)",
            "                device_id = master_key_id.split(\":\", 1)[1]",
            "                if device_id not in devicemap:",
            "                    logger.debug(",
            "                        \"upload signature: could not find signature for device %s\",",
            "                        device_id,",
            "                    )",
            "                    # set device to None so that the failure gets",
            "                    # marked on all the signatures",
            "                    device_id = None",
            "                    raise NotFoundError(\"Unknown device\")",
            "                key = devicemap[device_id]",
            "                other_devices = [k for k in devicemap.keys() if k != device_id]",
            "                if other_devices:",
            "                    # other devices were signed -- mark those as failures",
            "                    logger.debug(\"upload signature: too many devices specified\")",
            "                    failure = _exception_to_failure(NotFoundError(\"Unknown device\"))",
            "                    failures[target_user] = {",
            "                        device: failure for device in other_devices",
            "                    }",
            "",
            "                if user_signing_key_id in master_key.get(\"signatures\", {}).get(",
            "                    user_id, {}",
            "                ):",
            "                    # we already have the signature, so we can skip it",
            "                    continue",
            "",
            "                _check_device_signature(",
            "                    user_id, user_signing_verify_key, key, master_key",
            "                )",
            "",
            "                signature = key[\"signatures\"][user_id][user_signing_key_id]",
            "                signature_list.append(",
            "                    SignatureListItem(",
            "                        user_signing_key_id, target_user, device_id, signature",
            "                    )",
            "                )",
            "            except SynapseError as e:",
            "                failure = _exception_to_failure(e)",
            "                if device_id is None:",
            "                    failures[target_user] = {",
            "                        device_id: failure for device_id in devicemap.keys()",
            "                    }",
            "                else:",
            "                    failures.setdefault(target_user, {})[device_id] = failure",
            "",
            "        return signature_list, failures",
            "",
            "    async def _get_e2e_cross_signing_verify_key(",
            "        self, user_id: str, key_type: str, from_user_id: Optional[str] = None",
            "    ) -> Tuple[JsonMapping, str, VerifyKey]:",
            "        \"\"\"Fetch locally or remotely query for a cross-signing public key.",
            "",
            "        First, attempt to fetch the cross-signing public key from storage.",
            "        If that fails, query the keys from the homeserver they belong to",
            "        and update our local copy.",
            "",
            "        Args:",
            "            user_id: the user whose key should be fetched",
            "            key_type: the type of key to fetch",
            "            from_user_id: the user that we are fetching the keys for.",
            "                This affects what signatures are fetched.",
            "",
            "        Returns:",
            "            The raw key data, the key ID, and the signedjson verify key",
            "",
            "        Raises:",
            "            NotFoundError: if the key is not found",
            "            SynapseError: if `user_id` is invalid",
            "        \"\"\"",
            "        user = UserID.from_string(user_id)",
            "        key = await self.store.get_e2e_cross_signing_key(",
            "            user_id, key_type, from_user_id",
            "        )",
            "",
            "        if key:",
            "            # We found a copy of this key in our database. Decode and return it",
            "            key_id, verify_key = get_verify_key_from_cross_signing_key(key)",
            "            return key, key_id, verify_key",
            "",
            "        # If we couldn't find the key locally, and we're looking for keys of",
            "        # another user then attempt to fetch the missing key from the remote",
            "        # user's server.",
            "        #",
            "        # We may run into this in possible edge cases where a user tries to",
            "        # cross-sign a remote user, but does not share any rooms with them yet.",
            "        # Thus, we would not have their key list yet. We instead fetch the key,",
            "        # store it and notify clients of new, associated device IDs.",
            "        if self.is_mine(user) or key_type not in [\"master\", \"self_signing\"]:",
            "            # Note that master and self_signing keys are the only cross-signing keys we",
            "            # can request over federation",
            "            raise NotFoundError(\"No %s key found for %s\" % (key_type, user_id))",
            "",
            "        cross_signing_keys = await self._retrieve_cross_signing_keys_for_remote_user(",
            "            user, key_type",
            "        )",
            "        if cross_signing_keys is None:",
            "            raise NotFoundError(\"No %s key found for %s\" % (key_type, user_id))",
            "",
            "        return cross_signing_keys",
            "",
            "    async def _retrieve_cross_signing_keys_for_remote_user(",
            "        self,",
            "        user: UserID,",
            "        desired_key_type: str,",
            "    ) -> Optional[Tuple[JsonMapping, str, VerifyKey]]:",
            "        \"\"\"Queries cross-signing keys for a remote user and saves them to the database",
            "",
            "        Only the key specified by `key_type` will be returned, while all retrieved keys",
            "        will be saved regardless",
            "",
            "        Args:",
            "            user: The user to query remote keys for",
            "            desired_key_type: The type of key to receive. One of \"master\", \"self_signing\"",
            "",
            "        Returns:",
            "            A tuple of the retrieved key content, the key's ID and the matching VerifyKey.",
            "            If the key cannot be retrieved, all values in the tuple will instead be None.",
            "        \"\"\"",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        try:",
            "            remote_result = await self.federation.query_user_devices(",
            "                user.domain, user.to_string()",
            "            )",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Unable to query %s for cross-signing keys of user %s: %s %s\",",
            "                user.domain,",
            "                user.to_string(),",
            "                type(e),",
            "                e,",
            "            )",
            "            return None",
            "",
            "        # Process each of the retrieved cross-signing keys",
            "        desired_key_data = None",
            "        retrieved_device_ids = []",
            "        for key_type in [\"master\", \"self_signing\"]:",
            "            key_content = remote_result.get(key_type + \"_key\")",
            "            if not key_content:",
            "                continue",
            "",
            "            # Ensure these keys belong to the correct user",
            "            if \"user_id\" not in key_content:",
            "                logger.warning(",
            "                    \"Invalid %s key retrieved, missing user_id field: %s\",",
            "                    key_type,",
            "                    key_content,",
            "                )",
            "                continue",
            "            if user.to_string() != key_content[\"user_id\"]:",
            "                logger.warning(",
            "                    \"Found %s key of user %s when querying for keys of user %s\",",
            "                    key_type,",
            "                    key_content[\"user_id\"],",
            "                    user.to_string(),",
            "                )",
            "                continue",
            "",
            "            # Validate the key contents",
            "            try:",
            "                # verify_key is a VerifyKey from signedjson, which uses",
            "                # .version to denote the portion of the key ID after the",
            "                # algorithm and colon, which is the device ID",
            "                key_id, verify_key = get_verify_key_from_cross_signing_key(key_content)",
            "            except ValueError as e:",
            "                logger.warning(",
            "                    \"Invalid %s key retrieved: %s - %s %s\",",
            "                    key_type,",
            "                    key_content,",
            "                    type(e),",
            "                    e,",
            "                )",
            "                continue",
            "",
            "            # Note down the device ID attached to this key",
            "            retrieved_device_ids.append(verify_key.version)",
            "",
            "            # If this is the desired key type, save it and its ID/VerifyKey",
            "            if key_type == desired_key_type:",
            "                desired_key_data = key_content, key_id, verify_key",
            "",
            "            # At the same time, store this key in the db for subsequent queries",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user.to_string(), key_type, key_content",
            "            )",
            "",
            "        # Notify clients that new devices for this user have been discovered",
            "        if retrieved_device_ids:",
            "            # XXX is this necessary?",
            "            await self.device_handler.notify_device_update(",
            "                user.to_string(), retrieved_device_ids",
            "            )",
            "",
            "        return desired_key_data",
            "",
            "    async def is_cross_signing_set_up_for_user(self, user_id: str) -> bool:",
            "        \"\"\"Checks if the user has cross-signing set up",
            "",
            "        Args:",
            "            user_id: The user to check",
            "",
            "        Returns:",
            "            True if the user has cross-signing set up, False otherwise",
            "        \"\"\"",
            "        existing_master_key = await self.store.get_e2e_cross_signing_key(",
            "            user_id, \"master\"",
            "        )",
            "        return existing_master_key is not None",
            "",
            "",
            "def _check_cross_signing_key(",
            "    key: JsonDict, user_id: str, key_type: str, signing_key: Optional[VerifyKey] = None",
            ") -> None:",
            "    \"\"\"Check a cross-signing key uploaded by a user.  Performs some basic sanity",
            "    checking, and ensures that it is signed, if a signature is required.",
            "",
            "    Args:",
            "        key: the key data to verify",
            "        user_id: the user whose key is being checked",
            "        key_type: the type of key that the key should be",
            "        signing_key: the signing key that the key should be signed with.  If",
            "            omitted, signatures will not be checked.",
            "    \"\"\"",
            "    if (",
            "        key.get(\"user_id\") != user_id",
            "        or key_type not in key.get(\"usage\", [])",
            "        or len(key.get(\"keys\", {})) != 1",
            "    ):",
            "        raise SynapseError(400, (\"Invalid %s key\" % (key_type,)), Codes.INVALID_PARAM)",
            "",
            "    if signing_key:",
            "        try:",
            "            verify_signed_json(key, user_id, signing_key)",
            "        except SignatureVerifyException:",
            "            raise SynapseError(",
            "                400, (\"Invalid signature on %s key\" % key_type), Codes.INVALID_SIGNATURE",
            "            )",
            "",
            "",
            "def _check_device_signature(",
            "    user_id: str,",
            "    verify_key: VerifyKey,",
            "    signed_device: JsonDict,",
            "    stored_device: JsonMapping,",
            ") -> None:",
            "    \"\"\"Check that a signature on a device or cross-signing key is correct and",
            "    matches the copy of the device/key that we have stored.  Throws an",
            "    exception if an error is detected.",
            "",
            "    Args:",
            "        user_id: the user ID whose signature is being checked",
            "        verify_key: the key to verify the device with",
            "        signed_device: the uploaded signed device data",
            "        stored_device: our previously stored copy of the device",
            "",
            "    Raises:",
            "        SynapseError: if the signature was invalid or the sent device is not the",
            "            same as the stored device",
            "",
            "    \"\"\"",
            "",
            "    # make sure that the device submitted matches what we have stored",
            "    stripped_signed_device = {",
            "        k: v for k, v in signed_device.items() if k not in [\"signatures\", \"unsigned\"]",
            "    }",
            "    stripped_stored_device = {",
            "        k: v for k, v in stored_device.items() if k not in [\"signatures\", \"unsigned\"]",
            "    }",
            "    if stripped_signed_device != stripped_stored_device:",
            "        logger.debug(",
            "            \"upload signatures: key does not match %s vs %s\",",
            "            signed_device,",
            "            stored_device,",
            "        )",
            "        raise SynapseError(400, \"Key does not match\")",
            "",
            "    try:",
            "        verify_signed_json(signed_device, user_id, verify_key)",
            "    except SignatureVerifyException:",
            "        logger.debug(\"invalid signature on key\")",
            "        raise SynapseError(400, \"Invalid signature\", Codes.INVALID_SIGNATURE)",
            "",
            "",
            "def _exception_to_failure(e: Exception) -> JsonDict:",
            "    if isinstance(e, SynapseError):",
            "        return {\"status\": e.code, \"errcode\": e.errcode, \"message\": str(e)}",
            "",
            "    if isinstance(e, CodeMessageException):",
            "        return {\"status\": e.code, \"message\": str(e)}",
            "",
            "    if isinstance(e, NotRetryingDestination):",
            "        return {\"status\": 503, \"message\": \"Not ready for retry\"}",
            "",
            "    # include ConnectionRefused and other errors",
            "    #",
            "    # Note that some Exceptions (notably twisted's ResponseFailed etc) don't",
            "    # give a string for e.message, which json then fails to serialize.",
            "    return {\"status\": 503, \"message\": str(e)}",
            "",
            "",
            "def _one_time_keys_match(old_key_json: str, new_key: JsonDict) -> bool:",
            "    old_key = json_decoder.decode(old_key_json)",
            "",
            "    # if either is a string rather than an object, they must match exactly",
            "    if not isinstance(old_key, dict) or not isinstance(new_key, dict):",
            "        return old_key == new_key",
            "",
            "    # otherwise, we strip off the 'signatures' if any, because it's legitimate",
            "    # for different upload attempts to have different signatures.",
            "    old_key.pop(\"signatures\", None)",
            "    new_key_copy = dict(new_key)",
            "    new_key_copy.pop(\"signatures\", None)",
            "",
            "    return old_key == new_key_copy",
            "",
            "",
            "@attr.s(slots=True, auto_attribs=True)",
            "class SignatureListItem:",
            "    \"\"\"An item in the signature list as used by upload_signatures_for_device_keys.\"\"\"",
            "",
            "    signing_key_id: str",
            "    target_user_id: str",
            "    target_device_id: str",
            "    signature: JsonDict",
            "",
            "",
            "class SigningKeyEduUpdater:",
            "    \"\"\"Handles incoming signing key updates from federation and updates the DB\"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self.federation = hs.get_federation_client()",
            "        self.clock = hs.get_clock()",
            "",
            "        device_handler = hs.get_device_handler()",
            "        assert isinstance(device_handler, DeviceHandler)",
            "        self._device_handler = device_handler",
            "",
            "        self._remote_edu_linearizer = Linearizer(name=\"remote_signing_key\")",
            "",
            "        # user_id -> list of updates waiting to be handled.",
            "        self._pending_updates: Dict[str, List[Tuple[JsonDict, JsonDict]]] = {}",
            "",
            "    async def incoming_signing_key_update(",
            "        self, origin: str, edu_content: JsonDict",
            "    ) -> None:",
            "        \"\"\"Called on incoming signing key update from federation. Responsible for",
            "        parsing the EDU and adding to pending updates list.",
            "",
            "        Args:",
            "            origin: the server that sent the EDU",
            "            edu_content: the contents of the EDU",
            "        \"\"\"",
            "",
            "        user_id = edu_content.pop(\"user_id\")",
            "        master_key = edu_content.pop(\"master_key\", None)",
            "        self_signing_key = edu_content.pop(\"self_signing_key\", None)",
            "",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.warning(\"Got signing key update edu for %r from %r\", user_id, origin)",
            "            return",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "        if not room_ids:",
            "            # We don't share any rooms with this user. Ignore update, as we",
            "            # probably won't get any further updates.",
            "            return",
            "",
            "        self._pending_updates.setdefault(user_id, []).append(",
            "            (master_key, self_signing_key)",
            "        )",
            "",
            "        await self._handle_signing_key_updates(user_id)",
            "",
            "    async def _handle_signing_key_updates(self, user_id: str) -> None:",
            "        \"\"\"Actually handle pending updates.",
            "",
            "        Args:",
            "            user_id: the user whose updates we are processing",
            "        \"\"\"",
            "",
            "        async with self._remote_edu_linearizer.queue(user_id):",
            "            pending_updates = self._pending_updates.pop(user_id, [])",
            "            if not pending_updates:",
            "                # This can happen since we batch updates",
            "                return",
            "",
            "            device_ids: List[str] = []",
            "",
            "            logger.info(\"pending updates: %r\", pending_updates)",
            "",
            "            for master_key, self_signing_key in pending_updates:",
            "                new_device_ids = await self._device_handler.device_list_updater.process_cross_signing_key_update(",
            "                    user_id,",
            "                    master_key,",
            "                    self_signing_key,",
            "                )",
            "                device_ids = device_ids + new_device_ids",
            "",
            "            await self._device_handler.notify_device_update(user_id, device_ids)"
        ],
        "afterPatchFile": [
            "# Copyright 2016 OpenMarket Ltd",
            "# Copyright 2018-2019 New Vector Ltd",
            "# Copyright 2019 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "from typing import TYPE_CHECKING, Dict, Iterable, List, Mapping, Optional, Tuple",
            "",
            "import attr",
            "from canonicaljson import encode_canonical_json",
            "from signedjson.key import VerifyKey, decode_verify_key_bytes",
            "from signedjson.sign import SignatureVerifyException, verify_signed_json",
            "from unpaddedbase64 import decode_base64",
            "",
            "from twisted.internet import defer",
            "",
            "from synapse.api.constants import EduTypes",
            "from synapse.api.errors import CodeMessageException, Codes, NotFoundError, SynapseError",
            "from synapse.handlers.device import DeviceHandler",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.logging.opentracing import log_kv, set_tag, tag_args, trace",
            "from synapse.types import (",
            "    JsonDict,",
            "    JsonMapping,",
            "    UserID,",
            "    get_domain_from_id,",
            "    get_verify_key_from_cross_signing_key,",
            ")",
            "from synapse.util import json_decoder",
            "from synapse.util.async_helpers import Linearizer, concurrently_execute",
            "from synapse.util.cancellation import cancellable",
            "from synapse.util.retryutils import NotRetryingDestination",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class E2eKeysHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.config = hs.config",
            "        self.store = hs.get_datastores().main",
            "        self.federation = hs.get_federation_client()",
            "        self.device_handler = hs.get_device_handler()",
            "        self._appservice_handler = hs.get_application_service_handler()",
            "        self.is_mine = hs.is_mine",
            "        self.clock = hs.get_clock()",
            "",
            "        federation_registry = hs.get_federation_registry()",
            "",
            "        is_master = hs.config.worker.worker_app is None",
            "        if is_master:",
            "            edu_updater = SigningKeyEduUpdater(hs)",
            "",
            "            # Only register this edu handler on master as it requires writing",
            "            # device updates to the db",
            "            federation_registry.register_edu_handler(",
            "                EduTypes.SIGNING_KEY_UPDATE,",
            "                edu_updater.incoming_signing_key_update,",
            "            )",
            "            # also handle the unstable version",
            "            # FIXME: remove this when enough servers have upgraded",
            "            federation_registry.register_edu_handler(",
            "                EduTypes.UNSTABLE_SIGNING_KEY_UPDATE,",
            "                edu_updater.incoming_signing_key_update,",
            "            )",
            "",
            "        # doesn't really work as part of the generic query API, because the",
            "        # query request requires an object POST, but we abuse the",
            "        # \"query handler\" interface.",
            "        federation_registry.register_query_handler(",
            "            \"client_keys\", self.on_federation_query_client_keys",
            "        )",
            "",
            "        # Limit the number of in-flight requests from a single device.",
            "        self._query_devices_linearizer = Linearizer(",
            "            name=\"query_devices\",",
            "            max_count=10,",
            "        )",
            "",
            "        self._query_appservices_for_otks = (",
            "            hs.config.experimental.msc3983_appservice_otk_claims",
            "        )",
            "        self._query_appservices_for_keys = (",
            "            hs.config.experimental.msc3984_appservice_key_query",
            "        )",
            "",
            "    @trace",
            "    @cancellable",
            "    async def query_devices(",
            "        self,",
            "        query_body: JsonDict,",
            "        timeout: int,",
            "        from_user_id: str,",
            "        from_device_id: Optional[str],",
            "    ) -> JsonDict:",
            "        \"\"\"Handle a device key query from a client",
            "",
            "        {",
            "            \"device_keys\": {",
            "                \"<user_id>\": [\"<device_id>\"]",
            "            }",
            "        }",
            "        ->",
            "        {",
            "            \"device_keys\": {",
            "                \"<user_id>\": {",
            "                    \"<device_id>\": {",
            "                        ...",
            "                    }",
            "                }",
            "            }",
            "        }",
            "",
            "        Args:",
            "            from_user_id: the user making the query.  This is used when",
            "                adding cross-signing signatures to limit what signatures users",
            "                can see.",
            "            from_device_id: the device making the query. This is used to limit",
            "                the number of in-flight queries at a time.",
            "        \"\"\"",
            "        async with self._query_devices_linearizer.queue((from_user_id, from_device_id)):",
            "            device_keys_query: Dict[str, List[str]] = query_body.get(\"device_keys\", {})",
            "",
            "            # separate users by domain.",
            "            # make a map from domain to user_id to device_ids",
            "            local_query = {}",
            "            remote_queries = {}",
            "",
            "            for user_id, device_ids in device_keys_query.items():",
            "                # we use UserID.from_string to catch invalid user ids",
            "                if self.is_mine(UserID.from_string(user_id)):",
            "                    local_query[user_id] = device_ids",
            "                else:",
            "                    remote_queries[user_id] = device_ids",
            "",
            "            set_tag(\"local_key_query\", str(local_query))",
            "            set_tag(\"remote_key_query\", str(remote_queries))",
            "",
            "            # First get local devices.",
            "            # A map of destination -> failure response.",
            "            failures: Dict[str, JsonDict] = {}",
            "            results = {}",
            "            if local_query:",
            "                local_result = await self.query_local_devices(local_query)",
            "                for user_id, keys in local_result.items():",
            "                    if user_id in local_query:",
            "                        results[user_id] = keys",
            "",
            "            # Get cached cross-signing keys",
            "            cross_signing_keys = await self.get_cross_signing_keys_from_cache(",
            "                device_keys_query, from_user_id",
            "            )",
            "",
            "            # Now attempt to get any remote devices from our local cache.",
            "            # A map of destination -> user ID -> device IDs.",
            "            remote_queries_not_in_cache: Dict[str, Dict[str, Iterable[str]]] = {}",
            "            if remote_queries:",
            "                user_ids = set()",
            "                user_and_device_ids: List[Tuple[str, str]] = []",
            "                for user_id, device_ids in remote_queries.items():",
            "                    if device_ids:",
            "                        user_and_device_ids.extend(",
            "                            (user_id, device_id) for device_id in device_ids",
            "                        )",
            "                    else:",
            "                        user_ids.add(user_id)",
            "",
            "                (",
            "                    user_ids_not_in_cache,",
            "                    remote_results,",
            "                ) = await self.store.get_user_devices_from_cache(",
            "                    user_ids, user_and_device_ids",
            "                )",
            "",
            "                # Check that the homeserver still shares a room with all cached users.",
            "                # Note that this check may be slightly racy when a remote user leaves a",
            "                # room after we have fetched their cached device list. In the worst case",
            "                # we will do extra federation queries for devices that we had cached.",
            "                cached_users = set(remote_results.keys())",
            "                valid_cached_users = (",
            "                    await self.store.get_users_server_still_shares_room_with(",
            "                        remote_results.keys()",
            "                    )",
            "                )",
            "                invalid_cached_users = cached_users - valid_cached_users",
            "                if invalid_cached_users:",
            "                    # Fix up results. If we get here, it means there was either a bug in",
            "                    # device list tracking, or we hit the race mentioned above.",
            "                    # TODO: In practice, this path is hit fairly often in existing",
            "                    #       deployments when clients query the keys of departed remote",
            "                    #       users. A background update to mark the appropriate device",
            "                    #       lists as unsubscribed is needed.",
            "                    #       https://github.com/matrix-org/synapse/issues/13651",
            "                    # Note that this currently introduces a failure mode when clients",
            "                    # are trying to decrypt old messages from a remote user whose",
            "                    # homeserver is no longer available. We may want to consider falling",
            "                    # back to the cached data when we fail to retrieve a device list",
            "                    # over federation for such remote users.",
            "                    user_ids_not_in_cache.update(invalid_cached_users)",
            "                    for invalid_user_id in invalid_cached_users:",
            "                        remote_results.pop(invalid_user_id)",
            "",
            "                for user_id, devices in remote_results.items():",
            "                    user_devices = results.setdefault(user_id, {})",
            "                    for device_id, device in devices.items():",
            "                        keys = device.get(\"keys\", None)",
            "                        device_display_name = device.get(\"device_display_name\", None)",
            "                        if keys:",
            "                            result = dict(keys)",
            "                            unsigned = result.setdefault(\"unsigned\", {})",
            "                            if device_display_name:",
            "                                unsigned[\"device_display_name\"] = device_display_name",
            "                            user_devices[device_id] = result",
            "",
            "                # check for missing cross-signing keys.",
            "                for user_id in remote_queries.keys():",
            "                    cached_cross_master = user_id in cross_signing_keys[\"master_keys\"]",
            "                    cached_cross_selfsigning = (",
            "                        user_id in cross_signing_keys[\"self_signing_keys\"]",
            "                    )",
            "",
            "                    # check if we are missing only one of cross-signing master or",
            "                    # self-signing key, but the other one is cached.",
            "                    # as we need both, this will issue a federation request.",
            "                    # if we don't have any of the keys, either the user doesn't have",
            "                    # cross-signing set up, or the cached device list",
            "                    # is not (yet) updated.",
            "                    if cached_cross_master ^ cached_cross_selfsigning:",
            "                        user_ids_not_in_cache.add(user_id)",
            "",
            "                # add those users to the list to fetch over federation.",
            "                for user_id in user_ids_not_in_cache:",
            "                    domain = get_domain_from_id(user_id)",
            "                    r = remote_queries_not_in_cache.setdefault(domain, {})",
            "                    r[user_id] = remote_queries[user_id]",
            "",
            "            # Now fetch any devices that we don't have in our cache",
            "            # TODO It might make sense to propagate cancellations into the",
            "            #      deferreds which are querying remote homeservers.",
            "            logger.debug(",
            "                \"%d destinations to query devices for\", len(remote_queries_not_in_cache)",
            "            )",
            "",
            "            async def _query(",
            "                destination_queries: Tuple[str, Dict[str, Iterable[str]]]",
            "            ) -> None:",
            "                destination, queries = destination_queries",
            "                return await self._query_devices_for_destination(",
            "                    results,",
            "                    cross_signing_keys,",
            "                    failures,",
            "                    destination,",
            "                    queries,",
            "                    timeout,",
            "                )",
            "",
            "            await concurrently_execute(",
            "                _query,",
            "                remote_queries_not_in_cache.items(),",
            "                10,",
            "                delay_cancellation=True,",
            "            )",
            "",
            "            return {\"device_keys\": results, \"failures\": failures, **cross_signing_keys}",
            "",
            "    @trace",
            "    async def _query_devices_for_destination(",
            "        self,",
            "        results: JsonDict,",
            "        cross_signing_keys: JsonDict,",
            "        failures: Dict[str, JsonDict],",
            "        destination: str,",
            "        destination_query: Dict[str, Iterable[str]],",
            "        timeout: int,",
            "    ) -> None:",
            "        \"\"\"This is called when we are querying the device list of a user on",
            "        a remote homeserver and their device list is not in the device list",
            "        cache. If we share a room with this user and we're not querying for",
            "        specific user we will update the cache with their device list.",
            "",
            "        Args:",
            "            results: A map from user ID to their device keys, which gets",
            "                updated with the newly fetched keys.",
            "            cross_signing_keys: Map from user ID to their cross signing keys,",
            "                which gets updated with the newly fetched keys.",
            "            failures: Map of destinations to failures that have occurred while",
            "                attempting to fetch keys.",
            "            destination: The remote server to query",
            "            destination_query: The query dict of devices to query the remote",
            "                server for.",
            "            timeout: The timeout for remote HTTP requests.",
            "        \"\"\"",
            "",
            "        # We first consider whether we wish to update the device list cache with",
            "        # the users device list. We want to track a user's devices when the",
            "        # authenticated user shares a room with the queried user and the query",
            "        # has not specified a particular device.",
            "        # If we update the cache for the queried user we remove them from further",
            "        # queries. We use the more efficient batched query_client_keys for all",
            "        # remaining users",
            "        user_ids_updated = []",
            "",
            "        # Perform a user device resync for each user only once and only as long as:",
            "        # - they have an empty device_list",
            "        # - they are in some rooms that this server can see",
            "        users_to_resync_devices = {",
            "            user_id",
            "            for (user_id, device_list) in destination_query.items()",
            "            if (not device_list) and (await self.store.get_rooms_for_user(user_id))",
            "        }",
            "",
            "        logger.debug(",
            "            \"%d users to resync devices for from destination %s\",",
            "            len(users_to_resync_devices),",
            "            destination,",
            "        )",
            "",
            "        try:",
            "            user_resync_results = (",
            "                await self.device_handler.device_list_updater.multi_user_device_resync(",
            "                    list(users_to_resync_devices)",
            "                )",
            "            )",
            "            for user_id in users_to_resync_devices:",
            "                resync_results = user_resync_results[user_id]",
            "",
            "                if resync_results is None:",
            "                    # TODO: It's weird that we'll store a failure against a",
            "                    #       destination, yet continue processing users from that",
            "                    #       destination.",
            "                    #       We might want to consider changing this, but for now",
            "                    #       I'm leaving it as I found it.",
            "                    failures[destination] = _exception_to_failure(",
            "                        ValueError(f\"Device resync failed for {user_id!r}\")",
            "                    )",
            "                    continue",
            "",
            "                # Add the device keys to the results.",
            "                user_devices = resync_results[\"devices\"]",
            "                user_results = results.setdefault(user_id, {})",
            "                for device in user_devices:",
            "                    user_results[device[\"device_id\"]] = device[\"keys\"]",
            "                user_ids_updated.append(user_id)",
            "",
            "                # Add any cross signing keys to the results.",
            "                master_key = resync_results.get(\"master_key\")",
            "                self_signing_key = resync_results.get(\"self_signing_key\")",
            "",
            "                if master_key:",
            "                    cross_signing_keys[\"master_keys\"][user_id] = master_key",
            "",
            "                if self_signing_key:",
            "                    cross_signing_keys[\"self_signing_keys\"][user_id] = self_signing_key",
            "        except Exception as e:",
            "            failures[destination] = _exception_to_failure(e)",
            "",
            "        if len(destination_query) == len(user_ids_updated):",
            "            # We've updated all the users in the query and we do not need to",
            "            # make any further remote calls.",
            "            return",
            "",
            "        # Remove all the users from the query which we have updated",
            "        for user_id in user_ids_updated:",
            "            destination_query.pop(user_id)",
            "",
            "        try:",
            "            remote_result = await self.federation.query_client_keys(",
            "                destination, {\"device_keys\": destination_query}, timeout=timeout",
            "            )",
            "",
            "            for user_id, keys in remote_result[\"device_keys\"].items():",
            "                if user_id in destination_query:",
            "                    results[user_id] = keys",
            "",
            "            if \"master_keys\" in remote_result:",
            "                for user_id, key in remote_result[\"master_keys\"].items():",
            "                    if user_id in destination_query:",
            "                        cross_signing_keys[\"master_keys\"][user_id] = key",
            "",
            "            if \"self_signing_keys\" in remote_result:",
            "                for user_id, key in remote_result[\"self_signing_keys\"].items():",
            "                    if user_id in destination_query:",
            "                        cross_signing_keys[\"self_signing_keys\"][user_id] = key",
            "",
            "        except Exception as e:",
            "            failure = _exception_to_failure(e)",
            "            failures[destination] = failure",
            "            set_tag(\"error\", True)",
            "            set_tag(\"reason\", str(failure))",
            "",
            "        return",
            "",
            "    @cancellable",
            "    async def get_cross_signing_keys_from_cache(",
            "        self, query: Iterable[str], from_user_id: Optional[str]",
            "    ) -> Dict[str, Dict[str, JsonMapping]]:",
            "        \"\"\"Get cross-signing keys for users from the database",
            "",
            "        Args:",
            "            query: an iterable of user IDs.  A dict whose keys",
            "                are user IDs satisfies this, so the query format used for",
            "                query_devices can be used here.",
            "            from_user_id: the user making the query.  This is used when",
            "                adding cross-signing signatures to limit what signatures users",
            "                can see.",
            "",
            "        Returns:",
            "            A map from (master_keys|self_signing_keys|user_signing_keys) -> user_id -> key",
            "        \"\"\"",
            "        master_keys = {}",
            "        self_signing_keys = {}",
            "        user_signing_keys = {}",
            "",
            "        user_ids = list(query)",
            "",
            "        keys = await self.store.get_e2e_cross_signing_keys_bulk(user_ids, from_user_id)",
            "",
            "        for user_id, user_info in keys.items():",
            "            if user_info is None:",
            "                continue",
            "            if \"master\" in user_info:",
            "                master_keys[user_id] = user_info[\"master\"]",
            "            if \"self_signing\" in user_info:",
            "                self_signing_keys[user_id] = user_info[\"self_signing\"]",
            "",
            "        # users can see other users' master and self-signing keys, but can",
            "        # only see their own user-signing keys",
            "        if from_user_id:",
            "            from_user_key = keys.get(from_user_id)",
            "            if from_user_key and \"user_signing\" in from_user_key:",
            "                user_signing_keys[from_user_id] = from_user_key[\"user_signing\"]",
            "",
            "        return {",
            "            \"master_keys\": master_keys,",
            "            \"self_signing_keys\": self_signing_keys,",
            "            \"user_signing_keys\": user_signing_keys,",
            "        }",
            "",
            "    @trace",
            "    @cancellable",
            "    async def query_local_devices(",
            "        self,",
            "        query: Mapping[str, Optional[List[str]]],",
            "        include_displaynames: bool = True,",
            "    ) -> Dict[str, Dict[str, dict]]:",
            "        \"\"\"Get E2E device keys for local users",
            "",
            "        Args:",
            "            query: map from user_id to a list",
            "                 of devices to query (None for all devices)",
            "            include_displaynames: Whether to include device displaynames in the returned",
            "                device details.",
            "",
            "        Returns:",
            "            A map from user_id -> device_id -> device details",
            "        \"\"\"",
            "        set_tag(\"local_query\", str(query))",
            "        local_query: List[Tuple[str, Optional[str]]] = []",
            "",
            "        result_dict: Dict[str, Dict[str, dict]] = {}",
            "        for user_id, device_ids in query.items():",
            "            # we use UserID.from_string to catch invalid user ids",
            "            if not self.is_mine(UserID.from_string(user_id)):",
            "                logger.warning(\"Request for keys for non-local user %s\", user_id)",
            "                log_kv(",
            "                    {",
            "                        \"message\": \"Requested a local key for a user which\"",
            "                        \" was not local to the homeserver\",",
            "                        \"user_id\": user_id,",
            "                    }",
            "                )",
            "                set_tag(\"error\", True)",
            "                raise SynapseError(400, \"Not a user here\")",
            "",
            "            if not device_ids:",
            "                local_query.append((user_id, None))",
            "            else:",
            "                for device_id in device_ids:",
            "                    local_query.append((user_id, device_id))",
            "",
            "            # make sure that each queried user appears in the result dict",
            "            result_dict[user_id] = {}",
            "",
            "        results = await self.store.get_e2e_device_keys_for_cs_api(",
            "            local_query, include_displaynames",
            "        )",
            "",
            "        # Check if the application services have any additional results.",
            "        if self._query_appservices_for_keys:",
            "            # Query the appservices for any keys.",
            "            appservice_results = await self._appservice_handler.query_keys(query)",
            "",
            "            # Merge results, overriding with what the appservice returned.",
            "            for user_id, devices in appservice_results.get(\"device_keys\", {}).items():",
            "                # Copy the appservice device info over the homeserver device info, but",
            "                # don't completely overwrite it.",
            "                results.setdefault(user_id, {}).update(devices)",
            "",
            "            # TODO Handle cross-signing keys.",
            "",
            "        # Build the result structure",
            "        for user_id, device_keys in results.items():",
            "            for device_id, device_info in device_keys.items():",
            "                result_dict[user_id][device_id] = device_info",
            "",
            "        log_kv(results)",
            "        return result_dict",
            "",
            "    async def on_federation_query_client_keys(",
            "        self, query_body: Dict[str, Dict[str, Optional[List[str]]]]",
            "    ) -> JsonDict:",
            "        \"\"\"Handle a device key query from a federated server:",
            "",
            "        Handles the path: GET /_matrix/federation/v1/users/keys/query",
            "",
            "        Args:",
            "            query_body: The body of the query request. Should contain a key",
            "                \"device_keys\" that map to a dictionary of user ID's -> list of",
            "                device IDs. If the list of device IDs is empty, all devices of",
            "                that user will be queried.",
            "",
            "        Returns:",
            "            A json dictionary containing the following:",
            "                - device_keys: A dictionary containing the requested device information.",
            "                - master_keys: An optional dictionary of user ID -> master cross-signing",
            "                   key info.",
            "                - self_signing_key: An optional dictionary of user ID -> self-signing",
            "                    key info.",
            "        \"\"\"",
            "        device_keys_query: Dict[str, Optional[List[str]]] = query_body.get(",
            "            \"device_keys\", {}",
            "        )",
            "        if any(",
            "            not self.is_mine(UserID.from_string(user_id))",
            "            for user_id in device_keys_query",
            "        ):",
            "            raise SynapseError(400, \"User is not hosted on this homeserver\")",
            "",
            "        res = await self.query_local_devices(",
            "            device_keys_query,",
            "            include_displaynames=(",
            "                self.config.federation.allow_device_name_lookup_over_federation",
            "            ),",
            "        )",
            "",
            "        # add in the cross-signing keys",
            "        cross_signing_keys = await self.get_cross_signing_keys_from_cache(",
            "            device_keys_query, None",
            "        )",
            "",
            "        return {\"device_keys\": res, **cross_signing_keys}",
            "",
            "    async def claim_local_one_time_keys(",
            "        self,",
            "        local_query: List[Tuple[str, str, str, int]],",
            "        always_include_fallback_keys: bool,",
            "    ) -> Iterable[Dict[str, Dict[str, Dict[str, JsonDict]]]]:",
            "        \"\"\"Claim one time keys for local users.",
            "",
            "        1. Attempt to claim OTKs from the database.",
            "        2. Ask application services if they provide OTKs.",
            "        3. Attempt to fetch fallback keys from the database.",
            "",
            "        Args:",
            "            local_query: An iterable of tuples of (user ID, device ID, algorithm).",
            "            always_include_fallback_keys: True to always include fallback keys.",
            "",
            "        Returns:",
            "            An iterable of maps of user ID -> a map device ID -> a map of key ID -> JSON bytes.",
            "        \"\"\"",
            "",
            "        # Cap the number of OTKs that can be claimed at once to avoid abuse.",
            "        local_query = [",
            "            (user_id, device_id, algorithm, min(count, 5))",
            "            for user_id, device_id, algorithm, count in local_query",
            "        ]",
            "",
            "        otk_results, not_found = await self.store.claim_e2e_one_time_keys(local_query)",
            "",
            "        # If the application services have not provided any keys via the C-S",
            "        # API, query it directly for one-time keys.",
            "        if self._query_appservices_for_otks:",
            "            # TODO Should this query for fallback keys of uploaded OTKs if",
            "            #      always_include_fallback_keys is True? The MSC is ambiguous.",
            "            (",
            "                appservice_results,",
            "                not_found,",
            "            ) = await self._appservice_handler.claim_e2e_one_time_keys(not_found)",
            "        else:",
            "            appservice_results = {}",
            "",
            "        # Calculate which user ID / device ID / algorithm tuples to get fallback",
            "        # keys for. This can be either only missing results *or* all results",
            "        # (which don't already have a fallback key).",
            "        if always_include_fallback_keys:",
            "            # Build the fallback query as any part of the original query where",
            "            # the appservice didn't respond with a fallback key.",
            "            fallback_query = []",
            "",
            "            # Iterate each item in the original query and search the results",
            "            # from the appservice for that user ID / device ID. If it is found,",
            "            # check if any of the keys match the requested algorithm & are a",
            "            # fallback key.",
            "            for user_id, device_id, algorithm, _count in local_query:",
            "                # Check if the appservice responded for this query.",
            "                as_result = appservice_results.get(user_id, {}).get(device_id, {})",
            "                found_otk = False",
            "                for key_id, key_json in as_result.items():",
            "                    if key_id.startswith(f\"{algorithm}:\"):",
            "                        # A OTK or fallback key was found for this query.",
            "                        found_otk = True",
            "                        # A fallback key was found for this query, no need to",
            "                        # query further.",
            "                        if key_json.get(\"fallback\", False):",
            "                            break",
            "",
            "                else:",
            "                    # No fallback key was found from appservices, query for it.",
            "                    # Only mark the fallback key as used if no OTK was found",
            "                    # (from either the database or appservices).",
            "                    mark_as_used = not found_otk and not any(",
            "                        key_id.startswith(f\"{algorithm}:\")",
            "                        for key_id in otk_results.get(user_id, {})",
            "                        .get(device_id, {})",
            "                        .keys()",
            "                    )",
            "                    # Note that it doesn't make sense to request more than 1 fallback key",
            "                    # per (user_id, device_id, algorithm).",
            "                    fallback_query.append((user_id, device_id, algorithm, mark_as_used))",
            "",
            "        else:",
            "            # All fallback keys get marked as used.",
            "            fallback_query = [",
            "                # Note that it doesn't make sense to request more than 1 fallback key",
            "                # per (user_id, device_id, algorithm).",
            "                (user_id, device_id, algorithm, True)",
            "                for user_id, device_id, algorithm, count in not_found",
            "            ]",
            "",
            "        # For each user that does not have a one-time keys available, see if",
            "        # there is a fallback key.",
            "        fallback_results = await self.store.claim_e2e_fallback_keys(fallback_query)",
            "",
            "        # Return the results in order, each item from the input query should",
            "        # only appear once in the combined list.",
            "        return (otk_results, appservice_results, fallback_results)",
            "",
            "    @trace",
            "    async def claim_one_time_keys(",
            "        self,",
            "        query: Dict[str, Dict[str, Dict[str, int]]],",
            "        user: UserID,",
            "        timeout: Optional[int],",
            "        always_include_fallback_keys: bool,",
            "    ) -> JsonDict:",
            "        local_query: List[Tuple[str, str, str, int]] = []",
            "        remote_queries: Dict[str, Dict[str, Dict[str, Dict[str, int]]]] = {}",
            "",
            "        for user_id, one_time_keys in query.items():",
            "            # we use UserID.from_string to catch invalid user ids",
            "            if self.is_mine(UserID.from_string(user_id)):",
            "                for device_id, algorithms in one_time_keys.items():",
            "                    for algorithm, count in algorithms.items():",
            "                        local_query.append((user_id, device_id, algorithm, count))",
            "            else:",
            "                domain = get_domain_from_id(user_id)",
            "                remote_queries.setdefault(domain, {})[user_id] = one_time_keys",
            "",
            "        set_tag(\"local_key_query\", str(local_query))",
            "        set_tag(\"remote_key_query\", str(remote_queries))",
            "",
            "        results = await self.claim_local_one_time_keys(",
            "            local_query, always_include_fallback_keys",
            "        )",
            "",
            "        # A map of user ID -> device ID -> key ID -> key.",
            "        json_result: Dict[str, Dict[str, Dict[str, JsonDict]]] = {}",
            "        for result in results:",
            "            for user_id, device_keys in result.items():",
            "                for device_id, keys in device_keys.items():",
            "                    for key_id, key in keys.items():",
            "                        json_result.setdefault(user_id, {}).setdefault(",
            "                            device_id, {}",
            "                        ).update({key_id: key})",
            "",
            "        # Remote failures.",
            "        failures: Dict[str, JsonDict] = {}",
            "",
            "        @trace",
            "        async def claim_client_keys(destination: str) -> None:",
            "            set_tag(\"destination\", destination)",
            "            device_keys = remote_queries[destination]",
            "            try:",
            "                remote_result = await self.federation.claim_client_keys(",
            "                    user, destination, device_keys, timeout=timeout",
            "                )",
            "                for user_id, keys in remote_result[\"one_time_keys\"].items():",
            "                    if user_id in device_keys:",
            "                        json_result[user_id] = keys",
            "",
            "            except Exception as e:",
            "                failure = _exception_to_failure(e)",
            "                failures[destination] = failure",
            "                set_tag(\"error\", True)",
            "                set_tag(\"reason\", str(failure))",
            "",
            "        await make_deferred_yieldable(",
            "            defer.gatherResults(",
            "                [",
            "                    run_in_background(claim_client_keys, destination)",
            "                    for destination in remote_queries",
            "                ],",
            "                consumeErrors=True,",
            "            )",
            "        )",
            "",
            "        logger.info(",
            "            \"Claimed one-time-keys: %s\",",
            "            \",\".join(",
            "                (",
            "                    \"%s for %s:%s\" % (key_id, user_id, device_id)",
            "                    for user_id, user_keys in json_result.items()",
            "                    for device_id, device_keys in user_keys.items()",
            "                    for key_id, _ in device_keys.items()",
            "                )",
            "            ),",
            "        )",
            "",
            "        log_kv({\"one_time_keys\": json_result, \"failures\": failures})",
            "        return {\"one_time_keys\": json_result, \"failures\": failures}",
            "",
            "    @tag_args",
            "    async def upload_keys_for_user(",
            "        self, user_id: str, device_id: str, keys: JsonDict",
            "    ) -> JsonDict:",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        time_now = self.clock.time_msec()",
            "",
            "        # TODO: Validate the JSON to make sure it has the right keys.",
            "        device_keys = keys.get(\"device_keys\", None)",
            "        if device_keys:",
            "            logger.info(",
            "                \"Updating device_keys for device %r for user %s at %d\",",
            "                device_id,",
            "                user_id,",
            "                time_now,",
            "            )",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Updating device_keys for user.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            # TODO: Sign the JSON with the server key",
            "            changed = await self.store.set_e2e_device_keys(",
            "                user_id, device_id, time_now, device_keys",
            "            )",
            "            if changed:",
            "                # Only notify about device updates *if* the keys actually changed",
            "                await self.device_handler.notify_device_update(user_id, [device_id])",
            "        else:",
            "            log_kv({\"message\": \"Not updating device_keys for user\", \"user_id\": user_id})",
            "        one_time_keys = keys.get(\"one_time_keys\", None)",
            "        if one_time_keys:",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Updating one_time_keys for device.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            await self._upload_one_time_keys_for_user(",
            "                user_id, device_id, time_now, one_time_keys",
            "            )",
            "        else:",
            "            log_kv(",
            "                {\"message\": \"Did not update one_time_keys\", \"reason\": \"no keys given\"}",
            "            )",
            "        fallback_keys = keys.get(\"fallback_keys\") or keys.get(",
            "            \"org.matrix.msc2732.fallback_keys\"",
            "        )",
            "        if fallback_keys and isinstance(fallback_keys, dict):",
            "            log_kv(",
            "                {",
            "                    \"message\": \"Updating fallback_keys for device.\",",
            "                    \"user_id\": user_id,",
            "                    \"device_id\": device_id,",
            "                }",
            "            )",
            "            await self.store.set_e2e_fallback_keys(user_id, device_id, fallback_keys)",
            "        elif fallback_keys:",
            "            log_kv({\"message\": \"Did not update fallback_keys\", \"reason\": \"not a dict\"})",
            "        else:",
            "            log_kv(",
            "                {\"message\": \"Did not update fallback_keys\", \"reason\": \"no keys given\"}",
            "            )",
            "",
            "        # the device should have been registered already, but it may have been",
            "        # deleted due to a race with a DELETE request. Or we may be using an",
            "        # old access_token without an associated device_id. Either way, we",
            "        # need to double-check the device is registered to avoid ending up with",
            "        # keys without a corresponding device.",
            "        await self.device_handler.check_device_registered(user_id, device_id)",
            "",
            "        result = await self.store.count_e2e_one_time_keys(user_id, device_id)",
            "",
            "        set_tag(\"one_time_key_counts\", str(result))",
            "        return {\"one_time_key_counts\": result}",
            "",
            "    async def _upload_one_time_keys_for_user(",
            "        self, user_id: str, device_id: str, time_now: int, one_time_keys: JsonDict",
            "    ) -> None:",
            "        logger.info(",
            "            \"Adding one_time_keys %r for device %r for user %r at %d\",",
            "            one_time_keys.keys(),",
            "            device_id,",
            "            user_id,",
            "            time_now,",
            "        )",
            "",
            "        # make a list of (alg, id, key) tuples",
            "        key_list = []",
            "        for key_id, key_obj in one_time_keys.items():",
            "            algorithm, key_id = key_id.split(\":\")",
            "            key_list.append((algorithm, key_id, key_obj))",
            "",
            "        # First we check if we have already persisted any of the keys.",
            "        existing_key_map = await self.store.get_e2e_one_time_keys(",
            "            user_id, device_id, [k_id for _, k_id, _ in key_list]",
            "        )",
            "",
            "        new_keys = []  # Keys that we need to insert. (alg, id, json) tuples.",
            "        for algorithm, key_id, key in key_list:",
            "            ex_json = existing_key_map.get((algorithm, key_id), None)",
            "            if ex_json:",
            "                if not _one_time_keys_match(ex_json, key):",
            "                    raise SynapseError(",
            "                        400,",
            "                        (",
            "                            \"One time key %s:%s already exists. \"",
            "                            \"Old key: %s; new key: %r\"",
            "                        )",
            "                        % (algorithm, key_id, ex_json, key),",
            "                    )",
            "            else:",
            "                new_keys.append(",
            "                    (algorithm, key_id, encode_canonical_json(key).decode(\"ascii\"))",
            "                )",
            "",
            "        log_kv({\"message\": \"Inserting new one_time_keys.\", \"keys\": new_keys})",
            "        await self.store.add_e2e_one_time_keys(user_id, device_id, time_now, new_keys)",
            "",
            "    async def upload_signing_keys_for_user(",
            "        self, user_id: str, keys: JsonDict",
            "    ) -> JsonDict:",
            "        \"\"\"Upload signing keys for cross-signing",
            "",
            "        Args:",
            "            user_id: the user uploading the keys",
            "            keys: the signing keys",
            "        \"\"\"",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        # if a master key is uploaded, then check it.  Otherwise, load the",
            "        # stored master key, to check signatures on other keys",
            "        if \"master_key\" in keys:",
            "            master_key = keys[\"master_key\"]",
            "",
            "            _check_cross_signing_key(master_key, user_id, \"master\")",
            "        else:",
            "            master_key = await self.store.get_e2e_cross_signing_key(user_id, \"master\")",
            "",
            "        # if there is no master key, then we can't do anything, because all the",
            "        # other cross-signing keys need to be signed by the master key",
            "        if not master_key:",
            "            raise SynapseError(400, \"No master key available\", Codes.MISSING_PARAM)",
            "",
            "        try:",
            "            master_key_id, master_verify_key = get_verify_key_from_cross_signing_key(",
            "                master_key",
            "            )",
            "        except ValueError:",
            "            if \"master_key\" in keys:",
            "                # the invalid key came from the request",
            "                raise SynapseError(400, \"Invalid master key\", Codes.INVALID_PARAM)",
            "            else:",
            "                # the invalid key came from the database",
            "                logger.error(\"Invalid master key found for user %s\", user_id)",
            "                raise SynapseError(500, \"Invalid master key\")",
            "",
            "        # for the other cross-signing keys, make sure that they have valid",
            "        # signatures from the master key",
            "        if \"self_signing_key\" in keys:",
            "            self_signing_key = keys[\"self_signing_key\"]",
            "",
            "            _check_cross_signing_key(",
            "                self_signing_key, user_id, \"self_signing\", master_verify_key",
            "            )",
            "",
            "        if \"user_signing_key\" in keys:",
            "            user_signing_key = keys[\"user_signing_key\"]",
            "",
            "            _check_cross_signing_key(",
            "                user_signing_key, user_id, \"user_signing\", master_verify_key",
            "            )",
            "",
            "        # if everything checks out, then store the keys and send notifications",
            "        deviceids = []",
            "        if \"master_key\" in keys:",
            "            await self.store.set_e2e_cross_signing_key(user_id, \"master\", master_key)",
            "            deviceids.append(master_verify_key.version)",
            "        if \"self_signing_key\" in keys:",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user_id, \"self_signing\", self_signing_key",
            "            )",
            "            try:",
            "                deviceids.append(",
            "                    get_verify_key_from_cross_signing_key(self_signing_key)[1].version",
            "                )",
            "            except ValueError:",
            "                raise SynapseError(400, \"Invalid self-signing key\", Codes.INVALID_PARAM)",
            "        if \"user_signing_key\" in keys:",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user_id, \"user_signing\", user_signing_key",
            "            )",
            "            # the signature stream matches the semantics that we want for",
            "            # user-signing key updates: only the user themselves is notified of",
            "            # their own user-signing key updates",
            "            await self.device_handler.notify_user_signature_update(user_id, [user_id])",
            "",
            "        # master key and self-signing key updates match the semantics of device",
            "        # list updates: all users who share an encrypted room are notified",
            "        if len(deviceids):",
            "            await self.device_handler.notify_device_update(user_id, deviceids)",
            "",
            "        return {}",
            "",
            "    async def upload_signatures_for_device_keys(",
            "        self, user_id: str, signatures: JsonDict",
            "    ) -> JsonDict:",
            "        \"\"\"Upload device signatures for cross-signing",
            "",
            "        Args:",
            "            user_id: the user uploading the signatures",
            "            signatures: map of users to devices to signed keys. This is the submission",
            "            from the user; an exception will be raised if it is malformed.",
            "        Returns:",
            "            The response to be sent back to the client.  The response will have",
            "                a \"failures\" key, which will be a dict mapping users to devices",
            "                to errors for the signatures that failed.",
            "        Raises:",
            "            SynapseError: if the signatures dict is not valid.",
            "        \"\"\"",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        failures = {}",
            "",
            "        # signatures to be stored.  Each item will be a SignatureListItem",
            "        signature_list = []",
            "",
            "        # split between checking signatures for own user and signatures for",
            "        # other users, since we verify them with different keys",
            "        self_signatures = signatures.get(user_id, {})",
            "        other_signatures = {k: v for k, v in signatures.items() if k != user_id}",
            "",
            "        self_signature_list, self_failures = await self._process_self_signatures(",
            "            user_id, self_signatures",
            "        )",
            "        signature_list.extend(self_signature_list)",
            "        failures.update(self_failures)",
            "",
            "        other_signature_list, other_failures = await self._process_other_signatures(",
            "            user_id, other_signatures",
            "        )",
            "        signature_list.extend(other_signature_list)",
            "        failures.update(other_failures)",
            "",
            "        # store the signature, and send the appropriate notifications for sync",
            "        logger.debug(\"upload signature failures: %r\", failures)",
            "        await self.store.store_e2e_cross_signing_signatures(user_id, signature_list)",
            "",
            "        self_device_ids = [item.target_device_id for item in self_signature_list]",
            "        if self_device_ids:",
            "            await self.device_handler.notify_device_update(user_id, self_device_ids)",
            "        signed_users = [item.target_user_id for item in other_signature_list]",
            "        if signed_users:",
            "            await self.device_handler.notify_user_signature_update(",
            "                user_id, signed_users",
            "            )",
            "",
            "        return {\"failures\": failures}",
            "",
            "    async def _process_self_signatures(",
            "        self, user_id: str, signatures: JsonDict",
            "    ) -> Tuple[List[\"SignatureListItem\"], Dict[str, Dict[str, dict]]]:",
            "        \"\"\"Process uploaded signatures of the user's own keys.",
            "",
            "        Signatures of the user's own keys from this API come in two forms:",
            "        - signatures of the user's devices by the user's self-signing key,",
            "        - signatures of the user's master key by the user's devices.",
            "",
            "        Args:",
            "            user_id: the user uploading the keys",
            "            signatures (dict[string, dict]): map of devices to signed keys",
            "",
            "        Returns:",
            "            A tuple of a list of signatures to store, and a map of users to",
            "            devices to failure reasons",
            "",
            "        Raises:",
            "            SynapseError: if the input is malformed",
            "        \"\"\"",
            "        signature_list: List[\"SignatureListItem\"] = []",
            "        failures: Dict[str, Dict[str, JsonDict]] = {}",
            "        if not signatures:",
            "            return signature_list, failures",
            "",
            "        if not isinstance(signatures, dict):",
            "            raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "",
            "        try:",
            "            # get our self-signing key to verify the signatures",
            "            (",
            "                _,",
            "                self_signing_key_id,",
            "                self_signing_verify_key,",
            "            ) = await self._get_e2e_cross_signing_verify_key(user_id, \"self_signing\")",
            "",
            "            # get our master key, since we may have received a signature of it.",
            "            # We need to fetch it here so that we know what its key ID is, so",
            "            # that we can check if a signature that was sent is a signature of",
            "            # the master key or of a device",
            "            (",
            "                master_key,",
            "                _,",
            "                master_verify_key,",
            "            ) = await self._get_e2e_cross_signing_verify_key(user_id, \"master\")",
            "",
            "            # fetch our stored devices.  This is used to 1. verify",
            "            # signatures on the master key, and 2. to compare with what",
            "            # was sent if the device was signed",
            "            devices = await self.store.get_e2e_device_keys_for_cs_api([(user_id, None)])",
            "",
            "            if user_id not in devices:",
            "                raise NotFoundError(\"No device keys found\")",
            "",
            "            devices = devices[user_id]",
            "        except SynapseError as e:",
            "            failure = _exception_to_failure(e)",
            "            failures[user_id] = {device: failure for device in signatures.keys()}",
            "            return signature_list, failures",
            "",
            "        for device_id, device in signatures.items():",
            "            # make sure submitted data is in the right form",
            "            if not isinstance(device, dict):",
            "                raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "",
            "            try:",
            "                if \"signatures\" not in device or user_id not in device[\"signatures\"]:",
            "                    # no signature was sent",
            "                    raise SynapseError(",
            "                        400, \"Invalid signature\", Codes.INVALID_SIGNATURE",
            "                    )",
            "",
            "                if device_id == master_verify_key.version:",
            "                    # The signature is of the master key. This needs to be",
            "                    # handled differently from signatures of normal devices.",
            "                    master_key_signature_list = self._check_master_key_signature(",
            "                        user_id, device_id, device, master_key, devices",
            "                    )",
            "                    signature_list.extend(master_key_signature_list)",
            "                    continue",
            "",
            "                # at this point, we have a device that should be signed",
            "                # by the self-signing key",
            "                if self_signing_key_id not in device[\"signatures\"][user_id]:",
            "                    # no signature was sent",
            "                    raise SynapseError(",
            "                        400, \"Invalid signature\", Codes.INVALID_SIGNATURE",
            "                    )",
            "",
            "                try:",
            "                    stored_device = devices[device_id]",
            "                except KeyError:",
            "                    raise NotFoundError(\"Unknown device\")",
            "                if self_signing_key_id in stored_device.get(\"signatures\", {}).get(",
            "                    user_id, {}",
            "                ):",
            "                    # we already have a signature on this device, so we",
            "                    # can skip it, since it should be exactly the same",
            "                    continue",
            "",
            "                _check_device_signature(",
            "                    user_id, self_signing_verify_key, device, stored_device",
            "                )",
            "",
            "                signature = device[\"signatures\"][user_id][self_signing_key_id]",
            "                signature_list.append(",
            "                    SignatureListItem(",
            "                        self_signing_key_id, user_id, device_id, signature",
            "                    )",
            "                )",
            "            except SynapseError as e:",
            "                failures.setdefault(user_id, {})[device_id] = _exception_to_failure(e)",
            "",
            "        return signature_list, failures",
            "",
            "    def _check_master_key_signature(",
            "        self,",
            "        user_id: str,",
            "        master_key_id: str,",
            "        signed_master_key: JsonDict,",
            "        stored_master_key: JsonMapping,",
            "        devices: Dict[str, Dict[str, JsonDict]],",
            "    ) -> List[\"SignatureListItem\"]:",
            "        \"\"\"Check signatures of a user's master key made by their devices.",
            "",
            "        Args:",
            "            user_id: the user whose master key is being checked",
            "            master_key_id: the ID of the user's master key",
            "            signed_master_key: the user's signed master key that was uploaded",
            "            stored_master_key: our previously-stored copy of the user's master key",
            "            devices: the user's devices",
            "",
            "        Returns:",
            "            A list of signatures to store",
            "",
            "        Raises:",
            "            SynapseError: if a signature is invalid",
            "        \"\"\"",
            "        # for each device that signed the master key, check the signature.",
            "        master_key_signature_list = []",
            "        sigs = signed_master_key[\"signatures\"]",
            "        for signing_key_id, signature in sigs[user_id].items():",
            "            _, signing_device_id = signing_key_id.split(\":\", 1)",
            "            if (",
            "                signing_device_id not in devices",
            "                or signing_key_id not in devices[signing_device_id][\"keys\"]",
            "            ):",
            "                # signed by an unknown device, or the",
            "                # device does not have the key",
            "                raise SynapseError(400, \"Invalid signature\", Codes.INVALID_SIGNATURE)",
            "",
            "            # get the key and check the signature",
            "            pubkey = devices[signing_device_id][\"keys\"][signing_key_id]",
            "            verify_key = decode_verify_key_bytes(signing_key_id, decode_base64(pubkey))",
            "            _check_device_signature(",
            "                user_id, verify_key, signed_master_key, stored_master_key",
            "            )",
            "",
            "            master_key_signature_list.append(",
            "                SignatureListItem(signing_key_id, user_id, master_key_id, signature)",
            "            )",
            "",
            "        return master_key_signature_list",
            "",
            "    async def _process_other_signatures(",
            "        self, user_id: str, signatures: Dict[str, dict]",
            "    ) -> Tuple[List[\"SignatureListItem\"], Dict[str, Dict[str, dict]]]:",
            "        \"\"\"Process uploaded signatures of other users' keys.  These will be the",
            "        target user's master keys, signed by the uploading user's user-signing",
            "        key.",
            "",
            "        Args:",
            "            user_id: the user uploading the keys",
            "            signatures: map of users to devices to signed keys",
            "",
            "        Returns:",
            "            A list of signatures to store, and a map of users to devices to failure",
            "            reasons",
            "",
            "        Raises:",
            "            SynapseError: if the input is malformed",
            "        \"\"\"",
            "        signature_list: List[\"SignatureListItem\"] = []",
            "        failures: Dict[str, Dict[str, JsonDict]] = {}",
            "        if not signatures:",
            "            return signature_list, failures",
            "",
            "        try:",
            "            # get our user-signing key to verify the signatures",
            "            (",
            "                user_signing_key,",
            "                user_signing_key_id,",
            "                user_signing_verify_key,",
            "            ) = await self._get_e2e_cross_signing_verify_key(user_id, \"user_signing\")",
            "        except SynapseError as e:",
            "            failure = _exception_to_failure(e)",
            "            for user, devicemap in signatures.items():",
            "                failures[user] = {device_id: failure for device_id in devicemap.keys()}",
            "            return signature_list, failures",
            "",
            "        for target_user, devicemap in signatures.items():",
            "            # make sure submitted data is in the right form",
            "            if not isinstance(devicemap, dict):",
            "                raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "            for device in devicemap.values():",
            "                if not isinstance(device, dict):",
            "                    raise SynapseError(400, \"Invalid parameter\", Codes.INVALID_PARAM)",
            "",
            "            device_id = None",
            "            try:",
            "                # get the target user's master key, to make sure it matches",
            "                # what was sent",
            "                (",
            "                    master_key,",
            "                    master_key_id,",
            "                    _,",
            "                ) = await self._get_e2e_cross_signing_verify_key(",
            "                    target_user, \"master\", user_id",
            "                )",
            "",
            "                # make sure that the target user's master key is the one that",
            "                # was signed (and no others)",
            "                device_id = master_key_id.split(\":\", 1)[1]",
            "                if device_id not in devicemap:",
            "                    logger.debug(",
            "                        \"upload signature: could not find signature for device %s\",",
            "                        device_id,",
            "                    )",
            "                    # set device to None so that the failure gets",
            "                    # marked on all the signatures",
            "                    device_id = None",
            "                    raise NotFoundError(\"Unknown device\")",
            "                key = devicemap[device_id]",
            "                other_devices = [k for k in devicemap.keys() if k != device_id]",
            "                if other_devices:",
            "                    # other devices were signed -- mark those as failures",
            "                    logger.debug(\"upload signature: too many devices specified\")",
            "                    failure = _exception_to_failure(NotFoundError(\"Unknown device\"))",
            "                    failures[target_user] = {",
            "                        device: failure for device in other_devices",
            "                    }",
            "",
            "                if user_signing_key_id in master_key.get(\"signatures\", {}).get(",
            "                    user_id, {}",
            "                ):",
            "                    # we already have the signature, so we can skip it",
            "                    continue",
            "",
            "                _check_device_signature(",
            "                    user_id, user_signing_verify_key, key, master_key",
            "                )",
            "",
            "                signature = key[\"signatures\"][user_id][user_signing_key_id]",
            "                signature_list.append(",
            "                    SignatureListItem(",
            "                        user_signing_key_id, target_user, device_id, signature",
            "                    )",
            "                )",
            "            except SynapseError as e:",
            "                failure = _exception_to_failure(e)",
            "                if device_id is None:",
            "                    failures[target_user] = {",
            "                        device_id: failure for device_id in devicemap.keys()",
            "                    }",
            "                else:",
            "                    failures.setdefault(target_user, {})[device_id] = failure",
            "",
            "        return signature_list, failures",
            "",
            "    async def _get_e2e_cross_signing_verify_key(",
            "        self, user_id: str, key_type: str, from_user_id: Optional[str] = None",
            "    ) -> Tuple[JsonMapping, str, VerifyKey]:",
            "        \"\"\"Fetch locally or remotely query for a cross-signing public key.",
            "",
            "        First, attempt to fetch the cross-signing public key from storage.",
            "        If that fails, query the keys from the homeserver they belong to",
            "        and update our local copy.",
            "",
            "        Args:",
            "            user_id: the user whose key should be fetched",
            "            key_type: the type of key to fetch",
            "            from_user_id: the user that we are fetching the keys for.",
            "                This affects what signatures are fetched.",
            "",
            "        Returns:",
            "            The raw key data, the key ID, and the signedjson verify key",
            "",
            "        Raises:",
            "            NotFoundError: if the key is not found",
            "            SynapseError: if `user_id` is invalid",
            "        \"\"\"",
            "        user = UserID.from_string(user_id)",
            "        key = await self.store.get_e2e_cross_signing_key(",
            "            user_id, key_type, from_user_id",
            "        )",
            "",
            "        if key:",
            "            # We found a copy of this key in our database. Decode and return it",
            "            key_id, verify_key = get_verify_key_from_cross_signing_key(key)",
            "            return key, key_id, verify_key",
            "",
            "        # If we couldn't find the key locally, and we're looking for keys of",
            "        # another user then attempt to fetch the missing key from the remote",
            "        # user's server.",
            "        #",
            "        # We may run into this in possible edge cases where a user tries to",
            "        # cross-sign a remote user, but does not share any rooms with them yet.",
            "        # Thus, we would not have their key list yet. We instead fetch the key,",
            "        # store it and notify clients of new, associated device IDs.",
            "        if self.is_mine(user) or key_type not in [\"master\", \"self_signing\"]:",
            "            # Note that master and self_signing keys are the only cross-signing keys we",
            "            # can request over federation",
            "            raise NotFoundError(\"No %s key found for %s\" % (key_type, user_id))",
            "",
            "        cross_signing_keys = await self._retrieve_cross_signing_keys_for_remote_user(",
            "            user, key_type",
            "        )",
            "        if cross_signing_keys is None:",
            "            raise NotFoundError(\"No %s key found for %s\" % (key_type, user_id))",
            "",
            "        return cross_signing_keys",
            "",
            "    async def _retrieve_cross_signing_keys_for_remote_user(",
            "        self,",
            "        user: UserID,",
            "        desired_key_type: str,",
            "    ) -> Optional[Tuple[JsonMapping, str, VerifyKey]]:",
            "        \"\"\"Queries cross-signing keys for a remote user and saves them to the database",
            "",
            "        Only the key specified by `key_type` will be returned, while all retrieved keys",
            "        will be saved regardless",
            "",
            "        Args:",
            "            user: The user to query remote keys for",
            "            desired_key_type: The type of key to receive. One of \"master\", \"self_signing\"",
            "",
            "        Returns:",
            "            A tuple of the retrieved key content, the key's ID and the matching VerifyKey.",
            "            If the key cannot be retrieved, all values in the tuple will instead be None.",
            "        \"\"\"",
            "        # This can only be called from the main process.",
            "        assert isinstance(self.device_handler, DeviceHandler)",
            "",
            "        try:",
            "            remote_result = await self.federation.query_user_devices(",
            "                user.domain, user.to_string()",
            "            )",
            "        except Exception as e:",
            "            logger.warning(",
            "                \"Unable to query %s for cross-signing keys of user %s: %s %s\",",
            "                user.domain,",
            "                user.to_string(),",
            "                type(e),",
            "                e,",
            "            )",
            "            return None",
            "",
            "        # Process each of the retrieved cross-signing keys",
            "        desired_key_data = None",
            "        retrieved_device_ids = []",
            "        for key_type in [\"master\", \"self_signing\"]:",
            "            key_content = remote_result.get(key_type + \"_key\")",
            "            if not key_content:",
            "                continue",
            "",
            "            # Ensure these keys belong to the correct user",
            "            if \"user_id\" not in key_content:",
            "                logger.warning(",
            "                    \"Invalid %s key retrieved, missing user_id field: %s\",",
            "                    key_type,",
            "                    key_content,",
            "                )",
            "                continue",
            "            if user.to_string() != key_content[\"user_id\"]:",
            "                logger.warning(",
            "                    \"Found %s key of user %s when querying for keys of user %s\",",
            "                    key_type,",
            "                    key_content[\"user_id\"],",
            "                    user.to_string(),",
            "                )",
            "                continue",
            "",
            "            # Validate the key contents",
            "            try:",
            "                # verify_key is a VerifyKey from signedjson, which uses",
            "                # .version to denote the portion of the key ID after the",
            "                # algorithm and colon, which is the device ID",
            "                key_id, verify_key = get_verify_key_from_cross_signing_key(key_content)",
            "            except ValueError as e:",
            "                logger.warning(",
            "                    \"Invalid %s key retrieved: %s - %s %s\",",
            "                    key_type,",
            "                    key_content,",
            "                    type(e),",
            "                    e,",
            "                )",
            "                continue",
            "",
            "            # Note down the device ID attached to this key",
            "            retrieved_device_ids.append(verify_key.version)",
            "",
            "            # If this is the desired key type, save it and its ID/VerifyKey",
            "            if key_type == desired_key_type:",
            "                desired_key_data = key_content, key_id, verify_key",
            "",
            "            # At the same time, store this key in the db for subsequent queries",
            "            await self.store.set_e2e_cross_signing_key(",
            "                user.to_string(), key_type, key_content",
            "            )",
            "",
            "        # Notify clients that new devices for this user have been discovered",
            "        if retrieved_device_ids:",
            "            # XXX is this necessary?",
            "            await self.device_handler.notify_device_update(",
            "                user.to_string(), retrieved_device_ids",
            "            )",
            "",
            "        return desired_key_data",
            "",
            "    async def is_cross_signing_set_up_for_user(self, user_id: str) -> bool:",
            "        \"\"\"Checks if the user has cross-signing set up",
            "",
            "        Args:",
            "            user_id: The user to check",
            "",
            "        Returns:",
            "            True if the user has cross-signing set up, False otherwise",
            "        \"\"\"",
            "        existing_master_key = await self.store.get_e2e_cross_signing_key(",
            "            user_id, \"master\"",
            "        )",
            "        return existing_master_key is not None",
            "",
            "",
            "def _check_cross_signing_key(",
            "    key: JsonDict, user_id: str, key_type: str, signing_key: Optional[VerifyKey] = None",
            ") -> None:",
            "    \"\"\"Check a cross-signing key uploaded by a user.  Performs some basic sanity",
            "    checking, and ensures that it is signed, if a signature is required.",
            "",
            "    Args:",
            "        key: the key data to verify",
            "        user_id: the user whose key is being checked",
            "        key_type: the type of key that the key should be",
            "        signing_key: the signing key that the key should be signed with.  If",
            "            omitted, signatures will not be checked.",
            "    \"\"\"",
            "    if (",
            "        key.get(\"user_id\") != user_id",
            "        or key_type not in key.get(\"usage\", [])",
            "        or len(key.get(\"keys\", {})) != 1",
            "    ):",
            "        raise SynapseError(400, (\"Invalid %s key\" % (key_type,)), Codes.INVALID_PARAM)",
            "",
            "    if signing_key:",
            "        try:",
            "            verify_signed_json(key, user_id, signing_key)",
            "        except SignatureVerifyException:",
            "            raise SynapseError(",
            "                400, (\"Invalid signature on %s key\" % key_type), Codes.INVALID_SIGNATURE",
            "            )",
            "",
            "",
            "def _check_device_signature(",
            "    user_id: str,",
            "    verify_key: VerifyKey,",
            "    signed_device: JsonDict,",
            "    stored_device: JsonMapping,",
            ") -> None:",
            "    \"\"\"Check that a signature on a device or cross-signing key is correct and",
            "    matches the copy of the device/key that we have stored.  Throws an",
            "    exception if an error is detected.",
            "",
            "    Args:",
            "        user_id: the user ID whose signature is being checked",
            "        verify_key: the key to verify the device with",
            "        signed_device: the uploaded signed device data",
            "        stored_device: our previously stored copy of the device",
            "",
            "    Raises:",
            "        SynapseError: if the signature was invalid or the sent device is not the",
            "            same as the stored device",
            "",
            "    \"\"\"",
            "",
            "    # make sure that the device submitted matches what we have stored",
            "    stripped_signed_device = {",
            "        k: v for k, v in signed_device.items() if k not in [\"signatures\", \"unsigned\"]",
            "    }",
            "    stripped_stored_device = {",
            "        k: v for k, v in stored_device.items() if k not in [\"signatures\", \"unsigned\"]",
            "    }",
            "    if stripped_signed_device != stripped_stored_device:",
            "        logger.debug(",
            "            \"upload signatures: key does not match %s vs %s\",",
            "            signed_device,",
            "            stored_device,",
            "        )",
            "        raise SynapseError(400, \"Key does not match\")",
            "",
            "    try:",
            "        verify_signed_json(signed_device, user_id, verify_key)",
            "    except SignatureVerifyException:",
            "        logger.debug(\"invalid signature on key\")",
            "        raise SynapseError(400, \"Invalid signature\", Codes.INVALID_SIGNATURE)",
            "",
            "",
            "def _exception_to_failure(e: Exception) -> JsonDict:",
            "    if isinstance(e, SynapseError):",
            "        return {\"status\": e.code, \"errcode\": e.errcode, \"message\": str(e)}",
            "",
            "    if isinstance(e, CodeMessageException):",
            "        return {\"status\": e.code, \"message\": str(e)}",
            "",
            "    if isinstance(e, NotRetryingDestination):",
            "        return {\"status\": 503, \"message\": \"Not ready for retry\"}",
            "",
            "    # include ConnectionRefused and other errors",
            "    #",
            "    # Note that some Exceptions (notably twisted's ResponseFailed etc) don't",
            "    # give a string for e.message, which json then fails to serialize.",
            "    return {\"status\": 503, \"message\": str(e)}",
            "",
            "",
            "def _one_time_keys_match(old_key_json: str, new_key: JsonDict) -> bool:",
            "    old_key = json_decoder.decode(old_key_json)",
            "",
            "    # if either is a string rather than an object, they must match exactly",
            "    if not isinstance(old_key, dict) or not isinstance(new_key, dict):",
            "        return old_key == new_key",
            "",
            "    # otherwise, we strip off the 'signatures' if any, because it's legitimate",
            "    # for different upload attempts to have different signatures.",
            "    old_key.pop(\"signatures\", None)",
            "    new_key_copy = dict(new_key)",
            "    new_key_copy.pop(\"signatures\", None)",
            "",
            "    return old_key == new_key_copy",
            "",
            "",
            "@attr.s(slots=True, auto_attribs=True)",
            "class SignatureListItem:",
            "    \"\"\"An item in the signature list as used by upload_signatures_for_device_keys.\"\"\"",
            "",
            "    signing_key_id: str",
            "    target_user_id: str",
            "    target_device_id: str",
            "    signature: JsonDict",
            "",
            "",
            "class SigningKeyEduUpdater:",
            "    \"\"\"Handles incoming signing key updates from federation and updates the DB\"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.store = hs.get_datastores().main",
            "        self.federation = hs.get_federation_client()",
            "        self.clock = hs.get_clock()",
            "",
            "        device_handler = hs.get_device_handler()",
            "        assert isinstance(device_handler, DeviceHandler)",
            "        self._device_handler = device_handler",
            "",
            "        self._remote_edu_linearizer = Linearizer(name=\"remote_signing_key\")",
            "",
            "        # user_id -> list of updates waiting to be handled.",
            "        self._pending_updates: Dict[str, List[Tuple[JsonDict, JsonDict]]] = {}",
            "",
            "    async def incoming_signing_key_update(",
            "        self, origin: str, edu_content: JsonDict",
            "    ) -> None:",
            "        \"\"\"Called on incoming signing key update from federation. Responsible for",
            "        parsing the EDU and adding to pending updates list.",
            "",
            "        Args:",
            "            origin: the server that sent the EDU",
            "            edu_content: the contents of the EDU",
            "        \"\"\"",
            "",
            "        user_id = edu_content.pop(\"user_id\")",
            "        master_key = edu_content.pop(\"master_key\", None)",
            "        self_signing_key = edu_content.pop(\"self_signing_key\", None)",
            "",
            "        if get_domain_from_id(user_id) != origin:",
            "            logger.warning(\"Got signing key update edu for %r from %r\", user_id, origin)",
            "            return",
            "",
            "        room_ids = await self.store.get_rooms_for_user(user_id)",
            "        if not room_ids:",
            "            # We don't share any rooms with this user. Ignore update, as we",
            "            # probably won't get any further updates.",
            "            return",
            "",
            "        self._pending_updates.setdefault(user_id, []).append(",
            "            (master_key, self_signing_key)",
            "        )",
            "",
            "        await self._handle_signing_key_updates(user_id)",
            "",
            "    async def _handle_signing_key_updates(self, user_id: str) -> None:",
            "        \"\"\"Actually handle pending updates.",
            "",
            "        Args:",
            "            user_id: the user whose updates we are processing",
            "        \"\"\"",
            "",
            "        async with self._remote_edu_linearizer.queue(user_id):",
            "            pending_updates = self._pending_updates.pop(user_id, [])",
            "            if not pending_updates:",
            "                # This can happen since we batch updates",
            "                return",
            "",
            "            device_ids: List[str] = []",
            "",
            "            logger.info(\"pending updates: %r\", pending_updates)",
            "",
            "            for master_key, self_signing_key in pending_updates:",
            "                new_device_ids = await self._device_handler.device_list_updater.process_cross_signing_key_update(",
            "                    user_id,",
            "                    master_key,",
            "                    self_signing_key,",
            "                )",
            "                device_ids = device_ids + new_device_ids",
            "",
            "            await self._device_handler.notify_device_update(user_id, device_ids)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "synapse.handlers.e2e_keys.E2eKeysHandler.self"
        ]
    }
}