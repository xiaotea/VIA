{
    "synapse/config/_util.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "         config: the configuration value to be validated"
            },
            "1": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "         config_path: the path within the config file. This will be used as a basis"
            },
            "2": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 35,
                "PatchRowcode": "            for the error message."
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+    Raises:"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+        ConfigError, if validation fails."
            },
            "6": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "     \"\"\""
            },
            "7": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "     try:"
            },
            "8": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "         jsonschema.validate(config, json_schema)"
            }
        },
        "frontPatchFile": [
            "# Copyright 2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "from typing import Any, Iterable",
            "",
            "import jsonschema",
            "",
            "from synapse.config._base import ConfigError",
            "from synapse.types import JsonDict",
            "",
            "",
            "def validate_config(",
            "    json_schema: JsonDict, config: Any, config_path: Iterable[str]",
            ") -> None:",
            "    \"\"\"Validates a config setting against a JsonSchema definition",
            "",
            "    This can be used to validate a section of the config file against a schema",
            "    definition. If the validation fails, a ConfigError is raised with a textual",
            "    description of the problem.",
            "",
            "    Args:",
            "        json_schema: the schema to validate against",
            "        config: the configuration value to be validated",
            "        config_path: the path within the config file. This will be used as a basis",
            "           for the error message.",
            "    \"\"\"",
            "    try:",
            "        jsonschema.validate(config, json_schema)",
            "    except jsonschema.ValidationError as e:",
            "        raise json_error_to_config_error(e, config_path)",
            "",
            "",
            "def json_error_to_config_error(",
            "    e: jsonschema.ValidationError, config_path: Iterable[str]",
            ") -> ConfigError:",
            "    \"\"\"Converts a json validation error to a user-readable ConfigError",
            "",
            "    Args:",
            "        e: the exception to be converted",
            "        config_path: the path within the config file. This will be used as a basis",
            "           for the error message.",
            "",
            "    Returns:",
            "        a ConfigError",
            "    \"\"\"",
            "    # copy `config_path` before modifying it.",
            "    path = list(config_path)",
            "    for p in list(e.absolute_path):",
            "        if isinstance(p, int):",
            "            path.append(\"<item %i>\" % p)",
            "        else:",
            "            path.append(str(p))",
            "    return ConfigError(e.message, path)"
        ],
        "afterPatchFile": [
            "# Copyright 2020 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "from typing import Any, Iterable",
            "",
            "import jsonschema",
            "",
            "from synapse.config._base import ConfigError",
            "from synapse.types import JsonDict",
            "",
            "",
            "def validate_config(",
            "    json_schema: JsonDict, config: Any, config_path: Iterable[str]",
            ") -> None:",
            "    \"\"\"Validates a config setting against a JsonSchema definition",
            "",
            "    This can be used to validate a section of the config file against a schema",
            "    definition. If the validation fails, a ConfigError is raised with a textual",
            "    description of the problem.",
            "",
            "    Args:",
            "        json_schema: the schema to validate against",
            "        config: the configuration value to be validated",
            "        config_path: the path within the config file. This will be used as a basis",
            "           for the error message.",
            "",
            "    Raises:",
            "        ConfigError, if validation fails.",
            "    \"\"\"",
            "    try:",
            "        jsonschema.validate(config, json_schema)",
            "    except jsonschema.ValidationError as e:",
            "        raise json_error_to_config_error(e, config_path)",
            "",
            "",
            "def json_error_to_config_error(",
            "    e: jsonschema.ValidationError, config_path: Iterable[str]",
            ") -> ConfigError:",
            "    \"\"\"Converts a json validation error to a user-readable ConfigError",
            "",
            "    Args:",
            "        e: the exception to be converted",
            "        config_path: the path within the config file. This will be used as a basis",
            "           for the error message.",
            "",
            "    Returns:",
            "        a ConfigError",
            "    \"\"\"",
            "    # copy `config_path` before modifying it.",
            "    path = list(config_path)",
            "    for p in list(e.absolute_path):",
            "        if isinstance(p, int):",
            "            path.append(\"<item %i>\" % p)",
            "        else:",
            "            path.append(str(p))",
            "    return ConfigError(e.message, path)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "jupyterhub.services.auth.HubOAuth"
        ]
    },
    "synapse/config/api.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 13,
                "PatchRowcode": " # limitations under the License."
            },
            "1": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " import logging"
            },
            "3": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from typing import Any, Iterable"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from typing import Any, Iterable, Optional, Tuple"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from synapse.api.constants import EventTypes"
            },
            "7": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from synapse.config._base import Config, ConfigError"
            },
            "8": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from synapse.config._util import validate_config"
            },
            "9": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from synapse.types import JsonDict"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+from synapse.types.state import StateFilter"
            },
            "11": {
                "beforePatchRowNumber": 22,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "12": {
                "beforePatchRowNumber": 23,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " logger = logging.getLogger(__name__)"
            },
            "13": {
                "beforePatchRowNumber": 24,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 25,
                "afterPatchRowNumber": 26,
                "PatchRowcode": " "
            },
            "15": {
                "beforePatchRowNumber": 26,
                "afterPatchRowNumber": 27,
                "PatchRowcode": " class ApiConfig(Config):"
            },
            "16": {
                "beforePatchRowNumber": 27,
                "afterPatchRowNumber": 28,
                "PatchRowcode": "     section = \"api\""
            },
            "17": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 30,
                "PatchRowcode": "+    room_prejoin_state: StateFilter"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+    track_puppetted_users_ips: bool"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 32,
                "PatchRowcode": "+"
            },
            "21": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 33,
                "PatchRowcode": "     def read_config(self, config: JsonDict, **kwargs: Any) -> None:"
            },
            "22": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 34,
                "PatchRowcode": "         validate_config(_MAIN_SCHEMA, config, ())"
            },
            "23": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.room_prejoin_state = list(self._get_prejoin_state_types(config))"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+        self.room_prejoin_state = StateFilter.from_types("
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+            self._get_prejoin_state_entries(config)"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+        )"
            },
            "27": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "         self.track_puppeted_user_ips = config.get(\"track_puppeted_user_ips\", False)"
            },
            "28": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " "
            },
            "29": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    def _get_prejoin_state_types(self, config: JsonDict) -> Iterable[str]:"
            },
            "30": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\"Get the event types to include in the prejoin state"
            },
            "31": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "32": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        Parses the config and returns an iterable of the event types to be included."
            },
            "33": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        \"\"\""
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 40,
                "PatchRowcode": "+    def _get_prejoin_state_entries("
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 41,
                "PatchRowcode": "+        self, config: JsonDict"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 42,
                "PatchRowcode": "+    ) -> Iterable[Tuple[str, Optional[str]]]:"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 43,
                "PatchRowcode": "+        \"\"\"Get the event types and state keys to include in the prejoin state.\"\"\""
            },
            "38": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "         room_prejoin_state_config = config.get(\"room_prejoin_state\") or {}"
            },
            "39": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 45,
                "PatchRowcode": " "
            },
            "40": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "         # backwards-compatibility support for room_invite_state_types"
            },
            "41": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 55,
                "PatchRowcode": " "
            },
            "42": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 56,
                "PatchRowcode": "             logger.warning(_ROOM_INVITE_STATE_TYPES_WARNING)"
            },
            "43": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 57,
                "PatchRowcode": " "
            },
            "44": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            yield from config[\"room_invite_state_types\"]"
            },
            "45": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 58,
                "PatchRowcode": "+            for event_type in config[\"room_invite_state_types\"]:"
            },
            "46": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 59,
                "PatchRowcode": "+                yield event_type, None"
            },
            "47": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 60,
                "PatchRowcode": "             return"
            },
            "48": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 61,
                "PatchRowcode": " "
            },
            "49": {
                "beforePatchRowNumber": 56,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "         if not room_prejoin_state_config.get(\"disable_default_event_types\"):"
            },
            "50": {
                "beforePatchRowNumber": 57,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            yield from _DEFAULT_PREJOIN_STATE_TYPES"
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 63,
                "PatchRowcode": "+            yield from _DEFAULT_PREJOIN_STATE_TYPES_AND_STATE_KEYS"
            },
            "52": {
                "beforePatchRowNumber": 58,
                "afterPatchRowNumber": 64,
                "PatchRowcode": " "
            },
            "53": {
                "beforePatchRowNumber": 59,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        yield from room_prejoin_state_config.get(\"additional_event_types\", [])"
            },
            "54": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 65,
                "PatchRowcode": "+        for entry in room_prejoin_state_config.get(\"additional_event_types\", []):"
            },
            "55": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 66,
                "PatchRowcode": "+            if isinstance(entry, str):"
            },
            "56": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 67,
                "PatchRowcode": "+                yield entry, None"
            },
            "57": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 68,
                "PatchRowcode": "+            else:"
            },
            "58": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 69,
                "PatchRowcode": "+                yield entry"
            },
            "59": {
                "beforePatchRowNumber": 60,
                "afterPatchRowNumber": 70,
                "PatchRowcode": " "
            },
            "60": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 71,
                "PatchRowcode": " "
            },
            "61": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 72,
                "PatchRowcode": " _ROOM_INVITE_STATE_TYPES_WARNING = \"\"\"\\"
            },
            "62": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 73,
                "PatchRowcode": " WARNING: The 'room_invite_state_types' configuration setting is now deprecated,"
            },
            "63": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 74,
                "PatchRowcode": " and replaced with 'room_prejoin_state'. New features may not work correctly"
            },
            "64": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-unless 'room_invite_state_types' is removed. See the sample configuration file for"
            },
            "65": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-details of 'room_prejoin_state'."
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 75,
                "PatchRowcode": "+unless 'room_invite_state_types' is removed. See the config documentation at"
            },
            "67": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 76,
                "PatchRowcode": "+    https://matrix-org.github.io/synapse/latest/usage/configuration/config_documentation.html#room_prejoin_state"
            },
            "68": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 77,
                "PatchRowcode": "+for details of 'room_prejoin_state'."
            },
            "69": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 78,
                "PatchRowcode": " --------------------------------------------------------------------------------"
            },
            "70": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 79,
                "PatchRowcode": " \"\"\""
            },
            "71": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 80,
                "PatchRowcode": " "
            },
            "72": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-_DEFAULT_PREJOIN_STATE_TYPES = ["
            },
            "73": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    EventTypes.JoinRules,"
            },
            "74": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    EventTypes.CanonicalAlias,"
            },
            "75": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    EventTypes.RoomAvatar,"
            },
            "76": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    EventTypes.RoomEncryption,"
            },
            "77": {
                "beforePatchRowNumber": 75,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    EventTypes.Name,"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 81,
                "PatchRowcode": "+_DEFAULT_PREJOIN_STATE_TYPES_AND_STATE_KEYS = ["
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 82,
                "PatchRowcode": "+    (EventTypes.JoinRules, \"\"),"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 83,
                "PatchRowcode": "+    (EventTypes.CanonicalAlias, \"\"),"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 84,
                "PatchRowcode": "+    (EventTypes.RoomAvatar, \"\"),"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+    (EventTypes.RoomEncryption, \"\"),"
            },
            "83": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 86,
                "PatchRowcode": "+    (EventTypes.Name, \"\"),"
            },
            "84": {
                "beforePatchRowNumber": 76,
                "afterPatchRowNumber": 87,
                "PatchRowcode": "     # Per MSC1772."
            },
            "85": {
                "beforePatchRowNumber": 77,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    EventTypes.Create,"
            },
            "86": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 88,
                "PatchRowcode": "+    (EventTypes.Create, \"\"),"
            },
            "87": {
                "beforePatchRowNumber": 78,
                "afterPatchRowNumber": 89,
                "PatchRowcode": "     # Per MSC3173."
            },
            "88": {
                "beforePatchRowNumber": 79,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    EventTypes.Topic,"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 90,
                "PatchRowcode": "+    (EventTypes.Topic, \"\"),"
            },
            "90": {
                "beforePatchRowNumber": 80,
                "afterPatchRowNumber": 91,
                "PatchRowcode": " ]"
            },
            "91": {
                "beforePatchRowNumber": 81,
                "afterPatchRowNumber": 92,
                "PatchRowcode": " "
            },
            "92": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 93,
                "PatchRowcode": " "
            },
            "93": {
                "beforePatchRowNumber": 90,
                "afterPatchRowNumber": 101,
                "PatchRowcode": "                 \"disable_default_event_types\": {\"type\": \"boolean\"},"
            },
            "94": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 102,
                "PatchRowcode": "                 \"additional_event_types\": {"
            },
            "95": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 103,
                "PatchRowcode": "                     \"type\": \"array\","
            },
            "96": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    \"items\": {\"type\": \"string\"},"
            },
            "97": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 104,
                "PatchRowcode": "+                    \"items\": {"
            },
            "98": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 105,
                "PatchRowcode": "+                        \"oneOf\": ["
            },
            "99": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 106,
                "PatchRowcode": "+                            {\"type\": \"string\"},"
            },
            "100": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 107,
                "PatchRowcode": "+                            {"
            },
            "101": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 108,
                "PatchRowcode": "+                                \"type\": \"array\","
            },
            "102": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+                                \"items\": {\"type\": \"string\"},"
            },
            "103": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 110,
                "PatchRowcode": "+                                \"minItems\": 2,"
            },
            "104": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 111,
                "PatchRowcode": "+                                \"maxItems\": 2,"
            },
            "105": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 112,
                "PatchRowcode": "+                            },"
            },
            "106": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 113,
                "PatchRowcode": "+                        ],"
            },
            "107": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 114,
                "PatchRowcode": "+                    },"
            },
            "108": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": 115,
                "PatchRowcode": "                 },"
            },
            "109": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": 116,
                "PatchRowcode": "             },"
            },
            "110": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": 117,
                "PatchRowcode": "         },"
            }
        },
        "frontPatchFile": [
            "# Copyright 2015-2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import logging",
            "from typing import Any, Iterable",
            "",
            "from synapse.api.constants import EventTypes",
            "from synapse.config._base import Config, ConfigError",
            "from synapse.config._util import validate_config",
            "from synapse.types import JsonDict",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class ApiConfig(Config):",
            "    section = \"api\"",
            "",
            "    def read_config(self, config: JsonDict, **kwargs: Any) -> None:",
            "        validate_config(_MAIN_SCHEMA, config, ())",
            "        self.room_prejoin_state = list(self._get_prejoin_state_types(config))",
            "        self.track_puppeted_user_ips = config.get(\"track_puppeted_user_ips\", False)",
            "",
            "    def _get_prejoin_state_types(self, config: JsonDict) -> Iterable[str]:",
            "        \"\"\"Get the event types to include in the prejoin state",
            "",
            "        Parses the config and returns an iterable of the event types to be included.",
            "        \"\"\"",
            "        room_prejoin_state_config = config.get(\"room_prejoin_state\") or {}",
            "",
            "        # backwards-compatibility support for room_invite_state_types",
            "        if \"room_invite_state_types\" in config:",
            "            # if both \"room_invite_state_types\" and \"room_prejoin_state\" are set, then",
            "            # we don't really know what to do.",
            "            if room_prejoin_state_config:",
            "                raise ConfigError(",
            "                    \"Can't specify both 'room_invite_state_types' and 'room_prejoin_state' \"",
            "                    \"in config\"",
            "                )",
            "",
            "            logger.warning(_ROOM_INVITE_STATE_TYPES_WARNING)",
            "",
            "            yield from config[\"room_invite_state_types\"]",
            "            return",
            "",
            "        if not room_prejoin_state_config.get(\"disable_default_event_types\"):",
            "            yield from _DEFAULT_PREJOIN_STATE_TYPES",
            "",
            "        yield from room_prejoin_state_config.get(\"additional_event_types\", [])",
            "",
            "",
            "_ROOM_INVITE_STATE_TYPES_WARNING = \"\"\"\\",
            "WARNING: The 'room_invite_state_types' configuration setting is now deprecated,",
            "and replaced with 'room_prejoin_state'. New features may not work correctly",
            "unless 'room_invite_state_types' is removed. See the sample configuration file for",
            "details of 'room_prejoin_state'.",
            "--------------------------------------------------------------------------------",
            "\"\"\"",
            "",
            "_DEFAULT_PREJOIN_STATE_TYPES = [",
            "    EventTypes.JoinRules,",
            "    EventTypes.CanonicalAlias,",
            "    EventTypes.RoomAvatar,",
            "    EventTypes.RoomEncryption,",
            "    EventTypes.Name,",
            "    # Per MSC1772.",
            "    EventTypes.Create,",
            "    # Per MSC3173.",
            "    EventTypes.Topic,",
            "]",
            "",
            "",
            "# room_prejoin_state can either be None (as it is in the default config), or",
            "# an object containing other config settings",
            "_ROOM_PREJOIN_STATE_CONFIG_SCHEMA = {",
            "    \"oneOf\": [",
            "        {",
            "            \"type\": \"object\",",
            "            \"properties\": {",
            "                \"disable_default_event_types\": {\"type\": \"boolean\"},",
            "                \"additional_event_types\": {",
            "                    \"type\": \"array\",",
            "                    \"items\": {\"type\": \"string\"},",
            "                },",
            "            },",
            "        },",
            "        {\"type\": \"null\"},",
            "    ]",
            "}",
            "",
            "# the legacy room_invite_state_types setting",
            "_ROOM_INVITE_STATE_TYPES_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}",
            "",
            "_MAIN_SCHEMA = {",
            "    \"type\": \"object\",",
            "    \"properties\": {",
            "        \"room_prejoin_state\": _ROOM_PREJOIN_STATE_CONFIG_SCHEMA,",
            "        \"room_invite_state_types\": _ROOM_INVITE_STATE_TYPES_SCHEMA,",
            "        \"track_puppeted_user_ips\": {",
            "            \"type\": \"boolean\",",
            "        },",
            "    },",
            "}"
        ],
        "afterPatchFile": [
            "# Copyright 2015-2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import logging",
            "from typing import Any, Iterable, Optional, Tuple",
            "",
            "from synapse.api.constants import EventTypes",
            "from synapse.config._base import Config, ConfigError",
            "from synapse.config._util import validate_config",
            "from synapse.types import JsonDict",
            "from synapse.types.state import StateFilter",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class ApiConfig(Config):",
            "    section = \"api\"",
            "",
            "    room_prejoin_state: StateFilter",
            "    track_puppetted_users_ips: bool",
            "",
            "    def read_config(self, config: JsonDict, **kwargs: Any) -> None:",
            "        validate_config(_MAIN_SCHEMA, config, ())",
            "        self.room_prejoin_state = StateFilter.from_types(",
            "            self._get_prejoin_state_entries(config)",
            "        )",
            "        self.track_puppeted_user_ips = config.get(\"track_puppeted_user_ips\", False)",
            "",
            "    def _get_prejoin_state_entries(",
            "        self, config: JsonDict",
            "    ) -> Iterable[Tuple[str, Optional[str]]]:",
            "        \"\"\"Get the event types and state keys to include in the prejoin state.\"\"\"",
            "        room_prejoin_state_config = config.get(\"room_prejoin_state\") or {}",
            "",
            "        # backwards-compatibility support for room_invite_state_types",
            "        if \"room_invite_state_types\" in config:",
            "            # if both \"room_invite_state_types\" and \"room_prejoin_state\" are set, then",
            "            # we don't really know what to do.",
            "            if room_prejoin_state_config:",
            "                raise ConfigError(",
            "                    \"Can't specify both 'room_invite_state_types' and 'room_prejoin_state' \"",
            "                    \"in config\"",
            "                )",
            "",
            "            logger.warning(_ROOM_INVITE_STATE_TYPES_WARNING)",
            "",
            "            for event_type in config[\"room_invite_state_types\"]:",
            "                yield event_type, None",
            "            return",
            "",
            "        if not room_prejoin_state_config.get(\"disable_default_event_types\"):",
            "            yield from _DEFAULT_PREJOIN_STATE_TYPES_AND_STATE_KEYS",
            "",
            "        for entry in room_prejoin_state_config.get(\"additional_event_types\", []):",
            "            if isinstance(entry, str):",
            "                yield entry, None",
            "            else:",
            "                yield entry",
            "",
            "",
            "_ROOM_INVITE_STATE_TYPES_WARNING = \"\"\"\\",
            "WARNING: The 'room_invite_state_types' configuration setting is now deprecated,",
            "and replaced with 'room_prejoin_state'. New features may not work correctly",
            "unless 'room_invite_state_types' is removed. See the config documentation at",
            "    https://matrix-org.github.io/synapse/latest/usage/configuration/config_documentation.html#room_prejoin_state",
            "for details of 'room_prejoin_state'.",
            "--------------------------------------------------------------------------------",
            "\"\"\"",
            "",
            "_DEFAULT_PREJOIN_STATE_TYPES_AND_STATE_KEYS = [",
            "    (EventTypes.JoinRules, \"\"),",
            "    (EventTypes.CanonicalAlias, \"\"),",
            "    (EventTypes.RoomAvatar, \"\"),",
            "    (EventTypes.RoomEncryption, \"\"),",
            "    (EventTypes.Name, \"\"),",
            "    # Per MSC1772.",
            "    (EventTypes.Create, \"\"),",
            "    # Per MSC3173.",
            "    (EventTypes.Topic, \"\"),",
            "]",
            "",
            "",
            "# room_prejoin_state can either be None (as it is in the default config), or",
            "# an object containing other config settings",
            "_ROOM_PREJOIN_STATE_CONFIG_SCHEMA = {",
            "    \"oneOf\": [",
            "        {",
            "            \"type\": \"object\",",
            "            \"properties\": {",
            "                \"disable_default_event_types\": {\"type\": \"boolean\"},",
            "                \"additional_event_types\": {",
            "                    \"type\": \"array\",",
            "                    \"items\": {",
            "                        \"oneOf\": [",
            "                            {\"type\": \"string\"},",
            "                            {",
            "                                \"type\": \"array\",",
            "                                \"items\": {\"type\": \"string\"},",
            "                                \"minItems\": 2,",
            "                                \"maxItems\": 2,",
            "                            },",
            "                        ],",
            "                    },",
            "                },",
            "            },",
            "        },",
            "        {\"type\": \"null\"},",
            "    ]",
            "}",
            "",
            "# the legacy room_invite_state_types setting",
            "_ROOM_INVITE_STATE_TYPES_SCHEMA = {\"type\": \"array\", \"items\": {\"type\": \"string\"}}",
            "",
            "_MAIN_SCHEMA = {",
            "    \"type\": \"object\",",
            "    \"properties\": {",
            "        \"room_prejoin_state\": _ROOM_PREJOIN_STATE_CONFIG_SCHEMA,",
            "        \"room_invite_state_types\": _ROOM_INVITE_STATE_TYPES_SCHEMA,",
            "        \"track_puppeted_user_ips\": {",
            "            \"type\": \"boolean\",",
            "        },",
            "    },",
            "}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "16": [],
            "31": [
                "ApiConfig",
                "read_config"
            ],
            "34": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "35": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "36": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "37": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "38": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "53": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "57": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "59": [
                "ApiConfig",
                "_get_prejoin_state_types"
            ],
            "65": [],
            "66": [],
            "70": [
                "_DEFAULT_PREJOIN_STATE_TYPES"
            ],
            "71": [],
            "72": [],
            "73": [],
            "74": [],
            "75": [],
            "77": [],
            "79": [],
            "93": []
        },
        "addLocation": []
    },
    "synapse/events/utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 28,
                "afterPatchRowNumber": 28,
                "PatchRowcode": " )"
            },
            "1": {
                "beforePatchRowNumber": 29,
                "afterPatchRowNumber": 29,
                "PatchRowcode": " "
            },
            "2": {
                "beforePatchRowNumber": 30,
                "afterPatchRowNumber": 30,
                "PatchRowcode": " import attr"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 31,
                "PatchRowcode": "+from canonicaljson import encode_canonical_json"
            },
            "4": {
                "beforePatchRowNumber": 31,
                "afterPatchRowNumber": 32,
                "PatchRowcode": " "
            },
            "5": {
                "beforePatchRowNumber": 32,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-from synapse.api.constants import EventContentFields, EventTypes, RelationTypes"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 33,
                "PatchRowcode": "+from synapse.api.constants import ("
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 34,
                "PatchRowcode": "+    MAX_PDU_SIZE,"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 35,
                "PatchRowcode": "+    EventContentFields,"
            },
            "9": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 36,
                "PatchRowcode": "+    EventTypes,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 37,
                "PatchRowcode": "+    RelationTypes,"
            },
            "11": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 38,
                "PatchRowcode": "+)"
            },
            "12": {
                "beforePatchRowNumber": 33,
                "afterPatchRowNumber": 39,
                "PatchRowcode": " from synapse.api.errors import Codes, SynapseError"
            },
            "13": {
                "beforePatchRowNumber": 34,
                "afterPatchRowNumber": 40,
                "PatchRowcode": " from synapse.api.room_versions import RoomVersion"
            },
            "14": {
                "beforePatchRowNumber": 35,
                "afterPatchRowNumber": 41,
                "PatchRowcode": " from synapse.types import JsonDict"
            },
            "15": {
                "beforePatchRowNumber": 674,
                "afterPatchRowNumber": 680,
                "PatchRowcode": "     elif not isinstance(value, (bool, str)) and value is not None:"
            },
            "16": {
                "beforePatchRowNumber": 675,
                "afterPatchRowNumber": 681,
                "PatchRowcode": "         # Other potential JSON values (bool, None, str) are safe."
            },
            "17": {
                "beforePatchRowNumber": 676,
                "afterPatchRowNumber": 682,
                "PatchRowcode": "         raise SynapseError(400, \"Unknown JSON value\", Codes.BAD_JSON)"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 683,
                "PatchRowcode": "+"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 684,
                "PatchRowcode": "+"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 685,
                "PatchRowcode": "+def maybe_upsert_event_field("
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 686,
                "PatchRowcode": "+    event: EventBase, container: JsonDict, key: str, value: object"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 687,
                "PatchRowcode": "+) -> bool:"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 688,
                "PatchRowcode": "+    \"\"\"Upsert an event field, but only if this doesn't make the event too large."
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 689,
                "PatchRowcode": "+"
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 690,
                "PatchRowcode": "+    Returns true iff the upsert took place."
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 691,
                "PatchRowcode": "+    \"\"\""
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 692,
                "PatchRowcode": "+    if key in container:"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 693,
                "PatchRowcode": "+        old_value: object = container[key]"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 694,
                "PatchRowcode": "+        container[key] = value"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 695,
                "PatchRowcode": "+        # NB: here and below, we assume that passing a non-None `time_now` argument to"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 696,
                "PatchRowcode": "+        # get_pdu_json doesn't increase the size of the encoded result."
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 697,
                "PatchRowcode": "+        upsert_okay = len(encode_canonical_json(event.get_pdu_json())) <= MAX_PDU_SIZE"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 698,
                "PatchRowcode": "+        if not upsert_okay:"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 699,
                "PatchRowcode": "+            container[key] = old_value"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 700,
                "PatchRowcode": "+    else:"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 701,
                "PatchRowcode": "+        container[key] = value"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 702,
                "PatchRowcode": "+        upsert_okay = len(encode_canonical_json(event.get_pdu_json())) <= MAX_PDU_SIZE"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 703,
                "PatchRowcode": "+        if not upsert_okay:"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 704,
                "PatchRowcode": "+            del container[key]"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 705,
                "PatchRowcode": "+"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 706,
                "PatchRowcode": "+    return upsert_okay"
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import collections.abc",
            "import re",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    MutableMapping,",
            "    Optional,",
            "    Union,",
            ")",
            "",
            "import attr",
            "",
            "from synapse.api.constants import EventContentFields, EventTypes, RelationTypes",
            "from synapse.api.errors import Codes, SynapseError",
            "from synapse.api.room_versions import RoomVersion",
            "from synapse.types import JsonDict",
            "from synapse.util.frozenutils import unfreeze",
            "",
            "from . import EventBase",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.handlers.relations import BundledAggregations",
            "",
            "",
            "# Split strings on \".\" but not \"\\.\" This uses a negative lookbehind assertion for '\\'",
            "# (?<!stuff) matches if the current position in the string is not preceded",
            "# by a match for 'stuff'.",
            "# TODO: This is fast, but fails to handle \"foo\\\\.bar\" which should be treated as",
            "#       the literal fields \"foo\\\" and \"bar\" but will instead be treated as \"foo\\\\.bar\"",
            "SPLIT_FIELD_REGEX = re.compile(r\"(?<!\\\\)\\.\")",
            "",
            "CANONICALJSON_MAX_INT = (2**53) - 1",
            "CANONICALJSON_MIN_INT = -CANONICALJSON_MAX_INT",
            "",
            "",
            "def prune_event(event: EventBase) -> EventBase:",
            "    \"\"\"Returns a pruned version of the given event, which removes all keys we",
            "    don't know about or think could potentially be dodgy.",
            "",
            "    This is used when we \"redact\" an event. We want to remove all fields that",
            "    the user has specified, but we do want to keep necessary information like",
            "    type, state_key etc.",
            "    \"\"\"",
            "    pruned_event_dict = prune_event_dict(event.room_version, event.get_dict())",
            "",
            "    from . import make_event_from_dict",
            "",
            "    pruned_event = make_event_from_dict(",
            "        pruned_event_dict, event.room_version, event.internal_metadata.get_dict()",
            "    )",
            "",
            "    # copy the internal fields",
            "    pruned_event.internal_metadata.stream_ordering = (",
            "        event.internal_metadata.stream_ordering",
            "    )",
            "",
            "    pruned_event.internal_metadata.outlier = event.internal_metadata.outlier",
            "",
            "    # Mark the event as redacted",
            "    pruned_event.internal_metadata.redacted = True",
            "",
            "    return pruned_event",
            "",
            "",
            "def prune_event_dict(room_version: RoomVersion, event_dict: JsonDict) -> JsonDict:",
            "    \"\"\"Redacts the event_dict in the same way as `prune_event`, except it",
            "    operates on dicts rather than event objects",
            "",
            "    Returns:",
            "        A copy of the pruned event dict",
            "    \"\"\"",
            "",
            "    allowed_keys = [",
            "        \"event_id\",",
            "        \"sender\",",
            "        \"room_id\",",
            "        \"hashes\",",
            "        \"signatures\",",
            "        \"content\",",
            "        \"type\",",
            "        \"state_key\",",
            "        \"depth\",",
            "        \"prev_events\",",
            "        \"auth_events\",",
            "        \"origin\",",
            "        \"origin_server_ts\",",
            "    ]",
            "",
            "    # Room versions from before MSC2176 had additional allowed keys.",
            "    if not room_version.msc2176_redaction_rules:",
            "        allowed_keys.extend([\"prev_state\", \"membership\"])",
            "",
            "    event_type = event_dict[\"type\"]",
            "",
            "    new_content = {}",
            "",
            "    def add_fields(*fields: str) -> None:",
            "        for field in fields:",
            "            if field in event_dict[\"content\"]:",
            "                new_content[field] = event_dict[\"content\"][field]",
            "",
            "    if event_type == EventTypes.Member:",
            "        add_fields(\"membership\")",
            "        if room_version.msc3375_redaction_rules:",
            "            add_fields(EventContentFields.AUTHORISING_USER)",
            "    elif event_type == EventTypes.Create:",
            "        # MSC2176 rules state that create events cannot be redacted.",
            "        if room_version.msc2176_redaction_rules:",
            "            return event_dict",
            "",
            "        add_fields(\"creator\")",
            "    elif event_type == EventTypes.JoinRules:",
            "        add_fields(\"join_rule\")",
            "        if room_version.msc3083_join_rules:",
            "            add_fields(\"allow\")",
            "    elif event_type == EventTypes.PowerLevels:",
            "        add_fields(",
            "            \"users\",",
            "            \"users_default\",",
            "            \"events\",",
            "            \"events_default\",",
            "            \"state_default\",",
            "            \"ban\",",
            "            \"kick\",",
            "            \"redact\",",
            "        )",
            "",
            "        if room_version.msc2176_redaction_rules:",
            "            add_fields(\"invite\")",
            "",
            "        if room_version.msc2716_historical:",
            "            add_fields(\"historical\")",
            "",
            "    elif event_type == EventTypes.Aliases and room_version.special_case_aliases_auth:",
            "        add_fields(\"aliases\")",
            "    elif event_type == EventTypes.RoomHistoryVisibility:",
            "        add_fields(\"history_visibility\")",
            "    elif event_type == EventTypes.Redaction and room_version.msc2176_redaction_rules:",
            "        add_fields(\"redacts\")",
            "    elif room_version.msc2716_redactions and event_type == EventTypes.MSC2716_INSERTION:",
            "        add_fields(EventContentFields.MSC2716_NEXT_BATCH_ID)",
            "    elif room_version.msc2716_redactions and event_type == EventTypes.MSC2716_BATCH:",
            "        add_fields(EventContentFields.MSC2716_BATCH_ID)",
            "    elif room_version.msc2716_redactions and event_type == EventTypes.MSC2716_MARKER:",
            "        add_fields(EventContentFields.MSC2716_INSERTION_EVENT_REFERENCE)",
            "",
            "    allowed_fields = {k: v for k, v in event_dict.items() if k in allowed_keys}",
            "",
            "    allowed_fields[\"content\"] = new_content",
            "",
            "    unsigned: JsonDict = {}",
            "    allowed_fields[\"unsigned\"] = unsigned",
            "",
            "    event_unsigned = event_dict.get(\"unsigned\", {})",
            "",
            "    if \"age_ts\" in event_unsigned:",
            "        unsigned[\"age_ts\"] = event_unsigned[\"age_ts\"]",
            "    if \"replaces_state\" in event_unsigned:",
            "        unsigned[\"replaces_state\"] = event_unsigned[\"replaces_state\"]",
            "",
            "    return allowed_fields",
            "",
            "",
            "def _copy_field(src: JsonDict, dst: JsonDict, field: List[str]) -> None:",
            "    \"\"\"Copy the field in 'src' to 'dst'.",
            "",
            "    For example, if src={\"foo\":{\"bar\":5}} and dst={}, and field=[\"foo\",\"bar\"]",
            "    then dst={\"foo\":{\"bar\":5}}.",
            "",
            "    Args:",
            "        src: The dict to read from.",
            "        dst: The dict to modify.",
            "        field: List of keys to drill down to in 'src'.",
            "    \"\"\"",
            "    if len(field) == 0:  # this should be impossible",
            "        return",
            "    if len(field) == 1:  # common case e.g. 'origin_server_ts'",
            "        if field[0] in src:",
            "            dst[field[0]] = src[field[0]]",
            "        return",
            "",
            "    # Else is a nested field e.g. 'content.body'",
            "    # Pop the last field as that's the key to move across and we need the",
            "    # parent dict in order to access the data. Drill down to the right dict.",
            "    key_to_move = field.pop(-1)",
            "    sub_dict = src",
            "    for sub_field in field:  # e.g. sub_field => \"content\"",
            "        if sub_field in sub_dict and isinstance(",
            "            sub_dict[sub_field], collections.abc.Mapping",
            "        ):",
            "            sub_dict = sub_dict[sub_field]",
            "        else:",
            "            return",
            "",
            "    if key_to_move not in sub_dict:",
            "        return",
            "",
            "    # Insert the key into the output dictionary, creating nested objects",
            "    # as required. We couldn't do this any earlier or else we'd need to delete",
            "    # the empty objects if the key didn't exist.",
            "    sub_out_dict = dst",
            "    for sub_field in field:",
            "        sub_out_dict = sub_out_dict.setdefault(sub_field, {})",
            "    sub_out_dict[key_to_move] = sub_dict[key_to_move]",
            "",
            "",
            "def only_fields(dictionary: JsonDict, fields: List[str]) -> JsonDict:",
            "    \"\"\"Return a new dict with only the fields in 'dictionary' which are present",
            "    in 'fields'.",
            "",
            "    If there are no event fields specified then all fields are included.",
            "    The entries may include '.' characters to indicate sub-fields.",
            "    So ['content.body'] will include the 'body' field of the 'content' object.",
            "    A literal '.' character in a field name may be escaped using a '\\'.",
            "",
            "    Args:",
            "        dictionary: The dictionary to read from.",
            "        fields: A list of fields to copy over. Only shallow refs are",
            "        taken.",
            "    Returns:",
            "        A new dictionary with only the given fields. If fields was empty,",
            "        the same dictionary is returned.",
            "    \"\"\"",
            "    if len(fields) == 0:",
            "        return dictionary",
            "",
            "    # for each field, convert it:",
            "    # [\"content.body.thing\\.with\\.dots\"] => [[\"content\", \"body\", \"thing\\.with\\.dots\"]]",
            "    split_fields = [SPLIT_FIELD_REGEX.split(f) for f in fields]",
            "",
            "    # for each element of the output array of arrays:",
            "    # remove escaping so we can use the right key names.",
            "    split_fields[:] = [",
            "        [f.replace(r\"\\.\", r\".\") for f in field_array] for field_array in split_fields",
            "    ]",
            "",
            "    output: JsonDict = {}",
            "    for field_array in split_fields:",
            "        _copy_field(dictionary, output, field_array)",
            "    return output",
            "",
            "",
            "def format_event_raw(d: JsonDict) -> JsonDict:",
            "    return d",
            "",
            "",
            "def format_event_for_client_v1(d: JsonDict) -> JsonDict:",
            "    d = format_event_for_client_v2(d)",
            "",
            "    sender = d.get(\"sender\")",
            "    if sender is not None:",
            "        d[\"user_id\"] = sender",
            "",
            "    copy_keys = (",
            "        \"age\",",
            "        \"redacted_because\",",
            "        \"replaces_state\",",
            "        \"prev_content\",",
            "        \"invite_room_state\",",
            "        \"knock_room_state\",",
            "    )",
            "    for key in copy_keys:",
            "        if key in d[\"unsigned\"]:",
            "            d[key] = d[\"unsigned\"][key]",
            "",
            "    return d",
            "",
            "",
            "def format_event_for_client_v2(d: JsonDict) -> JsonDict:",
            "    drop_keys = (",
            "        \"auth_events\",",
            "        \"prev_events\",",
            "        \"hashes\",",
            "        \"signatures\",",
            "        \"depth\",",
            "        \"origin\",",
            "        \"prev_state\",",
            "    )",
            "    for key in drop_keys:",
            "        d.pop(key, None)",
            "    return d",
            "",
            "",
            "def format_event_for_client_v2_without_room_id(d: JsonDict) -> JsonDict:",
            "    d = format_event_for_client_v2(d)",
            "    d.pop(\"room_id\", None)",
            "    return d",
            "",
            "",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class SerializeEventConfig:",
            "    as_client_event: bool = True",
            "    # Function to convert from federation format to client format",
            "    event_format: Callable[[JsonDict], JsonDict] = format_event_for_client_v1",
            "    # ID of the user's auth token - used for namespacing of transaction IDs",
            "    token_id: Optional[int] = None",
            "    # List of event fields to include. If empty, all fields will be returned.",
            "    only_event_fields: Optional[List[str]] = None",
            "    # Some events can have stripped room state stored in the `unsigned` field.",
            "    # This is required for invite and knock functionality. If this option is",
            "    # False, that state will be removed from the event before it is returned.",
            "    # Otherwise, it will be kept.",
            "    include_stripped_room_state: bool = False",
            "",
            "",
            "_DEFAULT_SERIALIZE_EVENT_CONFIG = SerializeEventConfig()",
            "",
            "",
            "def serialize_event(",
            "    e: Union[JsonDict, EventBase],",
            "    time_now_ms: int,",
            "    *,",
            "    config: SerializeEventConfig = _DEFAULT_SERIALIZE_EVENT_CONFIG,",
            ") -> JsonDict:",
            "    \"\"\"Serialize event for clients",
            "",
            "    Args:",
            "        e",
            "        time_now_ms",
            "        config: Event serialization config",
            "",
            "    Returns:",
            "        The serialized event dictionary.",
            "    \"\"\"",
            "",
            "    # FIXME(erikj): To handle the case of presence events and the like",
            "    if not isinstance(e, EventBase):",
            "        return e",
            "",
            "    time_now_ms = int(time_now_ms)",
            "",
            "    # Should this strip out None's?",
            "    d = {k: v for k, v in e.get_dict().items()}",
            "",
            "    d[\"event_id\"] = e.event_id",
            "",
            "    if \"age_ts\" in d[\"unsigned\"]:",
            "        d[\"unsigned\"][\"age\"] = time_now_ms - d[\"unsigned\"][\"age_ts\"]",
            "        del d[\"unsigned\"][\"age_ts\"]",
            "",
            "    if \"redacted_because\" in e.unsigned:",
            "        d[\"unsigned\"][\"redacted_because\"] = serialize_event(",
            "            e.unsigned[\"redacted_because\"], time_now_ms, config=config",
            "        )",
            "",
            "    if config.token_id is not None:",
            "        if config.token_id == getattr(e.internal_metadata, \"token_id\", None):",
            "            txn_id = getattr(e.internal_metadata, \"txn_id\", None)",
            "            if txn_id is not None:",
            "                d[\"unsigned\"][\"transaction_id\"] = txn_id",
            "",
            "    # invite_room_state and knock_room_state are a list of stripped room state events",
            "    # that are meant to provide metadata about a room to an invitee/knocker. They are",
            "    # intended to only be included in specific circumstances, such as down sync, and",
            "    # should not be included in any other case.",
            "    if not config.include_stripped_room_state:",
            "        d[\"unsigned\"].pop(\"invite_room_state\", None)",
            "        d[\"unsigned\"].pop(\"knock_room_state\", None)",
            "",
            "    if config.as_client_event:",
            "        d = config.event_format(d)",
            "",
            "    only_event_fields = config.only_event_fields",
            "    if only_event_fields:",
            "        if not isinstance(only_event_fields, list) or not all(",
            "            isinstance(f, str) for f in only_event_fields",
            "        ):",
            "            raise TypeError(\"only_event_fields must be a list of strings\")",
            "        d = only_fields(d, only_event_fields)",
            "",
            "    return d",
            "",
            "",
            "class EventClientSerializer:",
            "    \"\"\"Serializes events that are to be sent to clients.",
            "",
            "    This is used for bundling extra information with any events to be sent to",
            "    clients.",
            "    \"\"\"",
            "",
            "    def serialize_event(",
            "        self,",
            "        event: Union[JsonDict, EventBase],",
            "        time_now: int,",
            "        *,",
            "        config: SerializeEventConfig = _DEFAULT_SERIALIZE_EVENT_CONFIG,",
            "        bundle_aggregations: Optional[Dict[str, \"BundledAggregations\"]] = None,",
            "        apply_edits: bool = True,",
            "    ) -> JsonDict:",
            "        \"\"\"Serializes a single event.",
            "",
            "        Args:",
            "            event: The event being serialized.",
            "            time_now: The current time in milliseconds",
            "            config: Event serialization config",
            "            bundle_aggregations: A map from event_id to the aggregations to be bundled",
            "               into the event.",
            "            apply_edits: Whether the content of the event should be modified to reflect",
            "               any replacement in `bundle_aggregations[<event_id>].replace`.",
            "        Returns:",
            "            The serialized event",
            "        \"\"\"",
            "        # To handle the case of presence events and the like",
            "        if not isinstance(event, EventBase):",
            "            return event",
            "",
            "        serialized_event = serialize_event(event, time_now, config=config)",
            "",
            "        # Check if there are any bundled aggregations to include with the event.",
            "        if bundle_aggregations:",
            "            if event.event_id in bundle_aggregations:",
            "                self._inject_bundled_aggregations(",
            "                    event,",
            "                    time_now,",
            "                    config,",
            "                    bundle_aggregations,",
            "                    serialized_event,",
            "                    apply_edits=apply_edits,",
            "                )",
            "",
            "        return serialized_event",
            "",
            "    def _apply_edit(",
            "        self, orig_event: EventBase, serialized_event: JsonDict, edit: EventBase",
            "    ) -> None:",
            "        \"\"\"Replace the content, preserving existing relations of the serialized event.",
            "",
            "        Args:",
            "            orig_event: The original event.",
            "            serialized_event: The original event, serialized. This is modified.",
            "            edit: The event which edits the above.",
            "        \"\"\"",
            "",
            "        # Ensure we take copies of the edit content, otherwise we risk modifying",
            "        # the original event.",
            "        edit_content = edit.content.copy()",
            "",
            "        # Unfreeze the event content if necessary, so that we may modify it below",
            "        edit_content = unfreeze(edit_content)",
            "        serialized_event[\"content\"] = edit_content.get(\"m.new_content\", {})",
            "",
            "        # Check for existing relations",
            "        relates_to = orig_event.content.get(\"m.relates_to\")",
            "        if relates_to:",
            "            # Keep the relations, ensuring we use a dict copy of the original",
            "            serialized_event[\"content\"][\"m.relates_to\"] = relates_to.copy()",
            "        else:",
            "            serialized_event[\"content\"].pop(\"m.relates_to\", None)",
            "",
            "    def _inject_bundled_aggregations(",
            "        self,",
            "        event: EventBase,",
            "        time_now: int,",
            "        config: SerializeEventConfig,",
            "        bundled_aggregations: Dict[str, \"BundledAggregations\"],",
            "        serialized_event: JsonDict,",
            "        apply_edits: bool,",
            "    ) -> None:",
            "        \"\"\"Potentially injects bundled aggregations into the unsigned portion of the serialized event.",
            "",
            "        Args:",
            "            event: The event being serialized.",
            "            time_now: The current time in milliseconds",
            "            config: Event serialization config",
            "            bundled_aggregations: Bundled aggregations to be injected.",
            "                A map from event_id to aggregation data. Must contain at least an",
            "                entry for `event`.",
            "",
            "                While serializing the bundled aggregations this map may be searched",
            "                again for additional events in a recursive manner.",
            "            serialized_event: The serialized event which may be modified.",
            "            apply_edits: Whether the content of the event should be modified to reflect",
            "               any replacement in `aggregations.replace`.",
            "        \"\"\"",
            "",
            "        # We have already checked that aggregations exist for this event.",
            "        event_aggregations = bundled_aggregations[event.event_id]",
            "",
            "        # The JSON dictionary to be added under the unsigned property of the event",
            "        # being serialized.",
            "        serialized_aggregations = {}",
            "",
            "        if event_aggregations.annotations:",
            "            serialized_aggregations[",
            "                RelationTypes.ANNOTATION",
            "            ] = event_aggregations.annotations",
            "",
            "        if event_aggregations.references:",
            "            serialized_aggregations[",
            "                RelationTypes.REFERENCE",
            "            ] = event_aggregations.references",
            "",
            "        if event_aggregations.replace:",
            "            # If there is an edit, optionally apply it to the event.",
            "            edit = event_aggregations.replace",
            "            if apply_edits:",
            "                self._apply_edit(event, serialized_event, edit)",
            "",
            "            # Include information about it in the relations dict.",
            "            serialized_aggregations[RelationTypes.REPLACE] = {",
            "                \"event_id\": edit.event_id,",
            "                \"origin_server_ts\": edit.origin_server_ts,",
            "                \"sender\": edit.sender,",
            "            }",
            "",
            "        # Include any threaded replies to this event.",
            "        if event_aggregations.thread:",
            "            thread = event_aggregations.thread",
            "",
            "            serialized_latest_event = self.serialize_event(",
            "                thread.latest_event,",
            "                time_now,",
            "                config=config,",
            "                bundle_aggregations=bundled_aggregations,",
            "            )",
            "",
            "            thread_summary = {",
            "                \"latest_event\": serialized_latest_event,",
            "                \"count\": thread.count,",
            "                \"current_user_participated\": thread.current_user_participated,",
            "            }",
            "            serialized_aggregations[RelationTypes.THREAD] = thread_summary",
            "",
            "        # Include the bundled aggregations in the event.",
            "        if serialized_aggregations:",
            "            # There is likely already an \"unsigned\" field, but a filter might",
            "            # have stripped it off (via the event_fields option). The server is",
            "            # allowed to return additional fields, so add it back.",
            "            serialized_event.setdefault(\"unsigned\", {}).setdefault(",
            "                \"m.relations\", {}",
            "            ).update(serialized_aggregations)",
            "",
            "    def serialize_events(",
            "        self,",
            "        events: Iterable[Union[JsonDict, EventBase]],",
            "        time_now: int,",
            "        *,",
            "        config: SerializeEventConfig = _DEFAULT_SERIALIZE_EVENT_CONFIG,",
            "        bundle_aggregations: Optional[Dict[str, \"BundledAggregations\"]] = None,",
            "    ) -> List[JsonDict]:",
            "        \"\"\"Serializes multiple events.",
            "",
            "        Args:",
            "            event",
            "            time_now: The current time in milliseconds",
            "            config: Event serialization config",
            "            bundle_aggregations: Whether to include the bundled aggregations for this",
            "                event. Only applies to non-state events. (State events never include",
            "                bundled aggregations.)",
            "",
            "        Returns:",
            "            The list of serialized events",
            "        \"\"\"",
            "        return [",
            "            self.serialize_event(",
            "                event,",
            "                time_now,",
            "                config=config,",
            "                bundle_aggregations=bundle_aggregations,",
            "            )",
            "            for event in events",
            "        ]",
            "",
            "",
            "_PowerLevel = Union[str, int]",
            "",
            "",
            "def copy_and_fixup_power_levels_contents(",
            "    old_power_levels: Mapping[str, Union[_PowerLevel, Mapping[str, _PowerLevel]]]",
            ") -> Dict[str, Union[int, Dict[str, int]]]:",
            "    \"\"\"Copy the content of a power_levels event, unfreezing frozendicts along the way.",
            "",
            "    We accept as input power level values which are strings, provided they represent an",
            "    integer, e.g. `\"`100\"` instead of 100. Such strings are converted to integers",
            "    in the returned dictionary (hence \"fixup\" in the function name).",
            "",
            "    Note that future room versions will outlaw such stringy power levels (see",
            "    https://github.com/matrix-org/matrix-spec/issues/853).",
            "",
            "    Raises:",
            "        TypeError if the input does not look like a valid power levels event content",
            "    \"\"\"",
            "    if not isinstance(old_power_levels, collections.abc.Mapping):",
            "        raise TypeError(\"Not a valid power-levels content: %r\" % (old_power_levels,))",
            "",
            "    power_levels: Dict[str, Union[int, Dict[str, int]]] = {}",
            "",
            "    for k, v in old_power_levels.items():",
            "        if isinstance(v, collections.abc.Mapping):",
            "            h: Dict[str, int] = {}",
            "            power_levels[k] = h",
            "            for k1, v1 in v.items():",
            "                _copy_power_level_value_as_integer(v1, h, k1)",
            "",
            "        else:",
            "            _copy_power_level_value_as_integer(v, power_levels, k)",
            "",
            "    return power_levels",
            "",
            "",
            "def _copy_power_level_value_as_integer(",
            "    old_value: object,",
            "    power_levels: MutableMapping[str, Any],",
            "    key: str,",
            ") -> None:",
            "    \"\"\"Set `power_levels[key]` to the integer represented by `old_value`.",
            "",
            "    :raises TypeError: if `old_value` is not an integer, nor a base-10 string",
            "        representation of an integer.",
            "    \"\"\"",
            "    if isinstance(old_value, int):",
            "        power_levels[key] = old_value",
            "        return",
            "",
            "    if isinstance(old_value, str):",
            "        try:",
            "            parsed_value = int(old_value, base=10)",
            "        except ValueError:",
            "            # Fall through to the final TypeError.",
            "            pass",
            "        else:",
            "            power_levels[key] = parsed_value",
            "            return",
            "",
            "    raise TypeError(f\"Invalid power_levels value for {key}: {old_value}\")",
            "",
            "",
            "def validate_canonicaljson(value: Any) -> None:",
            "    \"\"\"",
            "    Ensure that the JSON object is valid according to the rules of canonical JSON.",
            "",
            "    See the appendix section 3.1: Canonical JSON.",
            "",
            "    This rejects JSON that has:",
            "    * An integer outside the range of [-2 ^ 53 + 1, 2 ^ 53 - 1]",
            "    * Floats",
            "    * NaN, Infinity, -Infinity",
            "    \"\"\"",
            "    if isinstance(value, int):",
            "        if value < CANONICALJSON_MIN_INT or CANONICALJSON_MAX_INT < value:",
            "            raise SynapseError(400, \"JSON integer out of range\", Codes.BAD_JSON)",
            "",
            "    elif isinstance(value, float):",
            "        # Note that Infinity, -Infinity, and NaN are also considered floats.",
            "        raise SynapseError(400, \"Bad JSON value: float\", Codes.BAD_JSON)",
            "",
            "    elif isinstance(value, collections.abc.Mapping):",
            "        for v in value.values():",
            "            validate_canonicaljson(v)",
            "",
            "    elif isinstance(value, (list, tuple)):",
            "        for i in value:",
            "            validate_canonicaljson(i)",
            "",
            "    elif not isinstance(value, (bool, str)) and value is not None:",
            "        # Other potential JSON values (bool, None, str) are safe.",
            "        raise SynapseError(400, \"Unknown JSON value\", Codes.BAD_JSON)"
        ],
        "afterPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2021 The Matrix.org Foundation C.I.C.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import collections.abc",
            "import re",
            "from typing import (",
            "    TYPE_CHECKING,",
            "    Any,",
            "    Callable,",
            "    Dict,",
            "    Iterable,",
            "    List,",
            "    Mapping,",
            "    MutableMapping,",
            "    Optional,",
            "    Union,",
            ")",
            "",
            "import attr",
            "from canonicaljson import encode_canonical_json",
            "",
            "from synapse.api.constants import (",
            "    MAX_PDU_SIZE,",
            "    EventContentFields,",
            "    EventTypes,",
            "    RelationTypes,",
            ")",
            "from synapse.api.errors import Codes, SynapseError",
            "from synapse.api.room_versions import RoomVersion",
            "from synapse.types import JsonDict",
            "from synapse.util.frozenutils import unfreeze",
            "",
            "from . import EventBase",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.handlers.relations import BundledAggregations",
            "",
            "",
            "# Split strings on \".\" but not \"\\.\" This uses a negative lookbehind assertion for '\\'",
            "# (?<!stuff) matches if the current position in the string is not preceded",
            "# by a match for 'stuff'.",
            "# TODO: This is fast, but fails to handle \"foo\\\\.bar\" which should be treated as",
            "#       the literal fields \"foo\\\" and \"bar\" but will instead be treated as \"foo\\\\.bar\"",
            "SPLIT_FIELD_REGEX = re.compile(r\"(?<!\\\\)\\.\")",
            "",
            "CANONICALJSON_MAX_INT = (2**53) - 1",
            "CANONICALJSON_MIN_INT = -CANONICALJSON_MAX_INT",
            "",
            "",
            "def prune_event(event: EventBase) -> EventBase:",
            "    \"\"\"Returns a pruned version of the given event, which removes all keys we",
            "    don't know about or think could potentially be dodgy.",
            "",
            "    This is used when we \"redact\" an event. We want to remove all fields that",
            "    the user has specified, but we do want to keep necessary information like",
            "    type, state_key etc.",
            "    \"\"\"",
            "    pruned_event_dict = prune_event_dict(event.room_version, event.get_dict())",
            "",
            "    from . import make_event_from_dict",
            "",
            "    pruned_event = make_event_from_dict(",
            "        pruned_event_dict, event.room_version, event.internal_metadata.get_dict()",
            "    )",
            "",
            "    # copy the internal fields",
            "    pruned_event.internal_metadata.stream_ordering = (",
            "        event.internal_metadata.stream_ordering",
            "    )",
            "",
            "    pruned_event.internal_metadata.outlier = event.internal_metadata.outlier",
            "",
            "    # Mark the event as redacted",
            "    pruned_event.internal_metadata.redacted = True",
            "",
            "    return pruned_event",
            "",
            "",
            "def prune_event_dict(room_version: RoomVersion, event_dict: JsonDict) -> JsonDict:",
            "    \"\"\"Redacts the event_dict in the same way as `prune_event`, except it",
            "    operates on dicts rather than event objects",
            "",
            "    Returns:",
            "        A copy of the pruned event dict",
            "    \"\"\"",
            "",
            "    allowed_keys = [",
            "        \"event_id\",",
            "        \"sender\",",
            "        \"room_id\",",
            "        \"hashes\",",
            "        \"signatures\",",
            "        \"content\",",
            "        \"type\",",
            "        \"state_key\",",
            "        \"depth\",",
            "        \"prev_events\",",
            "        \"auth_events\",",
            "        \"origin\",",
            "        \"origin_server_ts\",",
            "    ]",
            "",
            "    # Room versions from before MSC2176 had additional allowed keys.",
            "    if not room_version.msc2176_redaction_rules:",
            "        allowed_keys.extend([\"prev_state\", \"membership\"])",
            "",
            "    event_type = event_dict[\"type\"]",
            "",
            "    new_content = {}",
            "",
            "    def add_fields(*fields: str) -> None:",
            "        for field in fields:",
            "            if field in event_dict[\"content\"]:",
            "                new_content[field] = event_dict[\"content\"][field]",
            "",
            "    if event_type == EventTypes.Member:",
            "        add_fields(\"membership\")",
            "        if room_version.msc3375_redaction_rules:",
            "            add_fields(EventContentFields.AUTHORISING_USER)",
            "    elif event_type == EventTypes.Create:",
            "        # MSC2176 rules state that create events cannot be redacted.",
            "        if room_version.msc2176_redaction_rules:",
            "            return event_dict",
            "",
            "        add_fields(\"creator\")",
            "    elif event_type == EventTypes.JoinRules:",
            "        add_fields(\"join_rule\")",
            "        if room_version.msc3083_join_rules:",
            "            add_fields(\"allow\")",
            "    elif event_type == EventTypes.PowerLevels:",
            "        add_fields(",
            "            \"users\",",
            "            \"users_default\",",
            "            \"events\",",
            "            \"events_default\",",
            "            \"state_default\",",
            "            \"ban\",",
            "            \"kick\",",
            "            \"redact\",",
            "        )",
            "",
            "        if room_version.msc2176_redaction_rules:",
            "            add_fields(\"invite\")",
            "",
            "        if room_version.msc2716_historical:",
            "            add_fields(\"historical\")",
            "",
            "    elif event_type == EventTypes.Aliases and room_version.special_case_aliases_auth:",
            "        add_fields(\"aliases\")",
            "    elif event_type == EventTypes.RoomHistoryVisibility:",
            "        add_fields(\"history_visibility\")",
            "    elif event_type == EventTypes.Redaction and room_version.msc2176_redaction_rules:",
            "        add_fields(\"redacts\")",
            "    elif room_version.msc2716_redactions and event_type == EventTypes.MSC2716_INSERTION:",
            "        add_fields(EventContentFields.MSC2716_NEXT_BATCH_ID)",
            "    elif room_version.msc2716_redactions and event_type == EventTypes.MSC2716_BATCH:",
            "        add_fields(EventContentFields.MSC2716_BATCH_ID)",
            "    elif room_version.msc2716_redactions and event_type == EventTypes.MSC2716_MARKER:",
            "        add_fields(EventContentFields.MSC2716_INSERTION_EVENT_REFERENCE)",
            "",
            "    allowed_fields = {k: v for k, v in event_dict.items() if k in allowed_keys}",
            "",
            "    allowed_fields[\"content\"] = new_content",
            "",
            "    unsigned: JsonDict = {}",
            "    allowed_fields[\"unsigned\"] = unsigned",
            "",
            "    event_unsigned = event_dict.get(\"unsigned\", {})",
            "",
            "    if \"age_ts\" in event_unsigned:",
            "        unsigned[\"age_ts\"] = event_unsigned[\"age_ts\"]",
            "    if \"replaces_state\" in event_unsigned:",
            "        unsigned[\"replaces_state\"] = event_unsigned[\"replaces_state\"]",
            "",
            "    return allowed_fields",
            "",
            "",
            "def _copy_field(src: JsonDict, dst: JsonDict, field: List[str]) -> None:",
            "    \"\"\"Copy the field in 'src' to 'dst'.",
            "",
            "    For example, if src={\"foo\":{\"bar\":5}} and dst={}, and field=[\"foo\",\"bar\"]",
            "    then dst={\"foo\":{\"bar\":5}}.",
            "",
            "    Args:",
            "        src: The dict to read from.",
            "        dst: The dict to modify.",
            "        field: List of keys to drill down to in 'src'.",
            "    \"\"\"",
            "    if len(field) == 0:  # this should be impossible",
            "        return",
            "    if len(field) == 1:  # common case e.g. 'origin_server_ts'",
            "        if field[0] in src:",
            "            dst[field[0]] = src[field[0]]",
            "        return",
            "",
            "    # Else is a nested field e.g. 'content.body'",
            "    # Pop the last field as that's the key to move across and we need the",
            "    # parent dict in order to access the data. Drill down to the right dict.",
            "    key_to_move = field.pop(-1)",
            "    sub_dict = src",
            "    for sub_field in field:  # e.g. sub_field => \"content\"",
            "        if sub_field in sub_dict and isinstance(",
            "            sub_dict[sub_field], collections.abc.Mapping",
            "        ):",
            "            sub_dict = sub_dict[sub_field]",
            "        else:",
            "            return",
            "",
            "    if key_to_move not in sub_dict:",
            "        return",
            "",
            "    # Insert the key into the output dictionary, creating nested objects",
            "    # as required. We couldn't do this any earlier or else we'd need to delete",
            "    # the empty objects if the key didn't exist.",
            "    sub_out_dict = dst",
            "    for sub_field in field:",
            "        sub_out_dict = sub_out_dict.setdefault(sub_field, {})",
            "    sub_out_dict[key_to_move] = sub_dict[key_to_move]",
            "",
            "",
            "def only_fields(dictionary: JsonDict, fields: List[str]) -> JsonDict:",
            "    \"\"\"Return a new dict with only the fields in 'dictionary' which are present",
            "    in 'fields'.",
            "",
            "    If there are no event fields specified then all fields are included.",
            "    The entries may include '.' characters to indicate sub-fields.",
            "    So ['content.body'] will include the 'body' field of the 'content' object.",
            "    A literal '.' character in a field name may be escaped using a '\\'.",
            "",
            "    Args:",
            "        dictionary: The dictionary to read from.",
            "        fields: A list of fields to copy over. Only shallow refs are",
            "        taken.",
            "    Returns:",
            "        A new dictionary with only the given fields. If fields was empty,",
            "        the same dictionary is returned.",
            "    \"\"\"",
            "    if len(fields) == 0:",
            "        return dictionary",
            "",
            "    # for each field, convert it:",
            "    # [\"content.body.thing\\.with\\.dots\"] => [[\"content\", \"body\", \"thing\\.with\\.dots\"]]",
            "    split_fields = [SPLIT_FIELD_REGEX.split(f) for f in fields]",
            "",
            "    # for each element of the output array of arrays:",
            "    # remove escaping so we can use the right key names.",
            "    split_fields[:] = [",
            "        [f.replace(r\"\\.\", r\".\") for f in field_array] for field_array in split_fields",
            "    ]",
            "",
            "    output: JsonDict = {}",
            "    for field_array in split_fields:",
            "        _copy_field(dictionary, output, field_array)",
            "    return output",
            "",
            "",
            "def format_event_raw(d: JsonDict) -> JsonDict:",
            "    return d",
            "",
            "",
            "def format_event_for_client_v1(d: JsonDict) -> JsonDict:",
            "    d = format_event_for_client_v2(d)",
            "",
            "    sender = d.get(\"sender\")",
            "    if sender is not None:",
            "        d[\"user_id\"] = sender",
            "",
            "    copy_keys = (",
            "        \"age\",",
            "        \"redacted_because\",",
            "        \"replaces_state\",",
            "        \"prev_content\",",
            "        \"invite_room_state\",",
            "        \"knock_room_state\",",
            "    )",
            "    for key in copy_keys:",
            "        if key in d[\"unsigned\"]:",
            "            d[key] = d[\"unsigned\"][key]",
            "",
            "    return d",
            "",
            "",
            "def format_event_for_client_v2(d: JsonDict) -> JsonDict:",
            "    drop_keys = (",
            "        \"auth_events\",",
            "        \"prev_events\",",
            "        \"hashes\",",
            "        \"signatures\",",
            "        \"depth\",",
            "        \"origin\",",
            "        \"prev_state\",",
            "    )",
            "    for key in drop_keys:",
            "        d.pop(key, None)",
            "    return d",
            "",
            "",
            "def format_event_for_client_v2_without_room_id(d: JsonDict) -> JsonDict:",
            "    d = format_event_for_client_v2(d)",
            "    d.pop(\"room_id\", None)",
            "    return d",
            "",
            "",
            "@attr.s(slots=True, frozen=True, auto_attribs=True)",
            "class SerializeEventConfig:",
            "    as_client_event: bool = True",
            "    # Function to convert from federation format to client format",
            "    event_format: Callable[[JsonDict], JsonDict] = format_event_for_client_v1",
            "    # ID of the user's auth token - used for namespacing of transaction IDs",
            "    token_id: Optional[int] = None",
            "    # List of event fields to include. If empty, all fields will be returned.",
            "    only_event_fields: Optional[List[str]] = None",
            "    # Some events can have stripped room state stored in the `unsigned` field.",
            "    # This is required for invite and knock functionality. If this option is",
            "    # False, that state will be removed from the event before it is returned.",
            "    # Otherwise, it will be kept.",
            "    include_stripped_room_state: bool = False",
            "",
            "",
            "_DEFAULT_SERIALIZE_EVENT_CONFIG = SerializeEventConfig()",
            "",
            "",
            "def serialize_event(",
            "    e: Union[JsonDict, EventBase],",
            "    time_now_ms: int,",
            "    *,",
            "    config: SerializeEventConfig = _DEFAULT_SERIALIZE_EVENT_CONFIG,",
            ") -> JsonDict:",
            "    \"\"\"Serialize event for clients",
            "",
            "    Args:",
            "        e",
            "        time_now_ms",
            "        config: Event serialization config",
            "",
            "    Returns:",
            "        The serialized event dictionary.",
            "    \"\"\"",
            "",
            "    # FIXME(erikj): To handle the case of presence events and the like",
            "    if not isinstance(e, EventBase):",
            "        return e",
            "",
            "    time_now_ms = int(time_now_ms)",
            "",
            "    # Should this strip out None's?",
            "    d = {k: v for k, v in e.get_dict().items()}",
            "",
            "    d[\"event_id\"] = e.event_id",
            "",
            "    if \"age_ts\" in d[\"unsigned\"]:",
            "        d[\"unsigned\"][\"age\"] = time_now_ms - d[\"unsigned\"][\"age_ts\"]",
            "        del d[\"unsigned\"][\"age_ts\"]",
            "",
            "    if \"redacted_because\" in e.unsigned:",
            "        d[\"unsigned\"][\"redacted_because\"] = serialize_event(",
            "            e.unsigned[\"redacted_because\"], time_now_ms, config=config",
            "        )",
            "",
            "    if config.token_id is not None:",
            "        if config.token_id == getattr(e.internal_metadata, \"token_id\", None):",
            "            txn_id = getattr(e.internal_metadata, \"txn_id\", None)",
            "            if txn_id is not None:",
            "                d[\"unsigned\"][\"transaction_id\"] = txn_id",
            "",
            "    # invite_room_state and knock_room_state are a list of stripped room state events",
            "    # that are meant to provide metadata about a room to an invitee/knocker. They are",
            "    # intended to only be included in specific circumstances, such as down sync, and",
            "    # should not be included in any other case.",
            "    if not config.include_stripped_room_state:",
            "        d[\"unsigned\"].pop(\"invite_room_state\", None)",
            "        d[\"unsigned\"].pop(\"knock_room_state\", None)",
            "",
            "    if config.as_client_event:",
            "        d = config.event_format(d)",
            "",
            "    only_event_fields = config.only_event_fields",
            "    if only_event_fields:",
            "        if not isinstance(only_event_fields, list) or not all(",
            "            isinstance(f, str) for f in only_event_fields",
            "        ):",
            "            raise TypeError(\"only_event_fields must be a list of strings\")",
            "        d = only_fields(d, only_event_fields)",
            "",
            "    return d",
            "",
            "",
            "class EventClientSerializer:",
            "    \"\"\"Serializes events that are to be sent to clients.",
            "",
            "    This is used for bundling extra information with any events to be sent to",
            "    clients.",
            "    \"\"\"",
            "",
            "    def serialize_event(",
            "        self,",
            "        event: Union[JsonDict, EventBase],",
            "        time_now: int,",
            "        *,",
            "        config: SerializeEventConfig = _DEFAULT_SERIALIZE_EVENT_CONFIG,",
            "        bundle_aggregations: Optional[Dict[str, \"BundledAggregations\"]] = None,",
            "        apply_edits: bool = True,",
            "    ) -> JsonDict:",
            "        \"\"\"Serializes a single event.",
            "",
            "        Args:",
            "            event: The event being serialized.",
            "            time_now: The current time in milliseconds",
            "            config: Event serialization config",
            "            bundle_aggregations: A map from event_id to the aggregations to be bundled",
            "               into the event.",
            "            apply_edits: Whether the content of the event should be modified to reflect",
            "               any replacement in `bundle_aggregations[<event_id>].replace`.",
            "        Returns:",
            "            The serialized event",
            "        \"\"\"",
            "        # To handle the case of presence events and the like",
            "        if not isinstance(event, EventBase):",
            "            return event",
            "",
            "        serialized_event = serialize_event(event, time_now, config=config)",
            "",
            "        # Check if there are any bundled aggregations to include with the event.",
            "        if bundle_aggregations:",
            "            if event.event_id in bundle_aggregations:",
            "                self._inject_bundled_aggregations(",
            "                    event,",
            "                    time_now,",
            "                    config,",
            "                    bundle_aggregations,",
            "                    serialized_event,",
            "                    apply_edits=apply_edits,",
            "                )",
            "",
            "        return serialized_event",
            "",
            "    def _apply_edit(",
            "        self, orig_event: EventBase, serialized_event: JsonDict, edit: EventBase",
            "    ) -> None:",
            "        \"\"\"Replace the content, preserving existing relations of the serialized event.",
            "",
            "        Args:",
            "            orig_event: The original event.",
            "            serialized_event: The original event, serialized. This is modified.",
            "            edit: The event which edits the above.",
            "        \"\"\"",
            "",
            "        # Ensure we take copies of the edit content, otherwise we risk modifying",
            "        # the original event.",
            "        edit_content = edit.content.copy()",
            "",
            "        # Unfreeze the event content if necessary, so that we may modify it below",
            "        edit_content = unfreeze(edit_content)",
            "        serialized_event[\"content\"] = edit_content.get(\"m.new_content\", {})",
            "",
            "        # Check for existing relations",
            "        relates_to = orig_event.content.get(\"m.relates_to\")",
            "        if relates_to:",
            "            # Keep the relations, ensuring we use a dict copy of the original",
            "            serialized_event[\"content\"][\"m.relates_to\"] = relates_to.copy()",
            "        else:",
            "            serialized_event[\"content\"].pop(\"m.relates_to\", None)",
            "",
            "    def _inject_bundled_aggregations(",
            "        self,",
            "        event: EventBase,",
            "        time_now: int,",
            "        config: SerializeEventConfig,",
            "        bundled_aggregations: Dict[str, \"BundledAggregations\"],",
            "        serialized_event: JsonDict,",
            "        apply_edits: bool,",
            "    ) -> None:",
            "        \"\"\"Potentially injects bundled aggregations into the unsigned portion of the serialized event.",
            "",
            "        Args:",
            "            event: The event being serialized.",
            "            time_now: The current time in milliseconds",
            "            config: Event serialization config",
            "            bundled_aggregations: Bundled aggregations to be injected.",
            "                A map from event_id to aggregation data. Must contain at least an",
            "                entry for `event`.",
            "",
            "                While serializing the bundled aggregations this map may be searched",
            "                again for additional events in a recursive manner.",
            "            serialized_event: The serialized event which may be modified.",
            "            apply_edits: Whether the content of the event should be modified to reflect",
            "               any replacement in `aggregations.replace`.",
            "        \"\"\"",
            "",
            "        # We have already checked that aggregations exist for this event.",
            "        event_aggregations = bundled_aggregations[event.event_id]",
            "",
            "        # The JSON dictionary to be added under the unsigned property of the event",
            "        # being serialized.",
            "        serialized_aggregations = {}",
            "",
            "        if event_aggregations.annotations:",
            "            serialized_aggregations[",
            "                RelationTypes.ANNOTATION",
            "            ] = event_aggregations.annotations",
            "",
            "        if event_aggregations.references:",
            "            serialized_aggregations[",
            "                RelationTypes.REFERENCE",
            "            ] = event_aggregations.references",
            "",
            "        if event_aggregations.replace:",
            "            # If there is an edit, optionally apply it to the event.",
            "            edit = event_aggregations.replace",
            "            if apply_edits:",
            "                self._apply_edit(event, serialized_event, edit)",
            "",
            "            # Include information about it in the relations dict.",
            "            serialized_aggregations[RelationTypes.REPLACE] = {",
            "                \"event_id\": edit.event_id,",
            "                \"origin_server_ts\": edit.origin_server_ts,",
            "                \"sender\": edit.sender,",
            "            }",
            "",
            "        # Include any threaded replies to this event.",
            "        if event_aggregations.thread:",
            "            thread = event_aggregations.thread",
            "",
            "            serialized_latest_event = self.serialize_event(",
            "                thread.latest_event,",
            "                time_now,",
            "                config=config,",
            "                bundle_aggregations=bundled_aggregations,",
            "            )",
            "",
            "            thread_summary = {",
            "                \"latest_event\": serialized_latest_event,",
            "                \"count\": thread.count,",
            "                \"current_user_participated\": thread.current_user_participated,",
            "            }",
            "            serialized_aggregations[RelationTypes.THREAD] = thread_summary",
            "",
            "        # Include the bundled aggregations in the event.",
            "        if serialized_aggregations:",
            "            # There is likely already an \"unsigned\" field, but a filter might",
            "            # have stripped it off (via the event_fields option). The server is",
            "            # allowed to return additional fields, so add it back.",
            "            serialized_event.setdefault(\"unsigned\", {}).setdefault(",
            "                \"m.relations\", {}",
            "            ).update(serialized_aggregations)",
            "",
            "    def serialize_events(",
            "        self,",
            "        events: Iterable[Union[JsonDict, EventBase]],",
            "        time_now: int,",
            "        *,",
            "        config: SerializeEventConfig = _DEFAULT_SERIALIZE_EVENT_CONFIG,",
            "        bundle_aggregations: Optional[Dict[str, \"BundledAggregations\"]] = None,",
            "    ) -> List[JsonDict]:",
            "        \"\"\"Serializes multiple events.",
            "",
            "        Args:",
            "            event",
            "            time_now: The current time in milliseconds",
            "            config: Event serialization config",
            "            bundle_aggregations: Whether to include the bundled aggregations for this",
            "                event. Only applies to non-state events. (State events never include",
            "                bundled aggregations.)",
            "",
            "        Returns:",
            "            The list of serialized events",
            "        \"\"\"",
            "        return [",
            "            self.serialize_event(",
            "                event,",
            "                time_now,",
            "                config=config,",
            "                bundle_aggregations=bundle_aggregations,",
            "            )",
            "            for event in events",
            "        ]",
            "",
            "",
            "_PowerLevel = Union[str, int]",
            "",
            "",
            "def copy_and_fixup_power_levels_contents(",
            "    old_power_levels: Mapping[str, Union[_PowerLevel, Mapping[str, _PowerLevel]]]",
            ") -> Dict[str, Union[int, Dict[str, int]]]:",
            "    \"\"\"Copy the content of a power_levels event, unfreezing frozendicts along the way.",
            "",
            "    We accept as input power level values which are strings, provided they represent an",
            "    integer, e.g. `\"`100\"` instead of 100. Such strings are converted to integers",
            "    in the returned dictionary (hence \"fixup\" in the function name).",
            "",
            "    Note that future room versions will outlaw such stringy power levels (see",
            "    https://github.com/matrix-org/matrix-spec/issues/853).",
            "",
            "    Raises:",
            "        TypeError if the input does not look like a valid power levels event content",
            "    \"\"\"",
            "    if not isinstance(old_power_levels, collections.abc.Mapping):",
            "        raise TypeError(\"Not a valid power-levels content: %r\" % (old_power_levels,))",
            "",
            "    power_levels: Dict[str, Union[int, Dict[str, int]]] = {}",
            "",
            "    for k, v in old_power_levels.items():",
            "        if isinstance(v, collections.abc.Mapping):",
            "            h: Dict[str, int] = {}",
            "            power_levels[k] = h",
            "            for k1, v1 in v.items():",
            "                _copy_power_level_value_as_integer(v1, h, k1)",
            "",
            "        else:",
            "            _copy_power_level_value_as_integer(v, power_levels, k)",
            "",
            "    return power_levels",
            "",
            "",
            "def _copy_power_level_value_as_integer(",
            "    old_value: object,",
            "    power_levels: MutableMapping[str, Any],",
            "    key: str,",
            ") -> None:",
            "    \"\"\"Set `power_levels[key]` to the integer represented by `old_value`.",
            "",
            "    :raises TypeError: if `old_value` is not an integer, nor a base-10 string",
            "        representation of an integer.",
            "    \"\"\"",
            "    if isinstance(old_value, int):",
            "        power_levels[key] = old_value",
            "        return",
            "",
            "    if isinstance(old_value, str):",
            "        try:",
            "            parsed_value = int(old_value, base=10)",
            "        except ValueError:",
            "            # Fall through to the final TypeError.",
            "            pass",
            "        else:",
            "            power_levels[key] = parsed_value",
            "            return",
            "",
            "    raise TypeError(f\"Invalid power_levels value for {key}: {old_value}\")",
            "",
            "",
            "def validate_canonicaljson(value: Any) -> None:",
            "    \"\"\"",
            "    Ensure that the JSON object is valid according to the rules of canonical JSON.",
            "",
            "    See the appendix section 3.1: Canonical JSON.",
            "",
            "    This rejects JSON that has:",
            "    * An integer outside the range of [-2 ^ 53 + 1, 2 ^ 53 - 1]",
            "    * Floats",
            "    * NaN, Infinity, -Infinity",
            "    \"\"\"",
            "    if isinstance(value, int):",
            "        if value < CANONICALJSON_MIN_INT or CANONICALJSON_MAX_INT < value:",
            "            raise SynapseError(400, \"JSON integer out of range\", Codes.BAD_JSON)",
            "",
            "    elif isinstance(value, float):",
            "        # Note that Infinity, -Infinity, and NaN are also considered floats.",
            "        raise SynapseError(400, \"Bad JSON value: float\", Codes.BAD_JSON)",
            "",
            "    elif isinstance(value, collections.abc.Mapping):",
            "        for v in value.values():",
            "            validate_canonicaljson(v)",
            "",
            "    elif isinstance(value, (list, tuple)):",
            "        for i in value:",
            "            validate_canonicaljson(i)",
            "",
            "    elif not isinstance(value, (bool, str)) and value is not None:",
            "        # Other potential JSON values (bool, None, str) are safe.",
            "        raise SynapseError(400, \"Unknown JSON value\", Codes.BAD_JSON)",
            "",
            "",
            "def maybe_upsert_event_field(",
            "    event: EventBase, container: JsonDict, key: str, value: object",
            ") -> bool:",
            "    \"\"\"Upsert an event field, but only if this doesn't make the event too large.",
            "",
            "    Returns true iff the upsert took place.",
            "    \"\"\"",
            "    if key in container:",
            "        old_value: object = container[key]",
            "        container[key] = value",
            "        # NB: here and below, we assume that passing a non-None `time_now` argument to",
            "        # get_pdu_json doesn't increase the size of the encoded result.",
            "        upsert_okay = len(encode_canonical_json(event.get_pdu_json())) <= MAX_PDU_SIZE",
            "        if not upsert_okay:",
            "            container[key] = old_value",
            "    else:",
            "        container[key] = value",
            "        upsert_okay = len(encode_canonical_json(event.get_pdu_json())) <= MAX_PDU_SIZE",
            "        if not upsert_okay:",
            "            del container[key]",
            "",
            "    return upsert_okay"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2"
        ],
        "dele_reviseLocation": {
            "32": []
        },
        "addLocation": []
    },
    "synapse/handlers/message.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " from synapse.events import EventBase, relation_from_event"
            },
            "1": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 51,
                "PatchRowcode": " from synapse.events.builder import EventBuilder"
            },
            "2": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 52,
                "PatchRowcode": " from synapse.events.snapshot import EventContext"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 53,
                "PatchRowcode": "+from synapse.events.utils import maybe_upsert_event_field"
            },
            "4": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": " from synapse.events.validator import EventValidator"
            },
            "5": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 55,
                "PatchRowcode": " from synapse.handlers.directory import DirectoryHandler"
            },
            "6": {
                "beforePatchRowNumber": 55,
                "afterPatchRowNumber": 56,
                "PatchRowcode": " from synapse.logging import opentracing"
            },
            "7": {
                "beforePatchRowNumber": 1739,
                "afterPatchRowNumber": 1740,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 1740,
                "afterPatchRowNumber": 1741,
                "PatchRowcode": "             if event.type == EventTypes.Member:"
            },
            "9": {
                "beforePatchRowNumber": 1741,
                "afterPatchRowNumber": 1742,
                "PatchRowcode": "                 if event.content[\"membership\"] == Membership.INVITE:"
            },
            "10": {
                "beforePatchRowNumber": 1742,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    event.unsigned["
            },
            "11": {
                "beforePatchRowNumber": 1743,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        \"invite_room_state\""
            },
            "12": {
                "beforePatchRowNumber": 1744,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    ] = await self.store.get_stripped_room_state_from_event_context("
            },
            "13": {
                "beforePatchRowNumber": 1745,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        context,"
            },
            "14": {
                "beforePatchRowNumber": 1746,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        self.room_prejoin_state_types,"
            },
            "15": {
                "beforePatchRowNumber": 1747,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        membership_user_id=event.sender,"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1743,
                "PatchRowcode": "+                    maybe_upsert_event_field("
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1744,
                "PatchRowcode": "+                        event,"
            },
            "18": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1745,
                "PatchRowcode": "+                        event.unsigned,"
            },
            "19": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1746,
                "PatchRowcode": "+                        \"invite_room_state\","
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1747,
                "PatchRowcode": "+                        await self.store.get_stripped_room_state_from_event_context("
            },
            "21": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1748,
                "PatchRowcode": "+                            context,"
            },
            "22": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1749,
                "PatchRowcode": "+                            self.room_prejoin_state_types,"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1750,
                "PatchRowcode": "+                            membership_user_id=event.sender,"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1751,
                "PatchRowcode": "+                        ),"
            },
            "25": {
                "beforePatchRowNumber": 1748,
                "afterPatchRowNumber": 1752,
                "PatchRowcode": "                     )"
            },
            "26": {
                "beforePatchRowNumber": 1749,
                "afterPatchRowNumber": 1753,
                "PatchRowcode": " "
            },
            "27": {
                "beforePatchRowNumber": 1750,
                "afterPatchRowNumber": 1754,
                "PatchRowcode": "                     invitee = UserID.from_string(event.state_key)"
            },
            "28": {
                "beforePatchRowNumber": 1762,
                "afterPatchRowNumber": 1766,
                "PatchRowcode": "                         event.signatures.update(returned_invite.signatures)"
            },
            "29": {
                "beforePatchRowNumber": 1763,
                "afterPatchRowNumber": 1767,
                "PatchRowcode": " "
            },
            "30": {
                "beforePatchRowNumber": 1764,
                "afterPatchRowNumber": 1768,
                "PatchRowcode": "                 if event.content[\"membership\"] == Membership.KNOCK:"
            },
            "31": {
                "beforePatchRowNumber": 1765,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    event.unsigned["
            },
            "32": {
                "beforePatchRowNumber": 1766,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        \"knock_room_state\""
            },
            "33": {
                "beforePatchRowNumber": 1767,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    ] = await self.store.get_stripped_room_state_from_event_context("
            },
            "34": {
                "beforePatchRowNumber": 1768,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        context,"
            },
            "35": {
                "beforePatchRowNumber": 1769,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        self.room_prejoin_state_types,"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1769,
                "PatchRowcode": "+                    maybe_upsert_event_field("
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1770,
                "PatchRowcode": "+                        event,"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1771,
                "PatchRowcode": "+                        event.unsigned,"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1772,
                "PatchRowcode": "+                        \"knock_room_state\","
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1773,
                "PatchRowcode": "+                        await self.store.get_stripped_room_state_from_event_context("
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1774,
                "PatchRowcode": "+                            context,"
            },
            "42": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1775,
                "PatchRowcode": "+                            self.room_prejoin_state_types,"
            },
            "43": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1776,
                "PatchRowcode": "+                        ),"
            },
            "44": {
                "beforePatchRowNumber": 1770,
                "afterPatchRowNumber": 1777,
                "PatchRowcode": "                     )"
            },
            "45": {
                "beforePatchRowNumber": 1771,
                "afterPatchRowNumber": 1778,
                "PatchRowcode": " "
            },
            "46": {
                "beforePatchRowNumber": 1772,
                "afterPatchRowNumber": 1779,
                "PatchRowcode": "             if event.type == EventTypes.Redaction:"
            }
        },
        "frontPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017-2018 New Vector Ltd",
            "# Copyright 2019-2020 The Matrix.org Foundation C.I.C.",
            "# Copyrignt 2020 Sorunome",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from http import HTTPStatus",
            "from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Tuple",
            "",
            "from canonicaljson import encode_canonical_json",
            "",
            "from twisted.internet.interfaces import IDelayedCall",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    HistoryVisibility,",
            "    Membership,",
            "    RelationTypes,",
            "    UserTypes,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    ConsentNotGivenError,",
            "    LimitExceededError,",
            "    NotFoundError,",
            "    ShadowBanError,",
            "    SynapseError,",
            "    UnstableSpecAuthError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS",
            "from synapse.api.urls import ConsentURIBuilder",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase, relation_from_event",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.snapshot import EventContext",
            "from synapse.events.validator import EventValidator",
            "from synapse.handlers.directory import DirectoryHandler",
            "from synapse.logging import opentracing",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.send_event import ReplicationSendEventRestServlet",
            "from synapse.replication.http.send_events import ReplicationSendEventsRestServlet",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    MutableStateMap,",
            "    PersistedEventPosition,",
            "    Requester,",
            "    RoomAlias,",
            "    StateMap,",
            "    StreamToken,",
            "    UserID,",
            "    create_requester,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util import json_decoder, json_encoder, log_failure, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, gather_results",
            "from synapse.util.caches.expiringcache import ExpiringCache",
            "from synapse.util.metrics import measure_func",
            "from synapse.visibility import get_effective_room_visibility_from_state",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.events.third_party_rules import ThirdPartyEventRules",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class MessageHandler:",
            "    \"\"\"Contains some read only APIs to get state about a room\"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.auth = hs.get_auth()",
            "        self.clock = hs.get_clock()",
            "        self.state = hs.get_state_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        # The scheduled call to self._expire_event. None if no call is currently",
            "        # scheduled.",
            "        self._scheduled_expiry: Optional[IDelayedCall] = None",
            "",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"_schedule_next_expiry\", self._schedule_next_expiry",
            "            )",
            "",
            "    async def get_room_data(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        event_type: str,",
            "        state_key: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get data from a room.",
            "",
            "        Args:",
            "            requester: The user who did the request.",
            "            room_id",
            "            event_type",
            "            state_key",
            "        Returns:",
            "            The path data content.",
            "        Raises:",
            "            SynapseError or AuthError if the user is not in the room",
            "        \"\"\"",
            "        (",
            "            membership,",
            "            membership_event_id,",
            "        ) = await self.auth.check_user_in_room_or_world_readable(",
            "            room_id, requester, allow_departed_users=True",
            "        )",
            "",
            "        if membership == Membership.JOIN:",
            "            data = await self._storage_controllers.state.get_current_state_event(",
            "                room_id, event_type, state_key",
            "            )",
            "        elif membership == Membership.LEAVE:",
            "            key = (event_type, state_key)",
            "            # If the membership is not JOIN, then the event ID should exist.",
            "            assert (",
            "                membership_event_id is not None",
            "            ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "            room_state = await self._state_storage_controller.get_state_for_events(",
            "                [membership_event_id], StateFilter.from_types([key])",
            "            )",
            "            data = room_state[membership_event_id].get(key)",
            "        else:",
            "            # check_user_in_room_or_world_readable, if it doesn't raise an AuthError, should",
            "            # only ever return a Membership.JOIN/LEAVE object",
            "            #",
            "            # Safeguard in case it returned something else",
            "            logger.error(",
            "                \"Attempted to retrieve data from a room for a user that has never been in it. \"",
            "                \"This should not have happened.\"",
            "            )",
            "            raise UnstableSpecAuthError(",
            "                403,",
            "                \"User not in room\",",
            "                errcode=Codes.NOT_JOINED,",
            "            )",
            "",
            "        return data",
            "",
            "    async def get_state_events(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        at_token: Optional[StreamToken] = None,",
            "    ) -> List[dict]:",
            "        \"\"\"Retrieve all state events for a given room. If the user is",
            "        joined to the room then return the current state. If the user has",
            "        left the room return the state events from when they left. If an explicit",
            "        'at' parameter is passed, return the state events as of that event, if",
            "        visible.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "            state_filter: The state filter used to fetch state from the database.",
            "            at_token: the stream token of the at which we are requesting",
            "                the stats. If the user is not allowed to view the state as of that",
            "                stream token, we raise a 403 SynapseError. If None, returns the current",
            "                state based on the current_state_events table.",
            "        Returns:",
            "            A list of dicts representing state events. [{}, {}, {}]",
            "        Raises:",
            "            NotFoundError (404) if the at token does not yield an event",
            "",
            "            AuthError (403) if the user doesn't have permission to view",
            "            members of this room.",
            "        \"\"\"",
            "        state_filter = state_filter or StateFilter.all()",
            "        user_id = requester.user.to_string()",
            "",
            "        if at_token:",
            "            last_event_id = (",
            "                await self.store.get_last_event_in_room_before_stream_ordering(",
            "                    room_id,",
            "                    end_token=at_token.room_key,",
            "                )",
            "            )",
            "",
            "            if not last_event_id:",
            "                raise NotFoundError(\"Can't find event for token %s\" % (at_token,))",
            "",
            "            if not await self._user_can_see_state_at_event(",
            "                user_id, room_id, last_event_id",
            "            ):",
            "                raise AuthError(",
            "                    403,",
            "                    \"User %s not allowed to view events in room %s at token %s\"",
            "                    % (user_id, room_id, at_token),",
            "                )",
            "",
            "            room_state_events = (",
            "                await self._state_storage_controller.get_state_for_events(",
            "                    [last_event_id], state_filter=state_filter",
            "                )",
            "            )",
            "            room_state: Mapping[Any, EventBase] = room_state_events[last_event_id]",
            "        else:",
            "            (",
            "                membership,",
            "                membership_event_id,",
            "            ) = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "",
            "            if membership == Membership.JOIN:",
            "                state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                    room_id, state_filter=state_filter",
            "                )",
            "                room_state = await self.store.get_events(state_ids.values())",
            "            elif membership == Membership.LEAVE:",
            "                # If the membership is not JOIN, then the event ID should exist.",
            "                assert (",
            "                    membership_event_id is not None",
            "                ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "                room_state_events = (",
            "                    await self._state_storage_controller.get_state_for_events(",
            "                        [membership_event_id], state_filter=state_filter",
            "                    )",
            "                )",
            "                room_state = room_state_events[membership_event_id]",
            "",
            "        now = self.clock.time_msec()",
            "        events = self._event_serializer.serialize_events(room_state.values(), now)",
            "        return events",
            "",
            "    async def _user_can_see_state_at_event(",
            "        self, user_id: str, room_id: str, event_id: str",
            "    ) -> bool:",
            "        # check whether the user was in the room, and the history visibility,",
            "        # at that time.",
            "        state_map = await self._state_storage_controller.get_state_for_event(",
            "            event_id,",
            "            StateFilter.from_types(",
            "                [",
            "                    (EventTypes.Member, user_id),",
            "                    (EventTypes.RoomHistoryVisibility, \"\"),",
            "                ]",
            "            ),",
            "        )",
            "",
            "        membership = None",
            "        membership_event = state_map.get((EventTypes.Member, user_id))",
            "        if membership_event:",
            "            membership = membership_event.membership",
            "",
            "        # if the user was a member of the room at the time of the event,",
            "        # they can see it.",
            "        if membership == Membership.JOIN:",
            "            return True",
            "",
            "        # otherwise, it depends on the history visibility.",
            "        visibility = get_effective_room_visibility_from_state(state_map)",
            "",
            "        if visibility == HistoryVisibility.JOINED:",
            "            # we weren't a member at the time of the event, so we can't see this event.",
            "            return False",
            "",
            "        # otherwise *invited* is good enough",
            "        if membership == Membership.INVITE:",
            "            return True",
            "",
            "        if visibility == HistoryVisibility.INVITED:",
            "            # we weren't invited, so we can't see this event.",
            "            return False",
            "",
            "        if visibility == HistoryVisibility.WORLD_READABLE:",
            "            return True",
            "",
            "        # So it's SHARED, and the user was not a member at the time. The user cannot",
            "        # see history, unless they have *subsequently* joined the room.",
            "        #",
            "        # XXX: if the user has subsequently joined and then left again,",
            "        # ideally we would share history up to the point they left. But",
            "        # we don't know when they left. We just treat it as though they",
            "        # never joined, and restrict access.",
            "",
            "        (",
            "            current_membership,",
            "            _,",
            "        ) = await self.store.get_local_current_membership_for_user_in_room(",
            "            user_id, event_id",
            "        )",
            "        return current_membership == Membership.JOIN",
            "",
            "    async def get_joined_members(self, requester: Requester, room_id: str) -> dict:",
            "        \"\"\"Get all the joined members in the room and their profile information.",
            "",
            "        If the user has left the room return the state events from when they left.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "        Returns:",
            "            A dict of user_id to profile info",
            "        \"\"\"",
            "        if not requester.app_service:",
            "            # We check AS auth after fetching the room membership, as it",
            "            # requires us to pull out all joined members anyway.",
            "            membership, _ = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "            if membership != Membership.JOIN:",
            "                raise SynapseError(",
            "                    code=403,",
            "                    errcode=Codes.FORBIDDEN,",
            "                    msg=\"Getting joined members while not being a current member of the room is forbidden.\",",
            "                )",
            "",
            "        users_with_profile = (",
            "            await self._state_storage_controller.get_users_in_room_with_profiles(",
            "                room_id",
            "            )",
            "        )",
            "",
            "        # If this is an AS, double check that they are allowed to see the members.",
            "        # This can either be because the AS user is in the room or because there",
            "        # is a user in the room that the AS is \"interested in\"",
            "        if (",
            "            requester.app_service",
            "            and requester.user.to_string() not in users_with_profile",
            "        ):",
            "            for uid in users_with_profile:",
            "                if requester.app_service.is_interested_in_user(uid):",
            "                    break",
            "            else:",
            "                # Loop fell through, AS has no interested users in room",
            "                raise UnstableSpecAuthError(",
            "                    403,",
            "                    \"Appservice not in room\",",
            "                    errcode=Codes.NOT_JOINED,",
            "                )",
            "",
            "        return {",
            "            user_id: {",
            "                \"avatar_url\": profile.avatar_url,",
            "                \"display_name\": profile.display_name,",
            "            }",
            "            for user_id, profile in users_with_profile.items()",
            "        }",
            "",
            "    def maybe_schedule_expiry(self, event: EventBase) -> None:",
            "        \"\"\"Schedule the expiry of an event if there's not already one scheduled,",
            "        or if the one running is for an event that will expire after the provided",
            "        timestamp.",
            "",
            "        This function needs to invalidate the event cache, which is only possible on",
            "        the master process, and therefore needs to be run on there.",
            "",
            "        Args:",
            "            event: The event to schedule the expiry of.",
            "        \"\"\"",
            "",
            "        expiry_ts = event.content.get(EventContentFields.SELF_DESTRUCT_AFTER)",
            "        if not isinstance(expiry_ts, int) or event.is_state():",
            "            return",
            "",
            "        # _schedule_expiry_for_event won't actually schedule anything if there's already",
            "        # a task scheduled for a timestamp that's sooner than the provided one.",
            "        self._schedule_expiry_for_event(event.event_id, expiry_ts)",
            "",
            "    async def _schedule_next_expiry(self) -> None:",
            "        \"\"\"Retrieve the ID and the expiry timestamp of the next event to be expired,",
            "        and schedule an expiry task for it.",
            "",
            "        If there's no event left to expire, set _expiry_scheduled to None so that a",
            "        future call to save_expiry_ts can schedule a new expiry task.",
            "        \"\"\"",
            "        # Try to get the expiry timestamp of the next event to expire.",
            "        res = await self.store.get_next_event_to_expire()",
            "        if res:",
            "            event_id, expiry_ts = res",
            "            self._schedule_expiry_for_event(event_id, expiry_ts)",
            "",
            "    def _schedule_expiry_for_event(self, event_id: str, expiry_ts: int) -> None:",
            "        \"\"\"Schedule an expiry task for the provided event if there's not already one",
            "        scheduled at a timestamp that's sooner than the provided one.",
            "",
            "        Args:",
            "            event_id: The ID of the event to expire.",
            "            expiry_ts: The timestamp at which to expire the event.",
            "        \"\"\"",
            "        if self._scheduled_expiry:",
            "            # If the provided timestamp refers to a time before the scheduled time of the",
            "            # next expiry task, cancel that task and reschedule it for this timestamp.",
            "            next_scheduled_expiry_ts = self._scheduled_expiry.getTime() * 1000",
            "            if expiry_ts < next_scheduled_expiry_ts:",
            "                self._scheduled_expiry.cancel()",
            "            else:",
            "                return",
            "",
            "        # Figure out how many seconds we need to wait before expiring the event.",
            "        now_ms = self.clock.time_msec()",
            "        delay = (expiry_ts - now_ms) / 1000",
            "",
            "        # callLater doesn't support negative delays, so trim the delay to 0 if we're",
            "        # in that case.",
            "        if delay < 0:",
            "            delay = 0",
            "",
            "        logger.info(\"Scheduling expiry for event %s in %.3fs\", event_id, delay)",
            "",
            "        self._scheduled_expiry = self.clock.call_later(",
            "            delay,",
            "            run_as_background_process,",
            "            \"_expire_event\",",
            "            self._expire_event,",
            "            event_id,",
            "        )",
            "",
            "    async def _expire_event(self, event_id: str) -> None:",
            "        \"\"\"Retrieve and expire an event that needs to be expired from the database.",
            "",
            "        If the event doesn't exist in the database, log it and delete the expiry date",
            "        from the database (so that we don't try to expire it again).",
            "        \"\"\"",
            "        assert self._ephemeral_events_enabled",
            "",
            "        self._scheduled_expiry = None",
            "",
            "        logger.info(\"Expiring event %s\", event_id)",
            "",
            "        try:",
            "            # Expire the event if we know about it. This function also deletes the expiry",
            "            # date from the database in the same database transaction.",
            "            await self.store.expire_event(event_id)",
            "        except Exception as e:",
            "            logger.error(\"Could not expire event %s: %r\", event_id, e)",
            "",
            "        # Schedule the expiry of the next event to expire.",
            "        await self._schedule_next_expiry()",
            "",
            "",
            "# The duration (in ms) after which rooms should be removed",
            "# `_rooms_to_exclude_from_dummy_event_insertion` (with the effect that we will try",
            "# to generate a dummy event for them once more)",
            "#",
            "_DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY = 7 * 24 * 60 * 60 * 1000",
            "",
            "",
            "class EventCreationHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "        self.auth_blocking = hs.get_auth_blocking()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.state = hs.get_state_handler()",
            "        self.clock = hs.get_clock()",
            "        self.validator = EventValidator()",
            "        self.profile_handler = hs.get_profile_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self.server_name = hs.hostname",
            "        self.notifier = hs.get_notifier()",
            "        self.config = hs.config",
            "        self.require_membership_for_aliases = (",
            "            hs.config.server.require_membership_for_aliases",
            "        )",
            "        self._events_shard_config = self.config.worker.events_shard_config",
            "        self._instance_name = hs.get_instance_name()",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self.room_prejoin_state_types = self.hs.config.api.room_prejoin_state",
            "",
            "        self.membership_types_to_include_profile_data_in = {",
            "            Membership.JOIN,",
            "            Membership.KNOCK,",
            "        }",
            "        if self.hs.config.server.include_profile_data_on_invite:",
            "            self.membership_types_to_include_profile_data_in.add(Membership.INVITE)",
            "",
            "        self.send_event = ReplicationSendEventRestServlet.make_client(hs)",
            "        self.send_events = ReplicationSendEventsRestServlet.make_client(hs)",
            "",
            "        self.request_ratelimiter = hs.get_request_ratelimiter()",
            "",
            "        # We arbitrarily limit concurrent event creation for a room to 5.",
            "        # This is to stop us from diverging history *too* much.",
            "        self.limiter = Linearizer(max_count=5, name=\"room_event_creation_limit\")",
            "",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "",
            "        self.spam_checker = hs.get_spam_checker()",
            "        self.third_party_event_rules: \"ThirdPartyEventRules\" = (",
            "            self.hs.get_third_party_event_rules()",
            "        )",
            "",
            "        self._block_events_without_consent_error = (",
            "            self.config.consent.block_events_without_consent_error",
            "        )",
            "",
            "        # we need to construct a ConsentURIBuilder here, as it checks that the necessary",
            "        # config options, but *only* if we have a configuration for which we are",
            "        # going to need it.",
            "        if self._block_events_without_consent_error:",
            "            self._consent_uri_builder = ConsentURIBuilder(self.config)",
            "",
            "        # Rooms which should be excluded from dummy insertion. (For instance,",
            "        # those without local users who can send events into the room).",
            "        #",
            "        # map from room id to time-of-last-attempt.",
            "        #",
            "        self._rooms_to_exclude_from_dummy_event_insertion: Dict[str, int] = {}",
            "        # The number of forward extremeities before a dummy event is sent.",
            "        self._dummy_events_threshold = hs.config.server.dummy_events_threshold",
            "",
            "        if (",
            "            self.config.worker.run_background_tasks",
            "            and self.config.server.cleanup_extremities_with_dummy_events",
            "        ):",
            "            self.clock.looping_call(",
            "                lambda: run_as_background_process(",
            "                    \"send_dummy_events_to_fill_extremities\",",
            "                    self._send_dummy_events_to_fill_extremities,",
            "                ),",
            "                5 * 60 * 1000,",
            "            )",
            "",
            "        self._message_handler = hs.get_message_handler()",
            "",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._external_cache = hs.get_external_cache()",
            "",
            "        # Stores the state groups we've recently added to the joined hosts",
            "        # external cache. Note that the timeout must be significantly less than",
            "        # the TTL on the external cache.",
            "        self._external_cache_joined_hosts_updates: Optional[ExpiringCache] = None",
            "        if self._external_cache.is_enabled():",
            "            self._external_cache_joined_hosts_updates = ExpiringCache(",
            "                \"_external_cache_joined_hosts_updates\",",
            "                self.clock,",
            "                expiry_ms=30 * 60 * 1000,",
            "            )",
            "",
            "    async def create_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        txn_id: Optional[str] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        require_consent: bool = True,",
            "        outlier: bool = False,",
            "        historical: bool = False,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"",
            "        Given a dict from a client, create a new event. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events.",
            "",
            "        Creates an FrozenEvent object, filling out auth_events, prev_events,",
            "        etc.",
            "",
            "        Adds display names to Join membership events.",
            "",
            "        Args:",
            "            requester",
            "            event_dict: An entire event",
            "            txn_id",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases like MSC2716.",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "                If non-None, prev_event_ids must also be provided.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This is used particularly by the MSC2716",
            "                /batch_send endpoint. One use case is with insertion events which float at",
            "                the beginning of a historical batch and don't have any `prev_events` to",
            "                derive from; we add all of these state events as the explicit state so the",
            "                rest of the historical batch can inherit the same state and state_group.",
            "                This should normally be left as None, which will cause the auth_event_ids",
            "                to be calculated based on the room state at the prev_events.",
            "",
            "            require_consent: Whether to check if the requester has",
            "                consented to the privacy policy.",
            "",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "",
            "            historical: Indicates whether the message is being inserted",
            "                back in time around some existing events. This is used to skip",
            "                a few checks and mark the event as backfilled.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Raises:",
            "            ResourceLimitError if server is blocked to some resource being",
            "            exceeded",
            "",
            "        Returns:",
            "            Tuple of created event, Context",
            "        \"\"\"",
            "        await self.auth_blocking.check_auth_blocking(requester=requester)",
            "",
            "        if event_dict[\"type\"] == EventTypes.Create and event_dict[\"state_key\"] == \"\":",
            "            room_version_id = event_dict[\"content\"][\"room_version\"]",
            "            maybe_room_version_obj = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "            if not maybe_room_version_obj:",
            "                # this can happen if support is withdrawn for a room version",
            "                raise UnsupportedRoomVersionError(room_version_id)",
            "            room_version_obj = maybe_room_version_obj",
            "        else:",
            "            try:",
            "                room_version_obj = await self.store.get_room_version(",
            "                    event_dict[\"room_id\"]",
            "                )",
            "            except NotFoundError:",
            "                raise AuthError(403, \"Unknown room\")",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        self.validator.validate_builder(builder)",
            "",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            target = UserID.from_string(builder.state_key)",
            "",
            "            if membership in self.membership_types_to_include_profile_data_in:",
            "                # If event doesn't include a display name, add one.",
            "                profile = self.profile_handler",
            "                content = builder.content",
            "",
            "                try:",
            "                    if \"displayname\" not in content:",
            "                        displayname = await profile.get_displayname(target)",
            "                        if displayname is not None:",
            "                            content[\"displayname\"] = displayname",
            "                    if \"avatar_url\" not in content:",
            "                        avatar_url = await profile.get_avatar_url(target)",
            "                        if avatar_url is not None:",
            "                            content[\"avatar_url\"] = avatar_url",
            "                except Exception as e:",
            "                    logger.info(",
            "                        \"Failed to get profile information for %r: %s\", target, e",
            "                    )",
            "",
            "        is_exempt = await self._is_exempt_from_privacy_policy(builder, requester)",
            "        if require_consent and not is_exempt:",
            "            await self.assert_accepted_privacy_policy(requester)",
            "",
            "        if requester.access_token_id is not None:",
            "            builder.internal_metadata.token_id = requester.access_token_id",
            "",
            "        if txn_id is not None:",
            "            builder.internal_metadata.txn_id = txn_id",
            "",
            "        builder.internal_metadata.outlier = outlier",
            "",
            "        builder.internal_metadata.historical = historical",
            "",
            "        event, context = await self.create_new_client_event(",
            "            builder=builder,",
            "            requester=requester,",
            "            allow_no_prev_events=allow_no_prev_events,",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=auth_event_ids,",
            "            state_event_ids=state_event_ids,",
            "            depth=depth,",
            "            state_map=state_map,",
            "            for_batch=for_batch,",
            "            current_state_group=current_state_group,",
            "        )",
            "",
            "        # In an ideal world we wouldn't need the second part of this condition. However,",
            "        # this behaviour isn't spec'd yet, meaning we should be able to deactivate this",
            "        # behaviour. Another reason is that this code is also evaluated each time a new",
            "        # m.room.aliases event is created, which includes hitting a /directory route.",
            "        # Therefore not including this condition here would render the similar one in",
            "        # synapse.handlers.directory pointless.",
            "        if builder.type == EventTypes.Aliases and self.require_membership_for_aliases:",
            "            # Ideally we'd do the membership check in event_auth.check(), which",
            "            # describes a spec'd algorithm for authenticating events received over",
            "            # federation as well as those created locally. As of room v3, aliases events",
            "            # can be created by users that are not in the room, therefore we have to",
            "            # tolerate them in event_auth.check().",
            "            if for_batch:",
            "                assert state_map is not None",
            "                prev_event_id = state_map.get((EventTypes.Member, event.sender))",
            "            else:",
            "                prev_state_ids = await context.get_prev_state_ids(",
            "                    StateFilter.from_types([(EventTypes.Member, None)])",
            "                )",
            "                prev_event_id = prev_state_ids.get((EventTypes.Member, event.sender))",
            "            prev_event = (",
            "                await self.store.get_event(prev_event_id, allow_none=True)",
            "                if prev_event_id",
            "                else None",
            "            )",
            "            if not prev_event or prev_event.membership != Membership.JOIN:",
            "                logger.warning(",
            "                    (",
            "                        \"Attempt to send `m.room.aliases` in room %s by user %s but\"",
            "                        \" membership is %s\"",
            "                    ),",
            "                    event.room_id,",
            "                    event.sender,",
            "                    prev_event.membership if prev_event else None,",
            "                )",
            "",
            "                raise AuthError(",
            "                    403, \"You must be in the room to create an alias for it\"",
            "                )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "",
            "        return event, context",
            "",
            "    async def _is_exempt_from_privacy_policy(",
            "        self, builder: EventBuilder, requester: Requester",
            "    ) -> bool:",
            "        \"\"\" \"Determine if an event to be sent is exempt from having to consent",
            "        to the privacy policy",
            "",
            "        Args:",
            "            builder: event being created",
            "            requester: user requesting this event",
            "",
            "        Returns:",
            "            true if the event can be sent without the user consenting",
            "        \"\"\"",
            "        # the only thing the user can do is join the server notices room.",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            if membership == Membership.JOIN:",
            "                return await self.store.is_server_notice_room(builder.room_id)",
            "            elif membership == Membership.LEAVE:",
            "                # the user is always allowed to leave (but not kick people)",
            "                return builder.state_key == requester.user.to_string()",
            "        return False",
            "",
            "    async def assert_accepted_privacy_policy(self, requester: Requester) -> None:",
            "        \"\"\"Check if a user has accepted the privacy policy",
            "",
            "        Called when the given user is about to do something that requires",
            "        privacy consent. We see if the user is exempt and otherwise check that",
            "        they have given consent. If they have not, a ConsentNotGiven error is",
            "        raised.",
            "",
            "        Args:",
            "            requester: The user making the request",
            "",
            "        Returns:",
            "            Returns normally if the user has consented or is exempt",
            "",
            "        Raises:",
            "            ConsentNotGivenError: if the user has not given consent yet",
            "        \"\"\"",
            "        if self._block_events_without_consent_error is None:",
            "            return",
            "",
            "        # exempt AS users from needing consent",
            "        if requester.app_service is not None:",
            "            return",
            "",
            "        user_id = requester.authenticated_entity",
            "        if not user_id.startswith(\"@\"):",
            "            # The authenticated entity might not be a user, e.g. if it's the",
            "            # server puppetting the user.",
            "            return",
            "",
            "        user = UserID.from_string(user_id)",
            "",
            "        # exempt the system notices user",
            "        if (",
            "            self.config.servernotices.server_notices_mxid is not None",
            "            and user_id == self.config.servernotices.server_notices_mxid",
            "        ):",
            "            return",
            "",
            "        u = await self.store.get_user_by_id(user_id)",
            "        assert u is not None",
            "        if u[\"user_type\"] in (UserTypes.SUPPORT, UserTypes.BOT):",
            "            # support and bot users are not required to consent",
            "            return",
            "        if u[\"appservice_id\"] is not None:",
            "            # users registered by an appservice are exempt",
            "            return",
            "        if u[\"consent_version\"] == self.config.consent.user_consent_version:",
            "            return",
            "",
            "        consent_uri = self._consent_uri_builder.build_user_consent_uri(user.localpart)",
            "        msg = self._block_events_without_consent_error % {\"consent_uri\": consent_uri}",
            "        raise ConsentNotGivenError(msg=msg, consent_uri=consent_uri)",
            "",
            "    async def deduplicate_state_event(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"",
            "        Checks whether event is in the latest resolved state in context.",
            "",
            "        Args:",
            "            event: The event to check for duplication.",
            "            context: The event context.",
            "",
            "        Returns:",
            "            The previous version of the event is returned, if it is found in the",
            "            event context. Otherwise, None is returned.",
            "        \"\"\"",
            "        if event.internal_metadata.is_outlier():",
            "            # This can happen due to out of band memberships",
            "            return None",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(event.type, None)])",
            "        )",
            "        prev_event_id = prev_state_ids.get((event.type, event.state_key))",
            "        if not prev_event_id:",
            "            return None",
            "        prev_event = await self.store.get_event(prev_event_id, allow_none=True)",
            "        if not prev_event:",
            "            return None",
            "",
            "        if prev_event and event.user_id == prev_event.user_id:",
            "            prev_content = encode_canonical_json(prev_event.content)",
            "            next_content = encode_canonical_json(event.content)",
            "            if prev_content == next_content:",
            "                return prev_event",
            "        return None",
            "",
            "    async def get_event_from_transaction(",
            "        self,",
            "        requester: Requester,",
            "        txn_id: str,",
            "        room_id: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"For the given transaction ID and room ID, check if there is a matching event.",
            "        If so, fetch it and return it.",
            "",
            "        Args:",
            "            requester: The requester making the request in the context of which we want",
            "                to fetch the event.",
            "            txn_id: The transaction ID.",
            "            room_id: The room ID.",
            "",
            "        Returns:",
            "            An event if one could be found, None otherwise.",
            "        \"\"\"",
            "        if requester.access_token_id:",
            "            existing_event_id = await self.store.get_event_id_from_transaction_id(",
            "                room_id,",
            "                requester.user.to_string(),",
            "                requester.access_token_id,",
            "                txn_id,",
            "            )",
            "            if existing_event_id:",
            "                return await self.store.get_event(existing_event_id)",
            "",
            "        return None",
            "",
            "    async def create_and_send_nonmember_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        ratelimit: bool = True,",
            "        txn_id: Optional[str] = None,",
            "        ignore_shadow_ban: bool = False,",
            "        outlier: bool = False,",
            "        historical: bool = False,",
            "        depth: Optional[int] = None,",
            "    ) -> Tuple[EventBase, int]:",
            "        \"\"\"",
            "        Creates an event, then sends it.",
            "",
            "        See self.create_event and self.handle_new_client_event.",
            "",
            "        Args:",
            "            requester: The requester sending the event.",
            "            event_dict: An entire event.",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases like MSC2716.",
            "            prev_event_ids:",
            "                The event IDs to use as the prev events.",
            "                Should normally be left as None to automatically request them",
            "                from the database.",
            "            state_event_ids:",
            "                The full state at a given event. This is used particularly by the MSC2716",
            "                /batch_send endpoint. One use case is with insertion events which float at",
            "                the beginning of a historical batch and don't have any `prev_events` to",
            "                derive from; we add all of these state events as the explicit state so the",
            "                rest of the historical batch can inherit the same state and state_group.",
            "                This should normally be left as None, which will cause the auth_event_ids",
            "                to be calculated based on the room state at the prev_events.",
            "            ratelimit: Whether to rate limit this send.",
            "            txn_id: The transaction ID.",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "            historical: Indicates whether the message is being inserted",
            "                back in time around some existing events. This is used to skip",
            "                a few checks and mark the event as backfilled.",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "        Returns:",
            "            The event, and its stream ordering (if deduplication happened,",
            "            the previous, duplicate event).",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "        \"\"\"",
            "",
            "        if event_dict[\"type\"] == EventTypes.Member:",
            "            raise SynapseError(",
            "                500, \"Tried to send member event through non-member codepath\"",
            "            )",
            "",
            "        if not ignore_shadow_ban and requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        if ratelimit:",
            "            await self.request_ratelimiter.ratelimit(requester, update=False)",
            "",
            "        # We limit the number of concurrent event sends in a room so that we",
            "        # don't fork the DAG too much. If we don't limit then we can end up in",
            "        # a situation where event persistence can't keep up, causing",
            "        # extremities to pile up, which in turn leads to state resolution",
            "        # taking longer.",
            "        async with self.limiter.queue(event_dict[\"room_id\"]):",
            "            if txn_id:",
            "                event = await self.get_event_from_transaction(",
            "                    requester, txn_id, event_dict[\"room_id\"]",
            "                )",
            "                if event:",
            "                    # we know it was persisted, so must have a stream ordering",
            "                    assert event.internal_metadata.stream_ordering",
            "                    return (",
            "                        event,",
            "                        event.internal_metadata.stream_ordering,",
            "                    )",
            "",
            "            event, context = await self.create_event(",
            "                requester,",
            "                event_dict,",
            "                txn_id=txn_id,",
            "                allow_no_prev_events=allow_no_prev_events,",
            "                prev_event_ids=prev_event_ids,",
            "                state_event_ids=state_event_ids,",
            "                outlier=outlier,",
            "                historical=historical,",
            "                depth=depth,",
            "            )",
            "",
            "            assert self.hs.is_mine_id(event.sender), \"User must be our own: %s\" % (",
            "                event.sender,",
            "            )",
            "",
            "            spam_check_result = await self.spam_checker.check_event_for_spam(event)",
            "            if spam_check_result != self.spam_checker.NOT_SPAM:",
            "                if isinstance(spam_check_result, tuple):",
            "                    try:",
            "                        [code, dict] = spam_check_result",
            "                        raise SynapseError(",
            "                            403,",
            "                            \"This message had been rejected as probable spam\",",
            "                            code,",
            "                            dict,",
            "                        )",
            "                    except ValueError:",
            "                        logger.error(",
            "                            \"Spam-check module returned invalid error value. Expecting [code, dict], got %s\",",
            "                            spam_check_result,",
            "                        )",
            "",
            "                        raise SynapseError(",
            "                            403,",
            "                            \"This message has been rejected as probable spam\",",
            "                            Codes.FORBIDDEN,",
            "                        )",
            "",
            "                # Backwards compatibility: if the return value is not an error code, it",
            "                # means the module returned an error message to be included in the",
            "                # SynapseError (which is now deprecated).",
            "                raise SynapseError(",
            "                    403,",
            "                    spam_check_result,",
            "                    Codes.FORBIDDEN,",
            "                )",
            "",
            "            ev = await self.handle_new_client_event(",
            "                requester=requester,",
            "                events_and_context=[(event, context)],",
            "                ratelimit=ratelimit,",
            "                ignore_shadow_ban=ignore_shadow_ban,",
            "            )",
            "",
            "        # we know it was persisted, so must have a stream ordering",
            "        assert ev.internal_metadata.stream_ordering",
            "        return ev, ev.internal_metadata.stream_ordering",
            "",
            "    @measure_func(\"create_new_client_event\")",
            "    async def create_new_client_event(",
            "        self,",
            "        builder: EventBuilder,",
            "        requester: Optional[Requester] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Create a new event for a local client. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events.",
            "",
            "        Args:",
            "            builder:",
            "            requester:",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases like MSC2716.",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This is used particularly by the MSC2716",
            "                /batch_send endpoint. One use case is with insertion events which float at",
            "                the beginning of a historical batch and don't have any `prev_events` to",
            "                derive from; we add all of these state events as the explicit state so the",
            "                rest of the historical batch can inherit the same state and state_group.",
            "                This should normally be left as None, which will cause the auth_event_ids",
            "                to be calculated based on the room state at the prev_events.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Returns:",
            "            Tuple of created event, context",
            "        \"\"\"",
            "        # Strip down the state_event_ids to only what we need to auth the event.",
            "        # For example, we don't need extra m.room.member that don't match event.sender",
            "        if state_event_ids is not None:",
            "            # Do a quick check to make sure that prev_event_ids is present to",
            "            # make the type-checking around `builder.build` happy.",
            "            # prev_event_ids could be an empty array though.",
            "            assert prev_event_ids is not None",
            "",
            "            temp_event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=state_event_ids,",
            "                depth=depth,",
            "            )",
            "            state_events = await self.store.get_events_as_list(state_event_ids)",
            "            # Create a StateMap[str]",
            "            current_state_ids = {",
            "                (e.type, e.state_key): e.event_id for e in state_events",
            "            }",
            "            # Actually strip down and only use the necessary auth events",
            "            auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "                event=temp_event,",
            "                current_state_ids=current_state_ids,",
            "                for_verification=False,",
            "            )",
            "",
            "        if prev_event_ids is not None:",
            "            assert (",
            "                len(prev_event_ids) <= 10",
            "            ), \"Attempting to create an event with %i prev_events\" % (",
            "                len(prev_event_ids),",
            "            )",
            "        else:",
            "            prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        # Do a quick sanity check here, rather than waiting until we've created the",
            "        # event and then try to auth it (which fails with a somewhat confusing \"No",
            "        # create event in auth events\")",
            "        if allow_no_prev_events:",
            "            # We allow events with no `prev_events` but it better have some `auth_events`",
            "            assert (",
            "                builder.type == EventTypes.Create",
            "                # Allow an event to have empty list of prev_event_ids",
            "                # only if it has auth_event_ids.",
            "                or auth_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events or auth_event_ids\"",
            "        else:",
            "            # we now ought to have some prev_events (unless it's a create event).",
            "            assert (",
            "                builder.type == EventTypes.Create or prev_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events\"",
            "",
            "        if for_batch:",
            "            assert prev_event_ids is not None",
            "            assert state_map is not None",
            "            assert current_state_group is not None",
            "            auth_ids = self._event_auth_handler.compute_auth_events(builder, state_map)",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids, auth_event_ids=auth_ids, depth=depth",
            "            )",
            "            context = await self.state.compute_event_context_for_batched(",
            "                event, state_map, current_state_group",
            "            )",
            "        else:",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=auth_event_ids,",
            "                depth=depth,",
            "            )",
            "",
            "            # Pass on the outlier property from the builder to the event",
            "            # after it is created",
            "            if builder.internal_metadata.outlier:",
            "                event.internal_metadata.outlier = True",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "            elif (",
            "                event.type == EventTypes.MSC2716_INSERTION",
            "                and state_event_ids",
            "                and builder.internal_metadata.is_historical()",
            "            ):",
            "                # Add explicit state to the insertion event so it has state to derive",
            "                # from even though it's floating with no `prev_events`. The rest of",
            "                # the batch can derive from this state and state_group.",
            "                #",
            "                # TODO(faster_joins): figure out how this works, and make sure that the",
            "                #   old state is complete.",
            "                #   https://github.com/matrix-org/synapse/issues/13003",
            "                metadata = await self.store.get_metadata_for_events(state_event_ids)",
            "",
            "                state_map_for_event: MutableStateMap[str] = {}",
            "                for state_id in state_event_ids:",
            "                    data = metadata.get(state_id)",
            "                    if data is None:",
            "                        # We're trying to persist a new historical batch of events",
            "                        # with the given state, e.g. via",
            "                        # `RoomBatchSendEventRestServlet`. The state can be inferred",
            "                        # by Synapse or set directly by the client.",
            "                        #",
            "                        # Either way, we should have persisted all the state before",
            "                        # getting here.",
            "                        raise Exception(",
            "                            f\"State event {state_id} not found in DB,\"",
            "                            \" Synapse should have persisted it before using it.\"",
            "                        )",
            "",
            "                    if data.state_key is None:",
            "                        raise Exception(",
            "                            f\"Trying to set non-state event {state_id} as state\"",
            "                        )",
            "",
            "                    state_map_for_event[(data.event_type, data.state_key)] = state_id",
            "",
            "                context = await self.state.compute_event_context(",
            "                    event,",
            "                    state_ids_before_event=state_map_for_event,",
            "                    # TODO(faster_joins): check how MSC2716 works and whether we can have",
            "                    #   partial state here",
            "                    #   https://github.com/matrix-org/synapse/issues/13003",
            "                    partial_state=False,",
            "                )",
            "            else:",
            "                context = await self.state.compute_event_context(event)",
            "",
            "        if requester:",
            "            context.app_service = requester.app_service",
            "",
            "        res, new_content = await self.third_party_event_rules.check_event_allowed(",
            "            event, context",
            "        )",
            "        if res is False:",
            "            logger.info(",
            "                \"Event %s forbidden by third-party rules\",",
            "                event,",
            "            )",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "        elif new_content is not None:",
            "            # the third-party rules want to replace the event. We'll need to build a new",
            "            # event.",
            "            event, context = await self._rebuild_event_after_third_party_rules(",
            "                new_content, event",
            "            )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "        await self._validate_event_relation(event)",
            "        logger.debug(\"Created event %s\", event.event_id)",
            "",
            "        return event, context",
            "",
            "    async def _validate_event_relation(self, event: EventBase) -> None:",
            "        \"\"\"",
            "        Ensure the relation data on a new event is not bogus.",
            "",
            "        Args:",
            "            event: The event being created.",
            "",
            "        Raises:",
            "            SynapseError if the event is invalid.",
            "        \"\"\"",
            "",
            "        relation = relation_from_event(event)",
            "        if not relation:",
            "            return",
            "",
            "        parent_event = await self.store.get_event(relation.parent_id, allow_none=True)",
            "        if parent_event:",
            "            # And in the same room.",
            "            if parent_event.room_id != event.room_id:",
            "                raise SynapseError(400, \"Relations must be in the same room\")",
            "",
            "        else:",
            "            # There must be some reason that the client knows the event exists,",
            "            # see if there are existing relations. If so, assume everything is fine.",
            "            if not await self.store.event_is_target_of_relation(relation.parent_id):",
            "                # Otherwise, the client can't know about the parent event!",
            "                raise SynapseError(400, \"Can't send relation to unknown event\")",
            "",
            "        # If this event is an annotation then we check that that the sender",
            "        # can't annotate the same way twice (e.g. stops users from liking an",
            "        # event multiple times).",
            "        if relation.rel_type == RelationTypes.ANNOTATION:",
            "            aggregation_key = relation.aggregation_key",
            "",
            "            if aggregation_key is None:",
            "                raise SynapseError(400, \"Missing aggregation key\")",
            "",
            "            if len(aggregation_key) > 500:",
            "                raise SynapseError(400, \"Aggregation key is too long\")",
            "",
            "            already_exists = await self.store.has_user_annotated_event(",
            "                relation.parent_id, event.type, aggregation_key, event.sender",
            "            )",
            "            if already_exists:",
            "                raise SynapseError(400, \"Can't send same reaction twice\")",
            "",
            "        # Don't attempt to start a thread if the parent event is a relation.",
            "        elif relation.rel_type == RelationTypes.THREAD:",
            "            if await self.store.event_includes_relation(relation.parent_id):",
            "                raise SynapseError(",
            "                    400, \"Cannot start threads from an event with a relation\"",
            "                )",
            "",
            "    @measure_func(\"handle_new_client_event\")",
            "    async def handle_new_client_event(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "        ignore_shadow_ban: bool = False,",
            "    ) -> EventBase:",
            "        \"\"\"Processes new events. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        This includes deduplicating, checking auth, persisting,",
            "        notifying users, sending to remote servers, etc.",
            "",
            "        If called from a worker will hit out to the master process for final",
            "        processing.",
            "",
            "        Args:",
            "            requester",
            "            events_and_context: A list of one or more tuples of event, context to be persisted",
            "            ratelimit",
            "            extra_users: Any extra users to notify about event",
            "",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "",
            "        Return:",
            "            If the event was deduplicated, the previous, duplicate, event. Otherwise,",
            "            `event`.",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "            SynapseError(503) if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            # we don't apply shadow-banning to membership events here. Invites are blocked",
            "            # higher up the stack, and we allow shadow-banned users to send join and leave",
            "            # events as normal.",
            "            if (",
            "                event.type != EventTypes.Member",
            "                and not ignore_shadow_ban",
            "                and requester.shadow_banned",
            "            ):",
            "                # We randomly sleep a bit just to annoy the requester.",
            "                await self.clock.sleep(random.randint(1, 10))",
            "                raise ShadowBanError()",
            "",
            "            if event.is_state():",
            "                prev_event = await self.deduplicate_state_event(event, context)",
            "                if prev_event is not None:",
            "                    logger.info(",
            "                        \"Not bothering to persist state event %s duplicated by %s\",",
            "                        event.event_id,",
            "                        prev_event.event_id,",
            "                    )",
            "                    return prev_event",
            "",
            "            if event.internal_metadata.is_out_of_band_membership():",
            "                # the only sort of out-of-band-membership events we expect to see here are",
            "                # invite rejections and rescinded knocks that we have generated ourselves.",
            "                assert event.type == EventTypes.Member",
            "                assert event.content[\"membership\"] == Membership.LEAVE",
            "            else:",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    # If we are persisting a batch of events the event(s) needed to auth the",
            "                    # current event may be part of the batch and will not be in the DB yet",
            "                    event_id_to_event = {e.event_id: e for e, _ in events_and_context}",
            "                    batched_auth_events = {}",
            "                    for event_id in event.auth_event_ids():",
            "                        auth_event = event_id_to_event.get(event_id)",
            "                        if auth_event:",
            "                            batched_auth_events[event_id] = auth_event",
            "                    await self._event_auth_handler.check_auth_rules_from_context(",
            "                        event, batched_auth_events",
            "                    )",
            "                except AuthError as err:",
            "                    logger.warning(\"Denying new event %r because %s\", event, err)",
            "                    raise err",
            "",
            "            # Ensure that we can round trip before trying to persist in db",
            "            try:",
            "                dump = json_encoder.encode(event.content)",
            "                json_decoder.decode(dump)",
            "            except Exception:",
            "                logger.exception(\"Failed to encode content: %r\", event.content)",
            "                raise",
            "",
            "        # We now persist the event (and update the cache in parallel, since we",
            "        # don't want to block on it).",
            "        event, context = events_and_context[0]",
            "        try:",
            "            result, _ = await make_deferred_yieldable(",
            "                gather_results(",
            "                    (",
            "                        run_in_background(",
            "                            self._persist_events,",
            "                            requester=requester,",
            "                            events_and_context=events_and_context,",
            "                            ratelimit=ratelimit,",
            "                            extra_users=extra_users,",
            "                        ),",
            "                        run_in_background(",
            "                            self.cache_joined_hosts_for_events, events_and_context",
            "                        ).addErrback(",
            "                            log_failure, \"cache_joined_hosts_for_event failed\"",
            "                        ),",
            "                    ),",
            "                    consumeErrors=True,",
            "                )",
            "            ).addErrback(unwrapFirstError)",
            "        except PartialStateConflictError as e:",
            "            # The event context needs to be recomputed.",
            "            # Turn the error into a 429, as a hint to the client to try again.",
            "            logger.info(",
            "                \"Room %s was un-partial stated while persisting client event.\",",
            "                event.room_id,",
            "            )",
            "            raise LimitExceededError(msg=e.msg, errcode=e.errcode, retry_after_ms=0)",
            "",
            "        return result",
            "",
            "    async def _persist_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Actually persists new events. Should only be called by",
            "        `handle_new_client_event`, and see its docstring for documentation of",
            "        the arguments. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "",
            "        await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "            events_and_context",
            "        )",
            "",
            "        try:",
            "            # If we're a worker we need to hit out to the master.",
            "            first_event, _ = events_and_context[0]",
            "            writer_instance = self._events_shard_config.get_instance(",
            "                first_event.room_id",
            "            )",
            "            if writer_instance != self._instance_name:",
            "                try:",
            "                    result = await self.send_events(",
            "                        instance_name=writer_instance,",
            "                        events_and_context=events_and_context,",
            "                        store=self.store,",
            "                        requester=requester,",
            "                        ratelimit=ratelimit,",
            "                        extra_users=extra_users,",
            "                    )",
            "                except SynapseError as e:",
            "                    if e.code == HTTPStatus.CONFLICT:",
            "                        raise PartialStateConflictError()",
            "                    raise",
            "                stream_id = result[\"stream_id\"]",
            "                event_id = result[\"event_id\"]",
            "",
            "                # If we batch persisted events we return the last persisted event, otherwise",
            "                # we return the one event that was persisted",
            "                event, _ = events_and_context[-1]",
            "",
            "                if event_id != event.event_id:",
            "                    # If we get a different event back then it means that its",
            "                    # been de-duplicated, so we replace the given event with the",
            "                    # one already persisted.",
            "                    event = await self.store.get_event(event_id)",
            "                else:",
            "                    # If we newly persisted the event then we need to update its",
            "                    # stream_ordering entry manually (as it was persisted on",
            "                    # another worker).",
            "                    event.internal_metadata.stream_ordering = stream_id",
            "                return event",
            "",
            "            event = await self.persist_and_notify_client_events(",
            "                requester,",
            "                events_and_context,",
            "                ratelimit=ratelimit,",
            "                extra_users=extra_users,",
            "            )",
            "",
            "            return event",
            "        except Exception:",
            "            for event, _ in events_and_context:",
            "                # Ensure that we actually remove the entries in the push actions",
            "                # staging area, if we calculated them.",
            "                await self.store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def cache_joined_hosts_for_events(",
            "        self, events_and_context: List[Tuple[EventBase, EventContext]]",
            "    ) -> None:",
            "        \"\"\"Precalculate the joined hosts at each of the given events, when using Redis, so that",
            "        external federation senders don't have to recalculate it themselves.",
            "        \"\"\"",
            "",
            "        for event, _ in events_and_context:",
            "            if not self._external_cache.is_enabled():",
            "                return",
            "",
            "            # If external cache is enabled we should always have this.",
            "            assert self._external_cache_joined_hosts_updates is not None",
            "",
            "            # We actually store two mappings, event ID -> prev state group,",
            "            # state group -> joined hosts, which is much more space efficient",
            "            # than event ID -> joined hosts.",
            "            #",
            "            # Note: We have to cache event ID -> prev state group, as we don't",
            "            # store that in the DB.",
            "            #",
            "            # Note: We set the state group -> joined hosts cache if it hasn't been",
            "            # set for a while, so that the expiry time is reset.",
            "",
            "            state_entry = await self.state.resolve_state_groups_for_events(",
            "                event.room_id, event_ids=event.prev_event_ids()",
            "            )",
            "",
            "            if state_entry.state_group:",
            "                await self._external_cache.set(",
            "                    \"event_to_prev_state_group\",",
            "                    event.event_id,",
            "                    state_entry.state_group,",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                if state_entry.state_group in self._external_cache_joined_hosts_updates:",
            "                    return",
            "",
            "                state = await state_entry.get_state(",
            "                    self._storage_controllers.state, StateFilter.all()",
            "                )",
            "                with opentracing.start_active_span(\"get_joined_hosts\"):",
            "                    joined_hosts = await self.store.get_joined_hosts(",
            "                        event.room_id, state, state_entry",
            "                    )",
            "",
            "                # Note that the expiry times must be larger than the expiry time in",
            "                # _external_cache_joined_hosts_updates.",
            "                await self._external_cache.set(",
            "                    \"get_joined_hosts\",",
            "                    str(state_entry.state_group),",
            "                    list(joined_hosts),",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                self._external_cache_joined_hosts_updates[",
            "                    state_entry.state_group",
            "                ] = None",
            "",
            "    async def _validate_canonical_alias(",
            "        self,",
            "        directory_handler: DirectoryHandler,",
            "        room_alias_str: str,",
            "        expected_room_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Ensure that the given room alias points to the expected room ID.",
            "",
            "        Args:",
            "            directory_handler: The directory handler object.",
            "            room_alias_str: The room alias to check.",
            "            expected_room_id: The room ID that the alias should point to.",
            "        \"\"\"",
            "        room_alias = RoomAlias.from_string(room_alias_str)",
            "        try:",
            "            mapping = await directory_handler.get_association(room_alias)",
            "        except SynapseError as e:",
            "            # Turn M_NOT_FOUND errors into M_BAD_ALIAS errors.",
            "            if e.errcode == Codes.NOT_FOUND:",
            "                raise SynapseError(",
            "                    400,",
            "                    \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                    Codes.BAD_ALIAS,",
            "                )",
            "            raise",
            "",
            "        if mapping[\"room_id\"] != expected_room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                Codes.BAD_ALIAS,",
            "            )",
            "",
            "    async def persist_and_notify_client_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Called when we have fully built the events, have already",
            "        calculated the push actions for the events, and checked auth.",
            "",
            "        This should only be run on the instance in charge of persisting events.",
            "",
            "        Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        Returns:",
            "            The persisted event, if one event is passed in, or the last event in the",
            "            list in the case of batch persisting. If only one event was persisted, the",
            "            returned event may be different than the given event if it was de-duplicated",
            "            (e.g. because we had already persisted an event with the same transaction ID.)",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            assert self._events_shard_config.should_handle(",
            "                self._instance_name, event.room_id",
            "            )",
            "",
            "            if ratelimit:",
            "                # We check if this is a room admin redacting an event so that we",
            "                # can apply different ratelimiting. We do this by simply checking",
            "                # it's not a self-redaction (to avoid having to look up whether the",
            "                # user is actually admin or not).",
            "                is_admin_redaction = False",
            "                if event.type == EventTypes.Redaction:",
            "                    assert event.redacts is not None",
            "",
            "                    original_event = await self.store.get_event(",
            "                        event.redacts,",
            "                        redact_behaviour=EventRedactBehaviour.as_is,",
            "                        get_prev_content=False,",
            "                        allow_rejected=False,",
            "                        allow_none=True,",
            "                    )",
            "",
            "                    is_admin_redaction = bool(",
            "                        original_event and event.sender != original_event.sender",
            "                    )",
            "",
            "                await self.request_ratelimiter.ratelimit(",
            "                    requester, is_admin_redaction=is_admin_redaction",
            "                )",
            "",
            "            # run checks/actions on event based on type",
            "            if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "                (",
            "                    current_membership,",
            "                    _,",
            "                ) = await self.store.get_local_current_membership_for_user_in_room(",
            "                    event.state_key, event.room_id",
            "                )",
            "                if current_membership != Membership.JOIN:",
            "                    self._notifier.notify_user_joined_room(",
            "                        event.event_id, event.room_id",
            "                    )",
            "",
            "            await self._maybe_kick_guest_users(event, context)",
            "",
            "            if event.type == EventTypes.CanonicalAlias:",
            "                # Validate a newly added alias or newly added alt_aliases.",
            "",
            "                original_alias = None",
            "                original_alt_aliases: object = []",
            "",
            "                original_event_id = event.unsigned.get(\"replaces_state\")",
            "                if original_event_id:",
            "                    original_alias_event = await self.store.get_event(original_event_id)",
            "",
            "                    if original_alias_event:",
            "                        original_alias = original_alias_event.content.get(\"alias\", None)",
            "                        original_alt_aliases = original_alias_event.content.get(",
            "                            \"alt_aliases\", []",
            "                        )",
            "",
            "                # Check the alias is currently valid (if it has changed).",
            "                room_alias_str = event.content.get(\"alias\", None)",
            "                directory_handler = self.hs.get_directory_handler()",
            "                if room_alias_str and room_alias_str != original_alias:",
            "                    await self._validate_canonical_alias(",
            "                        directory_handler, room_alias_str, event.room_id",
            "                    )",
            "",
            "                # Check that alt_aliases is the proper form.",
            "                alt_aliases = event.content.get(\"alt_aliases\", [])",
            "                if not isinstance(alt_aliases, (list, tuple)):",
            "                    raise SynapseError(",
            "                        400,",
            "                        \"The alt_aliases property must be a list.\",",
            "                        Codes.INVALID_PARAM,",
            "                    )",
            "",
            "                # If the old version of alt_aliases is of an unknown form,",
            "                # completely replace it.",
            "                if not isinstance(original_alt_aliases, (list, tuple)):",
            "                    # TODO: check that the original_alt_aliases' entries are all strings",
            "                    original_alt_aliases = []",
            "",
            "                # Check that each alias is currently valid.",
            "                new_alt_aliases = set(alt_aliases) - set(original_alt_aliases)",
            "                if new_alt_aliases:",
            "                    for alias_str in new_alt_aliases:",
            "                        await self._validate_canonical_alias(",
            "                            directory_handler, alias_str, event.room_id",
            "                        )",
            "",
            "            federation_handler = self.hs.get_federation_handler()",
            "",
            "            if event.type == EventTypes.Member:",
            "                if event.content[\"membership\"] == Membership.INVITE:",
            "                    event.unsigned[",
            "                        \"invite_room_state\"",
            "                    ] = await self.store.get_stripped_room_state_from_event_context(",
            "                        context,",
            "                        self.room_prejoin_state_types,",
            "                        membership_user_id=event.sender,",
            "                    )",
            "",
            "                    invitee = UserID.from_string(event.state_key)",
            "                    if not self.hs.is_mine(invitee):",
            "                        # TODO: Can we add signature from remote server in a nicer",
            "                        # way? If we have been invited by a remote server, we need",
            "                        # to get them to sign the event.",
            "",
            "                        returned_invite = await federation_handler.send_invite(",
            "                            invitee.domain, event",
            "                        )",
            "                        event.unsigned.pop(\"room_state\", None)",
            "",
            "                        # TODO: Make sure the signatures actually are correct.",
            "                        event.signatures.update(returned_invite.signatures)",
            "",
            "                if event.content[\"membership\"] == Membership.KNOCK:",
            "                    event.unsigned[",
            "                        \"knock_room_state\"",
            "                    ] = await self.store.get_stripped_room_state_from_event_context(",
            "                        context,",
            "                        self.room_prejoin_state_types,",
            "                    )",
            "",
            "            if event.type == EventTypes.Redaction:",
            "                assert event.redacts is not None",
            "",
            "                original_event = await self.store.get_event(",
            "                    event.redacts,",
            "                    redact_behaviour=EventRedactBehaviour.as_is,",
            "                    get_prev_content=False,",
            "                    allow_rejected=False,",
            "                    allow_none=True,",
            "                )",
            "",
            "                room_version = await self.store.get_room_version_id(event.room_id)",
            "                room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "                # we can make some additional checks now if we have the original event.",
            "                if original_event:",
            "                    if original_event.type == EventTypes.Create:",
            "                        raise AuthError(403, \"Redacting create events is not permitted\")",
            "",
            "                    if original_event.room_id != event.room_id:",
            "                        raise SynapseError(",
            "                            400, \"Cannot redact event from a different room\"",
            "                        )",
            "",
            "                    if original_event.type == EventTypes.ServerACL:",
            "                        raise AuthError(",
            "                            403, \"Redacting server ACL events is not permitted\"",
            "                        )",
            "",
            "                    # Add a little safety stop-gap to prevent people from trying to",
            "                    # redact MSC2716 related events when they're in a room version",
            "                    # which does not support it yet. We allow people to use MSC2716",
            "                    # events in existing room versions but only from the room",
            "                    # creator since it does not require any changes to the auth",
            "                    # rules and in effect, the redaction algorithm . In the",
            "                    # supported room version, we add the `historical` power level to",
            "                    # auth the MSC2716 related events and adjust the redaction",
            "                    # algorthim to keep the `historical` field around (redacting an",
            "                    # event should only strip fields which don't affect the",
            "                    # structural protocol level).",
            "                    is_msc2716_event = (",
            "                        original_event.type == EventTypes.MSC2716_INSERTION",
            "                        or original_event.type == EventTypes.MSC2716_BATCH",
            "                        or original_event.type == EventTypes.MSC2716_MARKER",
            "                    )",
            "                    if not room_version_obj.msc2716_historical and is_msc2716_event:",
            "                        raise AuthError(",
            "                            403,",
            "                            \"Redacting MSC2716 events is not supported in this room version\",",
            "                        )",
            "",
            "                event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "                prev_state_ids = await context.get_prev_state_ids(",
            "                    StateFilter.from_types(event_types)",
            "                )",
            "",
            "                auth_events_ids = self._event_auth_handler.compute_auth_events(",
            "                    event, prev_state_ids, for_verification=True",
            "                )",
            "                auth_events_map = await self.store.get_events(auth_events_ids)",
            "                auth_events = {",
            "                    (e.type, e.state_key): e for e in auth_events_map.values()",
            "                }",
            "",
            "                if event_auth.check_redaction(",
            "                    room_version_obj, event, auth_events=auth_events",
            "                ):",
            "                    # this user doesn't have 'redact' rights, so we need to do some more",
            "                    # checks on the original event. Let's start by checking the original",
            "                    # event exists.",
            "                    if not original_event:",
            "                        raise NotFoundError(",
            "                            \"Could not find event %s\" % (event.redacts,)",
            "                        )",
            "",
            "                    if event.user_id != original_event.user_id:",
            "                        raise AuthError(",
            "                            403, \"You don't have permission to redact events\"",
            "                        )",
            "",
            "                    # all the checks are done.",
            "                    event.internal_metadata.recheck_redaction = False",
            "",
            "            if event.type == EventTypes.Create:",
            "                prev_state_ids = await context.get_prev_state_ids()",
            "                if prev_state_ids:",
            "                    raise AuthError(403, \"Changing the room create event is forbidden\")",
            "",
            "            if event.type == EventTypes.MSC2716_INSERTION:",
            "                room_version = await self.store.get_room_version_id(event.room_id)",
            "                room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "                create_event = await self.store.get_create_event_for_room(event.room_id)",
            "                room_creator = create_event.content.get(EventContentFields.ROOM_CREATOR)",
            "",
            "                # Only check an insertion event if the room version",
            "                # supports it or the event is from the room creator.",
            "                if room_version_obj.msc2716_historical or (",
            "                    self.config.experimental.msc2716_enabled",
            "                    and event.sender == room_creator",
            "                ):",
            "                    next_batch_id = event.content.get(",
            "                        EventContentFields.MSC2716_NEXT_BATCH_ID",
            "                    )",
            "                    conflicting_insertion_event_id = None",
            "                    if next_batch_id:",
            "                        conflicting_insertion_event_id = (",
            "                            await self.store.get_insertion_event_id_by_batch_id(",
            "                                event.room_id, next_batch_id",
            "                            )",
            "                        )",
            "                    if conflicting_insertion_event_id is not None:",
            "                        # The current insertion event that we're processing is invalid",
            "                        # because an insertion event already exists in the room with the",
            "                        # same next_batch_id. We can't allow multiple because the batch",
            "                        # pointing will get weird, e.g. we can't determine which insertion",
            "                        # event the batch event is pointing to.",
            "                        raise SynapseError(",
            "                            HTTPStatus.BAD_REQUEST,",
            "                            \"Another insertion event already exists with the same next_batch_id\",",
            "                            errcode=Codes.INVALID_PARAM,",
            "                        )",
            "",
            "            # Mark any `m.historical` messages as backfilled so they don't appear",
            "            # in `/sync` and have the proper decrementing `stream_ordering` as we import",
            "            backfilled = False",
            "            if event.internal_metadata.is_historical():",
            "                backfilled = True",
            "",
            "        assert self._storage_controllers.persistence is not None",
            "        (",
            "            persisted_events,",
            "            max_stream_token,",
            "        ) = await self._storage_controllers.persistence.persist_events(",
            "            events_and_context, backfilled=backfilled",
            "        )",
            "",
            "        events_and_pos = []",
            "        for event in persisted_events:",
            "            if self._ephemeral_events_enabled:",
            "                # If there's an expiry timestamp on the event, schedule its expiry.",
            "                self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            stream_ordering = event.internal_metadata.stream_ordering",
            "            assert stream_ordering is not None",
            "            pos = PersistedEventPosition(self._instance_name, stream_ordering)",
            "            events_and_pos.append((event, pos))",
            "",
            "            if event.type == EventTypes.Message:",
            "                # We don't want to block sending messages on any presence code. This",
            "                # matters as sometimes presence code can take a while.",
            "                run_in_background(self._bump_active_time, requester.user)",
            "",
            "        async def _notify() -> None:",
            "            try:",
            "                await self.notifier.on_new_room_events(",
            "                    events_and_pos, max_stream_token, extra_users=extra_users",
            "                )",
            "            except Exception:",
            "                logger.exception(\"Error notifying about new room events\")",
            "",
            "        run_in_background(_notify)",
            "",
            "        return persisted_events[-1]",
            "",
            "    async def _maybe_kick_guest_users(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state_ids = await context.get_current_state_ids()",
            "",
            "        # since this is a client-generated event, it cannot be an outlier and we must",
            "        # therefore have the state ids.",
            "        assert current_state_ids is not None",
            "        current_state_dict = await self.store.get_events(",
            "            list(current_state_ids.values())",
            "        )",
            "        current_state = list(current_state_dict.values())",
            "        logger.info(\"maybe_kick_guest_users %r\", current_state)",
            "        await self.hs.get_room_member_handler().kick_guest_users(current_state)",
            "",
            "    async def _bump_active_time(self, user: UserID) -> None:",
            "        try:",
            "            presence = self.hs.get_presence_handler()",
            "            await presence.bump_presence_active_time(user)",
            "        except Exception:",
            "            logger.exception(\"Error bumping presence active time\")",
            "",
            "    async def _send_dummy_events_to_fill_extremities(self) -> None:",
            "        \"\"\"Background task to send dummy events into rooms that have a large",
            "        number of extremities",
            "        \"\"\"",
            "        self._expire_rooms_to_exclude_from_dummy_event_insertion()",
            "        room_ids = await self.store.get_rooms_with_many_extremities(",
            "            min_count=self._dummy_events_threshold,",
            "            limit=5,",
            "            room_id_filter=self._rooms_to_exclude_from_dummy_event_insertion.keys(),",
            "        )",
            "",
            "        for room_id in room_ids:",
            "            dummy_event_sent = await self._send_dummy_event_for_room(room_id)",
            "",
            "            if not dummy_event_sent:",
            "                # Did not find a valid user in the room, so remove from future attempts",
            "                # Exclusion is time limited, so the room will be rechecked in the future",
            "                # dependent on _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s. Will exclude it from \"",
            "                    \"future attempts until cache expires\" % (room_id,)",
            "                )",
            "                now = self.clock.time_msec()",
            "                self._rooms_to_exclude_from_dummy_event_insertion[room_id] = now",
            "",
            "    async def _send_dummy_event_for_room(self, room_id: str) -> bool:",
            "        \"\"\"Attempt to send a dummy event for the given room.",
            "",
            "        Args:",
            "            room_id: room to try to send an event from",
            "",
            "        Returns:",
            "            True if a dummy event was successfully sent. False if no user was able",
            "            to send an event.",
            "        \"\"\"",
            "",
            "        # For each room we need to find a joined member we can use to send",
            "        # the dummy event with.",
            "        members = await self.store.get_local_users_in_room(room_id)",
            "        for user_id in members:",
            "            requester = create_requester(user_id, authenticated_entity=self.server_name)",
            "            try:",
            "                event, context = await self.create_event(",
            "                    requester,",
            "                    {",
            "                        \"type\": EventTypes.Dummy,",
            "                        \"content\": {},",
            "                        \"room_id\": room_id,",
            "                        \"sender\": user_id,",
            "                    },",
            "                )",
            "",
            "                event.internal_metadata.proactively_send = False",
            "",
            "                # Since this is a dummy-event it is OK if it is sent by a",
            "                # shadow-banned user.",
            "                await self.handle_new_client_event(",
            "                    requester,",
            "                    events_and_context=[(event, context)],",
            "                    ratelimit=False,",
            "                    ignore_shadow_ban=True,",
            "                )",
            "                return True",
            "            except AuthError:",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s for user %s due to \"",
            "                    \"lack of power. Will try another user\" % (room_id, user_id)",
            "                )",
            "        return False",
            "",
            "    def _expire_rooms_to_exclude_from_dummy_event_insertion(self) -> None:",
            "        expire_before = self.clock.time_msec() - _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "        to_expire = set()",
            "        for room_id, time in self._rooms_to_exclude_from_dummy_event_insertion.items():",
            "            if time < expire_before:",
            "                to_expire.add(room_id)",
            "        for room_id in to_expire:",
            "            logger.debug(",
            "                \"Expiring room id %s from dummy event insertion exclusion cache\",",
            "                room_id,",
            "            )",
            "            del self._rooms_to_exclude_from_dummy_event_insertion[room_id]",
            "",
            "    async def _rebuild_event_after_third_party_rules(",
            "        self, third_party_result: dict, original_event: EventBase",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        # the third_party_event_rules want to replace the event.",
            "        # we do some basic checks, and then return the replacement event and context.",
            "",
            "        # Construct a new EventBuilder and validate it, which helps with the",
            "        # rest of these checks.",
            "        try:",
            "            builder = self.event_builder_factory.for_room_version(",
            "                original_event.room_version, third_party_result",
            "            )",
            "            self.validator.validate_builder(builder)",
            "        except SynapseError as e:",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \" + e.msg,",
            "            )",
            "",
            "        immutable_fields = [",
            "            # changing the room is going to break things: we've already checked that the",
            "            # room exists, and are holding a concurrency limiter token for that room.",
            "            # Also, we might need to use a different room version.",
            "            \"room_id\",",
            "            # changing the type or state key might work, but we'd need to check that the",
            "            # calling functions aren't making assumptions about them.",
            "            \"type\",",
            "            \"state_key\",",
            "        ]",
            "",
            "        for k in immutable_fields:",
            "            if getattr(builder, k, None) != original_event.get(k):",
            "                raise Exception(",
            "                    \"Third party rules module created an invalid event: \"",
            "                    \"cannot change field \" + k",
            "                )",
            "",
            "        # check that the new sender belongs to this HS",
            "        if not self.hs.is_mine_id(builder.sender):",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \"",
            "                \"invalid sender \" + builder.sender",
            "            )",
            "",
            "        # copy over the original internal metadata",
            "        for k, v in original_event.internal_metadata.get_dict().items():",
            "            setattr(builder.internal_metadata, k, v)",
            "",
            "        # modules can send new state events, so we re-calculate the auth events just in",
            "        # case.",
            "        prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        event = await builder.build(",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=None,",
            "        )",
            "",
            "        # we rebuild the event context, to be on the safe side. If nothing else,",
            "        # delta_ids might need an update.",
            "        context = await self.state.compute_event_context(event)",
            "        return event, context"
        ],
        "afterPatchFile": [
            "# Copyright 2014-2016 OpenMarket Ltd",
            "# Copyright 2017-2018 New Vector Ltd",
            "# Copyright 2019-2020 The Matrix.org Foundation C.I.C.",
            "# Copyrignt 2020 Sorunome",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "import logging",
            "import random",
            "from http import HTTPStatus",
            "from typing import TYPE_CHECKING, Any, Dict, List, Mapping, Optional, Tuple",
            "",
            "from canonicaljson import encode_canonical_json",
            "",
            "from twisted.internet.interfaces import IDelayedCall",
            "",
            "from synapse import event_auth",
            "from synapse.api.constants import (",
            "    EventContentFields,",
            "    EventTypes,",
            "    GuestAccess,",
            "    HistoryVisibility,",
            "    Membership,",
            "    RelationTypes,",
            "    UserTypes,",
            ")",
            "from synapse.api.errors import (",
            "    AuthError,",
            "    Codes,",
            "    ConsentNotGivenError,",
            "    LimitExceededError,",
            "    NotFoundError,",
            "    ShadowBanError,",
            "    SynapseError,",
            "    UnstableSpecAuthError,",
            "    UnsupportedRoomVersionError,",
            ")",
            "from synapse.api.room_versions import KNOWN_ROOM_VERSIONS",
            "from synapse.api.urls import ConsentURIBuilder",
            "from synapse.event_auth import validate_event_for_room_version",
            "from synapse.events import EventBase, relation_from_event",
            "from synapse.events.builder import EventBuilder",
            "from synapse.events.snapshot import EventContext",
            "from synapse.events.utils import maybe_upsert_event_field",
            "from synapse.events.validator import EventValidator",
            "from synapse.handlers.directory import DirectoryHandler",
            "from synapse.logging import opentracing",
            "from synapse.logging.context import make_deferred_yieldable, run_in_background",
            "from synapse.metrics.background_process_metrics import run_as_background_process",
            "from synapse.replication.http.send_event import ReplicationSendEventRestServlet",
            "from synapse.replication.http.send_events import ReplicationSendEventsRestServlet",
            "from synapse.storage.databases.main.events import PartialStateConflictError",
            "from synapse.storage.databases.main.events_worker import EventRedactBehaviour",
            "from synapse.types import (",
            "    MutableStateMap,",
            "    PersistedEventPosition,",
            "    Requester,",
            "    RoomAlias,",
            "    StateMap,",
            "    StreamToken,",
            "    UserID,",
            "    create_requester,",
            ")",
            "from synapse.types.state import StateFilter",
            "from synapse.util import json_decoder, json_encoder, log_failure, unwrapFirstError",
            "from synapse.util.async_helpers import Linearizer, gather_results",
            "from synapse.util.caches.expiringcache import ExpiringCache",
            "from synapse.util.metrics import measure_func",
            "from synapse.visibility import get_effective_room_visibility_from_state",
            "",
            "if TYPE_CHECKING:",
            "    from synapse.events.third_party_rules import ThirdPartyEventRules",
            "    from synapse.server import HomeServer",
            "",
            "logger = logging.getLogger(__name__)",
            "",
            "",
            "class MessageHandler:",
            "    \"\"\"Contains some read only APIs to get state about a room\"\"\"",
            "",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.auth = hs.get_auth()",
            "        self.clock = hs.get_clock()",
            "        self.state = hs.get_state_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self._state_storage_controller = self._storage_controllers.state",
            "        self._event_serializer = hs.get_event_client_serializer()",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        # The scheduled call to self._expire_event. None if no call is currently",
            "        # scheduled.",
            "        self._scheduled_expiry: Optional[IDelayedCall] = None",
            "",
            "        if not hs.config.worker.worker_app:",
            "            run_as_background_process(",
            "                \"_schedule_next_expiry\", self._schedule_next_expiry",
            "            )",
            "",
            "    async def get_room_data(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        event_type: str,",
            "        state_key: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"Get data from a room.",
            "",
            "        Args:",
            "            requester: The user who did the request.",
            "            room_id",
            "            event_type",
            "            state_key",
            "        Returns:",
            "            The path data content.",
            "        Raises:",
            "            SynapseError or AuthError if the user is not in the room",
            "        \"\"\"",
            "        (",
            "            membership,",
            "            membership_event_id,",
            "        ) = await self.auth.check_user_in_room_or_world_readable(",
            "            room_id, requester, allow_departed_users=True",
            "        )",
            "",
            "        if membership == Membership.JOIN:",
            "            data = await self._storage_controllers.state.get_current_state_event(",
            "                room_id, event_type, state_key",
            "            )",
            "        elif membership == Membership.LEAVE:",
            "            key = (event_type, state_key)",
            "            # If the membership is not JOIN, then the event ID should exist.",
            "            assert (",
            "                membership_event_id is not None",
            "            ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "            room_state = await self._state_storage_controller.get_state_for_events(",
            "                [membership_event_id], StateFilter.from_types([key])",
            "            )",
            "            data = room_state[membership_event_id].get(key)",
            "        else:",
            "            # check_user_in_room_or_world_readable, if it doesn't raise an AuthError, should",
            "            # only ever return a Membership.JOIN/LEAVE object",
            "            #",
            "            # Safeguard in case it returned something else",
            "            logger.error(",
            "                \"Attempted to retrieve data from a room for a user that has never been in it. \"",
            "                \"This should not have happened.\"",
            "            )",
            "            raise UnstableSpecAuthError(",
            "                403,",
            "                \"User not in room\",",
            "                errcode=Codes.NOT_JOINED,",
            "            )",
            "",
            "        return data",
            "",
            "    async def get_state_events(",
            "        self,",
            "        requester: Requester,",
            "        room_id: str,",
            "        state_filter: Optional[StateFilter] = None,",
            "        at_token: Optional[StreamToken] = None,",
            "    ) -> List[dict]:",
            "        \"\"\"Retrieve all state events for a given room. If the user is",
            "        joined to the room then return the current state. If the user has",
            "        left the room return the state events from when they left. If an explicit",
            "        'at' parameter is passed, return the state events as of that event, if",
            "        visible.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "            state_filter: The state filter used to fetch state from the database.",
            "            at_token: the stream token of the at which we are requesting",
            "                the stats. If the user is not allowed to view the state as of that",
            "                stream token, we raise a 403 SynapseError. If None, returns the current",
            "                state based on the current_state_events table.",
            "        Returns:",
            "            A list of dicts representing state events. [{}, {}, {}]",
            "        Raises:",
            "            NotFoundError (404) if the at token does not yield an event",
            "",
            "            AuthError (403) if the user doesn't have permission to view",
            "            members of this room.",
            "        \"\"\"",
            "        state_filter = state_filter or StateFilter.all()",
            "        user_id = requester.user.to_string()",
            "",
            "        if at_token:",
            "            last_event_id = (",
            "                await self.store.get_last_event_in_room_before_stream_ordering(",
            "                    room_id,",
            "                    end_token=at_token.room_key,",
            "                )",
            "            )",
            "",
            "            if not last_event_id:",
            "                raise NotFoundError(\"Can't find event for token %s\" % (at_token,))",
            "",
            "            if not await self._user_can_see_state_at_event(",
            "                user_id, room_id, last_event_id",
            "            ):",
            "                raise AuthError(",
            "                    403,",
            "                    \"User %s not allowed to view events in room %s at token %s\"",
            "                    % (user_id, room_id, at_token),",
            "                )",
            "",
            "            room_state_events = (",
            "                await self._state_storage_controller.get_state_for_events(",
            "                    [last_event_id], state_filter=state_filter",
            "                )",
            "            )",
            "            room_state: Mapping[Any, EventBase] = room_state_events[last_event_id]",
            "        else:",
            "            (",
            "                membership,",
            "                membership_event_id,",
            "            ) = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "",
            "            if membership == Membership.JOIN:",
            "                state_ids = await self._state_storage_controller.get_current_state_ids(",
            "                    room_id, state_filter=state_filter",
            "                )",
            "                room_state = await self.store.get_events(state_ids.values())",
            "            elif membership == Membership.LEAVE:",
            "                # If the membership is not JOIN, then the event ID should exist.",
            "                assert (",
            "                    membership_event_id is not None",
            "                ), \"check_user_in_room_or_world_readable returned invalid data\"",
            "                room_state_events = (",
            "                    await self._state_storage_controller.get_state_for_events(",
            "                        [membership_event_id], state_filter=state_filter",
            "                    )",
            "                )",
            "                room_state = room_state_events[membership_event_id]",
            "",
            "        now = self.clock.time_msec()",
            "        events = self._event_serializer.serialize_events(room_state.values(), now)",
            "        return events",
            "",
            "    async def _user_can_see_state_at_event(",
            "        self, user_id: str, room_id: str, event_id: str",
            "    ) -> bool:",
            "        # check whether the user was in the room, and the history visibility,",
            "        # at that time.",
            "        state_map = await self._state_storage_controller.get_state_for_event(",
            "            event_id,",
            "            StateFilter.from_types(",
            "                [",
            "                    (EventTypes.Member, user_id),",
            "                    (EventTypes.RoomHistoryVisibility, \"\"),",
            "                ]",
            "            ),",
            "        )",
            "",
            "        membership = None",
            "        membership_event = state_map.get((EventTypes.Member, user_id))",
            "        if membership_event:",
            "            membership = membership_event.membership",
            "",
            "        # if the user was a member of the room at the time of the event,",
            "        # they can see it.",
            "        if membership == Membership.JOIN:",
            "            return True",
            "",
            "        # otherwise, it depends on the history visibility.",
            "        visibility = get_effective_room_visibility_from_state(state_map)",
            "",
            "        if visibility == HistoryVisibility.JOINED:",
            "            # we weren't a member at the time of the event, so we can't see this event.",
            "            return False",
            "",
            "        # otherwise *invited* is good enough",
            "        if membership == Membership.INVITE:",
            "            return True",
            "",
            "        if visibility == HistoryVisibility.INVITED:",
            "            # we weren't invited, so we can't see this event.",
            "            return False",
            "",
            "        if visibility == HistoryVisibility.WORLD_READABLE:",
            "            return True",
            "",
            "        # So it's SHARED, and the user was not a member at the time. The user cannot",
            "        # see history, unless they have *subsequently* joined the room.",
            "        #",
            "        # XXX: if the user has subsequently joined and then left again,",
            "        # ideally we would share history up to the point they left. But",
            "        # we don't know when they left. We just treat it as though they",
            "        # never joined, and restrict access.",
            "",
            "        (",
            "            current_membership,",
            "            _,",
            "        ) = await self.store.get_local_current_membership_for_user_in_room(",
            "            user_id, event_id",
            "        )",
            "        return current_membership == Membership.JOIN",
            "",
            "    async def get_joined_members(self, requester: Requester, room_id: str) -> dict:",
            "        \"\"\"Get all the joined members in the room and their profile information.",
            "",
            "        If the user has left the room return the state events from when they left.",
            "",
            "        Args:",
            "            requester: The user requesting state events.",
            "            room_id: The room ID to get all state events from.",
            "        Returns:",
            "            A dict of user_id to profile info",
            "        \"\"\"",
            "        if not requester.app_service:",
            "            # We check AS auth after fetching the room membership, as it",
            "            # requires us to pull out all joined members anyway.",
            "            membership, _ = await self.auth.check_user_in_room_or_world_readable(",
            "                room_id, requester, allow_departed_users=True",
            "            )",
            "            if membership != Membership.JOIN:",
            "                raise SynapseError(",
            "                    code=403,",
            "                    errcode=Codes.FORBIDDEN,",
            "                    msg=\"Getting joined members while not being a current member of the room is forbidden.\",",
            "                )",
            "",
            "        users_with_profile = (",
            "            await self._state_storage_controller.get_users_in_room_with_profiles(",
            "                room_id",
            "            )",
            "        )",
            "",
            "        # If this is an AS, double check that they are allowed to see the members.",
            "        # This can either be because the AS user is in the room or because there",
            "        # is a user in the room that the AS is \"interested in\"",
            "        if (",
            "            requester.app_service",
            "            and requester.user.to_string() not in users_with_profile",
            "        ):",
            "            for uid in users_with_profile:",
            "                if requester.app_service.is_interested_in_user(uid):",
            "                    break",
            "            else:",
            "                # Loop fell through, AS has no interested users in room",
            "                raise UnstableSpecAuthError(",
            "                    403,",
            "                    \"Appservice not in room\",",
            "                    errcode=Codes.NOT_JOINED,",
            "                )",
            "",
            "        return {",
            "            user_id: {",
            "                \"avatar_url\": profile.avatar_url,",
            "                \"display_name\": profile.display_name,",
            "            }",
            "            for user_id, profile in users_with_profile.items()",
            "        }",
            "",
            "    def maybe_schedule_expiry(self, event: EventBase) -> None:",
            "        \"\"\"Schedule the expiry of an event if there's not already one scheduled,",
            "        or if the one running is for an event that will expire after the provided",
            "        timestamp.",
            "",
            "        This function needs to invalidate the event cache, which is only possible on",
            "        the master process, and therefore needs to be run on there.",
            "",
            "        Args:",
            "            event: The event to schedule the expiry of.",
            "        \"\"\"",
            "",
            "        expiry_ts = event.content.get(EventContentFields.SELF_DESTRUCT_AFTER)",
            "        if not isinstance(expiry_ts, int) or event.is_state():",
            "            return",
            "",
            "        # _schedule_expiry_for_event won't actually schedule anything if there's already",
            "        # a task scheduled for a timestamp that's sooner than the provided one.",
            "        self._schedule_expiry_for_event(event.event_id, expiry_ts)",
            "",
            "    async def _schedule_next_expiry(self) -> None:",
            "        \"\"\"Retrieve the ID and the expiry timestamp of the next event to be expired,",
            "        and schedule an expiry task for it.",
            "",
            "        If there's no event left to expire, set _expiry_scheduled to None so that a",
            "        future call to save_expiry_ts can schedule a new expiry task.",
            "        \"\"\"",
            "        # Try to get the expiry timestamp of the next event to expire.",
            "        res = await self.store.get_next_event_to_expire()",
            "        if res:",
            "            event_id, expiry_ts = res",
            "            self._schedule_expiry_for_event(event_id, expiry_ts)",
            "",
            "    def _schedule_expiry_for_event(self, event_id: str, expiry_ts: int) -> None:",
            "        \"\"\"Schedule an expiry task for the provided event if there's not already one",
            "        scheduled at a timestamp that's sooner than the provided one.",
            "",
            "        Args:",
            "            event_id: The ID of the event to expire.",
            "            expiry_ts: The timestamp at which to expire the event.",
            "        \"\"\"",
            "        if self._scheduled_expiry:",
            "            # If the provided timestamp refers to a time before the scheduled time of the",
            "            # next expiry task, cancel that task and reschedule it for this timestamp.",
            "            next_scheduled_expiry_ts = self._scheduled_expiry.getTime() * 1000",
            "            if expiry_ts < next_scheduled_expiry_ts:",
            "                self._scheduled_expiry.cancel()",
            "            else:",
            "                return",
            "",
            "        # Figure out how many seconds we need to wait before expiring the event.",
            "        now_ms = self.clock.time_msec()",
            "        delay = (expiry_ts - now_ms) / 1000",
            "",
            "        # callLater doesn't support negative delays, so trim the delay to 0 if we're",
            "        # in that case.",
            "        if delay < 0:",
            "            delay = 0",
            "",
            "        logger.info(\"Scheduling expiry for event %s in %.3fs\", event_id, delay)",
            "",
            "        self._scheduled_expiry = self.clock.call_later(",
            "            delay,",
            "            run_as_background_process,",
            "            \"_expire_event\",",
            "            self._expire_event,",
            "            event_id,",
            "        )",
            "",
            "    async def _expire_event(self, event_id: str) -> None:",
            "        \"\"\"Retrieve and expire an event that needs to be expired from the database.",
            "",
            "        If the event doesn't exist in the database, log it and delete the expiry date",
            "        from the database (so that we don't try to expire it again).",
            "        \"\"\"",
            "        assert self._ephemeral_events_enabled",
            "",
            "        self._scheduled_expiry = None",
            "",
            "        logger.info(\"Expiring event %s\", event_id)",
            "",
            "        try:",
            "            # Expire the event if we know about it. This function also deletes the expiry",
            "            # date from the database in the same database transaction.",
            "            await self.store.expire_event(event_id)",
            "        except Exception as e:",
            "            logger.error(\"Could not expire event %s: %r\", event_id, e)",
            "",
            "        # Schedule the expiry of the next event to expire.",
            "        await self._schedule_next_expiry()",
            "",
            "",
            "# The duration (in ms) after which rooms should be removed",
            "# `_rooms_to_exclude_from_dummy_event_insertion` (with the effect that we will try",
            "# to generate a dummy event for them once more)",
            "#",
            "_DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY = 7 * 24 * 60 * 60 * 1000",
            "",
            "",
            "class EventCreationHandler:",
            "    def __init__(self, hs: \"HomeServer\"):",
            "        self.hs = hs",
            "        self.auth_blocking = hs.get_auth_blocking()",
            "        self._event_auth_handler = hs.get_event_auth_handler()",
            "        self.store = hs.get_datastores().main",
            "        self._storage_controllers = hs.get_storage_controllers()",
            "        self.state = hs.get_state_handler()",
            "        self.clock = hs.get_clock()",
            "        self.validator = EventValidator()",
            "        self.profile_handler = hs.get_profile_handler()",
            "        self.event_builder_factory = hs.get_event_builder_factory()",
            "        self.server_name = hs.hostname",
            "        self.notifier = hs.get_notifier()",
            "        self.config = hs.config",
            "        self.require_membership_for_aliases = (",
            "            hs.config.server.require_membership_for_aliases",
            "        )",
            "        self._events_shard_config = self.config.worker.events_shard_config",
            "        self._instance_name = hs.get_instance_name()",
            "        self._notifier = hs.get_notifier()",
            "",
            "        self.room_prejoin_state_types = self.hs.config.api.room_prejoin_state",
            "",
            "        self.membership_types_to_include_profile_data_in = {",
            "            Membership.JOIN,",
            "            Membership.KNOCK,",
            "        }",
            "        if self.hs.config.server.include_profile_data_on_invite:",
            "            self.membership_types_to_include_profile_data_in.add(Membership.INVITE)",
            "",
            "        self.send_event = ReplicationSendEventRestServlet.make_client(hs)",
            "        self.send_events = ReplicationSendEventsRestServlet.make_client(hs)",
            "",
            "        self.request_ratelimiter = hs.get_request_ratelimiter()",
            "",
            "        # We arbitrarily limit concurrent event creation for a room to 5.",
            "        # This is to stop us from diverging history *too* much.",
            "        self.limiter = Linearizer(max_count=5, name=\"room_event_creation_limit\")",
            "",
            "        self._bulk_push_rule_evaluator = hs.get_bulk_push_rule_evaluator()",
            "",
            "        self.spam_checker = hs.get_spam_checker()",
            "        self.third_party_event_rules: \"ThirdPartyEventRules\" = (",
            "            self.hs.get_third_party_event_rules()",
            "        )",
            "",
            "        self._block_events_without_consent_error = (",
            "            self.config.consent.block_events_without_consent_error",
            "        )",
            "",
            "        # we need to construct a ConsentURIBuilder here, as it checks that the necessary",
            "        # config options, but *only* if we have a configuration for which we are",
            "        # going to need it.",
            "        if self._block_events_without_consent_error:",
            "            self._consent_uri_builder = ConsentURIBuilder(self.config)",
            "",
            "        # Rooms which should be excluded from dummy insertion. (For instance,",
            "        # those without local users who can send events into the room).",
            "        #",
            "        # map from room id to time-of-last-attempt.",
            "        #",
            "        self._rooms_to_exclude_from_dummy_event_insertion: Dict[str, int] = {}",
            "        # The number of forward extremeities before a dummy event is sent.",
            "        self._dummy_events_threshold = hs.config.server.dummy_events_threshold",
            "",
            "        if (",
            "            self.config.worker.run_background_tasks",
            "            and self.config.server.cleanup_extremities_with_dummy_events",
            "        ):",
            "            self.clock.looping_call(",
            "                lambda: run_as_background_process(",
            "                    \"send_dummy_events_to_fill_extremities\",",
            "                    self._send_dummy_events_to_fill_extremities,",
            "                ),",
            "                5 * 60 * 1000,",
            "            )",
            "",
            "        self._message_handler = hs.get_message_handler()",
            "",
            "        self._ephemeral_events_enabled = hs.config.server.enable_ephemeral_messages",
            "",
            "        self._external_cache = hs.get_external_cache()",
            "",
            "        # Stores the state groups we've recently added to the joined hosts",
            "        # external cache. Note that the timeout must be significantly less than",
            "        # the TTL on the external cache.",
            "        self._external_cache_joined_hosts_updates: Optional[ExpiringCache] = None",
            "        if self._external_cache.is_enabled():",
            "            self._external_cache_joined_hosts_updates = ExpiringCache(",
            "                \"_external_cache_joined_hosts_updates\",",
            "                self.clock,",
            "                expiry_ms=30 * 60 * 1000,",
            "            )",
            "",
            "    async def create_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        txn_id: Optional[str] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        require_consent: bool = True,",
            "        outlier: bool = False,",
            "        historical: bool = False,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"",
            "        Given a dict from a client, create a new event. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events.",
            "",
            "        Creates an FrozenEvent object, filling out auth_events, prev_events,",
            "        etc.",
            "",
            "        Adds display names to Join membership events.",
            "",
            "        Args:",
            "            requester",
            "            event_dict: An entire event",
            "            txn_id",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases like MSC2716.",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "                If non-None, prev_event_ids must also be provided.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This is used particularly by the MSC2716",
            "                /batch_send endpoint. One use case is with insertion events which float at",
            "                the beginning of a historical batch and don't have any `prev_events` to",
            "                derive from; we add all of these state events as the explicit state so the",
            "                rest of the historical batch can inherit the same state and state_group.",
            "                This should normally be left as None, which will cause the auth_event_ids",
            "                to be calculated based on the room state at the prev_events.",
            "",
            "            require_consent: Whether to check if the requester has",
            "                consented to the privacy policy.",
            "",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "",
            "            historical: Indicates whether the message is being inserted",
            "                back in time around some existing events. This is used to skip",
            "                a few checks and mark the event as backfilled.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Raises:",
            "            ResourceLimitError if server is blocked to some resource being",
            "            exceeded",
            "",
            "        Returns:",
            "            Tuple of created event, Context",
            "        \"\"\"",
            "        await self.auth_blocking.check_auth_blocking(requester=requester)",
            "",
            "        if event_dict[\"type\"] == EventTypes.Create and event_dict[\"state_key\"] == \"\":",
            "            room_version_id = event_dict[\"content\"][\"room_version\"]",
            "            maybe_room_version_obj = KNOWN_ROOM_VERSIONS.get(room_version_id)",
            "            if not maybe_room_version_obj:",
            "                # this can happen if support is withdrawn for a room version",
            "                raise UnsupportedRoomVersionError(room_version_id)",
            "            room_version_obj = maybe_room_version_obj",
            "        else:",
            "            try:",
            "                room_version_obj = await self.store.get_room_version(",
            "                    event_dict[\"room_id\"]",
            "                )",
            "            except NotFoundError:",
            "                raise AuthError(403, \"Unknown room\")",
            "",
            "        builder = self.event_builder_factory.for_room_version(",
            "            room_version_obj, event_dict",
            "        )",
            "",
            "        self.validator.validate_builder(builder)",
            "",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            target = UserID.from_string(builder.state_key)",
            "",
            "            if membership in self.membership_types_to_include_profile_data_in:",
            "                # If event doesn't include a display name, add one.",
            "                profile = self.profile_handler",
            "                content = builder.content",
            "",
            "                try:",
            "                    if \"displayname\" not in content:",
            "                        displayname = await profile.get_displayname(target)",
            "                        if displayname is not None:",
            "                            content[\"displayname\"] = displayname",
            "                    if \"avatar_url\" not in content:",
            "                        avatar_url = await profile.get_avatar_url(target)",
            "                        if avatar_url is not None:",
            "                            content[\"avatar_url\"] = avatar_url",
            "                except Exception as e:",
            "                    logger.info(",
            "                        \"Failed to get profile information for %r: %s\", target, e",
            "                    )",
            "",
            "        is_exempt = await self._is_exempt_from_privacy_policy(builder, requester)",
            "        if require_consent and not is_exempt:",
            "            await self.assert_accepted_privacy_policy(requester)",
            "",
            "        if requester.access_token_id is not None:",
            "            builder.internal_metadata.token_id = requester.access_token_id",
            "",
            "        if txn_id is not None:",
            "            builder.internal_metadata.txn_id = txn_id",
            "",
            "        builder.internal_metadata.outlier = outlier",
            "",
            "        builder.internal_metadata.historical = historical",
            "",
            "        event, context = await self.create_new_client_event(",
            "            builder=builder,",
            "            requester=requester,",
            "            allow_no_prev_events=allow_no_prev_events,",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=auth_event_ids,",
            "            state_event_ids=state_event_ids,",
            "            depth=depth,",
            "            state_map=state_map,",
            "            for_batch=for_batch,",
            "            current_state_group=current_state_group,",
            "        )",
            "",
            "        # In an ideal world we wouldn't need the second part of this condition. However,",
            "        # this behaviour isn't spec'd yet, meaning we should be able to deactivate this",
            "        # behaviour. Another reason is that this code is also evaluated each time a new",
            "        # m.room.aliases event is created, which includes hitting a /directory route.",
            "        # Therefore not including this condition here would render the similar one in",
            "        # synapse.handlers.directory pointless.",
            "        if builder.type == EventTypes.Aliases and self.require_membership_for_aliases:",
            "            # Ideally we'd do the membership check in event_auth.check(), which",
            "            # describes a spec'd algorithm for authenticating events received over",
            "            # federation as well as those created locally. As of room v3, aliases events",
            "            # can be created by users that are not in the room, therefore we have to",
            "            # tolerate them in event_auth.check().",
            "            if for_batch:",
            "                assert state_map is not None",
            "                prev_event_id = state_map.get((EventTypes.Member, event.sender))",
            "            else:",
            "                prev_state_ids = await context.get_prev_state_ids(",
            "                    StateFilter.from_types([(EventTypes.Member, None)])",
            "                )",
            "                prev_event_id = prev_state_ids.get((EventTypes.Member, event.sender))",
            "            prev_event = (",
            "                await self.store.get_event(prev_event_id, allow_none=True)",
            "                if prev_event_id",
            "                else None",
            "            )",
            "            if not prev_event or prev_event.membership != Membership.JOIN:",
            "                logger.warning(",
            "                    (",
            "                        \"Attempt to send `m.room.aliases` in room %s by user %s but\"",
            "                        \" membership is %s\"",
            "                    ),",
            "                    event.room_id,",
            "                    event.sender,",
            "                    prev_event.membership if prev_event else None,",
            "                )",
            "",
            "                raise AuthError(",
            "                    403, \"You must be in the room to create an alias for it\"",
            "                )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "",
            "        return event, context",
            "",
            "    async def _is_exempt_from_privacy_policy(",
            "        self, builder: EventBuilder, requester: Requester",
            "    ) -> bool:",
            "        \"\"\" \"Determine if an event to be sent is exempt from having to consent",
            "        to the privacy policy",
            "",
            "        Args:",
            "            builder: event being created",
            "            requester: user requesting this event",
            "",
            "        Returns:",
            "            true if the event can be sent without the user consenting",
            "        \"\"\"",
            "        # the only thing the user can do is join the server notices room.",
            "        if builder.type == EventTypes.Member:",
            "            membership = builder.content.get(\"membership\", None)",
            "            if membership == Membership.JOIN:",
            "                return await self.store.is_server_notice_room(builder.room_id)",
            "            elif membership == Membership.LEAVE:",
            "                # the user is always allowed to leave (but not kick people)",
            "                return builder.state_key == requester.user.to_string()",
            "        return False",
            "",
            "    async def assert_accepted_privacy_policy(self, requester: Requester) -> None:",
            "        \"\"\"Check if a user has accepted the privacy policy",
            "",
            "        Called when the given user is about to do something that requires",
            "        privacy consent. We see if the user is exempt and otherwise check that",
            "        they have given consent. If they have not, a ConsentNotGiven error is",
            "        raised.",
            "",
            "        Args:",
            "            requester: The user making the request",
            "",
            "        Returns:",
            "            Returns normally if the user has consented or is exempt",
            "",
            "        Raises:",
            "            ConsentNotGivenError: if the user has not given consent yet",
            "        \"\"\"",
            "        if self._block_events_without_consent_error is None:",
            "            return",
            "",
            "        # exempt AS users from needing consent",
            "        if requester.app_service is not None:",
            "            return",
            "",
            "        user_id = requester.authenticated_entity",
            "        if not user_id.startswith(\"@\"):",
            "            # The authenticated entity might not be a user, e.g. if it's the",
            "            # server puppetting the user.",
            "            return",
            "",
            "        user = UserID.from_string(user_id)",
            "",
            "        # exempt the system notices user",
            "        if (",
            "            self.config.servernotices.server_notices_mxid is not None",
            "            and user_id == self.config.servernotices.server_notices_mxid",
            "        ):",
            "            return",
            "",
            "        u = await self.store.get_user_by_id(user_id)",
            "        assert u is not None",
            "        if u[\"user_type\"] in (UserTypes.SUPPORT, UserTypes.BOT):",
            "            # support and bot users are not required to consent",
            "            return",
            "        if u[\"appservice_id\"] is not None:",
            "            # users registered by an appservice are exempt",
            "            return",
            "        if u[\"consent_version\"] == self.config.consent.user_consent_version:",
            "            return",
            "",
            "        consent_uri = self._consent_uri_builder.build_user_consent_uri(user.localpart)",
            "        msg = self._block_events_without_consent_error % {\"consent_uri\": consent_uri}",
            "        raise ConsentNotGivenError(msg=msg, consent_uri=consent_uri)",
            "",
            "    async def deduplicate_state_event(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"",
            "        Checks whether event is in the latest resolved state in context.",
            "",
            "        Args:",
            "            event: The event to check for duplication.",
            "            context: The event context.",
            "",
            "        Returns:",
            "            The previous version of the event is returned, if it is found in the",
            "            event context. Otherwise, None is returned.",
            "        \"\"\"",
            "        if event.internal_metadata.is_outlier():",
            "            # This can happen due to out of band memberships",
            "            return None",
            "",
            "        prev_state_ids = await context.get_prev_state_ids(",
            "            StateFilter.from_types([(event.type, None)])",
            "        )",
            "        prev_event_id = prev_state_ids.get((event.type, event.state_key))",
            "        if not prev_event_id:",
            "            return None",
            "        prev_event = await self.store.get_event(prev_event_id, allow_none=True)",
            "        if not prev_event:",
            "            return None",
            "",
            "        if prev_event and event.user_id == prev_event.user_id:",
            "            prev_content = encode_canonical_json(prev_event.content)",
            "            next_content = encode_canonical_json(event.content)",
            "            if prev_content == next_content:",
            "                return prev_event",
            "        return None",
            "",
            "    async def get_event_from_transaction(",
            "        self,",
            "        requester: Requester,",
            "        txn_id: str,",
            "        room_id: str,",
            "    ) -> Optional[EventBase]:",
            "        \"\"\"For the given transaction ID and room ID, check if there is a matching event.",
            "        If so, fetch it and return it.",
            "",
            "        Args:",
            "            requester: The requester making the request in the context of which we want",
            "                to fetch the event.",
            "            txn_id: The transaction ID.",
            "            room_id: The room ID.",
            "",
            "        Returns:",
            "            An event if one could be found, None otherwise.",
            "        \"\"\"",
            "        if requester.access_token_id:",
            "            existing_event_id = await self.store.get_event_id_from_transaction_id(",
            "                room_id,",
            "                requester.user.to_string(),",
            "                requester.access_token_id,",
            "                txn_id,",
            "            )",
            "            if existing_event_id:",
            "                return await self.store.get_event(existing_event_id)",
            "",
            "        return None",
            "",
            "    async def create_and_send_nonmember_event(",
            "        self,",
            "        requester: Requester,",
            "        event_dict: dict,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        ratelimit: bool = True,",
            "        txn_id: Optional[str] = None,",
            "        ignore_shadow_ban: bool = False,",
            "        outlier: bool = False,",
            "        historical: bool = False,",
            "        depth: Optional[int] = None,",
            "    ) -> Tuple[EventBase, int]:",
            "        \"\"\"",
            "        Creates an event, then sends it.",
            "",
            "        See self.create_event and self.handle_new_client_event.",
            "",
            "        Args:",
            "            requester: The requester sending the event.",
            "            event_dict: An entire event.",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases like MSC2716.",
            "            prev_event_ids:",
            "                The event IDs to use as the prev events.",
            "                Should normally be left as None to automatically request them",
            "                from the database.",
            "            state_event_ids:",
            "                The full state at a given event. This is used particularly by the MSC2716",
            "                /batch_send endpoint. One use case is with insertion events which float at",
            "                the beginning of a historical batch and don't have any `prev_events` to",
            "                derive from; we add all of these state events as the explicit state so the",
            "                rest of the historical batch can inherit the same state and state_group.",
            "                This should normally be left as None, which will cause the auth_event_ids",
            "                to be calculated based on the room state at the prev_events.",
            "            ratelimit: Whether to rate limit this send.",
            "            txn_id: The transaction ID.",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "            outlier: Indicates whether the event is an `outlier`, i.e. if",
            "                it's from an arbitrary point and floating in the DAG as",
            "                opposed to being inline with the current DAG.",
            "            historical: Indicates whether the message is being inserted",
            "                back in time around some existing events. This is used to skip",
            "                a few checks and mark the event as backfilled.",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "        Returns:",
            "            The event, and its stream ordering (if deduplication happened,",
            "            the previous, duplicate event).",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "        \"\"\"",
            "",
            "        if event_dict[\"type\"] == EventTypes.Member:",
            "            raise SynapseError(",
            "                500, \"Tried to send member event through non-member codepath\"",
            "            )",
            "",
            "        if not ignore_shadow_ban and requester.shadow_banned:",
            "            # We randomly sleep a bit just to annoy the requester.",
            "            await self.clock.sleep(random.randint(1, 10))",
            "            raise ShadowBanError()",
            "",
            "        if ratelimit:",
            "            await self.request_ratelimiter.ratelimit(requester, update=False)",
            "",
            "        # We limit the number of concurrent event sends in a room so that we",
            "        # don't fork the DAG too much. If we don't limit then we can end up in",
            "        # a situation where event persistence can't keep up, causing",
            "        # extremities to pile up, which in turn leads to state resolution",
            "        # taking longer.",
            "        async with self.limiter.queue(event_dict[\"room_id\"]):",
            "            if txn_id:",
            "                event = await self.get_event_from_transaction(",
            "                    requester, txn_id, event_dict[\"room_id\"]",
            "                )",
            "                if event:",
            "                    # we know it was persisted, so must have a stream ordering",
            "                    assert event.internal_metadata.stream_ordering",
            "                    return (",
            "                        event,",
            "                        event.internal_metadata.stream_ordering,",
            "                    )",
            "",
            "            event, context = await self.create_event(",
            "                requester,",
            "                event_dict,",
            "                txn_id=txn_id,",
            "                allow_no_prev_events=allow_no_prev_events,",
            "                prev_event_ids=prev_event_ids,",
            "                state_event_ids=state_event_ids,",
            "                outlier=outlier,",
            "                historical=historical,",
            "                depth=depth,",
            "            )",
            "",
            "            assert self.hs.is_mine_id(event.sender), \"User must be our own: %s\" % (",
            "                event.sender,",
            "            )",
            "",
            "            spam_check_result = await self.spam_checker.check_event_for_spam(event)",
            "            if spam_check_result != self.spam_checker.NOT_SPAM:",
            "                if isinstance(spam_check_result, tuple):",
            "                    try:",
            "                        [code, dict] = spam_check_result",
            "                        raise SynapseError(",
            "                            403,",
            "                            \"This message had been rejected as probable spam\",",
            "                            code,",
            "                            dict,",
            "                        )",
            "                    except ValueError:",
            "                        logger.error(",
            "                            \"Spam-check module returned invalid error value. Expecting [code, dict], got %s\",",
            "                            spam_check_result,",
            "                        )",
            "",
            "                        raise SynapseError(",
            "                            403,",
            "                            \"This message has been rejected as probable spam\",",
            "                            Codes.FORBIDDEN,",
            "                        )",
            "",
            "                # Backwards compatibility: if the return value is not an error code, it",
            "                # means the module returned an error message to be included in the",
            "                # SynapseError (which is now deprecated).",
            "                raise SynapseError(",
            "                    403,",
            "                    spam_check_result,",
            "                    Codes.FORBIDDEN,",
            "                )",
            "",
            "            ev = await self.handle_new_client_event(",
            "                requester=requester,",
            "                events_and_context=[(event, context)],",
            "                ratelimit=ratelimit,",
            "                ignore_shadow_ban=ignore_shadow_ban,",
            "            )",
            "",
            "        # we know it was persisted, so must have a stream ordering",
            "        assert ev.internal_metadata.stream_ordering",
            "        return ev, ev.internal_metadata.stream_ordering",
            "",
            "    @measure_func(\"create_new_client_event\")",
            "    async def create_new_client_event(",
            "        self,",
            "        builder: EventBuilder,",
            "        requester: Optional[Requester] = None,",
            "        allow_no_prev_events: bool = False,",
            "        prev_event_ids: Optional[List[str]] = None,",
            "        auth_event_ids: Optional[List[str]] = None,",
            "        state_event_ids: Optional[List[str]] = None,",
            "        depth: Optional[int] = None,",
            "        state_map: Optional[StateMap[str]] = None,",
            "        for_batch: bool = False,",
            "        current_state_group: Optional[int] = None,",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        \"\"\"Create a new event for a local client. If bool for_batch is true, will",
            "        create an event using the prev_event_ids, and will create an event context for",
            "        the event using the parameters state_map and current_state_group, thus these parameters",
            "        must be provided in this case if for_batch is True. The subsequently created event",
            "        and context are suitable for being batched up and bulk persisted to the database",
            "        with other similarly created events.",
            "",
            "        Args:",
            "            builder:",
            "            requester:",
            "            allow_no_prev_events: Whether to allow this event to be created an empty",
            "                list of prev_events. Normally this is prohibited just because most",
            "                events should have a prev_event and we should only use this in special",
            "                cases like MSC2716.",
            "            prev_event_ids:",
            "                the forward extremities to use as the prev_events for the",
            "                new event.",
            "",
            "                If None, they will be requested from the database.",
            "",
            "            auth_event_ids:",
            "                The event ids to use as the auth_events for the new event.",
            "                Should normally be left as None, which will cause them to be calculated",
            "                based on the room state at the prev_events.",
            "",
            "            state_event_ids:",
            "                The full state at a given event. This is used particularly by the MSC2716",
            "                /batch_send endpoint. One use case is with insertion events which float at",
            "                the beginning of a historical batch and don't have any `prev_events` to",
            "                derive from; we add all of these state events as the explicit state so the",
            "                rest of the historical batch can inherit the same state and state_group.",
            "                This should normally be left as None, which will cause the auth_event_ids",
            "                to be calculated based on the room state at the prev_events.",
            "",
            "            depth: Override the depth used to order the event in the DAG.",
            "                Should normally be set to None, which will cause the depth to be calculated",
            "                based on the prev_events.",
            "",
            "            state_map: A state map of previously created events, used only when creating events",
            "                for batch persisting",
            "",
            "            for_batch: whether the event is being created for batch persisting to the db",
            "",
            "            current_state_group: the current state group, used only for creating events for",
            "                batch persisting",
            "",
            "        Returns:",
            "            Tuple of created event, context",
            "        \"\"\"",
            "        # Strip down the state_event_ids to only what we need to auth the event.",
            "        # For example, we don't need extra m.room.member that don't match event.sender",
            "        if state_event_ids is not None:",
            "            # Do a quick check to make sure that prev_event_ids is present to",
            "            # make the type-checking around `builder.build` happy.",
            "            # prev_event_ids could be an empty array though.",
            "            assert prev_event_ids is not None",
            "",
            "            temp_event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=state_event_ids,",
            "                depth=depth,",
            "            )",
            "            state_events = await self.store.get_events_as_list(state_event_ids)",
            "            # Create a StateMap[str]",
            "            current_state_ids = {",
            "                (e.type, e.state_key): e.event_id for e in state_events",
            "            }",
            "            # Actually strip down and only use the necessary auth events",
            "            auth_event_ids = self._event_auth_handler.compute_auth_events(",
            "                event=temp_event,",
            "                current_state_ids=current_state_ids,",
            "                for_verification=False,",
            "            )",
            "",
            "        if prev_event_ids is not None:",
            "            assert (",
            "                len(prev_event_ids) <= 10",
            "            ), \"Attempting to create an event with %i prev_events\" % (",
            "                len(prev_event_ids),",
            "            )",
            "        else:",
            "            prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        # Do a quick sanity check here, rather than waiting until we've created the",
            "        # event and then try to auth it (which fails with a somewhat confusing \"No",
            "        # create event in auth events\")",
            "        if allow_no_prev_events:",
            "            # We allow events with no `prev_events` but it better have some `auth_events`",
            "            assert (",
            "                builder.type == EventTypes.Create",
            "                # Allow an event to have empty list of prev_event_ids",
            "                # only if it has auth_event_ids.",
            "                or auth_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events or auth_event_ids\"",
            "        else:",
            "            # we now ought to have some prev_events (unless it's a create event).",
            "            assert (",
            "                builder.type == EventTypes.Create or prev_event_ids",
            "            ), \"Attempting to create a non-m.room.create event with no prev_events\"",
            "",
            "        if for_batch:",
            "            assert prev_event_ids is not None",
            "            assert state_map is not None",
            "            assert current_state_group is not None",
            "            auth_ids = self._event_auth_handler.compute_auth_events(builder, state_map)",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids, auth_event_ids=auth_ids, depth=depth",
            "            )",
            "            context = await self.state.compute_event_context_for_batched(",
            "                event, state_map, current_state_group",
            "            )",
            "        else:",
            "            event = await builder.build(",
            "                prev_event_ids=prev_event_ids,",
            "                auth_event_ids=auth_event_ids,",
            "                depth=depth,",
            "            )",
            "",
            "            # Pass on the outlier property from the builder to the event",
            "            # after it is created",
            "            if builder.internal_metadata.outlier:",
            "                event.internal_metadata.outlier = True",
            "                context = EventContext.for_outlier(self._storage_controllers)",
            "            elif (",
            "                event.type == EventTypes.MSC2716_INSERTION",
            "                and state_event_ids",
            "                and builder.internal_metadata.is_historical()",
            "            ):",
            "                # Add explicit state to the insertion event so it has state to derive",
            "                # from even though it's floating with no `prev_events`. The rest of",
            "                # the batch can derive from this state and state_group.",
            "                #",
            "                # TODO(faster_joins): figure out how this works, and make sure that the",
            "                #   old state is complete.",
            "                #   https://github.com/matrix-org/synapse/issues/13003",
            "                metadata = await self.store.get_metadata_for_events(state_event_ids)",
            "",
            "                state_map_for_event: MutableStateMap[str] = {}",
            "                for state_id in state_event_ids:",
            "                    data = metadata.get(state_id)",
            "                    if data is None:",
            "                        # We're trying to persist a new historical batch of events",
            "                        # with the given state, e.g. via",
            "                        # `RoomBatchSendEventRestServlet`. The state can be inferred",
            "                        # by Synapse or set directly by the client.",
            "                        #",
            "                        # Either way, we should have persisted all the state before",
            "                        # getting here.",
            "                        raise Exception(",
            "                            f\"State event {state_id} not found in DB,\"",
            "                            \" Synapse should have persisted it before using it.\"",
            "                        )",
            "",
            "                    if data.state_key is None:",
            "                        raise Exception(",
            "                            f\"Trying to set non-state event {state_id} as state\"",
            "                        )",
            "",
            "                    state_map_for_event[(data.event_type, data.state_key)] = state_id",
            "",
            "                context = await self.state.compute_event_context(",
            "                    event,",
            "                    state_ids_before_event=state_map_for_event,",
            "                    # TODO(faster_joins): check how MSC2716 works and whether we can have",
            "                    #   partial state here",
            "                    #   https://github.com/matrix-org/synapse/issues/13003",
            "                    partial_state=False,",
            "                )",
            "            else:",
            "                context = await self.state.compute_event_context(event)",
            "",
            "        if requester:",
            "            context.app_service = requester.app_service",
            "",
            "        res, new_content = await self.third_party_event_rules.check_event_allowed(",
            "            event, context",
            "        )",
            "        if res is False:",
            "            logger.info(",
            "                \"Event %s forbidden by third-party rules\",",
            "                event,",
            "            )",
            "            raise SynapseError(",
            "                403, \"This event is not allowed in this context\", Codes.FORBIDDEN",
            "            )",
            "        elif new_content is not None:",
            "            # the third-party rules want to replace the event. We'll need to build a new",
            "            # event.",
            "            event, context = await self._rebuild_event_after_third_party_rules(",
            "                new_content, event",
            "            )",
            "",
            "        self.validator.validate_new(event, self.config)",
            "        await self._validate_event_relation(event)",
            "        logger.debug(\"Created event %s\", event.event_id)",
            "",
            "        return event, context",
            "",
            "    async def _validate_event_relation(self, event: EventBase) -> None:",
            "        \"\"\"",
            "        Ensure the relation data on a new event is not bogus.",
            "",
            "        Args:",
            "            event: The event being created.",
            "",
            "        Raises:",
            "            SynapseError if the event is invalid.",
            "        \"\"\"",
            "",
            "        relation = relation_from_event(event)",
            "        if not relation:",
            "            return",
            "",
            "        parent_event = await self.store.get_event(relation.parent_id, allow_none=True)",
            "        if parent_event:",
            "            # And in the same room.",
            "            if parent_event.room_id != event.room_id:",
            "                raise SynapseError(400, \"Relations must be in the same room\")",
            "",
            "        else:",
            "            # There must be some reason that the client knows the event exists,",
            "            # see if there are existing relations. If so, assume everything is fine.",
            "            if not await self.store.event_is_target_of_relation(relation.parent_id):",
            "                # Otherwise, the client can't know about the parent event!",
            "                raise SynapseError(400, \"Can't send relation to unknown event\")",
            "",
            "        # If this event is an annotation then we check that that the sender",
            "        # can't annotate the same way twice (e.g. stops users from liking an",
            "        # event multiple times).",
            "        if relation.rel_type == RelationTypes.ANNOTATION:",
            "            aggregation_key = relation.aggregation_key",
            "",
            "            if aggregation_key is None:",
            "                raise SynapseError(400, \"Missing aggregation key\")",
            "",
            "            if len(aggregation_key) > 500:",
            "                raise SynapseError(400, \"Aggregation key is too long\")",
            "",
            "            already_exists = await self.store.has_user_annotated_event(",
            "                relation.parent_id, event.type, aggregation_key, event.sender",
            "            )",
            "            if already_exists:",
            "                raise SynapseError(400, \"Can't send same reaction twice\")",
            "",
            "        # Don't attempt to start a thread if the parent event is a relation.",
            "        elif relation.rel_type == RelationTypes.THREAD:",
            "            if await self.store.event_includes_relation(relation.parent_id):",
            "                raise SynapseError(",
            "                    400, \"Cannot start threads from an event with a relation\"",
            "                )",
            "",
            "    @measure_func(\"handle_new_client_event\")",
            "    async def handle_new_client_event(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "        ignore_shadow_ban: bool = False,",
            "    ) -> EventBase:",
            "        \"\"\"Processes new events. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        This includes deduplicating, checking auth, persisting,",
            "        notifying users, sending to remote servers, etc.",
            "",
            "        If called from a worker will hit out to the master process for final",
            "        processing.",
            "",
            "        Args:",
            "            requester",
            "            events_and_context: A list of one or more tuples of event, context to be persisted",
            "            ratelimit",
            "            extra_users: Any extra users to notify about event",
            "",
            "            ignore_shadow_ban: True if shadow-banned users should be allowed to",
            "                send this event.",
            "",
            "        Return:",
            "            If the event was deduplicated, the previous, duplicate, event. Otherwise,",
            "            `event`.",
            "",
            "        Raises:",
            "            ShadowBanError if the requester has been shadow-banned.",
            "            SynapseError(503) if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            # we don't apply shadow-banning to membership events here. Invites are blocked",
            "            # higher up the stack, and we allow shadow-banned users to send join and leave",
            "            # events as normal.",
            "            if (",
            "                event.type != EventTypes.Member",
            "                and not ignore_shadow_ban",
            "                and requester.shadow_banned",
            "            ):",
            "                # We randomly sleep a bit just to annoy the requester.",
            "                await self.clock.sleep(random.randint(1, 10))",
            "                raise ShadowBanError()",
            "",
            "            if event.is_state():",
            "                prev_event = await self.deduplicate_state_event(event, context)",
            "                if prev_event is not None:",
            "                    logger.info(",
            "                        \"Not bothering to persist state event %s duplicated by %s\",",
            "                        event.event_id,",
            "                        prev_event.event_id,",
            "                    )",
            "                    return prev_event",
            "",
            "            if event.internal_metadata.is_out_of_band_membership():",
            "                # the only sort of out-of-band-membership events we expect to see here are",
            "                # invite rejections and rescinded knocks that we have generated ourselves.",
            "                assert event.type == EventTypes.Member",
            "                assert event.content[\"membership\"] == Membership.LEAVE",
            "            else:",
            "                try:",
            "                    validate_event_for_room_version(event)",
            "                    # If we are persisting a batch of events the event(s) needed to auth the",
            "                    # current event may be part of the batch and will not be in the DB yet",
            "                    event_id_to_event = {e.event_id: e for e, _ in events_and_context}",
            "                    batched_auth_events = {}",
            "                    for event_id in event.auth_event_ids():",
            "                        auth_event = event_id_to_event.get(event_id)",
            "                        if auth_event:",
            "                            batched_auth_events[event_id] = auth_event",
            "                    await self._event_auth_handler.check_auth_rules_from_context(",
            "                        event, batched_auth_events",
            "                    )",
            "                except AuthError as err:",
            "                    logger.warning(\"Denying new event %r because %s\", event, err)",
            "                    raise err",
            "",
            "            # Ensure that we can round trip before trying to persist in db",
            "            try:",
            "                dump = json_encoder.encode(event.content)",
            "                json_decoder.decode(dump)",
            "            except Exception:",
            "                logger.exception(\"Failed to encode content: %r\", event.content)",
            "                raise",
            "",
            "        # We now persist the event (and update the cache in parallel, since we",
            "        # don't want to block on it).",
            "        event, context = events_and_context[0]",
            "        try:",
            "            result, _ = await make_deferred_yieldable(",
            "                gather_results(",
            "                    (",
            "                        run_in_background(",
            "                            self._persist_events,",
            "                            requester=requester,",
            "                            events_and_context=events_and_context,",
            "                            ratelimit=ratelimit,",
            "                            extra_users=extra_users,",
            "                        ),",
            "                        run_in_background(",
            "                            self.cache_joined_hosts_for_events, events_and_context",
            "                        ).addErrback(",
            "                            log_failure, \"cache_joined_hosts_for_event failed\"",
            "                        ),",
            "                    ),",
            "                    consumeErrors=True,",
            "                )",
            "            ).addErrback(unwrapFirstError)",
            "        except PartialStateConflictError as e:",
            "            # The event context needs to be recomputed.",
            "            # Turn the error into a 429, as a hint to the client to try again.",
            "            logger.info(",
            "                \"Room %s was un-partial stated while persisting client event.\",",
            "                event.room_id,",
            "            )",
            "            raise LimitExceededError(msg=e.msg, errcode=e.errcode, retry_after_ms=0)",
            "",
            "        return result",
            "",
            "    async def _persist_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Actually persists new events. Should only be called by",
            "        `handle_new_client_event`, and see its docstring for documentation of",
            "        the arguments. Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        PartialStateConflictError: if attempting to persist a partial state event in",
            "            a room that has been un-partial stated.",
            "        \"\"\"",
            "",
            "        await self._bulk_push_rule_evaluator.action_for_events_by_user(",
            "            events_and_context",
            "        )",
            "",
            "        try:",
            "            # If we're a worker we need to hit out to the master.",
            "            first_event, _ = events_and_context[0]",
            "            writer_instance = self._events_shard_config.get_instance(",
            "                first_event.room_id",
            "            )",
            "            if writer_instance != self._instance_name:",
            "                try:",
            "                    result = await self.send_events(",
            "                        instance_name=writer_instance,",
            "                        events_and_context=events_and_context,",
            "                        store=self.store,",
            "                        requester=requester,",
            "                        ratelimit=ratelimit,",
            "                        extra_users=extra_users,",
            "                    )",
            "                except SynapseError as e:",
            "                    if e.code == HTTPStatus.CONFLICT:",
            "                        raise PartialStateConflictError()",
            "                    raise",
            "                stream_id = result[\"stream_id\"]",
            "                event_id = result[\"event_id\"]",
            "",
            "                # If we batch persisted events we return the last persisted event, otherwise",
            "                # we return the one event that was persisted",
            "                event, _ = events_and_context[-1]",
            "",
            "                if event_id != event.event_id:",
            "                    # If we get a different event back then it means that its",
            "                    # been de-duplicated, so we replace the given event with the",
            "                    # one already persisted.",
            "                    event = await self.store.get_event(event_id)",
            "                else:",
            "                    # If we newly persisted the event then we need to update its",
            "                    # stream_ordering entry manually (as it was persisted on",
            "                    # another worker).",
            "                    event.internal_metadata.stream_ordering = stream_id",
            "                return event",
            "",
            "            event = await self.persist_and_notify_client_events(",
            "                requester,",
            "                events_and_context,",
            "                ratelimit=ratelimit,",
            "                extra_users=extra_users,",
            "            )",
            "",
            "            return event",
            "        except Exception:",
            "            for event, _ in events_and_context:",
            "                # Ensure that we actually remove the entries in the push actions",
            "                # staging area, if we calculated them.",
            "                await self.store.remove_push_actions_from_staging(event.event_id)",
            "            raise",
            "",
            "    async def cache_joined_hosts_for_events(",
            "        self, events_and_context: List[Tuple[EventBase, EventContext]]",
            "    ) -> None:",
            "        \"\"\"Precalculate the joined hosts at each of the given events, when using Redis, so that",
            "        external federation senders don't have to recalculate it themselves.",
            "        \"\"\"",
            "",
            "        for event, _ in events_and_context:",
            "            if not self._external_cache.is_enabled():",
            "                return",
            "",
            "            # If external cache is enabled we should always have this.",
            "            assert self._external_cache_joined_hosts_updates is not None",
            "",
            "            # We actually store two mappings, event ID -> prev state group,",
            "            # state group -> joined hosts, which is much more space efficient",
            "            # than event ID -> joined hosts.",
            "            #",
            "            # Note: We have to cache event ID -> prev state group, as we don't",
            "            # store that in the DB.",
            "            #",
            "            # Note: We set the state group -> joined hosts cache if it hasn't been",
            "            # set for a while, so that the expiry time is reset.",
            "",
            "            state_entry = await self.state.resolve_state_groups_for_events(",
            "                event.room_id, event_ids=event.prev_event_ids()",
            "            )",
            "",
            "            if state_entry.state_group:",
            "                await self._external_cache.set(",
            "                    \"event_to_prev_state_group\",",
            "                    event.event_id,",
            "                    state_entry.state_group,",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                if state_entry.state_group in self._external_cache_joined_hosts_updates:",
            "                    return",
            "",
            "                state = await state_entry.get_state(",
            "                    self._storage_controllers.state, StateFilter.all()",
            "                )",
            "                with opentracing.start_active_span(\"get_joined_hosts\"):",
            "                    joined_hosts = await self.store.get_joined_hosts(",
            "                        event.room_id, state, state_entry",
            "                    )",
            "",
            "                # Note that the expiry times must be larger than the expiry time in",
            "                # _external_cache_joined_hosts_updates.",
            "                await self._external_cache.set(",
            "                    \"get_joined_hosts\",",
            "                    str(state_entry.state_group),",
            "                    list(joined_hosts),",
            "                    expiry_ms=60 * 60 * 1000,",
            "                )",
            "",
            "                self._external_cache_joined_hosts_updates[",
            "                    state_entry.state_group",
            "                ] = None",
            "",
            "    async def _validate_canonical_alias(",
            "        self,",
            "        directory_handler: DirectoryHandler,",
            "        room_alias_str: str,",
            "        expected_room_id: str,",
            "    ) -> None:",
            "        \"\"\"",
            "        Ensure that the given room alias points to the expected room ID.",
            "",
            "        Args:",
            "            directory_handler: The directory handler object.",
            "            room_alias_str: The room alias to check.",
            "            expected_room_id: The room ID that the alias should point to.",
            "        \"\"\"",
            "        room_alias = RoomAlias.from_string(room_alias_str)",
            "        try:",
            "            mapping = await directory_handler.get_association(room_alias)",
            "        except SynapseError as e:",
            "            # Turn M_NOT_FOUND errors into M_BAD_ALIAS errors.",
            "            if e.errcode == Codes.NOT_FOUND:",
            "                raise SynapseError(",
            "                    400,",
            "                    \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                    Codes.BAD_ALIAS,",
            "                )",
            "            raise",
            "",
            "        if mapping[\"room_id\"] != expected_room_id:",
            "            raise SynapseError(",
            "                400,",
            "                \"Room alias %s does not point to the room\" % (room_alias_str,),",
            "                Codes.BAD_ALIAS,",
            "            )",
            "",
            "    async def persist_and_notify_client_events(",
            "        self,",
            "        requester: Requester,",
            "        events_and_context: List[Tuple[EventBase, EventContext]],",
            "        ratelimit: bool = True,",
            "        extra_users: Optional[List[UserID]] = None,",
            "    ) -> EventBase:",
            "        \"\"\"Called when we have fully built the events, have already",
            "        calculated the push actions for the events, and checked auth.",
            "",
            "        This should only be run on the instance in charge of persisting events.",
            "",
            "        Please note that if batch persisting events, an error in",
            "        handling any one of these events will result in all of the events being dropped.",
            "",
            "        Returns:",
            "            The persisted event, if one event is passed in, or the last event in the",
            "            list in the case of batch persisting. If only one event was persisted, the",
            "            returned event may be different than the given event if it was de-duplicated",
            "            (e.g. because we had already persisted an event with the same transaction ID.)",
            "",
            "        Raises:",
            "            PartialStateConflictError: if attempting to persist a partial state event in",
            "                a room that has been un-partial stated.",
            "        \"\"\"",
            "        extra_users = extra_users or []",
            "",
            "        for event, context in events_and_context:",
            "            assert self._events_shard_config.should_handle(",
            "                self._instance_name, event.room_id",
            "            )",
            "",
            "            if ratelimit:",
            "                # We check if this is a room admin redacting an event so that we",
            "                # can apply different ratelimiting. We do this by simply checking",
            "                # it's not a self-redaction (to avoid having to look up whether the",
            "                # user is actually admin or not).",
            "                is_admin_redaction = False",
            "                if event.type == EventTypes.Redaction:",
            "                    assert event.redacts is not None",
            "",
            "                    original_event = await self.store.get_event(",
            "                        event.redacts,",
            "                        redact_behaviour=EventRedactBehaviour.as_is,",
            "                        get_prev_content=False,",
            "                        allow_rejected=False,",
            "                        allow_none=True,",
            "                    )",
            "",
            "                    is_admin_redaction = bool(",
            "                        original_event and event.sender != original_event.sender",
            "                    )",
            "",
            "                await self.request_ratelimiter.ratelimit(",
            "                    requester, is_admin_redaction=is_admin_redaction",
            "                )",
            "",
            "            # run checks/actions on event based on type",
            "            if event.type == EventTypes.Member and event.membership == Membership.JOIN:",
            "                (",
            "                    current_membership,",
            "                    _,",
            "                ) = await self.store.get_local_current_membership_for_user_in_room(",
            "                    event.state_key, event.room_id",
            "                )",
            "                if current_membership != Membership.JOIN:",
            "                    self._notifier.notify_user_joined_room(",
            "                        event.event_id, event.room_id",
            "                    )",
            "",
            "            await self._maybe_kick_guest_users(event, context)",
            "",
            "            if event.type == EventTypes.CanonicalAlias:",
            "                # Validate a newly added alias or newly added alt_aliases.",
            "",
            "                original_alias = None",
            "                original_alt_aliases: object = []",
            "",
            "                original_event_id = event.unsigned.get(\"replaces_state\")",
            "                if original_event_id:",
            "                    original_alias_event = await self.store.get_event(original_event_id)",
            "",
            "                    if original_alias_event:",
            "                        original_alias = original_alias_event.content.get(\"alias\", None)",
            "                        original_alt_aliases = original_alias_event.content.get(",
            "                            \"alt_aliases\", []",
            "                        )",
            "",
            "                # Check the alias is currently valid (if it has changed).",
            "                room_alias_str = event.content.get(\"alias\", None)",
            "                directory_handler = self.hs.get_directory_handler()",
            "                if room_alias_str and room_alias_str != original_alias:",
            "                    await self._validate_canonical_alias(",
            "                        directory_handler, room_alias_str, event.room_id",
            "                    )",
            "",
            "                # Check that alt_aliases is the proper form.",
            "                alt_aliases = event.content.get(\"alt_aliases\", [])",
            "                if not isinstance(alt_aliases, (list, tuple)):",
            "                    raise SynapseError(",
            "                        400,",
            "                        \"The alt_aliases property must be a list.\",",
            "                        Codes.INVALID_PARAM,",
            "                    )",
            "",
            "                # If the old version of alt_aliases is of an unknown form,",
            "                # completely replace it.",
            "                if not isinstance(original_alt_aliases, (list, tuple)):",
            "                    # TODO: check that the original_alt_aliases' entries are all strings",
            "                    original_alt_aliases = []",
            "",
            "                # Check that each alias is currently valid.",
            "                new_alt_aliases = set(alt_aliases) - set(original_alt_aliases)",
            "                if new_alt_aliases:",
            "                    for alias_str in new_alt_aliases:",
            "                        await self._validate_canonical_alias(",
            "                            directory_handler, alias_str, event.room_id",
            "                        )",
            "",
            "            federation_handler = self.hs.get_federation_handler()",
            "",
            "            if event.type == EventTypes.Member:",
            "                if event.content[\"membership\"] == Membership.INVITE:",
            "                    maybe_upsert_event_field(",
            "                        event,",
            "                        event.unsigned,",
            "                        \"invite_room_state\",",
            "                        await self.store.get_stripped_room_state_from_event_context(",
            "                            context,",
            "                            self.room_prejoin_state_types,",
            "                            membership_user_id=event.sender,",
            "                        ),",
            "                    )",
            "",
            "                    invitee = UserID.from_string(event.state_key)",
            "                    if not self.hs.is_mine(invitee):",
            "                        # TODO: Can we add signature from remote server in a nicer",
            "                        # way? If we have been invited by a remote server, we need",
            "                        # to get them to sign the event.",
            "",
            "                        returned_invite = await federation_handler.send_invite(",
            "                            invitee.domain, event",
            "                        )",
            "                        event.unsigned.pop(\"room_state\", None)",
            "",
            "                        # TODO: Make sure the signatures actually are correct.",
            "                        event.signatures.update(returned_invite.signatures)",
            "",
            "                if event.content[\"membership\"] == Membership.KNOCK:",
            "                    maybe_upsert_event_field(",
            "                        event,",
            "                        event.unsigned,",
            "                        \"knock_room_state\",",
            "                        await self.store.get_stripped_room_state_from_event_context(",
            "                            context,",
            "                            self.room_prejoin_state_types,",
            "                        ),",
            "                    )",
            "",
            "            if event.type == EventTypes.Redaction:",
            "                assert event.redacts is not None",
            "",
            "                original_event = await self.store.get_event(",
            "                    event.redacts,",
            "                    redact_behaviour=EventRedactBehaviour.as_is,",
            "                    get_prev_content=False,",
            "                    allow_rejected=False,",
            "                    allow_none=True,",
            "                )",
            "",
            "                room_version = await self.store.get_room_version_id(event.room_id)",
            "                room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "                # we can make some additional checks now if we have the original event.",
            "                if original_event:",
            "                    if original_event.type == EventTypes.Create:",
            "                        raise AuthError(403, \"Redacting create events is not permitted\")",
            "",
            "                    if original_event.room_id != event.room_id:",
            "                        raise SynapseError(",
            "                            400, \"Cannot redact event from a different room\"",
            "                        )",
            "",
            "                    if original_event.type == EventTypes.ServerACL:",
            "                        raise AuthError(",
            "                            403, \"Redacting server ACL events is not permitted\"",
            "                        )",
            "",
            "                    # Add a little safety stop-gap to prevent people from trying to",
            "                    # redact MSC2716 related events when they're in a room version",
            "                    # which does not support it yet. We allow people to use MSC2716",
            "                    # events in existing room versions but only from the room",
            "                    # creator since it does not require any changes to the auth",
            "                    # rules and in effect, the redaction algorithm . In the",
            "                    # supported room version, we add the `historical` power level to",
            "                    # auth the MSC2716 related events and adjust the redaction",
            "                    # algorthim to keep the `historical` field around (redacting an",
            "                    # event should only strip fields which don't affect the",
            "                    # structural protocol level).",
            "                    is_msc2716_event = (",
            "                        original_event.type == EventTypes.MSC2716_INSERTION",
            "                        or original_event.type == EventTypes.MSC2716_BATCH",
            "                        or original_event.type == EventTypes.MSC2716_MARKER",
            "                    )",
            "                    if not room_version_obj.msc2716_historical and is_msc2716_event:",
            "                        raise AuthError(",
            "                            403,",
            "                            \"Redacting MSC2716 events is not supported in this room version\",",
            "                        )",
            "",
            "                event_types = event_auth.auth_types_for_event(event.room_version, event)",
            "                prev_state_ids = await context.get_prev_state_ids(",
            "                    StateFilter.from_types(event_types)",
            "                )",
            "",
            "                auth_events_ids = self._event_auth_handler.compute_auth_events(",
            "                    event, prev_state_ids, for_verification=True",
            "                )",
            "                auth_events_map = await self.store.get_events(auth_events_ids)",
            "                auth_events = {",
            "                    (e.type, e.state_key): e for e in auth_events_map.values()",
            "                }",
            "",
            "                if event_auth.check_redaction(",
            "                    room_version_obj, event, auth_events=auth_events",
            "                ):",
            "                    # this user doesn't have 'redact' rights, so we need to do some more",
            "                    # checks on the original event. Let's start by checking the original",
            "                    # event exists.",
            "                    if not original_event:",
            "                        raise NotFoundError(",
            "                            \"Could not find event %s\" % (event.redacts,)",
            "                        )",
            "",
            "                    if event.user_id != original_event.user_id:",
            "                        raise AuthError(",
            "                            403, \"You don't have permission to redact events\"",
            "                        )",
            "",
            "                    # all the checks are done.",
            "                    event.internal_metadata.recheck_redaction = False",
            "",
            "            if event.type == EventTypes.Create:",
            "                prev_state_ids = await context.get_prev_state_ids()",
            "                if prev_state_ids:",
            "                    raise AuthError(403, \"Changing the room create event is forbidden\")",
            "",
            "            if event.type == EventTypes.MSC2716_INSERTION:",
            "                room_version = await self.store.get_room_version_id(event.room_id)",
            "                room_version_obj = KNOWN_ROOM_VERSIONS[room_version]",
            "",
            "                create_event = await self.store.get_create_event_for_room(event.room_id)",
            "                room_creator = create_event.content.get(EventContentFields.ROOM_CREATOR)",
            "",
            "                # Only check an insertion event if the room version",
            "                # supports it or the event is from the room creator.",
            "                if room_version_obj.msc2716_historical or (",
            "                    self.config.experimental.msc2716_enabled",
            "                    and event.sender == room_creator",
            "                ):",
            "                    next_batch_id = event.content.get(",
            "                        EventContentFields.MSC2716_NEXT_BATCH_ID",
            "                    )",
            "                    conflicting_insertion_event_id = None",
            "                    if next_batch_id:",
            "                        conflicting_insertion_event_id = (",
            "                            await self.store.get_insertion_event_id_by_batch_id(",
            "                                event.room_id, next_batch_id",
            "                            )",
            "                        )",
            "                    if conflicting_insertion_event_id is not None:",
            "                        # The current insertion event that we're processing is invalid",
            "                        # because an insertion event already exists in the room with the",
            "                        # same next_batch_id. We can't allow multiple because the batch",
            "                        # pointing will get weird, e.g. we can't determine which insertion",
            "                        # event the batch event is pointing to.",
            "                        raise SynapseError(",
            "                            HTTPStatus.BAD_REQUEST,",
            "                            \"Another insertion event already exists with the same next_batch_id\",",
            "                            errcode=Codes.INVALID_PARAM,",
            "                        )",
            "",
            "            # Mark any `m.historical` messages as backfilled so they don't appear",
            "            # in `/sync` and have the proper decrementing `stream_ordering` as we import",
            "            backfilled = False",
            "            if event.internal_metadata.is_historical():",
            "                backfilled = True",
            "",
            "        assert self._storage_controllers.persistence is not None",
            "        (",
            "            persisted_events,",
            "            max_stream_token,",
            "        ) = await self._storage_controllers.persistence.persist_events(",
            "            events_and_context, backfilled=backfilled",
            "        )",
            "",
            "        events_and_pos = []",
            "        for event in persisted_events:",
            "            if self._ephemeral_events_enabled:",
            "                # If there's an expiry timestamp on the event, schedule its expiry.",
            "                self._message_handler.maybe_schedule_expiry(event)",
            "",
            "            stream_ordering = event.internal_metadata.stream_ordering",
            "            assert stream_ordering is not None",
            "            pos = PersistedEventPosition(self._instance_name, stream_ordering)",
            "            events_and_pos.append((event, pos))",
            "",
            "            if event.type == EventTypes.Message:",
            "                # We don't want to block sending messages on any presence code. This",
            "                # matters as sometimes presence code can take a while.",
            "                run_in_background(self._bump_active_time, requester.user)",
            "",
            "        async def _notify() -> None:",
            "            try:",
            "                await self.notifier.on_new_room_events(",
            "                    events_and_pos, max_stream_token, extra_users=extra_users",
            "                )",
            "            except Exception:",
            "                logger.exception(\"Error notifying about new room events\")",
            "",
            "        run_in_background(_notify)",
            "",
            "        return persisted_events[-1]",
            "",
            "    async def _maybe_kick_guest_users(",
            "        self, event: EventBase, context: EventContext",
            "    ) -> None:",
            "        if event.type != EventTypes.GuestAccess:",
            "            return",
            "",
            "        guest_access = event.content.get(EventContentFields.GUEST_ACCESS)",
            "        if guest_access == GuestAccess.CAN_JOIN:",
            "            return",
            "",
            "        current_state_ids = await context.get_current_state_ids()",
            "",
            "        # since this is a client-generated event, it cannot be an outlier and we must",
            "        # therefore have the state ids.",
            "        assert current_state_ids is not None",
            "        current_state_dict = await self.store.get_events(",
            "            list(current_state_ids.values())",
            "        )",
            "        current_state = list(current_state_dict.values())",
            "        logger.info(\"maybe_kick_guest_users %r\", current_state)",
            "        await self.hs.get_room_member_handler().kick_guest_users(current_state)",
            "",
            "    async def _bump_active_time(self, user: UserID) -> None:",
            "        try:",
            "            presence = self.hs.get_presence_handler()",
            "            await presence.bump_presence_active_time(user)",
            "        except Exception:",
            "            logger.exception(\"Error bumping presence active time\")",
            "",
            "    async def _send_dummy_events_to_fill_extremities(self) -> None:",
            "        \"\"\"Background task to send dummy events into rooms that have a large",
            "        number of extremities",
            "        \"\"\"",
            "        self._expire_rooms_to_exclude_from_dummy_event_insertion()",
            "        room_ids = await self.store.get_rooms_with_many_extremities(",
            "            min_count=self._dummy_events_threshold,",
            "            limit=5,",
            "            room_id_filter=self._rooms_to_exclude_from_dummy_event_insertion.keys(),",
            "        )",
            "",
            "        for room_id in room_ids:",
            "            dummy_event_sent = await self._send_dummy_event_for_room(room_id)",
            "",
            "            if not dummy_event_sent:",
            "                # Did not find a valid user in the room, so remove from future attempts",
            "                # Exclusion is time limited, so the room will be rechecked in the future",
            "                # dependent on _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s. Will exclude it from \"",
            "                    \"future attempts until cache expires\" % (room_id,)",
            "                )",
            "                now = self.clock.time_msec()",
            "                self._rooms_to_exclude_from_dummy_event_insertion[room_id] = now",
            "",
            "    async def _send_dummy_event_for_room(self, room_id: str) -> bool:",
            "        \"\"\"Attempt to send a dummy event for the given room.",
            "",
            "        Args:",
            "            room_id: room to try to send an event from",
            "",
            "        Returns:",
            "            True if a dummy event was successfully sent. False if no user was able",
            "            to send an event.",
            "        \"\"\"",
            "",
            "        # For each room we need to find a joined member we can use to send",
            "        # the dummy event with.",
            "        members = await self.store.get_local_users_in_room(room_id)",
            "        for user_id in members:",
            "            requester = create_requester(user_id, authenticated_entity=self.server_name)",
            "            try:",
            "                event, context = await self.create_event(",
            "                    requester,",
            "                    {",
            "                        \"type\": EventTypes.Dummy,",
            "                        \"content\": {},",
            "                        \"room_id\": room_id,",
            "                        \"sender\": user_id,",
            "                    },",
            "                )",
            "",
            "                event.internal_metadata.proactively_send = False",
            "",
            "                # Since this is a dummy-event it is OK if it is sent by a",
            "                # shadow-banned user.",
            "                await self.handle_new_client_event(",
            "                    requester,",
            "                    events_and_context=[(event, context)],",
            "                    ratelimit=False,",
            "                    ignore_shadow_ban=True,",
            "                )",
            "                return True",
            "            except AuthError:",
            "                logger.info(",
            "                    \"Failed to send dummy event into room %s for user %s due to \"",
            "                    \"lack of power. Will try another user\" % (room_id, user_id)",
            "                )",
            "        return False",
            "",
            "    def _expire_rooms_to_exclude_from_dummy_event_insertion(self) -> None:",
            "        expire_before = self.clock.time_msec() - _DUMMY_EVENT_ROOM_EXCLUSION_EXPIRY",
            "        to_expire = set()",
            "        for room_id, time in self._rooms_to_exclude_from_dummy_event_insertion.items():",
            "            if time < expire_before:",
            "                to_expire.add(room_id)",
            "        for room_id in to_expire:",
            "            logger.debug(",
            "                \"Expiring room id %s from dummy event insertion exclusion cache\",",
            "                room_id,",
            "            )",
            "            del self._rooms_to_exclude_from_dummy_event_insertion[room_id]",
            "",
            "    async def _rebuild_event_after_third_party_rules(",
            "        self, third_party_result: dict, original_event: EventBase",
            "    ) -> Tuple[EventBase, EventContext]:",
            "        # the third_party_event_rules want to replace the event.",
            "        # we do some basic checks, and then return the replacement event and context.",
            "",
            "        # Construct a new EventBuilder and validate it, which helps with the",
            "        # rest of these checks.",
            "        try:",
            "            builder = self.event_builder_factory.for_room_version(",
            "                original_event.room_version, third_party_result",
            "            )",
            "            self.validator.validate_builder(builder)",
            "        except SynapseError as e:",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \" + e.msg,",
            "            )",
            "",
            "        immutable_fields = [",
            "            # changing the room is going to break things: we've already checked that the",
            "            # room exists, and are holding a concurrency limiter token for that room.",
            "            # Also, we might need to use a different room version.",
            "            \"room_id\",",
            "            # changing the type or state key might work, but we'd need to check that the",
            "            # calling functions aren't making assumptions about them.",
            "            \"type\",",
            "            \"state_key\",",
            "        ]",
            "",
            "        for k in immutable_fields:",
            "            if getattr(builder, k, None) != original_event.get(k):",
            "                raise Exception(",
            "                    \"Third party rules module created an invalid event: \"",
            "                    \"cannot change field \" + k",
            "                )",
            "",
            "        # check that the new sender belongs to this HS",
            "        if not self.hs.is_mine_id(builder.sender):",
            "            raise Exception(",
            "                \"Third party rules module created an invalid event: \"",
            "                \"invalid sender \" + builder.sender",
            "            )",
            "",
            "        # copy over the original internal metadata",
            "        for k, v in original_event.internal_metadata.get_dict().items():",
            "            setattr(builder.internal_metadata, k, v)",
            "",
            "        # modules can send new state events, so we re-calculate the auth events just in",
            "        # case.",
            "        prev_event_ids = await self.store.get_prev_events_for_room(builder.room_id)",
            "",
            "        event = await builder.build(",
            "            prev_event_ids=prev_event_ids,",
            "            auth_event_ids=None,",
            "        )",
            "",
            "        # we rebuild the event context, to be on the safe side. If nothing else,",
            "        # delta_ids might need an update.",
            "        context = await self.state.compute_event_context(event)",
            "        return event, context"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1742": [
                "EventCreationHandler"
            ],
            "1743": [
                "EventCreationHandler"
            ],
            "1744": [
                "EventCreationHandler"
            ],
            "1745": [
                "EventCreationHandler"
            ],
            "1746": [
                "EventCreationHandler"
            ],
            "1747": [
                "EventCreationHandler"
            ],
            "1765": [
                "EventCreationHandler"
            ],
            "1766": [
                "EventCreationHandler"
            ],
            "1767": [
                "EventCreationHandler"
            ],
            "1768": [
                "EventCreationHandler"
            ],
            "1769": [
                "EventCreationHandler"
            ]
        },
        "addLocation": []
    }
}