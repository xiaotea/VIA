{
    "lollms/app.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 14,
                "PatchRowcode": " from lollms.databases.skills_database import SkillsLibrary"
            },
            "1": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 15,
                "PatchRowcode": " from lollms.tasks import TasksLibrary"
            },
            "2": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from safe_store import TextVectorizer, VectorizationMethod, VisualizationMethod"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 18,
                "PatchRowcode": "+from lollmsvectordb.database_elements.chunk import Chunk"
            },
            "5": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " from typing import Callable"
            },
            "6": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from pathlib import Path"
            },
            "7": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 21,
                "PatchRowcode": " from datetime import datetime"
            },
            "8": {
                "beforePatchRowNumber": 905,
                "afterPatchRowNumber": 907,
                "PatchRowcode": " "
            },
            "9": {
                "beforePatchRowNumber": 906,
                "afterPatchRowNumber": 908,
                "PatchRowcode": "         if len(conditionning)>0:"
            },
            "10": {
                "beforePatchRowNumber": 907,
                "afterPatchRowNumber": 909,
                "PatchRowcode": "             conditionning =  self.start_header_id_template + system_message_template + self.end_header_id_template + self.personality.replace_keys(conditionning, self.personality.conditionning_commands) + (\"\" if conditionning[-1]==self.separator_template else self.separator_template)"
            },
            "11": {
                "beforePatchRowNumber": 908,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            "
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 910,
                "PatchRowcode": "+"
            },
            "13": {
                "beforePatchRowNumber": 909,
                "afterPatchRowNumber": 911,
                "PatchRowcode": "         # Check if there are document files to add to the prompt"
            },
            "14": {
                "beforePatchRowNumber": 910,
                "afterPatchRowNumber": 912,
                "PatchRowcode": "         internet_search_results = \"\""
            },
            "15": {
                "beforePatchRowNumber": 911,
                "afterPatchRowNumber": 913,
                "PatchRowcode": "         internet_search_infos = []"
            },
            "16": {
                "beforePatchRowNumber": 1093,
                "afterPatchRowNumber": 1095,
                "PatchRowcode": "                             query = current_message.content"
            },
            "17": {
                "beforePatchRowNumber": 1094,
                "afterPatchRowNumber": 1096,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 1095,
                "afterPatchRowNumber": 1097,
                "PatchRowcode": "                     try:"
            },
            "19": {
                "beforePatchRowNumber": 1096,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        if self.config.data_vectorization_force_first_chunk and len(client.discussion.vectorizer.chunks)>0:"
            },
            "20": {
                "beforePatchRowNumber": 1097,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            doc_index = list(client.discussion.vectorizer.chunks.keys())[0]"
            },
            "21": {
                "beforePatchRowNumber": 1098,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "22": {
                "beforePatchRowNumber": 1099,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            doc_id = client.discussion.vectorizer.chunks[doc_index]['document_id']"
            },
            "23": {
                "beforePatchRowNumber": 1100,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            content = client.discussion.vectorizer.chunks[doc_index]['chunk_text']"
            },
            "24": {
                "beforePatchRowNumber": 1101,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            "
            },
            "25": {
                "beforePatchRowNumber": 1102,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            if self.config.data_vectorization_put_chunk_informations_into_context:"
            },
            "26": {
                "beforePatchRowNumber": 1103,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\nchunk_infos:{doc_id}\\ncontent:{content}\\n\""
            },
            "27": {
                "beforePatchRowNumber": 1104,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            else:"
            },
            "28": {
                "beforePatchRowNumber": 1105,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{content}\\n\""
            },
            "29": {
                "beforePatchRowNumber": 1106,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "30": {
                "beforePatchRowNumber": 1107,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        docs, sorted_similarities, document_ids = client.discussion.vectorizer.recover_text(query, top_k=int(self.config.data_vectorization_nb_chunks))"
            },
            "31": {
                "beforePatchRowNumber": 1108,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        for doc, infos in zip(docs, sorted_similarities):"
            },
            "32": {
                "beforePatchRowNumber": 1109,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                            if self.config.data_vectorization_force_first_chunk and len(client.discussion.vectorizer.chunks)>0 and infos[0]==doc_id:"
            },
            "33": {
                "beforePatchRowNumber": 1110,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                continue"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1098,
                "PatchRowcode": "+                        chunks:List[Chunk] = client.discussion.vectorizer.search(query, int(self.config.rag_n_chunks))"
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1099,
                "PatchRowcode": "+                        for chunk in chunks:"
            },
            "36": {
                "beforePatchRowNumber": 1111,
                "afterPatchRowNumber": 1100,
                "PatchRowcode": "                             if self.config.data_vectorization_put_chunk_informations_into_context:"
            },
            "37": {
                "beforePatchRowNumber": 1112,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\nchunk path: {infos[0]}\\nchunk content:\\n{doc}\\n\""
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1101,
                "PatchRowcode": "+                                documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\ndocument title: {chunk.doc.title}\\nchunk content:\\n{chunk.text}\\n\""
            },
            "39": {
                "beforePatchRowNumber": 1113,
                "afterPatchRowNumber": 1102,
                "PatchRowcode": "                             else:"
            },
            "40": {
                "beforePatchRowNumber": 1114,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{doc}\\n\""
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1103,
                "PatchRowcode": "+                                documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{chunk.text}\\n\""
            },
            "42": {
                "beforePatchRowNumber": 1115,
                "afterPatchRowNumber": 1104,
                "PatchRowcode": " "
            },
            "43": {
                "beforePatchRowNumber": 1116,
                "afterPatchRowNumber": 1105,
                "PatchRowcode": "                         documentation += f\"{self.separator_template}{self.start_header_id_template}important information: Use the documentation data to answer the user questions. If the data is not present in the documentation, please tell the user that the information he is asking for does not exist in the documentation section. It is strictly forbidden to give the user an answer without having actual proof from the documentation.\\n\""
            },
            "44": {
                "beforePatchRowNumber": 1117,
                "afterPatchRowNumber": 1106,
                "PatchRowcode": "                     except Exception as ex:"
            }
        },
        "frontPatchFile": [
            "from lollms.main_config import LOLLMSConfig",
            "from lollms.paths import LollmsPaths",
            "from lollms.personality import PersonalityBuilder, AIPersonality",
            "from lollms.binding import LLMBinding, BindingBuilder, ModelBuilder",
            "from lollms.databases.discussions_database import Message",
            "from lollms.extension import LOLLMSExtension, ExtensionBuilder",
            "from lollms.config import InstallOption",
            "from lollms.helpers import ASCIIColors, trace_exception",
            "from lollms.com import NotificationType, NotificationDisplayType, LoLLMsCom",
            "from lollms.terminal import MainMenu",
            "from lollms.types import MSG_TYPE, SENDER_TYPES",
            "from lollms.utilities import PromptReshaper",
            "from lollms.client_session import Client, Session",
            "from lollms.databases.skills_database import SkillsLibrary",
            "from lollms.tasks import TasksLibrary",
            "from safe_store import TextVectorizer, VectorizationMethod, VisualizationMethod",
            "from typing import Callable",
            "from pathlib import Path",
            "from datetime import datetime",
            "from functools import partial",
            "from socketio import AsyncServer",
            "from typing import Tuple, List, Dict",
            "import subprocess",
            "import importlib",
            "import sys, os",
            "import platform",
            "import gc",
            "import yaml",
            "import time",
            "from lollms.utilities import PackageManager",
            "",
            "class LollmsApplication(LoLLMsCom):",
            "    def __init__(",
            "                    self, ",
            "                    app_name:str, ",
            "                    config:LOLLMSConfig, ",
            "                    lollms_paths:LollmsPaths, ",
            "                    load_binding=True, ",
            "                    load_model=True, ",
            "                    try_select_binding=False, ",
            "                    try_select_model=False,",
            "                    callback=None,",
            "                    sio:AsyncServer=None,",
            "                    free_mode=False",
            "                ) -> None:",
            "        \"\"\"",
            "        Creates a LOLLMS Application",
            "        \"\"\"",
            "        super().__init__(sio)",
            "        self.app_name                   = app_name",
            "        self.config                     = config",
            "        self.lollms_paths               = lollms_paths",
            "",
            "        # TODO : implement",
            "        self.embedding_models           = []",
            "",
            "        self.menu                       = MainMenu(self, callback)",
            "        self.mounted_personalities      = []",
            "        self.personality:AIPersonality  = None",
            "",
            "        self.mounted_extensions         = []",
            "        self.binding                    = None",
            "        self.model:LLMBinding           = None",
            "        self.long_term_memory           = None",
            "",
            "        self.tts                        = None",
            "        self.session                    = Session(lollms_paths)",
            "        self.skills_library             = SkillsLibrary(self.lollms_paths.personal_skills_path/(self.config.skills_lib_database_name+\".db\"))",
            "",
            "        self.tasks_library              = TasksLibrary(self)",
            "",
            "        self.handle_generate_msg: Callable[[str, Dict], None]               = None",
            "        self.generate_msg_with_internet: Callable[[str, Dict], None]        = None",
            "        self.handle_continue_generate_msg_from: Callable[[str, Dict], None] = None",
            "        ",
            "        # Trust store ",
            "        self.bk_store = None",
            "        ",
            "        # services",
            "        self.ollama         = None",
            "        self.vllm           = None",
            "        self.whisper        = None",
            "        self.xtts           = None",
            "        self.sd             = None",
            "        self.comfyui        = None",
            "        self.motion_ctrl    = None",
            "",
            "        self.tti = None",
            "        self.tts = None",
            "        self.stt = None",
            "        self.ttm = None",
            "        self.ttv = None",
            "        ",
            "        self.rt_com = None",
            "        if not free_mode:",
            "            try:",
            "                if config.auto_update:",
            "                    # Clone the repository to the target path",
            "                    if self.lollms_paths.lollms_core_path.exists():",
            "                        ASCIIColors.info(\"Lollms_core found in the app space.\\nPulling last lollms_core\")",
            "                        subprocess.run([\"git\", \"-C\", self.lollms_paths.lollms_core_path, \"pull\"])            ",
            "                    if self.lollms_paths.safe_store_path.exists():",
            "                        ASCIIColors.info(\"safe_store_path found in the app space.\\nPulling last safe_store_path\")",
            "                        subprocess.run([\"git\", \"-C\", self.lollms_paths.safe_store_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ",
            "                    ASCIIColors.info(\"Bindings zoo found in your personal space.\\nPulling last bindings zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.bindings_zoo_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ASCIIColors.info(\"Personalities zoo found in your personal space.\\nPulling last personalities zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.personalities_zoo_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ASCIIColors.info(\"Extensions zoo found in your personal space.\\nPulling last Extensions zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.extensions_zoo_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ASCIIColors.info(\"Models zoo found in your personal space.\\nPulling last Models zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.models_zoo_path, \"pull\"])            ",
            "            except Exception as ex:",
            "                ASCIIColors.error(\"Couldn't pull zoos. Please contact the main dev on our discord channel and report the problem.\")",
            "                trace_exception(ex)",
            "",
            "            if self.config.binding_name is None:",
            "                ASCIIColors.warning(f\"No binding selected\")",
            "                if try_select_binding:",
            "                    ASCIIColors.info(\"Please select a valid model or install a new one from a url\")",
            "                    self.menu.select_binding()",
            "            else:",
            "                if load_binding:",
            "                    try:",
            "                        ASCIIColors.info(f\">Loading binding {self.config.binding_name}. Please wait ...\")",
            "                        self.binding            = self.load_binding()",
            "                    except Exception as ex:",
            "                        ASCIIColors.error(f\"Failed to load binding.\\nReturned exception: {ex}\")",
            "                        trace_exception(ex)",
            "",
            "                    if self.binding is not None:",
            "                        ASCIIColors.success(f\"Binding {self.config.binding_name} loaded successfully.\")",
            "                        if load_model:",
            "                            if self.config.model_name is None:",
            "                                ASCIIColors.warning(f\"No model selected\")",
            "                                if try_select_model:",
            "                                    print(\"Please select a valid model\")",
            "                                    self.menu.select_model()",
            "                                    ",
            "                            if self.config.model_name is not None:",
            "                                try:",
            "                                    ASCIIColors.info(f\">Loading model {self.config.model_name}. Please wait ...\")",
            "                                    self.model          = self.load_model()",
            "                                    if self.model is not None:",
            "                                        ASCIIColors.success(f\"Model {self.config.model_name} loaded successfully.\")",
            "",
            "                                except Exception as ex:",
            "                                    ASCIIColors.error(f\"Failed to load model.\\nReturned exception: {ex}\")",
            "                                    trace_exception(ex)",
            "                    else:",
            "                        ASCIIColors.warning(f\"Couldn't load binding {self.config.binding_name}.\")",
            "                ",
            "            self.mount_personalities()",
            "            self.mount_extensions()",
            "            ",
            "            try:",
            "                self.load_rag_dbs()",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "",
            "",
            "",
            "    def backup_trust_store(self):",
            "        self.bk_store = None",
            "        if 'REQUESTS_CA_BUNDLE' in os.environ:",
            "            self.bk_store = os.environ['REQUESTS_CA_BUNDLE']",
            "            del os.environ['REQUESTS_CA_BUNDLE']",
            "",
            "    def restore_trust_store(self):",
            "        if self.bk_store is not None:",
            "            os.environ['REQUESTS_CA_BUNDLE'] = self.bk_store",
            "            ",
            "    def select_model(self, binding_name, model_name):",
            "        self.config[\"binding_name\"] = binding_name",
            "        self.config[\"model_name\"] = model_name",
            "        print(f\"New binding selected : {binding_name}\")",
            "",
            "        try:",
            "            if self.binding:",
            "                self.binding.destroy_model()",
            "            self.binding = None",
            "            self.model = None",
            "            for per in self.mounted_personalities:",
            "                if per is not None:",
            "                    per.model = None",
            "            gc.collect()",
            "            self.binding = BindingBuilder().build_binding(self.config, self.lollms_paths, InstallOption.INSTALL_IF_NECESSARY, lollmsCom=self)",
            "            self.config[\"model_name\"] = model_name",
            "            self.model = self.binding.build_model()",
            "            for per in self.mounted_personalities:",
            "                if per is not None:",
            "                    per.model = self.model",
            "            self.config.save_config()",
            "            ASCIIColors.green(\"Binding loaded successfully\")",
            "            return True",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't build binding: [{ex}]\")",
            "            trace_exception(ex)",
            "            return False",
            "        ",
            "    def add_discussion_to_skills_library(self, client: Client):",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        separator_template          = self.config.separator_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "",
            "        messages = client.discussion.get_messages()",
            "",
            "        # Extract relevant information from messages",
            "        def cb(str, MSG_TYPE_=MSG_TYPE.MSG_TYPE_FULL, dict=None, list=None):",
            "            if MSG_TYPE_!=MSG_TYPE.MSG_TYPE_CHUNK:",
            "                self.ShowBlockingMessage(f\"Learning\\n{str}\")",
            "        bk_cb = self.tasks_library.callback",
            "        self.tasks_library.callback = cb",
            "        content = self._extract_content(messages, cb)",
            "        self.tasks_library.callback = bk_cb",
            "",
            "        # Generate title",
            "        title_prompt =  f\"{separator_template}\".join([",
            "            f\"{self.start_header_id_template}{system_message_template}{end_header_id_template}Generate a concise and descriptive title for the following content.\",",
            "            \"The title should summarize the main topic or subject of the content.\",",
            "            \"Do not mention the format of the content (e.g., bullet points, discussion, etc.) in the title.\",",
            "            \"Provide only the title without any additional explanations or context.\",",
            "            f\"{self.start_header_id_template}content{end_header_id_template}\",",
            "            f\"{content}\",",
            "            f\"{self.start_header_id_template}title{end_header_id_template}\"",
            "            ])",
            "",
            "        title = self._generate_text(title_prompt)",
            "",
            "        # Determine category",
            "        category_prompt = f\"{self.start_header_id_template}{system_message_template}{end_header_id_template}Analyze the following title, and determine the most appropriate generic category that encompasses the main subject or theme. The category should be broad enough to include multiple related skill entries. Provide only the category name without any additional explanations or context:\\n\\nTitle:\\n{title}\\n{separator_template}{self.start_header_id_template}Category:\\n\"",
            "        category = self._generate_text(category_prompt)",
            "",
            "        # Add entry to skills library",
            "        self.skills_library.add_entry(1, category, title, content)",
            "        return category, title, content",
            "",
            "    def _extract_content(self, messages:List[Message], callback = None):      ",
            "        message_content = \"\"",
            "",
            "        for message in messages:",
            "            rank = message.rank",
            "            sender = message.sender",
            "            text = message.content",
            "            message_content += f\"Rank {rank} - {sender}: {text}\\n\"",
            "",
            "        return self.tasks_library.summerize_text(",
            "            message_content, ",
            "            \"\\n\".join([",
            "                \"Extract useful information from this discussion.\"",
            "            ]),",
            "            doc_name=\"discussion\",",
            "            callback=callback)",
            "        ",
            "",
            "    def _generate_text(self, prompt):",
            "        max_tokens = self.config.ctx_size - self.model.get_nb_tokens(prompt)",
            "        generated_text = self.model.generate(prompt, max_tokens)",
            "        return generated_text.strip()",
            "",
            "",
            "    def get_uploads_path(self, client_id):",
            "        return self.lollms_paths.personal_uploads_path",
            "    ",
            "    def load_rag_dbs(self):",
            "        self.active_rag_dbs = []",
            "        for rag_db in self.config.rag_databases:",
            "            parts = rag_db.split(\"::\")",
            "            db_name = parts[0]",
            "            if parts[-1]==\"mounted\":",
            "                if not PackageManager.check_package_installed(\"lollmsvectordb\"):",
            "                    PackageManager.install_package(\"lollmsvectordb\")",
            "                ",
            "                from lollmsvectordb import VectorDatabase",
            "                from lollmsvectordb.text_document_loader import TextDocumentsLoader",
            "                from lollmsvectordb.lollms_tokenizers.tiktoken_tokenizer import TikTokenTokenizer",
            "                if self.config.rag_vectorizer == \"bert\":",
            "                    self.backup_trust_store()",
            "                    from lollmsvectordb.lollms_vectorizers.bert_vectorizer import BERTVectorizer",
            "                    v = BERTVectorizer()",
            "                    self.restore_trust_store()",
            "                elif self.config.rag_vectorizer == \"tfidf\":",
            "                    from lollmsvectordb.lollms_vectorizers.tfidf_vectorizer import TFIDFVectorizer",
            "                    v = TFIDFVectorizer()",
            "                elif self.config.rag_vectorizer == \"word2vec\":",
            "                    from lollmsvectordb.lollms_vectorizers.word2vec_vectorizer import Word2VecVectorizer",
            "                    v = Word2VecVectorizer()",
            "",
            "                vdb = VectorDatabase(Path(parts[1])/f\"{db_name}.sqlite\", v, self.model if self.model else TikTokenTokenizer(), n_neighbors=self.config.rag_n_chunks)       ",
            "                self.active_rag_dbs.append({\"name\":parts[0],\"path\":parts[1],\"vectorizer\":vdb})",
            "",
            "",
            "    def start_servers(self):",
            "",
            "        ASCIIColors.yellow(\"* - * - * - Starting services - * - * - *\")",
            "",
            "        ASCIIColors.blue(\"Loading local TTT services\")",
            "        if self.config.enable_ollama_service:",
            "            try:",
            "                from lollms.services.ollama.lollms_ollama import Service",
            "                self.ollama = Service(self, base_url=self.config.ollama_base_url)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "                self.warning(f\"Couldn't load Ollama\")",
            "",
            "        if self.config.enable_vllm_service:",
            "            try:",
            "                from lollms.services.vllm.lollms_vllm import Service",
            "                self.vllm = Service(self, base_url=self.config.vllm_url)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "                self.warning(f\"Couldn't load vllm\")",
            "",
            "        ASCIIColors.blue(\"Loading loacal STT services\")",
            "        if self.config.whisper_activate or self.config.active_stt_service == \"whisper\":",
            "            try:",
            "                from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "                self.whisper = LollmsWhisper(self, self.config.whisper_model, self.lollms_paths.personal_outputs_path)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "",
            "        ASCIIColors.blue(\"Loading local TTS services\")",
            "        if self.config.xtts_enable or self.config.active_tts_service == \"xtts\":",
            "            ASCIIColors.yellow(\"Loading XTTS\")",
            "            try:",
            "                from lollms.services.xtts.lollms_xtts import LollmsXTTS",
            "                voice=self.config.xtts_current_voice",
            "                if voice!=\"main_voice\":",
            "                    voices_folder = self.lollms_paths.custom_voices_path",
            "                else:",
            "                    voices_folder = Path(__file__).parent.parent.parent/\"services/xtts/voices\"",
            "",
            "                self.xtts = LollmsXTTS(",
            "                                        self,",
            "                                        voices_folder=voices_folder,",
            "                                        voice_samples_path=self.lollms_paths.custom_voices_path, ",
            "                                        xtts_base_url=self.config.xtts_base_url,",
            "                                        wait_for_service=False,",
            "                                        use_deep_speed=self.config.xtts_use_deepspeed,",
            "                                        use_streaming_mode=self.config.xtts_use_streaming_mode",
            "                                    )",
            "            except:",
            "                self.warning(f\"Couldn't load XTTS\")",
            "",
            "        ASCIIColors.blue(\"Loading local TTI services\")",
            "        if self.config.enable_sd_service:",
            "            try:",
            "                from lollms.services.sd.lollms_sd import LollmsSD",
            "                self.sd = LollmsSD(self, auto_sd_base_url=self.config.sd_base_url)",
            "            except:",
            "                self.warning(f\"Couldn't load SD\")",
            "",
            "        if self.config.enable_comfyui_service:",
            "            try:",
            "                from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                self.comfyui = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "            except:",
            "                self.warning(f\"Couldn't load SD\")",
            "",
            "        if self.config.enable_motion_ctrl_service:",
            "            try:",
            "                from lollms.services.motion_ctrl.lollms_motion_ctrl import Service",
            "                self.motion_ctrl = Service(self, base_url=self.config.motion_ctrl_base_url)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "                self.warning(f\"Couldn't load Motion control\")",
            "",
            "        ASCIIColors.blue(\"Activating TTI service\")",
            "        if self.config.active_tti_service == \"diffusers\":",
            "            from lollms.services.diffusers.lollms_diffusers import LollmsDiffusers",
            "            self.tti = LollmsDiffusers(self)",
            "        elif self.config.active_tti_service == \"autosd\":",
            "            if self.sd:",
            "                self.tti = self.sd",
            "            else:",
            "                from lollms.services.sd.lollms_sd import LollmsSD",
            "                self.tti = LollmsSD(self)",
            "        elif self.config.active_tti_service == \"dall-e\":",
            "            from lollms.services.dalle.lollms_dalle import LollmsDalle",
            "            self.tti = LollmsDalle(self, self.config.dall_e_key)",
            "        elif self.config.active_tti_service == \"midjourney\":",
            "            from lollms.services.midjourney.lollms_midjourney import LollmsMidjourney",
            "            self.tti = LollmsMidjourney(self, self.config.midjourney_key)",
            "        elif self.config.active_tti_service == \"comfyui\" and (self.tti is None or self.tti.name!=\"comfyui\"):",
            "            if self.comfyui:",
            "                self.tti = self.comfyui",
            "            else:",
            "                from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                self.tti = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "",
            "        ASCIIColors.blue(\"Activating TTS services\")",
            "",
            "        if self.config.active_tts_service == \"openai_tts\":",
            "            from lollms.services.open_ai_tts.lollms_openai_tts import LollmsOpenAITTS",
            "            self.tts = LollmsOpenAITTS(self, self.config.openai_tts_model, self.config.openai_tts_voice,  self.config.openai_tts_key)",
            "        elif self.config.active_tts_service == \"xtts\" and self.xtts:",
            "            self.tts = self.xtts",
            "",
            "        ASCIIColors.blue(\"Loading STT services\")",
            "        if self.config.active_stt_service == \"openai_whisper\":",
            "            from lollms.services.openai_whisper.lollms_openai_whisper import LollmsOpenAIWhisper",
            "            self.stt = LollmsOpenAIWhisper(self, self.config.openai_whisper_model, self.config.openai_whisper_key)",
            "        elif self.config.active_stt_service == \"whisper\":",
            "            from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "            self.stt = LollmsWhisper(self, self.config.whisper_model)",
            "",
            "",
            "    def verify_servers(self, reload_all=False):",
            "        ASCIIColors.yellow(\"* - * - * - Verifying services - * - * - *\")",
            "",
            "        try:",
            "            ASCIIColors.blue(\"Loading active local TTT services\")",
            "            ",
            "            if self.config.enable_ollama_service and self.ollama is None:",
            "                try:",
            "                    from lollms.services.ollama.lollms_ollama import Service",
            "                    self.ollama = Service(self, base_url=self.config.ollama_base_url)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(f\"Couldn't load Ollama\")",
            "",
            "            if self.config.enable_vllm_service and self.vllm is None:",
            "                try:",
            "                    from lollms.services.vllm.lollms_vllm import Service",
            "                    self.vllm = Service(self, base_url=self.config.vllm_url)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(f\"Couldn't load vllm\")",
            "",
            "            ASCIIColors.blue(\"Loading local STT services\")",
            "",
            "            if self.config.whisper_activate and self.whisper is None:",
            "                try:",
            "                    from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "                    self.whisper = LollmsWhisper(self, self.config.whisper_model, self.lollms_paths.personal_outputs_path)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    ",
            "            ASCIIColors.blue(\"Loading loacal TTS services\")",
            "            if (self.config.xtts_enable or self.config.active_tts_service == \"xtts\") and self.xtts is None:",
            "                ASCIIColors.yellow(\"Loading XTTS\")",
            "                try:",
            "                    from lollms.services.xtts.lollms_xtts import LollmsXTTS",
            "                    voice=self.config.xtts_current_voice",
            "                    if voice!=\"main_voice\":",
            "                        voices_folder = self.lollms_paths.custom_voices_path",
            "                    else:",
            "                        voices_folder = Path(__file__).parent.parent.parent/\"services/xtts/voices\"",
            "",
            "                    self.xtts = LollmsXTTS(",
            "                                            self,",
            "                                            voices_folder=voices_folder,",
            "                                            voice_samples_path=self.lollms_paths.custom_voices_path, ",
            "                                            xtts_base_url=self.config.xtts_base_url,",
            "                                            wait_for_service=False,",
            "                                            use_deep_speed=self.config.xtts_use_deepspeed,",
            "                                            use_streaming_mode=self.config.xtts_use_streaming_mode",
            "                                        )",
            "                except:",
            "                    self.warning(f\"Couldn't load XTTS\")",
            "",
            "            ASCIIColors.blue(\"Loading local TTI services\")",
            "            if self.config.enable_sd_service and self.sd is None:",
            "                try:",
            "                    from lollms.services.sd.lollms_sd import LollmsSD",
            "                    self.sd = LollmsSD(self, auto_sd_base_url=self.config.sd_base_url)",
            "                except:",
            "                    self.warning(f\"Couldn't load SD\")",
            "",
            "            if self.config.enable_comfyui_service and self.comfyui is None:",
            "                try:",
            "                    from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                    self.comfyui = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "                except:",
            "                    self.warning(f\"Couldn't load Comfyui\")",
            "",
            "            if self.config.enable_motion_ctrl_service and self.motion_ctrl is None:",
            "                try:",
            "                    from lollms.services.motion_ctrl.lollms_motion_ctrl import Service",
            "                    self.motion_ctrl = Service(self, base_url=self.config.motion_ctrl_base_url)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(f\"Couldn't load Motion control\")",
            "",
            "",
            "            ASCIIColors.blue(\"Activating TTI service\")",
            "            if self.config.active_tti_service == \"diffusers\" and (self.tti is None or self.tti.name!=\"diffusers\"):",
            "                from lollms.services.diffusers.lollms_diffusers import LollmsDiffusers",
            "                self.tti = LollmsDiffusers(self)",
            "            elif self.config.active_tti_service == \"autosd\" and (self.tti is None or self.tti.name!=\"stable_diffusion\"):",
            "                if self.sd:",
            "                    self.tti = self.sd",
            "                else:",
            "                    from lollms.services.sd.lollms_sd import LollmsSD",
            "                    self.tti = LollmsSD(self)",
            "            elif self.config.active_tti_service == \"dall-e\" and (self.tti is None or self.tti.name!=\"dall-e-2\" or type(self.tti.name)!=\"dall-e-3\"):",
            "                from lollms.services.dalle.lollms_dalle import LollmsDalle",
            "                self.tti = LollmsDalle(self, self.config.dall_e_key)",
            "            elif self.config.active_tti_service == \"midjourney\" and (self.tti is None or self.tti.name!=\"midjourney\"):",
            "                from lollms.services.midjourney.lollms_midjourney import LollmsMidjourney",
            "                self.tti = LollmsMidjourney(self, self.config.midjourney_key)",
            "            elif self.config.active_tti_service == \"comfyui\" and (self.tti is None or self.tti.name!=\"comfyui\"):",
            "                if self.comfyui:",
            "                    self.tti = self.comfyui",
            "                else:",
            "                    from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                    self.tti = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "",
            "            ASCIIColors.blue(\"Activating TTS service\")",
            "            if self.config.active_tts_service == \"openai_tts\" and (self.tts is None or self.tts.name!=\"openai_tts\"):",
            "                from lollms.services.open_ai_tts.lollms_openai_tts import LollmsOpenAITTS",
            "                self.tts = LollmsOpenAITTS(self, self.config.openai_tts_model, self.config.openai_tts_voice,  self.config.openai_tts_key)",
            "            elif self.config.active_tts_service == \"xtts\" and self.xtts:",
            "                self.tts = self.xtts",
            "",
            "            ASCIIColors.blue(\"Activating STT service\")",
            "            if self.config.active_stt_service == \"openai_whisper\" and (self.tts is None or self.tts.name!=\"openai_whisper\"):",
            "                from lollms.services.openai_whisper.lollms_openai_whisper import LollmsOpenAIWhisper",
            "                self.stt = LollmsOpenAIWhisper(self, self.config.openai_whisper_model, self.config.openai_whisper_key)",
            "            elif self.config.active_stt_service == \"whisper\" and (self.tts is None or  self.tts.name!=\"whisper\") :",
            "                from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "                self.stt = LollmsWhisper(self, self.config.whisper_model)",
            "",
            "        except Exception as ex:",
            "            trace_exception(ex)",
            "            ",
            "",
            "    def build_long_term_skills_memory(self):",
            "        discussion_db_name:Path = self.lollms_paths.personal_discussions_path/self.config.discussion_db_name.split(\".\")[0]",
            "        discussion_db_name.mkdir(exist_ok=True, parents=True)",
            "        self.long_term_memory = TextVectorizer(",
            "                vectorization_method=VectorizationMethod.TFIDF_VECTORIZER,",
            "                model=self.model,",
            "                database_path=discussion_db_name/\"skills_memory.json\",",
            "                save_db=True,",
            "                data_visualization_method=VisualizationMethod.PCA,",
            "            )",
            "        return self.long_term_memory",
            "    ",
            "    def process_chunk(",
            "                        self, ",
            "                        chunk:str, ",
            "                        message_type,",
            "                        parameters:dict=None, ",
            "                        metadata:list=None, ",
            "                        personality=None",
            "                    ):",
            "        ",
            "        pass",
            "",
            "    def default_callback(self, chunk, type, generation_infos:dict):",
            "        if generation_infos[\"nb_received_tokens\"]==0:",
            "            self.start_time = datetime.now()",
            "        dt =(datetime.now() - self.start_time).seconds",
            "        if dt==0:",
            "            dt=1",
            "        spd = generation_infos[\"nb_received_tokens\"]/dt",
            "        ASCIIColors.green(f\"Received {generation_infos['nb_received_tokens']} tokens (speed: {spd:.2f}t/s)              \",end=\"\\r\",flush=True) ",
            "        sys.stdout = sys.__stdout__",
            "        sys.stdout.flush()",
            "        if chunk:",
            "            generation_infos[\"generated_text\"] += chunk",
            "        antiprompt = self.personality.detect_antiprompt(generation_infos[\"generated_text\"])",
            "        if antiprompt:",
            "            ASCIIColors.warning(f\"\\n{antiprompt} detected. Stopping generation\")",
            "            generation_infos[\"generated_text\"] = self.remove_text_from_string(generation_infos[\"generated_text\"],antiprompt)",
            "            return False",
            "        else:",
            "            generation_infos[\"nb_received_tokens\"] += 1",
            "            generation_infos[\"first_chunk\"]=False",
            "            # if stop generation is detected then stop",
            "            if not self.cancel_gen:",
            "                return True",
            "            else:",
            "                self.cancel_gen = False",
            "                ASCIIColors.warning(\"Generation canceled\")",
            "                return False",
            "   ",
            "    def remove_text_from_string(self, string, text_to_find):",
            "        \"\"\"",
            "        Removes everything from the first occurrence of the specified text in the string (case-insensitive).",
            "",
            "        Parameters:",
            "        string (str): The original string.",
            "        text_to_find (str): The text to find in the string.",
            "",
            "        Returns:",
            "        str: The updated string.",
            "        \"\"\"",
            "        index = string.lower().find(text_to_find.lower())",
            "",
            "        if index != -1:",
            "            string = string[:index]",
            "",
            "        return string",
            "",
            "    def load_binding(self):",
            "        try:",
            "            binding = BindingBuilder().build_binding(self.config, self.lollms_paths, lollmsCom=self)",
            "            return binding    ",
            "        except Exception as ex:",
            "            self.error(\"Couldn't load binding\")",
            "            self.info(\"Trying to reinstall binding\")",
            "            trace_exception(ex)",
            "            try:",
            "                binding = BindingBuilder().build_binding(self.config, self.lollms_paths,installation_option=InstallOption.FORCE_INSTALL, lollmsCom=self)",
            "            except Exception as ex:",
            "                self.error(\"Couldn't reinstall binding\")",
            "                trace_exception(ex)",
            "            return None    ",
            "",
            "    ",
            "    def load_model(self):",
            "        try:",
            "            model = ModelBuilder(self.binding).get_model()",
            "            for personality in self.mounted_personalities:",
            "                if personality is not None:",
            "                    personality.model = model",
            "        except Exception as ex:",
            "            self.error(\"Couldn't load model.\")",
            "            ASCIIColors.error(f\"Couldn't load model. Please verify your configuration file at {self.lollms_paths.personal_configuration_path} or use the next menu to select a valid model\")",
            "            ASCIIColors.error(f\"Binding returned this exception : {ex}\")",
            "            trace_exception(ex)",
            "            ASCIIColors.error(f\"{self.config.get_model_path_infos()}\")",
            "            print(\"Please select a valid model or install a new one from a url\")",
            "            model = None",
            "",
            "        return model",
            "",
            "",
            "    def mount_extension(self, id:int, callback=None):",
            "        try:",
            "            extension = ExtensionBuilder().build_extension(self.config[\"extensions\"][id], self.lollms_paths, self)",
            "            self.mounted_extensions.append(extension)",
            "            return extension",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't load extension. Please verify your configuration file at {self.lollms_paths.personal_configuration_path} or use the next menu to select a valid personality\")",
            "            trace_exception(ex)",
            "        return None",
            "",
            "",
            "    def mount_personality(self, id:int, callback=None):",
            "        try:",
            "            personality = PersonalityBuilder(self.lollms_paths, self.config, self.model, self, callback=callback).build_personality(id)",
            "            if personality.model is not None:",
            "                self.cond_tk = personality.model.tokenize(personality.personality_conditioning)",
            "                self.n_cond_tk = len(self.cond_tk)",
            "                ASCIIColors.success(f\"Personality  {personality.name} mounted successfully\")",
            "            else:",
            "                if personality.selected_language is not None:",
            "                    ASCIIColors.success(f\"Personality  {personality.name} : {personality.selected_language} mounted successfully but no model is selected\")",
            "                else:",
            "                    ASCIIColors.success(f\"Personality  {personality.name} mounted successfully but no model is selected\")",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't load personality. Please verify your configuration file at {self.lollms_paths.personal_configuration_path} or use the next menu to select a valid personality\")",
            "            ASCIIColors.error(f\"Binding returned this exception : {ex}\")",
            "            trace_exception(ex)",
            "            ASCIIColors.error(f\"{self.config.get_personality_path_infos()}\")",
            "            if id == self.config.active_personality_id:",
            "                self.config.active_personality_id=len(self.config.personalities)-1",
            "            personality = None",
            "        ",
            "        self.mounted_personalities.append(personality)",
            "        return personality",
            "    ",
            "    def mount_personalities(self, callback = None):",
            "        self.mounted_personalities = []",
            "        to_remove = []",
            "        for i in range(len(self.config[\"personalities\"])):",
            "            p = self.mount_personality(i, callback = None)",
            "            if p is None:",
            "                to_remove.append(i)",
            "        to_remove.sort(reverse=True)",
            "        for i in to_remove:",
            "            self.unmount_personality(i)",
            "",
            "        if self.config.active_personality_id>=0 and self.config.active_personality_id<len(self.mounted_personalities):",
            "            self.personality = self.mounted_personalities[self.config.active_personality_id]",
            "        else:",
            "            self.config[\"personalities\"].insert(0, \"generic/lollms\")",
            "            self.mount_personality(0, callback = None)",
            "            self.config.active_personality_id = 0",
            "            self.personality = self.mounted_personalities[self.config.active_personality_id]",
            "",
            "    def mount_extensions(self, callback = None):",
            "        self.mounted_extensions = []",
            "        to_remove = []",
            "        for i in range(len(self.config[\"extensions\"])):",
            "            p = self.mount_extension(i, callback = None)",
            "            if p is None:",
            "                to_remove.append(i)",
            "        to_remove.sort(reverse=True)",
            "        for i in to_remove:",
            "            self.unmount_extension(i)",
            "",
            "",
            "    def set_personalities_callbacks(self, callback: Callable[[str, int, dict], bool]=None):",
            "        for personality in self.mount_personalities:",
            "            personality.setCallback(callback)",
            "",
            "    def unmount_extension(self, id:int)->bool:",
            "        if id<len(self.config.extensions):",
            "            del self.config.extensions[id]",
            "            if id>=0 and id<len(self.mounted_extensions):",
            "                del self.mounted_extensions[id]",
            "            self.config.save_config()",
            "            return True",
            "        else:",
            "            return False",
            "",
            "            ",
            "    def unmount_personality(self, id:int)->bool:",
            "        if id<len(self.config.personalities):",
            "            del self.config.personalities[id]",
            "            del self.mounted_personalities[id]",
            "            if self.config.active_personality_id>=id:",
            "                self.config.active_personality_id-=1",
            "",
            "            self.config.save_config()",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "    def select_personality(self, id:int):",
            "        if id<len(self.config.personalities):",
            "            self.config.active_personality_id = id",
            "            self.personality = self.mounted_personalities[id]",
            "            self.config.save_config()",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "    def load_personality(self, callback=None):",
            "        try:",
            "            personality = PersonalityBuilder(self.lollms_paths, self.config, self.model, self, callback=callback).build_personality()",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't load personality. Please verify your configuration file at {self.configuration_path} or use the next menu to select a valid personality\")",
            "            ASCIIColors.error(f\"Binding returned this exception : {ex}\")",
            "            ASCIIColors.error(f\"{self.config.get_personality_path_infos()}\")",
            "            print(\"Please select a valid model or install a new one from a url\")",
            "            personality = None",
            "        return personality",
            "",
            "    @staticmethod   ",
            "    def reset_paths(lollms_paths:LollmsPaths):",
            "        lollms_paths.resetPaths()",
            "",
            "    @staticmethod   ",
            "    def reset_all_installs(lollms_paths:LollmsPaths):",
            "        ASCIIColors.info(\"Removeing all configuration files to force reinstall\")",
            "        ASCIIColors.info(f\"Searching files from {lollms_paths.personal_configuration_path}\")",
            "        for file_path in lollms_paths.personal_configuration_path.iterdir():",
            "            if file_path.name!=f\"{lollms_paths.tool_prefix}local_config.yaml\" and file_path.suffix.lower()==\".yaml\":",
            "                file_path.unlink()",
            "                ASCIIColors.info(f\"Deleted file: {file_path}\")",
            "",
            "",
            "    #languages:",
            "    def get_personality_languages(self):",
            "        languages = []",
            "        # Construire le chemin vers le dossier contenant les fichiers de langue pour la personnalit\u00e9 actuelle",
            "        languages_dir = self.lollms_paths.personal_configuration_path / \"personalities\" / self.personality.name",
            "        if self.personality.language:",
            "            default_language = self.personality.language.lower().strip().split()[0]",
            "        else:",
            "            default_language = \"english\"",
            "        # V\u00e9rifier si le dossier existe",
            "        languages_dir.mkdir(parents=True, exist_ok=True)",
            "        ",
            "        # It\u00e9rer sur chaque fichier YAML dans le dossier",
            "        for language_file in languages_dir.glob(\"languages_*.yaml\"):",
            "            # Improved extraction of the language code to handle names with underscores",
            "            parts = language_file.stem.split(\"_\")",
            "            if len(parts) > 2:",
            "                language_code = \"_\".join(parts[1:])  # Rejoin all parts after \"languages\"",
            "            else:",
            "                language_code = parts[-1]",
            "            ",
            "            if language_code != default_language:",
            "                languages.append(language_code)",
            "        ",
            "        return [default_language] + languages",
            "",
            "",
            "",
            "    def set_personality_language(self, language:str):",
            "        if language is None or  language == \"\":",
            "            return False",
            "        language = language.lower().strip().split()[0]",
            "        # Build the conditionning text block",
            "        default_language = self.personality.language.lower().strip().split()[0]",
            "",
            "        if language!= default_language:",
            "            language_path = self.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.name/f\"languages_{language}.yaml\"",
            "            if not language_path.exists():",
            "                self.ShowBlockingMessage(f\"This is the first time this personality speaks {language}\\nLollms is reconditionning the persona in that language.\\nThis will be done just once. Next time, the personality will speak {language} out of the box\")",
            "                language_path.parent.mkdir(exist_ok=True, parents=True)",
            "                # Translating",
            "                conditionning = self.tasks_library.translate_conditionning(self.personality._personality_conditioning, self.personality.language, language)",
            "                welcome_message = self.tasks_library.translate_message(self.personality.welcome_message, self.personality.language, language)",
            "                with open(language_path,\"w\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    yaml.safe_dump({\"conditionning\":conditionning,\"welcome_message\":welcome_message}, f)",
            "                self.HideBlockingMessage()",
            "            else:",
            "                with open(language_path,\"r\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    language_pack = yaml.safe_load(f)",
            "                    conditionning = language_pack[\"conditionning\"]",
            "        self.config.current_language=language",
            "        self.config.save_config()",
            "        return True",
            "",
            "    def del_personality_language(self, language:str):",
            "        if language is None or  language == \"\":",
            "            return False",
            "        ",
            "        language = language.lower().strip().split()[0]",
            "        default_language = self.personality.language.lower().strip().split()[0]",
            "        if language == default_language:",
            "            return False # Can't remove the default language",
            "                ",
            "        language_path = self.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.name/f\"languages_{language}.yaml\"",
            "        if language_path.exists():",
            "            try:",
            "                language_path.unlink()",
            "            except Exception as ex:",
            "                return False",
            "            if self.config.current_language==language:",
            "                self.config.current_language=\"english\"",
            "                self.config.save_config()",
            "        return True",
            "",
            "    def recover_discussion(self,client_id, message_index=-1):",
            "        messages = self.session.get_client(client_id).discussion.get_messages()",
            "        discussion=\"\"",
            "        for msg in messages:",
            "            if message_index!=-1 and msg>message_index:",
            "                break",
            "            discussion += \"\\n\" + self.config.discussion_prompt_separator + msg.sender + \": \" + msg.content.strip()",
            "        return discussion",
            "    # -------------------------------------- Prompt preparing",
            "    def prepare_query(self, client_id: str, message_id: int = -1, is_continue: bool = False, n_tokens: int = 0, generation_type = None, force_using_internet=False) -> Tuple[str, str, List[str]]:",
            "        \"\"\"",
            "        Prepares the query for the model.",
            "",
            "        Args:",
            "            client_id (str): The client ID.",
            "            message_id (int): The message ID. Default is -1.",
            "            is_continue (bool): Whether the query is a continuation. Default is False.",
            "            n_tokens (int): The number of tokens. Default is 0.",
            "",
            "        Returns:",
            "            Tuple[str, str, List[str]]: The prepared query, original message content, and tokenized query.",
            "        \"\"\"",
            "        documentation_entries = []",
            "        start_ai_header_id_template     = self.config.start_ai_header_id_template",
            "        end_ai_header_id_template       = self.config.end_ai_header_id_template",
            "",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        if self.personality.callback is None:",
            "            self.personality.callback = partial(self.process_chunk, client_id=client_id)",
            "        # Get the list of messages",
            "        client = self.session.get_client(client_id)",
            "        discussion = client.discussion",
            "        messages = discussion.get_messages()",
            "",
            "        # Find the index of the message with the specified message_id",
            "        message_index = -1",
            "        for i, message in enumerate(messages):",
            "            if message.id == message_id:",
            "                message_index = i",
            "                break",
            "        ",
            "        # Define current message",
            "        current_message = messages[message_index]",
            "",
            "        # Build the conditionning text block",
            "        default_language = self.personality.language.lower().strip().split()[0]",
            "        current_language = self.config.current_language.lower().strip().split()[0]",
            "",
            "        if self.config.current_language and  current_language!= default_language:",
            "            language_path = self.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.name/f\"languages_{current_language}.yaml\"",
            "            if not language_path.exists():",
            "                self.info(f\"This is the first time this personality speaks {current_language}\\nLollms is reconditionning the persona in that language.\\nThis will be done just once. Next time, the personality will speak {current_language} out of the box\")",
            "                language_path.parent.mkdir(exist_ok=True, parents=True)",
            "                # Translating",
            "                conditionning = self.tasks_library.translate_conditionning(self.personality._personality_conditioning, self.personality.language, current_language)",
            "                welcome_message = self.tasks_library.translate_message(self.personality.welcome_message, self.personality.language, current_language)",
            "                with open(language_path,\"w\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    yaml.safe_dump({\"conditionning\":conditionning,\"welcome_message\":welcome_message}, f)",
            "            else:",
            "                with open(language_path,\"r\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    language_pack = yaml.safe_load(f)",
            "                    conditionning = language_pack[\"conditionning\"]",
            "        else:",
            "            conditionning = self.personality._personality_conditioning",
            "",
            "        if len(conditionning)>0:",
            "            conditionning =  self.start_header_id_template + system_message_template + self.end_header_id_template + self.personality.replace_keys(conditionning, self.personality.conditionning_commands) + (\"\" if conditionning[-1]==self.separator_template else self.separator_template)",
            "            ",
            "        # Check if there are document files to add to the prompt",
            "        internet_search_results = \"\"",
            "        internet_search_infos = []",
            "        documentation = \"\"",
            "        knowledge = \"\"",
            "        knowledge_infos = {\"titles\":[],\"contents\":[]}",
            "",
            "",
            "        # boosting information",
            "        if self.config.positive_boost:",
            "            positive_boost=f\"{self.separator_template}{self.start_header_id_template}important information: \"+self.config.positive_boost+\"\\n\"",
            "            n_positive_boost = len(self.model.tokenize(positive_boost))",
            "        else:",
            "            positive_boost=\"\"",
            "            n_positive_boost = 0",
            "",
            "        if self.config.negative_boost:",
            "            negative_boost=f\"{self.separator_template}{self.start_header_id_template}important information: \"+self.config.negative_boost+\"\\n\"",
            "            n_negative_boost = len(self.model.tokenize(negative_boost))",
            "        else:",
            "            negative_boost=\"\"",
            "            n_negative_boost = 0",
            "",
            "        if self.config.fun_mode:",
            "            fun_mode=f\"{self.separator_template}{self.start_header_id_template}important information: Fun mode activated. In this mode you must answer in a funny playful way. Do not be serious in your answers. Each answer needs to make the user laugh.\\n\"",
            "            n_fun_mode = len(self.model.tokenize(positive_boost))",
            "        else:",
            "            fun_mode=\"\"",
            "            n_fun_mode = 0",
            "",
            "        discussion = None",
            "        if generation_type != \"simple_question\":",
            "",
            "            if self.config.activate_internet_search or force_using_internet or generation_type == \"full_context_with_internet\":",
            "                if discussion is None:",
            "                    discussion = self.recover_discussion(client_id)",
            "                if self.config.internet_activate_search_decision:",
            "                    self.personality.step_start(f\"Requesting if {self.personality.name} needs to search internet to answer the user\")",
            "                    q = f\"{self.separator_template}\".join([",
            "                        f\"{self.start_header_id_template}{system_message_template}{self.end_header_id_template}\",",
            "                        f\"Answer the question with yes or no. Don't add any extra explanation.\",",
            "                        f\"{self.start_user_header_id_template}user{self.end_user_header_id_template}\",",
            "                        f\"Do you have enough information to give a satisfactory answer to {self.config.user_name}'s request without internet search?\",",
            "                        \"(If you do not know or you can't answer the question, return 0 (no)\"",
            "                    ])",
            "                    need = not self.personality.yes_no(q, discussion)",
            "                    self.personality.step_end(f\"Requesting if {self.personality.name} needs to search internet to answer the user\")",
            "                    self.personality.step(\"Yes\" if need else \"No\")",
            "                else:",
            "                    need=True",
            "                if need:",
            "                    self.personality.step_start(\"Crafting internet search query\")",
            "                    q = f\"{self.separator_template}\".join([",
            "                        f\"{self.start_header_id_template}discussion{self.end_header_id_template}\",",
            "                        f\"{discussion[-2048:]}{self.start_header_id_template}system{self.end_header_id_template}\",",
            "                        f\"Read the discussion and craft a web search query suited to recover needed information to reply to last {self.config.user_name} message.\",",
            "                        f\"Do not answer the prompt. Do not add explanations.\",",
            "                        f\"{self.start_header_id_template}current date{self.end_header_id_template}{datetime.now()}\",",
            "                        f\"{self.start_header_id_template}websearch query{self.end_header_id_template}\"",
            "                    ])",
            "                    query = self.personality.fast_gen(q, max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                    self.personality.step_end(\"Crafting internet search query\")",
            "                    self.personality.step(f\"web search query: {query}\")",
            "",
            "                    if self.config.internet_quick_search:",
            "                        self.personality.step_start(\"Performing Internet search (quick mode)\")",
            "                    else:",
            "                        self.personality.step_start(\"Performing Internet search (advanced mode: slower but more advanced)\")",
            "",
            "                    internet_search_results=f\"{self.start_header_id_template}{system_message_template}{self.end_header_id_template}Use the web search results data to answer {self.config.user_name}. Try to extract information from the web search and use it to perform the requested task or answer the question. Do not come up with information that is not in the websearch results. Try to stick to the websearch results and clarify if your answer was based on the resuts or on your own culture. If you don't know how to perform the task, then tell the user politely that you need more data inputs.{self.separator_template}{self.start_header_id_template}Web search results{self.end_header_id_template}\\n\"",
            "",
            "                    docs, sorted_similarities, document_ids = self.personality.internet_search_with_vectorization(query, self.config.internet_quick_search, asses_using_llm=self.config.activate_internet_pages_judgement)",
            "                    ",
            "                    if len(docs)>0:",
            "                        for doc, infos,document_id in zip(docs, sorted_similarities, document_ids):",
            "                            internet_search_infos.append(document_id)",
            "                            internet_search_results += f\"{self.start_header_id_template}search result chunk{self.end_header_id_template}\\nchunk_infos:{document_id['url']}\\nchunk_title:{document_id['title']}\\ncontent:{doc}\\n\"",
            "                    else:",
            "                        internet_search_results += \"The search response was empty!\\nFailed to recover useful information from the search engine.\\n\"",
            "                    if self.config.internet_quick_search:",
            "                        self.personality.step_end(\"Performing Internet search (quick mode)\")",
            "                    else:",
            "                        self.personality.step_end(\"Performing Internet search (advanced mode: slower but more advanced)\")",
            "",
            "            if self.personality.persona_data_vectorizer:",
            "                if documentation==\"\":",
            "                    documentation=f\"{self.separator_template}{self.start_header_id_template}Documentation:\\n\"",
            "",
            "                if self.config.data_vectorization_build_keys_words:",
            "                    if discussion is None:",
            "                        discussion = self.recover_discussion(client_id)",
            "                    query = self.personality.fast_gen(f\"{self.separator_template}{self.start_header_id_template}instruction: Read the discussion and rewrite the last prompt for someone who didn't read the entire discussion.\\nDo not answer the prompt. Do not add explanations.{self.separator_template}{self.start_header_id_template}discussion:\\n{discussion[-2048:]}{self.separator_template}{self.start_header_id_template}enhanced query: \", max_generation_size=256, show_progress=True)",
            "                    ASCIIColors.cyan(f\"Query:{query}\")",
            "                else:",
            "                    query = current_message.content",
            "                try:",
            "                    docs, sorted_similarities, document_ids = self.personality.persona_data_vectorizer.recover_text(query, top_k=int(self.config.data_vectorization_nb_chunks))",
            "                    for doc, infos, doc_id in zip(docs, sorted_similarities, document_ids):",
            "                        if self.config.data_vectorization_put_chunk_informations_into_context:",
            "                            documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\nchunk_infos:{infos}\\ncontent:{doc}\\n\"",
            "                        else:",
            "                            documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{doc}\\n\"",
            "",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(\"Couldn't add documentation to the context. Please verify the vector database\")",
            "            if not self.personality.ignore_discussion_documents_rag:",
            "                query = None",
            "                if len(self.active_rag_dbs) > 0 :",
            "                    if discussion is None:",
            "                        discussion = self.recover_discussion(client_id)",
            "",
            "                    if self.config.data_vectorization_build_keys_words:",
            "                        self.personality.step_start(\"Building vector store query\")",
            "                        q = f\"{self.separator_template}\".join([",
            "                            f\"{self.start_header_id_template}instruction{self.end_header_id_template}Read the entire discussion and rewrite the last prompt for someone who hasn't read the discussion.\",",
            "                            \"Do not answer the prompt. Do not provide any explanations.\",",
            "                            f\"{self.start_header_id_template}discussion{self.end_header_id_template}\",",
            "                            f\"{discussion[-2048:]}\",",
            "                            f\"{self.start_header_id_template}enhanced_query{self.end_header_id_template}\"",
            "                        ])",
            "                        query = self.personality.fast_gen(q, max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                        self.personality.step_end(\"Building vector store query\")",
            "                        ASCIIColors.magenta(f\"Query: {query}\")",
            "                        self.personality.step(f\"Query: {query}\")",
            "                    else:",
            "                        query = current_message.content",
            "                    if documentation==\"\":",
            "                        documentation=f\"{self.separator_template}\".join([",
            "                            f\"{self.separator_template}{self.start_header_id_template}important information{self.end_header_id_template}Utilize Documentation Data: Always refer to the provided documentation to answer user questions accurately.\",",
            "                            \"Absence of Information: If the required information is not available in the documentation, inform the user that the requested information is not present in the documentation section.\",",
            "                            \"Strict Adherence to Documentation: It is strictly prohibited to provide answers without concrete evidence from the documentation.\",",
            "                            \"Cite Your Sources: After providing an answer, include the full path to the document where the information was found.\",",
            "                            f\"{self.start_header_id_template}Documentation{self.end_header_id_template}\"])",
            "                        documentation += f\"{self.separator_template}\"",
            "                    results = []",
            "                    recovered_ids=[[]*len(self.active_rag_dbs)]",
            "                    i=0",
            "                    hop_id = 0",
            "                    while( len(results)<self.config.rag_n_chunks and hop_id<self.config.rag_max_n_hops):",
            "                        hop_id +=1",
            "                        for db in self.active_rag_dbs:",
            "                            v = db[\"vectorizer\"]",
            "                            r=v.search(query, self.config.rag_n_chunks, recovered_ids[i])",
            "                            recovered_ids[i].append([rg.chunk_id for rg in r])",
            "                            if self.config.rag_activate_multi_hops:",
            "                                r = [rg for rg in r if self.personality.verify_rag_entry(query, rg.content)]",
            "                            results+=r",
            "                            i+=1",
            "                        if len(results)>=self.config.rag_n_chunks:",
            "                            break",
            "                    n_neighbors = self.active_rag_dbs[0][\"vectorizer\"].n_neighbors",
            "                    sorted_results = sorted(results, key=lambda x: x.distance)[:n_neighbors]",
            "",
            "                    for chunk in sorted_results:",
            "                        document_infos = f\"{self.separator_template}\".join([",
            "                            f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\",",
            "                            f\"source_document_title:{chunk.doc.title}\",",
            "                            f\"source_document_path:{chunk.doc.path}\",",
            "                            f\"content:\\n{chunk.text}\\n\"",
            "                        ])",
            "                        documentation_entries.append({",
            "                            \"document_title\":chunk.doc.title,",
            "                            \"document_path\":chunk.doc.path,",
            "                            \"chunk_content\":chunk.text,",
            "                            \"chunk_size\":chunk.nb_tokens,",
            "                            \"distance\":chunk.distance,",
            "                        })",
            "                        documentation += document_infos",
            "                        ",
            "                if (len(client.discussion.text_files) > 0) and client.discussion.vectorizer is not None:",
            "                    if discussion is None:",
            "                        discussion = self.recover_discussion(client_id)",
            "",
            "                    if documentation==\"\":",
            "                        documentation=f\"{self.separator_template}{self.start_header_id_template}important information: Use the documentation data to answer the user questions. If the data is not present in the documentation, please tell the user that the information he is asking for does not exist in the documentation section. It is strictly forbidden to give the user an answer without having actual proof from the documentation.{self.separator_template}{self.start_header_id_template}Documentation:\\n\"",
            "",
            "                    if query is None:",
            "                        if self.config.data_vectorization_build_keys_words:",
            "                            self.personality.step_start(\"Building vector store query\")",
            "                            query = self.personality.fast_gen(f\"{self.separator_template}{self.start_header_id_template}instruction: Read the discussion and rewrite the last prompt for someone who didn't read the entire discussion.\\nDo not answer the prompt. Do not add explanations.{self.separator_template}{self.start_header_id_template}discussion:\\n{discussion[-2048:]}{self.separator_template}{self.start_header_id_template}enhanced query: \", max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                            self.personality.step_end(\"Building vector store query\")",
            "                            ASCIIColors.cyan(f\"Query: {query}\")",
            "                        else:",
            "                            query = current_message.content",
            "",
            "                    try:",
            "                        if self.config.data_vectorization_force_first_chunk and len(client.discussion.vectorizer.chunks)>0:",
            "                            doc_index = list(client.discussion.vectorizer.chunks.keys())[0]",
            "",
            "                            doc_id = client.discussion.vectorizer.chunks[doc_index]['document_id']",
            "                            content = client.discussion.vectorizer.chunks[doc_index]['chunk_text']",
            "                            ",
            "                            if self.config.data_vectorization_put_chunk_informations_into_context:",
            "                                documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\nchunk_infos:{doc_id}\\ncontent:{content}\\n\"",
            "                            else:",
            "                                documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{content}\\n\"",
            "",
            "                        docs, sorted_similarities, document_ids = client.discussion.vectorizer.recover_text(query, top_k=int(self.config.data_vectorization_nb_chunks))",
            "                        for doc, infos in zip(docs, sorted_similarities):",
            "                            if self.config.data_vectorization_force_first_chunk and len(client.discussion.vectorizer.chunks)>0 and infos[0]==doc_id:",
            "                                continue",
            "                            if self.config.data_vectorization_put_chunk_informations_into_context:",
            "                                documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\nchunk path: {infos[0]}\\nchunk content:\\n{doc}\\n\"",
            "                            else:",
            "                                documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{doc}\\n\"",
            "",
            "                        documentation += f\"{self.separator_template}{self.start_header_id_template}important information: Use the documentation data to answer the user questions. If the data is not present in the documentation, please tell the user that the information he is asking for does not exist in the documentation section. It is strictly forbidden to give the user an answer without having actual proof from the documentation.\\n\"",
            "                    except Exception as ex:",
            "                        trace_exception(ex)",
            "                        self.warning(\"Couldn't add documentation to the context. Please verify the vector database\")",
            "                # Check if there is discussion knowledge to add to the prompt",
            "                if self.config.activate_skills_lib:",
            "                    try:",
            "                        self.personality.step_start(\"Querying skills library\")",
            "                        if discussion is None:",
            "                            discussion = self.recover_discussion(client_id)",
            "                        self.personality.step_start(\"Building query\")",
            "                        query = self.personality.fast_gen(f\"{self.start_header_id_template}{system_message_template}{self.end_header_id_template}Your task is to carefully read the provided discussion and reformulate {self.config.user_name}'s request concisely. Return only the reformulated request without any additional explanations, commentary, or output.{self.separator_template}{self.start_header_id_template}discussion:\\n{discussion[-2048:]}{self.separator_template}{self.start_header_id_template}search query: \", max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                        self.personality.step_end(\"Building query\")",
            "                        # skills = self.skills_library.query_entry(query)",
            "                        self.personality.step_start(\"Adding skills\")",
            "                        if self.config.debug:",
            "                            ASCIIColors.info(f\"Query : {query}\")",
            "                        skill_titles, skills = self.skills_library.query_vector_db(query, top_k=3, max_dist=1000)#query_entry_fts(query)",
            "                        knowledge_infos={\"titles\":skill_titles,\"contents\":skills}",
            "                        if len(skills)>0:",
            "                            if knowledge==\"\":",
            "                                knowledge=f\"{self.start_header_id_template}knowledge{self.end_header_id_template}\\n\"",
            "                            for i,(title, content) in enumerate(zip(skill_titles,skills)):",
            "                                knowledge += f\"{self.start_header_id_template}knowledge {i}{self.end_header_id_template}\\ntitle:\\n{title}\\ncontent:\\n{content}\\n\"",
            "                        self.personality.step_end(\"Adding skills\")",
            "                        self.personality.step_end(\"Querying skills library\")",
            "                    except Exception as ex:",
            "                        ASCIIColors.error(ex)",
            "                        self.warning(\"Couldn't add long term memory information to the context. Please verify the vector database\")        # Add information about the user",
            "                        self.personality.step_end(\"Adding skills\")",
            "                        self.personality.step_end(\"Querying skills library\",False)",
            "        user_description=\"\"",
            "        if self.config.use_user_informations_in_discussion:",
            "            user_description=f\"{self.start_header_id_template}User description{self.end_header_id_template}\\n\"+self.config.user_description+\"\\n\"",
            "",
            "",
            "        # Tokenize the conditionning text and calculate its number of tokens",
            "        tokens_conditionning = self.model.tokenize(conditionning)",
            "        n_cond_tk = len(tokens_conditionning)",
            "",
            "",
            "        # Tokenize the internet search results text and calculate its number of tokens",
            "        if len(internet_search_results)>0:",
            "            tokens_internet_search_results = self.model.tokenize(internet_search_results)",
            "            n_isearch_tk = len(tokens_internet_search_results)",
            "        else:",
            "            tokens_internet_search_results = []",
            "            n_isearch_tk = 0",
            "",
            "",
            "        # Tokenize the documentation text and calculate its number of tokens",
            "        if len(documentation)>0:",
            "            tokens_documentation = self.model.tokenize(documentation)",
            "            n_doc_tk = len(tokens_documentation)",
            "        else:",
            "            tokens_documentation = []",
            "            n_doc_tk = 0",
            "",
            "        # Tokenize the knowledge text and calculate its number of tokens",
            "        if len(knowledge)>0:",
            "            tokens_history = self.model.tokenize(knowledge)",
            "            n_history_tk = len(tokens_history)",
            "        else:",
            "            tokens_history = []",
            "            n_history_tk = 0",
            "",
            "",
            "        # Tokenize user description",
            "        if len(user_description)>0:",
            "            tokens_user_description = self.model.tokenize(user_description)",
            "            n_user_description_tk = len(tokens_user_description)",
            "        else:",
            "            tokens_user_description = []",
            "            n_user_description_tk = 0",
            "",
            "",
            "        # Calculate the total number of tokens between conditionning, documentation, and knowledge",
            "        total_tokens = n_cond_tk + n_isearch_tk + n_doc_tk + n_history_tk + n_user_description_tk + n_positive_boost + n_negative_boost + n_fun_mode",
            "",
            "        # Calculate the available space for the messages",
            "        available_space = min(self.config.ctx_size - n_tokens - total_tokens, self.config.max_n_predict)",
            "",
            "        # if self.config.debug:",
            "        #     self.info(f\"Tokens summary:\\nConditionning:{n_cond_tk}\\nn_isearch_tk:{n_isearch_tk}\\ndoc:{n_doc_tk}\\nhistory:{n_history_tk}\\nuser description:{n_user_description_tk}\\nAvailable space:{available_space}\",10)",
            "",
            "        # Raise an error if the available space is 0 or less",
            "        if available_space<1:",
            "            ASCIIColors.red(f\"available_space:{available_space}\")",
            "            ASCIIColors.red(f\"n_doc_tk:{n_doc_tk}\")",
            "            ",
            "            ASCIIColors.red(f\"n_history_tk:{n_history_tk}\")",
            "            ASCIIColors.red(f\"n_isearch_tk:{n_isearch_tk}\")",
            "            ",
            "            ASCIIColors.red(f\"n_tokens:{n_tokens}\")",
            "            ASCIIColors.red(f\"self.config.max_n_predict:{self.config.max_n_predict}\")",
            "            self.InfoMessage(f\"Not enough space in context!!\\nVerify that your vectorization settings for documents or internet search are realistic compared to your context size.\\nYou are {available_space} short of context!\")",
            "            raise Exception(\"Not enough space in context!!\")",
            "",
            "        # Accumulate messages until the cumulative number of tokens exceeds available_space",
            "        tokens_accumulated = 0",
            "",
            "",
            "        # Initialize a list to store the full messages",
            "        full_message_list = []",
            "        # If this is not a continue request, we add the AI prompt",
            "        if not is_continue:",
            "            message_tokenized = self.model.tokenize(",
            "                self.personality.ai_message_prefix.strip()",
            "            )",
            "            full_message_list.append(message_tokenized)",
            "            # Update the cumulative number of tokens",
            "            tokens_accumulated += len(message_tokenized)",
            "",
            "",
            "        if generation_type != \"simple_question\":",
            "            # Accumulate messages starting from message_index",
            "            for i in range(message_index, -1, -1):",
            "                message = messages[i]",
            "",
            "                # Check if the message content is not empty and visible to the AI",
            "                if message.content != '' and (",
            "                        message.message_type <= MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER.value and message.message_type != MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI.value):",
            "",
            "                    # Tokenize the message content",
            "                    if self.config.use_model_name_in_discussions:",
            "                        if message.model:",
            "                            msg = f\"{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}({message.model}){end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip() + f\"{self.separator_template}\"",
            "                        else:",
            "                            msg = f\"{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip() + f\"{self.separator_template}\"",
            "                        message_tokenized = self.model.tokenize(msg)",
            "                    else:",
            "                        msg_value= f\"{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip() + f\"{self.separator_template}\"",
            "                        message_tokenized = self.model.tokenize(",
            "                            msg_value",
            "                        )",
            "                    # Check if adding the message will exceed the available space",
            "                    if tokens_accumulated + len(message_tokenized) > available_space:",
            "                        # Update the cumulative number of tokens",
            "                        msg = message_tokenized[-(available_space-tokens_accumulated):]",
            "                        tokens_accumulated += available_space-tokens_accumulated",
            "                        full_message_list.insert(0, msg)",
            "                        break",
            "",
            "                    # Add the tokenized message to the full_message_list",
            "                    full_message_list.insert(0, message_tokenized)",
            "",
            "                    # Update the cumulative number of tokens",
            "                    tokens_accumulated += len(message_tokenized)",
            "        else:",
            "            message = messages[message_index]",
            "",
            "            # Check if the message content is not empty and visible to the AI",
            "            if message.content != '' and (",
            "                    message.message_type <= MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER.value and message.message_type != MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI.value):",
            "",
            "                if self.config.use_model_name_in_discussions:",
            "                    if message.model:",
            "                        msg = f\"{self.separator_template}{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}({message.model}){end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip()",
            "                    else:",
            "                        msg = f\"{self.separator_template}{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip()",
            "                    message_tokenized = self.model.tokenize(msg)",
            "                else:",
            "                    message_tokenized = self.model.tokenize(",
            "                        f\"{self.separator_template}{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip()",
            "                    )",
            "",
            "                # Add the tokenized message to the full_message_list",
            "                full_message_list.insert(0, message_tokenized)",
            "",
            "                # Update the cumulative number of tokens",
            "                tokens_accumulated += len(message_tokenized)",
            "",
            "        # Build the final discussion messages by detokenizing the full_message_list",
            "        discussion_messages = \"\"",
            "        for i in range(len(full_message_list)-1 if not is_continue else len(full_message_list)):",
            "            message_tokens = full_message_list[i]",
            "            discussion_messages += self.model.detokenize(message_tokens)",
            "        ",
            "        if len(full_message_list)>0:",
            "            ai_prefix = self.personality.ai_message_prefix",
            "        else:",
            "            ai_prefix = \"\"",
            "        # Build the final prompt by concatenating the conditionning and discussion messages",
            "        prompt_data = conditionning + internet_search_results + documentation + knowledge + user_description + discussion_messages + positive_boost + negative_boost + fun_mode + (start_ai_header_id_template + ai_prefix + end_ai_header_id_template if not is_continue else '')",
            "",
            "        # Tokenize the prompt data",
            "        tokens = self.model.tokenize(prompt_data)",
            "",
            "        # Details",
            "        context_details = {",
            "            \"client_id\":client_id,",
            "            \"conditionning\":conditionning,",
            "            \"internet_search_infos\":internet_search_infos,",
            "            \"internet_search_results\":internet_search_results,",
            "            \"documentation\":documentation,",
            "            \"documentation_entries\":documentation_entries,",
            "            \"knowledge\":knowledge,",
            "            \"knowledge_infos\":knowledge_infos,",
            "            \"user_description\":user_description,",
            "            \"discussion_messages\":discussion_messages,",
            "            \"positive_boost\":positive_boost,",
            "            \"negative_boost\":negative_boost,",
            "            \"current_language\":self.config.current_language,",
            "            \"fun_mode\":fun_mode,",
            "            \"ai_prefix\":ai_prefix,",
            "            \"extra\":\"\"",
            "        }    ",
            "        if self.config.debug:",
            "            ASCIIColors.highlight(documentation,\"source_document_title\", ASCIIColors.color_yellow, ASCIIColors.color_red, False)",
            "        # Return the prepared query, original message content, and tokenized query",
            "        return prompt_data, current_message.content, tokens, context_details, internet_search_infos                ",
            "",
            "",
            "    # Properties ===============================================",
            "    @property",
            "    def start_header_id_template(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return self.config.start_header_id_template",
            "",
            "    @property",
            "    def end_header_id_template(self) -> str:",
            "        \"\"\"Get the end_header_id_template.\"\"\"",
            "        return self.config.end_header_id_template",
            "    ",
            "    @property",
            "    def system_message_template(self) -> str:",
            "        \"\"\"Get the system_message_template.\"\"\"",
            "        return self.config.system_message_template",
            "",
            "",
            "    @property",
            "    def separator_template(self) -> str:",
            "        \"\"\"Get the separator template.\"\"\"",
            "        return self.config.separator_template",
            "",
            "",
            "    @property",
            "    def start_user_header_id_template(self) -> str:",
            "        \"\"\"Get the start_user_header_id_template.\"\"\"",
            "        return self.config.start_user_header_id_template",
            "    @property",
            "    def end_user_header_id_template(self) -> str:",
            "        \"\"\"Get the end_user_header_id_template.\"\"\"",
            "        return self.config.end_user_header_id_template",
            "    @property",
            "    def end_user_message_id_template(self) -> str:",
            "        \"\"\"Get the end_user_message_id_template.\"\"\"",
            "        return self.config.end_user_message_id_template",
            "",
            "",
            "",
            "",
            "    # Properties ===============================================",
            "    @property",
            "    def start_header_id_template(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return self.config.start_header_id_template",
            "",
            "    @property",
            "    def end_header_id_template(self) -> str:",
            "        \"\"\"Get the end_header_id_template.\"\"\"",
            "        return self.config.end_header_id_template",
            "    ",
            "    @property",
            "    def system_message_template(self) -> str:",
            "        \"\"\"Get the system_message_template.\"\"\"",
            "        return self.config.system_message_template",
            "",
            "",
            "    @property",
            "    def separator_template(self) -> str:",
            "        \"\"\"Get the separator template.\"\"\"",
            "        return self.config.separator_template",
            "",
            "",
            "    @property",
            "    def start_user_header_id_template(self) -> str:",
            "        \"\"\"Get the start_user_header_id_template.\"\"\"",
            "        return self.config.start_user_header_id_template",
            "    @property",
            "    def end_user_header_id_template(self) -> str:",
            "        \"\"\"Get the end_user_header_id_template.\"\"\"",
            "        return self.config.end_user_header_id_template",
            "    @property",
            "    def end_user_message_id_template(self) -> str:",
            "        \"\"\"Get the end_user_message_id_template.\"\"\"",
            "        return self.config.end_user_message_id_template",
            "",
            "",
            "",
            "",
            "    @property",
            "    def start_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the start_ai_header_id_template.\"\"\"",
            "        return self.config.start_ai_header_id_template",
            "    @property",
            "    def end_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_header_id_template.\"\"\"",
            "        return self.config.end_ai_header_id_template",
            "    @property",
            "    def end_ai_message_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_message_id_template.\"\"\"",
            "        return self.config.end_ai_message_id_template",
            "    @property",
            "    def system_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_header_id_template}{self.system_message_template}{self.end_header_id_template}\"",
            "    @property",
            "    def user_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.config.user_name}{self.end_user_header_id_template}\"",
            "    @property",
            "    def ai_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.personality.name}{self.end_user_header_id_template}\"",
            "",
            "    def system_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\"",
            "",
            "    def ai_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\""
        ],
        "afterPatchFile": [
            "from lollms.main_config import LOLLMSConfig",
            "from lollms.paths import LollmsPaths",
            "from lollms.personality import PersonalityBuilder, AIPersonality",
            "from lollms.binding import LLMBinding, BindingBuilder, ModelBuilder",
            "from lollms.databases.discussions_database import Message",
            "from lollms.extension import LOLLMSExtension, ExtensionBuilder",
            "from lollms.config import InstallOption",
            "from lollms.helpers import ASCIIColors, trace_exception",
            "from lollms.com import NotificationType, NotificationDisplayType, LoLLMsCom",
            "from lollms.terminal import MainMenu",
            "from lollms.types import MSG_TYPE, SENDER_TYPES",
            "from lollms.utilities import PromptReshaper",
            "from lollms.client_session import Client, Session",
            "from lollms.databases.skills_database import SkillsLibrary",
            "from lollms.tasks import TasksLibrary",
            "from safe_store import TextVectorizer, VectorizationMethod, VisualizationMethod",
            "",
            "from lollmsvectordb.database_elements.chunk import Chunk",
            "from typing import Callable",
            "from pathlib import Path",
            "from datetime import datetime",
            "from functools import partial",
            "from socketio import AsyncServer",
            "from typing import Tuple, List, Dict",
            "import subprocess",
            "import importlib",
            "import sys, os",
            "import platform",
            "import gc",
            "import yaml",
            "import time",
            "from lollms.utilities import PackageManager",
            "",
            "class LollmsApplication(LoLLMsCom):",
            "    def __init__(",
            "                    self, ",
            "                    app_name:str, ",
            "                    config:LOLLMSConfig, ",
            "                    lollms_paths:LollmsPaths, ",
            "                    load_binding=True, ",
            "                    load_model=True, ",
            "                    try_select_binding=False, ",
            "                    try_select_model=False,",
            "                    callback=None,",
            "                    sio:AsyncServer=None,",
            "                    free_mode=False",
            "                ) -> None:",
            "        \"\"\"",
            "        Creates a LOLLMS Application",
            "        \"\"\"",
            "        super().__init__(sio)",
            "        self.app_name                   = app_name",
            "        self.config                     = config",
            "        self.lollms_paths               = lollms_paths",
            "",
            "        # TODO : implement",
            "        self.embedding_models           = []",
            "",
            "        self.menu                       = MainMenu(self, callback)",
            "        self.mounted_personalities      = []",
            "        self.personality:AIPersonality  = None",
            "",
            "        self.mounted_extensions         = []",
            "        self.binding                    = None",
            "        self.model:LLMBinding           = None",
            "        self.long_term_memory           = None",
            "",
            "        self.tts                        = None",
            "        self.session                    = Session(lollms_paths)",
            "        self.skills_library             = SkillsLibrary(self.lollms_paths.personal_skills_path/(self.config.skills_lib_database_name+\".db\"))",
            "",
            "        self.tasks_library              = TasksLibrary(self)",
            "",
            "        self.handle_generate_msg: Callable[[str, Dict], None]               = None",
            "        self.generate_msg_with_internet: Callable[[str, Dict], None]        = None",
            "        self.handle_continue_generate_msg_from: Callable[[str, Dict], None] = None",
            "        ",
            "        # Trust store ",
            "        self.bk_store = None",
            "        ",
            "        # services",
            "        self.ollama         = None",
            "        self.vllm           = None",
            "        self.whisper        = None",
            "        self.xtts           = None",
            "        self.sd             = None",
            "        self.comfyui        = None",
            "        self.motion_ctrl    = None",
            "",
            "        self.tti = None",
            "        self.tts = None",
            "        self.stt = None",
            "        self.ttm = None",
            "        self.ttv = None",
            "        ",
            "        self.rt_com = None",
            "        if not free_mode:",
            "            try:",
            "                if config.auto_update:",
            "                    # Clone the repository to the target path",
            "                    if self.lollms_paths.lollms_core_path.exists():",
            "                        ASCIIColors.info(\"Lollms_core found in the app space.\\nPulling last lollms_core\")",
            "                        subprocess.run([\"git\", \"-C\", self.lollms_paths.lollms_core_path, \"pull\"])            ",
            "                    if self.lollms_paths.safe_store_path.exists():",
            "                        ASCIIColors.info(\"safe_store_path found in the app space.\\nPulling last safe_store_path\")",
            "                        subprocess.run([\"git\", \"-C\", self.lollms_paths.safe_store_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ",
            "                    ASCIIColors.info(\"Bindings zoo found in your personal space.\\nPulling last bindings zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.bindings_zoo_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ASCIIColors.info(\"Personalities zoo found in your personal space.\\nPulling last personalities zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.personalities_zoo_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ASCIIColors.info(\"Extensions zoo found in your personal space.\\nPulling last Extensions zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.extensions_zoo_path, \"pull\"])            ",
            "                    # Pull the repository if it already exists",
            "                    ASCIIColors.info(\"Models zoo found in your personal space.\\nPulling last Models zoo\")",
            "                    subprocess.run([\"git\", \"-C\", self.lollms_paths.models_zoo_path, \"pull\"])            ",
            "            except Exception as ex:",
            "                ASCIIColors.error(\"Couldn't pull zoos. Please contact the main dev on our discord channel and report the problem.\")",
            "                trace_exception(ex)",
            "",
            "            if self.config.binding_name is None:",
            "                ASCIIColors.warning(f\"No binding selected\")",
            "                if try_select_binding:",
            "                    ASCIIColors.info(\"Please select a valid model or install a new one from a url\")",
            "                    self.menu.select_binding()",
            "            else:",
            "                if load_binding:",
            "                    try:",
            "                        ASCIIColors.info(f\">Loading binding {self.config.binding_name}. Please wait ...\")",
            "                        self.binding            = self.load_binding()",
            "                    except Exception as ex:",
            "                        ASCIIColors.error(f\"Failed to load binding.\\nReturned exception: {ex}\")",
            "                        trace_exception(ex)",
            "",
            "                    if self.binding is not None:",
            "                        ASCIIColors.success(f\"Binding {self.config.binding_name} loaded successfully.\")",
            "                        if load_model:",
            "                            if self.config.model_name is None:",
            "                                ASCIIColors.warning(f\"No model selected\")",
            "                                if try_select_model:",
            "                                    print(\"Please select a valid model\")",
            "                                    self.menu.select_model()",
            "                                    ",
            "                            if self.config.model_name is not None:",
            "                                try:",
            "                                    ASCIIColors.info(f\">Loading model {self.config.model_name}. Please wait ...\")",
            "                                    self.model          = self.load_model()",
            "                                    if self.model is not None:",
            "                                        ASCIIColors.success(f\"Model {self.config.model_name} loaded successfully.\")",
            "",
            "                                except Exception as ex:",
            "                                    ASCIIColors.error(f\"Failed to load model.\\nReturned exception: {ex}\")",
            "                                    trace_exception(ex)",
            "                    else:",
            "                        ASCIIColors.warning(f\"Couldn't load binding {self.config.binding_name}.\")",
            "                ",
            "            self.mount_personalities()",
            "            self.mount_extensions()",
            "            ",
            "            try:",
            "                self.load_rag_dbs()",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "",
            "",
            "",
            "    def backup_trust_store(self):",
            "        self.bk_store = None",
            "        if 'REQUESTS_CA_BUNDLE' in os.environ:",
            "            self.bk_store = os.environ['REQUESTS_CA_BUNDLE']",
            "            del os.environ['REQUESTS_CA_BUNDLE']",
            "",
            "    def restore_trust_store(self):",
            "        if self.bk_store is not None:",
            "            os.environ['REQUESTS_CA_BUNDLE'] = self.bk_store",
            "            ",
            "    def select_model(self, binding_name, model_name):",
            "        self.config[\"binding_name\"] = binding_name",
            "        self.config[\"model_name\"] = model_name",
            "        print(f\"New binding selected : {binding_name}\")",
            "",
            "        try:",
            "            if self.binding:",
            "                self.binding.destroy_model()",
            "            self.binding = None",
            "            self.model = None",
            "            for per in self.mounted_personalities:",
            "                if per is not None:",
            "                    per.model = None",
            "            gc.collect()",
            "            self.binding = BindingBuilder().build_binding(self.config, self.lollms_paths, InstallOption.INSTALL_IF_NECESSARY, lollmsCom=self)",
            "            self.config[\"model_name\"] = model_name",
            "            self.model = self.binding.build_model()",
            "            for per in self.mounted_personalities:",
            "                if per is not None:",
            "                    per.model = self.model",
            "            self.config.save_config()",
            "            ASCIIColors.green(\"Binding loaded successfully\")",
            "            return True",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't build binding: [{ex}]\")",
            "            trace_exception(ex)",
            "            return False",
            "        ",
            "    def add_discussion_to_skills_library(self, client: Client):",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        separator_template          = self.config.separator_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "",
            "        messages = client.discussion.get_messages()",
            "",
            "        # Extract relevant information from messages",
            "        def cb(str, MSG_TYPE_=MSG_TYPE.MSG_TYPE_FULL, dict=None, list=None):",
            "            if MSG_TYPE_!=MSG_TYPE.MSG_TYPE_CHUNK:",
            "                self.ShowBlockingMessage(f\"Learning\\n{str}\")",
            "        bk_cb = self.tasks_library.callback",
            "        self.tasks_library.callback = cb",
            "        content = self._extract_content(messages, cb)",
            "        self.tasks_library.callback = bk_cb",
            "",
            "        # Generate title",
            "        title_prompt =  f\"{separator_template}\".join([",
            "            f\"{self.start_header_id_template}{system_message_template}{end_header_id_template}Generate a concise and descriptive title for the following content.\",",
            "            \"The title should summarize the main topic or subject of the content.\",",
            "            \"Do not mention the format of the content (e.g., bullet points, discussion, etc.) in the title.\",",
            "            \"Provide only the title without any additional explanations or context.\",",
            "            f\"{self.start_header_id_template}content{end_header_id_template}\",",
            "            f\"{content}\",",
            "            f\"{self.start_header_id_template}title{end_header_id_template}\"",
            "            ])",
            "",
            "        title = self._generate_text(title_prompt)",
            "",
            "        # Determine category",
            "        category_prompt = f\"{self.start_header_id_template}{system_message_template}{end_header_id_template}Analyze the following title, and determine the most appropriate generic category that encompasses the main subject or theme. The category should be broad enough to include multiple related skill entries. Provide only the category name without any additional explanations or context:\\n\\nTitle:\\n{title}\\n{separator_template}{self.start_header_id_template}Category:\\n\"",
            "        category = self._generate_text(category_prompt)",
            "",
            "        # Add entry to skills library",
            "        self.skills_library.add_entry(1, category, title, content)",
            "        return category, title, content",
            "",
            "    def _extract_content(self, messages:List[Message], callback = None):      ",
            "        message_content = \"\"",
            "",
            "        for message in messages:",
            "            rank = message.rank",
            "            sender = message.sender",
            "            text = message.content",
            "            message_content += f\"Rank {rank} - {sender}: {text}\\n\"",
            "",
            "        return self.tasks_library.summerize_text(",
            "            message_content, ",
            "            \"\\n\".join([",
            "                \"Extract useful information from this discussion.\"",
            "            ]),",
            "            doc_name=\"discussion\",",
            "            callback=callback)",
            "        ",
            "",
            "    def _generate_text(self, prompt):",
            "        max_tokens = self.config.ctx_size - self.model.get_nb_tokens(prompt)",
            "        generated_text = self.model.generate(prompt, max_tokens)",
            "        return generated_text.strip()",
            "",
            "",
            "    def get_uploads_path(self, client_id):",
            "        return self.lollms_paths.personal_uploads_path",
            "    ",
            "    def load_rag_dbs(self):",
            "        self.active_rag_dbs = []",
            "        for rag_db in self.config.rag_databases:",
            "            parts = rag_db.split(\"::\")",
            "            db_name = parts[0]",
            "            if parts[-1]==\"mounted\":",
            "                if not PackageManager.check_package_installed(\"lollmsvectordb\"):",
            "                    PackageManager.install_package(\"lollmsvectordb\")",
            "                ",
            "                from lollmsvectordb import VectorDatabase",
            "                from lollmsvectordb.text_document_loader import TextDocumentsLoader",
            "                from lollmsvectordb.lollms_tokenizers.tiktoken_tokenizer import TikTokenTokenizer",
            "                if self.config.rag_vectorizer == \"bert\":",
            "                    self.backup_trust_store()",
            "                    from lollmsvectordb.lollms_vectorizers.bert_vectorizer import BERTVectorizer",
            "                    v = BERTVectorizer()",
            "                    self.restore_trust_store()",
            "                elif self.config.rag_vectorizer == \"tfidf\":",
            "                    from lollmsvectordb.lollms_vectorizers.tfidf_vectorizer import TFIDFVectorizer",
            "                    v = TFIDFVectorizer()",
            "                elif self.config.rag_vectorizer == \"word2vec\":",
            "                    from lollmsvectordb.lollms_vectorizers.word2vec_vectorizer import Word2VecVectorizer",
            "                    v = Word2VecVectorizer()",
            "",
            "                vdb = VectorDatabase(Path(parts[1])/f\"{db_name}.sqlite\", v, self.model if self.model else TikTokenTokenizer(), n_neighbors=self.config.rag_n_chunks)       ",
            "                self.active_rag_dbs.append({\"name\":parts[0],\"path\":parts[1],\"vectorizer\":vdb})",
            "",
            "",
            "    def start_servers(self):",
            "",
            "        ASCIIColors.yellow(\"* - * - * - Starting services - * - * - *\")",
            "",
            "        ASCIIColors.blue(\"Loading local TTT services\")",
            "        if self.config.enable_ollama_service:",
            "            try:",
            "                from lollms.services.ollama.lollms_ollama import Service",
            "                self.ollama = Service(self, base_url=self.config.ollama_base_url)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "                self.warning(f\"Couldn't load Ollama\")",
            "",
            "        if self.config.enable_vllm_service:",
            "            try:",
            "                from lollms.services.vllm.lollms_vllm import Service",
            "                self.vllm = Service(self, base_url=self.config.vllm_url)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "                self.warning(f\"Couldn't load vllm\")",
            "",
            "        ASCIIColors.blue(\"Loading loacal STT services\")",
            "        if self.config.whisper_activate or self.config.active_stt_service == \"whisper\":",
            "            try:",
            "                from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "                self.whisper = LollmsWhisper(self, self.config.whisper_model, self.lollms_paths.personal_outputs_path)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "",
            "        ASCIIColors.blue(\"Loading local TTS services\")",
            "        if self.config.xtts_enable or self.config.active_tts_service == \"xtts\":",
            "            ASCIIColors.yellow(\"Loading XTTS\")",
            "            try:",
            "                from lollms.services.xtts.lollms_xtts import LollmsXTTS",
            "                voice=self.config.xtts_current_voice",
            "                if voice!=\"main_voice\":",
            "                    voices_folder = self.lollms_paths.custom_voices_path",
            "                else:",
            "                    voices_folder = Path(__file__).parent.parent.parent/\"services/xtts/voices\"",
            "",
            "                self.xtts = LollmsXTTS(",
            "                                        self,",
            "                                        voices_folder=voices_folder,",
            "                                        voice_samples_path=self.lollms_paths.custom_voices_path, ",
            "                                        xtts_base_url=self.config.xtts_base_url,",
            "                                        wait_for_service=False,",
            "                                        use_deep_speed=self.config.xtts_use_deepspeed,",
            "                                        use_streaming_mode=self.config.xtts_use_streaming_mode",
            "                                    )",
            "            except:",
            "                self.warning(f\"Couldn't load XTTS\")",
            "",
            "        ASCIIColors.blue(\"Loading local TTI services\")",
            "        if self.config.enable_sd_service:",
            "            try:",
            "                from lollms.services.sd.lollms_sd import LollmsSD",
            "                self.sd = LollmsSD(self, auto_sd_base_url=self.config.sd_base_url)",
            "            except:",
            "                self.warning(f\"Couldn't load SD\")",
            "",
            "        if self.config.enable_comfyui_service:",
            "            try:",
            "                from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                self.comfyui = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "            except:",
            "                self.warning(f\"Couldn't load SD\")",
            "",
            "        if self.config.enable_motion_ctrl_service:",
            "            try:",
            "                from lollms.services.motion_ctrl.lollms_motion_ctrl import Service",
            "                self.motion_ctrl = Service(self, base_url=self.config.motion_ctrl_base_url)",
            "            except Exception as ex:",
            "                trace_exception(ex)",
            "                self.warning(f\"Couldn't load Motion control\")",
            "",
            "        ASCIIColors.blue(\"Activating TTI service\")",
            "        if self.config.active_tti_service == \"diffusers\":",
            "            from lollms.services.diffusers.lollms_diffusers import LollmsDiffusers",
            "            self.tti = LollmsDiffusers(self)",
            "        elif self.config.active_tti_service == \"autosd\":",
            "            if self.sd:",
            "                self.tti = self.sd",
            "            else:",
            "                from lollms.services.sd.lollms_sd import LollmsSD",
            "                self.tti = LollmsSD(self)",
            "        elif self.config.active_tti_service == \"dall-e\":",
            "            from lollms.services.dalle.lollms_dalle import LollmsDalle",
            "            self.tti = LollmsDalle(self, self.config.dall_e_key)",
            "        elif self.config.active_tti_service == \"midjourney\":",
            "            from lollms.services.midjourney.lollms_midjourney import LollmsMidjourney",
            "            self.tti = LollmsMidjourney(self, self.config.midjourney_key)",
            "        elif self.config.active_tti_service == \"comfyui\" and (self.tti is None or self.tti.name!=\"comfyui\"):",
            "            if self.comfyui:",
            "                self.tti = self.comfyui",
            "            else:",
            "                from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                self.tti = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "",
            "        ASCIIColors.blue(\"Activating TTS services\")",
            "",
            "        if self.config.active_tts_service == \"openai_tts\":",
            "            from lollms.services.open_ai_tts.lollms_openai_tts import LollmsOpenAITTS",
            "            self.tts = LollmsOpenAITTS(self, self.config.openai_tts_model, self.config.openai_tts_voice,  self.config.openai_tts_key)",
            "        elif self.config.active_tts_service == \"xtts\" and self.xtts:",
            "            self.tts = self.xtts",
            "",
            "        ASCIIColors.blue(\"Loading STT services\")",
            "        if self.config.active_stt_service == \"openai_whisper\":",
            "            from lollms.services.openai_whisper.lollms_openai_whisper import LollmsOpenAIWhisper",
            "            self.stt = LollmsOpenAIWhisper(self, self.config.openai_whisper_model, self.config.openai_whisper_key)",
            "        elif self.config.active_stt_service == \"whisper\":",
            "            from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "            self.stt = LollmsWhisper(self, self.config.whisper_model)",
            "",
            "",
            "    def verify_servers(self, reload_all=False):",
            "        ASCIIColors.yellow(\"* - * - * - Verifying services - * - * - *\")",
            "",
            "        try:",
            "            ASCIIColors.blue(\"Loading active local TTT services\")",
            "            ",
            "            if self.config.enable_ollama_service and self.ollama is None:",
            "                try:",
            "                    from lollms.services.ollama.lollms_ollama import Service",
            "                    self.ollama = Service(self, base_url=self.config.ollama_base_url)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(f\"Couldn't load Ollama\")",
            "",
            "            if self.config.enable_vllm_service and self.vllm is None:",
            "                try:",
            "                    from lollms.services.vllm.lollms_vllm import Service",
            "                    self.vllm = Service(self, base_url=self.config.vllm_url)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(f\"Couldn't load vllm\")",
            "",
            "            ASCIIColors.blue(\"Loading local STT services\")",
            "",
            "            if self.config.whisper_activate and self.whisper is None:",
            "                try:",
            "                    from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "                    self.whisper = LollmsWhisper(self, self.config.whisper_model, self.lollms_paths.personal_outputs_path)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    ",
            "            ASCIIColors.blue(\"Loading loacal TTS services\")",
            "            if (self.config.xtts_enable or self.config.active_tts_service == \"xtts\") and self.xtts is None:",
            "                ASCIIColors.yellow(\"Loading XTTS\")",
            "                try:",
            "                    from lollms.services.xtts.lollms_xtts import LollmsXTTS",
            "                    voice=self.config.xtts_current_voice",
            "                    if voice!=\"main_voice\":",
            "                        voices_folder = self.lollms_paths.custom_voices_path",
            "                    else:",
            "                        voices_folder = Path(__file__).parent.parent.parent/\"services/xtts/voices\"",
            "",
            "                    self.xtts = LollmsXTTS(",
            "                                            self,",
            "                                            voices_folder=voices_folder,",
            "                                            voice_samples_path=self.lollms_paths.custom_voices_path, ",
            "                                            xtts_base_url=self.config.xtts_base_url,",
            "                                            wait_for_service=False,",
            "                                            use_deep_speed=self.config.xtts_use_deepspeed,",
            "                                            use_streaming_mode=self.config.xtts_use_streaming_mode",
            "                                        )",
            "                except:",
            "                    self.warning(f\"Couldn't load XTTS\")",
            "",
            "            ASCIIColors.blue(\"Loading local TTI services\")",
            "            if self.config.enable_sd_service and self.sd is None:",
            "                try:",
            "                    from lollms.services.sd.lollms_sd import LollmsSD",
            "                    self.sd = LollmsSD(self, auto_sd_base_url=self.config.sd_base_url)",
            "                except:",
            "                    self.warning(f\"Couldn't load SD\")",
            "",
            "            if self.config.enable_comfyui_service and self.comfyui is None:",
            "                try:",
            "                    from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                    self.comfyui = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "                except:",
            "                    self.warning(f\"Couldn't load Comfyui\")",
            "",
            "            if self.config.enable_motion_ctrl_service and self.motion_ctrl is None:",
            "                try:",
            "                    from lollms.services.motion_ctrl.lollms_motion_ctrl import Service",
            "                    self.motion_ctrl = Service(self, base_url=self.config.motion_ctrl_base_url)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(f\"Couldn't load Motion control\")",
            "",
            "",
            "            ASCIIColors.blue(\"Activating TTI service\")",
            "            if self.config.active_tti_service == \"diffusers\" and (self.tti is None or self.tti.name!=\"diffusers\"):",
            "                from lollms.services.diffusers.lollms_diffusers import LollmsDiffusers",
            "                self.tti = LollmsDiffusers(self)",
            "            elif self.config.active_tti_service == \"autosd\" and (self.tti is None or self.tti.name!=\"stable_diffusion\"):",
            "                if self.sd:",
            "                    self.tti = self.sd",
            "                else:",
            "                    from lollms.services.sd.lollms_sd import LollmsSD",
            "                    self.tti = LollmsSD(self)",
            "            elif self.config.active_tti_service == \"dall-e\" and (self.tti is None or self.tti.name!=\"dall-e-2\" or type(self.tti.name)!=\"dall-e-3\"):",
            "                from lollms.services.dalle.lollms_dalle import LollmsDalle",
            "                self.tti = LollmsDalle(self, self.config.dall_e_key)",
            "            elif self.config.active_tti_service == \"midjourney\" and (self.tti is None or self.tti.name!=\"midjourney\"):",
            "                from lollms.services.midjourney.lollms_midjourney import LollmsMidjourney",
            "                self.tti = LollmsMidjourney(self, self.config.midjourney_key)",
            "            elif self.config.active_tti_service == \"comfyui\" and (self.tti is None or self.tti.name!=\"comfyui\"):",
            "                if self.comfyui:",
            "                    self.tti = self.comfyui",
            "                else:",
            "                    from lollms.services.comfyui.lollms_comfyui import LollmsComfyUI",
            "                    self.tti = LollmsComfyUI(self, comfyui_base_url=self.config.comfyui_base_url)",
            "",
            "            ASCIIColors.blue(\"Activating TTS service\")",
            "            if self.config.active_tts_service == \"openai_tts\" and (self.tts is None or self.tts.name!=\"openai_tts\"):",
            "                from lollms.services.open_ai_tts.lollms_openai_tts import LollmsOpenAITTS",
            "                self.tts = LollmsOpenAITTS(self, self.config.openai_tts_model, self.config.openai_tts_voice,  self.config.openai_tts_key)",
            "            elif self.config.active_tts_service == \"xtts\" and self.xtts:",
            "                self.tts = self.xtts",
            "",
            "            ASCIIColors.blue(\"Activating STT service\")",
            "            if self.config.active_stt_service == \"openai_whisper\" and (self.tts is None or self.tts.name!=\"openai_whisper\"):",
            "                from lollms.services.openai_whisper.lollms_openai_whisper import LollmsOpenAIWhisper",
            "                self.stt = LollmsOpenAIWhisper(self, self.config.openai_whisper_model, self.config.openai_whisper_key)",
            "            elif self.config.active_stt_service == \"whisper\" and (self.tts is None or  self.tts.name!=\"whisper\") :",
            "                from lollms.services.whisper.lollms_whisper import LollmsWhisper",
            "                self.stt = LollmsWhisper(self, self.config.whisper_model)",
            "",
            "        except Exception as ex:",
            "            trace_exception(ex)",
            "            ",
            "",
            "    def build_long_term_skills_memory(self):",
            "        discussion_db_name:Path = self.lollms_paths.personal_discussions_path/self.config.discussion_db_name.split(\".\")[0]",
            "        discussion_db_name.mkdir(exist_ok=True, parents=True)",
            "        self.long_term_memory = TextVectorizer(",
            "                vectorization_method=VectorizationMethod.TFIDF_VECTORIZER,",
            "                model=self.model,",
            "                database_path=discussion_db_name/\"skills_memory.json\",",
            "                save_db=True,",
            "                data_visualization_method=VisualizationMethod.PCA,",
            "            )",
            "        return self.long_term_memory",
            "    ",
            "    def process_chunk(",
            "                        self, ",
            "                        chunk:str, ",
            "                        message_type,",
            "                        parameters:dict=None, ",
            "                        metadata:list=None, ",
            "                        personality=None",
            "                    ):",
            "        ",
            "        pass",
            "",
            "    def default_callback(self, chunk, type, generation_infos:dict):",
            "        if generation_infos[\"nb_received_tokens\"]==0:",
            "            self.start_time = datetime.now()",
            "        dt =(datetime.now() - self.start_time).seconds",
            "        if dt==0:",
            "            dt=1",
            "        spd = generation_infos[\"nb_received_tokens\"]/dt",
            "        ASCIIColors.green(f\"Received {generation_infos['nb_received_tokens']} tokens (speed: {spd:.2f}t/s)              \",end=\"\\r\",flush=True) ",
            "        sys.stdout = sys.__stdout__",
            "        sys.stdout.flush()",
            "        if chunk:",
            "            generation_infos[\"generated_text\"] += chunk",
            "        antiprompt = self.personality.detect_antiprompt(generation_infos[\"generated_text\"])",
            "        if antiprompt:",
            "            ASCIIColors.warning(f\"\\n{antiprompt} detected. Stopping generation\")",
            "            generation_infos[\"generated_text\"] = self.remove_text_from_string(generation_infos[\"generated_text\"],antiprompt)",
            "            return False",
            "        else:",
            "            generation_infos[\"nb_received_tokens\"] += 1",
            "            generation_infos[\"first_chunk\"]=False",
            "            # if stop generation is detected then stop",
            "            if not self.cancel_gen:",
            "                return True",
            "            else:",
            "                self.cancel_gen = False",
            "                ASCIIColors.warning(\"Generation canceled\")",
            "                return False",
            "   ",
            "    def remove_text_from_string(self, string, text_to_find):",
            "        \"\"\"",
            "        Removes everything from the first occurrence of the specified text in the string (case-insensitive).",
            "",
            "        Parameters:",
            "        string (str): The original string.",
            "        text_to_find (str): The text to find in the string.",
            "",
            "        Returns:",
            "        str: The updated string.",
            "        \"\"\"",
            "        index = string.lower().find(text_to_find.lower())",
            "",
            "        if index != -1:",
            "            string = string[:index]",
            "",
            "        return string",
            "",
            "    def load_binding(self):",
            "        try:",
            "            binding = BindingBuilder().build_binding(self.config, self.lollms_paths, lollmsCom=self)",
            "            return binding    ",
            "        except Exception as ex:",
            "            self.error(\"Couldn't load binding\")",
            "            self.info(\"Trying to reinstall binding\")",
            "            trace_exception(ex)",
            "            try:",
            "                binding = BindingBuilder().build_binding(self.config, self.lollms_paths,installation_option=InstallOption.FORCE_INSTALL, lollmsCom=self)",
            "            except Exception as ex:",
            "                self.error(\"Couldn't reinstall binding\")",
            "                trace_exception(ex)",
            "            return None    ",
            "",
            "    ",
            "    def load_model(self):",
            "        try:",
            "            model = ModelBuilder(self.binding).get_model()",
            "            for personality in self.mounted_personalities:",
            "                if personality is not None:",
            "                    personality.model = model",
            "        except Exception as ex:",
            "            self.error(\"Couldn't load model.\")",
            "            ASCIIColors.error(f\"Couldn't load model. Please verify your configuration file at {self.lollms_paths.personal_configuration_path} or use the next menu to select a valid model\")",
            "            ASCIIColors.error(f\"Binding returned this exception : {ex}\")",
            "            trace_exception(ex)",
            "            ASCIIColors.error(f\"{self.config.get_model_path_infos()}\")",
            "            print(\"Please select a valid model or install a new one from a url\")",
            "            model = None",
            "",
            "        return model",
            "",
            "",
            "    def mount_extension(self, id:int, callback=None):",
            "        try:",
            "            extension = ExtensionBuilder().build_extension(self.config[\"extensions\"][id], self.lollms_paths, self)",
            "            self.mounted_extensions.append(extension)",
            "            return extension",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't load extension. Please verify your configuration file at {self.lollms_paths.personal_configuration_path} or use the next menu to select a valid personality\")",
            "            trace_exception(ex)",
            "        return None",
            "",
            "",
            "    def mount_personality(self, id:int, callback=None):",
            "        try:",
            "            personality = PersonalityBuilder(self.lollms_paths, self.config, self.model, self, callback=callback).build_personality(id)",
            "            if personality.model is not None:",
            "                self.cond_tk = personality.model.tokenize(personality.personality_conditioning)",
            "                self.n_cond_tk = len(self.cond_tk)",
            "                ASCIIColors.success(f\"Personality  {personality.name} mounted successfully\")",
            "            else:",
            "                if personality.selected_language is not None:",
            "                    ASCIIColors.success(f\"Personality  {personality.name} : {personality.selected_language} mounted successfully but no model is selected\")",
            "                else:",
            "                    ASCIIColors.success(f\"Personality  {personality.name} mounted successfully but no model is selected\")",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't load personality. Please verify your configuration file at {self.lollms_paths.personal_configuration_path} or use the next menu to select a valid personality\")",
            "            ASCIIColors.error(f\"Binding returned this exception : {ex}\")",
            "            trace_exception(ex)",
            "            ASCIIColors.error(f\"{self.config.get_personality_path_infos()}\")",
            "            if id == self.config.active_personality_id:",
            "                self.config.active_personality_id=len(self.config.personalities)-1",
            "            personality = None",
            "        ",
            "        self.mounted_personalities.append(personality)",
            "        return personality",
            "    ",
            "    def mount_personalities(self, callback = None):",
            "        self.mounted_personalities = []",
            "        to_remove = []",
            "        for i in range(len(self.config[\"personalities\"])):",
            "            p = self.mount_personality(i, callback = None)",
            "            if p is None:",
            "                to_remove.append(i)",
            "        to_remove.sort(reverse=True)",
            "        for i in to_remove:",
            "            self.unmount_personality(i)",
            "",
            "        if self.config.active_personality_id>=0 and self.config.active_personality_id<len(self.mounted_personalities):",
            "            self.personality = self.mounted_personalities[self.config.active_personality_id]",
            "        else:",
            "            self.config[\"personalities\"].insert(0, \"generic/lollms\")",
            "            self.mount_personality(0, callback = None)",
            "            self.config.active_personality_id = 0",
            "            self.personality = self.mounted_personalities[self.config.active_personality_id]",
            "",
            "    def mount_extensions(self, callback = None):",
            "        self.mounted_extensions = []",
            "        to_remove = []",
            "        for i in range(len(self.config[\"extensions\"])):",
            "            p = self.mount_extension(i, callback = None)",
            "            if p is None:",
            "                to_remove.append(i)",
            "        to_remove.sort(reverse=True)",
            "        for i in to_remove:",
            "            self.unmount_extension(i)",
            "",
            "",
            "    def set_personalities_callbacks(self, callback: Callable[[str, int, dict], bool]=None):",
            "        for personality in self.mount_personalities:",
            "            personality.setCallback(callback)",
            "",
            "    def unmount_extension(self, id:int)->bool:",
            "        if id<len(self.config.extensions):",
            "            del self.config.extensions[id]",
            "            if id>=0 and id<len(self.mounted_extensions):",
            "                del self.mounted_extensions[id]",
            "            self.config.save_config()",
            "            return True",
            "        else:",
            "            return False",
            "",
            "            ",
            "    def unmount_personality(self, id:int)->bool:",
            "        if id<len(self.config.personalities):",
            "            del self.config.personalities[id]",
            "            del self.mounted_personalities[id]",
            "            if self.config.active_personality_id>=id:",
            "                self.config.active_personality_id-=1",
            "",
            "            self.config.save_config()",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "    def select_personality(self, id:int):",
            "        if id<len(self.config.personalities):",
            "            self.config.active_personality_id = id",
            "            self.personality = self.mounted_personalities[id]",
            "            self.config.save_config()",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "    def load_personality(self, callback=None):",
            "        try:",
            "            personality = PersonalityBuilder(self.lollms_paths, self.config, self.model, self, callback=callback).build_personality()",
            "        except Exception as ex:",
            "            ASCIIColors.error(f\"Couldn't load personality. Please verify your configuration file at {self.configuration_path} or use the next menu to select a valid personality\")",
            "            ASCIIColors.error(f\"Binding returned this exception : {ex}\")",
            "            ASCIIColors.error(f\"{self.config.get_personality_path_infos()}\")",
            "            print(\"Please select a valid model or install a new one from a url\")",
            "            personality = None",
            "        return personality",
            "",
            "    @staticmethod   ",
            "    def reset_paths(lollms_paths:LollmsPaths):",
            "        lollms_paths.resetPaths()",
            "",
            "    @staticmethod   ",
            "    def reset_all_installs(lollms_paths:LollmsPaths):",
            "        ASCIIColors.info(\"Removeing all configuration files to force reinstall\")",
            "        ASCIIColors.info(f\"Searching files from {lollms_paths.personal_configuration_path}\")",
            "        for file_path in lollms_paths.personal_configuration_path.iterdir():",
            "            if file_path.name!=f\"{lollms_paths.tool_prefix}local_config.yaml\" and file_path.suffix.lower()==\".yaml\":",
            "                file_path.unlink()",
            "                ASCIIColors.info(f\"Deleted file: {file_path}\")",
            "",
            "",
            "    #languages:",
            "    def get_personality_languages(self):",
            "        languages = []",
            "        # Construire le chemin vers le dossier contenant les fichiers de langue pour la personnalit\u00e9 actuelle",
            "        languages_dir = self.lollms_paths.personal_configuration_path / \"personalities\" / self.personality.name",
            "        if self.personality.language:",
            "            default_language = self.personality.language.lower().strip().split()[0]",
            "        else:",
            "            default_language = \"english\"",
            "        # V\u00e9rifier si le dossier existe",
            "        languages_dir.mkdir(parents=True, exist_ok=True)",
            "        ",
            "        # It\u00e9rer sur chaque fichier YAML dans le dossier",
            "        for language_file in languages_dir.glob(\"languages_*.yaml\"):",
            "            # Improved extraction of the language code to handle names with underscores",
            "            parts = language_file.stem.split(\"_\")",
            "            if len(parts) > 2:",
            "                language_code = \"_\".join(parts[1:])  # Rejoin all parts after \"languages\"",
            "            else:",
            "                language_code = parts[-1]",
            "            ",
            "            if language_code != default_language:",
            "                languages.append(language_code)",
            "        ",
            "        return [default_language] + languages",
            "",
            "",
            "",
            "    def set_personality_language(self, language:str):",
            "        if language is None or  language == \"\":",
            "            return False",
            "        language = language.lower().strip().split()[0]",
            "        # Build the conditionning text block",
            "        default_language = self.personality.language.lower().strip().split()[0]",
            "",
            "        if language!= default_language:",
            "            language_path = self.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.name/f\"languages_{language}.yaml\"",
            "            if not language_path.exists():",
            "                self.ShowBlockingMessage(f\"This is the first time this personality speaks {language}\\nLollms is reconditionning the persona in that language.\\nThis will be done just once. Next time, the personality will speak {language} out of the box\")",
            "                language_path.parent.mkdir(exist_ok=True, parents=True)",
            "                # Translating",
            "                conditionning = self.tasks_library.translate_conditionning(self.personality._personality_conditioning, self.personality.language, language)",
            "                welcome_message = self.tasks_library.translate_message(self.personality.welcome_message, self.personality.language, language)",
            "                with open(language_path,\"w\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    yaml.safe_dump({\"conditionning\":conditionning,\"welcome_message\":welcome_message}, f)",
            "                self.HideBlockingMessage()",
            "            else:",
            "                with open(language_path,\"r\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    language_pack = yaml.safe_load(f)",
            "                    conditionning = language_pack[\"conditionning\"]",
            "        self.config.current_language=language",
            "        self.config.save_config()",
            "        return True",
            "",
            "    def del_personality_language(self, language:str):",
            "        if language is None or  language == \"\":",
            "            return False",
            "        ",
            "        language = language.lower().strip().split()[0]",
            "        default_language = self.personality.language.lower().strip().split()[0]",
            "        if language == default_language:",
            "            return False # Can't remove the default language",
            "                ",
            "        language_path = self.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.name/f\"languages_{language}.yaml\"",
            "        if language_path.exists():",
            "            try:",
            "                language_path.unlink()",
            "            except Exception as ex:",
            "                return False",
            "            if self.config.current_language==language:",
            "                self.config.current_language=\"english\"",
            "                self.config.save_config()",
            "        return True",
            "",
            "    def recover_discussion(self,client_id, message_index=-1):",
            "        messages = self.session.get_client(client_id).discussion.get_messages()",
            "        discussion=\"\"",
            "        for msg in messages:",
            "            if message_index!=-1 and msg>message_index:",
            "                break",
            "            discussion += \"\\n\" + self.config.discussion_prompt_separator + msg.sender + \": \" + msg.content.strip()",
            "        return discussion",
            "    # -------------------------------------- Prompt preparing",
            "    def prepare_query(self, client_id: str, message_id: int = -1, is_continue: bool = False, n_tokens: int = 0, generation_type = None, force_using_internet=False) -> Tuple[str, str, List[str]]:",
            "        \"\"\"",
            "        Prepares the query for the model.",
            "",
            "        Args:",
            "            client_id (str): The client ID.",
            "            message_id (int): The message ID. Default is -1.",
            "            is_continue (bool): Whether the query is a continuation. Default is False.",
            "            n_tokens (int): The number of tokens. Default is 0.",
            "",
            "        Returns:",
            "            Tuple[str, str, List[str]]: The prepared query, original message content, and tokenized query.",
            "        \"\"\"",
            "        documentation_entries = []",
            "        start_ai_header_id_template     = self.config.start_ai_header_id_template",
            "        end_ai_header_id_template       = self.config.end_ai_header_id_template",
            "",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        if self.personality.callback is None:",
            "            self.personality.callback = partial(self.process_chunk, client_id=client_id)",
            "        # Get the list of messages",
            "        client = self.session.get_client(client_id)",
            "        discussion = client.discussion",
            "        messages = discussion.get_messages()",
            "",
            "        # Find the index of the message with the specified message_id",
            "        message_index = -1",
            "        for i, message in enumerate(messages):",
            "            if message.id == message_id:",
            "                message_index = i",
            "                break",
            "        ",
            "        # Define current message",
            "        current_message = messages[message_index]",
            "",
            "        # Build the conditionning text block",
            "        default_language = self.personality.language.lower().strip().split()[0]",
            "        current_language = self.config.current_language.lower().strip().split()[0]",
            "",
            "        if self.config.current_language and  current_language!= default_language:",
            "            language_path = self.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.name/f\"languages_{current_language}.yaml\"",
            "            if not language_path.exists():",
            "                self.info(f\"This is the first time this personality speaks {current_language}\\nLollms is reconditionning the persona in that language.\\nThis will be done just once. Next time, the personality will speak {current_language} out of the box\")",
            "                language_path.parent.mkdir(exist_ok=True, parents=True)",
            "                # Translating",
            "                conditionning = self.tasks_library.translate_conditionning(self.personality._personality_conditioning, self.personality.language, current_language)",
            "                welcome_message = self.tasks_library.translate_message(self.personality.welcome_message, self.personality.language, current_language)",
            "                with open(language_path,\"w\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    yaml.safe_dump({\"conditionning\":conditionning,\"welcome_message\":welcome_message}, f)",
            "            else:",
            "                with open(language_path,\"r\",encoding=\"utf-8\", errors=\"ignore\") as f:",
            "                    language_pack = yaml.safe_load(f)",
            "                    conditionning = language_pack[\"conditionning\"]",
            "        else:",
            "            conditionning = self.personality._personality_conditioning",
            "",
            "        if len(conditionning)>0:",
            "            conditionning =  self.start_header_id_template + system_message_template + self.end_header_id_template + self.personality.replace_keys(conditionning, self.personality.conditionning_commands) + (\"\" if conditionning[-1]==self.separator_template else self.separator_template)",
            "",
            "        # Check if there are document files to add to the prompt",
            "        internet_search_results = \"\"",
            "        internet_search_infos = []",
            "        documentation = \"\"",
            "        knowledge = \"\"",
            "        knowledge_infos = {\"titles\":[],\"contents\":[]}",
            "",
            "",
            "        # boosting information",
            "        if self.config.positive_boost:",
            "            positive_boost=f\"{self.separator_template}{self.start_header_id_template}important information: \"+self.config.positive_boost+\"\\n\"",
            "            n_positive_boost = len(self.model.tokenize(positive_boost))",
            "        else:",
            "            positive_boost=\"\"",
            "            n_positive_boost = 0",
            "",
            "        if self.config.negative_boost:",
            "            negative_boost=f\"{self.separator_template}{self.start_header_id_template}important information: \"+self.config.negative_boost+\"\\n\"",
            "            n_negative_boost = len(self.model.tokenize(negative_boost))",
            "        else:",
            "            negative_boost=\"\"",
            "            n_negative_boost = 0",
            "",
            "        if self.config.fun_mode:",
            "            fun_mode=f\"{self.separator_template}{self.start_header_id_template}important information: Fun mode activated. In this mode you must answer in a funny playful way. Do not be serious in your answers. Each answer needs to make the user laugh.\\n\"",
            "            n_fun_mode = len(self.model.tokenize(positive_boost))",
            "        else:",
            "            fun_mode=\"\"",
            "            n_fun_mode = 0",
            "",
            "        discussion = None",
            "        if generation_type != \"simple_question\":",
            "",
            "            if self.config.activate_internet_search or force_using_internet or generation_type == \"full_context_with_internet\":",
            "                if discussion is None:",
            "                    discussion = self.recover_discussion(client_id)",
            "                if self.config.internet_activate_search_decision:",
            "                    self.personality.step_start(f\"Requesting if {self.personality.name} needs to search internet to answer the user\")",
            "                    q = f\"{self.separator_template}\".join([",
            "                        f\"{self.start_header_id_template}{system_message_template}{self.end_header_id_template}\",",
            "                        f\"Answer the question with yes or no. Don't add any extra explanation.\",",
            "                        f\"{self.start_user_header_id_template}user{self.end_user_header_id_template}\",",
            "                        f\"Do you have enough information to give a satisfactory answer to {self.config.user_name}'s request without internet search?\",",
            "                        \"(If you do not know or you can't answer the question, return 0 (no)\"",
            "                    ])",
            "                    need = not self.personality.yes_no(q, discussion)",
            "                    self.personality.step_end(f\"Requesting if {self.personality.name} needs to search internet to answer the user\")",
            "                    self.personality.step(\"Yes\" if need else \"No\")",
            "                else:",
            "                    need=True",
            "                if need:",
            "                    self.personality.step_start(\"Crafting internet search query\")",
            "                    q = f\"{self.separator_template}\".join([",
            "                        f\"{self.start_header_id_template}discussion{self.end_header_id_template}\",",
            "                        f\"{discussion[-2048:]}{self.start_header_id_template}system{self.end_header_id_template}\",",
            "                        f\"Read the discussion and craft a web search query suited to recover needed information to reply to last {self.config.user_name} message.\",",
            "                        f\"Do not answer the prompt. Do not add explanations.\",",
            "                        f\"{self.start_header_id_template}current date{self.end_header_id_template}{datetime.now()}\",",
            "                        f\"{self.start_header_id_template}websearch query{self.end_header_id_template}\"",
            "                    ])",
            "                    query = self.personality.fast_gen(q, max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                    self.personality.step_end(\"Crafting internet search query\")",
            "                    self.personality.step(f\"web search query: {query}\")",
            "",
            "                    if self.config.internet_quick_search:",
            "                        self.personality.step_start(\"Performing Internet search (quick mode)\")",
            "                    else:",
            "                        self.personality.step_start(\"Performing Internet search (advanced mode: slower but more advanced)\")",
            "",
            "                    internet_search_results=f\"{self.start_header_id_template}{system_message_template}{self.end_header_id_template}Use the web search results data to answer {self.config.user_name}. Try to extract information from the web search and use it to perform the requested task or answer the question. Do not come up with information that is not in the websearch results. Try to stick to the websearch results and clarify if your answer was based on the resuts or on your own culture. If you don't know how to perform the task, then tell the user politely that you need more data inputs.{self.separator_template}{self.start_header_id_template}Web search results{self.end_header_id_template}\\n\"",
            "",
            "                    docs, sorted_similarities, document_ids = self.personality.internet_search_with_vectorization(query, self.config.internet_quick_search, asses_using_llm=self.config.activate_internet_pages_judgement)",
            "                    ",
            "                    if len(docs)>0:",
            "                        for doc, infos,document_id in zip(docs, sorted_similarities, document_ids):",
            "                            internet_search_infos.append(document_id)",
            "                            internet_search_results += f\"{self.start_header_id_template}search result chunk{self.end_header_id_template}\\nchunk_infos:{document_id['url']}\\nchunk_title:{document_id['title']}\\ncontent:{doc}\\n\"",
            "                    else:",
            "                        internet_search_results += \"The search response was empty!\\nFailed to recover useful information from the search engine.\\n\"",
            "                    if self.config.internet_quick_search:",
            "                        self.personality.step_end(\"Performing Internet search (quick mode)\")",
            "                    else:",
            "                        self.personality.step_end(\"Performing Internet search (advanced mode: slower but more advanced)\")",
            "",
            "            if self.personality.persona_data_vectorizer:",
            "                if documentation==\"\":",
            "                    documentation=f\"{self.separator_template}{self.start_header_id_template}Documentation:\\n\"",
            "",
            "                if self.config.data_vectorization_build_keys_words:",
            "                    if discussion is None:",
            "                        discussion = self.recover_discussion(client_id)",
            "                    query = self.personality.fast_gen(f\"{self.separator_template}{self.start_header_id_template}instruction: Read the discussion and rewrite the last prompt for someone who didn't read the entire discussion.\\nDo not answer the prompt. Do not add explanations.{self.separator_template}{self.start_header_id_template}discussion:\\n{discussion[-2048:]}{self.separator_template}{self.start_header_id_template}enhanced query: \", max_generation_size=256, show_progress=True)",
            "                    ASCIIColors.cyan(f\"Query:{query}\")",
            "                else:",
            "                    query = current_message.content",
            "                try:",
            "                    docs, sorted_similarities, document_ids = self.personality.persona_data_vectorizer.recover_text(query, top_k=int(self.config.data_vectorization_nb_chunks))",
            "                    for doc, infos, doc_id in zip(docs, sorted_similarities, document_ids):",
            "                        if self.config.data_vectorization_put_chunk_informations_into_context:",
            "                            documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\nchunk_infos:{infos}\\ncontent:{doc}\\n\"",
            "                        else:",
            "                            documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{doc}\\n\"",
            "",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.warning(\"Couldn't add documentation to the context. Please verify the vector database\")",
            "            if not self.personality.ignore_discussion_documents_rag:",
            "                query = None",
            "                if len(self.active_rag_dbs) > 0 :",
            "                    if discussion is None:",
            "                        discussion = self.recover_discussion(client_id)",
            "",
            "                    if self.config.data_vectorization_build_keys_words:",
            "                        self.personality.step_start(\"Building vector store query\")",
            "                        q = f\"{self.separator_template}\".join([",
            "                            f\"{self.start_header_id_template}instruction{self.end_header_id_template}Read the entire discussion and rewrite the last prompt for someone who hasn't read the discussion.\",",
            "                            \"Do not answer the prompt. Do not provide any explanations.\",",
            "                            f\"{self.start_header_id_template}discussion{self.end_header_id_template}\",",
            "                            f\"{discussion[-2048:]}\",",
            "                            f\"{self.start_header_id_template}enhanced_query{self.end_header_id_template}\"",
            "                        ])",
            "                        query = self.personality.fast_gen(q, max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                        self.personality.step_end(\"Building vector store query\")",
            "                        ASCIIColors.magenta(f\"Query: {query}\")",
            "                        self.personality.step(f\"Query: {query}\")",
            "                    else:",
            "                        query = current_message.content",
            "                    if documentation==\"\":",
            "                        documentation=f\"{self.separator_template}\".join([",
            "                            f\"{self.separator_template}{self.start_header_id_template}important information{self.end_header_id_template}Utilize Documentation Data: Always refer to the provided documentation to answer user questions accurately.\",",
            "                            \"Absence of Information: If the required information is not available in the documentation, inform the user that the requested information is not present in the documentation section.\",",
            "                            \"Strict Adherence to Documentation: It is strictly prohibited to provide answers without concrete evidence from the documentation.\",",
            "                            \"Cite Your Sources: After providing an answer, include the full path to the document where the information was found.\",",
            "                            f\"{self.start_header_id_template}Documentation{self.end_header_id_template}\"])",
            "                        documentation += f\"{self.separator_template}\"",
            "                    results = []",
            "                    recovered_ids=[[]*len(self.active_rag_dbs)]",
            "                    i=0",
            "                    hop_id = 0",
            "                    while( len(results)<self.config.rag_n_chunks and hop_id<self.config.rag_max_n_hops):",
            "                        hop_id +=1",
            "                        for db in self.active_rag_dbs:",
            "                            v = db[\"vectorizer\"]",
            "                            r=v.search(query, self.config.rag_n_chunks, recovered_ids[i])",
            "                            recovered_ids[i].append([rg.chunk_id for rg in r])",
            "                            if self.config.rag_activate_multi_hops:",
            "                                r = [rg for rg in r if self.personality.verify_rag_entry(query, rg.content)]",
            "                            results+=r",
            "                            i+=1",
            "                        if len(results)>=self.config.rag_n_chunks:",
            "                            break",
            "                    n_neighbors = self.active_rag_dbs[0][\"vectorizer\"].n_neighbors",
            "                    sorted_results = sorted(results, key=lambda x: x.distance)[:n_neighbors]",
            "",
            "                    for chunk in sorted_results:",
            "                        document_infos = f\"{self.separator_template}\".join([",
            "                            f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\",",
            "                            f\"source_document_title:{chunk.doc.title}\",",
            "                            f\"source_document_path:{chunk.doc.path}\",",
            "                            f\"content:\\n{chunk.text}\\n\"",
            "                        ])",
            "                        documentation_entries.append({",
            "                            \"document_title\":chunk.doc.title,",
            "                            \"document_path\":chunk.doc.path,",
            "                            \"chunk_content\":chunk.text,",
            "                            \"chunk_size\":chunk.nb_tokens,",
            "                            \"distance\":chunk.distance,",
            "                        })",
            "                        documentation += document_infos",
            "                        ",
            "                if (len(client.discussion.text_files) > 0) and client.discussion.vectorizer is not None:",
            "                    if discussion is None:",
            "                        discussion = self.recover_discussion(client_id)",
            "",
            "                    if documentation==\"\":",
            "                        documentation=f\"{self.separator_template}{self.start_header_id_template}important information: Use the documentation data to answer the user questions. If the data is not present in the documentation, please tell the user that the information he is asking for does not exist in the documentation section. It is strictly forbidden to give the user an answer without having actual proof from the documentation.{self.separator_template}{self.start_header_id_template}Documentation:\\n\"",
            "",
            "                    if query is None:",
            "                        if self.config.data_vectorization_build_keys_words:",
            "                            self.personality.step_start(\"Building vector store query\")",
            "                            query = self.personality.fast_gen(f\"{self.separator_template}{self.start_header_id_template}instruction: Read the discussion and rewrite the last prompt for someone who didn't read the entire discussion.\\nDo not answer the prompt. Do not add explanations.{self.separator_template}{self.start_header_id_template}discussion:\\n{discussion[-2048:]}{self.separator_template}{self.start_header_id_template}enhanced query: \", max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                            self.personality.step_end(\"Building vector store query\")",
            "                            ASCIIColors.cyan(f\"Query: {query}\")",
            "                        else:",
            "                            query = current_message.content",
            "",
            "                    try:",
            "                        chunks:List[Chunk] = client.discussion.vectorizer.search(query, int(self.config.rag_n_chunks))",
            "                        for chunk in chunks:",
            "                            if self.config.data_vectorization_put_chunk_informations_into_context:",
            "                                documentation += f\"{self.start_header_id_template}document chunk{self.end_header_id_template}\\ndocument title: {chunk.doc.title}\\nchunk content:\\n{chunk.text}\\n\"",
            "                            else:",
            "                                documentation += f\"{self.start_header_id_template}chunk{self.end_header_id_template}\\n{chunk.text}\\n\"",
            "",
            "                        documentation += f\"{self.separator_template}{self.start_header_id_template}important information: Use the documentation data to answer the user questions. If the data is not present in the documentation, please tell the user that the information he is asking for does not exist in the documentation section. It is strictly forbidden to give the user an answer without having actual proof from the documentation.\\n\"",
            "                    except Exception as ex:",
            "                        trace_exception(ex)",
            "                        self.warning(\"Couldn't add documentation to the context. Please verify the vector database\")",
            "                # Check if there is discussion knowledge to add to the prompt",
            "                if self.config.activate_skills_lib:",
            "                    try:",
            "                        self.personality.step_start(\"Querying skills library\")",
            "                        if discussion is None:",
            "                            discussion = self.recover_discussion(client_id)",
            "                        self.personality.step_start(\"Building query\")",
            "                        query = self.personality.fast_gen(f\"{self.start_header_id_template}{system_message_template}{self.end_header_id_template}Your task is to carefully read the provided discussion and reformulate {self.config.user_name}'s request concisely. Return only the reformulated request without any additional explanations, commentary, or output.{self.separator_template}{self.start_header_id_template}discussion:\\n{discussion[-2048:]}{self.separator_template}{self.start_header_id_template}search query: \", max_generation_size=256, show_progress=True, callback=self.personality.sink)",
            "                        self.personality.step_end(\"Building query\")",
            "                        # skills = self.skills_library.query_entry(query)",
            "                        self.personality.step_start(\"Adding skills\")",
            "                        if self.config.debug:",
            "                            ASCIIColors.info(f\"Query : {query}\")",
            "                        skill_titles, skills = self.skills_library.query_vector_db(query, top_k=3, max_dist=1000)#query_entry_fts(query)",
            "                        knowledge_infos={\"titles\":skill_titles,\"contents\":skills}",
            "                        if len(skills)>0:",
            "                            if knowledge==\"\":",
            "                                knowledge=f\"{self.start_header_id_template}knowledge{self.end_header_id_template}\\n\"",
            "                            for i,(title, content) in enumerate(zip(skill_titles,skills)):",
            "                                knowledge += f\"{self.start_header_id_template}knowledge {i}{self.end_header_id_template}\\ntitle:\\n{title}\\ncontent:\\n{content}\\n\"",
            "                        self.personality.step_end(\"Adding skills\")",
            "                        self.personality.step_end(\"Querying skills library\")",
            "                    except Exception as ex:",
            "                        ASCIIColors.error(ex)",
            "                        self.warning(\"Couldn't add long term memory information to the context. Please verify the vector database\")        # Add information about the user",
            "                        self.personality.step_end(\"Adding skills\")",
            "                        self.personality.step_end(\"Querying skills library\",False)",
            "        user_description=\"\"",
            "        if self.config.use_user_informations_in_discussion:",
            "            user_description=f\"{self.start_header_id_template}User description{self.end_header_id_template}\\n\"+self.config.user_description+\"\\n\"",
            "",
            "",
            "        # Tokenize the conditionning text and calculate its number of tokens",
            "        tokens_conditionning = self.model.tokenize(conditionning)",
            "        n_cond_tk = len(tokens_conditionning)",
            "",
            "",
            "        # Tokenize the internet search results text and calculate its number of tokens",
            "        if len(internet_search_results)>0:",
            "            tokens_internet_search_results = self.model.tokenize(internet_search_results)",
            "            n_isearch_tk = len(tokens_internet_search_results)",
            "        else:",
            "            tokens_internet_search_results = []",
            "            n_isearch_tk = 0",
            "",
            "",
            "        # Tokenize the documentation text and calculate its number of tokens",
            "        if len(documentation)>0:",
            "            tokens_documentation = self.model.tokenize(documentation)",
            "            n_doc_tk = len(tokens_documentation)",
            "        else:",
            "            tokens_documentation = []",
            "            n_doc_tk = 0",
            "",
            "        # Tokenize the knowledge text and calculate its number of tokens",
            "        if len(knowledge)>0:",
            "            tokens_history = self.model.tokenize(knowledge)",
            "            n_history_tk = len(tokens_history)",
            "        else:",
            "            tokens_history = []",
            "            n_history_tk = 0",
            "",
            "",
            "        # Tokenize user description",
            "        if len(user_description)>0:",
            "            tokens_user_description = self.model.tokenize(user_description)",
            "            n_user_description_tk = len(tokens_user_description)",
            "        else:",
            "            tokens_user_description = []",
            "            n_user_description_tk = 0",
            "",
            "",
            "        # Calculate the total number of tokens between conditionning, documentation, and knowledge",
            "        total_tokens = n_cond_tk + n_isearch_tk + n_doc_tk + n_history_tk + n_user_description_tk + n_positive_boost + n_negative_boost + n_fun_mode",
            "",
            "        # Calculate the available space for the messages",
            "        available_space = min(self.config.ctx_size - n_tokens - total_tokens, self.config.max_n_predict)",
            "",
            "        # if self.config.debug:",
            "        #     self.info(f\"Tokens summary:\\nConditionning:{n_cond_tk}\\nn_isearch_tk:{n_isearch_tk}\\ndoc:{n_doc_tk}\\nhistory:{n_history_tk}\\nuser description:{n_user_description_tk}\\nAvailable space:{available_space}\",10)",
            "",
            "        # Raise an error if the available space is 0 or less",
            "        if available_space<1:",
            "            ASCIIColors.red(f\"available_space:{available_space}\")",
            "            ASCIIColors.red(f\"n_doc_tk:{n_doc_tk}\")",
            "            ",
            "            ASCIIColors.red(f\"n_history_tk:{n_history_tk}\")",
            "            ASCIIColors.red(f\"n_isearch_tk:{n_isearch_tk}\")",
            "            ",
            "            ASCIIColors.red(f\"n_tokens:{n_tokens}\")",
            "            ASCIIColors.red(f\"self.config.max_n_predict:{self.config.max_n_predict}\")",
            "            self.InfoMessage(f\"Not enough space in context!!\\nVerify that your vectorization settings for documents or internet search are realistic compared to your context size.\\nYou are {available_space} short of context!\")",
            "            raise Exception(\"Not enough space in context!!\")",
            "",
            "        # Accumulate messages until the cumulative number of tokens exceeds available_space",
            "        tokens_accumulated = 0",
            "",
            "",
            "        # Initialize a list to store the full messages",
            "        full_message_list = []",
            "        # If this is not a continue request, we add the AI prompt",
            "        if not is_continue:",
            "            message_tokenized = self.model.tokenize(",
            "                self.personality.ai_message_prefix.strip()",
            "            )",
            "            full_message_list.append(message_tokenized)",
            "            # Update the cumulative number of tokens",
            "            tokens_accumulated += len(message_tokenized)",
            "",
            "",
            "        if generation_type != \"simple_question\":",
            "            # Accumulate messages starting from message_index",
            "            for i in range(message_index, -1, -1):",
            "                message = messages[i]",
            "",
            "                # Check if the message content is not empty and visible to the AI",
            "                if message.content != '' and (",
            "                        message.message_type <= MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER.value and message.message_type != MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI.value):",
            "",
            "                    # Tokenize the message content",
            "                    if self.config.use_model_name_in_discussions:",
            "                        if message.model:",
            "                            msg = f\"{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}({message.model}){end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip() + f\"{self.separator_template}\"",
            "                        else:",
            "                            msg = f\"{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip() + f\"{self.separator_template}\"",
            "                        message_tokenized = self.model.tokenize(msg)",
            "                    else:",
            "                        msg_value= f\"{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip() + f\"{self.separator_template}\"",
            "                        message_tokenized = self.model.tokenize(",
            "                            msg_value",
            "                        )",
            "                    # Check if adding the message will exceed the available space",
            "                    if tokens_accumulated + len(message_tokenized) > available_space:",
            "                        # Update the cumulative number of tokens",
            "                        msg = message_tokenized[-(available_space-tokens_accumulated):]",
            "                        tokens_accumulated += available_space-tokens_accumulated",
            "                        full_message_list.insert(0, msg)",
            "                        break",
            "",
            "                    # Add the tokenized message to the full_message_list",
            "                    full_message_list.insert(0, message_tokenized)",
            "",
            "                    # Update the cumulative number of tokens",
            "                    tokens_accumulated += len(message_tokenized)",
            "        else:",
            "            message = messages[message_index]",
            "",
            "            # Check if the message content is not empty and visible to the AI",
            "            if message.content != '' and (",
            "                    message.message_type <= MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER.value and message.message_type != MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI.value):",
            "",
            "                if self.config.use_model_name_in_discussions:",
            "                    if message.model:",
            "                        msg = f\"{self.separator_template}{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}({message.model}){end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip()",
            "                    else:",
            "                        msg = f\"{self.separator_template}{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip()",
            "                    message_tokenized = self.model.tokenize(msg)",
            "                else:",
            "                    message_tokenized = self.model.tokenize(",
            "                        f\"{self.separator_template}{start_ai_header_id_template if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.start_user_header_id_template}{message.sender}{end_ai_header_id_template  if message.sender_type == SENDER_TYPES.SENDER_TYPES_AI else self.end_user_header_id_template}\" + message.content.strip()",
            "                    )",
            "",
            "                # Add the tokenized message to the full_message_list",
            "                full_message_list.insert(0, message_tokenized)",
            "",
            "                # Update the cumulative number of tokens",
            "                tokens_accumulated += len(message_tokenized)",
            "",
            "        # Build the final discussion messages by detokenizing the full_message_list",
            "        discussion_messages = \"\"",
            "        for i in range(len(full_message_list)-1 if not is_continue else len(full_message_list)):",
            "            message_tokens = full_message_list[i]",
            "            discussion_messages += self.model.detokenize(message_tokens)",
            "        ",
            "        if len(full_message_list)>0:",
            "            ai_prefix = self.personality.ai_message_prefix",
            "        else:",
            "            ai_prefix = \"\"",
            "        # Build the final prompt by concatenating the conditionning and discussion messages",
            "        prompt_data = conditionning + internet_search_results + documentation + knowledge + user_description + discussion_messages + positive_boost + negative_boost + fun_mode + (start_ai_header_id_template + ai_prefix + end_ai_header_id_template if not is_continue else '')",
            "",
            "        # Tokenize the prompt data",
            "        tokens = self.model.tokenize(prompt_data)",
            "",
            "        # Details",
            "        context_details = {",
            "            \"client_id\":client_id,",
            "            \"conditionning\":conditionning,",
            "            \"internet_search_infos\":internet_search_infos,",
            "            \"internet_search_results\":internet_search_results,",
            "            \"documentation\":documentation,",
            "            \"documentation_entries\":documentation_entries,",
            "            \"knowledge\":knowledge,",
            "            \"knowledge_infos\":knowledge_infos,",
            "            \"user_description\":user_description,",
            "            \"discussion_messages\":discussion_messages,",
            "            \"positive_boost\":positive_boost,",
            "            \"negative_boost\":negative_boost,",
            "            \"current_language\":self.config.current_language,",
            "            \"fun_mode\":fun_mode,",
            "            \"ai_prefix\":ai_prefix,",
            "            \"extra\":\"\"",
            "        }    ",
            "        if self.config.debug:",
            "            ASCIIColors.highlight(documentation,\"source_document_title\", ASCIIColors.color_yellow, ASCIIColors.color_red, False)",
            "        # Return the prepared query, original message content, and tokenized query",
            "        return prompt_data, current_message.content, tokens, context_details, internet_search_infos                ",
            "",
            "",
            "    # Properties ===============================================",
            "    @property",
            "    def start_header_id_template(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return self.config.start_header_id_template",
            "",
            "    @property",
            "    def end_header_id_template(self) -> str:",
            "        \"\"\"Get the end_header_id_template.\"\"\"",
            "        return self.config.end_header_id_template",
            "    ",
            "    @property",
            "    def system_message_template(self) -> str:",
            "        \"\"\"Get the system_message_template.\"\"\"",
            "        return self.config.system_message_template",
            "",
            "",
            "    @property",
            "    def separator_template(self) -> str:",
            "        \"\"\"Get the separator template.\"\"\"",
            "        return self.config.separator_template",
            "",
            "",
            "    @property",
            "    def start_user_header_id_template(self) -> str:",
            "        \"\"\"Get the start_user_header_id_template.\"\"\"",
            "        return self.config.start_user_header_id_template",
            "    @property",
            "    def end_user_header_id_template(self) -> str:",
            "        \"\"\"Get the end_user_header_id_template.\"\"\"",
            "        return self.config.end_user_header_id_template",
            "    @property",
            "    def end_user_message_id_template(self) -> str:",
            "        \"\"\"Get the end_user_message_id_template.\"\"\"",
            "        return self.config.end_user_message_id_template",
            "",
            "",
            "",
            "",
            "    # Properties ===============================================",
            "    @property",
            "    def start_header_id_template(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return self.config.start_header_id_template",
            "",
            "    @property",
            "    def end_header_id_template(self) -> str:",
            "        \"\"\"Get the end_header_id_template.\"\"\"",
            "        return self.config.end_header_id_template",
            "    ",
            "    @property",
            "    def system_message_template(self) -> str:",
            "        \"\"\"Get the system_message_template.\"\"\"",
            "        return self.config.system_message_template",
            "",
            "",
            "    @property",
            "    def separator_template(self) -> str:",
            "        \"\"\"Get the separator template.\"\"\"",
            "        return self.config.separator_template",
            "",
            "",
            "    @property",
            "    def start_user_header_id_template(self) -> str:",
            "        \"\"\"Get the start_user_header_id_template.\"\"\"",
            "        return self.config.start_user_header_id_template",
            "    @property",
            "    def end_user_header_id_template(self) -> str:",
            "        \"\"\"Get the end_user_header_id_template.\"\"\"",
            "        return self.config.end_user_header_id_template",
            "    @property",
            "    def end_user_message_id_template(self) -> str:",
            "        \"\"\"Get the end_user_message_id_template.\"\"\"",
            "        return self.config.end_user_message_id_template",
            "",
            "",
            "",
            "",
            "    @property",
            "    def start_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the start_ai_header_id_template.\"\"\"",
            "        return self.config.start_ai_header_id_template",
            "    @property",
            "    def end_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_header_id_template.\"\"\"",
            "        return self.config.end_ai_header_id_template",
            "    @property",
            "    def end_ai_message_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_message_id_template.\"\"\"",
            "        return self.config.end_ai_message_id_template",
            "    @property",
            "    def system_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_header_id_template}{self.system_message_template}{self.end_header_id_template}\"",
            "    @property",
            "    def user_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.config.user_name}{self.end_user_header_id_template}\"",
            "    @property",
            "    def ai_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.personality.name}{self.end_user_header_id_template}\"",
            "",
            "    def system_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\"",
            "",
            "    def ai_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\""
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "908": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1096": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1097": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1098": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1099": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1100": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1101": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1102": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1103": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1104": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1105": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1106": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1107": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1108": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1109": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1110": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1112": [
                "LollmsApplication",
                "prepare_query"
            ],
            "1114": [
                "LollmsApplication",
                "prepare_query"
            ]
        },
        "addLocation": []
    },
    "lollms/com.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 48,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "         self.rt_com = None"
            },
            "2": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        "
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 51,
                "PatchRowcode": "+        self.model = None"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 52,
                "PatchRowcode": "+                "
            },
            "6": {
                "beforePatchRowNumber": 52,
                "afterPatchRowNumber": 53,
                "PatchRowcode": "     def InfoMessage(self, content, client_id=None, verbose:bool=None):"
            },
            "7": {
                "beforePatchRowNumber": 53,
                "afterPatchRowNumber": 54,
                "PatchRowcode": "         self.notify("
            },
            "8": {
                "beforePatchRowNumber": 54,
                "afterPatchRowNumber": 55,
                "PatchRowcode": "                 content, "
            }
        },
        "frontPatchFile": [
            "from ascii_colors import ASCIIColors",
            "from lollms.types import MSG_TYPE, SENDER_TYPES",
            "from typing import Callable",
            "import socketio",
            "from enum import Enum",
            "class NotificationType(Enum):",
            "    \"\"\"Notification types.\"\"\"",
            "    ",
            "    NOTIF_ERROR = 0",
            "    \"\"\"This is an error notification.\"\"\"",
            "    ",
            "    NOTIF_SUCCESS = 1",
            "    \"\"\"This is a success notification.\"\"\"",
            "",
            "    NOTIF_INFO = 2",
            "    \"\"\"This is an information notification.\"\"\"",
            "",
            "    NOTIF_WARNING = 3",
            "    \"\"\"This is a warining notification.\"\"\"",
            "",
            "class NotificationDisplayType(Enum):",
            "    \"\"\"Notification display types.\"\"\"",
            "    ",
            "    TOAST = 0",
            "    \"\"\"This is a toast.\"\"\"",
            "    ",
            "    MESSAGE_BOX = 1",
            "    \"\"\"This is a message box.\"\"\"",
            "",
            "    YESNO_MESSAGE = 2",
            "    \"\"\"This is a yes not messagebox.\"\"\"",
            "",
            "    SHOW_BLOCKING_MESSAGE = 3",
            "    \"\"\"This shows a blocking messagebox.\"\"\"",
            "",
            "    HIDE_BLOCKING_MESSAGE = 4",
            "    \"\"\"This hides a blocking messagebox.\"\"\"",
            "",
            "",
            "class LoLLMsCom:",
            "    def __init__(self, sio:socketio.AsyncServer=None, verbose:bool=False) -> None:",
            "        self.sio= sio",
            "        self.verbose = verbose",
            "        self.config = None ",
            "        self.tti = None",
            "        self.tts = None",
            "        self.stt = None",
            "",
            "        self.rt_com = None",
            "",
            "        ",
            "    def InfoMessage(self, content, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.MESSAGE_BOX,",
            "                verbose=verbose",
            "            )",
            "    def ShowBlockingMessage(self, content, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.SHOW_BLOCKING_MESSAGE,",
            "                verbose=verbose",
            "            )        ",
            "        ",
            "    def HideBlockingMessage(self, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                \"\", ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.HIDE_BLOCKING_MESSAGE,",
            "                verbose=verbose",
            "            )        ",
            "",
            "",
            "",
            "    def YesNoMessage(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        infos={",
            "            \"wait\":True,",
            "            \"result\":False",
            "        }",
            "        @self.sio.on('yesNoRes')",
            "        def yesnores(result):",
            "            infos[\"result\"] = result[\"yesRes\"]",
            "            infos[\"wait\"]=False",
            "",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.YESNO_MESSAGE,",
            "                verbose=verbose",
            "            )",
            "        # wait",
            "        ASCIIColors.yellow(\"Waiting for yes no question to be answered\")",
            "        while infos[\"wait\"]:",
            "            self.sio.sleep(1)",
            "        return infos[\"result\"]",
            "",
            "    def close_message(self, client_id):",
            "        pass",
            "    ",
            "    def info(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose=verbose",
            "            )",
            "",
            "    def warning(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_WARNING, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose=verbose",
            "            )",
            "",
            "    def success(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose=verbose",
            "            )",
            "        ",
            "    def error(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_ERROR, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose = verbose",
            "            )",
            "        ",
            "    def new_message(self, ",
            "                            client_id, ",
            "                            sender=None, ",
            "                            content=\"\",",
            "                            parameters=None,",
            "                            metadata=None,",
            "                            ui=None,",
            "                            message_type:MSG_TYPE=MSG_TYPE.MSG_TYPE_FULL, ",
            "                            sender_type:SENDER_TYPES=SENDER_TYPES.SENDER_TYPES_AI,",
            "                            open=False",
            "                        ):",
            "        pass",
            "    def full(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        pass",
            "",
            "    ",
            "    def emit_socket_io_info(self, name, data, client_id):",
            "        pass",
            "",
            "    def notify(",
            "                self, ",
            "                content:str, ",
            "                notification_type:NotificationType=NotificationType.NOTIF_SUCCESS, ",
            "                duration:int=4, ",
            "                client_id=None, ",
            "                display_type:NotificationDisplayType=NotificationDisplayType.TOAST,",
            "                verbose:bool|None=None",
            "            ):",
            "        if verbose is None:",
            "            verbose = self.verbose",
            "",
            "        if verbose:",
            "            if notification_type==NotificationType.NOTIF_SUCCESS:",
            "                ASCIIColors.success(content)",
            "            elif notification_type==NotificationType.NOTIF_INFO:",
            "                ASCIIColors.info(content)",
            "            elif notification_type==NotificationType.NOTIF_WARNING:",
            "                ASCIIColors.warning(content)",
            "            else:",
            "                ASCIIColors.red(content)",
            "",
            "",
            "    def notify_model_install(self, ",
            "                            installation_path,",
            "                            model_name,",
            "                            binding_folder,",
            "                            model_url,",
            "                            start_time,",
            "                            total_size,",
            "                            downloaded_size,",
            "                            progress,",
            "                            speed,",
            "                            client_id,",
            "                            status=True,",
            "                            error=\"\",",
            "                             ):",
            "        pass"
        ],
        "afterPatchFile": [
            "from ascii_colors import ASCIIColors",
            "from lollms.types import MSG_TYPE, SENDER_TYPES",
            "from typing import Callable",
            "import socketio",
            "from enum import Enum",
            "class NotificationType(Enum):",
            "    \"\"\"Notification types.\"\"\"",
            "    ",
            "    NOTIF_ERROR = 0",
            "    \"\"\"This is an error notification.\"\"\"",
            "    ",
            "    NOTIF_SUCCESS = 1",
            "    \"\"\"This is a success notification.\"\"\"",
            "",
            "    NOTIF_INFO = 2",
            "    \"\"\"This is an information notification.\"\"\"",
            "",
            "    NOTIF_WARNING = 3",
            "    \"\"\"This is a warining notification.\"\"\"",
            "",
            "class NotificationDisplayType(Enum):",
            "    \"\"\"Notification display types.\"\"\"",
            "    ",
            "    TOAST = 0",
            "    \"\"\"This is a toast.\"\"\"",
            "    ",
            "    MESSAGE_BOX = 1",
            "    \"\"\"This is a message box.\"\"\"",
            "",
            "    YESNO_MESSAGE = 2",
            "    \"\"\"This is a yes not messagebox.\"\"\"",
            "",
            "    SHOW_BLOCKING_MESSAGE = 3",
            "    \"\"\"This shows a blocking messagebox.\"\"\"",
            "",
            "    HIDE_BLOCKING_MESSAGE = 4",
            "    \"\"\"This hides a blocking messagebox.\"\"\"",
            "",
            "",
            "class LoLLMsCom:",
            "    def __init__(self, sio:socketio.AsyncServer=None, verbose:bool=False) -> None:",
            "        self.sio= sio",
            "        self.verbose = verbose",
            "        self.config = None ",
            "        self.tti = None",
            "        self.tts = None",
            "        self.stt = None",
            "",
            "        self.rt_com = None",
            "",
            "        self.model = None",
            "                ",
            "    def InfoMessage(self, content, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.MESSAGE_BOX,",
            "                verbose=verbose",
            "            )",
            "    def ShowBlockingMessage(self, content, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.SHOW_BLOCKING_MESSAGE,",
            "                verbose=verbose",
            "            )        ",
            "        ",
            "    def HideBlockingMessage(self, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                \"\", ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.HIDE_BLOCKING_MESSAGE,",
            "                verbose=verbose",
            "            )        ",
            "",
            "",
            "",
            "    def YesNoMessage(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        infos={",
            "            \"wait\":True,",
            "            \"result\":False",
            "        }",
            "        @self.sio.on('yesNoRes')",
            "        def yesnores(result):",
            "            infos[\"result\"] = result[\"yesRes\"]",
            "            infos[\"wait\"]=False",
            "",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.YESNO_MESSAGE,",
            "                verbose=verbose",
            "            )",
            "        # wait",
            "        ASCIIColors.yellow(\"Waiting for yes no question to be answered\")",
            "        while infos[\"wait\"]:",
            "            self.sio.sleep(1)",
            "        return infos[\"result\"]",
            "",
            "    def close_message(self, client_id):",
            "        pass",
            "    ",
            "    def info(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose=verbose",
            "            )",
            "",
            "    def warning(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_WARNING, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose=verbose",
            "            )",
            "",
            "    def success(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose=verbose",
            "            )",
            "        ",
            "    def error(self, content, duration:int=4, client_id=None, verbose:bool=None):",
            "        self.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_ERROR, ",
            "                duration=duration, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.TOAST,",
            "                verbose = verbose",
            "            )",
            "        ",
            "    def new_message(self, ",
            "                            client_id, ",
            "                            sender=None, ",
            "                            content=\"\",",
            "                            parameters=None,",
            "                            metadata=None,",
            "                            ui=None,",
            "                            message_type:MSG_TYPE=MSG_TYPE.MSG_TYPE_FULL, ",
            "                            sender_type:SENDER_TYPES=SENDER_TYPES.SENDER_TYPES_AI,",
            "                            open=False",
            "                        ):",
            "        pass",
            "    def full(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        pass",
            "",
            "    ",
            "    def emit_socket_io_info(self, name, data, client_id):",
            "        pass",
            "",
            "    def notify(",
            "                self, ",
            "                content:str, ",
            "                notification_type:NotificationType=NotificationType.NOTIF_SUCCESS, ",
            "                duration:int=4, ",
            "                client_id=None, ",
            "                display_type:NotificationDisplayType=NotificationDisplayType.TOAST,",
            "                verbose:bool|None=None",
            "            ):",
            "        if verbose is None:",
            "            verbose = self.verbose",
            "",
            "        if verbose:",
            "            if notification_type==NotificationType.NOTIF_SUCCESS:",
            "                ASCIIColors.success(content)",
            "            elif notification_type==NotificationType.NOTIF_INFO:",
            "                ASCIIColors.info(content)",
            "            elif notification_type==NotificationType.NOTIF_WARNING:",
            "                ASCIIColors.warning(content)",
            "            else:",
            "                ASCIIColors.red(content)",
            "",
            "",
            "    def notify_model_install(self, ",
            "                            installation_path,",
            "                            model_name,",
            "                            binding_folder,",
            "                            model_url,",
            "                            start_time,",
            "                            total_size,",
            "                            downloaded_size,",
            "                            progress,",
            "                            speed,",
            "                            client_id,",
            "                            status=True,",
            "                            error=\"\",",
            "                             ):",
            "        pass"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "51": [
                "LoLLMsCom"
            ]
        },
        "addLocation": []
    },
    "lollms/databases/discussions_database.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 10,
                "afterPatchRowNumber": 10,
                "PatchRowcode": " from lollms.databases.skills_database import SkillsLibrary"
            },
            "1": {
                "beforePatchRowNumber": 11,
                "afterPatchRowNumber": 11,
                "PatchRowcode": " from lollms.com import LoLLMsCom"
            },
            "2": {
                "beforePatchRowNumber": 12,
                "afterPatchRowNumber": 12,
                "PatchRowcode": " from safe_store import TextVectorizer, VisualizationMethod, GenericDataLoader"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 13,
                "PatchRowcode": "+from lollmsvectordb.vector_database import VectorDatabase"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 14,
                "PatchRowcode": "+from lollmsvectordb.lollms_vectorizers.bert_vectorizer import BERTVectorizer"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 15,
                "PatchRowcode": "+from lollmsvectordb.lollms_vectorizers.tfidf_vectorizer import TFIDFVectorizer"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 16,
                "PatchRowcode": "+from lollmsvectordb.text_document_loader import TextDocumentsLoader"
            },
            "7": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 17,
                "PatchRowcode": "+import gc"
            },
            "8": {
                "beforePatchRowNumber": 13,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " import json"
            },
            "9": {
                "beforePatchRowNumber": 14,
                "afterPatchRowNumber": 19,
                "PatchRowcode": " import shutil"
            },
            "10": {
                "beforePatchRowNumber": 15,
                "afterPatchRowNumber": 20,
                "PatchRowcode": " from lollms.tasks import TasksLibrary"
            },
            "11": {
                "beforePatchRowNumber": 651,
                "afterPatchRowNumber": 656,
                "PatchRowcode": "         # Initialize the file lists"
            },
            "12": {
                "beforePatchRowNumber": 652,
                "afterPatchRowNumber": 657,
                "PatchRowcode": "         self.update_file_lists()"
            },
            "13": {
                "beforePatchRowNumber": 653,
                "afterPatchRowNumber": 658,
                "PatchRowcode": " "
            },
            "14": {
                "beforePatchRowNumber": 654,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-"
            },
            "15": {
                "beforePatchRowNumber": 655,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.vectorizer = TextVectorizer("
            },
            "16": {
                "beforePatchRowNumber": 656,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            self.lollms.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\""
            },
            "17": {
                "beforePatchRowNumber": 657,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            model=self.lollms.model, #needed in case of using model_embedding"
            },
            "18": {
                "beforePatchRowNumber": 658,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            database_path=self.discussion_rag_folder/\"db.json\","
            },
            "19": {
                "beforePatchRowNumber": 659,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            save_db=self.lollms.config.data_vectorization_save_db,"
            },
            "20": {
                "beforePatchRowNumber": 660,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            data_visualization_method=VisualizationMethod.PCA,"
            },
            "21": {
                "beforePatchRowNumber": 661,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            database_dict=None)"
            },
            "22": {
                "beforePatchRowNumber": 662,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-    "
            },
            "23": {
                "beforePatchRowNumber": 663,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        if len(self.vectorizer.chunks)==0 and len(self.text_files)>0:"
            },
            "24": {
                "beforePatchRowNumber": 664,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            for path in self.text_files:"
            },
            "25": {
                "beforePatchRowNumber": 665,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                data = GenericDataLoader.read_file(path)"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 659,
                "PatchRowcode": "+        if len(self.text_files)>0:"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 660,
                "PatchRowcode": "+            self.vectorizer = VectorDatabase("
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 661,
                "PatchRowcode": "+                                        self.discussion_rag_folder/\"db.sqli\","
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 662,
                "PatchRowcode": "+                                        BERTVectorizer(self.lollms.config.rag_vectorizer_model) if self.lollms.config.rag_vectorizer==\"bert\" else TFIDFVectorizer(),"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 663,
                "PatchRowcode": "+                                        self.lollms.model,"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 664,
                "PatchRowcode": "+                                        chunk_size=self.lollms.config.rag_chunk_size,"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 665,
                "PatchRowcode": "+                                        overlap=self.lollms.config.rag_overlap"
            },
            "33": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 666,
                "PatchRowcode": "+                                        )"
            },
            "34": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 667,
                "PatchRowcode": "+            "
            },
            "35": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 668,
                "PatchRowcode": "+            if len(self.vectorizer.list_documents())==0 and len(self.text_files)>0:"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 669,
                "PatchRowcode": "+                for path in self.text_files:"
            },
            "37": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 670,
                "PatchRowcode": "+                    data = GenericDataLoader.read_file(path)"
            },
            "38": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 671,
                "PatchRowcode": "+                    try:"
            },
            "39": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 672,
                "PatchRowcode": "+                        self.vectorizer.add_document(path.stem, data, path, True)"
            },
            "40": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 673,
                "PatchRowcode": "+                    except Exception as ex:"
            },
            "41": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 674,
                "PatchRowcode": "+                        trace_exception(ex)            "
            },
            "42": {
                "beforePatchRowNumber": 666,
                "afterPatchRowNumber": 675,
                "PatchRowcode": "                 try:"
            },
            "43": {
                "beforePatchRowNumber": 667,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.vectorizer.add_document(path, data, self.lollms.config.data_vectorization_chunk_size, self.lollms.config.data_vectorization_overlap_size, add_first_line_to_all_chunks=True if path.suffix==\".csv\" else False)"
            },
            "44": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 676,
                "PatchRowcode": "+                    self.vectorizer.index()"
            },
            "45": {
                "beforePatchRowNumber": 668,
                "afterPatchRowNumber": 677,
                "PatchRowcode": "                 except Exception as ex:"
            },
            "46": {
                "beforePatchRowNumber": 669,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    trace_exception(ex)            "
            },
            "47": {
                "beforePatchRowNumber": 670,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            try:"
            },
            "48": {
                "beforePatchRowNumber": 671,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                self.vectorizer.index()"
            },
            "49": {
                "beforePatchRowNumber": 672,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            except Exception as ex:"
            },
            "50": {
                "beforePatchRowNumber": 673,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                trace_exception(ex)            "
            },
            "51": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 678,
                "PatchRowcode": "+                    trace_exception(ex)"
            },
            "52": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 679,
                "PatchRowcode": "+        else:"
            },
            "53": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 680,
                "PatchRowcode": "+            self.vectorizer = None"
            },
            "54": {
                "beforePatchRowNumber": 674,
                "afterPatchRowNumber": 681,
                "PatchRowcode": " "
            },
            "55": {
                "beforePatchRowNumber": 675,
                "afterPatchRowNumber": 682,
                "PatchRowcode": "     def update_file_lists(self):"
            },
            "56": {
                "beforePatchRowNumber": 676,
                "afterPatchRowNumber": 683,
                "PatchRowcode": "         self.text_files = [Path(file) for file in self.discussion_text_folder.glob('*')]"
            },
            "57": {
                "beforePatchRowNumber": 685,
                "afterPatchRowNumber": 692,
                "PatchRowcode": "             if any(file_name == entry.name for entry in self.text_files):"
            },
            "58": {
                "beforePatchRowNumber": 686,
                "afterPatchRowNumber": 693,
                "PatchRowcode": "                 fn = [entry for entry in self.text_files if entry.name == file_name][0]"
            },
            "59": {
                "beforePatchRowNumber": 687,
                "afterPatchRowNumber": 694,
                "PatchRowcode": "                 self.text_files = [entry for entry in self.text_files if entry.name != file_name]"
            },
            "60": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 695,
                "PatchRowcode": "+                try:"
            },
            "61": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 696,
                "PatchRowcode": "+                    text = TextDocumentsLoader.read_file(fn)"
            },
            "62": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 697,
                "PatchRowcode": "+                    hash = self.vectorizer._hash_document(text)"
            },
            "63": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 698,
                "PatchRowcode": "+                    self.vectorizer.remove_document(hash)"
            },
            "64": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 699,
                "PatchRowcode": "+                except Exception as ex:"
            },
            "65": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 700,
                "PatchRowcode": "+                    trace_exception(ex)"
            },
            "66": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 701,
                "PatchRowcode": "+"
            },
            "67": {
                "beforePatchRowNumber": 688,
                "afterPatchRowNumber": 702,
                "PatchRowcode": "                 Path(fn).unlink()"
            },
            "68": {
                "beforePatchRowNumber": 689,
                "afterPatchRowNumber": 703,
                "PatchRowcode": "                 if len(self.text_files)>0:"
            },
            "69": {
                "beforePatchRowNumber": 690,
                "afterPatchRowNumber": 704,
                "PatchRowcode": "                     try:"
            },
            "70": {
                "beforePatchRowNumber": 713,
                "afterPatchRowNumber": 727,
                "PatchRowcode": "     def remove_all_files(self):"
            },
            "71": {
                "beforePatchRowNumber": 714,
                "afterPatchRowNumber": 728,
                "PatchRowcode": "         # Iterate over each directory and remove all files"
            },
            "72": {
                "beforePatchRowNumber": 715,
                "afterPatchRowNumber": 729,
                "PatchRowcode": "         for path in [self.discussion_images_folder, self.discussion_rag_folder, self.discussion_audio_folder, self.discussion_text_folder]:"
            },
            "73": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 730,
                "PatchRowcode": "+            "
            },
            "74": {
                "beforePatchRowNumber": 716,
                "afterPatchRowNumber": 731,
                "PatchRowcode": "             for file in path.glob('*'):"
            },
            "75": {
                "beforePatchRowNumber": 717,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                if file.is_file():  # Ensure it's a file, not a directory"
            },
            "76": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 732,
                "PatchRowcode": "+                if file.is_file() and file.suffix!=\".sqli\":  # Ensure it's a file, not a directory"
            },
            "77": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 733,
                "PatchRowcode": "+                    try:"
            },
            "78": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 734,
                "PatchRowcode": "+                        text = TextDocumentsLoader.read_file(file)"
            },
            "79": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 735,
                "PatchRowcode": "+                        hash = self.vectorizer._hash_document(text)"
            },
            "80": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 736,
                "PatchRowcode": "+                        self.vectorizer.remove_document(hash)"
            },
            "81": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 737,
                "PatchRowcode": "+                    except Exception as ex:"
            },
            "82": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 738,
                "PatchRowcode": "+                        trace_exception(ex)"
            },
            "83": {
                "beforePatchRowNumber": 718,
                "afterPatchRowNumber": 739,
                "PatchRowcode": "                     file.unlink()  # Delete the file"
            },
            "84": {
                "beforePatchRowNumber": 719,
                "afterPatchRowNumber": 740,
                "PatchRowcode": "                     "
            },
            "85": {
                "beforePatchRowNumber": 720,
                "afterPatchRowNumber": 741,
                "PatchRowcode": "         # Clear the lists to reflect the current state (empty directories)"
            },
            "86": {
                "beforePatchRowNumber": 721,
                "afterPatchRowNumber": 742,
                "PatchRowcode": "         self.text_files.clear()"
            },
            "87": {
                "beforePatchRowNumber": 722,
                "afterPatchRowNumber": 743,
                "PatchRowcode": "         self.image_files.clear()"
            },
            "88": {
                "beforePatchRowNumber": 723,
                "afterPatchRowNumber": 744,
                "PatchRowcode": "         self.audio_files.clear()"
            },
            "89": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 745,
                "PatchRowcode": "+        self.vectorizer = None"
            },
            "90": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 746,
                "PatchRowcode": "+        gc.collect()"
            },
            "91": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 747,
                "PatchRowcode": "+        fn = self.discussion_rag_folder/\"db.sqli\""
            },
            "92": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 748,
                "PatchRowcode": "+        try:"
            },
            "93": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 749,
                "PatchRowcode": "+            fn.unlink()"
            },
            "94": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 750,
                "PatchRowcode": "+        except Exception as ex:"
            },
            "95": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 751,
                "PatchRowcode": "+            trace_exception(ex)"
            },
            "96": {
                "beforePatchRowNumber": 724,
                "afterPatchRowNumber": 752,
                "PatchRowcode": " "
            },
            "97": {
                "beforePatchRowNumber": 725,
                "afterPatchRowNumber": 753,
                "PatchRowcode": "     def add_file(self, path, client, tasks_library:TasksLibrary, callback=None, process=True):"
            },
            "98": {
                "beforePatchRowNumber": 726,
                "afterPatchRowNumber": 754,
                "PatchRowcode": "         output = \"\""
            },
            "99": {
                "beforePatchRowNumber": 787,
                "afterPatchRowNumber": 815,
                "PatchRowcode": "                 self.lollms.ShowBlockingMessage(\"Processing file\\nPlease wait ...\")"
            },
            "100": {
                "beforePatchRowNumber": 788,
                "afterPatchRowNumber": 816,
                "PatchRowcode": "                 if process:"
            },
            "101": {
                "beforePatchRowNumber": 789,
                "afterPatchRowNumber": 817,
                "PatchRowcode": "                     if self.vectorizer is None:"
            },
            "102": {
                "beforePatchRowNumber": 790,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        self.vectorizer = TextVectorizer("
            },
            "103": {
                "beforePatchRowNumber": 791,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    self.lollms.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\""
            },
            "104": {
                "beforePatchRowNumber": 792,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    model=self.lollms.model, #needed in case of using model_embedding"
            },
            "105": {
                "beforePatchRowNumber": 793,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    database_path=self.discussion_rag_folder/\"db.json\","
            },
            "106": {
                "beforePatchRowNumber": 794,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    save_db=self.lollms.config.data_vectorization_save_db,"
            },
            "107": {
                "beforePatchRowNumber": 795,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    data_visualization_method=VisualizationMethod.PCA,"
            },
            "108": {
                "beforePatchRowNumber": 796,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    database_dict=None)"
            },
            "109": {
                "beforePatchRowNumber": 797,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    data = GenericDataLoader.read_file(path)"
            },
            "110": {
                "beforePatchRowNumber": 798,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.vectorizer.add_document(path, data, self.lollms.config.data_vectorization_chunk_size, self.lollms.config.data_vectorization_overlap_size, add_first_line_to_all_chunks=True if path.suffix==\".csv\" else False)"
            },
            "111": {
                "beforePatchRowNumber": 799,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.vectorizer.index()"
            },
            "112": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 818,
                "PatchRowcode": "+                        self.vectorizer = VectorDatabase("
            },
            "113": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 819,
                "PatchRowcode": "+                                    self.discussion_rag_folder/\"db.sqli\","
            },
            "114": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 820,
                "PatchRowcode": "+                                    BERTVectorizer(self.lollms.config.rag_vectorizer_model) if self.lollms.config.rag_vectorizer==\"bert\" else TFIDFVectorizer(),"
            },
            "115": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 821,
                "PatchRowcode": "+                                    self.lollms.model,"
            },
            "116": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 822,
                "PatchRowcode": "+                                    )"
            },
            "117": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 823,
                "PatchRowcode": "+                    data = TextDocumentsLoader.read_file(path)"
            },
            "118": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 824,
                "PatchRowcode": "+                    self.vectorizer.add_document(path.stem, data, path, True)"
            },
            "119": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 825,
                "PatchRowcode": "+                    self.vectorizer.build_index()"
            },
            "120": {
                "beforePatchRowNumber": 800,
                "afterPatchRowNumber": 826,
                "PatchRowcode": "                     if callback is not None:"
            },
            "121": {
                "beforePatchRowNumber": 801,
                "afterPatchRowNumber": 827,
                "PatchRowcode": "                         callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)"
            },
            "122": {
                "beforePatchRowNumber": 802,
                "afterPatchRowNumber": 828,
                "PatchRowcode": "                     self.lollms.HideBlockingMessage(client.client_id)"
            }
        },
        "frontPatchFile": [
            "",
            "import sqlite3",
            "from pathlib import Path",
            "from datetime import datetime",
            "from ascii_colors import ASCIIColors, trace_exception",
            "from lollms.types import MSG_TYPE",
            "from lollms.types import BindingType",
            "from lollms.utilities import PackageManager, discussion_path_to_url",
            "from lollms.paths import LollmsPaths",
            "from lollms.databases.skills_database import SkillsLibrary",
            "from lollms.com import LoLLMsCom",
            "from safe_store import TextVectorizer, VisualizationMethod, GenericDataLoader",
            "import json",
            "import shutil",
            "from lollms.tasks import TasksLibrary",
            "__author__ = \"parisneo\"",
            "__github__ = \"https://github.com/ParisNeo/lollms-webui\"",
            "__copyright__ = \"Copyright 2023, \"",
            "__license__ = \"Apache 2.0\"",
            "",
            "",
            "# =================================== Database ==================================================================",
            "class DiscussionsDB:",
            "    ",
            "    def __init__(self, lollms:LoLLMsCom, lollms_paths:LollmsPaths, discussion_db_name=\"default\"):",
            "        self.lollms = lollms",
            "        self.lollms_paths = lollms_paths",
            "        ",
            "        self.discussion_db_name = discussion_db_name",
            "        self.discussion_db_path = self.lollms_paths.personal_discussions_path/discussion_db_name",
            "",
            "        self.discussion_db_path.mkdir(exist_ok=True, parents= True)",
            "        self.discussion_db_file_path = self.discussion_db_path/\"database.db\"",
            "",
            "    def create_tables(self):",
            "        db_version = 12",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.cursor()",
            "",
            "            cursor.execute(\"\"\"",
            "                CREATE TABLE IF NOT EXISTS schema_version (",
            "                    id INTEGER PRIMARY KEY AUTOINCREMENT,",
            "                    version INTEGER NOT NULL",
            "                )",
            "            \"\"\")",
            "",
            "            cursor.execute(\"\"\"",
            "                CREATE TABLE IF NOT EXISTS discussion (",
            "                    id INTEGER PRIMARY KEY AUTOINCREMENT,",
            "                    title TEXT,",
            "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP",
            "                )",
            "            \"\"\")",
            "",
            "            cursor.execute(\"\"\"",
            "                CREATE TABLE IF NOT EXISTS message (",
            "                    id INTEGER PRIMARY KEY AUTOINCREMENT,",
            "                    binding TEXT,",
            "                    model TEXT,",
            "                    personality TEXT,",
            "                    sender TEXT NOT NULL,",
            "                    content TEXT NOT NULL,",
            "                    message_type INT NOT NULL,",
            "                    sender_type INT DEFAULT 0,",
            "                    rank INT NOT NULL DEFAULT 0,",
            "                    parent_message_id INT,",
            "                    created_at TIMESTAMP,",
            "                    started_generating_at TIMESTAMP,",
            "                    finished_generating_at TIMESTAMP,",
            "                    nb_tokens INT,                    ",
            "                    discussion_id INTEGER NOT NULL,",
            "                    metadata TEXT,",
            "                    ui TEXT,",
            "                    FOREIGN KEY (discussion_id) REFERENCES discussion(id),",
            "                    FOREIGN KEY (parent_message_id) REFERENCES message(id)",
            "                )",
            "            \"\"\")",
            "",
            "            cursor.execute(\"SELECT * FROM schema_version\")",
            "            row = cursor.fetchone()",
            "",
            "            if row is None:",
            "                cursor.execute(\"INSERT INTO schema_version (version) VALUES (?)\", (db_version,))",
            "            else:",
            "                cursor.execute(\"UPDATE schema_version SET version = ?\", (db_version,))            ",
            "",
            "            conn.commit()",
            "",
            "    def add_missing_columns(self):",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.cursor()",
            "",
            "            table_columns = {",
            "                'discussion': [",
            "                    'id',",
            "                    'title',",
            "                    'created_at'",
            "                ],",
            "                'message': [",
            "                    'id',",
            "                    'binding',",
            "                    'model',",
            "                    'personality',",
            "                    'sender',",
            "                    'content',",
            "                    'message_type',",
            "                    'sender_type',",
            "                    'rank',",
            "                    'parent_message_id',",
            "                    'created_at',",
            "                    'metadata',",
            "                    'ui',",
            "                    'started_generating_at',",
            "                    'finished_generating_at',",
            "                    'nb_tokens',                    ",
            "                    'discussion_id'",
            "                ]",
            "            }",
            "",
            "            for table, columns in table_columns.items():",
            "                cursor.execute(f\"PRAGMA table_info({table})\")",
            "                existing_columns = [column[1] for column in cursor.fetchall()]",
            "",
            "                for column in columns:",
            "                    if column not in existing_columns:",
            "                        if column == 'id':",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} INTEGER PRIMARY KEY AUTOINCREMENT\")",
            "                        elif column.endswith('_at'):",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} TIMESTAMP\")",
            "                        elif column=='metadata':",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} TEXT\")",
            "                        elif column=='message_type':",
            "                            cursor.execute(f\"ALTER TABLE {table} RENAME COLUMN type TO {column}\")",
            "                        elif column=='sender_type':",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} INT DEFAULT 0\")",
            "                        elif column=='parent_message_id':",
            "                            cursor.execute(f\"ALTER TABLE {table} RENAME COLUMN parent TO {column}\")",
            "                        else:",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} TEXT\")",
            "                        ASCIIColors.yellow(f\"Added column :{column}\")",
            "            conn.commit()",
            "",
            "",
            "    def select(self, query, params=None, fetch_all=True):",
            "        \"\"\"",
            "        Execute the specified SQL select query on the database,",
            "        with optional parameters.",
            "        Returns the cursor object for further processing.",
            "        \"\"\"",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            if params is None:",
            "                cursor = conn.execute(query)",
            "            else:",
            "                cursor = conn.execute(query, params)",
            "            if fetch_all:",
            "                return cursor.fetchall()",
            "            else:",
            "                return cursor.fetchone()",
            "            ",
            "",
            "    def delete(self, query, params=None):",
            "        \"\"\"",
            "        Execute the specified SQL delete query on the database,",
            "        with optional parameters.",
            "        Returns the cursor object for further processing.",
            "        \"\"\"",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.cursor()",
            "            if params is None:",
            "                cursor.execute(query)",
            "            else:",
            "                cursor.execute(query, params)",
            "            conn.commit()",
            "   ",
            "    def insert(self, query, params=None):",
            "        \"\"\"",
            "        Execute the specified INSERT SQL query on the database,",
            "        with optional parameters.",
            "        Returns the ID of the newly inserted row.",
            "        \"\"\"",
            "        ",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.execute(query, params)",
            "            rowid = cursor.lastrowid",
            "            conn.commit()",
            "        self.conn = None",
            "        return rowid",
            "",
            "    def update(self, query, params:tuple=None):",
            "        \"\"\"",
            "        Execute the specified Update SQL query on the database,",
            "        with optional parameters.",
            "        Returns the ID of the newly inserted row.",
            "        \"\"\"",
            "        ",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            conn.execute(query, params)",
            "            conn.commit()",
            "    ",
            "    def load_last_discussion(self):",
            "        last_discussion_id = self.select(\"SELECT id FROM discussion ORDER BY id DESC LIMIT 1\", fetch_all=False)",
            "        if last_discussion_id is None:",
            "            last_discussion = self.create_discussion()",
            "            last_discussion_id = last_discussion.discussion_id",
            "        else:",
            "            last_discussion_id = last_discussion_id[0]",
            "        self.current_message_id = self.select(\"SELECT id FROM message WHERE discussion_id=? ORDER BY id DESC LIMIT 1\", (last_discussion_id,), fetch_all=False)",
            "        return Discussion(self.lollms, last_discussion_id, self)",
            "    ",
            "    def create_discussion(self, title=\"untitled\"):",
            "        \"\"\"Creates a new discussion",
            "",
            "        Args:",
            "            title (str, optional): The title of the discussion. Defaults to \"untitled\".",
            "",
            "        Returns:",
            "            Discussion: A Discussion instance ",
            "        \"\"\"",
            "        discussion_id = self.insert(f\"INSERT INTO discussion (title) VALUES (?)\",(title,))",
            "        return Discussion(self.lollms, discussion_id, self)",
            "",
            "    def build_discussion(self, discussion_id=0):",
            "        return Discussion(self.lollms, discussion_id, self)",
            "",
            "    def get_discussions(self):",
            "        rows = self.select(\"SELECT * FROM discussion\")         ",
            "        return [{\"id\": row[0], \"title\": row[1]} for row in rows]",
            "",
            "    def does_last_discussion_have_messages(self):",
            "        last_discussion_id = self.select(\"SELECT id FROM discussion ORDER BY id DESC LIMIT 1\", fetch_all=False)",
            "        if last_discussion_id is None:",
            "            last_discussion = self.create_discussion()",
            "            last_discussion_id = last_discussion.discussion_id",
            "        else:",
            "            last_discussion_id = last_discussion_id[0]",
            "        last_message = self.select(\"SELECT * FROM message WHERE discussion_id=?\", (last_discussion_id,), fetch_all=False)",
            "        return last_message is not None",
            "    ",
            "    def remove_discussions(self):",
            "        self.delete(\"DELETE FROM message\")",
            "        self.delete(\"DELETE FROM discussion\")",
            "",
            "",
            "    def export_to_json(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a JSON format.",
            "",
            "        Returns:",
            "            list: A list of dictionaries representing discussions and their messages.",
            "                Each dictionary contains the discussion ID, title, and a list of messages.",
            "                Each message dictionary contains the sender, content, message type, rank,",
            "                parent message ID, binding, model, personality, created at, and finished",
            "                generating at fields.",
            "        \"\"\"        ",
            "        db_discussions = self.select(\"SELECT * FROM discussion\")",
            "        discussions = []",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussion = {\"id\": discussion_id, \"title\":discussion_title, \"messages\": []}",
            "",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[10]",
            "                nb_tokens =  message_row[11]",
            "",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"parent_message_id\": parent_message_id, \"binding\": binding, \"model\":model, \"personality\":personality, \"created_at\":created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\":finished_generating_at, \"nb_tokens\":nb_tokens}",
            "                )",
            "            discussions.append(discussion)",
            "",
            "        return discussions",
            "",
            "    def export_all_as_markdown_list_for_vectorization(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a Markdown list format.",
            "",
            "        Returns:",
            "            list: A list of lists representing discussions and their messages in a Markdown format.",
            "                Each inner list contains the discussion title and a string representing all",
            "                messages in the discussion in a Markdown format.",
            "        \"\"\"        ",
            "        data = self.export_all_discussions_to_json()",
            "        # Initialize an empty result string",
            "        discussions = []",
            "        # Iterate through discussions in the JSON data",
            "        for discussion in data:",
            "            # Extract the title",
            "            title = discussion['title']",
            "            messages = \"\"",
            "            # Iterate through messages in the discussion",
            "            for message in discussion['messages']:",
            "                sender = message['sender']",
            "                content = message['content']",
            "                # Append the sender and content in a Markdown format",
            "                messages += f'{sender}: {content}\\n'",
            "            discussions.append([title, messages])",
            "        return discussions",
            "        ",
            "    def export_all_as_markdown(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a Markdown format.",
            "",
            "        Returns:",
            "            str: A string representing all discussions and their messages in a Markdown format.",
            "                Each discussion is represented as a Markdown heading, and each message is",
            "                represented with the sender and content in a Markdown format.",
            "        \"\"\"        ",
            "        data = self.export_all_discussions_to_json()",
            "",
            "        # Initialize an empty result string",
            "        result = ''",
            "",
            "        # Iterate through discussions in the JSON data",
            "        for discussion in data:",
            "            # Extract the title",
            "            title = discussion['title']",
            "            # Append the title with '#' as Markdown heading",
            "            result += f'#{title}\\n'",
            "",
            "            # Iterate through messages in the discussion",
            "            for message in discussion['messages']:",
            "                sender = message['sender']",
            "                content = message['content']",
            "                # Append the sender and content in a Markdown format",
            "                result += f'{sender}: {content}\\n'",
            "",
            "        return result",
            "",
            "    def export_all_discussions_to_json(self):",
            "        # Convert the list of discussion IDs to a tuple",
            "        db_discussions = self.select(",
            "            f\"SELECT * FROM discussion\"",
            "        )",
            "        discussions = []",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussion = {\"id\": discussion_id, \"title\":discussion_title, \"messages\": []}",
            "",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[10]",
            "                nb_tokens = message_row[11]",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"parent_message_id\": parent_message_id, \"binding\": binding, \"model\":model, \"personality\":personality, \"created_at\":created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\": finished_generating_at,\"nb_tokens\":nb_tokens}",
            "                )",
            "            discussions.append(discussion)",
            "        return discussions",
            "",
            "    def export_discussions_to_json(self, discussions_ids:list):",
            "        # Convert the list of discussion IDs to a tuple",
            "        discussions_ids_tuple = tuple(discussions_ids)",
            "        txt = ','.join(['?'] * len(discussions_ids_tuple))",
            "        db_discussions = self.select(",
            "            f\"SELECT * FROM discussion WHERE id IN ({txt})\",",
            "            discussions_ids_tuple",
            "        )",
            "        discussions = []",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussion = {\"id\": discussion_id, \"title\":discussion_title, \"messages\": []}",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[9]",
            "                nb_tokens = message_row[9]",
            "                ",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"parent_message_id\": parent_message_id, \"binding\": binding, \"model\":model, \"personality\":personality, \"created_at\":created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\": finished_generating_at, \"nb_tokens\":nb_tokens}",
            "                )",
            "            discussions.append(discussion)",
            "        return discussions",
            "    ",
            "    def import_from_json(self, json_data):",
            "        discussions = []",
            "        data = json_data",
            "        for discussion_data in data:",
            "            discussion_id = discussion_data.id",
            "            discussion_title = discussion_data.title",
            "            messages_data = discussion_data.messages",
            "            discussion = {\"id\": discussion_id, \"title\": discussion_title, \"messages\": []}",
            "",
            "            # Insert discussion into the database",
            "            discussion_id = self.insert(\"INSERT INTO discussion (title) VALUES (?)\", (discussion_title,))",
            "",
            "            for message_data in messages_data:",
            "                sender = message_data.get(\"sender\")",
            "                content = message_data.get(\"content\")",
            "                content_type = message_data.get(\"message_type\",message_data.get(\"type\"))",
            "                rank = message_data.get(\"rank\")",
            "                parent_message_id = message_data.get(\"parent_message_id\")",
            "                binding = message_data.get(\"binding\",\"\")",
            "                model = message_data.get(\"model\",\"\")",
            "                personality = message_data.get(\"personality\",\"\")",
            "                created_at = message_data.get(\"created_at\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "",
            "                started_generating_at = message_data.get(\"started_generating_at\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "                finished_generating_at = message_data.get(\"finished_generating_at\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "                nb_tokens = message_data.get(\"nb_tokens\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"binding\": binding, \"model\": model, \"personality\": personality, \"created_at\": created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\": finished_generating_at, \"nb_tokens\":nb_tokens}",
            "                )",
            "",
            "                # Insert message into the database",
            "                self.insert(\"INSERT INTO message (sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",",
            "                            (sender, content, content_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id))",
            "",
            "            discussions.append(discussion)",
            "",
            "        return discussions",
            "",
            "    def export_discussions_to_markdown(self, discussions_ids:list, title = \"\"):",
            "        # Convert the list of discussion IDs to a tuple",
            "        discussions_ids_tuple = tuple(discussions_ids)",
            "        txt = ','.join(['?'] * len(discussions_ids_tuple))",
            "        db_discussions = self.select(",
            "            f\"SELECT * FROM discussion WHERE id IN ({txt})\",",
            "            discussions_ids_tuple",
            "        )",
            "        discussions = f\"# {title}\" if title!=\"\" else \"\"",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussions += f\"## {discussion_title}\\n\"",
            "",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[10]",
            "                nb_tokens = message_row[11]",
            "",
            "                discussions +=f\"### {sender}:\\n{content}\\n\"",
            "            discussions +=f\"\\n\"",
            "        return discussions",
            "",
            "",
            "class Message:",
            "    def __init__(",
            "                    self,",
            "                    discussion_id,",
            "                    discussions_db,",
            "                    message_type,",
            "                    sender_type,",
            "                    sender,",
            "                    content,",
            "                    metadata                = None,",
            "                    ui                      = None,",
            "                    rank                    = 0,",
            "                    parent_message_id       = 0,",
            "                    binding                 = \"\",",
            "                    model                   = \"\",",
            "                    personality             = \"\",",
            "                    created_at              = None,",
            "                    started_generating_at   = None,",
            "                    finished_generating_at  = None,",
            "                    nb_tokens     = None,",
            "                    id                      = None,",
            "                    insert_into_db          = False",
            "                    ):",
            "        ",
            "        self.discussion_id = discussion_id",
            "        self.discussions_db = discussions_db",
            "        self.self = self",
            "        self.sender = sender",
            "        self.sender_type = sender_type",
            "        self.content = content",
            "        self.message_type = message_type",
            "        self.rank = rank",
            "        self.parent_message_id = parent_message_id",
            "        self.binding = binding",
            "        self.model = model",
            "        self.metadata = json.dumps(metadata, indent=4) if metadata is not None and type(metadata)== dict else metadata",
            "        self.ui = ui",
            "        self.personality = personality",
            "        self.created_at = created_at",
            "        self.started_generating_at = started_generating_at",
            "        self.finished_generating_at = finished_generating_at",
            "        self.nb_tokens = nb_tokens",
            "",
            "        if insert_into_db:",
            "            self.id = self.discussions_db.insert(",
            "                \"INSERT INTO message (sender,  message_type,  sender_type,  sender,  content,  metadata, ui,  rank,  parent_message_id,  binding,  model,  personality,  created_at, started_generating_at,  finished_generating_at, nb_tokens,  discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", ",
            "                (sender, message_type, sender_type, sender, content, metadata, ui, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id)",
            "            )",
            "        else:",
            "            self.id = id",
            "",
            "",
            "    @staticmethod",
            "    def get_fields():",
            "        return [",
            "            \"id\",",
            "            \"message_type\",",
            "            \"sender_type\",",
            "            \"sender\",",
            "            \"content\",",
            "            \"metadata\",",
            "            \"ui\",",
            "            \"rank\",",
            "            \"parent_message_id\",",
            "            \"binding\",",
            "            \"model\",",
            "            \"personality\",",
            "            \"created_at\",",
            "            \"started_generating_at\",",
            "            \"finished_generating_at\",",
            "            \"nb_tokens\",",
            "            \"discussion_id\"",
            "        ]        ",
            "",
            "    @staticmethod",
            "    def from_db(discussions_db, message_id):",
            "        columns = Message.get_fields()",
            "        rows = discussions_db.select(",
            "            f\"SELECT {','.join(columns)} FROM message WHERE id=?\", (message_id,)",
            "        )",
            "        data_dict={",
            "            col:rows[0][i]",
            "            for i,col in enumerate(columns)",
            "        }",
            "        data_dict[\"discussions_db\"]=discussions_db",
            "        return Message(",
            "            **data_dict",
            "        )",
            "",
            "    @staticmethod",
            "    def from_dict(discussions_db,data_dict):",
            "        data_dict[\"discussions_db\"]=discussions_db",
            "        return Message(",
            "            **data_dict",
            "        )",
            "",
            "    def insert_into_db(self):",
            "        self.message_id = self.discussions_db.insert(",
            "            \"INSERT INTO message (sender, content, metadata, ui, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", ",
            "            (self.sender, self.content, self.metadata, self.ui, self.message_type, self.rank, self.parent_message_id, self.binding, self.model, self.personality, self.created_at, self.started_generating_at, self.finished_generating_at, self.nb_tokens, self.discussion_id)",
            "        )",
            "",
            "    def update_db(self):",
            "        self.message_id = self.discussions_db.insert(",
            "            \"INSERT INTO message (sender, content, metadata, ui, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", ",
            "            (self.sender, self.content, self.metadata, self.ui, self.message_type, self.rank, self.parent_message_id, self.binding, self.model, self.personality, self.created_at, self.started_generating_at, self.finished_generating_at, nb_tokens, self.discussion_id)",
            "        )",
            "",
            "    def update(self, new_content, new_metadata=None, new_ui=None, started_generating_at=None, nb_tokens=None, commit=True):",
            "        self.finished_generating_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "        text = f\"UPDATE message SET content = ?\"",
            "        params = [new_content]",
            "        if new_metadata is not None:",
            "            text+=\", metadata = ?\"",
            "            params.append(new_metadata)",
            "        if new_ui is not None:",
            "            text+=\", ui = ?\"",
            "            params.append(new_ui)",
            "",
            "        if started_generating_at is not None:",
            "            text+=\", started_generating_at = ?\"",
            "            params.append(started_generating_at)",
            "",
            "        if nb_tokens is not None:",
            "            text+=\", nb_tokens = ?\"",
            "            params.append(nb_tokens)",
            "",
            "",
            "        text +=\", finished_generating_at = ? WHERE id = ?\"",
            "        params.append(self.finished_generating_at)",
            "        params.append(self.id)",
            "        self.discussions_db.update(",
            "            text, tuple(params)",
            "        )        ",
            "",
            "    def to_json(self):",
            "        attributes = Message.get_fields()",
            "        msgJson = {}",
            "        for attribute_name in attributes:",
            "            attribute_value = getattr(self, attribute_name, None)",
            "            if attribute_name==\"metadata\":",
            "                if type(attribute_value) == str:",
            "                    msgJson[attribute_name] = json.loads(attribute_value)",
            "                else:",
            "                    msgJson[attribute_name] = attribute_value",
            "            else:",
            "                msgJson[attribute_name] = attribute_value",
            "        return msgJson",
            "",
            "class Discussion:",
            "    def __init__(self, lollms:LoLLMsCom, discussion_id, discussions_db:DiscussionsDB):",
            "        self.lollms = lollms",
            "        self.current_message = None",
            "        self.discussion_id = discussion_id",
            "        self.discussions_db = discussions_db",
            "        self.discussion_folder = self.discussions_db.discussion_db_path/f\"{discussion_id}\"",
            "        self.discussion_audio_folder = self.discussion_folder / \"audio\"",
            "        self.discussion_images_folder = self.discussion_folder / \"images\"",
            "        self.discussion_text_folder = self.discussion_folder / \"text_data\"",
            "        self.discussion_skills_folder = self.discussion_folder / \"skills\"",
            "        self.discussion_rag_folder = self.discussion_folder / \"rag\"",
            "        self.discussion_view_images_folder = self.discussion_folder / \"view_images\"",
            "",
            "        self.discussion_folder.mkdir(exist_ok=True)",
            "        self.discussion_images_folder.mkdir(exist_ok=True)",
            "        self.discussion_text_folder.mkdir(exist_ok=True)",
            "        self.discussion_skills_folder.mkdir(exist_ok=True)",
            "        self.discussion_rag_folder.mkdir(exist_ok=True)",
            "        self.discussion_view_images_folder.mkdir(exist_ok=True)",
            "        self.messages = self.get_messages()",
            "        ",
            "        if len(self.messages)>0:",
            "            self.current_message = self.messages[-1]",
            "",
            "        # Initialize the file lists",
            "        self.update_file_lists()",
            "",
            "",
            "        self.vectorizer = TextVectorizer(",
            "            self.lollms.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\"",
            "            model=self.lollms.model, #needed in case of using model_embedding",
            "            database_path=self.discussion_rag_folder/\"db.json\",",
            "            save_db=self.lollms.config.data_vectorization_save_db,",
            "            data_visualization_method=VisualizationMethod.PCA,",
            "            database_dict=None)",
            "    ",
            "        if len(self.vectorizer.chunks)==0 and len(self.text_files)>0:",
            "            for path in self.text_files:",
            "                data = GenericDataLoader.read_file(path)",
            "                try:",
            "                    self.vectorizer.add_document(path, data, self.lollms.config.data_vectorization_chunk_size, self.lollms.config.data_vectorization_overlap_size, add_first_line_to_all_chunks=True if path.suffix==\".csv\" else False)",
            "                except Exception as ex:",
            "                    trace_exception(ex)            ",
            "            try:",
            "                self.vectorizer.index()",
            "            except Exception as ex:",
            "                trace_exception(ex)            ",
            "",
            "    def update_file_lists(self):",
            "        self.text_files = [Path(file) for file in self.discussion_text_folder.glob('*')]",
            "        self.image_files = [Path(file) for file in self.discussion_images_folder.glob('*')]",
            "        self.audio_files = [Path(file) for file in self.discussion_audio_folder.glob('*')]",
            "        self.rag_db = [Path(file) for file in self.discussion_rag_folder.glob('*')]",
            "",
            "",
            "    def remove_file(self, file_name, callback=None):",
            "        try:",
            "            all_files = self.text_files+self.image_files+self.audio_files",
            "            if any(file_name == entry.name for entry in self.text_files):",
            "                fn = [entry for entry in self.text_files if entry.name == file_name][0]",
            "                self.text_files = [entry for entry in self.text_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "                if len(self.text_files)>0:",
            "                    try:",
            "                        self.vectorizer.remove_document(fn)",
            "                        if callback is not None:",
            "                            callback(\"File removed successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                        return True",
            "                    except ValueError as ve:",
            "                        ASCIIColors.error(f\"Couldn't remove the file\")",
            "                        return False",
            "                else:",
            "                    self.vectorizer = None",
            "            elif any(file_name == entry.name for entry in self.image_files):",
            "                fn = [entry for entry in self.image_files if entry.name == file_name][0]",
            "                self.image_files = [entry for entry in self.image_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "            elif any(file_name == entry.name for entry in self.audio_files):",
            "                fn = [entry for entry in self.audio_files if entry.name == file_name][0]",
            "                self.audio_files = [entry for entry in self.audio_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "",
            "        except Exception as ex:",
            "            trace_exception(ex)",
            "            ASCIIColors.warning(f\"Couldn't remove the file {file_name}\")",
            "",
            "    def remove_all_files(self):",
            "        # Iterate over each directory and remove all files",
            "        for path in [self.discussion_images_folder, self.discussion_rag_folder, self.discussion_audio_folder, self.discussion_text_folder]:",
            "            for file in path.glob('*'):",
            "                if file.is_file():  # Ensure it's a file, not a directory",
            "                    file.unlink()  # Delete the file",
            "                    ",
            "        # Clear the lists to reflect the current state (empty directories)",
            "        self.text_files.clear()",
            "        self.image_files.clear()",
            "        self.audio_files.clear()",
            "",
            "    def add_file(self, path, client, tasks_library:TasksLibrary, callback=None, process=True):",
            "        output = \"\"",
            "",
            "        path = Path(path)",
            "        if path.suffix in [\".wav\",\".mp3\"]:",
            "            self.audio_files.append(path)",
            "            if process:",
            "                self.lollms.new_message(client.client_id if client is not None else 0, content = \"\", message_type = MSG_TYPE.MSG_TYPE_FULL)",
            "                if self.lollms.stt is None:",
            "                    self.lollms.info(\"Please select an stt engine in the services settings first\")",
            "                self.lollms.info(f\"Transcribing ... \")",
            "                transcription = self.lollms.stt.transcribe(str(path))",
            "                transcription_fn = self.discussion_text_folder/(path.stem+\".txt\")",
            "                with open(transcription_fn, \"w\", encoding=\"utf-8\") as f:",
            "                    f.write(transcription)",
            "                self.text_files.append(transcription_fn)",
            "                tasks_library.info(f\"Transcription saved to {transcription_fn}\")",
            "",
            "        elif path.suffix in [\".png\",\".jpg\",\".jpeg\",\".gif\",\".bmp\",\".svg\",\".webp\"]:",
            "            self.image_files.append(path)",
            "            if process:",
            "                ",
            "                try:",
            "                    view_file = self.discussion_view_images_folder/path.name",
            "                    shutil.copyfile(path, view_file)",
            "                    pth = str(view_file).replace(\"\\\\\",\"/\").split('/')",
            "                    if \"discussion_databases\" in pth:",
            "                        pth = discussion_path_to_url(view_file)",
            "                        self.lollms.new_message(client.client_id if client is not None else 0, content = \"\", message_type = MSG_TYPE.MSG_TYPE_FULL)",
            "                        output = f'<img src=\"{pth}\" width=\"800\">\\n\\n'",
            "                        self.lollms.full(output, client_id=client.client_id)",
            "                        self.lollms.close_message(client.client_id if client is not None else 0)",
            "",
            "                    if self.lollms.model.binding_type not in [BindingType.TEXT_IMAGE, BindingType.TEXT_IMAGE_VIDEO]:",
            "                        # self.ShowBlockingMessage(\"Understanding image (please wait)\")",
            "                        from PIL import Image",
            "                        img = Image.open(str(view_file))",
            "                        # Convert the image to RGB mode",
            "                        img = img.convert(\"RGB\")",
            "                        output += \"## image description :\\n\"+ self.lollms.model.interrogate_blip([img])[0]",
            "                        # output += \"## image description :\\n\"+ self.lollms.model.qna_blip([img],\"q:Describe this photo with as much details as possible.\\na:\")[0]",
            "                        self.lollms.full(output)",
            "                        self.lollms.close_message(client.client_id if client is not None else 0)",
            "                        self.lollms.HideBlockingMessage(\"Understanding image (please wait)\")",
            "                        if self.lollms.config.debug:",
            "                            ASCIIColors.yellow(output)",
            "                    else:",
            "                        # self.ShowBlockingMessage(\"Importing image (please wait)\")",
            "                        self.lollms.HideBlockingMessage(\"Importing image (please wait)\")",
            "",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.lollms.HideBlockingMessage(\"Understanding image (please wait)\", False)",
            "                    ASCIIColors.error(\"Couldn't create new message\")",
            "            ASCIIColors.info(\"Received image file\")",
            "            if callback is not None:",
            "                callback(\"Image file added successfully\", MSG_TYPE.MSG_TYPE_INFO)",
            "        else:",
            "            try:",
            "                # self.ShowBlockingMessage(\"Adding file to vector store.\\nPlease stand by\")",
            "                self.text_files.append(path)",
            "                ASCIIColors.info(\"Received text compatible file\")",
            "                self.lollms.ShowBlockingMessage(\"Processing file\\nPlease wait ...\")",
            "                if process:",
            "                    if self.vectorizer is None:",
            "                        self.vectorizer = TextVectorizer(",
            "                                    self.lollms.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\"",
            "                                    model=self.lollms.model, #needed in case of using model_embedding",
            "                                    database_path=self.discussion_rag_folder/\"db.json\",",
            "                                    save_db=self.lollms.config.data_vectorization_save_db,",
            "                                    data_visualization_method=VisualizationMethod.PCA,",
            "                                    database_dict=None)",
            "                    data = GenericDataLoader.read_file(path)",
            "                    self.vectorizer.add_document(path, data, self.lollms.config.data_vectorization_chunk_size, self.lollms.config.data_vectorization_overlap_size, add_first_line_to_all_chunks=True if path.suffix==\".csv\" else False)",
            "                    self.vectorizer.index()",
            "                    if callback is not None:",
            "                        callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                    self.lollms.HideBlockingMessage(client.client_id)",
            "                    return True",
            "            except Exception as e:",
            "                trace_exception(e)",
            "                self.lollms.InfoMessage(f\"Unsupported file format or empty file.\\nSupported formats are {GenericDataLoader.get_supported_file_types()}\",client_id=client.client_id)",
            "                return False",
            "",
            "    def load_message(self, id):",
            "        \"\"\"Gets a list of messages information",
            "",
            "        Returns:",
            "            list: List of entries in the format {\"id\":message id, \"sender\":sender name, \"content\":message content, \"message_type\":message type, \"rank\": message rank}",
            "        \"\"\"",
            "        self.current_message = Message.from_db(self.discussions_db, id)",
            "        return self.current_message",
            "    ",
            "    def add_message(",
            "                    self, ",
            "                    message_type,",
            "                    sender_type,",
            "                    sender,",
            "                    content,",
            "                    metadata=None,",
            "                    ui=None,",
            "                    rank=0, ",
            "                    parent_message_id=0, ",
            "                    binding=\"\", ",
            "                    model =\"\", ",
            "                    personality=\"\", ",
            "                    created_at=None, ",
            "                    started_generating_at=None,",
            "                    finished_generating_at=None,",
            "                    nb_tokens=None",
            "                ):",
            "        \"\"\"Adds a new message to the discussion",
            "",
            "        Args:",
            "            sender (str): The sender name",
            "            content (str): The text sent by the sender",
            "",
            "        Returns:",
            "            int: The added message id",
            "        \"\"\"",
            "        if created_at is None:",
            "            created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "",
            "        if started_generating_at is None:",
            "            started_generating_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "",
            "        if finished_generating_at is None:",
            "            finished_generating_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "",
            "        if nb_tokens is None:",
            "            nb_tokens = 0",
            "",
            "        self.current_message = Message(",
            "            self.discussion_id,",
            "            self.discussions_db,",
            "            message_type,",
            "            sender_type,",
            "            sender,",
            "            content,",
            "            metadata,",
            "            ui,",
            "            rank,",
            "            parent_message_id,",
            "            binding,",
            "            model,",
            "            personality,",
            "            created_at,",
            "            started_generating_at,",
            "            finished_generating_at,",
            "            nb_tokens,",
            "            insert_into_db=True",
            "        )",
            "",
            "        self.messages.append(self.current_message)",
            "        return self.current_message",
            "",
            "    def rename(self, new_title):",
            "        \"\"\"Renames the discussion",
            "",
            "        Args:",
            "            new_title (str): The nex discussion name",
            "        \"\"\"",
            "        self.discussions_db.update(",
            "            f\"UPDATE discussion SET title=? WHERE id=?\",(new_title,self.discussion_id)",
            "        )",
            "",
            "    def title(self):",
            "        \"\"\"Renames the discussion",
            "",
            "        Args:",
            "            new_title (str): The nex discussion name",
            "        \"\"\"",
            "        rows = self.discussions_db.select(",
            "            f\"Select title from discussion WHERE id={self.discussion_id}\"",
            "        )",
            "        return rows[0][0]",
            "",
            "    def delete_discussion(self):",
            "        \"\"\"Deletes the discussion",
            "        \"\"\"",
            "        self.discussions_db.delete(",
            "            f\"DELETE FROM message WHERE discussion_id={self.discussion_id}\"",
            "        )",
            "        self.discussions_db.delete(",
            "            f\"DELETE FROM discussion WHERE id={self.discussion_id}\"",
            "        )",
            "",
            "    def get_messages(self):",
            "        \"\"\"Gets a list of messages information",
            "",
            "        Returns:",
            "            list: List of entries in the format {\"id\":message id, \"sender\":sender name, \"content\":message content, \"message_type\":message type, \"rank\": message rank}",
            "        \"\"\"",
            "        columns = Message.get_fields()",
            "",
            "        rows = self.discussions_db.select(",
            "            f\"SELECT {','.join(columns)} FROM message WHERE discussion_id=?\", (self.discussion_id,)",
            "        )",
            "        msg_dict = [{ c:row[i] for i,c in enumerate(columns)} for row in rows]",
            "        self.messages=[]",
            "        for msg in msg_dict:",
            "            self.messages.append(Message.from_dict(self.discussions_db, msg))",
            "",
            "        if len(self.messages)>0:",
            "            self.current_message = self.messages[-1]",
            "",
            "        return self.messages",
            "",
            "    def get_message(self, message_id):",
            "        for message in self.messages:",
            "            if message.id == int(message_id):",
            "                self.current_message = message",
            "                return message",
            "        return None",
            "",
            "    def select_message(self, message_id):",
            "        msg = self.get_message(message_id)",
            "        if msg is not None:",
            "            self.current_message = msg",
            "            return True",
            "        else:",
            "            return False ",
            "",
            "    def update_message(self, new_content, new_metadata=None, new_ui=None, started_generating_at=None, nb_tokens=None):",
            "        \"\"\"Updates the content of a message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "            new_content (str): The nex message content",
            "        \"\"\"",
            "        self.current_message.update(new_content, new_metadata, new_ui, started_generating_at, nb_tokens)",
            "",
            "    def edit_message(self, message_id, new_content, new_metadata=None, new_ui=None):",
            "        \"\"\"Edits the content of a message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "            new_content (str): The nex message content",
            "        \"\"\"",
            "        msg = self.get_message(message_id)",
            "        if msg:",
            "            msg.update(new_content, new_metadata, new_ui)",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "    def message_rank_up(self, message_id):",
            "        \"\"\"Increments the rank of the message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "        \"\"\"",
            "        # Retrieve current rank value for message_id",
            "        current_rank = self.discussions_db.select(\"SELECT rank FROM message WHERE id=?\", (message_id,),False)[0]",
            "",
            "        # Increment current rank value by 1",
            "        new_rank = current_rank + 1        ",
            "        self.discussions_db.update(",
            "            f\"UPDATE message SET rank = ? WHERE id = ?\",(new_rank,message_id)",
            "        )",
            "        return new_rank",
            "",
            "    def message_rank_down(self, message_id):",
            "        \"\"\"Increments the rank of the message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "        \"\"\"",
            "        # Retrieve current rank value for message_id",
            "        current_rank = self.discussions_db.select(\"SELECT rank FROM message WHERE id=?\", (message_id,),False)[0]",
            "",
            "        # Increment current rank value by 1",
            "        new_rank = current_rank - 1        ",
            "        self.discussions_db.update(",
            "            f\"UPDATE message SET rank = ? WHERE id = ?\",(new_rank,message_id)",
            "        )",
            "        return new_rank",
            "    ",
            "    def delete_message(self, message_id):",
            "        \"\"\"Delete the message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be deleted",
            "        \"\"\"",
            "        # Retrieve current rank value for message_id",
            "        self.discussions_db.delete(\"DELETE FROM message WHERE id=?\", (message_id,))",
            "",
            "    def export_for_vectorization(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a Markdown list format.",
            "",
            "        Returns:",
            "            list: A list of lists representing discussions and their messages in a Markdown format.",
            "                Each inner list contains the discussion title and a string representing all",
            "                messages in the discussion in a Markdown format.",
            "        \"\"\"        ",
            "        # Extract the title",
            "        title = self.title()",
            "        messages = \"\"",
            "        # Iterate through messages in the discussion",
            "        for message in self.messages:",
            "            sender = message.sender",
            "            content = message.content",
            "            # Append the sender and content in a Markdown format",
            "            messages += f'{sender}: {content}\\n'",
            "        return title, messages",
            " ",
            "    def format_discussion(self, max_allowed_tokens, splitter_text=None):",
            "        if not splitter_text:",
            "            splitter_text = self.lollms.config.discussion_prompt_separator",
            "        formatted_text = \"\"",
            "        for message in reversed(self.messages):  # Start from the newest message",
            "            formatted_message = f\"{splitter_text}{message.sender.replace(':','').replace(splitter_text,'')}:\\n{message.content}\\n\"",
            "            tokenized_message = self.lollms.model.tokenize(formatted_message)",
            "            if len(tokenized_message) + len(self.lollms.model.tokenize(formatted_text)) <= max_allowed_tokens:",
            "                formatted_text = formatted_message + formatted_text",
            "            else:",
            "                break  # Stop if adding the next message would exceed the limit",
            "        return formatted_text   ",
            "# ========================================================================================================================"
        ],
        "afterPatchFile": [
            "",
            "import sqlite3",
            "from pathlib import Path",
            "from datetime import datetime",
            "from ascii_colors import ASCIIColors, trace_exception",
            "from lollms.types import MSG_TYPE",
            "from lollms.types import BindingType",
            "from lollms.utilities import PackageManager, discussion_path_to_url",
            "from lollms.paths import LollmsPaths",
            "from lollms.databases.skills_database import SkillsLibrary",
            "from lollms.com import LoLLMsCom",
            "from safe_store import TextVectorizer, VisualizationMethod, GenericDataLoader",
            "from lollmsvectordb.vector_database import VectorDatabase",
            "from lollmsvectordb.lollms_vectorizers.bert_vectorizer import BERTVectorizer",
            "from lollmsvectordb.lollms_vectorizers.tfidf_vectorizer import TFIDFVectorizer",
            "from lollmsvectordb.text_document_loader import TextDocumentsLoader",
            "import gc",
            "import json",
            "import shutil",
            "from lollms.tasks import TasksLibrary",
            "__author__ = \"parisneo\"",
            "__github__ = \"https://github.com/ParisNeo/lollms-webui\"",
            "__copyright__ = \"Copyright 2023, \"",
            "__license__ = \"Apache 2.0\"",
            "",
            "",
            "# =================================== Database ==================================================================",
            "class DiscussionsDB:",
            "    ",
            "    def __init__(self, lollms:LoLLMsCom, lollms_paths:LollmsPaths, discussion_db_name=\"default\"):",
            "        self.lollms = lollms",
            "        self.lollms_paths = lollms_paths",
            "        ",
            "        self.discussion_db_name = discussion_db_name",
            "        self.discussion_db_path = self.lollms_paths.personal_discussions_path/discussion_db_name",
            "",
            "        self.discussion_db_path.mkdir(exist_ok=True, parents= True)",
            "        self.discussion_db_file_path = self.discussion_db_path/\"database.db\"",
            "",
            "    def create_tables(self):",
            "        db_version = 12",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.cursor()",
            "",
            "            cursor.execute(\"\"\"",
            "                CREATE TABLE IF NOT EXISTS schema_version (",
            "                    id INTEGER PRIMARY KEY AUTOINCREMENT,",
            "                    version INTEGER NOT NULL",
            "                )",
            "            \"\"\")",
            "",
            "            cursor.execute(\"\"\"",
            "                CREATE TABLE IF NOT EXISTS discussion (",
            "                    id INTEGER PRIMARY KEY AUTOINCREMENT,",
            "                    title TEXT,",
            "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP",
            "                )",
            "            \"\"\")",
            "",
            "            cursor.execute(\"\"\"",
            "                CREATE TABLE IF NOT EXISTS message (",
            "                    id INTEGER PRIMARY KEY AUTOINCREMENT,",
            "                    binding TEXT,",
            "                    model TEXT,",
            "                    personality TEXT,",
            "                    sender TEXT NOT NULL,",
            "                    content TEXT NOT NULL,",
            "                    message_type INT NOT NULL,",
            "                    sender_type INT DEFAULT 0,",
            "                    rank INT NOT NULL DEFAULT 0,",
            "                    parent_message_id INT,",
            "                    created_at TIMESTAMP,",
            "                    started_generating_at TIMESTAMP,",
            "                    finished_generating_at TIMESTAMP,",
            "                    nb_tokens INT,                    ",
            "                    discussion_id INTEGER NOT NULL,",
            "                    metadata TEXT,",
            "                    ui TEXT,",
            "                    FOREIGN KEY (discussion_id) REFERENCES discussion(id),",
            "                    FOREIGN KEY (parent_message_id) REFERENCES message(id)",
            "                )",
            "            \"\"\")",
            "",
            "            cursor.execute(\"SELECT * FROM schema_version\")",
            "            row = cursor.fetchone()",
            "",
            "            if row is None:",
            "                cursor.execute(\"INSERT INTO schema_version (version) VALUES (?)\", (db_version,))",
            "            else:",
            "                cursor.execute(\"UPDATE schema_version SET version = ?\", (db_version,))            ",
            "",
            "            conn.commit()",
            "",
            "    def add_missing_columns(self):",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.cursor()",
            "",
            "            table_columns = {",
            "                'discussion': [",
            "                    'id',",
            "                    'title',",
            "                    'created_at'",
            "                ],",
            "                'message': [",
            "                    'id',",
            "                    'binding',",
            "                    'model',",
            "                    'personality',",
            "                    'sender',",
            "                    'content',",
            "                    'message_type',",
            "                    'sender_type',",
            "                    'rank',",
            "                    'parent_message_id',",
            "                    'created_at',",
            "                    'metadata',",
            "                    'ui',",
            "                    'started_generating_at',",
            "                    'finished_generating_at',",
            "                    'nb_tokens',                    ",
            "                    'discussion_id'",
            "                ]",
            "            }",
            "",
            "            for table, columns in table_columns.items():",
            "                cursor.execute(f\"PRAGMA table_info({table})\")",
            "                existing_columns = [column[1] for column in cursor.fetchall()]",
            "",
            "                for column in columns:",
            "                    if column not in existing_columns:",
            "                        if column == 'id':",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} INTEGER PRIMARY KEY AUTOINCREMENT\")",
            "                        elif column.endswith('_at'):",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} TIMESTAMP\")",
            "                        elif column=='metadata':",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} TEXT\")",
            "                        elif column=='message_type':",
            "                            cursor.execute(f\"ALTER TABLE {table} RENAME COLUMN type TO {column}\")",
            "                        elif column=='sender_type':",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} INT DEFAULT 0\")",
            "                        elif column=='parent_message_id':",
            "                            cursor.execute(f\"ALTER TABLE {table} RENAME COLUMN parent TO {column}\")",
            "                        else:",
            "                            cursor.execute(f\"ALTER TABLE {table} ADD COLUMN {column} TEXT\")",
            "                        ASCIIColors.yellow(f\"Added column :{column}\")",
            "            conn.commit()",
            "",
            "",
            "    def select(self, query, params=None, fetch_all=True):",
            "        \"\"\"",
            "        Execute the specified SQL select query on the database,",
            "        with optional parameters.",
            "        Returns the cursor object for further processing.",
            "        \"\"\"",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            if params is None:",
            "                cursor = conn.execute(query)",
            "            else:",
            "                cursor = conn.execute(query, params)",
            "            if fetch_all:",
            "                return cursor.fetchall()",
            "            else:",
            "                return cursor.fetchone()",
            "            ",
            "",
            "    def delete(self, query, params=None):",
            "        \"\"\"",
            "        Execute the specified SQL delete query on the database,",
            "        with optional parameters.",
            "        Returns the cursor object for further processing.",
            "        \"\"\"",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.cursor()",
            "            if params is None:",
            "                cursor.execute(query)",
            "            else:",
            "                cursor.execute(query, params)",
            "            conn.commit()",
            "   ",
            "    def insert(self, query, params=None):",
            "        \"\"\"",
            "        Execute the specified INSERT SQL query on the database,",
            "        with optional parameters.",
            "        Returns the ID of the newly inserted row.",
            "        \"\"\"",
            "        ",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            cursor = conn.execute(query, params)",
            "            rowid = cursor.lastrowid",
            "            conn.commit()",
            "        self.conn = None",
            "        return rowid",
            "",
            "    def update(self, query, params:tuple=None):",
            "        \"\"\"",
            "        Execute the specified Update SQL query on the database,",
            "        with optional parameters.",
            "        Returns the ID of the newly inserted row.",
            "        \"\"\"",
            "        ",
            "        with sqlite3.connect(self.discussion_db_file_path) as conn:",
            "            conn.execute(query, params)",
            "            conn.commit()",
            "    ",
            "    def load_last_discussion(self):",
            "        last_discussion_id = self.select(\"SELECT id FROM discussion ORDER BY id DESC LIMIT 1\", fetch_all=False)",
            "        if last_discussion_id is None:",
            "            last_discussion = self.create_discussion()",
            "            last_discussion_id = last_discussion.discussion_id",
            "        else:",
            "            last_discussion_id = last_discussion_id[0]",
            "        self.current_message_id = self.select(\"SELECT id FROM message WHERE discussion_id=? ORDER BY id DESC LIMIT 1\", (last_discussion_id,), fetch_all=False)",
            "        return Discussion(self.lollms, last_discussion_id, self)",
            "    ",
            "    def create_discussion(self, title=\"untitled\"):",
            "        \"\"\"Creates a new discussion",
            "",
            "        Args:",
            "            title (str, optional): The title of the discussion. Defaults to \"untitled\".",
            "",
            "        Returns:",
            "            Discussion: A Discussion instance ",
            "        \"\"\"",
            "        discussion_id = self.insert(f\"INSERT INTO discussion (title) VALUES (?)\",(title,))",
            "        return Discussion(self.lollms, discussion_id, self)",
            "",
            "    def build_discussion(self, discussion_id=0):",
            "        return Discussion(self.lollms, discussion_id, self)",
            "",
            "    def get_discussions(self):",
            "        rows = self.select(\"SELECT * FROM discussion\")         ",
            "        return [{\"id\": row[0], \"title\": row[1]} for row in rows]",
            "",
            "    def does_last_discussion_have_messages(self):",
            "        last_discussion_id = self.select(\"SELECT id FROM discussion ORDER BY id DESC LIMIT 1\", fetch_all=False)",
            "        if last_discussion_id is None:",
            "            last_discussion = self.create_discussion()",
            "            last_discussion_id = last_discussion.discussion_id",
            "        else:",
            "            last_discussion_id = last_discussion_id[0]",
            "        last_message = self.select(\"SELECT * FROM message WHERE discussion_id=?\", (last_discussion_id,), fetch_all=False)",
            "        return last_message is not None",
            "    ",
            "    def remove_discussions(self):",
            "        self.delete(\"DELETE FROM message\")",
            "        self.delete(\"DELETE FROM discussion\")",
            "",
            "",
            "    def export_to_json(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a JSON format.",
            "",
            "        Returns:",
            "            list: A list of dictionaries representing discussions and their messages.",
            "                Each dictionary contains the discussion ID, title, and a list of messages.",
            "                Each message dictionary contains the sender, content, message type, rank,",
            "                parent message ID, binding, model, personality, created at, and finished",
            "                generating at fields.",
            "        \"\"\"        ",
            "        db_discussions = self.select(\"SELECT * FROM discussion\")",
            "        discussions = []",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussion = {\"id\": discussion_id, \"title\":discussion_title, \"messages\": []}",
            "",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[10]",
            "                nb_tokens =  message_row[11]",
            "",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"parent_message_id\": parent_message_id, \"binding\": binding, \"model\":model, \"personality\":personality, \"created_at\":created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\":finished_generating_at, \"nb_tokens\":nb_tokens}",
            "                )",
            "            discussions.append(discussion)",
            "",
            "        return discussions",
            "",
            "    def export_all_as_markdown_list_for_vectorization(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a Markdown list format.",
            "",
            "        Returns:",
            "            list: A list of lists representing discussions and their messages in a Markdown format.",
            "                Each inner list contains the discussion title and a string representing all",
            "                messages in the discussion in a Markdown format.",
            "        \"\"\"        ",
            "        data = self.export_all_discussions_to_json()",
            "        # Initialize an empty result string",
            "        discussions = []",
            "        # Iterate through discussions in the JSON data",
            "        for discussion in data:",
            "            # Extract the title",
            "            title = discussion['title']",
            "            messages = \"\"",
            "            # Iterate through messages in the discussion",
            "            for message in discussion['messages']:",
            "                sender = message['sender']",
            "                content = message['content']",
            "                # Append the sender and content in a Markdown format",
            "                messages += f'{sender}: {content}\\n'",
            "            discussions.append([title, messages])",
            "        return discussions",
            "        ",
            "    def export_all_as_markdown(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a Markdown format.",
            "",
            "        Returns:",
            "            str: A string representing all discussions and their messages in a Markdown format.",
            "                Each discussion is represented as a Markdown heading, and each message is",
            "                represented with the sender and content in a Markdown format.",
            "        \"\"\"        ",
            "        data = self.export_all_discussions_to_json()",
            "",
            "        # Initialize an empty result string",
            "        result = ''",
            "",
            "        # Iterate through discussions in the JSON data",
            "        for discussion in data:",
            "            # Extract the title",
            "            title = discussion['title']",
            "            # Append the title with '#' as Markdown heading",
            "            result += f'#{title}\\n'",
            "",
            "            # Iterate through messages in the discussion",
            "            for message in discussion['messages']:",
            "                sender = message['sender']",
            "                content = message['content']",
            "                # Append the sender and content in a Markdown format",
            "                result += f'{sender}: {content}\\n'",
            "",
            "        return result",
            "",
            "    def export_all_discussions_to_json(self):",
            "        # Convert the list of discussion IDs to a tuple",
            "        db_discussions = self.select(",
            "            f\"SELECT * FROM discussion\"",
            "        )",
            "        discussions = []",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussion = {\"id\": discussion_id, \"title\":discussion_title, \"messages\": []}",
            "",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[10]",
            "                nb_tokens = message_row[11]",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"parent_message_id\": parent_message_id, \"binding\": binding, \"model\":model, \"personality\":personality, \"created_at\":created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\": finished_generating_at,\"nb_tokens\":nb_tokens}",
            "                )",
            "            discussions.append(discussion)",
            "        return discussions",
            "",
            "    def export_discussions_to_json(self, discussions_ids:list):",
            "        # Convert the list of discussion IDs to a tuple",
            "        discussions_ids_tuple = tuple(discussions_ids)",
            "        txt = ','.join(['?'] * len(discussions_ids_tuple))",
            "        db_discussions = self.select(",
            "            f\"SELECT * FROM discussion WHERE id IN ({txt})\",",
            "            discussions_ids_tuple",
            "        )",
            "        discussions = []",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussion = {\"id\": discussion_id, \"title\":discussion_title, \"messages\": []}",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[9]",
            "                nb_tokens = message_row[9]",
            "                ",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"parent_message_id\": parent_message_id, \"binding\": binding, \"model\":model, \"personality\":personality, \"created_at\":created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\": finished_generating_at, \"nb_tokens\":nb_tokens}",
            "                )",
            "            discussions.append(discussion)",
            "        return discussions",
            "    ",
            "    def import_from_json(self, json_data):",
            "        discussions = []",
            "        data = json_data",
            "        for discussion_data in data:",
            "            discussion_id = discussion_data.id",
            "            discussion_title = discussion_data.title",
            "            messages_data = discussion_data.messages",
            "            discussion = {\"id\": discussion_id, \"title\": discussion_title, \"messages\": []}",
            "",
            "            # Insert discussion into the database",
            "            discussion_id = self.insert(\"INSERT INTO discussion (title) VALUES (?)\", (discussion_title,))",
            "",
            "            for message_data in messages_data:",
            "                sender = message_data.get(\"sender\")",
            "                content = message_data.get(\"content\")",
            "                content_type = message_data.get(\"message_type\",message_data.get(\"type\"))",
            "                rank = message_data.get(\"rank\")",
            "                parent_message_id = message_data.get(\"parent_message_id\")",
            "                binding = message_data.get(\"binding\",\"\")",
            "                model = message_data.get(\"model\",\"\")",
            "                personality = message_data.get(\"personality\",\"\")",
            "                created_at = message_data.get(\"created_at\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "",
            "                started_generating_at = message_data.get(\"started_generating_at\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "                finished_generating_at = message_data.get(\"finished_generating_at\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "                nb_tokens = message_data.get(\"nb_tokens\",datetime.now().strftime('%Y-%m-%d %H:%M:%S'))",
            "                discussion[\"messages\"].append(",
            "                    {\"sender\": sender, \"content\": content, \"message_type\": content_type, \"rank\": rank, \"binding\": binding, \"model\": model, \"personality\": personality, \"created_at\": created_at, \"started_generating_at\":started_generating_at, \"finished_generating_at\": finished_generating_at, \"nb_tokens\":nb_tokens}",
            "                )",
            "",
            "                # Insert message into the database",
            "                self.insert(\"INSERT INTO message (sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",",
            "                            (sender, content, content_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id))",
            "",
            "            discussions.append(discussion)",
            "",
            "        return discussions",
            "",
            "    def export_discussions_to_markdown(self, discussions_ids:list, title = \"\"):",
            "        # Convert the list of discussion IDs to a tuple",
            "        discussions_ids_tuple = tuple(discussions_ids)",
            "        txt = ','.join(['?'] * len(discussions_ids_tuple))",
            "        db_discussions = self.select(",
            "            f\"SELECT * FROM discussion WHERE id IN ({txt})\",",
            "            discussions_ids_tuple",
            "        )",
            "        discussions = f\"# {title}\" if title!=\"\" else \"\"",
            "        for row in db_discussions:",
            "            discussion_id = row[0]",
            "            discussion_title = row[1]",
            "            discussions += f\"## {discussion_title}\\n\"",
            "",
            "            rows = self.select(f\"SELECT sender, content, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens FROM message WHERE discussion_id=?\",(discussion_id,))",
            "            for message_row in rows:",
            "                sender = message_row[0]",
            "                content = message_row[1]",
            "                content_type = message_row[2]",
            "                rank = message_row[3]",
            "                parent_message_id = message_row[4]",
            "                binding = message_row[5]",
            "                model = message_row[6]",
            "                personality = message_row[7]",
            "                created_at = message_row[8]",
            "                started_generating_at = message_row[9]",
            "                finished_generating_at = message_row[10]",
            "                nb_tokens = message_row[11]",
            "",
            "                discussions +=f\"### {sender}:\\n{content}\\n\"",
            "            discussions +=f\"\\n\"",
            "        return discussions",
            "",
            "",
            "class Message:",
            "    def __init__(",
            "                    self,",
            "                    discussion_id,",
            "                    discussions_db,",
            "                    message_type,",
            "                    sender_type,",
            "                    sender,",
            "                    content,",
            "                    metadata                = None,",
            "                    ui                      = None,",
            "                    rank                    = 0,",
            "                    parent_message_id       = 0,",
            "                    binding                 = \"\",",
            "                    model                   = \"\",",
            "                    personality             = \"\",",
            "                    created_at              = None,",
            "                    started_generating_at   = None,",
            "                    finished_generating_at  = None,",
            "                    nb_tokens     = None,",
            "                    id                      = None,",
            "                    insert_into_db          = False",
            "                    ):",
            "        ",
            "        self.discussion_id = discussion_id",
            "        self.discussions_db = discussions_db",
            "        self.self = self",
            "        self.sender = sender",
            "        self.sender_type = sender_type",
            "        self.content = content",
            "        self.message_type = message_type",
            "        self.rank = rank",
            "        self.parent_message_id = parent_message_id",
            "        self.binding = binding",
            "        self.model = model",
            "        self.metadata = json.dumps(metadata, indent=4) if metadata is not None and type(metadata)== dict else metadata",
            "        self.ui = ui",
            "        self.personality = personality",
            "        self.created_at = created_at",
            "        self.started_generating_at = started_generating_at",
            "        self.finished_generating_at = finished_generating_at",
            "        self.nb_tokens = nb_tokens",
            "",
            "        if insert_into_db:",
            "            self.id = self.discussions_db.insert(",
            "                \"INSERT INTO message (sender,  message_type,  sender_type,  sender,  content,  metadata, ui,  rank,  parent_message_id,  binding,  model,  personality,  created_at, started_generating_at,  finished_generating_at, nb_tokens,  discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", ",
            "                (sender, message_type, sender_type, sender, content, metadata, ui, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id)",
            "            )",
            "        else:",
            "            self.id = id",
            "",
            "",
            "    @staticmethod",
            "    def get_fields():",
            "        return [",
            "            \"id\",",
            "            \"message_type\",",
            "            \"sender_type\",",
            "            \"sender\",",
            "            \"content\",",
            "            \"metadata\",",
            "            \"ui\",",
            "            \"rank\",",
            "            \"parent_message_id\",",
            "            \"binding\",",
            "            \"model\",",
            "            \"personality\",",
            "            \"created_at\",",
            "            \"started_generating_at\",",
            "            \"finished_generating_at\",",
            "            \"nb_tokens\",",
            "            \"discussion_id\"",
            "        ]        ",
            "",
            "    @staticmethod",
            "    def from_db(discussions_db, message_id):",
            "        columns = Message.get_fields()",
            "        rows = discussions_db.select(",
            "            f\"SELECT {','.join(columns)} FROM message WHERE id=?\", (message_id,)",
            "        )",
            "        data_dict={",
            "            col:rows[0][i]",
            "            for i,col in enumerate(columns)",
            "        }",
            "        data_dict[\"discussions_db\"]=discussions_db",
            "        return Message(",
            "            **data_dict",
            "        )",
            "",
            "    @staticmethod",
            "    def from_dict(discussions_db,data_dict):",
            "        data_dict[\"discussions_db\"]=discussions_db",
            "        return Message(",
            "            **data_dict",
            "        )",
            "",
            "    def insert_into_db(self):",
            "        self.message_id = self.discussions_db.insert(",
            "            \"INSERT INTO message (sender, content, metadata, ui, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", ",
            "            (self.sender, self.content, self.metadata, self.ui, self.message_type, self.rank, self.parent_message_id, self.binding, self.model, self.personality, self.created_at, self.started_generating_at, self.finished_generating_at, self.nb_tokens, self.discussion_id)",
            "        )",
            "",
            "    def update_db(self):",
            "        self.message_id = self.discussions_db.insert(",
            "            \"INSERT INTO message (sender, content, metadata, ui, message_type, rank, parent_message_id, binding, model, personality, created_at, started_generating_at, finished_generating_at, nb_tokens, discussion_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", ",
            "            (self.sender, self.content, self.metadata, self.ui, self.message_type, self.rank, self.parent_message_id, self.binding, self.model, self.personality, self.created_at, self.started_generating_at, self.finished_generating_at, nb_tokens, self.discussion_id)",
            "        )",
            "",
            "    def update(self, new_content, new_metadata=None, new_ui=None, started_generating_at=None, nb_tokens=None, commit=True):",
            "        self.finished_generating_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "        text = f\"UPDATE message SET content = ?\"",
            "        params = [new_content]",
            "        if new_metadata is not None:",
            "            text+=\", metadata = ?\"",
            "            params.append(new_metadata)",
            "        if new_ui is not None:",
            "            text+=\", ui = ?\"",
            "            params.append(new_ui)",
            "",
            "        if started_generating_at is not None:",
            "            text+=\", started_generating_at = ?\"",
            "            params.append(started_generating_at)",
            "",
            "        if nb_tokens is not None:",
            "            text+=\", nb_tokens = ?\"",
            "            params.append(nb_tokens)",
            "",
            "",
            "        text +=\", finished_generating_at = ? WHERE id = ?\"",
            "        params.append(self.finished_generating_at)",
            "        params.append(self.id)",
            "        self.discussions_db.update(",
            "            text, tuple(params)",
            "        )        ",
            "",
            "    def to_json(self):",
            "        attributes = Message.get_fields()",
            "        msgJson = {}",
            "        for attribute_name in attributes:",
            "            attribute_value = getattr(self, attribute_name, None)",
            "            if attribute_name==\"metadata\":",
            "                if type(attribute_value) == str:",
            "                    msgJson[attribute_name] = json.loads(attribute_value)",
            "                else:",
            "                    msgJson[attribute_name] = attribute_value",
            "            else:",
            "                msgJson[attribute_name] = attribute_value",
            "        return msgJson",
            "",
            "class Discussion:",
            "    def __init__(self, lollms:LoLLMsCom, discussion_id, discussions_db:DiscussionsDB):",
            "        self.lollms = lollms",
            "        self.current_message = None",
            "        self.discussion_id = discussion_id",
            "        self.discussions_db = discussions_db",
            "        self.discussion_folder = self.discussions_db.discussion_db_path/f\"{discussion_id}\"",
            "        self.discussion_audio_folder = self.discussion_folder / \"audio\"",
            "        self.discussion_images_folder = self.discussion_folder / \"images\"",
            "        self.discussion_text_folder = self.discussion_folder / \"text_data\"",
            "        self.discussion_skills_folder = self.discussion_folder / \"skills\"",
            "        self.discussion_rag_folder = self.discussion_folder / \"rag\"",
            "        self.discussion_view_images_folder = self.discussion_folder / \"view_images\"",
            "",
            "        self.discussion_folder.mkdir(exist_ok=True)",
            "        self.discussion_images_folder.mkdir(exist_ok=True)",
            "        self.discussion_text_folder.mkdir(exist_ok=True)",
            "        self.discussion_skills_folder.mkdir(exist_ok=True)",
            "        self.discussion_rag_folder.mkdir(exist_ok=True)",
            "        self.discussion_view_images_folder.mkdir(exist_ok=True)",
            "        self.messages = self.get_messages()",
            "        ",
            "        if len(self.messages)>0:",
            "            self.current_message = self.messages[-1]",
            "",
            "        # Initialize the file lists",
            "        self.update_file_lists()",
            "",
            "        if len(self.text_files)>0:",
            "            self.vectorizer = VectorDatabase(",
            "                                        self.discussion_rag_folder/\"db.sqli\",",
            "                                        BERTVectorizer(self.lollms.config.rag_vectorizer_model) if self.lollms.config.rag_vectorizer==\"bert\" else TFIDFVectorizer(),",
            "                                        self.lollms.model,",
            "                                        chunk_size=self.lollms.config.rag_chunk_size,",
            "                                        overlap=self.lollms.config.rag_overlap",
            "                                        )",
            "            ",
            "            if len(self.vectorizer.list_documents())==0 and len(self.text_files)>0:",
            "                for path in self.text_files:",
            "                    data = GenericDataLoader.read_file(path)",
            "                    try:",
            "                        self.vectorizer.add_document(path.stem, data, path, True)",
            "                    except Exception as ex:",
            "                        trace_exception(ex)            ",
            "                try:",
            "                    self.vectorizer.index()",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "        else:",
            "            self.vectorizer = None",
            "",
            "    def update_file_lists(self):",
            "        self.text_files = [Path(file) for file in self.discussion_text_folder.glob('*')]",
            "        self.image_files = [Path(file) for file in self.discussion_images_folder.glob('*')]",
            "        self.audio_files = [Path(file) for file in self.discussion_audio_folder.glob('*')]",
            "        self.rag_db = [Path(file) for file in self.discussion_rag_folder.glob('*')]",
            "",
            "",
            "    def remove_file(self, file_name, callback=None):",
            "        try:",
            "            all_files = self.text_files+self.image_files+self.audio_files",
            "            if any(file_name == entry.name for entry in self.text_files):",
            "                fn = [entry for entry in self.text_files if entry.name == file_name][0]",
            "                self.text_files = [entry for entry in self.text_files if entry.name != file_name]",
            "                try:",
            "                    text = TextDocumentsLoader.read_file(fn)",
            "                    hash = self.vectorizer._hash_document(text)",
            "                    self.vectorizer.remove_document(hash)",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "",
            "                Path(fn).unlink()",
            "                if len(self.text_files)>0:",
            "                    try:",
            "                        self.vectorizer.remove_document(fn)",
            "                        if callback is not None:",
            "                            callback(\"File removed successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                        return True",
            "                    except ValueError as ve:",
            "                        ASCIIColors.error(f\"Couldn't remove the file\")",
            "                        return False",
            "                else:",
            "                    self.vectorizer = None",
            "            elif any(file_name == entry.name for entry in self.image_files):",
            "                fn = [entry for entry in self.image_files if entry.name == file_name][0]",
            "                self.image_files = [entry for entry in self.image_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "            elif any(file_name == entry.name for entry in self.audio_files):",
            "                fn = [entry for entry in self.audio_files if entry.name == file_name][0]",
            "                self.audio_files = [entry for entry in self.audio_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "",
            "        except Exception as ex:",
            "            trace_exception(ex)",
            "            ASCIIColors.warning(f\"Couldn't remove the file {file_name}\")",
            "",
            "    def remove_all_files(self):",
            "        # Iterate over each directory and remove all files",
            "        for path in [self.discussion_images_folder, self.discussion_rag_folder, self.discussion_audio_folder, self.discussion_text_folder]:",
            "            ",
            "            for file in path.glob('*'):",
            "                if file.is_file() and file.suffix!=\".sqli\":  # Ensure it's a file, not a directory",
            "                    try:",
            "                        text = TextDocumentsLoader.read_file(file)",
            "                        hash = self.vectorizer._hash_document(text)",
            "                        self.vectorizer.remove_document(hash)",
            "                    except Exception as ex:",
            "                        trace_exception(ex)",
            "                    file.unlink()  # Delete the file",
            "                    ",
            "        # Clear the lists to reflect the current state (empty directories)",
            "        self.text_files.clear()",
            "        self.image_files.clear()",
            "        self.audio_files.clear()",
            "        self.vectorizer = None",
            "        gc.collect()",
            "        fn = self.discussion_rag_folder/\"db.sqli\"",
            "        try:",
            "            fn.unlink()",
            "        except Exception as ex:",
            "            trace_exception(ex)",
            "",
            "    def add_file(self, path, client, tasks_library:TasksLibrary, callback=None, process=True):",
            "        output = \"\"",
            "",
            "        path = Path(path)",
            "        if path.suffix in [\".wav\",\".mp3\"]:",
            "            self.audio_files.append(path)",
            "            if process:",
            "                self.lollms.new_message(client.client_id if client is not None else 0, content = \"\", message_type = MSG_TYPE.MSG_TYPE_FULL)",
            "                if self.lollms.stt is None:",
            "                    self.lollms.info(\"Please select an stt engine in the services settings first\")",
            "                self.lollms.info(f\"Transcribing ... \")",
            "                transcription = self.lollms.stt.transcribe(str(path))",
            "                transcription_fn = self.discussion_text_folder/(path.stem+\".txt\")",
            "                with open(transcription_fn, \"w\", encoding=\"utf-8\") as f:",
            "                    f.write(transcription)",
            "                self.text_files.append(transcription_fn)",
            "                tasks_library.info(f\"Transcription saved to {transcription_fn}\")",
            "",
            "        elif path.suffix in [\".png\",\".jpg\",\".jpeg\",\".gif\",\".bmp\",\".svg\",\".webp\"]:",
            "            self.image_files.append(path)",
            "            if process:",
            "                ",
            "                try:",
            "                    view_file = self.discussion_view_images_folder/path.name",
            "                    shutil.copyfile(path, view_file)",
            "                    pth = str(view_file).replace(\"\\\\\",\"/\").split('/')",
            "                    if \"discussion_databases\" in pth:",
            "                        pth = discussion_path_to_url(view_file)",
            "                        self.lollms.new_message(client.client_id if client is not None else 0, content = \"\", message_type = MSG_TYPE.MSG_TYPE_FULL)",
            "                        output = f'<img src=\"{pth}\" width=\"800\">\\n\\n'",
            "                        self.lollms.full(output, client_id=client.client_id)",
            "                        self.lollms.close_message(client.client_id if client is not None else 0)",
            "",
            "                    if self.lollms.model.binding_type not in [BindingType.TEXT_IMAGE, BindingType.TEXT_IMAGE_VIDEO]:",
            "                        # self.ShowBlockingMessage(\"Understanding image (please wait)\")",
            "                        from PIL import Image",
            "                        img = Image.open(str(view_file))",
            "                        # Convert the image to RGB mode",
            "                        img = img.convert(\"RGB\")",
            "                        output += \"## image description :\\n\"+ self.lollms.model.interrogate_blip([img])[0]",
            "                        # output += \"## image description :\\n\"+ self.lollms.model.qna_blip([img],\"q:Describe this photo with as much details as possible.\\na:\")[0]",
            "                        self.lollms.full(output)",
            "                        self.lollms.close_message(client.client_id if client is not None else 0)",
            "                        self.lollms.HideBlockingMessage(\"Understanding image (please wait)\")",
            "                        if self.lollms.config.debug:",
            "                            ASCIIColors.yellow(output)",
            "                    else:",
            "                        # self.ShowBlockingMessage(\"Importing image (please wait)\")",
            "                        self.lollms.HideBlockingMessage(\"Importing image (please wait)\")",
            "",
            "                except Exception as ex:",
            "                    trace_exception(ex)",
            "                    self.lollms.HideBlockingMessage(\"Understanding image (please wait)\", False)",
            "                    ASCIIColors.error(\"Couldn't create new message\")",
            "            ASCIIColors.info(\"Received image file\")",
            "            if callback is not None:",
            "                callback(\"Image file added successfully\", MSG_TYPE.MSG_TYPE_INFO)",
            "        else:",
            "            try:",
            "                # self.ShowBlockingMessage(\"Adding file to vector store.\\nPlease stand by\")",
            "                self.text_files.append(path)",
            "                ASCIIColors.info(\"Received text compatible file\")",
            "                self.lollms.ShowBlockingMessage(\"Processing file\\nPlease wait ...\")",
            "                if process:",
            "                    if self.vectorizer is None:",
            "                        self.vectorizer = VectorDatabase(",
            "                                    self.discussion_rag_folder/\"db.sqli\",",
            "                                    BERTVectorizer(self.lollms.config.rag_vectorizer_model) if self.lollms.config.rag_vectorizer==\"bert\" else TFIDFVectorizer(),",
            "                                    self.lollms.model,",
            "                                    )",
            "                    data = TextDocumentsLoader.read_file(path)",
            "                    self.vectorizer.add_document(path.stem, data, path, True)",
            "                    self.vectorizer.build_index()",
            "                    if callback is not None:",
            "                        callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                    self.lollms.HideBlockingMessage(client.client_id)",
            "                    return True",
            "            except Exception as e:",
            "                trace_exception(e)",
            "                self.lollms.InfoMessage(f\"Unsupported file format or empty file.\\nSupported formats are {GenericDataLoader.get_supported_file_types()}\",client_id=client.client_id)",
            "                return False",
            "",
            "    def load_message(self, id):",
            "        \"\"\"Gets a list of messages information",
            "",
            "        Returns:",
            "            list: List of entries in the format {\"id\":message id, \"sender\":sender name, \"content\":message content, \"message_type\":message type, \"rank\": message rank}",
            "        \"\"\"",
            "        self.current_message = Message.from_db(self.discussions_db, id)",
            "        return self.current_message",
            "    ",
            "    def add_message(",
            "                    self, ",
            "                    message_type,",
            "                    sender_type,",
            "                    sender,",
            "                    content,",
            "                    metadata=None,",
            "                    ui=None,",
            "                    rank=0, ",
            "                    parent_message_id=0, ",
            "                    binding=\"\", ",
            "                    model =\"\", ",
            "                    personality=\"\", ",
            "                    created_at=None, ",
            "                    started_generating_at=None,",
            "                    finished_generating_at=None,",
            "                    nb_tokens=None",
            "                ):",
            "        \"\"\"Adds a new message to the discussion",
            "",
            "        Args:",
            "            sender (str): The sender name",
            "            content (str): The text sent by the sender",
            "",
            "        Returns:",
            "            int: The added message id",
            "        \"\"\"",
            "        if created_at is None:",
            "            created_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "",
            "        if started_generating_at is None:",
            "            started_generating_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "",
            "        if finished_generating_at is None:",
            "            finished_generating_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
            "",
            "        if nb_tokens is None:",
            "            nb_tokens = 0",
            "",
            "        self.current_message = Message(",
            "            self.discussion_id,",
            "            self.discussions_db,",
            "            message_type,",
            "            sender_type,",
            "            sender,",
            "            content,",
            "            metadata,",
            "            ui,",
            "            rank,",
            "            parent_message_id,",
            "            binding,",
            "            model,",
            "            personality,",
            "            created_at,",
            "            started_generating_at,",
            "            finished_generating_at,",
            "            nb_tokens,",
            "            insert_into_db=True",
            "        )",
            "",
            "        self.messages.append(self.current_message)",
            "        return self.current_message",
            "",
            "    def rename(self, new_title):",
            "        \"\"\"Renames the discussion",
            "",
            "        Args:",
            "            new_title (str): The nex discussion name",
            "        \"\"\"",
            "        self.discussions_db.update(",
            "            f\"UPDATE discussion SET title=? WHERE id=?\",(new_title,self.discussion_id)",
            "        )",
            "",
            "    def title(self):",
            "        \"\"\"Renames the discussion",
            "",
            "        Args:",
            "            new_title (str): The nex discussion name",
            "        \"\"\"",
            "        rows = self.discussions_db.select(",
            "            f\"Select title from discussion WHERE id={self.discussion_id}\"",
            "        )",
            "        return rows[0][0]",
            "",
            "    def delete_discussion(self):",
            "        \"\"\"Deletes the discussion",
            "        \"\"\"",
            "        self.discussions_db.delete(",
            "            f\"DELETE FROM message WHERE discussion_id={self.discussion_id}\"",
            "        )",
            "        self.discussions_db.delete(",
            "            f\"DELETE FROM discussion WHERE id={self.discussion_id}\"",
            "        )",
            "",
            "    def get_messages(self):",
            "        \"\"\"Gets a list of messages information",
            "",
            "        Returns:",
            "            list: List of entries in the format {\"id\":message id, \"sender\":sender name, \"content\":message content, \"message_type\":message type, \"rank\": message rank}",
            "        \"\"\"",
            "        columns = Message.get_fields()",
            "",
            "        rows = self.discussions_db.select(",
            "            f\"SELECT {','.join(columns)} FROM message WHERE discussion_id=?\", (self.discussion_id,)",
            "        )",
            "        msg_dict = [{ c:row[i] for i,c in enumerate(columns)} for row in rows]",
            "        self.messages=[]",
            "        for msg in msg_dict:",
            "            self.messages.append(Message.from_dict(self.discussions_db, msg))",
            "",
            "        if len(self.messages)>0:",
            "            self.current_message = self.messages[-1]",
            "",
            "        return self.messages",
            "",
            "    def get_message(self, message_id):",
            "        for message in self.messages:",
            "            if message.id == int(message_id):",
            "                self.current_message = message",
            "                return message",
            "        return None",
            "",
            "    def select_message(self, message_id):",
            "        msg = self.get_message(message_id)",
            "        if msg is not None:",
            "            self.current_message = msg",
            "            return True",
            "        else:",
            "            return False ",
            "",
            "    def update_message(self, new_content, new_metadata=None, new_ui=None, started_generating_at=None, nb_tokens=None):",
            "        \"\"\"Updates the content of a message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "            new_content (str): The nex message content",
            "        \"\"\"",
            "        self.current_message.update(new_content, new_metadata, new_ui, started_generating_at, nb_tokens)",
            "",
            "    def edit_message(self, message_id, new_content, new_metadata=None, new_ui=None):",
            "        \"\"\"Edits the content of a message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "            new_content (str): The nex message content",
            "        \"\"\"",
            "        msg = self.get_message(message_id)",
            "        if msg:",
            "            msg.update(new_content, new_metadata, new_ui)",
            "            return True",
            "        else:",
            "            return False",
            "",
            "",
            "    def message_rank_up(self, message_id):",
            "        \"\"\"Increments the rank of the message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "        \"\"\"",
            "        # Retrieve current rank value for message_id",
            "        current_rank = self.discussions_db.select(\"SELECT rank FROM message WHERE id=?\", (message_id,),False)[0]",
            "",
            "        # Increment current rank value by 1",
            "        new_rank = current_rank + 1        ",
            "        self.discussions_db.update(",
            "            f\"UPDATE message SET rank = ? WHERE id = ?\",(new_rank,message_id)",
            "        )",
            "        return new_rank",
            "",
            "    def message_rank_down(self, message_id):",
            "        \"\"\"Increments the rank of the message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be changed",
            "        \"\"\"",
            "        # Retrieve current rank value for message_id",
            "        current_rank = self.discussions_db.select(\"SELECT rank FROM message WHERE id=?\", (message_id,),False)[0]",
            "",
            "        # Increment current rank value by 1",
            "        new_rank = current_rank - 1        ",
            "        self.discussions_db.update(",
            "            f\"UPDATE message SET rank = ? WHERE id = ?\",(new_rank,message_id)",
            "        )",
            "        return new_rank",
            "    ",
            "    def delete_message(self, message_id):",
            "        \"\"\"Delete the message",
            "",
            "        Args:",
            "            message_id (int): The id of the message to be deleted",
            "        \"\"\"",
            "        # Retrieve current rank value for message_id",
            "        self.discussions_db.delete(\"DELETE FROM message WHERE id=?\", (message_id,))",
            "",
            "    def export_for_vectorization(self):",
            "        \"\"\"",
            "        Export all discussions and their messages from the database to a Markdown list format.",
            "",
            "        Returns:",
            "            list: A list of lists representing discussions and their messages in a Markdown format.",
            "                Each inner list contains the discussion title and a string representing all",
            "                messages in the discussion in a Markdown format.",
            "        \"\"\"        ",
            "        # Extract the title",
            "        title = self.title()",
            "        messages = \"\"",
            "        # Iterate through messages in the discussion",
            "        for message in self.messages:",
            "            sender = message.sender",
            "            content = message.content",
            "            # Append the sender and content in a Markdown format",
            "            messages += f'{sender}: {content}\\n'",
            "        return title, messages",
            " ",
            "    def format_discussion(self, max_allowed_tokens, splitter_text=None):",
            "        if not splitter_text:",
            "            splitter_text = self.lollms.config.discussion_prompt_separator",
            "        formatted_text = \"\"",
            "        for message in reversed(self.messages):  # Start from the newest message",
            "            formatted_message = f\"{splitter_text}{message.sender.replace(':','').replace(splitter_text,'')}:\\n{message.content}\\n\"",
            "            tokenized_message = self.lollms.model.tokenize(formatted_message)",
            "            if len(tokenized_message) + len(self.lollms.model.tokenize(formatted_text)) <= max_allowed_tokens:",
            "                formatted_text = formatted_message + formatted_text",
            "            else:",
            "                break  # Stop if adding the next message would exceed the limit",
            "        return formatted_text   ",
            "# ========================================================================================================================"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "2",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "654": [
                "Discussion",
                "__init__"
            ],
            "655": [
                "Discussion",
                "__init__"
            ],
            "656": [
                "Discussion",
                "__init__"
            ],
            "657": [
                "Discussion",
                "__init__"
            ],
            "658": [
                "Discussion",
                "__init__"
            ],
            "659": [
                "Discussion",
                "__init__"
            ],
            "660": [
                "Discussion",
                "__init__"
            ],
            "661": [
                "Discussion",
                "__init__"
            ],
            "662": [
                "Discussion",
                "__init__"
            ],
            "663": [
                "Discussion",
                "__init__"
            ],
            "664": [
                "Discussion",
                "__init__"
            ],
            "665": [
                "Discussion",
                "__init__"
            ],
            "667": [
                "Discussion",
                "__init__"
            ],
            "669": [
                "Discussion",
                "__init__"
            ],
            "670": [
                "Discussion",
                "__init__"
            ],
            "671": [
                "Discussion",
                "__init__"
            ],
            "672": [
                "Discussion",
                "__init__"
            ],
            "673": [
                "Discussion",
                "__init__"
            ],
            "717": [
                "Discussion",
                "remove_all_files"
            ],
            "790": [
                "Discussion",
                "add_file"
            ],
            "791": [
                "Discussion",
                "add_file"
            ],
            "792": [
                "Discussion",
                "add_file"
            ],
            "793": [
                "Discussion",
                "add_file"
            ],
            "794": [
                "Discussion",
                "add_file"
            ],
            "795": [
                "Discussion",
                "add_file"
            ],
            "796": [
                "Discussion",
                "add_file"
            ],
            "797": [
                "Discussion",
                "add_file"
            ],
            "798": [
                "Discussion",
                "add_file"
            ],
            "799": [
                "Discussion",
                "add_file"
            ]
        },
        "addLocation": []
    },
    "lollms/personality.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 16,
                "afterPatchRowNumber": 16,
                "PatchRowcode": " from lollms.utilities import PromptReshaper, PackageManager, discussion_path_to_url, process_ai_output, remove_text_from_string"
            },
            "1": {
                "beforePatchRowNumber": 17,
                "afterPatchRowNumber": 17,
                "PatchRowcode": " from lollms.com import NotificationType, NotificationDisplayType"
            },
            "2": {
                "beforePatchRowNumber": 18,
                "afterPatchRowNumber": 18,
                "PatchRowcode": " from lollms.client_session import Session, Client"
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 19,
                "PatchRowcode": "+from lollmsvectordb.vector_database import VectorDatabase"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 20,
                "PatchRowcode": "+from lollmsvectordb.lollms_vectorizers.bert_vectorizer import BERTVectorizer"
            },
            "5": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 21,
                "PatchRowcode": "+from lollmsvectordb.lollms_vectorizers.tfidf_vectorizer import TFIDFVectorizer"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 22,
                "PatchRowcode": "+from lollmsvectordb.text_document_loader import TextDocumentsLoader"
            },
            "7": {
                "beforePatchRowNumber": 19,
                "afterPatchRowNumber": 23,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 20,
                "afterPatchRowNumber": 24,
                "PatchRowcode": " import pkg_resources"
            },
            "9": {
                "beforePatchRowNumber": 21,
                "afterPatchRowNumber": 25,
                "PatchRowcode": " from pathlib import Path"
            },
            "10": {
                "beforePatchRowNumber": 1081,
                "afterPatchRowNumber": 1085,
                "PatchRowcode": "                 self.ShowBlockingMessage(\"Processing file\\nPlease wait ...\")"
            },
            "11": {
                "beforePatchRowNumber": 1082,
                "afterPatchRowNumber": 1086,
                "PatchRowcode": "                 if process:"
            },
            "12": {
                "beforePatchRowNumber": 1083,
                "afterPatchRowNumber": 1087,
                "PatchRowcode": "                     if self.vectorizer is None:"
            },
            "13": {
                "beforePatchRowNumber": 1084,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                        self.vectorizer = TextVectorizer("
            },
            "14": {
                "beforePatchRowNumber": 1085,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    self.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\""
            },
            "15": {
                "beforePatchRowNumber": 1086,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    model=self.model, #needed in case of using model_embedding"
            },
            "16": {
                "beforePatchRowNumber": 1087,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    database_path=client.discussion.discussion_rag_folder/\"db.json\","
            },
            "17": {
                "beforePatchRowNumber": 1088,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    save_db=self.config.data_vectorization_save_db,"
            },
            "18": {
                "beforePatchRowNumber": 1089,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    data_visualization_method=VisualizationMethod.PCA,"
            },
            "19": {
                "beforePatchRowNumber": 1090,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                                    database_dict=None)"
            },
            "20": {
                "beforePatchRowNumber": 1091,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    data = GenericDataLoader.read_file(path)"
            },
            "21": {
                "beforePatchRowNumber": 1092,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.vectorizer.add_document(path, data, self.config.data_vectorization_chunk_size, self.config.data_vectorization_overlap_size, add_first_line_to_all_chunks=True if path.suffix==\".csv\" else False)"
            },
            "22": {
                "beforePatchRowNumber": 1093,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                    self.vectorizer.index()"
            },
            "23": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1088,
                "PatchRowcode": "+                        self.vectorizer = VectorDatabase("
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1089,
                "PatchRowcode": "+                                    client.discussion.discussion_rag_folder/\"db.sqli\","
            },
            "25": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1090,
                "PatchRowcode": "+                                    BERTVectorizer(self.config.rag_vectorizer_model) if self.config.rag_vectorizer==\"bert\" else TFIDFVectorizer(),"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1091,
                "PatchRowcode": "+                                    self.model,"
            },
            "27": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1092,
                "PatchRowcode": "+                                    chunk_size=self.config.rag_chunk_size,"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1093,
                "PatchRowcode": "+                                    overlap=self.config.rag_overlap"
            },
            "29": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1094,
                "PatchRowcode": "+                                    )"
            },
            "30": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1095,
                "PatchRowcode": "+                    data = TextDocumentsLoader.read_file(path)"
            },
            "31": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1096,
                "PatchRowcode": "+                    self.vectorizer.add_document(path.stem, data, path, True)"
            },
            "32": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1097,
                "PatchRowcode": "+                    self.vectorizer.build_index()"
            },
            "33": {
                "beforePatchRowNumber": 1094,
                "afterPatchRowNumber": 1098,
                "PatchRowcode": "                     if callback is not None:"
            },
            "34": {
                "beforePatchRowNumber": 1095,
                "afterPatchRowNumber": 1099,
                "PatchRowcode": "                         callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)"
            },
            "35": {
                "beforePatchRowNumber": 1096,
                "afterPatchRowNumber": 1100,
                "PatchRowcode": "                     self.HideBlockingMessage(client.client_id)"
            }
        },
        "frontPatchFile": [
            "######",
            "# Project       : lollms",
            "# File          : personality.py",
            "# Author        : ParisNeo with the help of the community",
            "# license       : Apache 2.0",
            "# Description   :",
            "# This is an interface class for lollms personalities.",
            "######",
            "from fastapi import Request",
            "from datetime import datetime",
            "from pathlib import Path",
            "from lollms.config import InstallOption, TypedConfig, BaseConfig",
            "from lollms.main_config import LOLLMSConfig",
            "from lollms.paths import LollmsPaths",
            "from lollms.binding import LLMBinding, BindingType",
            "from lollms.utilities import PromptReshaper, PackageManager, discussion_path_to_url, process_ai_output, remove_text_from_string",
            "from lollms.com import NotificationType, NotificationDisplayType",
            "from lollms.client_session import Session, Client",
            "",
            "import pkg_resources",
            "from pathlib import Path",
            "from PIL import Image",
            "import re",
            "",
            "from datetime import datetime",
            "import importlib",
            "import shutil",
            "import subprocess",
            "import yaml",
            "from ascii_colors import ASCIIColors",
            "import time",
            "from lollms.types import MSG_TYPE, SUMMARY_MODE",
            "import json",
            "from typing import Any, List, Optional, Type, Callable, Dict, Any, Union",
            "import json",
            "from safe_store import TextVectorizer, GenericDataLoader, VisualizationMethod, VectorizationMethod, DocumentDecomposer",
            "from functools import partial",
            "import sys",
            "from lollms.com import LoLLMsCom",
            "from lollms.helpers import trace_exception",
            "from lollms.utilities import PackageManager",
            "",
            "from lollms.code_parser import compress_js, compress_python, compress_html",
            "",
            "",
            "import requests",
            "from bs4 import BeautifulSoup",
            "",
            "def get_element_id(url, text):",
            "    response = requests.get(url)",
            "    soup = BeautifulSoup(response.content, 'html.parser')",
            "    element = soup.find('span', text=text)",
            "    if element:",
            "        return element['id']",
            "    else:",
            "        return None",
            "",
            "def craft_a_tag_to_specific_text(url, text, caption):",
            "    # Encode the text to be used in the URL",
            "    encoded_text = text.replace(' ', '%20')",
            "",
            "    # Construct the URL with the anchor tag",
            "    anchor_url = f\"{url}#{encoded_text}\"",
            "",
            "    # Return the anchor tag",
            "    return anchor_url",
            "",
            "def is_package_installed(package_name):",
            "    try:",
            "        dist = pkg_resources.get_distribution(package_name)",
            "        return True",
            "    except pkg_resources.DistributionNotFound:",
            "        return False",
            "",
            "",
            "def install_package(package_name):",
            "    try:",
            "        # Check if the package is already installed",
            "        __import__(package_name)",
            "        print(f\"{package_name} is already installed.\")",
            "    except ImportError:",
            "        print(f\"{package_name} is not installed. Installing...\")",
            "",
            "        # Install the package using pip",
            "        subprocess.check_call([\"pip\", \"install\", package_name])",
            "",
            "        print(f\"{package_name} has been successfully installed.\")",
            "",
            "",
            "def fix_json(json_text):",
            "    try:",
            "        json_text.replace(\"}\\n{\",\"},\\n{\")",
            "        # Try to load the JSON string",
            "        json_obj = json.loads(json_text)",
            "        return json_obj",
            "    except json.JSONDecodeError as e:",
            "        trace_exception(e)",
            "class AIPersonality:",
            "",
            "    # Extra",
            "    def __init__(",
            "                    self,",
            "                    personality_package_path: str|Path,",
            "                    lollms_paths:LollmsPaths,",
            "                    config:LOLLMSConfig,",
            "                    model:LLMBinding=None,",
            "                    app:LoLLMsCom=None,",
            "                    run_scripts=True,",
            "                    selected_language=None,",
            "                    ignore_discussion_documents_rag=False,",
            "                    is_relative_path=True,",
            "                    installation_option:InstallOption=InstallOption.INSTALL_IF_NECESSARY,",
            "                    callback: Callable[[str, MSG_TYPE, dict, list], bool]=None",
            "                ):",
            "        \"\"\"",
            "        Initialize an AIPersonality instance.",
            "",
            "        Parameters:",
            "        personality_package_path (str or Path): The path to the folder containing the personality package.",
            "",
            "        Raises:",
            "        ValueError: If the provided path is not a folder or does not contain a config.yaml file.",
            "        \"\"\"",
            "        self.config = config",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        self.bot_says = \"\"",
            "",
            "        self.lollms_paths = lollms_paths",
            "        self.model = model",
            "        self.callback = callback",
            "        self.app = app",
            "",
            "        self.text_files = []",
            "        self.image_files = []",
            "        self.audio_files = []",
            "        self.images_descriptions = []",
            "        self.vectorizer = None",
            "",
            "        self.installation_option = installation_option",
            "",
            "        # Whisper to transcribe audio",
            "        self.whisper = None",
            "",
            "        # First setup a default personality",
            "        # Version",
            "        self._version = pkg_resources.get_distribution('lollms').version",
            "",
            "        self.run_scripts = run_scripts",
            "",
            "        #General information",
            "        self._author: str = \"ParisNeo\"",
            "        self._name: str = \"lollms\"",
            "        self._user_name: str = \"user\"",
            "        self._category: str = \"General\"",
            "        self._category_desc: str = \"General\"",
            "        self._language: str = \"english\"",
            "        self._supported_languages: str = []",
            "        self._selected_language: str = selected_language",
            "        self._ignore_discussion_documents_rag:bool = ignore_discussion_documents_rag",
            "",
            "        self._languages: List[dict]=[]",
            "",
            "",
            "",
            "        # Conditionning",
            "        self._personality_description: str = \"This personality is a helpful and Kind AI ready to help you solve your problems\"",
            "        self._personality_conditioning: str = \"\\n\".join([",
            "            \"lollms (Lord of LLMs) is a smart and helpful Assistant built by the computer geek ParisNeo.\",",
            "            \"It is compatible with many bindings to LLM models such as llama, gpt4all, gptj, autogptq etc.\",",
            "            \"It can discuss with humans and assist them on many subjects.\",",
            "            \"It runs locally on your machine. No need to connect to the internet.\",",
            "            \"It answers the questions with precise details\",",
            "            \"Its performance depends on the underlying model size and training.\",",
            "            \"Try to answer with as much details as you can\",",
            "            \"Date: {{date}}\",",
            "        ])",
            "        self._welcome_message: str = \"Welcome! I am lollms (Lord of LLMs) A free and open assistant built by ParisNeo. What can I do for you today?\"",
            "        self._include_welcome_message_in_discussion: bool = True",
            "        self._user_message_prefix: str = f\"human:\"",
            "        self._link_text: str = \"\\n\"",
            "        self._ai_message_prefix: str = f\"lollms:\"",
            "",
            "        # Extra",
            "        self._dependencies: List[str] = []",
            "",
            "        # Disclaimer",
            "        self._disclaimer: str = \"\"",
            "        self._help: str = \"\"",
            "        self._commands: list = []",
            "",
            "        # Default model parameters",
            "        self._model_temperature: float = 0.1 # higher: more creative, lower more deterministic",
            "        self._model_top_k: int = 50",
            "        self._model_top_p: float = 0.95",
            "        self._model_repeat_penalty: float = 1.3",
            "        self._model_repeat_last_n: int = 40",
            "",
            "        self._processor_cfg: dict = {}",
            "",
            "        self._logo: Optional[Image.Image] = None",
            "        self._processor = None",
            "        self._data = None",
            "",
            "",
            "",
            "        if personality_package_path is None:",
            "            self.config = {}",
            "            self.assets_list = []",
            "            self.personality_package_path = None",
            "            return",
            "        else:",
            "            parts = str(personality_package_path).split(\"/\")",
            "            self._category = parts[0]",
            "            if parts[0] == \"custom_personalities\":",
            "                self.personality_package_path = self.lollms_paths.custom_personalities_path/parts[1]",
            "            else:",
            "                if is_relative_path:",
            "                    self.personality_package_path = self.lollms_paths.personalities_zoo_path/personality_package_path",
            "                else:",
            "                    self.personality_package_path = Path(personality_package_path)",
            "",
            "            # Validate that the path exists",
            "            if not self.personality_package_path.exists():",
            "                raise ValueError(f\"Could not find the personality package:{self.personality_package_path}\")",
            "",
            "            # Validate that the path format is OK with at least a config.yaml file present in the folder",
            "            if not self.personality_package_path.is_dir():",
            "                raise ValueError(f\"Personality package path is not a folder:{self.personality_package_path}\")",
            "",
            "            self.personality_folder_name = self.personality_package_path.stem",
            "",
            "",
            "            self.personality_output_folder = lollms_paths.personal_outputs_path/self.name",
            "            self.personality_output_folder.mkdir(parents=True, exist_ok=True)",
            "            # Open and store the personality",
            "            self.load_personality()",
            "",
            "",
            "",
            "    def InfoMessage(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.InfoMessage(content=content, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.white(content)",
            "",
            "    def ShowBlockingMessage(self, content, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.ShowBlockingMessage(content=content, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.white(content)",
            "",
            "    def HideBlockingMessage(self, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.HideBlockingMessage(client_id=client_id, verbose=verbose)",
            "",
            "",
            "    def info(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.info(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.info(content)",
            "",
            "    def warning(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.warning(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.warning(content)",
            "",
            "    def success(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.success(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.success(content)",
            "",
            "    def error(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.error(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.error(content)",
            "",
            "    def notify( self,",
            "                content,",
            "                notification_type:NotificationType=NotificationType.NOTIF_SUCCESS,",
            "                duration:int=4,",
            "                client_id=None,",
            "                display_type:NotificationDisplayType=NotificationDisplayType.TOAST,",
            "                verbose=True",
            "            ):",
            "        if self.app:",
            "            return self.app.error(content=content, notification_type=notification_type, duration=duration, client_id=client_id, display_type=display_type, verbose=verbose)",
            "        ASCIIColors.white(content)",
            "",
            "",
            "",
            "    def new_message(self, message_text:str, message_type:MSG_TYPE= MSG_TYPE.MSG_TYPE_FULL, metadata=[], callback: Callable[[str, int, dict, list, Any], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(message_text, MSG_TYPE.MSG_TYPE_NEW_MESSAGE, parameters={'type':message_type.value,'metadata':metadata}, personality=self)",
            "",
            "    def full(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL)",
            "",
            "    def ui(self, ui_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends ui text to front end",
            "",
            "        Args:",
            "            ui_text (dict): The ui code to be sent to the front end",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(ui_text, MSG_TYPE.MSG_TYPE_UI)",
            "",
            "",
            "    def full_invisible_to_ai(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to AI)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI)",
            "",
            "    def full_invisible_to_user(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to user)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER)",
            "",
            "",
            "    def build_prompt(self, prompt_parts:List[str], sacrifice_id:int=-1, context_size:int=None, minimum_spare_context_size:int=None):",
            "        \"\"\"",
            "        Builds the prompt for code generation.",
            "",
            "        Args:",
            "            prompt_parts (List[str]): A list of strings representing the parts of the prompt.",
            "            sacrifice_id (int, optional): The ID of the part to sacrifice.",
            "            context_size (int, optional): The size of the context.",
            "            minimum_spare_context_size (int, optional): The minimum spare context size.",
            "",
            "        Returns:",
            "            str: The built prompt.",
            "        \"\"\"",
            "        if context_size is None:",
            "            context_size = self.config.ctx_size",
            "        if minimum_spare_context_size is None:",
            "            minimum_spare_context_size = self.config.min_n_predict",
            "",
            "        if sacrifice_id == -1 or len(prompt_parts[sacrifice_id])<50:",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "        else:",
            "            part_tokens=[]",
            "            nb_tokens=0",
            "            for i,part in enumerate(prompt_parts):",
            "                tk = self.model.tokenize(part)",
            "                part_tokens.append(tk)",
            "                if i != sacrifice_id:",
            "                    nb_tokens += len(tk)",
            "            if len(part_tokens[sacrifice_id])>0:",
            "                sacrifice_tk = part_tokens[sacrifice_id]",
            "                sacrifice_tk= sacrifice_tk[-(context_size-nb_tokens-minimum_spare_context_size):]",
            "                sacrifice_text = self.model.detokenize(sacrifice_tk)",
            "            else:",
            "                sacrifice_text = \"\"",
            "            prompt_parts[sacrifice_id] = sacrifice_text",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "",
            "    def add_collapsible_entry(self, title, content):",
            "        return \"\\n\".join(",
            "        [",
            "        f'<details class=\"flex w-fit rounded-xl border border-gray-200 bg-white shadow-sm dark:border-gray-800 dark:bg-gray-900 mb-3.5 max-w-full svelte-1escu1z\" open=\"\">',",
            "        f'    <summary class=\"grid min-w-72 select-none grid-cols-[40px,1fr] items-center gap-2.5 p-2 svelte-1escu1z\">',",
            "        f'        <dl class=\"leading-4\">',",
            "        f'        <dd class=\"text-sm\">{title}</dd>',",
            "        f'        <dt class=\"flex items-center gap-1 truncate whitespace-nowrap text-[.82rem] text-gray-400\">.Completed</dt>',",
            "        f'        </dl>',",
            "        f'    </summary>',",
            "        f' <div class=\"content px-5 pb-5 pt-4\">',",
            "        content,",
            "        f' </div>',",
            "        f' </details>\\n'",
            "        ])",
            "",
            "    def internet_search_with_vectorization(self, query, quick_search:bool=False, asses_using_llm=True):",
            "        \"\"\"",
            "        Do internet search and return the result",
            "        \"\"\"",
            "        from lollms.internet import internet_search_with_vectorization",
            "        return internet_search_with_vectorization(",
            "                                                    query,",
            "                                                    internet_nb_search_pages=int(self.config.internet_nb_search_pages),",
            "                                                    internet_vectorization_chunk_size=int(self.config.internet_vectorization_chunk_size),",
            "                                                    internet_vectorization_overlap_size=int(self.config.internet_vectorization_overlap_size),",
            "                                                    internet_vectorization_nb_chunks=int(self.config.internet_vectorization_nb_chunks),",
            "                                                    model = self.model,",
            "                                                    quick_search=quick_search,",
            "                                                    asses_using_llm=asses_using_llm,",
            "                                                    yes_no = self.yes_no",
            "                                                    )",
            "",
            "    def sink(self, s=None,i=None,d=None):",
            "        pass",
            "",
            "    def yes_no(self, question: str, context:str=\"\", max_answer_length: int = 50, conditionning=\"\") -> bool:",
            "        \"\"\"",
            "        Analyzes the user prompt and answers whether it is asking to generate an image.",
            "",
            "        Args:",
            "            question (str): The user's message.",
            "            max_answer_length (int, optional): The maximum length of the generated answer. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "        Returns:",
            "            bool: True if the user prompt is asking to generate an image, False otherwise.",
            "        \"\"\"",
            "        return self.multichoice_question(question, [\"no\",\"yes\"], context, max_answer_length, conditionning=conditionning)>0",
            "",
            "    def multichoice_question(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Interprets a multi-choice question from a users response. This function expects only one choice as true. All other choices are considered false. If none are correct, returns -1.",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                       f\"{start_header_id_template}context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "        elements += [",
            "                f\"{start_header_id_template}question{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50, callback=self.sink).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        selection = gen.strip().split()[0].replace(\",\",\"\").replace(\".\",\"\")",
            "        self.print_prompt(\"Multi choice selection\",prompt+gen)",
            "        try:",
            "            return int(selection)",
            "        except:",
            "            ASCIIColors.cyan(\"Model failed to answer the question\")",
            "            return -1",
            "",
            "    def multichoice_ranking(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Ranks answers for a question from best to worst. returns a list of integers",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "                f\"{start_header_id_template}{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                       f\"{start_header_id_template}context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        self.print_prompt(\"Multi choice ranking\",prompt+gen)",
            "        if gen.index(\"]\")>=0:",
            "            try:",
            "                ranks = eval(gen.split(\"]\")[0]+\"]\")",
            "                return ranks",
            "            except:",
            "                ASCIIColors.red(\"Model failed to rank inputs\")",
            "                return None",
            "        else:",
            "            ASCIIColors.red(\"Model failed to rank inputs\")",
            "            return None",
            "",
            "    def step_start(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step start",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step start to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_START)",
            "",
            "    def step_end(self, step_text, status=True, callback: Callable[[str, int, dict, list], bool]=None):",
            "        \"\"\"This triggers a step end",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step end to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_END, {'status':status})",
            "",
            "    def step(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step information",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP)",
            "",
            "    def print_prompt(self, title, prompt):",
            "        ASCIIColors.red(\"*-*-*-*-*-*-*-* \", end=\"\")",
            "        ASCIIColors.red(title, end=\"\")",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "        ASCIIColors.yellow(prompt)",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "",
            "",
            "",
            "    def fast_gen_with_images(self, prompt: str, images:list, max_generation_size: int=None, placeholders: dict = {}, sacrifice: list = [\"previous_discussion\"], debug: bool  = False, callback=None, show_progress=False) -> str:",
            "        \"\"\"",
            "        Fast way to generate text from text and images",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        prompt = \"\\n\".join([",
            "            f\"{start_header_id_template}{system_message_template}{end_header_id_template}I am an AI assistant that can converse and analyze images. When asked to locate something in an image you send, I will reply with:\",",
            "            \"boundingbox(image_index, label, left, top, width, height)\",",
            "            \"Where:\",",
            "            \"image_index: 0-based index of the image\",",
            "            \"label: brief description of what is located\",",
            "            \"left, top: x,y coordinates of top-left box corner (0-1 scale)\",",
            "            \"width, height: box dimensions as fraction of image size\",",
            "            \"Coordinates have origin (0,0) at top-left, (1,1) at bottom-right.\",",
            "            \"For other queries, I will respond conversationally to the best of my abilities.\",",
            "            prompt",
            "        ])",
            "        if debug == False:",
            "            debug = self.config.debug",
            "",
            "        if max_generation_size is None:",
            "            prompt_size = self.model.tokenize(prompt)",
            "            max_generation_size = self.model.config.ctx_size - len(prompt_size)",
            "",
            "        pr = PromptReshaper(prompt)",
            "        prompt = pr.build(placeholders,",
            "                        self.model.tokenize,",
            "                        self.model.detokenize,",
            "                        self.model.config.ctx_size - max_generation_size,",
            "                        sacrifice",
            "                        )",
            "        ntk = len(self.model.tokenize(prompt))",
            "        max_generation_size = min(self.model.config.ctx_size - ntk, max_generation_size)",
            "        # TODO : add show progress",
            "",
            "        gen = self.generate_with_images(prompt, images, max_generation_size, callback=callback, show_progress=show_progress).strip().replace(\"</s>\", \"\").replace(\"<s>\", \"\")",
            "        try:",
            "            gen = process_ai_output(gen, images, \"/discussions/\")",
            "        except Exception as ex:",
            "            pass",
            "        if debug:",
            "            self.print_prompt(\"prompt\", prompt+gen)",
            "",
            "        return gen",
            "",
            "    def fast_gen(",
            "                    self, ",
            "                    prompt: str, ",
            "                    max_generation_size: int=None, ",
            "                    placeholders: dict = {}, ",
            "                    sacrifice: list = [\"previous_discussion\"], ",
            "                    debug: bool  = False, ",
            "                    callback=None, ",
            "                    show_progress=False, ",
            "                    temperature = None, ",
            "                    top_k = None, ",
            "                    top_p=None, ",
            "                    repeat_penalty=None, ",
            "                    repeat_last_n=None",
            "                ) -> str:",
            "        \"\"\"",
            "        Fast way to generate code",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        if debug == False:",
            "            debug = self.config.debug",
            "",
            "        if max_generation_size is None:",
            "            prompt_size = self.model.tokenize(prompt)",
            "            max_generation_size = self.model.config.ctx_size - len(prompt_size)",
            "",
            "        pr = PromptReshaper(prompt)",
            "        prompt = pr.build(placeholders,",
            "                        self.model.tokenize,",
            "                        self.model.detokenize,",
            "                        self.model.config.ctx_size - max_generation_size,",
            "                        sacrifice",
            "                        )",
            "        ntk = len(self.model.tokenize(prompt))",
            "        max_generation_size = min(self.model.config.ctx_size - ntk, max_generation_size)",
            "        # TODO : add show progress",
            "",
            "        gen = self.generate(prompt, max_generation_size, temperature = temperature, top_k = top_k, top_p=top_p, repeat_penalty=repeat_penalty, repeat_last_n=repeat_last_n, callback=callback, show_progress=show_progress).strip().replace(\"</s>\", \"\").replace(\"<s>\", \"\")",
            "",
            "        return gen",
            "",
            "",
            "",
            "    def process(self, text:str, message_type:MSG_TYPE, callback=None, show_progress=False):",
            "        if callback is None:",
            "            callback = self.callback",
            "        if text is None:",
            "            return True",
            "        if message_type==MSG_TYPE.MSG_TYPE_CHUNK:",
            "            bot_says = self.bot_says + text",
            "        elif  message_type==MSG_TYPE.MSG_TYPE_FULL:",
            "            bot_says = text",
            "",
            "        if show_progress:",
            "            if self.nb_received_tokens==0:",
            "                self.start_time = datetime.now()",
            "            dt =(datetime.now() - self.start_time).seconds",
            "            if dt==0:",
            "                dt=1",
            "            spd = self.nb_received_tokens/dt",
            "            ASCIIColors.green(f\"Received {self.nb_received_tokens} tokens (speed: {spd:.2f}t/s)              \",end=\"\\r\",flush=True)",
            "            sys.stdout = sys.__stdout__",
            "            sys.stdout.flush()",
            "            self.nb_received_tokens+=1",
            "",
            "",
            "        antiprompt = self.detect_antiprompt(bot_says)",
            "        if antiprompt:",
            "            self.bot_says = remove_text_from_string(bot_says,antiprompt)",
            "            ASCIIColors.warning(f\"\\n{antiprompt} detected. Stopping generation\")",
            "            return False",
            "        else:",
            "            if callback:",
            "                callback(text,message_type)",
            "            self.bot_says = bot_says",
            "            return True",
            "",
            "    def generate_with_images(self, prompt, images, max_size, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False, show_progress=False ):",
            "        ASCIIColors.info(\"Text generation started: Warming up\")",
            "        self.nb_received_tokens = 0",
            "        self.bot_says = \"\"",
            "        if debug:",
            "            self.print_prompt(\"gen\",prompt)",
            "",
            "        self.model.generate_with_images(",
            "                                prompt,",
            "                                images,",
            "                                max_size,",
            "                                partial(self.process, callback=callback, show_progress=show_progress),",
            "                                temperature=self.model_temperature if temperature is None else temperature,",
            "                                top_k=self.model_top_k if top_k is None else top_k,",
            "                                top_p=self.model_top_p if top_p is None else top_p,",
            "                                repeat_penalty=self.model_repeat_penalty if repeat_penalty is None else repeat_penalty,",
            "                                repeat_last_n = self.model_repeat_last_n if repeat_last_n is None else repeat_last_n",
            "                                ).strip()",
            "        return self.bot_says",
            "",
            "    def generate(self, prompt, max_size = None, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False, show_progress=False ):",
            "        ASCIIColors.info(\"Text generation started: Warming up\")",
            "        self.nb_received_tokens = 0",
            "        self.bot_says = \"\"",
            "        if debug:",
            "            self.print_prompt(\"gen\",prompt)",
            "        self.model.generate(",
            "                                prompt,",
            "                                max_size if max_size else (self.config.ctx_size-len(self.model.tokenize(prompt))),",
            "                                partial(self.process, callback=callback, show_progress=show_progress),",
            "                                temperature=self.model_temperature if temperature is None else temperature,",
            "                                top_k=self.model_top_k if top_k is None else top_k,",
            "                                top_p=self.model_top_p if top_p is None else top_p,",
            "                                repeat_penalty=self.model_repeat_penalty if repeat_penalty is None else repeat_penalty,",
            "                                repeat_last_n = self.model_repeat_last_n if repeat_last_n is None else repeat_last_n,",
            "                                ).strip()",
            "        if debug:",
            "            self.print_prompt(\"prompt\", prompt+self.bot_says)",
            "        ",
            "        return self.bot_says",
            "",
            "    def setCallback(self, callback: Callable[[str, MSG_TYPE, dict, list], bool]):",
            "        self.callback = callback",
            "        if self._processor:",
            "            self._processor.callback = callback",
            "",
            "",
            "    def __str__(self):",
            "        return f\"{self.category}/{self.name}\"",
            "",
            "",
            "    def load_personality(self, package_path=None):",
            "        \"\"\"",
            "        Load personality parameters from a YAML configuration file.",
            "",
            "        Args:",
            "            package_path (str or Path): The path to the package directory.",
            "",
            "        Raises:",
            "            ValueError: If the configuration file does not exist.",
            "        \"\"\"",
            "        if package_path is None:",
            "            package_path = self.personality_package_path",
            "        else:",
            "            package_path = Path(package_path)",
            "",
            "        # Verify that there is at least a configuration file",
            "        config_file = package_path / \"config.yaml\"",
            "        if not config_file.exists():",
            "            raise ValueError(f\"The provided folder {package_path} does not exist.\")",
            "",
            "        with open(config_file, \"r\", encoding='utf-8') as f:",
            "            config = yaml.safe_load(f)",
            "",
            "        secret_file = package_path / \"secret.yaml\"",
            "        if secret_file.exists():",
            "            with open(secret_file, \"r\", encoding='utf-8') as f:",
            "                self._secret_cfg = yaml.safe_load(f)",
            "        else:",
            "            self._secret_cfg = None",
            "",
            "        languages = package_path / \"languages\"",
            "",
            "        if languages.exists():",
            "            self._supported_languages = []",
            "            for language in [l for l in languages.iterdir()]:",
            "                self._supported_languages.append(language.stem)",
            "",
            "            if self._selected_language is not None and self._selected_language in self._supported_languages:",
            "                config_file = languages / (self._selected_language+\".yaml\")",
            "                with open(config_file, \"r\", encoding='utf-8') as f:",
            "                    config = yaml.safe_load(f)",
            "",
            "",
            "",
            "        # Load parameters from the configuration file",
            "        self._version = config.get(\"version\", self._version)",
            "        self._author = config.get(\"author\", self._author)",
            "        self._name = config.get(\"name\", self._name)",
            "        self._user_name = config.get(\"user_name\", self._user_name)",
            "        self._category_desc = config.get(\"category\", self._category)",
            "        self._language = config.get(\"language\", self._language)",
            "",
            "        self._ignore_discussion_documents_rag = config.get(\"ignore_discussion_documents_rag\", self._ignore_discussion_documents_rag)",
            "",
            "",
            "        self._personality_description = config.get(\"personality_description\", self._personality_description)",
            "        self._personality_conditioning = config.get(\"personality_conditioning\", self._personality_conditioning)",
            "        self._welcome_message = config.get(\"welcome_message\", self._welcome_message)",
            "        self._include_welcome_message_in_discussion = config.get(\"include_welcome_message_in_discussion\", self._include_welcome_message_in_discussion)",
            "",
            "        self._user_message_prefix = config.get(\"user_message_prefix\", self._user_message_prefix)",
            "        self._link_text = config.get(\"link_text\", self._link_text)",
            "        self._ai_message_prefix = config.get(\"ai_message_prefix\", self._ai_message_prefix)",
            "        self._dependencies = config.get(\"dependencies\", self._dependencies)",
            "        self._disclaimer = config.get(\"disclaimer\", self._disclaimer)",
            "        self._help = config.get(\"help\", self._help)",
            "        self._commands = config.get(\"commands\", self._commands)",
            "        self._model_temperature = config.get(\"model_temperature\", self._model_temperature)",
            "        self._model_top_k = config.get(\"model_top_k\", self._model_top_k)",
            "        self._model_top_p = config.get(\"model_top_p\", self._model_top_p)",
            "        self._model_repeat_penalty = config.get(\"model_repeat_penalty\", self._model_repeat_penalty)",
            "        self._model_repeat_last_n = config.get(\"model_repeat_last_n\", self._model_repeat_last_n)",
            "",
            "        # Script parameters (for example keys to connect to search engine or any other usage)",
            "        self._processor_cfg = config.get(\"processor_cfg\", self._processor_cfg)",
            "",
            "",
            "        #set package path",
            "        self.personality_package_path = package_path",
            "",
            "        # Check for a logo file",
            "        self.logo_path = self.personality_package_path / \"assets\" / \"logo.png\"",
            "        if self.logo_path.is_file():",
            "            self._logo = Image.open(self.logo_path)",
            "",
            "        # Get the assets folder path",
            "        self.assets_path = self.personality_package_path / \"assets\"",
            "        # Get the scripts folder path",
            "        self.scripts_path = self.personality_package_path / \"scripts\"",
            "        # Get the languages folder path",
            "        self.languages_path = self.personality_package_path / \"languages\"",
            "        # Get the data folder path",
            "        self.data_path = self.personality_package_path / \"data\"",
            "        # Get the data folder path",
            "        self.audio_path = self.personality_package_path / \"audio\"",
            "        # Get the data folder path",
            "        self.welcome_audio_path = self.personality_package_path / \"welcome_audio\"",
            "",
            "",
            "        # If not exist recreate",
            "        self.assets_path.mkdir(parents=True, exist_ok=True)",
            "",
            "        # If not exist recreate",
            "        self.scripts_path.mkdir(parents=True, exist_ok=True)",
            "",
            "        # If not exist recreate",
            "        self.audio_path.mkdir(parents=True, exist_ok=True)",
            "",
            "        # samples",
            "        self.audio_samples = [f for f in self.audio_path.iterdir()]",
            "",
            "        # Verify if the persona has a data folder",
            "        if self.data_path.exists():",
            "            self.database_path = self.data_path / \"db.json\"",
            "            if self.database_path.exists():",
            "                ASCIIColors.info(\"Loading database ...\",end=\"\")",
            "                self.persona_data_vectorizer = TextVectorizer(",
            "                            \"tfidf_vectorizer\", # self.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\"",
            "                            model=self.model, #needed in case of using model_embedding",
            "                            save_db=True,",
            "                            database_path=self.database_path,",
            "                            data_visualization_method=VisualizationMethod.PCA,",
            "                            database_dict=None)",
            "                ASCIIColors.green(\"Ok\")",
            "            else:",
            "                files = [f for f in self.data_path.iterdir() if f.suffix.lower() in ['.asm', '.bat', '.c', '.cpp', '.cs', '.csproj', '.css',",
            "                    '.csv', '.docx', '.h', '.hh', '.hpp', '.html', '.inc', '.ini', '.java', '.js', '.json', '.log',",
            "                    '.lua', '.map', '.md', '.pas', '.pdf', '.php', '.pptx', '.ps1', '.py', '.rb', '.rtf', '.s', '.se', '.sh', '.sln',",
            "                    '.snippet', '.snippets', '.sql', '.sym', '.ts', '.txt', '.xlsx', '.xml', '.yaml', '.yml', '.msg'] ]",
            "                if len(files)>0:",
            "                    dl = GenericDataLoader()",
            "                    self.persona_data_vectorizer = TextVectorizer(",
            "                                \"tfidf_vectorizer\", # self.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\"",
            "                                model=self.model, #needed in case of using model_embedding",
            "                                save_db=True,",
            "                                database_path=self.database_path,",
            "                                data_visualization_method=VisualizationMethod.PCA,",
            "                                database_dict=None)",
            "                    for f in files:",
            "                        text = dl.read_file(f)",
            "                        self.persona_data_vectorizer.add_document(f.name,text,self.config.data_vectorization_chunk_size, self.config.data_vectorization_overlap_size)",
            "                        # data_vectorization_chunk_size: 512 # chunk size",
            "                        # data_vectorization_overlap_size: 128 # overlap between chunks size",
            "                        # data_vectorization_nb_chunks: 2 # number of chunks to use",
            "                    self.persona_data_vectorizer.index()",
            "                    self.persona_data_vectorizer.save_db()",
            "                else:",
            "                    self.persona_data_vectorizer = None",
            "                    self._data = None",
            "",
            "        else:",
            "            self.persona_data_vectorizer = None",
            "            self._data = None",
            "",
            "        self.personality_output_folder = self.lollms_paths.personal_outputs_path/self.name",
            "        self.personality_output_folder.mkdir(parents=True, exist_ok=True)",
            "",
            "",
            "        if self.run_scripts:",
            "            # Search for any processor code",
            "            processor_file_name = \"processor.py\"",
            "            self.processor_script_path = self.scripts_path / processor_file_name",
            "            if self.processor_script_path.exists():",
            "                module_name = processor_file_name[:-3]  # Remove the \".py\" extension",
            "                module_spec = importlib.util.spec_from_file_location(module_name, str(self.processor_script_path))",
            "                module = importlib.util.module_from_spec(module_spec)",
            "                module_spec.loader.exec_module(module)",
            "                if hasattr(module, \"Processor\"):",
            "                    self._processor = module.Processor(self, callback=self.callback)",
            "                else:",
            "                    self._processor = None",
            "            else:",
            "                self._processor = None",
            "        # Get a list of all files in the assets folder",
            "        contents = [str(file) for file in self.assets_path.iterdir() if file.is_file()]",
            "",
            "        self._assets_list = contents",
            "        return config",
            "",
            "",
            "",
            "    def remove_file(self, file_name, callback=None):",
            "        try:",
            "            if any(file_name == entry.name for entry in self.text_files):",
            "                fn = [entry for entry in self.text_files if entry.name == file_name][0]",
            "                self.text_files = [entry for entry in self.text_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "                if len(self.text_files)>0:",
            "                    try:",
            "                        self.vectorizer.remove_document(fn)",
            "                        if callback is not None:",
            "                            callback(\"File removed successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                        return True",
            "                    except ValueError as ve:",
            "                        ASCIIColors.error(f\"Couldn't remove the file\")",
            "                        return False",
            "                else:",
            "                    self.vectorizer = None",
            "            elif any(file_name == entry.name for entry in self.image_files):",
            "                fn = [entry for entry in self.image_files if entry.name == file_name][0]",
            "                self.text_files = [entry for entry in self.image_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "",
            "        except Exception as ex:",
            "            ASCIIColors.warning(f\"Couldn't remove the file {file_name}\")",
            "",
            "    def remove_all_files(self, callback=None):",
            "        for file in self.text_files:",
            "            try:",
            "                Path(file).unlink()",
            "            except Exception as ex:",
            "                ASCIIColors.warning(f\"Couldn't remove the file {file}\")",
            "        for file in self.image_files:",
            "            try:",
            "                Path(file).unlink()",
            "            except Exception as ex:",
            "                ASCIIColors.warning(f\"Couldn't remove the file {file}\")",
            "        self.text_files=[]",
            "        self.image_files=[]",
            "        self.vectorizer = None",
            "        return True",
            "",
            "    def add_file(self, path, client:Client, callback=None, process=True):",
            "        output = \"\"",
            "        if not self.callback:",
            "            self.callback = callback",
            "",
            "        path = Path(path)",
            "        if path.suffix in [\".wav\",\".mp3\"]:",
            "            self.audio_files.append(path)",
            "            if process:",
            "                self.new_message(\"\")",
            "                self.ShowBlockingMessage(f\"Transcribing ... \")",
            "                if self.app.stt is None:",
            "                    self.InfoMessage(\"No STT service is up.\\nPlease configure your default STT service in the settings page.\")",
            "                    return",
            "                text = self.app.stt.transcribe(str(path))",
            "                transcription_fn = str(path)+\".txt\"",
            "                with open(transcription_fn, \"w\", encoding=\"utf-8\") as f:",
            "                    f.write(text)",
            "",
            "                self.info(f\"File saved to {transcription_fn}\")",
            "                self.full(text)",
            "        elif path.suffix in [\".png\",\".jpg\",\".jpeg\",\".gif\",\".bmp\",\".svg\",\".webp\"]:",
            "            self.image_files.append(path)",
            "            if process:",
            "                if self.callback:",
            "                    try:",
            "                        pth = str(path).replace(\"\\\\\",\"/\").split('/')",
            "                        if \"discussion_databases\" in pth:",
            "                            pth = discussion_path_to_url(path)",
            "                            self.new_message(\"\",MSG_TYPE.MSG_TYPE_FULL)",
            "                            output = f'<img src=\"{pth}\" width=\"800\">\\n\\n'",
            "                            self.full(output)",
            "                            self.app.close_message(client.client_id if client is not None else 0)",
            "",
            "                        if self.model.binding_type not in [BindingType.TEXT_IMAGE, BindingType.TEXT_IMAGE_VIDEO]:",
            "                            # self.ShowBlockingMessage(\"Understanding image (please wait)\")",
            "                            from PIL import Image",
            "                            img = Image.open(str(path))",
            "                            # Convert the image to RGB mode",
            "                            img = img.convert(\"RGB\")",
            "                            output += \"## image description :\\n\"+ self.model.interrogate_blip([img])[0]",
            "                            # output += \"## image description :\\n\"+ self.model.qna_blip([img],\"q:Describe this photo with as much details as possible.\\na:\")[0]",
            "                            self.full(output)",
            "                            self.app.close_message(client.client_id if client is not None else 0)",
            "                            self.HideBlockingMessage(\"Understanding image (please wait)\")",
            "                            if self.config.debug:",
            "                                ASCIIColors.yellow(output)",
            "                        else:",
            "                            # self.ShowBlockingMessage(\"Importing image (please wait)\")",
            "                            self.HideBlockingMessage(\"Importing image (please wait)\")",
            "",
            "                    except Exception as ex:",
            "                        trace_exception(ex)",
            "                        self.HideBlockingMessage(\"Understanding image (please wait)\", False)",
            "                        ASCIIColors.error(\"Couldn't create new message\")",
            "            ASCIIColors.info(\"Received image file\")",
            "            if callback is not None:",
            "                callback(\"Image file added successfully\", MSG_TYPE.MSG_TYPE_INFO)",
            "        else:",
            "            try:",
            "                # self.ShowBlockingMessage(\"Adding file to vector store.\\nPlease stand by\")",
            "                self.text_files.append(path)",
            "                ASCIIColors.info(\"Received text compatible file\")",
            "                self.ShowBlockingMessage(\"Processing file\\nPlease wait ...\")",
            "                if process:",
            "                    if self.vectorizer is None:",
            "                        self.vectorizer = TextVectorizer(",
            "                                    self.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\"",
            "                                    model=self.model, #needed in case of using model_embedding",
            "                                    database_path=client.discussion.discussion_rag_folder/\"db.json\",",
            "                                    save_db=self.config.data_vectorization_save_db,",
            "                                    data_visualization_method=VisualizationMethod.PCA,",
            "                                    database_dict=None)",
            "                    data = GenericDataLoader.read_file(path)",
            "                    self.vectorizer.add_document(path, data, self.config.data_vectorization_chunk_size, self.config.data_vectorization_overlap_size, add_first_line_to_all_chunks=True if path.suffix==\".csv\" else False)",
            "                    self.vectorizer.index()",
            "                    if callback is not None:",
            "                        callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                    self.HideBlockingMessage(client.client_id)",
            "                    return True",
            "            except Exception as e:",
            "                trace_exception(e)",
            "                self.InfoMessage(f\"Unsupported file format or empty file.\\nSupported formats are {GenericDataLoader.get_supported_file_types()}\",client_id=client.client_id)",
            "                return False",
            "    def save_personality(self, package_path=None):",
            "        \"\"\"",
            "        Save the personality parameters to a YAML configuration file.",
            "",
            "        Args:",
            "            package_path (str or Path): The path to the package directory.",
            "        \"\"\"",
            "        if package_path is None:",
            "            package_path = self.personality_package_path",
            "        else:",
            "            package_path = Path(package_path)",
            "",
            "        # Building output path",
            "        config_file = package_path / \"config.yaml\"",
            "        assets_folder = package_path / \"assets\"",
            "",
            "        # Create assets folder if it doesn't exist",
            "        if not assets_folder.exists():",
            "            assets_folder.mkdir(exist_ok=True, parents=True)",
            "",
            "        # Create the configuration dictionary",
            "        config = {",
            "            \"author\": self._author,",
            "            \"version\": self._version,",
            "            \"name\": self._name,",
            "            \"user_name\": self._user_name,",
            "            \"category\": self._category,",
            "            \"language\": self._language,",
            "            \"supported_languages\": self._supported_languages,",
            "            \"selected_language\": self._selected_language,",
            "            \"ignore_discussion_documents_rag\": self._ignore_discussion_documents_rag,",
            "            \"personality_description\": self._personality_description,",
            "            \"personality_conditioning\": self._personality_conditioning,",
            "            \"welcome_message\": self._welcome_message,",
            "            \"include_welcome_message_in_discussion\": self._include_welcome_message_in_discussion,",
            "            \"user_message_prefix\": self._user_message_prefix,",
            "            \"link_text\": self._link_text,",
            "            \"ai_message_prefix\": self._ai_message_prefix,",
            "            \"dependencies\": self._dependencies,",
            "            \"disclaimer\": self._disclaimer,",
            "            \"help\": self._help,",
            "            \"commands\": self._commands,",
            "            \"model_temperature\": self._model_temperature,",
            "            \"model_top_k\": self._model_top_k,",
            "            \"model_top_p\": self._model_top_p,",
            "            \"model_repeat_penalty\": self._model_repeat_penalty,",
            "            \"model_repeat_last_n\": self._model_repeat_last_n",
            "        }",
            "",
            "        # Save the configuration to the YAML file",
            "        with open(config_file, \"w\") as f:",
            "            yaml.dump(config, f)",
            "",
            "",
            "",
            "    def as_dict(self):",
            "        \"\"\"",
            "        Convert the personality parameters to a dictionary.",
            "",
            "        Returns:",
            "            dict: The personality parameters as a dictionary.",
            "        \"\"\"",
            "        return {",
            "            \"author\": self._author,",
            "            \"version\": self._version,",
            "            \"name\": self._name,",
            "            \"user_name\": self._user_name,",
            "            \"category\": self._category,",
            "            \"language\": self._language,",
            "            \"supported_languages\": self._supported_languages,",
            "            \"selected_language\": self._selected_language,",
            "            \"ignore_discussion_documents_rag\": self._ignore_discussion_documents_rag,",
            "            \"personality_description\": self._personality_description,",
            "            \"personality_conditioning\": self._personality_conditioning,",
            "            \"welcome_message\": self._welcome_message,",
            "            \"include_welcome_message_in_discussion\": self._include_welcome_message_in_discussion,",
            "            \"user_message_prefix\": self._user_message_prefix,",
            "            \"link_text\": self._link_text,",
            "            \"ai_message_prefix\": self._ai_message_prefix,",
            "            \"dependencies\": self._dependencies,",
            "            \"disclaimer\": self._disclaimer,",
            "            \"help\": self._help,",
            "            \"commands\": self._commands,",
            "            \"model_temperature\": self._model_temperature,",
            "            \"model_top_k\": self._model_top_k,",
            "            \"model_top_p\": self._model_top_p,",
            "            \"model_repeat_penalty\": self._model_repeat_penalty,",
            "            \"model_repeat_last_n\": self._model_repeat_last_n,",
            "            \"assets_list\":self._assets_list",
            "        }",
            "",
            "    # ========================================== Properties ===========================================",
            "    @property",
            "    def conditionning_commands(self):",
            "        return {",
            "            \"date_time\": datetime.now().strftime(\"%A, %B %d, %Y %I:%M:%S %p\"), # Replaces {{date}} with actual date",
            "            \"date\": datetime.now().strftime(\"%A, %B %d, %Y\"), # Replaces {{date}} with actual date",
            "            \"time\": datetime.now().strftime(\"%H:%M:%S\"), # Replaces {{time}} with actual time",
            "        }",
            "",
            "    @property",
            "    def logo(self):",
            "        \"\"\"",
            "        Get the personality logo.",
            "",
            "        Returns:",
            "        PIL.Image.Image: The personality logo as a Pillow Image object.",
            "        \"\"\"",
            "        if hasattr(self, '_logo'):",
            "            return self._logo",
            "        else:",
            "            return None",
            "    @property",
            "    def version(self):",
            "        \"\"\"Get the version of the package.\"\"\"",
            "        return self._version",
            "",
            "    @version.setter",
            "    def version(self, value):",
            "        \"\"\"Set the version of the package.\"\"\"",
            "        self._version = value",
            "",
            "    @property",
            "    def author(self):",
            "        \"\"\"Get the author of the package.\"\"\"",
            "        return self._author",
            "",
            "    @author.setter",
            "    def author(self, value):",
            "        \"\"\"Set the author of the package.\"\"\"",
            "        self._author = value",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        \"\"\"Get the name.\"\"\"",
            "        return self._name",
            "",
            "    @name.setter",
            "    def name(self, value: str):",
            "        \"\"\"Set the name.\"\"\"",
            "        self._name = value",
            "",
            "    @property",
            "    def user_name(self) -> str:",
            "        \"\"\"Get the user name.\"\"\"",
            "        return self._user_name",
            "",
            "    @user_name.setter",
            "    def user_name(self, value: str):",
            "        \"\"\"Set the user name.\"\"\"",
            "        self._user_name = value",
            "",
            "",
            "    @property",
            "    def language(self) -> str:",
            "        \"\"\"Get the language.\"\"\"",
            "        return self._language",
            "",
            "    @property",
            "    def category(self) -> str:",
            "        \"\"\"Get the category.\"\"\"",
            "        return self._category",
            "",
            "    @property",
            "    def category_desc(self) -> str:",
            "        \"\"\"Get the category.\"\"\"",
            "        return self._category_desc",
            "",
            "    @language.setter",
            "    def language(self, value: str):",
            "        \"\"\"Set the language.\"\"\"",
            "        self._language = value",
            "",
            "    @category.setter",
            "    def category(self, value: str):",
            "        \"\"\"Set the category.\"\"\"",
            "        self._category = value",
            "",
            "    @category_desc.setter",
            "    def category_desc(self, value: str):",
            "        \"\"\"Set the category.\"\"\"",
            "        self._category_desc = value",
            "",
            "",
            "    @property",
            "    def supported_languages(self) -> str:",
            "        \"\"\"Get the supported_languages.\"\"\"",
            "        return self._supported_languages",
            "",
            "    @supported_languages.setter",
            "    def supported_languages(self, value: str):",
            "        \"\"\"Set the supported_languages.\"\"\"",
            "        self._supported_languages = value",
            "",
            "",
            "    @property",
            "    def selected_language(self) -> str:",
            "        \"\"\"Get the selected_language.\"\"\"",
            "        return self._selected_language",
            "",
            "    @selected_language.setter",
            "    def selected_language(self, value: str):",
            "        \"\"\"Set the selected_language.\"\"\"",
            "        self._selected_language = value",
            "",
            "    @property",
            "    def ignore_discussion_documents_rag(self) -> str:",
            "        \"\"\"Get the ignore_discussion_documents_rag.\"\"\"",
            "        return self._ignore_discussion_documents_rag",
            "",
            "    @ignore_discussion_documents_rag.setter",
            "    def ignore_discussion_documents_rag(self, value: str):",
            "        \"\"\"Set the ignore_discussion_documents_rag.\"\"\"",
            "        self._ignore_discussion_documents_rag = value",
            "",
            "",
            "    @property",
            "    def personality_description(self) -> str:",
            "        \"\"\"",
            "        Getter for the personality description.",
            "",
            "        Returns:",
            "            str: The personality description of the AI assistant.",
            "        \"\"\"",
            "        return self._personality_description",
            "",
            "    @personality_description.setter",
            "    def personality_description(self, description: str):",
            "        \"\"\"",
            "        Setter for the personality description.",
            "",
            "        Args:",
            "            description (str): The new personality description for the AI assistant.",
            "        \"\"\"",
            "        self._personality_description = description",
            "",
            "    @property",
            "    def personality_conditioning(self) -> str:",
            "        \"\"\"",
            "        Getter for the personality conditioning.",
            "",
            "        Returns:",
            "            str: The personality conditioning of the AI assistant.",
            "        \"\"\"",
            "        return self.replace_keys(self._personality_conditioning, self.conditionning_commands)",
            "",
            "    @personality_conditioning.setter",
            "    def personality_conditioning(self, conditioning: str):",
            "        \"\"\"",
            "        Setter for the personality conditioning.",
            "",
            "        Args:",
            "            conditioning (str): The new personality conditioning for the AI assistant.",
            "        \"\"\"",
            "        self._personality_conditioning = conditioning",
            "",
            "    @property",
            "    def welcome_message(self) -> str:",
            "        \"\"\"",
            "        Getter for the welcome message.",
            "",
            "        Returns:",
            "            str: The welcome message of the AI assistant.",
            "        \"\"\"",
            "        return self.replace_keys(self._welcome_message, self.conditionning_commands)",
            "",
            "    @welcome_message.setter",
            "    def welcome_message(self, message: str):",
            "        \"\"\"",
            "        Setter for the welcome message.",
            "",
            "        Args:",
            "            message (str): The new welcome message for the AI assistant.",
            "        \"\"\"",
            "        self._welcome_message = message",
            "",
            "    @property",
            "    def include_welcome_message_in_discussion(self) -> bool:",
            "        \"\"\"",
            "        Getter for the include welcome message in disucssion.",
            "",
            "        Returns:",
            "            bool: whether to add the welcome message to tje discussion or not.",
            "        \"\"\"",
            "        return self._include_welcome_message_in_discussion",
            "",
            "    @include_welcome_message_in_discussion.setter",
            "    def include_welcome_message_in_discussion(self, message: bool):",
            "        \"\"\"",
            "        Setter for the welcome message.",
            "",
            "        Args:",
            "            message (str): The new welcome message for the AI assistant.",
            "        \"\"\"",
            "        self._include_welcome_message_in_discussion = message",
            "",
            "",
            "    @property",
            "    def user_message_prefix(self) -> str:",
            "        \"\"\"",
            "        Getter for the user message prefix.",
            "",
            "        Returns:",
            "            str: The user message prefix of the AI assistant.",
            "        \"\"\"",
            "        return self._user_message_prefix",
            "",
            "    @user_message_prefix.setter",
            "    def user_message_prefix(self, prefix: str):",
            "        \"\"\"",
            "        Setter for the user message prefix.",
            "",
            "        Args:",
            "            prefix (str): The new user message prefix for the AI assistant.",
            "        \"\"\"",
            "        self._user_message_prefix = prefix",
            "",
            "    @property",
            "    def link_text(self) -> str:",
            "        \"\"\"",
            "        Getter for the link text.",
            "",
            "        Returns:",
            "            str: The link text of the AI assistant.",
            "        \"\"\"",
            "        return self._link_text",
            "",
            "    @link_text.setter",
            "    def link_text(self, text: str):",
            "        \"\"\"",
            "        Setter for the link text.",
            "",
            "        Args:",
            "            text (str): The new link text for the AI assistant.",
            "        \"\"\"",
            "        self._link_text = text",
            "    @property",
            "    def ai_message_prefix(self):",
            "        \"\"\"",
            "        Get the AI message prefix.",
            "",
            "        Returns:",
            "            str: The AI message prefix.",
            "        \"\"\"",
            "        return self._ai_message_prefix",
            "",
            "    @ai_message_prefix.setter",
            "    def ai_message_prefix(self, prefix):",
            "        \"\"\"",
            "        Set the AI message prefix.",
            "",
            "        Args:",
            "            prefix (str): The AI message prefix to set.",
            "        \"\"\"",
            "        self._ai_message_prefix = prefix",
            "",
            "    @property",
            "    def dependencies(self) -> List[str]:",
            "        \"\"\"Getter method for the dependencies attribute.",
            "",
            "        Returns:",
            "            List[str]: The list of dependencies.",
            "        \"\"\"",
            "        return self._dependencies",
            "",
            "    @dependencies.setter",
            "    def dependencies(self, dependencies: List[str]):",
            "        \"\"\"Setter method for the dependencies attribute.",
            "",
            "        Args:",
            "            dependencies (List[str]): The list of dependencies.",
            "        \"\"\"",
            "        self._dependencies = dependencies",
            "",
            "    @property",
            "    def disclaimer(self) -> str:",
            "        \"\"\"Getter method for the disclaimer attribute.",
            "",
            "        Returns:",
            "            str: The disclaimer text.",
            "        \"\"\"",
            "        return self._disclaimer",
            "",
            "    @disclaimer.setter",
            "    def disclaimer(self, disclaimer: str):",
            "        \"\"\"Setter method for the disclaimer attribute.",
            "",
            "        Args:",
            "            disclaimer (str): The disclaimer text.",
            "        \"\"\"",
            "        self._disclaimer = disclaimer",
            "",
            "    @property",
            "    def help(self) -> str:",
            "        \"\"\"Getter method for the help attribute.",
            "",
            "        Returns:",
            "            str: The help text.",
            "        \"\"\"",
            "        return self._help",
            "",
            "    @help.setter",
            "    def help(self, help: str):",
            "        \"\"\"Setter method for the help attribute.",
            "",
            "        Args:",
            "            help (str): The help text.",
            "        \"\"\"",
            "        self._help = help",
            "",
            "",
            "",
            "    @property",
            "    def commands(self) -> str:",
            "        \"\"\"Getter method for the commands attribute.",
            "",
            "        Returns:",
            "            str: The commands text.",
            "        \"\"\"",
            "        return self._commands",
            "",
            "    @commands.setter",
            "    def commands(self, commands: str):",
            "        \"\"\"Setter method for the commands attribute.",
            "",
            "        Args:",
            "            commands (str): The commands text.",
            "        \"\"\"",
            "        self._commands = commands",
            "",
            "",
            "    @property",
            "    def model_temperature(self) -> float:",
            "        \"\"\"Get the model's temperature.\"\"\"",
            "        return self._model_temperature",
            "",
            "    @model_temperature.setter",
            "    def model_temperature(self, value: float):",
            "        \"\"\"Set the model's temperature.",
            "",
            "        Args:",
            "            value (float): The new temperature value.",
            "        \"\"\"",
            "        self._model_temperature = value",
            "",
            "    @property",
            "    def model_top_k(self) -> int:",
            "        \"\"\"Get the model's top-k value.\"\"\"",
            "        return self._model_top_k",
            "",
            "    @model_top_k.setter",
            "    def model_top_k(self, value: int):",
            "        \"\"\"Set the model's top-k value.",
            "",
            "        Args:",
            "            value (int): The new top-k value.",
            "        \"\"\"",
            "        self._model_top_k = value",
            "",
            "    @property",
            "    def model_top_p(self) -> float:",
            "        \"\"\"Get the model's top-p value.\"\"\"",
            "        return self._model_top_p",
            "",
            "    @model_top_p.setter",
            "    def model_top_p(self, value: float):",
            "        \"\"\"Set the model's top-p value.",
            "",
            "        Args:",
            "            value (float): The new top-p value.",
            "        \"\"\"",
            "        self._model_top_p = value",
            "",
            "    @property",
            "    def model_repeat_penalty(self) -> float:",
            "        \"\"\"Get the model's repeat penalty value.\"\"\"",
            "        return self._model_repeat_penalty",
            "",
            "    @model_repeat_penalty.setter",
            "    def model_repeat_penalty(self, value: float):",
            "        \"\"\"Set the model's repeat penalty value.",
            "",
            "        Args:",
            "            value (float): The new repeat penalty value.",
            "        \"\"\"",
            "        self._model_repeat_penalty = value",
            "",
            "    @property",
            "    def model_repeat_last_n(self) -> int:",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._model_repeat_last_n",
            "",
            "    @model_repeat_last_n.setter",
            "    def model_repeat_last_n(self, value: int):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._model_repeat_last_n = value",
            "",
            "",
            "    @property",
            "    def assets_list(self) -> list:",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._assets_list",
            "",
            "    @assets_list.setter",
            "    def assets_list(self, value: list):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._assets_list = value",
            "",
            "    @property",
            "    def processor(self) -> 'APScript':",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._processor",
            "",
            "    @processor.setter",
            "    def processor(self, value: 'APScript'):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._processor = value",
            "",
            "",
            "    @property",
            "    def processor_cfg(self) -> list:",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._processor_cfg",
            "",
            "    @processor_cfg.setter",
            "    def processor_cfg(self, value: dict):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._processor_cfg = value",
            "",
            "    # ========================================== Helper methods ==========================================",
            "    def detect_antiprompt(self, text:str) -> bool:",
            "        \"\"\"",
            "        Detects if any of the antiprompts in self.anti_prompts are present in the given text.",
            "        Used for the Hallucination suppression system",
            "",
            "        Args:",
            "            text (str): The text to check for antiprompts.",
            "",
            "        Returns:",
            "            bool: True if any antiprompt is found in the text (ignoring case), False otherwise.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        separator_template          = self.config.separator_template",
            "",
            "        anti_prompts = [start_header_id_template, self.app.config.discussion_prompt_separator]",
            "        if self.app.config.separator_template!=\"\\n\":",
            "            anti_prompts.append(self.app.config.separator_template)",
            "",
            "        for prompt in anti_prompts:",
            "            if prompt.lower() in text.lower():",
            "                return prompt.lower()",
            "        return None",
            "",
            "",
            "    # Helper functions",
            "    @staticmethod",
            "    def replace_keys(input_string, replacements):",
            "        \"\"\"",
            "        Replaces all occurrences of keys in the input string with their corresponding",
            "        values from the replacements dictionary.",
            "",
            "        Args:",
            "            input_string (str): The input string to replace keys in.",
            "            replacements (dict): A dictionary of key-value pairs, where the key is the",
            "                string to be replaced and the value is the replacement string.",
            "",
            "        Returns:",
            "            str: The input string with all occurrences of keys replaced by their",
            "                corresponding values.",
            "        \"\"\"",
            "        pattern = r\"\\{\\{(\\w+)\\}\\}\"",
            "        # The pattern matches \"{{key}}\" and captures \"key\" in a group.",
            "        # The \"\\w+\" matches one or more word characters (letters, digits, or underscore).",
            "",
            "        def replace(match):",
            "            key = match.group(1)",
            "            return replacements.get(key, match.group(0))",
            "",
            "        output_string = re.sub(pattern, replace, input_string)",
            "        return output_string",
            "",
            "",
            "",
            "class StateMachine:",
            "    def __init__(self, states_list):",
            "        \"\"\"",
            "        states structure is the following",
            "        [",
            "            {",
            "                \"name\": the state name,",
            "                \"commands\": [ # list of commands",
            "                    \"command\": function",
            "                ],",
            "                \"default\": default function",
            "            }",
            "        ]",
            "        \"\"\"",
            "        self.states_list = states_list",
            "        self.current_state_id = 0",
            "        self.callback = None",
            "",
            "    def goto_state(self, state):",
            "        \"\"\"",
            "        Transition to the state with the given name or index.",
            "",
            "        Args:",
            "            state (str or int): The name or index of the state to transition to.",
            "",
            "        Raises:",
            "            ValueError: If no state is found with the given name or index.",
            "        \"\"\"",
            "        if isinstance(state, str):",
            "            for i, state_dict in enumerate(self.states_list):",
            "                if state_dict[\"name\"] == state:",
            "                    self.current_state_id = i",
            "                    return",
            "        elif isinstance(state, int):",
            "            if 0 <= state < len(self.states_list):",
            "                self.current_state_id = state",
            "                return",
            "        raise ValueError(f\"No state found with name or index: {state}\")",
            "",
            "",
            "",
            "    def process_state(self, command, full_context, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None, context_state:dict=None, client:Client=None):",
            "        \"\"\"",
            "        Process the given command based on the current state.",
            "",
            "        Args:",
            "            command: The command to process.",
            "",
            "        Raises:",
            "            ValueError: If the current state doesn't have the command and no default function is defined.",
            "        \"\"\"",
            "        if callback:",
            "            self.callback=callback",
            "",
            "        current_state = self.states_list[self.current_state_id]",
            "        commands = current_state[\"commands\"]",
            "        command = command.strip()",
            "",
            "        for cmd, func in commands.items():",
            "            if cmd == command[0:len(cmd)]:",
            "                try:",
            "                    func(command, full_context, callback, context_state, client)",
            "                except:# retrocompatibility",
            "                    func(command, full_context)",
            "                return",
            "",
            "        default_func = current_state.get(\"default\")",
            "        if default_func is not None:",
            "            default_func(command, full_context, callback, context_state, client)",
            "        else:",
            "            raise ValueError(f\"Command '{command}' not found in current state and no default function defined.\")",
            "",
            "",
            "class LoLLMsActionParameters:",
            "    def __init__(self, name: str, parameter_type: Type, range: Optional[List] = None, options: Optional[List] = None, value: Any = None) -> None:",
            "        self.name = name",
            "        self.parameter_type = parameter_type",
            "        self.range = range",
            "        self.options = options",
            "        self.value = value",
            "",
            "    def __str__(self) -> str:",
            "        parameter_dict = {",
            "            'name': self.name,",
            "            'parameter_type': self.parameter_type.__name__,",
            "            'value': self.value",
            "        }",
            "        if self.range is not None:",
            "            parameter_dict['range'] = self.range",
            "        if self.options is not None:",
            "            parameter_dict['options'] = self.options",
            "        return json.dumps(parameter_dict, indent=4)",
            "",
            "    @staticmethod",
            "    def from_str(string: str) -> 'LoLLMsActionParameters':",
            "        parameter_dict = json.loads(string)",
            "        name = parameter_dict['name']",
            "        parameter_type = eval(parameter_dict['parameter_type'])",
            "        range = parameter_dict.get('range', None)",
            "        options = parameter_dict.get('options', None)",
            "        value = parameter_dict['value']",
            "        return LoLLMsActionParameters(name, parameter_type, range, options, value)",
            "",
            "    @staticmethod",
            "    def from_dict(parameter_dict: dict) -> 'LoLLMsActionParameters':",
            "        name = parameter_dict['name']",
            "        parameter_type = eval(parameter_dict['parameter_type'])",
            "        range = parameter_dict.get('range', None)",
            "        options = parameter_dict.get('options', None)",
            "        value = parameter_dict['value']",
            "        return LoLLMsActionParameters(name, parameter_type, range, options, value)",
            "",
            "",
            "class LoLLMsActionParametersEncoder(json.JSONEncoder):",
            "    def default(self, obj):",
            "        if isinstance(obj, LoLLMsActionParameters):",
            "            parameter_dict = {",
            "                'name': obj.name,",
            "                'parameter_type': obj.parameter_type.__name__,",
            "                'value': obj.value",
            "            }",
            "            if obj.range is not None:",
            "                parameter_dict['range'] = obj.range",
            "            if obj.options is not None:",
            "                parameter_dict['options'] = obj.options",
            "            return parameter_dict",
            "        return super().default(obj)",
            "",
            "class LoLLMsAction:",
            "    def __init__(self, name, parameters: List[LoLLMsActionParameters], callback: Callable, description:str=\"\") -> None:",
            "        self.name           = name",
            "        self.parameters     = parameters",
            "        self.callback       = callback",
            "        self.description    = description",
            "",
            "    def __str__(self) -> str:",
            "        action_dict = {",
            "            'name': self.name,",
            "            'parameters': self.parameters,",
            "            'description': self.description",
            "        }",
            "        return json.dumps(action_dict, indent=4, cls=LoLLMsActionParametersEncoder)",
            "",
            "    @staticmethod",
            "    def from_str(string: str) -> 'LoLLMsAction':",
            "        action_dict = json.loads(string)",
            "        name = action_dict['name']",
            "        parameters = [LoLLMsActionParameters.from_dict(param_str) for param_str in action_dict['parameters']]",
            "        return LoLLMsAction(name, parameters, None)",
            "",
            "    @staticmethod",
            "    def from_dict(action_dict: dict) -> 'LoLLMsAction':",
            "        name = action_dict['name']",
            "        parameters = [LoLLMsActionParameters.from_dict(param_str) for param_str in action_dict['parameters']]",
            "        return LoLLMsAction(name, parameters, None)",
            "",
            "",
            "    def run(self) -> None:",
            "        args = {param.name: param.value for param in self.parameters}",
            "        self.callback(**args)",
            "",
            "def generate_actions(potential_actions: List[LoLLMsAction], parsed_text: dict) -> List[LoLLMsAction]:",
            "    actions = []",
            "    try:",
            "        for action_data in parsed_text[\"actions\"]:",
            "            name = action_data['name']",
            "            parameters = action_data['parameters']",
            "            matching_action = next((action for action in potential_actions if action.name == name), None)",
            "            if matching_action:",
            "                action = LoLLMsAction.from_str(str(matching_action))",
            "                action.callback = matching_action.callback",
            "                if type(parameters)==dict:",
            "                    for param_name, param_value in parameters.items():",
            "                        matching_param = next((param for param in action.parameters if param.name == param_name), None)",
            "                        if matching_param:",
            "                            matching_param.value = param_value",
            "                else:",
            "                    for param in parameters:",
            "                        if \"name\" in param:",
            "                            param_name = param[\"name\"]",
            "                            param_value = param[\"value\"]",
            "                        else:",
            "                            param_name = list(param.keys())[0]",
            "                            param_value = param[param_name]",
            "                        matching_param = next((param for param in action.parameters if param.name == param_name), None)",
            "                        if matching_param:",
            "                            matching_param.value = param_value",
            "                actions.append(action)",
            "    except json.JSONDecodeError:",
            "        print(\"Invalid JSON format.\")",
            "    return actions",
            "",
            "class APScript(StateMachine):",
            "    \"\"\"",
            "    Template class for implementing personality processor classes in the APScript framework.",
            "",
            "    This class provides a basic structure and placeholder methods for processing model inputs and outputs.",
            "    Personality-specific processor classes should inherit from this class and override the necessary methods.",
            "    \"\"\"",
            "    def __init__(",
            "                    self,",
            "                    personality         :AIPersonality,",
            "                    personality_config  :TypedConfig,",
            "                    states_list         :dict   = {},",
            "                    callback            = None",
            "                ) -> None:",
            "        super().__init__(states_list)",
            "        self.function_definitions               = [] # New! useful for 3rd gen personalities ",
            "        self.notify                             = personality.app.notify",
            "",
            "        self.personality                        = personality",
            "        self.config                             = personality.config",
            "        self.personality_config                 = personality_config",
            "        self.installation_option                = personality.installation_option",
            "        self.configuration_file_path            = self.personality.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.personality_folder_name/f\"config.yaml\"",
            "        self.configuration_file_path.parent.mkdir(parents=True, exist_ok=True)",
            "",
            "        self.personality_config.config.file_path    = self.configuration_file_path",
            "",
            "        self.callback = callback",
            "",
            "        # Installation",
            "        if (not self.configuration_file_path.exists() or self.installation_option==InstallOption.FORCE_INSTALL) and self.installation_option!=InstallOption.NEVER_INSTALL:",
            "            self.install()",
            "            self.personality_config.config.save_config()",
            "        else:",
            "            self.load_personality_config()",
            "",
            "    def sink(self, s=None,i=None,d=None):",
            "        pass",
            "",
            "    def settings_updated(self):",
            "        \"\"\"",
            "        To be implemented by the processor when the settings have changed",
            "        \"\"\"",
            "        pass",
            "",
            "    def mounted(self):",
            "        \"\"\"",
            "        triggered when mounted",
            "        \"\"\"",
            "        pass",
            "",
            "    def get_welcome(self, welcome_message:str, client:Client):",
            "        \"\"\"",
            "        triggered when a new conversation is created",
            "        \"\"\"",
            "        return welcome_message",
            "        ",
            "    def selected(self):",
            "        \"\"\"",
            "        triggered when mounted",
            "        \"\"\"",
            "        pass",
            "",
            "    def execute_command(self, command: str, parameters:list=[], client:Client=None):",
            "        \"\"\"",
            "        Recovers user commands and executes them. Each personality can define a set of commands that they can receive and execute",
            "        Args:",
            "            command: The command name",
            "            parameters: A list of the command parameters",
            "",
            "        \"\"\"",
            "        try:",
            "            self.process_state(command, \"\", self.callback, client)",
            "        except Exception as ex:",
            "            trace_exception(ex)",
            "            self.warning(f\"Couldn't execute command {command}\")",
            "",
            "    async def handle_request(self, request: Request) -> Dict[str, Any]:",
            "        \"\"\"",
            "        Handle client requests.",
            "",
            "        Args:",
            "            data (dict): A dictionary containing the request data.",
            "",
            "        Returns:",
            "            dict: A dictionary containing the response, including at least a \"status\" key.",
            "",
            "        This method should be implemented by a class that inherits from this one.",
            "",
            "        Example usage:",
            "        ```",
            "        handler = YourHandlerClass()",
            "        request_data = {\"command\": \"some_command\", \"parameters\": {...}}",
            "        response = await handler.handle_request(request_data)",
            "        ```",
            "        \"\"\"",
            "        return {\"status\":True}",
            "",
            "",
            "    def load_personality_config(self):",
            "        \"\"\"",
            "        Load the content of local_config.yaml file.",
            "",
            "        The function reads the content of the local_config.yaml file and returns it as a Python dictionary.",
            "",
            "        Args:",
            "            None",
            "",
            "        Returns:",
            "            dict: A dictionary containing the loaded data from the local_config.yaml file.",
            "        \"\"\"",
            "        try:",
            "            self.personality_config.config.load_config()",
            "        except:",
            "            self.personality_config.config.save_config()",
            "        self.personality_config.sync()",
            "",
            "    def install(self):",
            "        \"\"\"",
            "        Installation procedure (to be implemented)",
            "        \"\"\"",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "        ASCIIColors.red(f\"Installing {self.personality.personality_folder_name}\")",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "",
            "",
            "    def uninstall(self):",
            "        \"\"\"",
            "        Installation procedure (to be implemented)",
            "        \"\"\"",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "        ASCIIColors.red(f\"Uninstalling {self.personality.personality_folder_name}\")",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "",
            "",
            "    def add_file(self, path, client:Client, callback=None, process=True):",
            "        self.personality.add_file(path, client=client,callback=callback, process=process)",
            "        if callback is not None:",
            "            callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "        return True",
            "",
            "    def remove_file(self, path):",
            "        if path in self.personality.text_files:",
            "            self.personality.text_files.remove(path)",
            "        elif path in self.personality.image_files:",
            "            self.personality.image_files.remove(path)",
            "",
            "",
            "    def load_config_file(self, path, default_config=None):",
            "        \"\"\"",
            "        Load the content of local_config.yaml file.",
            "",
            "        The function reads the content of the local_config.yaml file and returns it as a Python dictionary.",
            "        If a default_config is provided, it fills any missing entries in the loaded dictionary.",
            "        If at least one field from default configuration was not present in the loaded configuration, the updated",
            "        configuration is saved.",
            "",
            "        Args:",
            "            path (str): The path to the local_config.yaml file.",
            "            default_config (dict, optional): A dictionary with default values to fill missing entries.",
            "",
            "        Returns:",
            "            dict: A dictionary containing the loaded data from the local_config.yaml file, with missing entries filled",
            "            by default_config if provided.",
            "        \"\"\"",
            "        with open(path, 'r') as file:",
            "            data = yaml.safe_load(file)",
            "",
            "        if default_config:",
            "            updated = False",
            "            for key, value in default_config.items():",
            "                if key not in data:",
            "                    data[key] = value",
            "                    updated = True",
            "",
            "            if updated:",
            "                self.save_config_file(path, data)",
            "",
            "        return data",
            "",
            "    def save_config_file(self, path, data):",
            "        \"\"\"",
            "        Save the configuration data to a local_config.yaml file.",
            "",
            "        Args:",
            "            path (str): The path to save the local_config.yaml file.",
            "            data (dict): The configuration data to be saved.",
            "",
            "        Returns:",
            "            None",
            "        \"\"\"",
            "        with open(path, 'w') as file:",
            "            yaml.dump(data, file)",
            "",
            "    def generate_with_images(self, prompt, images, max_size = None, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False ):",
            "        return self.personality.generate_with_images(prompt, images, max_size, temperature, top_k, top_p, repeat_penalty, repeat_last_n, callback, debug=debug)",
            "",
            "    def generate(self, prompt, max_size = None, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False ):",
            "        return self.personality.generate(prompt, max_size, temperature, top_k, top_p, repeat_penalty, repeat_last_n, callback, debug=debug)",
            "",
            "",
            "    def run_workflow(self, prompt:str, previous_discussion_text:str=\"\", callback: Callable[[str, MSG_TYPE, dict, list], bool]=None, context_details:dict=None, client:Client=None):",
            "        \"\"\"",
            "        This function generates code based on the given parameters.",
            "",
            "        Args:",
            "            full_prompt (str): The full prompt for code generation.",
            "            prompt (str): The prompt for code generation.",
            "            context_details (dict): A dictionary containing the following context details for code generation:",
            "                - conditionning (str): The conditioning information.",
            "                - documentation (str): The documentation information.",
            "                - knowledge (str): The knowledge information.",
            "                - user_description (str): The user description information.",
            "                - discussion_messages (str): The discussion messages information.",
            "                - positive_boost (str): The positive boost information.",
            "                - negative_boost (str): The negative boost information.",
            "                - current_language (str): The force language information.",
            "                - fun_mode (str): The fun mode conditionning text",
            "                - ai_prefix (str): The AI prefix information.",
            "            n_predict (int): The number of predictions to generate.",
            "            client_id: The client ID for code generation.",
            "            callback (function, optional): The callback function for code generation.",
            "",
            "        Returns:",
            "            None",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "",
            "    # ================================================= Advanced methods ===========================================",
            "    def compile_latex(self, file_path, pdf_latex_path=None):",
            "        try:",
            "            # Determine the pdflatex command based on the provided or default path",
            "            if pdf_latex_path:",
            "                pdflatex_command = pdf_latex_path",
            "            else:",
            "                pdflatex_command = self.personality.config.pdf_latex_path if self.personality.config.pdf_latex_path is not None else 'pdflatex'",
            "",
            "            # Set the execution path to the folder containing the tmp_file",
            "            execution_path = file_path.parent",
            "            # Run the pdflatex command with the file path",
            "            result = subprocess.run([pdflatex_command, \"-interaction=nonstopmode\", file_path], check=True, capture_output=True, text=True, cwd=execution_path)",
            "            # Check the return code of the pdflatex command",
            "            if result.returncode != 0:",
            "                error_message = result.stderr.strip()",
            "                return {\"status\":False,\"error\":error_message}",
            "",
            "            # If the compilation is successful, you will get a PDF file",
            "            pdf_file = file_path.with_suffix('.pdf')",
            "            print(f\"PDF file generated: {pdf_file}\")",
            "            return {\"status\":True,\"file_path\":pdf_file}",
            "",
            "        except subprocess.CalledProcessError as e:",
            "            print(f\"Error occurred while compiling LaTeX: {e}\")",
            "            return {\"status\":False,\"error\":e}",
            "",
            "    def find_numeric_value(self, text):",
            "        pattern = r'\\d+[.,]?\\d*'",
            "        match = re.search(pattern, text)",
            "        if match:",
            "            return float(match.group().replace(',', '.'))",
            "        else:",
            "            return None",
            "    def remove_backticks(self, text):",
            "        if text.startswith(\"```\"):",
            "            split_text = text.split(\"\\n\")",
            "            text = \"\\n\".join(split_text[1:])",
            "        if text.endswith(\"```\"):",
            "            text= text[:-3]",
            "        return text",
            "",
            "    def search_duckduckgo(self, query: str, max_results: int = 10, instant_answers: bool = True, regular_search_queries: bool = True, get_webpage_content: bool = False) -> List[Dict[str, Union[str, None]]]:",
            "        \"\"\"",
            "        Perform a search using the DuckDuckGo search engine and return the results as a list of dictionaries.",
            "",
            "        Args:",
            "            query (str): The search query to use in the search. This argument is required.",
            "            max_results (int, optional): The maximum number of search results to return. Defaults to 10.",
            "            instant_answers (bool, optional): Whether to include instant answers in the search results. Defaults to True.",
            "            regular_search_queries (bool, optional): Whether to include regular search queries in the search results. Defaults to True.",
            "            get_webpage_content (bool, optional): Whether to retrieve and include the website content for each result. Defaults to False.",
            "",
            "        Returns:",
            "            list[dict]: A list of dictionaries containing the search results. Each dictionary will contain 'title', 'body', and 'href' keys.",
            "",
            "        Raises:",
            "            ValueError: If neither instant_answers nor regular_search_queries is set to True.",
            "        \"\"\"",
            "        if not PackageManager.check_package_installed(\"duckduckgo_search\"):",
            "            PackageManager.install_package(\"duckduckgo_search\")",
            "        from duckduckgo_search import DDGS",
            "        if not (instant_answers or regular_search_queries):",
            "            raise ValueError(\"One of ('instant_answers', 'regular_search_queries') must be True\")",
            "",
            "        query = query.strip(\"\\\"'\")",
            "",
            "        with DDGS() as ddgs:",
            "            if instant_answers:",
            "                answer_list = list(ddgs.answers(query))",
            "                if answer_list:",
            "                    answer_dict = answer_list[0]",
            "                    answer_dict[\"title\"] = query",
            "                    answer_dict[\"body\"] = next((item['Text'] for item in answer_dict['AbstractText']), None)",
            "                    answer_dict[\"href\"] = answer_dict.get('FirstURL', '')",
            "            else:",
            "                answer_list = []",
            "",
            "            if regular_search_queries:",
            "                results = ddgs.text(query, safe=False, result_type='link')",
            "                for result in results[:max_results]:",
            "                    title = result['Text'] or query",
            "                    body = None",
            "                    href = result['FirstURL'] or ''",
            "                    answer_dict = {'title': title, 'body': body, 'href': href}",
            "                    answer_list.append(answer_dict)",
            "",
            "            if get_webpage_content:",
            "                for i, result in enumerate(answer_list):",
            "                    try:",
            "                        response = requests.get(result['href'])",
            "                        if response.status_code == 200:",
            "                            content = response.text",
            "                            answer_list[i]['body'] = content",
            "                    except Exception as e:",
            "                        print(f\"Error retrieving webpage content for {result['href']}: {str(e)}\")",
            "",
            "            return answer_list",
            "",
            "",
            "    def translate(self, text_chunk, output_language=\"french\", max_generation_size=3000):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        translated = self.fast_gen(",
            "                                \"\\n\".join([",
            "                                    f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                                    f\"Translate the following text to {output_language}.\",",
            "                                    \"Be faithful to the original text and do not add or remove any information.\",",
            "                                    \"Respond only with the translated text.\",",
            "                                    \"Do not add comments or explanations.\",",
            "                                    f\"{start_header_id_template}text to translate{end_header_id_template}\",",
            "                                    f\"{text_chunk}\",",
            "                                    f\"{start_header_id_template}translation{end_header_id_template}\",",
            "                                    ]),",
            "                                    max_generation_size=max_generation_size, callback=self.sink)",
            "        return translated",
            "",
            "    def summerize_text(",
            "                        self,",
            "                        text,",
            "                        summary_instruction=\"summerize\",",
            "                        doc_name=\"chunk\",",
            "                        answer_start=\"\",",
            "                        max_generation_size=3000,",
            "                        max_summary_size=512,",
            "                        callback=None,",
            "                        chunk_summary_post_processing=None,",
            "                        summary_mode=SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL",
            "                    ):",
            "        tk = self.personality.model.tokenize(text)",
            "        prev_len = len(tk)",
            "        document_chunks=None",
            "        while len(tk)>max_summary_size and (document_chunks is None or len(document_chunks)>1):",
            "            self.step_start(f\"Comprerssing {doc_name}...\")",
            "            chunk_size = int(self.personality.config.ctx_size*0.6)",
            "            document_chunks = DocumentDecomposer.decompose_document(text, chunk_size, 0, self.personality.model.tokenize, self.personality.model.detokenize, True)",
            "            text = self.summerize_chunks(",
            "                                            document_chunks,",
            "                                            summary_instruction, ",
            "                                            doc_name, ",
            "                                            answer_start, ",
            "                                            max_generation_size, ",
            "                                            callback, ",
            "                                            chunk_summary_post_processing=chunk_summary_post_processing,",
            "                                            summary_mode=summary_mode)",
            "            tk = self.personality.model.tokenize(text)",
            "            tk = self.personality.model.tokenize(text)",
            "            dtk_ln=prev_len-len(tk)",
            "            prev_len = len(tk)",
            "            self.step(f\"Current text size : {prev_len}, max summary size : {max_summary_size}\")",
            "            self.step_end(f\"Comprerssing {doc_name}...\")",
            "            if dtk_ln<=10: # it is not summarizing",
            "                break",
            "        return text",
            "",
            "    def smart_data_extraction(",
            "                                self,",
            "                                text,",
            "                                data_extraction_instruction=\"summerize\",",
            "                                final_task_instruction=\"reformulate with better wording\",",
            "                                doc_name=\"chunk\",",
            "                                answer_start=\"\",",
            "                                max_generation_size=3000,",
            "                                max_summary_size=512,",
            "                                callback=None,",
            "                                chunk_summary_post_processing=None,",
            "                                summary_mode=SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL",
            "                            ):",
            "        tk = self.personality.model.tokenize(text)",
            "        prev_len = len(tk)",
            "        while len(tk)>max_summary_size:",
            "            chunk_size = int(self.personality.config.ctx_size*0.6)",
            "            document_chunks = DocumentDecomposer.decompose_document(text, chunk_size, 0, self.personality.model.tokenize, self.personality.model.detokenize, True)",
            "            text = self.summerize_chunks(",
            "                                            document_chunks, ",
            "                                            data_extraction_instruction, ",
            "                                            doc_name, ",
            "                                            answer_start, ",
            "                                            max_generation_size, ",
            "                                            callback, ",
            "                                            chunk_summary_post_processing=chunk_summary_post_processing, ",
            "                                            summary_mode=summary_mode",
            "                                        )",
            "            tk = self.personality.model.tokenize(text)",
            "            dtk_ln=prev_len-len(tk)",
            "            prev_len = len(tk)",
            "            self.step(f\"Current text size : {prev_len}, max summary size : {max_summary_size}\")",
            "            if dtk_ln<=10: # it is not sumlmarizing",
            "                break",
            "        self.step_start(f\"Rewriting ...\")",
            "        text = self.summerize_chunks(",
            "                                        [text],",
            "                                        final_task_instruction, ",
            "                                        doc_name, answer_start, ",
            "                                        max_generation_size, ",
            "                                        callback, ",
            "                                        chunk_summary_post_processing=chunk_summary_post_processing",
            "                                    )",
            "        self.step_end(f\"Rewriting ...\")",
            "",
            "        return text",
            "",
            "    def summerize_chunks(",
            "                            self,",
            "                            chunks,",
            "                            summary_instruction=\"summerize\",",
            "                            doc_name=\"chunk\",",
            "                            answer_start=\"\",",
            "                            max_generation_size=3000,",
            "                            callback=None,",
            "                            chunk_summary_post_processing=None,",
            "                            summary_mode=SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL",
            "                        ):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        if summary_mode==SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL:",
            "            summary = \"\"",
            "            for i, chunk in enumerate(chunks):",
            "                self.step_start(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "                summary = f\"{answer_start}\"+ self.fast_gen(",
            "                            \"\\n\".join([",
            "                                f\"{start_header_id_template}Previous_chunks_summary{end_header_id_template}\",",
            "                                f\"{summary}\",",
            "                                f\"{start_header_id_template}Current_chunk{end_header_id_template}\",",
            "                                f\"{chunk}\",",
            "                                f\"{start_header_id_template}{system_message_template}{end_header_id_template}{summary_instruction}\",",
            "                                f\"Summerize the current chunk and fuse it with previous chunk summary ion order to keep the required informations.\",",
            "                                f\"The summary needs to keep all relevant information.\",",
            "                                f\"Be precise and do not invent information that does not exist in the previous summary or the current chunk.\",",
            "                                f\"Answer directly with the summary with no extra comments.\",",
            "                                f\"{start_header_id_template}summary{end_header_id_template}\",",
            "                                f\"{answer_start}\"",
            "                                ]),",
            "                                max_generation_size=max_generation_size,",
            "                                callback=callback)",
            "                if chunk_summary_post_processing:",
            "                    summary = chunk_summary_post_processing(summary)",
            "                self.step_end(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "            return summary",
            "        else:",
            "            summeries = []",
            "            for i, chunk in enumerate(chunks):",
            "                self.step_start(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "                summary = f\"{answer_start}\"+ self.fast_gen(",
            "                            \"\\n\".join([",
            "                                f\"{start_header_id_template}Document_chunk [{doc_name}]{end_header_id_template}\",",
            "                                f\"{chunk}\",",
            "                                f\"{start_header_id_template}{system_message_template}{end_header_id_template}{summary_instruction}\",",
            "                                f\"Answer directly with the summary with no extra comments.\",",
            "                                f\"{start_header_id_template}summary{end_header_id_template}\",",
            "                                f\"{answer_start}\"",
            "                                ]),",
            "                                max_generation_size=max_generation_size,",
            "                                callback=callback)",
            "                if chunk_summary_post_processing:",
            "                    summary = chunk_summary_post_processing(summary)",
            "                summeries.append(summary)",
            "                self.step_end(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "            return \"\\n\".join(summeries)",
            "",
            "    def sequencial_chunks_summary(",
            "                            self,",
            "                            chunks,",
            "                            summary_instruction=\"summerize\",",
            "                            doc_name=\"chunk\",",
            "                            answer_start=\"\",",
            "                            max_generation_size=3000,",
            "                            callback=None,",
            "                            chunk_summary_post_processing=None",
            "                        ):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        summeries = []",
            "        for i, chunk in enumerate(chunks):",
            "            if i<len(chunks)-1:",
            "                chunk1 = chunks[i+1]",
            "            else:",
            "                chunk1=\"\"",
            "            if i>0:",
            "                chunk=summary",
            "            self.step_start(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "            summary = f\"{answer_start}\"+ self.fast_gen(",
            "                        \"\\n\".join([",
            "                            f\"{start_header_id_template}Document_chunk: {doc_name}{end_header_id_template}\",",
            "                            f\"Block1:\",",
            "                            f\"{chunk}\",",
            "                            f\"Block2:\",",
            "                            f\"{chunk1}\",",
            "                            f\"{start_header_id_template}{system_message_template}{end_header_id_template}{summary_instruction}\",",
            "                            f\"Answer directly with the summary with no extra comments.\",",
            "                            f\"{start_header_id_template}summary{end_header_id_template}\",",
            "                            f\"{answer_start}\"",
            "                            ]),",
            "                            max_generation_size=max_generation_size,",
            "                            callback=callback)",
            "            if chunk_summary_post_processing:",
            "                summary = chunk_summary_post_processing(summary)",
            "            summeries.append(summary)",
            "            self.step_end(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "        return \"\\n\".join(summeries)",
            "",
            "    def build_prompt_from_context_details(self, context_details:dict, custom_entries=\"\"):",
            "        \"\"\"",
            "        Builds a prompt from the provided context details.",
            "",
            "        This function concatenates various parts of the context into a single string, which is then used to build a prompt.",
            "        The context details can include conditioning, documentation, knowledge, user description, positive and negative boosts,",
            "        current language, fun mode, discussion window, and any extra information.",
            "",
            "        Parameters:",
            "        context_details (dict): A dictionary containing various context details.",
            "        custom_entries (str): Additional custom entries to be included in the prompt.",
            "",
            "        Returns:",
            "        str: The constructed prompt.",
            "",
            "        Raises:",
            "        KeyError: If any required key is missing in the context_details dictionary.",
            "        \"\"\"",
            "        full_context = []",
            "        sacrifice_id = 0",
            "        if context_details[\"conditionning\"]:",
            "            full_context.append( \"\\n\".join([",
            "                context_details[\"conditionning\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "        if context_details[\"documentation\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"documentation\"),",
            "                context_details[\"documentation\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"knowledge\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"knowledge\"),",
            "                context_details[\"knowledge\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"user_description\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"user_description\"),",
            "                context_details[\"user_description\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"positive_boost\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"positive_boost\"),",
            "                context_details[\"positive_boost\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"positive_boost\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"positive_boost\"),",
            "                context_details[\"positive_boost\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"negative_boost\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"negative_boost\"),",
            "                context_details[\"negative_boost\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"current_language\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"current_language\"),",
            "                context_details[\"current_language\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"fun_mode\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"fun_mode\"),",
            "                context_details[\"fun_mode\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "",
            "        if context_details[\"discussion_messages\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"discussion_messages\"),",
            "                context_details[\"discussion_messages\"]",
            "            ]))",
            "",
            "        if context_details[\"extra\"]:",
            "            full_context.append( \"\\n\".join([",
            "                context_details[\"extra\"]",
            "            ]))",
            "",
            "        if custom_entries:",
            "            full_context.append( \"\\n\".join([",
            "                custom_entries",
            "            ]))",
            "",
            "        full_context.append( \"\\n\".join([",
            "            self.ai_custom_header(context_details[\"ai_prefix\"])",
            "        ]))",
            "",
            "        return self.build_prompt(full_context, sacrifice_id)",
            "    ",
            "    def build_prompt(self, prompt_parts:List[str], sacrifice_id:int=-1, context_size:int=None, minimum_spare_context_size:int=None):",
            "        \"\"\"",
            "        Builds the prompt for code generation.",
            "",
            "        Args:",
            "            prompt_parts (List[str]): A list of strings representing the parts of the prompt.",
            "            sacrifice_id (int, optional): The ID of the part to sacrifice.",
            "            context_size (int, optional): The size of the context.",
            "            minimum_spare_context_size (int, optional): The minimum spare context size.",
            "",
            "        Returns:",
            "            str: The built prompt.",
            "        \"\"\"",
            "        if context_size is None:",
            "            context_size = self.personality.config.ctx_size",
            "        if minimum_spare_context_size is None:",
            "            minimum_spare_context_size = self.personality.config.min_n_predict",
            "",
            "        if sacrifice_id == -1 or len(prompt_parts[sacrifice_id])<50:",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "        else:",
            "            part_tokens=[]",
            "            nb_tokens=0",
            "            for i, part in enumerate(prompt_parts):",
            "                part_s=part.strip()",
            "                tk = self.personality.model.tokenize(part_s)",
            "                part_tokens.append(tk)",
            "                if i != sacrifice_id:",
            "                    nb_tokens += len(tk)",
            "                    ",
            "            if len(part_tokens[sacrifice_id])>0:",
            "                sacrifice_tk = part_tokens[sacrifice_id]",
            "                sacrifice_tk= sacrifice_tk[-(context_size-nb_tokens-minimum_spare_context_size):]",
            "                sacrifice_text = self.personality.model.detokenize(sacrifice_tk)",
            "            else:",
            "                sacrifice_text = \"\"",
            "            prompt_parts[sacrifice_id] = sacrifice_text",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "    # ================================================= Sending commands to ui ===========================================",
            "    def add_collapsible_entry(self, title, content, subtitle=\"\"):",
            "        return \"\\n\".join(",
            "        [",
            "        f'<details class=\"flex w-full rounded-xl border border-gray-200 bg-white shadow-sm dark:border-gray-800 dark:bg-gray-900 mb-3.5 max-w-full svelte-1escu1z\" open=\"\">',",
            "        f'    <summary class=\"grid w-full select-none grid-cols-[40px,1fr] items-center gap-2.5 p-2 svelte-1escu1z\">',",
            "        f'        <dl class=\"leading-4\">',",
            "        f'          <dd class=\"text-sm\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"feather feather-arrow-right\">',",
            "        f'          <line x1=\"5\" y1=\"12\" x2=\"19\" y2=\"12\"></line>',",
            "        f'          <polyline points=\"12 5 19 12 12 19\"></polyline>',",
            "        f'          </svg>',",
            "        f'          </dd>',",
            "        f'        </dl>',",
            "        f'        <dl class=\"leading-4\">',",
            "        f'        <dd class=\"text-sm\"><h3>{title}</h3></dd>',",
            "        f'        <dt class=\"flex items-center gap-1 truncate whitespace-nowrap text-[.82rem] text-gray-400\">{subtitle}</dt>',",
            "        f'        </dl>',",
            "        f'    </summary>',",
            "        f' <div class=\"content px-5 pb-5 pt-4\">',",
            "        content,",
            "        f' </div>',",
            "        f' </details>\\n'",
            "        ])",
            "",
            "    def internet_search_with_vectorization(self, query, quick_search:bool=False ):",
            "        \"\"\"",
            "        Do internet search and return the result",
            "        \"\"\"",
            "        return self.personality.internet_search_with_vectorization(query, quick_search=quick_search)",
            "",
            "",
            "    def vectorize_and_query(self, text, query, max_chunk_size=512, overlap_size=20, internet_vectorization_nb_chunks=3):",
            "        vectorizer = TextVectorizer(VectorizationMethod.TFIDF_VECTORIZER, model = self.personality.model)",
            "        decomposer = DocumentDecomposer()",
            "        chunks = decomposer.decompose_document(text, max_chunk_size, overlap_size,self.personality.model.tokenize,self.personality.model.detokenize)",
            "        for i, chunk in enumerate(chunks):",
            "            vectorizer.add_document(f\"chunk_{i}\", self.personality.model.detokenize(chunk))",
            "        vectorizer.index()",
            "        docs, sorted_similarities, document_ids = vectorizer.recover_text(query, internet_vectorization_nb_chunks)",
            "        return docs, sorted_similarities",
            "",
            "",
            "    def step_start(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step start",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step start to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_START)",
            "",
            "    def step_end(self, step_text, status=True, callback: Callable[[str, int, dict, list], bool]=None):",
            "        \"\"\"This triggers a step end",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step end to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_END, {'status':status})",
            "",
            "    def step(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step information",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP)",
            "",
            "    def exception(self, ex, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends exception to the client",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(str(ex), MSG_TYPE.MSG_TYPE_EXCEPTION)",
            "",
            "    def warning(self, warning:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends exception to the client",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(warning, MSG_TYPE.MSG_TYPE_EXCEPTION)",
            "",
            "",
            "    def json(self, title:str, json_infos:dict, callback: Callable[[str, int, dict, list], bool]=None, indent=4):",
            "        \"\"\"This sends json data to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(\"\", MSG_TYPE.MSG_TYPE_JSON_INFOS, metadata = [{\"title\":title, \"content\":json.dumps(json_infos, indent=indent)}])",
            "",
            "    def ui(self, html_ui:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends ui elements to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(html_ui, MSG_TYPE.MSG_TYPE_UI)",
            "",
            "    def code(self, code:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends code to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(code, MSG_TYPE.MSG_TYPE_CODE)",
            "",
            "    def chunk(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_CHUNK)",
            "",
            "",
            "    def full(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None, msg_type:MSG_TYPE = MSG_TYPE.MSG_TYPE_FULL):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, msg_type)",
            "",
            "    def full_invisible_to_ai(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to AI)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI)",
            "",
            "    def full_invisible_to_user(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to user)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER)",
            "",
            "",
            "",
            "",
            "    def execute_python(self, code, code_folder=None, code_file_name=None):",
            "        if code_folder is not None:",
            "            code_folder = Path(code_folder)",
            "",
            "        \"\"\"Executes Python code and returns the output as JSON.\"\"\"",
            "        # Create a temporary file.",
            "        root_folder = code_folder if code_folder is not None else self.personality.personality_output_folder",
            "        root_folder.mkdir(parents=True,exist_ok=True)",
            "        tmp_file = root_folder/(code_file_name if code_file_name is not None else f\"ai_code.py\")",
            "        with open(tmp_file,\"w\") as f:",
            "            f.write(code)",
            "",
            "        # Execute the Python code in a temporary file.",
            "        process = subprocess.Popen(",
            "            [\"python\", str(tmp_file)],",
            "            stdout=subprocess.PIPE,",
            "            stderr=subprocess.PIPE,",
            "            cwd=root_folder",
            "        )",
            "",
            "        # Get the output and error from the process.",
            "        output, error = process.communicate()",
            "",
            "        # Check if the process was successful.",
            "        if process.returncode != 0:",
            "            # The child process threw an exception.",
            "            error_message = f\"Error executing Python code: {error.decode('utf8')}\"",
            "            return error_message",
            "",
            "        # The child process was successful.",
            "        return output.decode(\"utf8\")",
            "",
            "    def build_python_code(self, prompt, max_title_length=4096):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        if not PackageManager.check_package_installed(\"autopep8\"):",
            "            PackageManager.install_package(\"autopep8\")",
            "        import autopep8",
            "        global_prompt = \"\\n\".join([",
            "            f\"{prompt}\",",
            "            f\"{start_header_id_template}Extra conditions{end_header_id_template}\",",
            "            \"- The code must be complete, not just snippets, and should be put inside a single python markdown code.\",",
            "            \"-Preceive each python codeblock with a line using this syntax:\",",
            "            \"$$file_name|the file path relative to the root folder of the project$$\",",
            "            \"```python\",",
            "            \"# Placeholder. Here you need to put the code for the file\",",
            "            \"```\",",
            "            f\"{start_header_id_template}Code Builder{end_header_id_template}\"",
            "        ])",
            "        code = self.fast_gen(global_prompt, max_title_length)",
            "        code_blocks = self.extract_code_blocks(code)",
            "        try:",
            "            back_quote_index = code.index(\"```\")  # Remove trailing backticks",
            "            if back_quote_index>=0:",
            "                # Removing any extra text",
            "                code = code[:back_quote_index]",
            "        except:",
            "            pass",
            "        formatted_code = autopep8.fix_code(code)  # Fix indentation errors",
            "        return formatted_code",
            "",
            "",
            "    def make_title(self, prompt, max_title_length: int = 50):",
            "        \"\"\"",
            "        Generates a title for a given prompt.",
            "",
            "        Args:",
            "            prompt (str): The prompt for which a title needs to be generated.",
            "            max_title_length (int, optional): The maximum length of the generated title. Defaults to 50.",
            "",
            "        Returns:",
            "            str: The generated title.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        separator_template          = self.config.separator_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        global_prompt = f\"{start_header_id_template}{system_message_template}{end_header_id_template}Based on the provided prompt, suggest a concise and relevant title that captures the main topic or theme of the conversation. Only return the suggested title, without any additional text or explanation.{separator_template}{start_header_id_template}prompt{end_header_id_template}{prompt}{separator_template}{start_header_id_template}title{end_header_id_template}\"",
            "        title = self.fast_gen(global_prompt,max_title_length)",
            "        return title",
            "",
            "",
            "    def plan_with_images(self, request: str, images:list, actions_list:list=[LoLLMsAction], context:str = \"\", max_answer_length: int = 512) -> List[LoLLMsAction]:",
            "        \"\"\"",
            "        creates a plan out of a request and a context",
            "",
            "        Args:",
            "            request (str): The request posed by the user.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        template = \"\\n\".join([",
            "            f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "            \"Act as plan builder, a tool capable of making plans to perform the user requested operation.\"",
            "        ])",
            "",
            "        if len(actions_list)>0:",
            "            template += \"\\n\".join([",
            "                \"The plan builder is an AI that responds in json format. It should plan a succession of actions in order to reach the objective.\",",
            "                f\"{start_header_id_template}list of action types information{end_header_id_template}\",",
            "                \"[\",",
            "                \"{actions_list}\",",
            "                \"]\",",
            "                \"The AI should respond in this format using data from actions_list:\",",
            "                \"{\",",
            "                '    \"actions\": [',",
            "                '    {',",
            "                '        \"name\": name of the action 1,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    },',",
            "                '    {',",
            "                '        \"name\": name of the action 2,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    }',",
            "                '    ...',",
            "                '    ]',",
            "                \"}\"",
            "            ])",
            "        if context != \"\":",
            "            template += \"\\n\".join([",
            "                f\"{start_header_id_template}context{end_header_id_template}\",",
            "                \"{context}Ok\"",
            "            ])",
            "",
            "        template += \"\\n\".join([",
            "            f\"{start_header_id_template}request{end_header_id_template}{{request}}\",",
            "            f\"{start_header_id_template}plan{end_header_id_template}To achieve the requested objective, this is the list of actions to follow, formatted as requested in json format:\\n```json\\n\"",
            "        ])",
            "        pr  = PromptReshaper(template)",
            "        prompt = pr.build({",
            "                \"context\":context,",
            "                \"request\":request,",
            "                \"actions_list\":\",\\n\".join([f\"{action}\" for action in actions_list])",
            "                },",
            "                self.personality.model.tokenize,",
            "                self.personality.model.detokenize,",
            "                self.personality.model.config.ctx_size,",
            "                [\"previous_discussion\"]",
            "                )",
            "        gen = self.generate_with_images(prompt, images, max_answer_length).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        gen = self.remove_backticks(gen)",
            "        self.print_prompt(\"full\",prompt+gen)",
            "        gen = fix_json(gen)",
            "        return generate_actions(actions_list, gen)",
            "",
            "    def plan(self, request: str, actions_list:list=[LoLLMsAction], context:str = \"\", max_answer_length: int = 512) -> List[LoLLMsAction]:",
            "        \"\"\"",
            "        creates a plan out of a request and a context",
            "",
            "        Args:",
            "            request (str): The request posed by the user.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        template = \"\\n\".join([",
            "            f\"{start_header_id_template}instruction:\",",
            "            \"Act as plan builder, a tool capable of making plans to perform the user requested operation.\"",
            "        ])",
            "",
            "        if len(actions_list) > 0:",
            "            template += \"\\n\".join([",
            "                \"The plan builder is an AI that responds in json format. It should plan a succession of actions in order to reach the objective.\",",
            "                f\"{start_header_id_template}list of action types information{end_header_id_template}\",",
            "                \"[\",",
            "                \"{actions_list}\",",
            "                \"]\",",
            "                \"The AI should respond in this format using data from actions_list:\",",
            "                \"{\",",
            "                '    \"actions\": [',",
            "                '    {',",
            "                '        \"name\": name of the action 1,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    },',",
            "                '    {',",
            "                '        \"name\": name of the action 2,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    }',",
            "                '    ...',",
            "                '    ]',",
            "                \"}\"",
            "            ])",
            "",
            "        if context != \"\":",
            "            template += \"\\n\".join([",
            "                f\"{start_header_id_template}context{end_header_id_template}\",",
            "                \"{context}Ok\"",
            "            ])",
            "",
            "        template += \"\\n\".join([",
            "            f\"{start_header_id_template}request{end_header_id_template}{{request}}\",",
            "            f\"{start_header_id_template}plan{end_header_id_template}To achieve the requested objective, this is the list of actions to follow, formatted as requested in json format:\\n```json\\n\"",
            "        ])",
            "        pr  = PromptReshaper(template)",
            "        prompt = pr.build({",
            "                \"context\":context,",
            "                \"request\":request,",
            "                \"actions_list\":\",\\n\".join([f\"{action}\" for action in actions_list])",
            "                },",
            "                self.personality.model.tokenize,",
            "                self.personality.model.detokenize,",
            "                self.personality.model.config.ctx_size,",
            "                [\"previous_discussion\"]",
            "                )",
            "        gen = self.generate(prompt, max_answer_length).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        gen = self.remove_backticks(gen).strip()",
            "        if gen[-1]!=\"}\":",
            "            gen+=\"}\"",
            "        self.print_prompt(\"full\",prompt+gen)",
            "        gen = fix_json(gen)",
            "        return generate_actions(actions_list, gen)",
            "",
            "",
            "    def parse_directory_structure(self, structure):",
            "        paths = []",
            "        lines = structure.strip().split('\\n')",
            "        stack = []",
            "",
            "        for line in lines:",
            "            line = line.rstrip()",
            "            level = (len(line) - len(line.lstrip())) // 4",
            "",
            "            if '/' in line or line.endswith(':'):",
            "                directory = line.strip(' \u251c\u2500\u2514\u2502').rstrip(':').rstrip('/')",
            "",
            "                while stack and level < stack[-1][0]:",
            "                    stack.pop()",
            "",
            "                stack.append((level, directory))",
            "                path = '/'.join([dir for _, dir in stack]) + '/'",
            "                paths.append(path)",
            "            else:",
            "                file = line.strip(' \u251c\u2500\u2514\u2502')",
            "                if stack:",
            "                    path = '/'.join([dir for _, dir in stack]) + '/' + file",
            "                    paths.append(path)",
            "",
            "        return paths",
            "",
            "    def extract_code_blocks(self, text: str) -> List[dict]:",
            "        \"\"\"",
            "        This function extracts code blocks from a given text.",
            "",
            "        Parameters:",
            "        text (str): The text from which to extract code blocks. Code blocks are identified by triple backticks (```).",
            "",
            "        Returns:",
            "        List[dict]: A list of dictionaries where each dictionary represents a code block and contains the following keys:",
            "            - 'index' (int): The index of the code block in the text.",
            "            - 'file_name' (str): An empty string. This field is not used in the current implementation.",
            "            - 'content' (str): The content of the code block.",
            "            - 'type' (str): The type of the code block. If the code block starts with a language specifier (like 'python' or 'java'), this field will contain that specifier. Otherwise, it will be set to 'language-specific'.",
            "",
            "        Note:",
            "        The function assumes that the number of triple backticks in the text is even.",
            "        If the number of triple backticks is odd, it will consider the rest of the text as the last code block.",
            "        \"\"\"        ",
            "        remaining = text",
            "        bloc_index = 0",
            "        first_index=0",
            "        indices = []",
            "        while len(remaining)>0:",
            "            try:",
            "                index = remaining.index(\"```\")",
            "                indices.append(index+first_index)",
            "                remaining = remaining[index+3:]",
            "                first_index += index+3",
            "                bloc_index +=1",
            "            except Exception as ex:",
            "                if bloc_index%2==1:",
            "                    index=len(remaining)",
            "                    indices.append(index)",
            "                remaining = \"\"",
            "",
            "        code_blocks = []",
            "        is_start = True",
            "        for index, code_delimiter_position in enumerate(indices):",
            "            block_infos = {",
            "                'index':index,",
            "                'file_name': \"\",",
            "                'content': \"\",",
            "                'type':\"\"",
            "            }",
            "            if is_start:",
            "",
            "                sub_text = text[code_delimiter_position+3:]",
            "                if len(sub_text)>0:",
            "                    try:",
            "                        find_space = sub_text.index(\" \")",
            "                    except:",
            "                        find_space = int(1e10)",
            "                    try:",
            "                        find_return = sub_text.index(\"\\n\")",
            "                    except:",
            "                        find_return = int(1e10)",
            "                    next_index = min(find_return, find_space)",
            "                    if '{' in sub_text[:next_index]:",
            "                        next_index =0",
            "                    start_pos = next_index",
            "                    if code_delimiter_position+3<len(text) and text[code_delimiter_position+3] in [\"\\n\",\" \",\"\\t\"] :",
            "                        # No",
            "                        block_infos[\"type\"]='language-specific'",
            "                    else:",
            "                        block_infos[\"type\"]=sub_text[:next_index]",
            "",
            "                    next_pos = indices[index+1]-code_delimiter_position",
            "                    if sub_text[next_pos-3]==\"`\":",
            "                        block_infos[\"content\"]=sub_text[start_pos:next_pos-3].strip()",
            "                    else:",
            "                        block_infos[\"content\"]=sub_text[start_pos:next_pos].strip()",
            "                    code_blocks.append(block_infos)",
            "                is_start = False",
            "            else:",
            "                is_start = True",
            "                continue",
            "",
            "        return code_blocks",
            "",
            "",
            "",
            "    def build_and_execute_python_code(self,context, instructions, execution_function_signature, extra_imports=\"\"):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        code = \"```python\\n\"+self.fast_gen(",
            "            self.build_prompt([",
            "            f\"{start_header_id_template}context{end_header_id_template}\",",
            "            context,",
            "            f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "            f\"{instructions}\",",
            "            f\"Here is the signature of the function:\\n{execution_function_signature}\",",
            "            \"Don't call the function, just write it\",",
            "            \"Do not provide usage example.\",",
            "            \"The code must me without comments\",",
            "            f\"{start_header_id_template}coder{end_header_id_template}Sure, in the following code, I import the necessary libraries, then define the function as you asked.\",",
            "            \"The function is ready to be used in your code and performs the task as you asked:\",",
            "            \"```python\\n\"",
            "            ],2), callback=self.sink)",
            "        code = code.replace(\"```python\\n```python\\n\", \"```python\\n\").replace(\"```\\n```\",\"```\")",
            "        code=self.extract_code_blocks(code)",
            "",
            "        if len(code)>0:",
            "            # Perform the search query",
            "            code = code[0][\"content\"]",
            "            code = \"\\n\".join([",
            "                        extra_imports,",
            "                        code",
            "                    ])",
            "            ASCIIColors.magenta(code)",
            "            module_name = 'custom_module'",
            "            spec = importlib.util.spec_from_loader(module_name, loader=None)",
            "            module = importlib.util.module_from_spec(spec)",
            "            exec(code, module.__dict__)",
            "            return module, code",
            "",
            "",
            "    def yes_no(self, question: str, context:str=\"\", max_answer_length: int = 50, conditionning=\"\") -> bool:",
            "        \"\"\"",
            "        Analyzes the user prompt and answers whether it is asking to generate an image.",
            "",
            "        Args:",
            "            question (str): The user's message.",
            "            max_answer_length (int, optional): The maximum length of the generated answer. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "        Returns:",
            "            bool: True if the user prompt is asking to generate an image, False otherwise.",
            "        \"\"\"",
            "        return self.multichoice_question(question, [\"no\",\"yes\"], context, max_answer_length, conditionning=conditionning)>0",
            "",
            "    def multichoice_question(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Interprets a multi-choice question from a users response. This function expects only one choice as true. All other choices are considered false. If none are correct, returns -1.",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                       f\"{start_header_id_template}Context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "        elements +=[",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "                \"Do not explain your reasons or add comments.\",",
            "                \"the output should be an integer.\"",
            "        ]",
            "        elements += [",
            "                f\"{start_header_id_template}question{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50, callback=self.sink).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        if len(gen)>0:",
            "            selection = gen.strip().split()[0].replace(\",\",\"\").replace(\".\",\"\")",
            "            self.print_prompt(\"Multi choice selection\",prompt+gen)",
            "            try:",
            "                return int(selection)",
            "            except:",
            "                ASCIIColors.cyan(\"Model failed to answer the question\")",
            "                return -1",
            "        else:",
            "            return -1",
            "",
            "    def multichoice_ranking(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Ranks answers for a question from best to worst. returns a list of integers",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}instructions{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "                f\"{start_header_id_template}question{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                        f\"{start_header_id_template}context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        self.print_prompt(\"Multi choice ranking\",prompt+gen)",
            "        if gen.index(\"]\")>=0:",
            "            try:",
            "                ranks = eval(gen.split(\"]\")[0]+\"]\")",
            "                return ranks",
            "            except:",
            "                ASCIIColors.red(\"Model failed to rank inputs\")",
            "                return None",
            "        else:",
            "            ASCIIColors.red(\"Model failed to rank inputs\")",
            "            return None",
            "",
            "",
            "",
            "    def build_html5_integration(self, html, ifram_name=\"unnamed\"):",
            "        \"\"\"",
            "        This function creates an HTML5 iframe with the given HTML content and iframe name.",
            "",
            "        Args:",
            "        html (str): The HTML content to be displayed in the iframe.",
            "        ifram_name (str, optional): The name of the iframe. Defaults to \"unnamed\".",
            "",
            "        Returns:",
            "        str: The HTML string for the iframe.",
            "        \"\"\"",
            "        return \"\\n\".join(",
            "            '<div style=\"width: 80%; margin: 0 auto;\">',",
            "            f'<iframe id=\"{ifram_name}\" srcdoc=\"',",
            "            html,",
            "            '\" style=\"width: 100%; height: 600px; border: none;\"></iframe>',",
            "            '</div>'",
            "        )",
            "",
            "",
            "    def InfoMessage(self, content, client_id=None, verbose:bool=None):",
            "        self.personality.app.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.MESSAGE_BOX,",
            "                verbose=verbose",
            "            )",
            "",
            "    def info(self, info_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends info text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the info to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(info_text, MSG_TYPE.MSG_TYPE_FULL)",
            "",
            "    def step_progress(self, step_text:str, progress:float, callback: Callable[[str, MSG_TYPE, dict, list, AIPersonality], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_PROGRESS, {'progress':progress})",
            "",
            "    def new_message(self, message_text:str, message_type:MSG_TYPE= MSG_TYPE.MSG_TYPE_FULL, metadata=[], callback: Callable[[str, int, dict, list, AIPersonality], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(message_text, MSG_TYPE.MSG_TYPE_NEW_MESSAGE, parameters={'type':message_type.value,'metadata':metadata},personality = self.personality)",
            "",
            "    def finished_message(self, message_text:str=\"\", callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(message_text, MSG_TYPE.MSG_TYPE_FINISHED_MESSAGE)",
            "",
            "    def print_prompt(self, title, prompt):",
            "        ASCIIColors.red(\"*-*-*-*-*-*-*-* \", end=\"\")",
            "        ASCIIColors.red(title, end=\"\")",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "        ASCIIColors.yellow(prompt)",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "",
            "",
            "    def fast_gen_with_images(self, prompt: str, images:list, max_generation_size: int= None, placeholders: dict = {}, sacrifice: list = [\"previous_discussion\"], debug: bool = False, callback=None, show_progress=False) -> str:",
            "        \"\"\"",
            "        Fast way to generate code",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        return self.personality.fast_gen_with_images(prompt=prompt, images=images, max_generation_size=max_generation_size,placeholders=placeholders, sacrifice=sacrifice, debug=debug, callback=callback, show_progress=show_progress)",
            "",
            "    def fast_gen(self, prompt: str, max_generation_size: int= None, placeholders: dict = {}, sacrifice: list = [\"previous_discussion\"], debug: bool = False, callback=None, show_progress=False) -> str:",
            "        \"\"\"",
            "        Fast way to generate code",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        return self.personality.fast_gen(prompt=prompt,max_generation_size=max_generation_size,placeholders=placeholders, sacrifice=sacrifice, debug=debug, callback=callback, show_progress=show_progress)",
            "",
            "",
            "",
            "    def generate_with_function_calls(self, context_details: dict, functions: List[Dict[str, Any]], max_answer_length: Optional[int] = None, callback = None) -> List[Dict[str, Any]]:",
            "        \"\"\"",
            "        Performs text generation with function calls.",
            "",
            "        Args:",
            "            context_details (dict): The full prompt (including conditioning, user discussion, extra data, and the user prompt).",
            "            functions (List[Dict[str, Any]]): A list of dictionaries describing functions that can be called.",
            "            max_answer_length (int, optional): Maximum string length allowed for the generated text.",
            "",
            "        Returns:",
            "            List[Dict[str, Any]]: A list of dictionaries with the function names and parameters to execute.",
            "        \"\"\"",
            "",
            "",
            "        # Upgrade the prompt with information about the function calls.",
            "        upgraded_prompt = self._upgrade_prompt_with_function_info(context_details, functions)",
            "",
            "        # Generate the initial text based on the upgraded prompt.",
            "        generated_text = self.fast_gen(upgraded_prompt, max_answer_length, callback=callback)",
            "",
            "        if self.config.debug:",
            "            self.print_prompt(\"Generated\", generated_text)",
            "",
            "        # Extract the function calls from the generated text.",
            "        function_calls = self.extract_function_calls_as_json(generated_text)",
            "",
            "        return generated_text, function_calls",
            "",
            "",
            "    def generate_with_function_calls_and_images(self, context_details: dict, images:list, functions: List[Dict[str, Any]], max_answer_length: Optional[int] = None, callback = None) -> List[Dict[str, Any]]:",
            "        \"\"\"",
            "        Performs text generation with function calls.",
            "",
            "        Args:",
            "            prompt (str): The full prompt (including conditioning, user discussion, extra data, and the user prompt).",
            "            functions (List[Dict[str, Any]]): A list of dictionaries describing functions that can be called.",
            "            max_answer_length (int, optional): Maximum string length allowed for the generated text.",
            "",
            "        Returns:",
            "            List[Dict[str, Any]]: A list of dictionaries with the function names and parameters to execute.",
            "        \"\"\"",
            "        # Upgrade the prompt with information about the function calls.",
            "        upgraded_prompt = self._upgrade_prompt_with_function_info(context_details, functions)",
            "",
            "        # Generate the initial text based on the upgraded prompt.",
            "        generated_text = self.fast_gen_with_images(upgraded_prompt, images, max_answer_length, callback=callback)",
            "",
            "        # Extract the function calls from the generated text.",
            "        function_calls = self.extract_function_calls_as_json(generated_text)",
            "",
            "        return generated_text, function_calls",
            "",
            "    def execute_function(self, code, function_definitions = None):",
            "        function_call = json.loads(code)",
            "        self.execute_function_calls([function_call], function_definitions=function_definitions )",
            "",
            "    def execute_function_calls(self, function_calls: List[Dict[str, Any]], function_definitions: List[Dict[str, Any]]) -> List[Any]:",
            "        \"\"\"",
            "        Executes the function calls with the parameters extracted from the generated text,",
            "        using the original functions list to find the right function to execute.",
            "",
            "        Args:",
            "            function_calls (List[Dict[str, Any]]): A list of dictionaries representing the function calls.",
            "            function_definitions (List[Dict[str, Any]]): The original list of functions with their descriptions and callable objects.",
            "",
            "        Returns:",
            "            List[Any]: A list of results from executing the function calls.",
            "        \"\"\"",
            "        if function_definitions is None:",
            "            function_definitions = self.function_definitions",
            "        results = []",
            "        # Convert function_definitions to a dict for easier lookup",
            "        functions_dict = {func['function_name']: func for func in function_definitions}",
            "",
            "        for call in function_calls:",
            "            keys = [k for k in call.keys()]",
            "            if not (\"function_name\" in keys):",
            "                key = keys[0] if len(keys)>0 else None",
            "                d = call[key] if key else None",
            "                function_name = key",
            "                parameters = d",
            "            else:",
            "                function_name = call.get(\"function_name\", None) or call.get(\"function\", None)",
            "                parameters = call.get(\"function_parameters\", None)",
            "            fn =  functions_dict.get(function_name)",
            "            if fn:",
            "                function = fn['function']",
            "                try:",
            "                    # Assuming parameters is a dictionary that maps directly to the function's arguments.",
            "                    if type(parameters)==list:",
            "                        f_parameters ={k:v for k,v in zip([p['name'] for p in fn['function_parameters']],parameters)}",
            "                        result = function(**f_parameters)",
            "                        results.append(result)",
            "                    elif type(parameters)==dict:",
            "                        result = function(**parameters)",
            "                        results.append(result)",
            "                except TypeError as e:",
            "                    trace_exception(e)",
            "                    # Handle cases where the function call fails due to incorrect parameters, etc.",
            "                    results.append(f\"Error calling {function_name}: {e}\")",
            "            else:",
            "                results.append(f\"Function {function_name} not found.\")",
            "",
            "        return results",
            "",
            "    def transform_functions_to_text(self, functions):",
            "        function_texts = []",
            "",
            "        for func in functions:",
            "            function_text = f'Function: {func[\"function_name\"]}\\nDescription: {func[\"function_description\"]}\\nParameters:\\n'",
            "            ",
            "            for param in func[\"function_parameters\"]:",
            "                param_type = \"string\" if param[\"type\"] == \"str\" else param[\"type\"]",
            "                param_description = param.get(\"description\", \"\")",
            "                function_text += f'  - {param[\"name\"]} ({param_type}): {param_description}\\n'",
            "            ",
            "            function_texts.append(function_text.strip())",
            "        ",
            "        return \"\\n\\n\".join(function_texts)",
            "    ",
            "    def transform_functions(self, functions):",
            "        tools = []",
            "",
            "        for func in functions:",
            "            function_dict = {",
            "                \"type\": \"function\",",
            "                \"function\": {",
            "                    \"name\": func[\"function_name\"],",
            "                    \"description\": func[\"function_description\"],",
            "                    \"parameters\": {",
            "                        \"type\": \"object\",",
            "                        \"properties\": {},",
            "                        \"required\": [],",
            "                    },",
            "                },",
            "            }",
            "            ",
            "            for param in func[\"function_parameters\"]:",
            "                function_dict[\"function\"][\"parameters\"][\"properties\"][param[\"name\"]] = {",
            "                    \"type\": \"string\" if param[\"type\"] == \"str\" else param[\"type\"],",
            "                    \"description\": param.get(\"description\", \"\"),",
            "                }",
            "                function_dict[\"function\"][\"parameters\"][\"required\"].append(param[\"name\"])",
            "            ",
            "            tools.append(function_dict)",
            "",
            "        return tools",
            "",
            "    def _upgrade_prompt_with_function_info(self, context_details: dict, functions: List[Dict[str, Any]]) -> str:",
            "        \"\"\"",
            "        Upgrades the prompt with information about function calls.",
            "",
            "        Args:",
            "            context_details (dict): The original prompt.",
            "            functions (List[Dict[str, Any]]): A list of dictionaries describing functions that can be called.",
            "",
            "        Returns:",
            "            str: The upgraded prompt that includes information about the function calls.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        separator_template          = self.config.separator_template",
            "",
            "",
            "        tools = self.transform_functions_to_text(functions)",
            "        import copy",
            "        cd = copy.deepcopy(context_details)",
            "        function_descriptions = [",
            "                                 f\"{start_header_id_template}Available functions{end_header_id_template}\\n\",",
            "                                 tools,",
            "                                 \"\",",
            "                                 cd[\"conditionning\"],",
            "                                 \"Your objective is interact with the user and if you need to call a function, then use the available functions above and call them using the following json format inside a markdown tag:\"",
            "                                 \"```function\",",
            "                                 \"{\",",
            "                                 '\"function_name\":the name of the function to be called,',",
            "                                 '\"function_parameters\": a list of  parameter values',",
            "                                 \"}\",",
            "                                 \"```\",",
            "                                 ]",
            "",
            "",
            "        # Combine the function descriptions with the original prompt.",
            "        function_info = '\\n'.join(function_descriptions)",
            "",
            "        cd[\"conditionning\"]=function_info",
            "        upgraded_prompt = self.build_prompt_from_context_details(cd)",
            "",
            "        ",
            "        return upgraded_prompt",
            "",
            "    def extract_function_calls_as_json(self, text: str) -> List[Dict[str, Any]]:",
            "        \"\"\"",
            "        Extracts function calls formatted as JSON inside markdown code blocks.",
            "",
            "        Args:",
            "            text (str): The generated text containing JSON markdown entries for function calls.",
            "",
            "        Returns:",
            "            List[Dict[str, Any]]: A list of dictionaries representing the function calls.",
            "        \"\"\"",
            "",
            "        # Extract markdown code blocks that contain JSON.",
            "        code_blocks = self.extract_code_blocks(text)",
            "",
            "        # Filter out and parse JSON entries.",
            "        function_calls = []",
            "        for block in code_blocks:",
            "            if block[\"type\"]==\"function\" or block[\"type\"]==\"json\" or block[\"type\"]==\"\":",
            "                content = block.get(\"content\", \"\")",
            "                try:",
            "                    # Attempt to parse the JSON content of the code block.",
            "                    function_call = json.loads(content)",
            "                    if type(function_call)==dict:",
            "                        function_calls.append(function_call)",
            "                    elif type(function_call)==list:",
            "                        function_calls+=function_call",
            "                except json.JSONDecodeError:",
            "                    # If the content is not valid JSON, skip it.",
            "                    continue",
            "",
            "        return function_calls",
            "",
            "",
            "    def interact(",
            "                    self, ",
            "                    context_details, ",
            "                    callback = None",
            "                    ):",
            "        upgraded_prompt = self.build_prompt_from_context_details(context_details)",
            "        if len(self.personality.image_files)>0:",
            "            # Generate the initial text based on the upgraded prompt.",
            "            generated_text = self.fast_gen_with_images(upgraded_prompt, self.personality.image_files, callback=callback)",
            "        else:    ",
            "            generated_text = self.fast_gen(upgraded_prompt, callback=callback)",
            "",
            "        return generated_text",
            "",
            "    def interact_with_function_call(",
            "                                        self, ",
            "                                        context_details, ",
            "                                        function_definitions, ",
            "                                        prompt_after_execution=True, ",
            "                                        callback = None, ",
            "                                        hide_function_call=False,",
            "                                        separate_output=False,",
            "                                        max_nested_function_calls=10):",
            "        ",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        separator_template          = self.config.separator_template",
            "",
            "        final_output = \"\"",
            "        if len(self.personality.image_files)>0:",
            "            out, function_calls = self.generate_with_function_calls_and_images(context_details, self.personality.image_files, function_definitions, callback=callback)",
            "        else:",
            "            out, function_calls = self.generate_with_function_calls(context_details, function_definitions, callback=callback)",
            "        nested_function_calls = 0",
            "        while len(function_calls)>0 and nested_function_calls<max_nested_function_calls:",
            "            nested_function_calls += 1",
            "            self.chunk(\"\\n\") ",
            "            if hide_function_call:",
            "                self.full(\"\") #Hide function ",
            "",
            "            if self.config.debug:",
            "                self.print_prompt(\"Function calls\", json.dumps(function_calls, indent=4))",
            "",
            "            outputs = self.execute_function_calls(function_calls,function_definitions)",
            "            final_output = \"\\n\".join([str(o) if type(o)==str else str(o[0]) if (type(o)==tuple or type(0)==list) and len(o)>0 else \"\" for o in outputs])",
            "            out += f\"{separator_template}{start_header_id_template}function calls results{end_header_id_template}\\n\" + final_output + \"\\n\"",
            "            if prompt_after_execution:",
            "                if separate_output:",
            "                    self.full(final_output)",
            "                    self.new_message(\"\")",
            "                context_details[\"discussion_messages\"] +=out",
            "                if len(self.personality.image_files)>0:",
            "                    out, function_calls = self.generate_with_function_calls_and_images(context_details, self.personality.image_files, function_definitions, callback=callback)",
            "                else:",
            "                    out, function_calls = self.generate_with_function_calls(context_details, function_definitions, callback=callback)",
            "                final_output += \"\\n\" + out",
            "        else:",
            "            final_output = out",
            "        return final_output",
            "",
            "    #Helper method to convert outputs path to url",
            "    def path2url(file):",
            "        file = str(file).replace(\"\\\\\",\"/\")",
            "        pth = file.split('/')",
            "        idx = pth.index(\"outputs\")",
            "        pth = \"/\".join(pth[idx:])",
            "        file_path = f\"![](/{pth})\\n\"",
            "        return file_path",
            "",
            "    def build_a_document_block(self, title=\"Title\", link=\"\", content=\"content\"):",
            "        if link!=\"\":",
            "            return f'''",
            "<div style=\"width: 100%; border: 1px solid #ccc; border-radius: 5px; padding: 20px; font-family: Arial, sans-serif; margin-bottom: 20px; box-sizing: border-box;\">",
            "    <h3 style=\"margin-top: 0;\">",
            "        <a href=\"{link}\" target=\"_blank\" style=\"text-decoration: none; color: #333;\">{title}</a>",
            "    </h3>",
            "    <pre style=\"white-space: pre-wrap;color: #666;\">{content}</pre>",
            "</div>",
            "'''",
            "        else:",
            "            return f'''",
            "<div style=\"width: 100%; border: 1px solid #ccc; border-radius: 5px; padding: 20px; font-family: Arial, sans-serif; margin-bottom: 20px; box-sizing: border-box;\">",
            "    <h3 style=\"margin-top: 0;\">",
            "        <p style=\"text-decoration: none; color: #333;\">{title}</p>",
            "    </h3>",
            "    <pre style=\"white-space: pre-wrap;color: #666;\">{content}</pre>",
            "</div>",
            "'''",
            "",
            "    def build_a_folder_link(self, folder_path, link_text=\"Open Folder\"):",
            "        folder_path = str(folder_path).replace('\\\\','/')",
            "        return '''",
            "<a href=\"#\" onclick=\"path=\\''''+f'{folder_path}'+'''\\';",
            "fetch('/open_folder', {",
            "    method: 'POST',",
            "    headers: {",
            "        'Content-Type': 'application/json'",
            "    },",
            "    body: JSON.stringify({ path: path })",
            "    })",
            "    .then(response => response.json())",
            "    .then(data => {",
            "    if (data.status) {",
            "        console.log('Folder opened successfully');",
            "    } else {",
            "        console.error('Error opening folder:', data.error);",
            "    }",
            "    })",
            "    .catch(error => {",
            "    console.error('Error:', error);",
            "    });",
            "\">'''+f'''{link_text}</a>'''",
            "    def build_a_file_link(self, file_path, link_text=\"Open Folder\"):",
            "        file_path = str(file_path).replace('\\\\','/')",
            "        return '''",
            "<a href=\"#\" onclick=\"path=\\''''+f'{file_path}'+'''\\';",
            "fetch('/open_file', {",
            "    method: 'POST',",
            "    headers: {",
            "        'Content-Type': 'application/json'",
            "    },",
            "    body: JSON.stringify({ path: path })",
            "    })",
            "    .then(response => response.json())",
            "    .then(data => {",
            "    if (data.status) {",
            "        console.log('Folder opened successfully');",
            "    } else {",
            "        console.error('Error opening folder:', data.error);",
            "    }",
            "    })",
            "    .catch(error => {",
            "    console.error('Error:', error);",
            "    });",
            "\">'''+f'''{link_text}</a>'''",
            "# ===========================================================",
            "    def compress_js(self, code):",
            "        return compress_js(code)",
            "    def compress_python(self, code):",
            "        return compress_python(code)",
            "    def compress_html(self, code):",
            "        return compress_html(code)",
            "",
            "",
            "",
            "",
            "# ===========================================================",
            "    def select_model(self, binding_name, model_name):",
            "        self.personality.app.select_model(binding_name, model_name)",
            "    def verify_rag_entry(self, query, rag_entry):",
            "        return self.yes_no(\"Does the text entry contain the answer to the query?\", self.system_custom_header(\"Query\")+query+\"\\n\"+self.system_custom_header(\"text entry\")+\":\\n\"+rag_entry)",
            "    # Properties ===============================================",
            "    @property",
            "    def start_header_id_template(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return self.config.start_header_id_template",
            "",
            "    @property",
            "    def end_header_id_template(self) -> str:",
            "        \"\"\"Get the end_header_id_template.\"\"\"",
            "        return self.config.end_header_id_template",
            "    ",
            "    @property",
            "    def system_message_template(self) -> str:",
            "        \"\"\"Get the system_message_template.\"\"\"",
            "        return self.config.system_message_template",
            "",
            "",
            "    @property",
            "    def separator_template(self) -> str:",
            "        \"\"\"Get the separator template.\"\"\"",
            "        return self.config.separator_template",
            "",
            "",
            "    @property",
            "    def start_user_header_id_template(self) -> str:",
            "        \"\"\"Get the start_user_header_id_template.\"\"\"",
            "        return self.config.start_user_header_id_template",
            "    @property",
            "    def end_user_header_id_template(self) -> str:",
            "        \"\"\"Get the end_user_header_id_template.\"\"\"",
            "        return self.config.end_user_header_id_template",
            "    @property",
            "    def end_user_message_id_template(self) -> str:",
            "        \"\"\"Get the end_user_message_id_template.\"\"\"",
            "        return self.config.end_user_message_id_template",
            "",
            "",
            "",
            "",
            "    @property",
            "    def start_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the start_ai_header_id_template.\"\"\"",
            "        return self.config.start_ai_header_id_template",
            "    @property",
            "    def end_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_header_id_template.\"\"\"",
            "        return self.config.end_ai_header_id_template",
            "    @property",
            "    def end_ai_message_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_message_id_template.\"\"\"",
            "        return self.config.end_ai_message_id_template",
            "    @property",
            "    def system_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_header_id_template}{self.system_message_template}{self.end_header_id_template}\"",
            "    @property",
            "    def user_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.config.user_name}{self.end_user_header_id_template}\"",
            "    @property",
            "    def ai_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.personality.name}{self.end_user_header_id_template}\"",
            "",
            "    def system_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\"",
            "",
            "    def ai_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\"",
            "",
            "",
            "class AIPersonalityInstaller:",
            "    def __init__(self, personality:AIPersonality) -> None:",
            "        self.personality = personality",
            "",
            "",
            "class PersonalityBuilder:",
            "    def __init__(",
            "                    self,",
            "                    lollms_paths:LollmsPaths,",
            "                    config:LOLLMSConfig,",
            "                    model:LLMBinding,",
            "                    app=None,",
            "                    installation_option:InstallOption=InstallOption.INSTALL_IF_NECESSARY,",
            "                    callback=None",
            "                ):",
            "        self.config = config",
            "        self.lollms_paths = lollms_paths",
            "        self.model = model",
            "        self.app = app",
            "        self.installation_option = installation_option",
            "        self.callback = callback",
            "",
            "",
            "    def build_personality(self, id:int=None):",
            "        if id is None:",
            "            id = self.config[\"active_personality_id\"]",
            "            if self.config[\"active_personality_id\"]>=len(self.config[\"personalities\"]):",
            "                ASCIIColors.warning(\"Personality ID was out of range. Resetting to 0.\")",
            "                self.config[\"active_personality_id\"]=0",
            "                id = 0",
            "        else:",
            "            if id>len(self.config[\"personalities\"]):",
            "                id = len(self.config[\"personalities\"])-1",
            "",
            "        if \":\" in self.config[\"personalities\"][id]:",
            "            elements = self.config[\"personalities\"][id].split(\":\")",
            "            personality_folder = elements[0]",
            "            personality_language = elements[1]",
            "        else:",
            "            personality_folder = self.config[\"personalities\"][id]",
            "            personality_language = None",
            "",
            "        if len(self.config[\"personalities\"][id].split(\"/\"))==2:",
            "            self.personality = AIPersonality(",
            "                                            personality_folder,",
            "                                            self.lollms_paths,",
            "                                            self.config,",
            "                                            self.model,",
            "                                            app=self.app,",
            "                                            selected_language=personality_language,",
            "                                            installation_option=self.installation_option,",
            "                                            callback=self.callback",
            "                                        )",
            "        else:",
            "            self.personality = AIPersonality(",
            "                                            personality_folder,",
            "                                            self.lollms_paths,",
            "                                            self.config,",
            "                                            self.model,",
            "                                            app=self.app,",
            "                                            is_relative_path=False,",
            "                                            selected_language=personality_language,",
            "                                            installation_option=self.installation_option,",
            "                                            callback=self.callback",
            "                                        )",
            "        return self.personality",
            "",
            "    def get_personality(self):",
            "        return self.personality",
            "",
            "    def extract_function_call(self, query):",
            "        # Match the pattern @@function|param1|param2@@",
            "        lq = len(query)",
            "        parts = query.split(\"@@\")",
            "        if len(parts)>1:",
            "            query_ = parts[1].split(\"@@\")",
            "            query_=query_[0]",
            "            parts = query_.split(\"|\")",
            "            fn = parts[0]",
            "            if len(parts)>1:",
            "                params = parts[1:]",
            "            else:",
            "                params=[]",
            "            try:",
            "                end_pos = query.index(\"@@\")",
            "            except:",
            "                end_pos = lq",
            "            return fn, params, end_pos",
            "",
            "        else:",
            "            return None, None, 0"
        ],
        "afterPatchFile": [
            "######",
            "# Project       : lollms",
            "# File          : personality.py",
            "# Author        : ParisNeo with the help of the community",
            "# license       : Apache 2.0",
            "# Description   :",
            "# This is an interface class for lollms personalities.",
            "######",
            "from fastapi import Request",
            "from datetime import datetime",
            "from pathlib import Path",
            "from lollms.config import InstallOption, TypedConfig, BaseConfig",
            "from lollms.main_config import LOLLMSConfig",
            "from lollms.paths import LollmsPaths",
            "from lollms.binding import LLMBinding, BindingType",
            "from lollms.utilities import PromptReshaper, PackageManager, discussion_path_to_url, process_ai_output, remove_text_from_string",
            "from lollms.com import NotificationType, NotificationDisplayType",
            "from lollms.client_session import Session, Client",
            "from lollmsvectordb.vector_database import VectorDatabase",
            "from lollmsvectordb.lollms_vectorizers.bert_vectorizer import BERTVectorizer",
            "from lollmsvectordb.lollms_vectorizers.tfidf_vectorizer import TFIDFVectorizer",
            "from lollmsvectordb.text_document_loader import TextDocumentsLoader",
            "",
            "import pkg_resources",
            "from pathlib import Path",
            "from PIL import Image",
            "import re",
            "",
            "from datetime import datetime",
            "import importlib",
            "import shutil",
            "import subprocess",
            "import yaml",
            "from ascii_colors import ASCIIColors",
            "import time",
            "from lollms.types import MSG_TYPE, SUMMARY_MODE",
            "import json",
            "from typing import Any, List, Optional, Type, Callable, Dict, Any, Union",
            "import json",
            "from safe_store import TextVectorizer, GenericDataLoader, VisualizationMethod, VectorizationMethod, DocumentDecomposer",
            "from functools import partial",
            "import sys",
            "from lollms.com import LoLLMsCom",
            "from lollms.helpers import trace_exception",
            "from lollms.utilities import PackageManager",
            "",
            "from lollms.code_parser import compress_js, compress_python, compress_html",
            "",
            "",
            "import requests",
            "from bs4 import BeautifulSoup",
            "",
            "def get_element_id(url, text):",
            "    response = requests.get(url)",
            "    soup = BeautifulSoup(response.content, 'html.parser')",
            "    element = soup.find('span', text=text)",
            "    if element:",
            "        return element['id']",
            "    else:",
            "        return None",
            "",
            "def craft_a_tag_to_specific_text(url, text, caption):",
            "    # Encode the text to be used in the URL",
            "    encoded_text = text.replace(' ', '%20')",
            "",
            "    # Construct the URL with the anchor tag",
            "    anchor_url = f\"{url}#{encoded_text}\"",
            "",
            "    # Return the anchor tag",
            "    return anchor_url",
            "",
            "def is_package_installed(package_name):",
            "    try:",
            "        dist = pkg_resources.get_distribution(package_name)",
            "        return True",
            "    except pkg_resources.DistributionNotFound:",
            "        return False",
            "",
            "",
            "def install_package(package_name):",
            "    try:",
            "        # Check if the package is already installed",
            "        __import__(package_name)",
            "        print(f\"{package_name} is already installed.\")",
            "    except ImportError:",
            "        print(f\"{package_name} is not installed. Installing...\")",
            "",
            "        # Install the package using pip",
            "        subprocess.check_call([\"pip\", \"install\", package_name])",
            "",
            "        print(f\"{package_name} has been successfully installed.\")",
            "",
            "",
            "def fix_json(json_text):",
            "    try:",
            "        json_text.replace(\"}\\n{\",\"},\\n{\")",
            "        # Try to load the JSON string",
            "        json_obj = json.loads(json_text)",
            "        return json_obj",
            "    except json.JSONDecodeError as e:",
            "        trace_exception(e)",
            "class AIPersonality:",
            "",
            "    # Extra",
            "    def __init__(",
            "                    self,",
            "                    personality_package_path: str|Path,",
            "                    lollms_paths:LollmsPaths,",
            "                    config:LOLLMSConfig,",
            "                    model:LLMBinding=None,",
            "                    app:LoLLMsCom=None,",
            "                    run_scripts=True,",
            "                    selected_language=None,",
            "                    ignore_discussion_documents_rag=False,",
            "                    is_relative_path=True,",
            "                    installation_option:InstallOption=InstallOption.INSTALL_IF_NECESSARY,",
            "                    callback: Callable[[str, MSG_TYPE, dict, list], bool]=None",
            "                ):",
            "        \"\"\"",
            "        Initialize an AIPersonality instance.",
            "",
            "        Parameters:",
            "        personality_package_path (str or Path): The path to the folder containing the personality package.",
            "",
            "        Raises:",
            "        ValueError: If the provided path is not a folder or does not contain a config.yaml file.",
            "        \"\"\"",
            "        self.config = config",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        self.bot_says = \"\"",
            "",
            "        self.lollms_paths = lollms_paths",
            "        self.model = model",
            "        self.callback = callback",
            "        self.app = app",
            "",
            "        self.text_files = []",
            "        self.image_files = []",
            "        self.audio_files = []",
            "        self.images_descriptions = []",
            "        self.vectorizer = None",
            "",
            "        self.installation_option = installation_option",
            "",
            "        # Whisper to transcribe audio",
            "        self.whisper = None",
            "",
            "        # First setup a default personality",
            "        # Version",
            "        self._version = pkg_resources.get_distribution('lollms').version",
            "",
            "        self.run_scripts = run_scripts",
            "",
            "        #General information",
            "        self._author: str = \"ParisNeo\"",
            "        self._name: str = \"lollms\"",
            "        self._user_name: str = \"user\"",
            "        self._category: str = \"General\"",
            "        self._category_desc: str = \"General\"",
            "        self._language: str = \"english\"",
            "        self._supported_languages: str = []",
            "        self._selected_language: str = selected_language",
            "        self._ignore_discussion_documents_rag:bool = ignore_discussion_documents_rag",
            "",
            "        self._languages: List[dict]=[]",
            "",
            "",
            "",
            "        # Conditionning",
            "        self._personality_description: str = \"This personality is a helpful and Kind AI ready to help you solve your problems\"",
            "        self._personality_conditioning: str = \"\\n\".join([",
            "            \"lollms (Lord of LLMs) is a smart and helpful Assistant built by the computer geek ParisNeo.\",",
            "            \"It is compatible with many bindings to LLM models such as llama, gpt4all, gptj, autogptq etc.\",",
            "            \"It can discuss with humans and assist them on many subjects.\",",
            "            \"It runs locally on your machine. No need to connect to the internet.\",",
            "            \"It answers the questions with precise details\",",
            "            \"Its performance depends on the underlying model size and training.\",",
            "            \"Try to answer with as much details as you can\",",
            "            \"Date: {{date}}\",",
            "        ])",
            "        self._welcome_message: str = \"Welcome! I am lollms (Lord of LLMs) A free and open assistant built by ParisNeo. What can I do for you today?\"",
            "        self._include_welcome_message_in_discussion: bool = True",
            "        self._user_message_prefix: str = f\"human:\"",
            "        self._link_text: str = \"\\n\"",
            "        self._ai_message_prefix: str = f\"lollms:\"",
            "",
            "        # Extra",
            "        self._dependencies: List[str] = []",
            "",
            "        # Disclaimer",
            "        self._disclaimer: str = \"\"",
            "        self._help: str = \"\"",
            "        self._commands: list = []",
            "",
            "        # Default model parameters",
            "        self._model_temperature: float = 0.1 # higher: more creative, lower more deterministic",
            "        self._model_top_k: int = 50",
            "        self._model_top_p: float = 0.95",
            "        self._model_repeat_penalty: float = 1.3",
            "        self._model_repeat_last_n: int = 40",
            "",
            "        self._processor_cfg: dict = {}",
            "",
            "        self._logo: Optional[Image.Image] = None",
            "        self._processor = None",
            "        self._data = None",
            "",
            "",
            "",
            "        if personality_package_path is None:",
            "            self.config = {}",
            "            self.assets_list = []",
            "            self.personality_package_path = None",
            "            return",
            "        else:",
            "            parts = str(personality_package_path).split(\"/\")",
            "            self._category = parts[0]",
            "            if parts[0] == \"custom_personalities\":",
            "                self.personality_package_path = self.lollms_paths.custom_personalities_path/parts[1]",
            "            else:",
            "                if is_relative_path:",
            "                    self.personality_package_path = self.lollms_paths.personalities_zoo_path/personality_package_path",
            "                else:",
            "                    self.personality_package_path = Path(personality_package_path)",
            "",
            "            # Validate that the path exists",
            "            if not self.personality_package_path.exists():",
            "                raise ValueError(f\"Could not find the personality package:{self.personality_package_path}\")",
            "",
            "            # Validate that the path format is OK with at least a config.yaml file present in the folder",
            "            if not self.personality_package_path.is_dir():",
            "                raise ValueError(f\"Personality package path is not a folder:{self.personality_package_path}\")",
            "",
            "            self.personality_folder_name = self.personality_package_path.stem",
            "",
            "",
            "            self.personality_output_folder = lollms_paths.personal_outputs_path/self.name",
            "            self.personality_output_folder.mkdir(parents=True, exist_ok=True)",
            "            # Open and store the personality",
            "            self.load_personality()",
            "",
            "",
            "",
            "    def InfoMessage(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.InfoMessage(content=content, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.white(content)",
            "",
            "    def ShowBlockingMessage(self, content, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.ShowBlockingMessage(content=content, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.white(content)",
            "",
            "    def HideBlockingMessage(self, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.HideBlockingMessage(client_id=client_id, verbose=verbose)",
            "",
            "",
            "    def info(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.info(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.info(content)",
            "",
            "    def warning(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.warning(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.warning(content)",
            "",
            "    def success(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.success(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.success(content)",
            "",
            "    def error(self, content, duration:int=4, client_id=None, verbose:bool=True):",
            "        if self.app:",
            "            return self.app.error(content=content, duration=duration, client_id=client_id, verbose=verbose)",
            "        ASCIIColors.error(content)",
            "",
            "    def notify( self,",
            "                content,",
            "                notification_type:NotificationType=NotificationType.NOTIF_SUCCESS,",
            "                duration:int=4,",
            "                client_id=None,",
            "                display_type:NotificationDisplayType=NotificationDisplayType.TOAST,",
            "                verbose=True",
            "            ):",
            "        if self.app:",
            "            return self.app.error(content=content, notification_type=notification_type, duration=duration, client_id=client_id, display_type=display_type, verbose=verbose)",
            "        ASCIIColors.white(content)",
            "",
            "",
            "",
            "    def new_message(self, message_text:str, message_type:MSG_TYPE= MSG_TYPE.MSG_TYPE_FULL, metadata=[], callback: Callable[[str, int, dict, list, Any], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(message_text, MSG_TYPE.MSG_TYPE_NEW_MESSAGE, parameters={'type':message_type.value,'metadata':metadata}, personality=self)",
            "",
            "    def full(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL)",
            "",
            "    def ui(self, ui_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends ui text to front end",
            "",
            "        Args:",
            "            ui_text (dict): The ui code to be sent to the front end",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(ui_text, MSG_TYPE.MSG_TYPE_UI)",
            "",
            "",
            "    def full_invisible_to_ai(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to AI)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI)",
            "",
            "    def full_invisible_to_user(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to user)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER)",
            "",
            "",
            "    def build_prompt(self, prompt_parts:List[str], sacrifice_id:int=-1, context_size:int=None, minimum_spare_context_size:int=None):",
            "        \"\"\"",
            "        Builds the prompt for code generation.",
            "",
            "        Args:",
            "            prompt_parts (List[str]): A list of strings representing the parts of the prompt.",
            "            sacrifice_id (int, optional): The ID of the part to sacrifice.",
            "            context_size (int, optional): The size of the context.",
            "            minimum_spare_context_size (int, optional): The minimum spare context size.",
            "",
            "        Returns:",
            "            str: The built prompt.",
            "        \"\"\"",
            "        if context_size is None:",
            "            context_size = self.config.ctx_size",
            "        if minimum_spare_context_size is None:",
            "            minimum_spare_context_size = self.config.min_n_predict",
            "",
            "        if sacrifice_id == -1 or len(prompt_parts[sacrifice_id])<50:",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "        else:",
            "            part_tokens=[]",
            "            nb_tokens=0",
            "            for i,part in enumerate(prompt_parts):",
            "                tk = self.model.tokenize(part)",
            "                part_tokens.append(tk)",
            "                if i != sacrifice_id:",
            "                    nb_tokens += len(tk)",
            "            if len(part_tokens[sacrifice_id])>0:",
            "                sacrifice_tk = part_tokens[sacrifice_id]",
            "                sacrifice_tk= sacrifice_tk[-(context_size-nb_tokens-minimum_spare_context_size):]",
            "                sacrifice_text = self.model.detokenize(sacrifice_tk)",
            "            else:",
            "                sacrifice_text = \"\"",
            "            prompt_parts[sacrifice_id] = sacrifice_text",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "",
            "    def add_collapsible_entry(self, title, content):",
            "        return \"\\n\".join(",
            "        [",
            "        f'<details class=\"flex w-fit rounded-xl border border-gray-200 bg-white shadow-sm dark:border-gray-800 dark:bg-gray-900 mb-3.5 max-w-full svelte-1escu1z\" open=\"\">',",
            "        f'    <summary class=\"grid min-w-72 select-none grid-cols-[40px,1fr] items-center gap-2.5 p-2 svelte-1escu1z\">',",
            "        f'        <dl class=\"leading-4\">',",
            "        f'        <dd class=\"text-sm\">{title}</dd>',",
            "        f'        <dt class=\"flex items-center gap-1 truncate whitespace-nowrap text-[.82rem] text-gray-400\">.Completed</dt>',",
            "        f'        </dl>',",
            "        f'    </summary>',",
            "        f' <div class=\"content px-5 pb-5 pt-4\">',",
            "        content,",
            "        f' </div>',",
            "        f' </details>\\n'",
            "        ])",
            "",
            "    def internet_search_with_vectorization(self, query, quick_search:bool=False, asses_using_llm=True):",
            "        \"\"\"",
            "        Do internet search and return the result",
            "        \"\"\"",
            "        from lollms.internet import internet_search_with_vectorization",
            "        return internet_search_with_vectorization(",
            "                                                    query,",
            "                                                    internet_nb_search_pages=int(self.config.internet_nb_search_pages),",
            "                                                    internet_vectorization_chunk_size=int(self.config.internet_vectorization_chunk_size),",
            "                                                    internet_vectorization_overlap_size=int(self.config.internet_vectorization_overlap_size),",
            "                                                    internet_vectorization_nb_chunks=int(self.config.internet_vectorization_nb_chunks),",
            "                                                    model = self.model,",
            "                                                    quick_search=quick_search,",
            "                                                    asses_using_llm=asses_using_llm,",
            "                                                    yes_no = self.yes_no",
            "                                                    )",
            "",
            "    def sink(self, s=None,i=None,d=None):",
            "        pass",
            "",
            "    def yes_no(self, question: str, context:str=\"\", max_answer_length: int = 50, conditionning=\"\") -> bool:",
            "        \"\"\"",
            "        Analyzes the user prompt and answers whether it is asking to generate an image.",
            "",
            "        Args:",
            "            question (str): The user's message.",
            "            max_answer_length (int, optional): The maximum length of the generated answer. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "        Returns:",
            "            bool: True if the user prompt is asking to generate an image, False otherwise.",
            "        \"\"\"",
            "        return self.multichoice_question(question, [\"no\",\"yes\"], context, max_answer_length, conditionning=conditionning)>0",
            "",
            "    def multichoice_question(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Interprets a multi-choice question from a users response. This function expects only one choice as true. All other choices are considered false. If none are correct, returns -1.",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                       f\"{start_header_id_template}context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "        elements += [",
            "                f\"{start_header_id_template}question{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50, callback=self.sink).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        selection = gen.strip().split()[0].replace(\",\",\"\").replace(\".\",\"\")",
            "        self.print_prompt(\"Multi choice selection\",prompt+gen)",
            "        try:",
            "            return int(selection)",
            "        except:",
            "            ASCIIColors.cyan(\"Model failed to answer the question\")",
            "            return -1",
            "",
            "    def multichoice_ranking(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Ranks answers for a question from best to worst. returns a list of integers",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "                f\"{start_header_id_template}{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                       f\"{start_header_id_template}context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        self.print_prompt(\"Multi choice ranking\",prompt+gen)",
            "        if gen.index(\"]\")>=0:",
            "            try:",
            "                ranks = eval(gen.split(\"]\")[0]+\"]\")",
            "                return ranks",
            "            except:",
            "                ASCIIColors.red(\"Model failed to rank inputs\")",
            "                return None",
            "        else:",
            "            ASCIIColors.red(\"Model failed to rank inputs\")",
            "            return None",
            "",
            "    def step_start(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step start",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step start to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_START)",
            "",
            "    def step_end(self, step_text, status=True, callback: Callable[[str, int, dict, list], bool]=None):",
            "        \"\"\"This triggers a step end",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step end to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_END, {'status':status})",
            "",
            "    def step(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step information",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP)",
            "",
            "    def print_prompt(self, title, prompt):",
            "        ASCIIColors.red(\"*-*-*-*-*-*-*-* \", end=\"\")",
            "        ASCIIColors.red(title, end=\"\")",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "        ASCIIColors.yellow(prompt)",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "",
            "",
            "",
            "    def fast_gen_with_images(self, prompt: str, images:list, max_generation_size: int=None, placeholders: dict = {}, sacrifice: list = [\"previous_discussion\"], debug: bool  = False, callback=None, show_progress=False) -> str:",
            "        \"\"\"",
            "        Fast way to generate text from text and images",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        prompt = \"\\n\".join([",
            "            f\"{start_header_id_template}{system_message_template}{end_header_id_template}I am an AI assistant that can converse and analyze images. When asked to locate something in an image you send, I will reply with:\",",
            "            \"boundingbox(image_index, label, left, top, width, height)\",",
            "            \"Where:\",",
            "            \"image_index: 0-based index of the image\",",
            "            \"label: brief description of what is located\",",
            "            \"left, top: x,y coordinates of top-left box corner (0-1 scale)\",",
            "            \"width, height: box dimensions as fraction of image size\",",
            "            \"Coordinates have origin (0,0) at top-left, (1,1) at bottom-right.\",",
            "            \"For other queries, I will respond conversationally to the best of my abilities.\",",
            "            prompt",
            "        ])",
            "        if debug == False:",
            "            debug = self.config.debug",
            "",
            "        if max_generation_size is None:",
            "            prompt_size = self.model.tokenize(prompt)",
            "            max_generation_size = self.model.config.ctx_size - len(prompt_size)",
            "",
            "        pr = PromptReshaper(prompt)",
            "        prompt = pr.build(placeholders,",
            "                        self.model.tokenize,",
            "                        self.model.detokenize,",
            "                        self.model.config.ctx_size - max_generation_size,",
            "                        sacrifice",
            "                        )",
            "        ntk = len(self.model.tokenize(prompt))",
            "        max_generation_size = min(self.model.config.ctx_size - ntk, max_generation_size)",
            "        # TODO : add show progress",
            "",
            "        gen = self.generate_with_images(prompt, images, max_generation_size, callback=callback, show_progress=show_progress).strip().replace(\"</s>\", \"\").replace(\"<s>\", \"\")",
            "        try:",
            "            gen = process_ai_output(gen, images, \"/discussions/\")",
            "        except Exception as ex:",
            "            pass",
            "        if debug:",
            "            self.print_prompt(\"prompt\", prompt+gen)",
            "",
            "        return gen",
            "",
            "    def fast_gen(",
            "                    self, ",
            "                    prompt: str, ",
            "                    max_generation_size: int=None, ",
            "                    placeholders: dict = {}, ",
            "                    sacrifice: list = [\"previous_discussion\"], ",
            "                    debug: bool  = False, ",
            "                    callback=None, ",
            "                    show_progress=False, ",
            "                    temperature = None, ",
            "                    top_k = None, ",
            "                    top_p=None, ",
            "                    repeat_penalty=None, ",
            "                    repeat_last_n=None",
            "                ) -> str:",
            "        \"\"\"",
            "        Fast way to generate code",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        if debug == False:",
            "            debug = self.config.debug",
            "",
            "        if max_generation_size is None:",
            "            prompt_size = self.model.tokenize(prompt)",
            "            max_generation_size = self.model.config.ctx_size - len(prompt_size)",
            "",
            "        pr = PromptReshaper(prompt)",
            "        prompt = pr.build(placeholders,",
            "                        self.model.tokenize,",
            "                        self.model.detokenize,",
            "                        self.model.config.ctx_size - max_generation_size,",
            "                        sacrifice",
            "                        )",
            "        ntk = len(self.model.tokenize(prompt))",
            "        max_generation_size = min(self.model.config.ctx_size - ntk, max_generation_size)",
            "        # TODO : add show progress",
            "",
            "        gen = self.generate(prompt, max_generation_size, temperature = temperature, top_k = top_k, top_p=top_p, repeat_penalty=repeat_penalty, repeat_last_n=repeat_last_n, callback=callback, show_progress=show_progress).strip().replace(\"</s>\", \"\").replace(\"<s>\", \"\")",
            "",
            "        return gen",
            "",
            "",
            "",
            "    def process(self, text:str, message_type:MSG_TYPE, callback=None, show_progress=False):",
            "        if callback is None:",
            "            callback = self.callback",
            "        if text is None:",
            "            return True",
            "        if message_type==MSG_TYPE.MSG_TYPE_CHUNK:",
            "            bot_says = self.bot_says + text",
            "        elif  message_type==MSG_TYPE.MSG_TYPE_FULL:",
            "            bot_says = text",
            "",
            "        if show_progress:",
            "            if self.nb_received_tokens==0:",
            "                self.start_time = datetime.now()",
            "            dt =(datetime.now() - self.start_time).seconds",
            "            if dt==0:",
            "                dt=1",
            "            spd = self.nb_received_tokens/dt",
            "            ASCIIColors.green(f\"Received {self.nb_received_tokens} tokens (speed: {spd:.2f}t/s)              \",end=\"\\r\",flush=True)",
            "            sys.stdout = sys.__stdout__",
            "            sys.stdout.flush()",
            "            self.nb_received_tokens+=1",
            "",
            "",
            "        antiprompt = self.detect_antiprompt(bot_says)",
            "        if antiprompt:",
            "            self.bot_says = remove_text_from_string(bot_says,antiprompt)",
            "            ASCIIColors.warning(f\"\\n{antiprompt} detected. Stopping generation\")",
            "            return False",
            "        else:",
            "            if callback:",
            "                callback(text,message_type)",
            "            self.bot_says = bot_says",
            "            return True",
            "",
            "    def generate_with_images(self, prompt, images, max_size, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False, show_progress=False ):",
            "        ASCIIColors.info(\"Text generation started: Warming up\")",
            "        self.nb_received_tokens = 0",
            "        self.bot_says = \"\"",
            "        if debug:",
            "            self.print_prompt(\"gen\",prompt)",
            "",
            "        self.model.generate_with_images(",
            "                                prompt,",
            "                                images,",
            "                                max_size,",
            "                                partial(self.process, callback=callback, show_progress=show_progress),",
            "                                temperature=self.model_temperature if temperature is None else temperature,",
            "                                top_k=self.model_top_k if top_k is None else top_k,",
            "                                top_p=self.model_top_p if top_p is None else top_p,",
            "                                repeat_penalty=self.model_repeat_penalty if repeat_penalty is None else repeat_penalty,",
            "                                repeat_last_n = self.model_repeat_last_n if repeat_last_n is None else repeat_last_n",
            "                                ).strip()",
            "        return self.bot_says",
            "",
            "    def generate(self, prompt, max_size = None, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False, show_progress=False ):",
            "        ASCIIColors.info(\"Text generation started: Warming up\")",
            "        self.nb_received_tokens = 0",
            "        self.bot_says = \"\"",
            "        if debug:",
            "            self.print_prompt(\"gen\",prompt)",
            "        self.model.generate(",
            "                                prompt,",
            "                                max_size if max_size else (self.config.ctx_size-len(self.model.tokenize(prompt))),",
            "                                partial(self.process, callback=callback, show_progress=show_progress),",
            "                                temperature=self.model_temperature if temperature is None else temperature,",
            "                                top_k=self.model_top_k if top_k is None else top_k,",
            "                                top_p=self.model_top_p if top_p is None else top_p,",
            "                                repeat_penalty=self.model_repeat_penalty if repeat_penalty is None else repeat_penalty,",
            "                                repeat_last_n = self.model_repeat_last_n if repeat_last_n is None else repeat_last_n,",
            "                                ).strip()",
            "        if debug:",
            "            self.print_prompt(\"prompt\", prompt+self.bot_says)",
            "        ",
            "        return self.bot_says",
            "",
            "    def setCallback(self, callback: Callable[[str, MSG_TYPE, dict, list], bool]):",
            "        self.callback = callback",
            "        if self._processor:",
            "            self._processor.callback = callback",
            "",
            "",
            "    def __str__(self):",
            "        return f\"{self.category}/{self.name}\"",
            "",
            "",
            "    def load_personality(self, package_path=None):",
            "        \"\"\"",
            "        Load personality parameters from a YAML configuration file.",
            "",
            "        Args:",
            "            package_path (str or Path): The path to the package directory.",
            "",
            "        Raises:",
            "            ValueError: If the configuration file does not exist.",
            "        \"\"\"",
            "        if package_path is None:",
            "            package_path = self.personality_package_path",
            "        else:",
            "            package_path = Path(package_path)",
            "",
            "        # Verify that there is at least a configuration file",
            "        config_file = package_path / \"config.yaml\"",
            "        if not config_file.exists():",
            "            raise ValueError(f\"The provided folder {package_path} does not exist.\")",
            "",
            "        with open(config_file, \"r\", encoding='utf-8') as f:",
            "            config = yaml.safe_load(f)",
            "",
            "        secret_file = package_path / \"secret.yaml\"",
            "        if secret_file.exists():",
            "            with open(secret_file, \"r\", encoding='utf-8') as f:",
            "                self._secret_cfg = yaml.safe_load(f)",
            "        else:",
            "            self._secret_cfg = None",
            "",
            "        languages = package_path / \"languages\"",
            "",
            "        if languages.exists():",
            "            self._supported_languages = []",
            "            for language in [l for l in languages.iterdir()]:",
            "                self._supported_languages.append(language.stem)",
            "",
            "            if self._selected_language is not None and self._selected_language in self._supported_languages:",
            "                config_file = languages / (self._selected_language+\".yaml\")",
            "                with open(config_file, \"r\", encoding='utf-8') as f:",
            "                    config = yaml.safe_load(f)",
            "",
            "",
            "",
            "        # Load parameters from the configuration file",
            "        self._version = config.get(\"version\", self._version)",
            "        self._author = config.get(\"author\", self._author)",
            "        self._name = config.get(\"name\", self._name)",
            "        self._user_name = config.get(\"user_name\", self._user_name)",
            "        self._category_desc = config.get(\"category\", self._category)",
            "        self._language = config.get(\"language\", self._language)",
            "",
            "        self._ignore_discussion_documents_rag = config.get(\"ignore_discussion_documents_rag\", self._ignore_discussion_documents_rag)",
            "",
            "",
            "        self._personality_description = config.get(\"personality_description\", self._personality_description)",
            "        self._personality_conditioning = config.get(\"personality_conditioning\", self._personality_conditioning)",
            "        self._welcome_message = config.get(\"welcome_message\", self._welcome_message)",
            "        self._include_welcome_message_in_discussion = config.get(\"include_welcome_message_in_discussion\", self._include_welcome_message_in_discussion)",
            "",
            "        self._user_message_prefix = config.get(\"user_message_prefix\", self._user_message_prefix)",
            "        self._link_text = config.get(\"link_text\", self._link_text)",
            "        self._ai_message_prefix = config.get(\"ai_message_prefix\", self._ai_message_prefix)",
            "        self._dependencies = config.get(\"dependencies\", self._dependencies)",
            "        self._disclaimer = config.get(\"disclaimer\", self._disclaimer)",
            "        self._help = config.get(\"help\", self._help)",
            "        self._commands = config.get(\"commands\", self._commands)",
            "        self._model_temperature = config.get(\"model_temperature\", self._model_temperature)",
            "        self._model_top_k = config.get(\"model_top_k\", self._model_top_k)",
            "        self._model_top_p = config.get(\"model_top_p\", self._model_top_p)",
            "        self._model_repeat_penalty = config.get(\"model_repeat_penalty\", self._model_repeat_penalty)",
            "        self._model_repeat_last_n = config.get(\"model_repeat_last_n\", self._model_repeat_last_n)",
            "",
            "        # Script parameters (for example keys to connect to search engine or any other usage)",
            "        self._processor_cfg = config.get(\"processor_cfg\", self._processor_cfg)",
            "",
            "",
            "        #set package path",
            "        self.personality_package_path = package_path",
            "",
            "        # Check for a logo file",
            "        self.logo_path = self.personality_package_path / \"assets\" / \"logo.png\"",
            "        if self.logo_path.is_file():",
            "            self._logo = Image.open(self.logo_path)",
            "",
            "        # Get the assets folder path",
            "        self.assets_path = self.personality_package_path / \"assets\"",
            "        # Get the scripts folder path",
            "        self.scripts_path = self.personality_package_path / \"scripts\"",
            "        # Get the languages folder path",
            "        self.languages_path = self.personality_package_path / \"languages\"",
            "        # Get the data folder path",
            "        self.data_path = self.personality_package_path / \"data\"",
            "        # Get the data folder path",
            "        self.audio_path = self.personality_package_path / \"audio\"",
            "        # Get the data folder path",
            "        self.welcome_audio_path = self.personality_package_path / \"welcome_audio\"",
            "",
            "",
            "        # If not exist recreate",
            "        self.assets_path.mkdir(parents=True, exist_ok=True)",
            "",
            "        # If not exist recreate",
            "        self.scripts_path.mkdir(parents=True, exist_ok=True)",
            "",
            "        # If not exist recreate",
            "        self.audio_path.mkdir(parents=True, exist_ok=True)",
            "",
            "        # samples",
            "        self.audio_samples = [f for f in self.audio_path.iterdir()]",
            "",
            "        # Verify if the persona has a data folder",
            "        if self.data_path.exists():",
            "            self.database_path = self.data_path / \"db.json\"",
            "            if self.database_path.exists():",
            "                ASCIIColors.info(\"Loading database ...\",end=\"\")",
            "                self.persona_data_vectorizer = TextVectorizer(",
            "                            \"tfidf_vectorizer\", # self.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\"",
            "                            model=self.model, #needed in case of using model_embedding",
            "                            save_db=True,",
            "                            database_path=self.database_path,",
            "                            data_visualization_method=VisualizationMethod.PCA,",
            "                            database_dict=None)",
            "                ASCIIColors.green(\"Ok\")",
            "            else:",
            "                files = [f for f in self.data_path.iterdir() if f.suffix.lower() in ['.asm', '.bat', '.c', '.cpp', '.cs', '.csproj', '.css',",
            "                    '.csv', '.docx', '.h', '.hh', '.hpp', '.html', '.inc', '.ini', '.java', '.js', '.json', '.log',",
            "                    '.lua', '.map', '.md', '.pas', '.pdf', '.php', '.pptx', '.ps1', '.py', '.rb', '.rtf', '.s', '.se', '.sh', '.sln',",
            "                    '.snippet', '.snippets', '.sql', '.sym', '.ts', '.txt', '.xlsx', '.xml', '.yaml', '.yml', '.msg'] ]",
            "                if len(files)>0:",
            "                    dl = GenericDataLoader()",
            "                    self.persona_data_vectorizer = TextVectorizer(",
            "                                \"tfidf_vectorizer\", # self.config.data_vectorization_method, # supported \"model_embedding\" or \"tfidf_vectorizer\"",
            "                                model=self.model, #needed in case of using model_embedding",
            "                                save_db=True,",
            "                                database_path=self.database_path,",
            "                                data_visualization_method=VisualizationMethod.PCA,",
            "                                database_dict=None)",
            "                    for f in files:",
            "                        text = dl.read_file(f)",
            "                        self.persona_data_vectorizer.add_document(f.name,text,self.config.data_vectorization_chunk_size, self.config.data_vectorization_overlap_size)",
            "                        # data_vectorization_chunk_size: 512 # chunk size",
            "                        # data_vectorization_overlap_size: 128 # overlap between chunks size",
            "                        # data_vectorization_nb_chunks: 2 # number of chunks to use",
            "                    self.persona_data_vectorizer.index()",
            "                    self.persona_data_vectorizer.save_db()",
            "                else:",
            "                    self.persona_data_vectorizer = None",
            "                    self._data = None",
            "",
            "        else:",
            "            self.persona_data_vectorizer = None",
            "            self._data = None",
            "",
            "        self.personality_output_folder = self.lollms_paths.personal_outputs_path/self.name",
            "        self.personality_output_folder.mkdir(parents=True, exist_ok=True)",
            "",
            "",
            "        if self.run_scripts:",
            "            # Search for any processor code",
            "            processor_file_name = \"processor.py\"",
            "            self.processor_script_path = self.scripts_path / processor_file_name",
            "            if self.processor_script_path.exists():",
            "                module_name = processor_file_name[:-3]  # Remove the \".py\" extension",
            "                module_spec = importlib.util.spec_from_file_location(module_name, str(self.processor_script_path))",
            "                module = importlib.util.module_from_spec(module_spec)",
            "                module_spec.loader.exec_module(module)",
            "                if hasattr(module, \"Processor\"):",
            "                    self._processor = module.Processor(self, callback=self.callback)",
            "                else:",
            "                    self._processor = None",
            "            else:",
            "                self._processor = None",
            "        # Get a list of all files in the assets folder",
            "        contents = [str(file) for file in self.assets_path.iterdir() if file.is_file()]",
            "",
            "        self._assets_list = contents",
            "        return config",
            "",
            "",
            "",
            "    def remove_file(self, file_name, callback=None):",
            "        try:",
            "            if any(file_name == entry.name for entry in self.text_files):",
            "                fn = [entry for entry in self.text_files if entry.name == file_name][0]",
            "                self.text_files = [entry for entry in self.text_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "                if len(self.text_files)>0:",
            "                    try:",
            "                        self.vectorizer.remove_document(fn)",
            "                        if callback is not None:",
            "                            callback(\"File removed successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                        return True",
            "                    except ValueError as ve:",
            "                        ASCIIColors.error(f\"Couldn't remove the file\")",
            "                        return False",
            "                else:",
            "                    self.vectorizer = None",
            "            elif any(file_name == entry.name for entry in self.image_files):",
            "                fn = [entry for entry in self.image_files if entry.name == file_name][0]",
            "                self.text_files = [entry for entry in self.image_files if entry.name != file_name]",
            "                Path(fn).unlink()",
            "",
            "        except Exception as ex:",
            "            ASCIIColors.warning(f\"Couldn't remove the file {file_name}\")",
            "",
            "    def remove_all_files(self, callback=None):",
            "        for file in self.text_files:",
            "            try:",
            "                Path(file).unlink()",
            "            except Exception as ex:",
            "                ASCIIColors.warning(f\"Couldn't remove the file {file}\")",
            "        for file in self.image_files:",
            "            try:",
            "                Path(file).unlink()",
            "            except Exception as ex:",
            "                ASCIIColors.warning(f\"Couldn't remove the file {file}\")",
            "        self.text_files=[]",
            "        self.image_files=[]",
            "        self.vectorizer = None",
            "        return True",
            "",
            "    def add_file(self, path, client:Client, callback=None, process=True):",
            "        output = \"\"",
            "        if not self.callback:",
            "            self.callback = callback",
            "",
            "        path = Path(path)",
            "        if path.suffix in [\".wav\",\".mp3\"]:",
            "            self.audio_files.append(path)",
            "            if process:",
            "                self.new_message(\"\")",
            "                self.ShowBlockingMessage(f\"Transcribing ... \")",
            "                if self.app.stt is None:",
            "                    self.InfoMessage(\"No STT service is up.\\nPlease configure your default STT service in the settings page.\")",
            "                    return",
            "                text = self.app.stt.transcribe(str(path))",
            "                transcription_fn = str(path)+\".txt\"",
            "                with open(transcription_fn, \"w\", encoding=\"utf-8\") as f:",
            "                    f.write(text)",
            "",
            "                self.info(f\"File saved to {transcription_fn}\")",
            "                self.full(text)",
            "        elif path.suffix in [\".png\",\".jpg\",\".jpeg\",\".gif\",\".bmp\",\".svg\",\".webp\"]:",
            "            self.image_files.append(path)",
            "            if process:",
            "                if self.callback:",
            "                    try:",
            "                        pth = str(path).replace(\"\\\\\",\"/\").split('/')",
            "                        if \"discussion_databases\" in pth:",
            "                            pth = discussion_path_to_url(path)",
            "                            self.new_message(\"\",MSG_TYPE.MSG_TYPE_FULL)",
            "                            output = f'<img src=\"{pth}\" width=\"800\">\\n\\n'",
            "                            self.full(output)",
            "                            self.app.close_message(client.client_id if client is not None else 0)",
            "",
            "                        if self.model.binding_type not in [BindingType.TEXT_IMAGE, BindingType.TEXT_IMAGE_VIDEO]:",
            "                            # self.ShowBlockingMessage(\"Understanding image (please wait)\")",
            "                            from PIL import Image",
            "                            img = Image.open(str(path))",
            "                            # Convert the image to RGB mode",
            "                            img = img.convert(\"RGB\")",
            "                            output += \"## image description :\\n\"+ self.model.interrogate_blip([img])[0]",
            "                            # output += \"## image description :\\n\"+ self.model.qna_blip([img],\"q:Describe this photo with as much details as possible.\\na:\")[0]",
            "                            self.full(output)",
            "                            self.app.close_message(client.client_id if client is not None else 0)",
            "                            self.HideBlockingMessage(\"Understanding image (please wait)\")",
            "                            if self.config.debug:",
            "                                ASCIIColors.yellow(output)",
            "                        else:",
            "                            # self.ShowBlockingMessage(\"Importing image (please wait)\")",
            "                            self.HideBlockingMessage(\"Importing image (please wait)\")",
            "",
            "                    except Exception as ex:",
            "                        trace_exception(ex)",
            "                        self.HideBlockingMessage(\"Understanding image (please wait)\", False)",
            "                        ASCIIColors.error(\"Couldn't create new message\")",
            "            ASCIIColors.info(\"Received image file\")",
            "            if callback is not None:",
            "                callback(\"Image file added successfully\", MSG_TYPE.MSG_TYPE_INFO)",
            "        else:",
            "            try:",
            "                # self.ShowBlockingMessage(\"Adding file to vector store.\\nPlease stand by\")",
            "                self.text_files.append(path)",
            "                ASCIIColors.info(\"Received text compatible file\")",
            "                self.ShowBlockingMessage(\"Processing file\\nPlease wait ...\")",
            "                if process:",
            "                    if self.vectorizer is None:",
            "                        self.vectorizer = VectorDatabase(",
            "                                    client.discussion.discussion_rag_folder/\"db.sqli\",",
            "                                    BERTVectorizer(self.config.rag_vectorizer_model) if self.config.rag_vectorizer==\"bert\" else TFIDFVectorizer(),",
            "                                    self.model,",
            "                                    chunk_size=self.config.rag_chunk_size,",
            "                                    overlap=self.config.rag_overlap",
            "                                    )",
            "                    data = TextDocumentsLoader.read_file(path)",
            "                    self.vectorizer.add_document(path.stem, data, path, True)",
            "                    self.vectorizer.build_index()",
            "                    if callback is not None:",
            "                        callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "                    self.HideBlockingMessage(client.client_id)",
            "                    return True",
            "            except Exception as e:",
            "                trace_exception(e)",
            "                self.InfoMessage(f\"Unsupported file format or empty file.\\nSupported formats are {GenericDataLoader.get_supported_file_types()}\",client_id=client.client_id)",
            "                return False",
            "    def save_personality(self, package_path=None):",
            "        \"\"\"",
            "        Save the personality parameters to a YAML configuration file.",
            "",
            "        Args:",
            "            package_path (str or Path): The path to the package directory.",
            "        \"\"\"",
            "        if package_path is None:",
            "            package_path = self.personality_package_path",
            "        else:",
            "            package_path = Path(package_path)",
            "",
            "        # Building output path",
            "        config_file = package_path / \"config.yaml\"",
            "        assets_folder = package_path / \"assets\"",
            "",
            "        # Create assets folder if it doesn't exist",
            "        if not assets_folder.exists():",
            "            assets_folder.mkdir(exist_ok=True, parents=True)",
            "",
            "        # Create the configuration dictionary",
            "        config = {",
            "            \"author\": self._author,",
            "            \"version\": self._version,",
            "            \"name\": self._name,",
            "            \"user_name\": self._user_name,",
            "            \"category\": self._category,",
            "            \"language\": self._language,",
            "            \"supported_languages\": self._supported_languages,",
            "            \"selected_language\": self._selected_language,",
            "            \"ignore_discussion_documents_rag\": self._ignore_discussion_documents_rag,",
            "            \"personality_description\": self._personality_description,",
            "            \"personality_conditioning\": self._personality_conditioning,",
            "            \"welcome_message\": self._welcome_message,",
            "            \"include_welcome_message_in_discussion\": self._include_welcome_message_in_discussion,",
            "            \"user_message_prefix\": self._user_message_prefix,",
            "            \"link_text\": self._link_text,",
            "            \"ai_message_prefix\": self._ai_message_prefix,",
            "            \"dependencies\": self._dependencies,",
            "            \"disclaimer\": self._disclaimer,",
            "            \"help\": self._help,",
            "            \"commands\": self._commands,",
            "            \"model_temperature\": self._model_temperature,",
            "            \"model_top_k\": self._model_top_k,",
            "            \"model_top_p\": self._model_top_p,",
            "            \"model_repeat_penalty\": self._model_repeat_penalty,",
            "            \"model_repeat_last_n\": self._model_repeat_last_n",
            "        }",
            "",
            "        # Save the configuration to the YAML file",
            "        with open(config_file, \"w\") as f:",
            "            yaml.dump(config, f)",
            "",
            "",
            "",
            "    def as_dict(self):",
            "        \"\"\"",
            "        Convert the personality parameters to a dictionary.",
            "",
            "        Returns:",
            "            dict: The personality parameters as a dictionary.",
            "        \"\"\"",
            "        return {",
            "            \"author\": self._author,",
            "            \"version\": self._version,",
            "            \"name\": self._name,",
            "            \"user_name\": self._user_name,",
            "            \"category\": self._category,",
            "            \"language\": self._language,",
            "            \"supported_languages\": self._supported_languages,",
            "            \"selected_language\": self._selected_language,",
            "            \"ignore_discussion_documents_rag\": self._ignore_discussion_documents_rag,",
            "            \"personality_description\": self._personality_description,",
            "            \"personality_conditioning\": self._personality_conditioning,",
            "            \"welcome_message\": self._welcome_message,",
            "            \"include_welcome_message_in_discussion\": self._include_welcome_message_in_discussion,",
            "            \"user_message_prefix\": self._user_message_prefix,",
            "            \"link_text\": self._link_text,",
            "            \"ai_message_prefix\": self._ai_message_prefix,",
            "            \"dependencies\": self._dependencies,",
            "            \"disclaimer\": self._disclaimer,",
            "            \"help\": self._help,",
            "            \"commands\": self._commands,",
            "            \"model_temperature\": self._model_temperature,",
            "            \"model_top_k\": self._model_top_k,",
            "            \"model_top_p\": self._model_top_p,",
            "            \"model_repeat_penalty\": self._model_repeat_penalty,",
            "            \"model_repeat_last_n\": self._model_repeat_last_n,",
            "            \"assets_list\":self._assets_list",
            "        }",
            "",
            "    # ========================================== Properties ===========================================",
            "    @property",
            "    def conditionning_commands(self):",
            "        return {",
            "            \"date_time\": datetime.now().strftime(\"%A, %B %d, %Y %I:%M:%S %p\"), # Replaces {{date}} with actual date",
            "            \"date\": datetime.now().strftime(\"%A, %B %d, %Y\"), # Replaces {{date}} with actual date",
            "            \"time\": datetime.now().strftime(\"%H:%M:%S\"), # Replaces {{time}} with actual time",
            "        }",
            "",
            "    @property",
            "    def logo(self):",
            "        \"\"\"",
            "        Get the personality logo.",
            "",
            "        Returns:",
            "        PIL.Image.Image: The personality logo as a Pillow Image object.",
            "        \"\"\"",
            "        if hasattr(self, '_logo'):",
            "            return self._logo",
            "        else:",
            "            return None",
            "    @property",
            "    def version(self):",
            "        \"\"\"Get the version of the package.\"\"\"",
            "        return self._version",
            "",
            "    @version.setter",
            "    def version(self, value):",
            "        \"\"\"Set the version of the package.\"\"\"",
            "        self._version = value",
            "",
            "    @property",
            "    def author(self):",
            "        \"\"\"Get the author of the package.\"\"\"",
            "        return self._author",
            "",
            "    @author.setter",
            "    def author(self, value):",
            "        \"\"\"Set the author of the package.\"\"\"",
            "        self._author = value",
            "",
            "    @property",
            "    def name(self) -> str:",
            "        \"\"\"Get the name.\"\"\"",
            "        return self._name",
            "",
            "    @name.setter",
            "    def name(self, value: str):",
            "        \"\"\"Set the name.\"\"\"",
            "        self._name = value",
            "",
            "    @property",
            "    def user_name(self) -> str:",
            "        \"\"\"Get the user name.\"\"\"",
            "        return self._user_name",
            "",
            "    @user_name.setter",
            "    def user_name(self, value: str):",
            "        \"\"\"Set the user name.\"\"\"",
            "        self._user_name = value",
            "",
            "",
            "    @property",
            "    def language(self) -> str:",
            "        \"\"\"Get the language.\"\"\"",
            "        return self._language",
            "",
            "    @property",
            "    def category(self) -> str:",
            "        \"\"\"Get the category.\"\"\"",
            "        return self._category",
            "",
            "    @property",
            "    def category_desc(self) -> str:",
            "        \"\"\"Get the category.\"\"\"",
            "        return self._category_desc",
            "",
            "    @language.setter",
            "    def language(self, value: str):",
            "        \"\"\"Set the language.\"\"\"",
            "        self._language = value",
            "",
            "    @category.setter",
            "    def category(self, value: str):",
            "        \"\"\"Set the category.\"\"\"",
            "        self._category = value",
            "",
            "    @category_desc.setter",
            "    def category_desc(self, value: str):",
            "        \"\"\"Set the category.\"\"\"",
            "        self._category_desc = value",
            "",
            "",
            "    @property",
            "    def supported_languages(self) -> str:",
            "        \"\"\"Get the supported_languages.\"\"\"",
            "        return self._supported_languages",
            "",
            "    @supported_languages.setter",
            "    def supported_languages(self, value: str):",
            "        \"\"\"Set the supported_languages.\"\"\"",
            "        self._supported_languages = value",
            "",
            "",
            "    @property",
            "    def selected_language(self) -> str:",
            "        \"\"\"Get the selected_language.\"\"\"",
            "        return self._selected_language",
            "",
            "    @selected_language.setter",
            "    def selected_language(self, value: str):",
            "        \"\"\"Set the selected_language.\"\"\"",
            "        self._selected_language = value",
            "",
            "    @property",
            "    def ignore_discussion_documents_rag(self) -> str:",
            "        \"\"\"Get the ignore_discussion_documents_rag.\"\"\"",
            "        return self._ignore_discussion_documents_rag",
            "",
            "    @ignore_discussion_documents_rag.setter",
            "    def ignore_discussion_documents_rag(self, value: str):",
            "        \"\"\"Set the ignore_discussion_documents_rag.\"\"\"",
            "        self._ignore_discussion_documents_rag = value",
            "",
            "",
            "    @property",
            "    def personality_description(self) -> str:",
            "        \"\"\"",
            "        Getter for the personality description.",
            "",
            "        Returns:",
            "            str: The personality description of the AI assistant.",
            "        \"\"\"",
            "        return self._personality_description",
            "",
            "    @personality_description.setter",
            "    def personality_description(self, description: str):",
            "        \"\"\"",
            "        Setter for the personality description.",
            "",
            "        Args:",
            "            description (str): The new personality description for the AI assistant.",
            "        \"\"\"",
            "        self._personality_description = description",
            "",
            "    @property",
            "    def personality_conditioning(self) -> str:",
            "        \"\"\"",
            "        Getter for the personality conditioning.",
            "",
            "        Returns:",
            "            str: The personality conditioning of the AI assistant.",
            "        \"\"\"",
            "        return self.replace_keys(self._personality_conditioning, self.conditionning_commands)",
            "",
            "    @personality_conditioning.setter",
            "    def personality_conditioning(self, conditioning: str):",
            "        \"\"\"",
            "        Setter for the personality conditioning.",
            "",
            "        Args:",
            "            conditioning (str): The new personality conditioning for the AI assistant.",
            "        \"\"\"",
            "        self._personality_conditioning = conditioning",
            "",
            "    @property",
            "    def welcome_message(self) -> str:",
            "        \"\"\"",
            "        Getter for the welcome message.",
            "",
            "        Returns:",
            "            str: The welcome message of the AI assistant.",
            "        \"\"\"",
            "        return self.replace_keys(self._welcome_message, self.conditionning_commands)",
            "",
            "    @welcome_message.setter",
            "    def welcome_message(self, message: str):",
            "        \"\"\"",
            "        Setter for the welcome message.",
            "",
            "        Args:",
            "            message (str): The new welcome message for the AI assistant.",
            "        \"\"\"",
            "        self._welcome_message = message",
            "",
            "    @property",
            "    def include_welcome_message_in_discussion(self) -> bool:",
            "        \"\"\"",
            "        Getter for the include welcome message in disucssion.",
            "",
            "        Returns:",
            "            bool: whether to add the welcome message to tje discussion or not.",
            "        \"\"\"",
            "        return self._include_welcome_message_in_discussion",
            "",
            "    @include_welcome_message_in_discussion.setter",
            "    def include_welcome_message_in_discussion(self, message: bool):",
            "        \"\"\"",
            "        Setter for the welcome message.",
            "",
            "        Args:",
            "            message (str): The new welcome message for the AI assistant.",
            "        \"\"\"",
            "        self._include_welcome_message_in_discussion = message",
            "",
            "",
            "    @property",
            "    def user_message_prefix(self) -> str:",
            "        \"\"\"",
            "        Getter for the user message prefix.",
            "",
            "        Returns:",
            "            str: The user message prefix of the AI assistant.",
            "        \"\"\"",
            "        return self._user_message_prefix",
            "",
            "    @user_message_prefix.setter",
            "    def user_message_prefix(self, prefix: str):",
            "        \"\"\"",
            "        Setter for the user message prefix.",
            "",
            "        Args:",
            "            prefix (str): The new user message prefix for the AI assistant.",
            "        \"\"\"",
            "        self._user_message_prefix = prefix",
            "",
            "    @property",
            "    def link_text(self) -> str:",
            "        \"\"\"",
            "        Getter for the link text.",
            "",
            "        Returns:",
            "            str: The link text of the AI assistant.",
            "        \"\"\"",
            "        return self._link_text",
            "",
            "    @link_text.setter",
            "    def link_text(self, text: str):",
            "        \"\"\"",
            "        Setter for the link text.",
            "",
            "        Args:",
            "            text (str): The new link text for the AI assistant.",
            "        \"\"\"",
            "        self._link_text = text",
            "    @property",
            "    def ai_message_prefix(self):",
            "        \"\"\"",
            "        Get the AI message prefix.",
            "",
            "        Returns:",
            "            str: The AI message prefix.",
            "        \"\"\"",
            "        return self._ai_message_prefix",
            "",
            "    @ai_message_prefix.setter",
            "    def ai_message_prefix(self, prefix):",
            "        \"\"\"",
            "        Set the AI message prefix.",
            "",
            "        Args:",
            "            prefix (str): The AI message prefix to set.",
            "        \"\"\"",
            "        self._ai_message_prefix = prefix",
            "",
            "    @property",
            "    def dependencies(self) -> List[str]:",
            "        \"\"\"Getter method for the dependencies attribute.",
            "",
            "        Returns:",
            "            List[str]: The list of dependencies.",
            "        \"\"\"",
            "        return self._dependencies",
            "",
            "    @dependencies.setter",
            "    def dependencies(self, dependencies: List[str]):",
            "        \"\"\"Setter method for the dependencies attribute.",
            "",
            "        Args:",
            "            dependencies (List[str]): The list of dependencies.",
            "        \"\"\"",
            "        self._dependencies = dependencies",
            "",
            "    @property",
            "    def disclaimer(self) -> str:",
            "        \"\"\"Getter method for the disclaimer attribute.",
            "",
            "        Returns:",
            "            str: The disclaimer text.",
            "        \"\"\"",
            "        return self._disclaimer",
            "",
            "    @disclaimer.setter",
            "    def disclaimer(self, disclaimer: str):",
            "        \"\"\"Setter method for the disclaimer attribute.",
            "",
            "        Args:",
            "            disclaimer (str): The disclaimer text.",
            "        \"\"\"",
            "        self._disclaimer = disclaimer",
            "",
            "    @property",
            "    def help(self) -> str:",
            "        \"\"\"Getter method for the help attribute.",
            "",
            "        Returns:",
            "            str: The help text.",
            "        \"\"\"",
            "        return self._help",
            "",
            "    @help.setter",
            "    def help(self, help: str):",
            "        \"\"\"Setter method for the help attribute.",
            "",
            "        Args:",
            "            help (str): The help text.",
            "        \"\"\"",
            "        self._help = help",
            "",
            "",
            "",
            "    @property",
            "    def commands(self) -> str:",
            "        \"\"\"Getter method for the commands attribute.",
            "",
            "        Returns:",
            "            str: The commands text.",
            "        \"\"\"",
            "        return self._commands",
            "",
            "    @commands.setter",
            "    def commands(self, commands: str):",
            "        \"\"\"Setter method for the commands attribute.",
            "",
            "        Args:",
            "            commands (str): The commands text.",
            "        \"\"\"",
            "        self._commands = commands",
            "",
            "",
            "    @property",
            "    def model_temperature(self) -> float:",
            "        \"\"\"Get the model's temperature.\"\"\"",
            "        return self._model_temperature",
            "",
            "    @model_temperature.setter",
            "    def model_temperature(self, value: float):",
            "        \"\"\"Set the model's temperature.",
            "",
            "        Args:",
            "            value (float): The new temperature value.",
            "        \"\"\"",
            "        self._model_temperature = value",
            "",
            "    @property",
            "    def model_top_k(self) -> int:",
            "        \"\"\"Get the model's top-k value.\"\"\"",
            "        return self._model_top_k",
            "",
            "    @model_top_k.setter",
            "    def model_top_k(self, value: int):",
            "        \"\"\"Set the model's top-k value.",
            "",
            "        Args:",
            "            value (int): The new top-k value.",
            "        \"\"\"",
            "        self._model_top_k = value",
            "",
            "    @property",
            "    def model_top_p(self) -> float:",
            "        \"\"\"Get the model's top-p value.\"\"\"",
            "        return self._model_top_p",
            "",
            "    @model_top_p.setter",
            "    def model_top_p(self, value: float):",
            "        \"\"\"Set the model's top-p value.",
            "",
            "        Args:",
            "            value (float): The new top-p value.",
            "        \"\"\"",
            "        self._model_top_p = value",
            "",
            "    @property",
            "    def model_repeat_penalty(self) -> float:",
            "        \"\"\"Get the model's repeat penalty value.\"\"\"",
            "        return self._model_repeat_penalty",
            "",
            "    @model_repeat_penalty.setter",
            "    def model_repeat_penalty(self, value: float):",
            "        \"\"\"Set the model's repeat penalty value.",
            "",
            "        Args:",
            "            value (float): The new repeat penalty value.",
            "        \"\"\"",
            "        self._model_repeat_penalty = value",
            "",
            "    @property",
            "    def model_repeat_last_n(self) -> int:",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._model_repeat_last_n",
            "",
            "    @model_repeat_last_n.setter",
            "    def model_repeat_last_n(self, value: int):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._model_repeat_last_n = value",
            "",
            "",
            "    @property",
            "    def assets_list(self) -> list:",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._assets_list",
            "",
            "    @assets_list.setter",
            "    def assets_list(self, value: list):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._assets_list = value",
            "",
            "    @property",
            "    def processor(self) -> 'APScript':",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._processor",
            "",
            "    @processor.setter",
            "    def processor(self, value: 'APScript'):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._processor = value",
            "",
            "",
            "    @property",
            "    def processor_cfg(self) -> list:",
            "        \"\"\"Get the number of words to consider for repeat penalty.\"\"\"",
            "        return self._processor_cfg",
            "",
            "    @processor_cfg.setter",
            "    def processor_cfg(self, value: dict):",
            "        \"\"\"Set the number of words to consider for repeat penalty.",
            "",
            "        Args:",
            "            value (int): The new number of words value.",
            "        \"\"\"",
            "        self._processor_cfg = value",
            "",
            "    # ========================================== Helper methods ==========================================",
            "    def detect_antiprompt(self, text:str) -> bool:",
            "        \"\"\"",
            "        Detects if any of the antiprompts in self.anti_prompts are present in the given text.",
            "        Used for the Hallucination suppression system",
            "",
            "        Args:",
            "            text (str): The text to check for antiprompts.",
            "",
            "        Returns:",
            "            bool: True if any antiprompt is found in the text (ignoring case), False otherwise.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        separator_template          = self.config.separator_template",
            "",
            "        anti_prompts = [start_header_id_template, self.app.config.discussion_prompt_separator]",
            "        if self.app.config.separator_template!=\"\\n\":",
            "            anti_prompts.append(self.app.config.separator_template)",
            "",
            "        for prompt in anti_prompts:",
            "            if prompt.lower() in text.lower():",
            "                return prompt.lower()",
            "        return None",
            "",
            "",
            "    # Helper functions",
            "    @staticmethod",
            "    def replace_keys(input_string, replacements):",
            "        \"\"\"",
            "        Replaces all occurrences of keys in the input string with their corresponding",
            "        values from the replacements dictionary.",
            "",
            "        Args:",
            "            input_string (str): The input string to replace keys in.",
            "            replacements (dict): A dictionary of key-value pairs, where the key is the",
            "                string to be replaced and the value is the replacement string.",
            "",
            "        Returns:",
            "            str: The input string with all occurrences of keys replaced by their",
            "                corresponding values.",
            "        \"\"\"",
            "        pattern = r\"\\{\\{(\\w+)\\}\\}\"",
            "        # The pattern matches \"{{key}}\" and captures \"key\" in a group.",
            "        # The \"\\w+\" matches one or more word characters (letters, digits, or underscore).",
            "",
            "        def replace(match):",
            "            key = match.group(1)",
            "            return replacements.get(key, match.group(0))",
            "",
            "        output_string = re.sub(pattern, replace, input_string)",
            "        return output_string",
            "",
            "",
            "",
            "class StateMachine:",
            "    def __init__(self, states_list):",
            "        \"\"\"",
            "        states structure is the following",
            "        [",
            "            {",
            "                \"name\": the state name,",
            "                \"commands\": [ # list of commands",
            "                    \"command\": function",
            "                ],",
            "                \"default\": default function",
            "            }",
            "        ]",
            "        \"\"\"",
            "        self.states_list = states_list",
            "        self.current_state_id = 0",
            "        self.callback = None",
            "",
            "    def goto_state(self, state):",
            "        \"\"\"",
            "        Transition to the state with the given name or index.",
            "",
            "        Args:",
            "            state (str or int): The name or index of the state to transition to.",
            "",
            "        Raises:",
            "            ValueError: If no state is found with the given name or index.",
            "        \"\"\"",
            "        if isinstance(state, str):",
            "            for i, state_dict in enumerate(self.states_list):",
            "                if state_dict[\"name\"] == state:",
            "                    self.current_state_id = i",
            "                    return",
            "        elif isinstance(state, int):",
            "            if 0 <= state < len(self.states_list):",
            "                self.current_state_id = state",
            "                return",
            "        raise ValueError(f\"No state found with name or index: {state}\")",
            "",
            "",
            "",
            "    def process_state(self, command, full_context, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None, context_state:dict=None, client:Client=None):",
            "        \"\"\"",
            "        Process the given command based on the current state.",
            "",
            "        Args:",
            "            command: The command to process.",
            "",
            "        Raises:",
            "            ValueError: If the current state doesn't have the command and no default function is defined.",
            "        \"\"\"",
            "        if callback:",
            "            self.callback=callback",
            "",
            "        current_state = self.states_list[self.current_state_id]",
            "        commands = current_state[\"commands\"]",
            "        command = command.strip()",
            "",
            "        for cmd, func in commands.items():",
            "            if cmd == command[0:len(cmd)]:",
            "                try:",
            "                    func(command, full_context, callback, context_state, client)",
            "                except:# retrocompatibility",
            "                    func(command, full_context)",
            "                return",
            "",
            "        default_func = current_state.get(\"default\")",
            "        if default_func is not None:",
            "            default_func(command, full_context, callback, context_state, client)",
            "        else:",
            "            raise ValueError(f\"Command '{command}' not found in current state and no default function defined.\")",
            "",
            "",
            "class LoLLMsActionParameters:",
            "    def __init__(self, name: str, parameter_type: Type, range: Optional[List] = None, options: Optional[List] = None, value: Any = None) -> None:",
            "        self.name = name",
            "        self.parameter_type = parameter_type",
            "        self.range = range",
            "        self.options = options",
            "        self.value = value",
            "",
            "    def __str__(self) -> str:",
            "        parameter_dict = {",
            "            'name': self.name,",
            "            'parameter_type': self.parameter_type.__name__,",
            "            'value': self.value",
            "        }",
            "        if self.range is not None:",
            "            parameter_dict['range'] = self.range",
            "        if self.options is not None:",
            "            parameter_dict['options'] = self.options",
            "        return json.dumps(parameter_dict, indent=4)",
            "",
            "    @staticmethod",
            "    def from_str(string: str) -> 'LoLLMsActionParameters':",
            "        parameter_dict = json.loads(string)",
            "        name = parameter_dict['name']",
            "        parameter_type = eval(parameter_dict['parameter_type'])",
            "        range = parameter_dict.get('range', None)",
            "        options = parameter_dict.get('options', None)",
            "        value = parameter_dict['value']",
            "        return LoLLMsActionParameters(name, parameter_type, range, options, value)",
            "",
            "    @staticmethod",
            "    def from_dict(parameter_dict: dict) -> 'LoLLMsActionParameters':",
            "        name = parameter_dict['name']",
            "        parameter_type = eval(parameter_dict['parameter_type'])",
            "        range = parameter_dict.get('range', None)",
            "        options = parameter_dict.get('options', None)",
            "        value = parameter_dict['value']",
            "        return LoLLMsActionParameters(name, parameter_type, range, options, value)",
            "",
            "",
            "class LoLLMsActionParametersEncoder(json.JSONEncoder):",
            "    def default(self, obj):",
            "        if isinstance(obj, LoLLMsActionParameters):",
            "            parameter_dict = {",
            "                'name': obj.name,",
            "                'parameter_type': obj.parameter_type.__name__,",
            "                'value': obj.value",
            "            }",
            "            if obj.range is not None:",
            "                parameter_dict['range'] = obj.range",
            "            if obj.options is not None:",
            "                parameter_dict['options'] = obj.options",
            "            return parameter_dict",
            "        return super().default(obj)",
            "",
            "class LoLLMsAction:",
            "    def __init__(self, name, parameters: List[LoLLMsActionParameters], callback: Callable, description:str=\"\") -> None:",
            "        self.name           = name",
            "        self.parameters     = parameters",
            "        self.callback       = callback",
            "        self.description    = description",
            "",
            "    def __str__(self) -> str:",
            "        action_dict = {",
            "            'name': self.name,",
            "            'parameters': self.parameters,",
            "            'description': self.description",
            "        }",
            "        return json.dumps(action_dict, indent=4, cls=LoLLMsActionParametersEncoder)",
            "",
            "    @staticmethod",
            "    def from_str(string: str) -> 'LoLLMsAction':",
            "        action_dict = json.loads(string)",
            "        name = action_dict['name']",
            "        parameters = [LoLLMsActionParameters.from_dict(param_str) for param_str in action_dict['parameters']]",
            "        return LoLLMsAction(name, parameters, None)",
            "",
            "    @staticmethod",
            "    def from_dict(action_dict: dict) -> 'LoLLMsAction':",
            "        name = action_dict['name']",
            "        parameters = [LoLLMsActionParameters.from_dict(param_str) for param_str in action_dict['parameters']]",
            "        return LoLLMsAction(name, parameters, None)",
            "",
            "",
            "    def run(self) -> None:",
            "        args = {param.name: param.value for param in self.parameters}",
            "        self.callback(**args)",
            "",
            "def generate_actions(potential_actions: List[LoLLMsAction], parsed_text: dict) -> List[LoLLMsAction]:",
            "    actions = []",
            "    try:",
            "        for action_data in parsed_text[\"actions\"]:",
            "            name = action_data['name']",
            "            parameters = action_data['parameters']",
            "            matching_action = next((action for action in potential_actions if action.name == name), None)",
            "            if matching_action:",
            "                action = LoLLMsAction.from_str(str(matching_action))",
            "                action.callback = matching_action.callback",
            "                if type(parameters)==dict:",
            "                    for param_name, param_value in parameters.items():",
            "                        matching_param = next((param for param in action.parameters if param.name == param_name), None)",
            "                        if matching_param:",
            "                            matching_param.value = param_value",
            "                else:",
            "                    for param in parameters:",
            "                        if \"name\" in param:",
            "                            param_name = param[\"name\"]",
            "                            param_value = param[\"value\"]",
            "                        else:",
            "                            param_name = list(param.keys())[0]",
            "                            param_value = param[param_name]",
            "                        matching_param = next((param for param in action.parameters if param.name == param_name), None)",
            "                        if matching_param:",
            "                            matching_param.value = param_value",
            "                actions.append(action)",
            "    except json.JSONDecodeError:",
            "        print(\"Invalid JSON format.\")",
            "    return actions",
            "",
            "class APScript(StateMachine):",
            "    \"\"\"",
            "    Template class for implementing personality processor classes in the APScript framework.",
            "",
            "    This class provides a basic structure and placeholder methods for processing model inputs and outputs.",
            "    Personality-specific processor classes should inherit from this class and override the necessary methods.",
            "    \"\"\"",
            "    def __init__(",
            "                    self,",
            "                    personality         :AIPersonality,",
            "                    personality_config  :TypedConfig,",
            "                    states_list         :dict   = {},",
            "                    callback            = None",
            "                ) -> None:",
            "        super().__init__(states_list)",
            "        self.function_definitions               = [] # New! useful for 3rd gen personalities ",
            "        self.notify                             = personality.app.notify",
            "",
            "        self.personality                        = personality",
            "        self.config                             = personality.config",
            "        self.personality_config                 = personality_config",
            "        self.installation_option                = personality.installation_option",
            "        self.configuration_file_path            = self.personality.lollms_paths.personal_configuration_path/\"personalities\"/self.personality.personality_folder_name/f\"config.yaml\"",
            "        self.configuration_file_path.parent.mkdir(parents=True, exist_ok=True)",
            "",
            "        self.personality_config.config.file_path    = self.configuration_file_path",
            "",
            "        self.callback = callback",
            "",
            "        # Installation",
            "        if (not self.configuration_file_path.exists() or self.installation_option==InstallOption.FORCE_INSTALL) and self.installation_option!=InstallOption.NEVER_INSTALL:",
            "            self.install()",
            "            self.personality_config.config.save_config()",
            "        else:",
            "            self.load_personality_config()",
            "",
            "    def sink(self, s=None,i=None,d=None):",
            "        pass",
            "",
            "    def settings_updated(self):",
            "        \"\"\"",
            "        To be implemented by the processor when the settings have changed",
            "        \"\"\"",
            "        pass",
            "",
            "    def mounted(self):",
            "        \"\"\"",
            "        triggered when mounted",
            "        \"\"\"",
            "        pass",
            "",
            "    def get_welcome(self, welcome_message:str, client:Client):",
            "        \"\"\"",
            "        triggered when a new conversation is created",
            "        \"\"\"",
            "        return welcome_message",
            "        ",
            "    def selected(self):",
            "        \"\"\"",
            "        triggered when mounted",
            "        \"\"\"",
            "        pass",
            "",
            "    def execute_command(self, command: str, parameters:list=[], client:Client=None):",
            "        \"\"\"",
            "        Recovers user commands and executes them. Each personality can define a set of commands that they can receive and execute",
            "        Args:",
            "            command: The command name",
            "            parameters: A list of the command parameters",
            "",
            "        \"\"\"",
            "        try:",
            "            self.process_state(command, \"\", self.callback, client)",
            "        except Exception as ex:",
            "            trace_exception(ex)",
            "            self.warning(f\"Couldn't execute command {command}\")",
            "",
            "    async def handle_request(self, request: Request) -> Dict[str, Any]:",
            "        \"\"\"",
            "        Handle client requests.",
            "",
            "        Args:",
            "            data (dict): A dictionary containing the request data.",
            "",
            "        Returns:",
            "            dict: A dictionary containing the response, including at least a \"status\" key.",
            "",
            "        This method should be implemented by a class that inherits from this one.",
            "",
            "        Example usage:",
            "        ```",
            "        handler = YourHandlerClass()",
            "        request_data = {\"command\": \"some_command\", \"parameters\": {...}}",
            "        response = await handler.handle_request(request_data)",
            "        ```",
            "        \"\"\"",
            "        return {\"status\":True}",
            "",
            "",
            "    def load_personality_config(self):",
            "        \"\"\"",
            "        Load the content of local_config.yaml file.",
            "",
            "        The function reads the content of the local_config.yaml file and returns it as a Python dictionary.",
            "",
            "        Args:",
            "            None",
            "",
            "        Returns:",
            "            dict: A dictionary containing the loaded data from the local_config.yaml file.",
            "        \"\"\"",
            "        try:",
            "            self.personality_config.config.load_config()",
            "        except:",
            "            self.personality_config.config.save_config()",
            "        self.personality_config.sync()",
            "",
            "    def install(self):",
            "        \"\"\"",
            "        Installation procedure (to be implemented)",
            "        \"\"\"",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "        ASCIIColors.red(f\"Installing {self.personality.personality_folder_name}\")",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "",
            "",
            "    def uninstall(self):",
            "        \"\"\"",
            "        Installation procedure (to be implemented)",
            "        \"\"\"",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "        ASCIIColors.red(f\"Uninstalling {self.personality.personality_folder_name}\")",
            "        ASCIIColors.blue(\"*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\")",
            "",
            "",
            "    def add_file(self, path, client:Client, callback=None, process=True):",
            "        self.personality.add_file(path, client=client,callback=callback, process=process)",
            "        if callback is not None:",
            "            callback(\"File added successfully\",MSG_TYPE.MSG_TYPE_INFO)",
            "        return True",
            "",
            "    def remove_file(self, path):",
            "        if path in self.personality.text_files:",
            "            self.personality.text_files.remove(path)",
            "        elif path in self.personality.image_files:",
            "            self.personality.image_files.remove(path)",
            "",
            "",
            "    def load_config_file(self, path, default_config=None):",
            "        \"\"\"",
            "        Load the content of local_config.yaml file.",
            "",
            "        The function reads the content of the local_config.yaml file and returns it as a Python dictionary.",
            "        If a default_config is provided, it fills any missing entries in the loaded dictionary.",
            "        If at least one field from default configuration was not present in the loaded configuration, the updated",
            "        configuration is saved.",
            "",
            "        Args:",
            "            path (str): The path to the local_config.yaml file.",
            "            default_config (dict, optional): A dictionary with default values to fill missing entries.",
            "",
            "        Returns:",
            "            dict: A dictionary containing the loaded data from the local_config.yaml file, with missing entries filled",
            "            by default_config if provided.",
            "        \"\"\"",
            "        with open(path, 'r') as file:",
            "            data = yaml.safe_load(file)",
            "",
            "        if default_config:",
            "            updated = False",
            "            for key, value in default_config.items():",
            "                if key not in data:",
            "                    data[key] = value",
            "                    updated = True",
            "",
            "            if updated:",
            "                self.save_config_file(path, data)",
            "",
            "        return data",
            "",
            "    def save_config_file(self, path, data):",
            "        \"\"\"",
            "        Save the configuration data to a local_config.yaml file.",
            "",
            "        Args:",
            "            path (str): The path to save the local_config.yaml file.",
            "            data (dict): The configuration data to be saved.",
            "",
            "        Returns:",
            "            None",
            "        \"\"\"",
            "        with open(path, 'w') as file:",
            "            yaml.dump(data, file)",
            "",
            "    def generate_with_images(self, prompt, images, max_size = None, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False ):",
            "        return self.personality.generate_with_images(prompt, images, max_size, temperature, top_k, top_p, repeat_penalty, repeat_last_n, callback, debug=debug)",
            "",
            "    def generate(self, prompt, max_size = None, temperature = None, top_k = None, top_p=None, repeat_penalty=None, repeat_last_n=None, callback=None, debug=False ):",
            "        return self.personality.generate(prompt, max_size, temperature, top_k, top_p, repeat_penalty, repeat_last_n, callback, debug=debug)",
            "",
            "",
            "    def run_workflow(self, prompt:str, previous_discussion_text:str=\"\", callback: Callable[[str, MSG_TYPE, dict, list], bool]=None, context_details:dict=None, client:Client=None):",
            "        \"\"\"",
            "        This function generates code based on the given parameters.",
            "",
            "        Args:",
            "            full_prompt (str): The full prompt for code generation.",
            "            prompt (str): The prompt for code generation.",
            "            context_details (dict): A dictionary containing the following context details for code generation:",
            "                - conditionning (str): The conditioning information.",
            "                - documentation (str): The documentation information.",
            "                - knowledge (str): The knowledge information.",
            "                - user_description (str): The user description information.",
            "                - discussion_messages (str): The discussion messages information.",
            "                - positive_boost (str): The positive boost information.",
            "                - negative_boost (str): The negative boost information.",
            "                - current_language (str): The force language information.",
            "                - fun_mode (str): The fun mode conditionning text",
            "                - ai_prefix (str): The AI prefix information.",
            "            n_predict (int): The number of predictions to generate.",
            "            client_id: The client ID for code generation.",
            "            callback (function, optional): The callback function for code generation.",
            "",
            "        Returns:",
            "            None",
            "        \"\"\"",
            "",
            "        return None",
            "",
            "",
            "    # ================================================= Advanced methods ===========================================",
            "    def compile_latex(self, file_path, pdf_latex_path=None):",
            "        try:",
            "            # Determine the pdflatex command based on the provided or default path",
            "            if pdf_latex_path:",
            "                pdflatex_command = pdf_latex_path",
            "            else:",
            "                pdflatex_command = self.personality.config.pdf_latex_path if self.personality.config.pdf_latex_path is not None else 'pdflatex'",
            "",
            "            # Set the execution path to the folder containing the tmp_file",
            "            execution_path = file_path.parent",
            "            # Run the pdflatex command with the file path",
            "            result = subprocess.run([pdflatex_command, \"-interaction=nonstopmode\", file_path], check=True, capture_output=True, text=True, cwd=execution_path)",
            "            # Check the return code of the pdflatex command",
            "            if result.returncode != 0:",
            "                error_message = result.stderr.strip()",
            "                return {\"status\":False,\"error\":error_message}",
            "",
            "            # If the compilation is successful, you will get a PDF file",
            "            pdf_file = file_path.with_suffix('.pdf')",
            "            print(f\"PDF file generated: {pdf_file}\")",
            "            return {\"status\":True,\"file_path\":pdf_file}",
            "",
            "        except subprocess.CalledProcessError as e:",
            "            print(f\"Error occurred while compiling LaTeX: {e}\")",
            "            return {\"status\":False,\"error\":e}",
            "",
            "    def find_numeric_value(self, text):",
            "        pattern = r'\\d+[.,]?\\d*'",
            "        match = re.search(pattern, text)",
            "        if match:",
            "            return float(match.group().replace(',', '.'))",
            "        else:",
            "            return None",
            "    def remove_backticks(self, text):",
            "        if text.startswith(\"```\"):",
            "            split_text = text.split(\"\\n\")",
            "            text = \"\\n\".join(split_text[1:])",
            "        if text.endswith(\"```\"):",
            "            text= text[:-3]",
            "        return text",
            "",
            "    def search_duckduckgo(self, query: str, max_results: int = 10, instant_answers: bool = True, regular_search_queries: bool = True, get_webpage_content: bool = False) -> List[Dict[str, Union[str, None]]]:",
            "        \"\"\"",
            "        Perform a search using the DuckDuckGo search engine and return the results as a list of dictionaries.",
            "",
            "        Args:",
            "            query (str): The search query to use in the search. This argument is required.",
            "            max_results (int, optional): The maximum number of search results to return. Defaults to 10.",
            "            instant_answers (bool, optional): Whether to include instant answers in the search results. Defaults to True.",
            "            regular_search_queries (bool, optional): Whether to include regular search queries in the search results. Defaults to True.",
            "            get_webpage_content (bool, optional): Whether to retrieve and include the website content for each result. Defaults to False.",
            "",
            "        Returns:",
            "            list[dict]: A list of dictionaries containing the search results. Each dictionary will contain 'title', 'body', and 'href' keys.",
            "",
            "        Raises:",
            "            ValueError: If neither instant_answers nor regular_search_queries is set to True.",
            "        \"\"\"",
            "        if not PackageManager.check_package_installed(\"duckduckgo_search\"):",
            "            PackageManager.install_package(\"duckduckgo_search\")",
            "        from duckduckgo_search import DDGS",
            "        if not (instant_answers or regular_search_queries):",
            "            raise ValueError(\"One of ('instant_answers', 'regular_search_queries') must be True\")",
            "",
            "        query = query.strip(\"\\\"'\")",
            "",
            "        with DDGS() as ddgs:",
            "            if instant_answers:",
            "                answer_list = list(ddgs.answers(query))",
            "                if answer_list:",
            "                    answer_dict = answer_list[0]",
            "                    answer_dict[\"title\"] = query",
            "                    answer_dict[\"body\"] = next((item['Text'] for item in answer_dict['AbstractText']), None)",
            "                    answer_dict[\"href\"] = answer_dict.get('FirstURL', '')",
            "            else:",
            "                answer_list = []",
            "",
            "            if regular_search_queries:",
            "                results = ddgs.text(query, safe=False, result_type='link')",
            "                for result in results[:max_results]:",
            "                    title = result['Text'] or query",
            "                    body = None",
            "                    href = result['FirstURL'] or ''",
            "                    answer_dict = {'title': title, 'body': body, 'href': href}",
            "                    answer_list.append(answer_dict)",
            "",
            "            if get_webpage_content:",
            "                for i, result in enumerate(answer_list):",
            "                    try:",
            "                        response = requests.get(result['href'])",
            "                        if response.status_code == 200:",
            "                            content = response.text",
            "                            answer_list[i]['body'] = content",
            "                    except Exception as e:",
            "                        print(f\"Error retrieving webpage content for {result['href']}: {str(e)}\")",
            "",
            "            return answer_list",
            "",
            "",
            "    def translate(self, text_chunk, output_language=\"french\", max_generation_size=3000):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        translated = self.fast_gen(",
            "                                \"\\n\".join([",
            "                                    f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                                    f\"Translate the following text to {output_language}.\",",
            "                                    \"Be faithful to the original text and do not add or remove any information.\",",
            "                                    \"Respond only with the translated text.\",",
            "                                    \"Do not add comments or explanations.\",",
            "                                    f\"{start_header_id_template}text to translate{end_header_id_template}\",",
            "                                    f\"{text_chunk}\",",
            "                                    f\"{start_header_id_template}translation{end_header_id_template}\",",
            "                                    ]),",
            "                                    max_generation_size=max_generation_size, callback=self.sink)",
            "        return translated",
            "",
            "    def summerize_text(",
            "                        self,",
            "                        text,",
            "                        summary_instruction=\"summerize\",",
            "                        doc_name=\"chunk\",",
            "                        answer_start=\"\",",
            "                        max_generation_size=3000,",
            "                        max_summary_size=512,",
            "                        callback=None,",
            "                        chunk_summary_post_processing=None,",
            "                        summary_mode=SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL",
            "                    ):",
            "        tk = self.personality.model.tokenize(text)",
            "        prev_len = len(tk)",
            "        document_chunks=None",
            "        while len(tk)>max_summary_size and (document_chunks is None or len(document_chunks)>1):",
            "            self.step_start(f\"Comprerssing {doc_name}...\")",
            "            chunk_size = int(self.personality.config.ctx_size*0.6)",
            "            document_chunks = DocumentDecomposer.decompose_document(text, chunk_size, 0, self.personality.model.tokenize, self.personality.model.detokenize, True)",
            "            text = self.summerize_chunks(",
            "                                            document_chunks,",
            "                                            summary_instruction, ",
            "                                            doc_name, ",
            "                                            answer_start, ",
            "                                            max_generation_size, ",
            "                                            callback, ",
            "                                            chunk_summary_post_processing=chunk_summary_post_processing,",
            "                                            summary_mode=summary_mode)",
            "            tk = self.personality.model.tokenize(text)",
            "            tk = self.personality.model.tokenize(text)",
            "            dtk_ln=prev_len-len(tk)",
            "            prev_len = len(tk)",
            "            self.step(f\"Current text size : {prev_len}, max summary size : {max_summary_size}\")",
            "            self.step_end(f\"Comprerssing {doc_name}...\")",
            "            if dtk_ln<=10: # it is not summarizing",
            "                break",
            "        return text",
            "",
            "    def smart_data_extraction(",
            "                                self,",
            "                                text,",
            "                                data_extraction_instruction=\"summerize\",",
            "                                final_task_instruction=\"reformulate with better wording\",",
            "                                doc_name=\"chunk\",",
            "                                answer_start=\"\",",
            "                                max_generation_size=3000,",
            "                                max_summary_size=512,",
            "                                callback=None,",
            "                                chunk_summary_post_processing=None,",
            "                                summary_mode=SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL",
            "                            ):",
            "        tk = self.personality.model.tokenize(text)",
            "        prev_len = len(tk)",
            "        while len(tk)>max_summary_size:",
            "            chunk_size = int(self.personality.config.ctx_size*0.6)",
            "            document_chunks = DocumentDecomposer.decompose_document(text, chunk_size, 0, self.personality.model.tokenize, self.personality.model.detokenize, True)",
            "            text = self.summerize_chunks(",
            "                                            document_chunks, ",
            "                                            data_extraction_instruction, ",
            "                                            doc_name, ",
            "                                            answer_start, ",
            "                                            max_generation_size, ",
            "                                            callback, ",
            "                                            chunk_summary_post_processing=chunk_summary_post_processing, ",
            "                                            summary_mode=summary_mode",
            "                                        )",
            "            tk = self.personality.model.tokenize(text)",
            "            dtk_ln=prev_len-len(tk)",
            "            prev_len = len(tk)",
            "            self.step(f\"Current text size : {prev_len}, max summary size : {max_summary_size}\")",
            "            if dtk_ln<=10: # it is not sumlmarizing",
            "                break",
            "        self.step_start(f\"Rewriting ...\")",
            "        text = self.summerize_chunks(",
            "                                        [text],",
            "                                        final_task_instruction, ",
            "                                        doc_name, answer_start, ",
            "                                        max_generation_size, ",
            "                                        callback, ",
            "                                        chunk_summary_post_processing=chunk_summary_post_processing",
            "                                    )",
            "        self.step_end(f\"Rewriting ...\")",
            "",
            "        return text",
            "",
            "    def summerize_chunks(",
            "                            self,",
            "                            chunks,",
            "                            summary_instruction=\"summerize\",",
            "                            doc_name=\"chunk\",",
            "                            answer_start=\"\",",
            "                            max_generation_size=3000,",
            "                            callback=None,",
            "                            chunk_summary_post_processing=None,",
            "                            summary_mode=SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL",
            "                        ):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        if summary_mode==SUMMARY_MODE.SUMMARY_MODE_SEQUENCIAL:",
            "            summary = \"\"",
            "            for i, chunk in enumerate(chunks):",
            "                self.step_start(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "                summary = f\"{answer_start}\"+ self.fast_gen(",
            "                            \"\\n\".join([",
            "                                f\"{start_header_id_template}Previous_chunks_summary{end_header_id_template}\",",
            "                                f\"{summary}\",",
            "                                f\"{start_header_id_template}Current_chunk{end_header_id_template}\",",
            "                                f\"{chunk}\",",
            "                                f\"{start_header_id_template}{system_message_template}{end_header_id_template}{summary_instruction}\",",
            "                                f\"Summerize the current chunk and fuse it with previous chunk summary ion order to keep the required informations.\",",
            "                                f\"The summary needs to keep all relevant information.\",",
            "                                f\"Be precise and do not invent information that does not exist in the previous summary or the current chunk.\",",
            "                                f\"Answer directly with the summary with no extra comments.\",",
            "                                f\"{start_header_id_template}summary{end_header_id_template}\",",
            "                                f\"{answer_start}\"",
            "                                ]),",
            "                                max_generation_size=max_generation_size,",
            "                                callback=callback)",
            "                if chunk_summary_post_processing:",
            "                    summary = chunk_summary_post_processing(summary)",
            "                self.step_end(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "            return summary",
            "        else:",
            "            summeries = []",
            "            for i, chunk in enumerate(chunks):",
            "                self.step_start(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "                summary = f\"{answer_start}\"+ self.fast_gen(",
            "                            \"\\n\".join([",
            "                                f\"{start_header_id_template}Document_chunk [{doc_name}]{end_header_id_template}\",",
            "                                f\"{chunk}\",",
            "                                f\"{start_header_id_template}{system_message_template}{end_header_id_template}{summary_instruction}\",",
            "                                f\"Answer directly with the summary with no extra comments.\",",
            "                                f\"{start_header_id_template}summary{end_header_id_template}\",",
            "                                f\"{answer_start}\"",
            "                                ]),",
            "                                max_generation_size=max_generation_size,",
            "                                callback=callback)",
            "                if chunk_summary_post_processing:",
            "                    summary = chunk_summary_post_processing(summary)",
            "                summeries.append(summary)",
            "                self.step_end(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "            return \"\\n\".join(summeries)",
            "",
            "    def sequencial_chunks_summary(",
            "                            self,",
            "                            chunks,",
            "                            summary_instruction=\"summerize\",",
            "                            doc_name=\"chunk\",",
            "                            answer_start=\"\",",
            "                            max_generation_size=3000,",
            "                            callback=None,",
            "                            chunk_summary_post_processing=None",
            "                        ):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        summeries = []",
            "        for i, chunk in enumerate(chunks):",
            "            if i<len(chunks)-1:",
            "                chunk1 = chunks[i+1]",
            "            else:",
            "                chunk1=\"\"",
            "            if i>0:",
            "                chunk=summary",
            "            self.step_start(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "            summary = f\"{answer_start}\"+ self.fast_gen(",
            "                        \"\\n\".join([",
            "                            f\"{start_header_id_template}Document_chunk: {doc_name}{end_header_id_template}\",",
            "                            f\"Block1:\",",
            "                            f\"{chunk}\",",
            "                            f\"Block2:\",",
            "                            f\"{chunk1}\",",
            "                            f\"{start_header_id_template}{system_message_template}{end_header_id_template}{summary_instruction}\",",
            "                            f\"Answer directly with the summary with no extra comments.\",",
            "                            f\"{start_header_id_template}summary{end_header_id_template}\",",
            "                            f\"{answer_start}\"",
            "                            ]),",
            "                            max_generation_size=max_generation_size,",
            "                            callback=callback)",
            "            if chunk_summary_post_processing:",
            "                summary = chunk_summary_post_processing(summary)",
            "            summeries.append(summary)",
            "            self.step_end(f\" Summary of {doc_name} - Processing chunk : {i+1}/{len(chunks)}\")",
            "        return \"\\n\".join(summeries)",
            "",
            "    def build_prompt_from_context_details(self, context_details:dict, custom_entries=\"\"):",
            "        \"\"\"",
            "        Builds a prompt from the provided context details.",
            "",
            "        This function concatenates various parts of the context into a single string, which is then used to build a prompt.",
            "        The context details can include conditioning, documentation, knowledge, user description, positive and negative boosts,",
            "        current language, fun mode, discussion window, and any extra information.",
            "",
            "        Parameters:",
            "        context_details (dict): A dictionary containing various context details.",
            "        custom_entries (str): Additional custom entries to be included in the prompt.",
            "",
            "        Returns:",
            "        str: The constructed prompt.",
            "",
            "        Raises:",
            "        KeyError: If any required key is missing in the context_details dictionary.",
            "        \"\"\"",
            "        full_context = []",
            "        sacrifice_id = 0",
            "        if context_details[\"conditionning\"]:",
            "            full_context.append( \"\\n\".join([",
            "                context_details[\"conditionning\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "        if context_details[\"documentation\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"documentation\"),",
            "                context_details[\"documentation\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"knowledge\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"knowledge\"),",
            "                context_details[\"knowledge\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"user_description\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"user_description\"),",
            "                context_details[\"user_description\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"positive_boost\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"positive_boost\"),",
            "                context_details[\"positive_boost\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"positive_boost\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"positive_boost\"),",
            "                context_details[\"positive_boost\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"negative_boost\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"negative_boost\"),",
            "                context_details[\"negative_boost\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"current_language\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"current_language\"),",
            "                context_details[\"current_language\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "        if context_details[\"fun_mode\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"fun_mode\"),",
            "                context_details[\"fun_mode\"]",
            "            ]))",
            "            sacrifice_id += 1",
            "",
            "",
            "        if context_details[\"discussion_messages\"]:",
            "            full_context.append( \"\\n\".join([",
            "                self.system_custom_header(\"discussion_messages\"),",
            "                context_details[\"discussion_messages\"]",
            "            ]))",
            "",
            "        if context_details[\"extra\"]:",
            "            full_context.append( \"\\n\".join([",
            "                context_details[\"extra\"]",
            "            ]))",
            "",
            "        if custom_entries:",
            "            full_context.append( \"\\n\".join([",
            "                custom_entries",
            "            ]))",
            "",
            "        full_context.append( \"\\n\".join([",
            "            self.ai_custom_header(context_details[\"ai_prefix\"])",
            "        ]))",
            "",
            "        return self.build_prompt(full_context, sacrifice_id)",
            "    ",
            "    def build_prompt(self, prompt_parts:List[str], sacrifice_id:int=-1, context_size:int=None, minimum_spare_context_size:int=None):",
            "        \"\"\"",
            "        Builds the prompt for code generation.",
            "",
            "        Args:",
            "            prompt_parts (List[str]): A list of strings representing the parts of the prompt.",
            "            sacrifice_id (int, optional): The ID of the part to sacrifice.",
            "            context_size (int, optional): The size of the context.",
            "            minimum_spare_context_size (int, optional): The minimum spare context size.",
            "",
            "        Returns:",
            "            str: The built prompt.",
            "        \"\"\"",
            "        if context_size is None:",
            "            context_size = self.personality.config.ctx_size",
            "        if minimum_spare_context_size is None:",
            "            minimum_spare_context_size = self.personality.config.min_n_predict",
            "",
            "        if sacrifice_id == -1 or len(prompt_parts[sacrifice_id])<50:",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "        else:",
            "            part_tokens=[]",
            "            nb_tokens=0",
            "            for i, part in enumerate(prompt_parts):",
            "                part_s=part.strip()",
            "                tk = self.personality.model.tokenize(part_s)",
            "                part_tokens.append(tk)",
            "                if i != sacrifice_id:",
            "                    nb_tokens += len(tk)",
            "                    ",
            "            if len(part_tokens[sacrifice_id])>0:",
            "                sacrifice_tk = part_tokens[sacrifice_id]",
            "                sacrifice_tk= sacrifice_tk[-(context_size-nb_tokens-minimum_spare_context_size):]",
            "                sacrifice_text = self.personality.model.detokenize(sacrifice_tk)",
            "            else:",
            "                sacrifice_text = \"\"",
            "            prompt_parts[sacrifice_id] = sacrifice_text",
            "            return \"\\n\".join([s for s in prompt_parts if s!=\"\"])",
            "    # ================================================= Sending commands to ui ===========================================",
            "    def add_collapsible_entry(self, title, content, subtitle=\"\"):",
            "        return \"\\n\".join(",
            "        [",
            "        f'<details class=\"flex w-full rounded-xl border border-gray-200 bg-white shadow-sm dark:border-gray-800 dark:bg-gray-900 mb-3.5 max-w-full svelte-1escu1z\" open=\"\">',",
            "        f'    <summary class=\"grid w-full select-none grid-cols-[40px,1fr] items-center gap-2.5 p-2 svelte-1escu1z\">',",
            "        f'        <dl class=\"leading-4\">',",
            "        f'          <dd class=\"text-sm\"><svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\" fill=\"none\" stroke=\"currentColor\" stroke-width=\"2\" stroke-linecap=\"round\" stroke-linejoin=\"round\" class=\"feather feather-arrow-right\">',",
            "        f'          <line x1=\"5\" y1=\"12\" x2=\"19\" y2=\"12\"></line>',",
            "        f'          <polyline points=\"12 5 19 12 12 19\"></polyline>',",
            "        f'          </svg>',",
            "        f'          </dd>',",
            "        f'        </dl>',",
            "        f'        <dl class=\"leading-4\">',",
            "        f'        <dd class=\"text-sm\"><h3>{title}</h3></dd>',",
            "        f'        <dt class=\"flex items-center gap-1 truncate whitespace-nowrap text-[.82rem] text-gray-400\">{subtitle}</dt>',",
            "        f'        </dl>',",
            "        f'    </summary>',",
            "        f' <div class=\"content px-5 pb-5 pt-4\">',",
            "        content,",
            "        f' </div>',",
            "        f' </details>\\n'",
            "        ])",
            "",
            "    def internet_search_with_vectorization(self, query, quick_search:bool=False ):",
            "        \"\"\"",
            "        Do internet search and return the result",
            "        \"\"\"",
            "        return self.personality.internet_search_with_vectorization(query, quick_search=quick_search)",
            "",
            "",
            "    def vectorize_and_query(self, text, query, max_chunk_size=512, overlap_size=20, internet_vectorization_nb_chunks=3):",
            "        vectorizer = TextVectorizer(VectorizationMethod.TFIDF_VECTORIZER, model = self.personality.model)",
            "        decomposer = DocumentDecomposer()",
            "        chunks = decomposer.decompose_document(text, max_chunk_size, overlap_size,self.personality.model.tokenize,self.personality.model.detokenize)",
            "        for i, chunk in enumerate(chunks):",
            "            vectorizer.add_document(f\"chunk_{i}\", self.personality.model.detokenize(chunk))",
            "        vectorizer.index()",
            "        docs, sorted_similarities, document_ids = vectorizer.recover_text(query, internet_vectorization_nb_chunks)",
            "        return docs, sorted_similarities",
            "",
            "",
            "    def step_start(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step start",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step start to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_START)",
            "",
            "    def step_end(self, step_text, status=True, callback: Callable[[str, int, dict, list], bool]=None):",
            "        \"\"\"This triggers a step end",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the step end to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_END, {'status':status})",
            "",
            "    def step(self, step_text, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This triggers a step information",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP)",
            "",
            "    def exception(self, ex, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends exception to the client",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(str(ex), MSG_TYPE.MSG_TYPE_EXCEPTION)",
            "",
            "    def warning(self, warning:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends exception to the client",
            "",
            "        Args:",
            "            step_text (str): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(warning, MSG_TYPE.MSG_TYPE_EXCEPTION)",
            "",
            "",
            "    def json(self, title:str, json_infos:dict, callback: Callable[[str, int, dict, list], bool]=None, indent=4):",
            "        \"\"\"This sends json data to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(\"\", MSG_TYPE.MSG_TYPE_JSON_INFOS, metadata = [{\"title\":title, \"content\":json.dumps(json_infos, indent=indent)}])",
            "",
            "    def ui(self, html_ui:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends ui elements to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(html_ui, MSG_TYPE.MSG_TYPE_UI)",
            "",
            "    def code(self, code:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends code to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE, dict, list) to send the step to. Defaults to None.",
            "            The callback has these fields:",
            "            - chunk",
            "            - Message Type : the type of message",
            "            - Parameters (optional) : a dictionary of parameters",
            "            - Metadata (optional) : a list of metadata",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(code, MSG_TYPE.MSG_TYPE_CODE)",
            "",
            "    def chunk(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_CHUNK)",
            "",
            "",
            "    def full(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None, msg_type:MSG_TYPE = MSG_TYPE.MSG_TYPE_FULL):",
            "        \"\"\"This sends full text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, msg_type)",
            "",
            "    def full_invisible_to_ai(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to AI)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_AI)",
            "",
            "    def full_invisible_to_user(self, full_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends full text to front end (INVISIBLE to user)",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the text to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(full_text, MSG_TYPE.MSG_TYPE_FULL_INVISIBLE_TO_USER)",
            "",
            "",
            "",
            "",
            "    def execute_python(self, code, code_folder=None, code_file_name=None):",
            "        if code_folder is not None:",
            "            code_folder = Path(code_folder)",
            "",
            "        \"\"\"Executes Python code and returns the output as JSON.\"\"\"",
            "        # Create a temporary file.",
            "        root_folder = code_folder if code_folder is not None else self.personality.personality_output_folder",
            "        root_folder.mkdir(parents=True,exist_ok=True)",
            "        tmp_file = root_folder/(code_file_name if code_file_name is not None else f\"ai_code.py\")",
            "        with open(tmp_file,\"w\") as f:",
            "            f.write(code)",
            "",
            "        # Execute the Python code in a temporary file.",
            "        process = subprocess.Popen(",
            "            [\"python\", str(tmp_file)],",
            "            stdout=subprocess.PIPE,",
            "            stderr=subprocess.PIPE,",
            "            cwd=root_folder",
            "        )",
            "",
            "        # Get the output and error from the process.",
            "        output, error = process.communicate()",
            "",
            "        # Check if the process was successful.",
            "        if process.returncode != 0:",
            "            # The child process threw an exception.",
            "            error_message = f\"Error executing Python code: {error.decode('utf8')}\"",
            "            return error_message",
            "",
            "        # The child process was successful.",
            "        return output.decode(\"utf8\")",
            "",
            "    def build_python_code(self, prompt, max_title_length=4096):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        if not PackageManager.check_package_installed(\"autopep8\"):",
            "            PackageManager.install_package(\"autopep8\")",
            "        import autopep8",
            "        global_prompt = \"\\n\".join([",
            "            f\"{prompt}\",",
            "            f\"{start_header_id_template}Extra conditions{end_header_id_template}\",",
            "            \"- The code must be complete, not just snippets, and should be put inside a single python markdown code.\",",
            "            \"-Preceive each python codeblock with a line using this syntax:\",",
            "            \"$$file_name|the file path relative to the root folder of the project$$\",",
            "            \"```python\",",
            "            \"# Placeholder. Here you need to put the code for the file\",",
            "            \"```\",",
            "            f\"{start_header_id_template}Code Builder{end_header_id_template}\"",
            "        ])",
            "        code = self.fast_gen(global_prompt, max_title_length)",
            "        code_blocks = self.extract_code_blocks(code)",
            "        try:",
            "            back_quote_index = code.index(\"```\")  # Remove trailing backticks",
            "            if back_quote_index>=0:",
            "                # Removing any extra text",
            "                code = code[:back_quote_index]",
            "        except:",
            "            pass",
            "        formatted_code = autopep8.fix_code(code)  # Fix indentation errors",
            "        return formatted_code",
            "",
            "",
            "    def make_title(self, prompt, max_title_length: int = 50):",
            "        \"\"\"",
            "        Generates a title for a given prompt.",
            "",
            "        Args:",
            "            prompt (str): The prompt for which a title needs to be generated.",
            "            max_title_length (int, optional): The maximum length of the generated title. Defaults to 50.",
            "",
            "        Returns:",
            "            str: The generated title.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        separator_template          = self.config.separator_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        global_prompt = f\"{start_header_id_template}{system_message_template}{end_header_id_template}Based on the provided prompt, suggest a concise and relevant title that captures the main topic or theme of the conversation. Only return the suggested title, without any additional text or explanation.{separator_template}{start_header_id_template}prompt{end_header_id_template}{prompt}{separator_template}{start_header_id_template}title{end_header_id_template}\"",
            "        title = self.fast_gen(global_prompt,max_title_length)",
            "        return title",
            "",
            "",
            "    def plan_with_images(self, request: str, images:list, actions_list:list=[LoLLMsAction], context:str = \"\", max_answer_length: int = 512) -> List[LoLLMsAction]:",
            "        \"\"\"",
            "        creates a plan out of a request and a context",
            "",
            "        Args:",
            "            request (str): The request posed by the user.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        template = \"\\n\".join([",
            "            f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "            \"Act as plan builder, a tool capable of making plans to perform the user requested operation.\"",
            "        ])",
            "",
            "        if len(actions_list)>0:",
            "            template += \"\\n\".join([",
            "                \"The plan builder is an AI that responds in json format. It should plan a succession of actions in order to reach the objective.\",",
            "                f\"{start_header_id_template}list of action types information{end_header_id_template}\",",
            "                \"[\",",
            "                \"{actions_list}\",",
            "                \"]\",",
            "                \"The AI should respond in this format using data from actions_list:\",",
            "                \"{\",",
            "                '    \"actions\": [',",
            "                '    {',",
            "                '        \"name\": name of the action 1,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    },',",
            "                '    {',",
            "                '        \"name\": name of the action 2,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    }',",
            "                '    ...',",
            "                '    ]',",
            "                \"}\"",
            "            ])",
            "        if context != \"\":",
            "            template += \"\\n\".join([",
            "                f\"{start_header_id_template}context{end_header_id_template}\",",
            "                \"{context}Ok\"",
            "            ])",
            "",
            "        template += \"\\n\".join([",
            "            f\"{start_header_id_template}request{end_header_id_template}{{request}}\",",
            "            f\"{start_header_id_template}plan{end_header_id_template}To achieve the requested objective, this is the list of actions to follow, formatted as requested in json format:\\n```json\\n\"",
            "        ])",
            "        pr  = PromptReshaper(template)",
            "        prompt = pr.build({",
            "                \"context\":context,",
            "                \"request\":request,",
            "                \"actions_list\":\",\\n\".join([f\"{action}\" for action in actions_list])",
            "                },",
            "                self.personality.model.tokenize,",
            "                self.personality.model.detokenize,",
            "                self.personality.model.config.ctx_size,",
            "                [\"previous_discussion\"]",
            "                )",
            "        gen = self.generate_with_images(prompt, images, max_answer_length).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        gen = self.remove_backticks(gen)",
            "        self.print_prompt(\"full\",prompt+gen)",
            "        gen = fix_json(gen)",
            "        return generate_actions(actions_list, gen)",
            "",
            "    def plan(self, request: str, actions_list:list=[LoLLMsAction], context:str = \"\", max_answer_length: int = 512) -> List[LoLLMsAction]:",
            "        \"\"\"",
            "        creates a plan out of a request and a context",
            "",
            "        Args:",
            "            request (str): The request posed by the user.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        template = \"\\n\".join([",
            "            f\"{start_header_id_template}instruction:\",",
            "            \"Act as plan builder, a tool capable of making plans to perform the user requested operation.\"",
            "        ])",
            "",
            "        if len(actions_list) > 0:",
            "            template += \"\\n\".join([",
            "                \"The plan builder is an AI that responds in json format. It should plan a succession of actions in order to reach the objective.\",",
            "                f\"{start_header_id_template}list of action types information{end_header_id_template}\",",
            "                \"[\",",
            "                \"{actions_list}\",",
            "                \"]\",",
            "                \"The AI should respond in this format using data from actions_list:\",",
            "                \"{\",",
            "                '    \"actions\": [',",
            "                '    {',",
            "                '        \"name\": name of the action 1,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    },',",
            "                '    {',",
            "                '        \"name\": name of the action 2,',",
            "                '        \"parameters\":[',",
            "                '            parameter name: parameter value',",
            "                '        ]',",
            "                '    }',",
            "                '    ...',",
            "                '    ]',",
            "                \"}\"",
            "            ])",
            "",
            "        if context != \"\":",
            "            template += \"\\n\".join([",
            "                f\"{start_header_id_template}context{end_header_id_template}\",",
            "                \"{context}Ok\"",
            "            ])",
            "",
            "        template += \"\\n\".join([",
            "            f\"{start_header_id_template}request{end_header_id_template}{{request}}\",",
            "            f\"{start_header_id_template}plan{end_header_id_template}To achieve the requested objective, this is the list of actions to follow, formatted as requested in json format:\\n```json\\n\"",
            "        ])",
            "        pr  = PromptReshaper(template)",
            "        prompt = pr.build({",
            "                \"context\":context,",
            "                \"request\":request,",
            "                \"actions_list\":\",\\n\".join([f\"{action}\" for action in actions_list])",
            "                },",
            "                self.personality.model.tokenize,",
            "                self.personality.model.detokenize,",
            "                self.personality.model.config.ctx_size,",
            "                [\"previous_discussion\"]",
            "                )",
            "        gen = self.generate(prompt, max_answer_length).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        gen = self.remove_backticks(gen).strip()",
            "        if gen[-1]!=\"}\":",
            "            gen+=\"}\"",
            "        self.print_prompt(\"full\",prompt+gen)",
            "        gen = fix_json(gen)",
            "        return generate_actions(actions_list, gen)",
            "",
            "",
            "    def parse_directory_structure(self, structure):",
            "        paths = []",
            "        lines = structure.strip().split('\\n')",
            "        stack = []",
            "",
            "        for line in lines:",
            "            line = line.rstrip()",
            "            level = (len(line) - len(line.lstrip())) // 4",
            "",
            "            if '/' in line or line.endswith(':'):",
            "                directory = line.strip(' \u251c\u2500\u2514\u2502').rstrip(':').rstrip('/')",
            "",
            "                while stack and level < stack[-1][0]:",
            "                    stack.pop()",
            "",
            "                stack.append((level, directory))",
            "                path = '/'.join([dir for _, dir in stack]) + '/'",
            "                paths.append(path)",
            "            else:",
            "                file = line.strip(' \u251c\u2500\u2514\u2502')",
            "                if stack:",
            "                    path = '/'.join([dir for _, dir in stack]) + '/' + file",
            "                    paths.append(path)",
            "",
            "        return paths",
            "",
            "    def extract_code_blocks(self, text: str) -> List[dict]:",
            "        \"\"\"",
            "        This function extracts code blocks from a given text.",
            "",
            "        Parameters:",
            "        text (str): The text from which to extract code blocks. Code blocks are identified by triple backticks (```).",
            "",
            "        Returns:",
            "        List[dict]: A list of dictionaries where each dictionary represents a code block and contains the following keys:",
            "            - 'index' (int): The index of the code block in the text.",
            "            - 'file_name' (str): An empty string. This field is not used in the current implementation.",
            "            - 'content' (str): The content of the code block.",
            "            - 'type' (str): The type of the code block. If the code block starts with a language specifier (like 'python' or 'java'), this field will contain that specifier. Otherwise, it will be set to 'language-specific'.",
            "",
            "        Note:",
            "        The function assumes that the number of triple backticks in the text is even.",
            "        If the number of triple backticks is odd, it will consider the rest of the text as the last code block.",
            "        \"\"\"        ",
            "        remaining = text",
            "        bloc_index = 0",
            "        first_index=0",
            "        indices = []",
            "        while len(remaining)>0:",
            "            try:",
            "                index = remaining.index(\"```\")",
            "                indices.append(index+first_index)",
            "                remaining = remaining[index+3:]",
            "                first_index += index+3",
            "                bloc_index +=1",
            "            except Exception as ex:",
            "                if bloc_index%2==1:",
            "                    index=len(remaining)",
            "                    indices.append(index)",
            "                remaining = \"\"",
            "",
            "        code_blocks = []",
            "        is_start = True",
            "        for index, code_delimiter_position in enumerate(indices):",
            "            block_infos = {",
            "                'index':index,",
            "                'file_name': \"\",",
            "                'content': \"\",",
            "                'type':\"\"",
            "            }",
            "            if is_start:",
            "",
            "                sub_text = text[code_delimiter_position+3:]",
            "                if len(sub_text)>0:",
            "                    try:",
            "                        find_space = sub_text.index(\" \")",
            "                    except:",
            "                        find_space = int(1e10)",
            "                    try:",
            "                        find_return = sub_text.index(\"\\n\")",
            "                    except:",
            "                        find_return = int(1e10)",
            "                    next_index = min(find_return, find_space)",
            "                    if '{' in sub_text[:next_index]:",
            "                        next_index =0",
            "                    start_pos = next_index",
            "                    if code_delimiter_position+3<len(text) and text[code_delimiter_position+3] in [\"\\n\",\" \",\"\\t\"] :",
            "                        # No",
            "                        block_infos[\"type\"]='language-specific'",
            "                    else:",
            "                        block_infos[\"type\"]=sub_text[:next_index]",
            "",
            "                    next_pos = indices[index+1]-code_delimiter_position",
            "                    if sub_text[next_pos-3]==\"`\":",
            "                        block_infos[\"content\"]=sub_text[start_pos:next_pos-3].strip()",
            "                    else:",
            "                        block_infos[\"content\"]=sub_text[start_pos:next_pos].strip()",
            "                    code_blocks.append(block_infos)",
            "                is_start = False",
            "            else:",
            "                is_start = True",
            "                continue",
            "",
            "        return code_blocks",
            "",
            "",
            "",
            "    def build_and_execute_python_code(self,context, instructions, execution_function_signature, extra_imports=\"\"):",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        code = \"```python\\n\"+self.fast_gen(",
            "            self.build_prompt([",
            "            f\"{start_header_id_template}context{end_header_id_template}\",",
            "            context,",
            "            f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "            f\"{instructions}\",",
            "            f\"Here is the signature of the function:\\n{execution_function_signature}\",",
            "            \"Don't call the function, just write it\",",
            "            \"Do not provide usage example.\",",
            "            \"The code must me without comments\",",
            "            f\"{start_header_id_template}coder{end_header_id_template}Sure, in the following code, I import the necessary libraries, then define the function as you asked.\",",
            "            \"The function is ready to be used in your code and performs the task as you asked:\",",
            "            \"```python\\n\"",
            "            ],2), callback=self.sink)",
            "        code = code.replace(\"```python\\n```python\\n\", \"```python\\n\").replace(\"```\\n```\",\"```\")",
            "        code=self.extract_code_blocks(code)",
            "",
            "        if len(code)>0:",
            "            # Perform the search query",
            "            code = code[0][\"content\"]",
            "            code = \"\\n\".join([",
            "                        extra_imports,",
            "                        code",
            "                    ])",
            "            ASCIIColors.magenta(code)",
            "            module_name = 'custom_module'",
            "            spec = importlib.util.spec_from_loader(module_name, loader=None)",
            "            module = importlib.util.module_from_spec(spec)",
            "            exec(code, module.__dict__)",
            "            return module, code",
            "",
            "",
            "    def yes_no(self, question: str, context:str=\"\", max_answer_length: int = 50, conditionning=\"\") -> bool:",
            "        \"\"\"",
            "        Analyzes the user prompt and answers whether it is asking to generate an image.",
            "",
            "        Args:",
            "            question (str): The user's message.",
            "            max_answer_length (int, optional): The maximum length of the generated answer. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "        Returns:",
            "            bool: True if the user prompt is asking to generate an image, False otherwise.",
            "        \"\"\"",
            "        return self.multichoice_question(question, [\"no\",\"yes\"], context, max_answer_length, conditionning=conditionning)>0",
            "",
            "    def multichoice_question(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Interprets a multi-choice question from a users response. This function expects only one choice as true. All other choices are considered false. If none are correct, returns -1.",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}{system_message_template}{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                       f\"{start_header_id_template}Context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "        elements +=[",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "                \"Do not explain your reasons or add comments.\",",
            "                \"the output should be an integer.\"",
            "        ]",
            "        elements += [",
            "                f\"{start_header_id_template}question{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50, callback=self.sink).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        if len(gen)>0:",
            "            selection = gen.strip().split()[0].replace(\",\",\"\").replace(\".\",\"\")",
            "            self.print_prompt(\"Multi choice selection\",prompt+gen)",
            "            try:",
            "                return int(selection)",
            "            except:",
            "                ASCIIColors.cyan(\"Model failed to answer the question\")",
            "                return -1",
            "        else:",
            "            return -1",
            "",
            "    def multichoice_ranking(self, question: str, possible_answers:list, context:str = \"\", max_answer_length: int = 50, conditionning=\"\") -> int:",
            "        \"\"\"",
            "        Ranks answers for a question from best to worst. returns a list of integers",
            "",
            "        Args:",
            "            question (str): The multi-choice question posed by the user.",
            "            possible_ansers (List[Any]): A list containing all valid options for the chosen value. For each item in the list, either 'True', 'False', None or another callable should be passed which will serve as the truth test function when checking against the actual user input.",
            "            max_answer_length (int, optional): Maximum string length allowed while interpreting the users' responses. Defaults to 50.",
            "            conditionning: An optional system message to put at the beginning of the prompt",
            "",
            "        Returns:",
            "            int: Index of the selected option within the possible_ansers list. Or -1 if there was not match found among any of them.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "",
            "        choices = \"\\n\".join([f\"{i}. {possible_answer}\" for i, possible_answer in enumerate(possible_answers)])",
            "        elements = [conditionning] if conditionning!=\"\" else []",
            "        elements += [",
            "                f\"{start_header_id_template}instructions{end_header_id_template}\",",
            "                \"Answer this multi choices question.\",",
            "                \"Answer with an id from the possible answers.\",",
            "                \"Do not answer with an id outside this possible answers.\",",
            "                f\"{start_header_id_template}question{end_header_id_template}{question}\",",
            "                f\"{start_header_id_template}possible answers{end_header_id_template}\",",
            "                f\"{choices}\",",
            "        ]",
            "        if context!=\"\":",
            "            elements+=[",
            "                        f\"{start_header_id_template}context{end_header_id_template}\",",
            "                        f\"{context}\",",
            "                    ]",
            "",
            "        elements += [f\"{start_header_id_template}answer{end_header_id_template}\"]",
            "        prompt = self.build_prompt(elements)",
            "",
            "        gen = self.generate(prompt, max_answer_length, temperature=0.1, top_k=50, top_p=0.9, repeat_penalty=1.0, repeat_last_n=50).strip().replace(\"</s>\",\"\").replace(\"<s>\",\"\")",
            "        self.print_prompt(\"Multi choice ranking\",prompt+gen)",
            "        if gen.index(\"]\")>=0:",
            "            try:",
            "                ranks = eval(gen.split(\"]\")[0]+\"]\")",
            "                return ranks",
            "            except:",
            "                ASCIIColors.red(\"Model failed to rank inputs\")",
            "                return None",
            "        else:",
            "            ASCIIColors.red(\"Model failed to rank inputs\")",
            "            return None",
            "",
            "",
            "",
            "    def build_html5_integration(self, html, ifram_name=\"unnamed\"):",
            "        \"\"\"",
            "        This function creates an HTML5 iframe with the given HTML content and iframe name.",
            "",
            "        Args:",
            "        html (str): The HTML content to be displayed in the iframe.",
            "        ifram_name (str, optional): The name of the iframe. Defaults to \"unnamed\".",
            "",
            "        Returns:",
            "        str: The HTML string for the iframe.",
            "        \"\"\"",
            "        return \"\\n\".join(",
            "            '<div style=\"width: 80%; margin: 0 auto;\">',",
            "            f'<iframe id=\"{ifram_name}\" srcdoc=\"',",
            "            html,",
            "            '\" style=\"width: 100%; height: 600px; border: none;\"></iframe>',",
            "            '</div>'",
            "        )",
            "",
            "",
            "    def InfoMessage(self, content, client_id=None, verbose:bool=None):",
            "        self.personality.app.notify(",
            "                content, ",
            "                notification_type=NotificationType.NOTIF_SUCCESS, ",
            "                duration=0, ",
            "                client_id=client_id, ",
            "                display_type=NotificationDisplayType.MESSAGE_BOX,",
            "                verbose=verbose",
            "            )",
            "",
            "    def info(self, info_text:str, callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends info text to front end",
            "",
            "        Args:",
            "            step_text (dict): The step text",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the info to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(info_text, MSG_TYPE.MSG_TYPE_FULL)",
            "",
            "    def step_progress(self, step_text:str, progress:float, callback: Callable[[str, MSG_TYPE, dict, list, AIPersonality], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(step_text, MSG_TYPE.MSG_TYPE_STEP_PROGRESS, {'progress':progress})",
            "",
            "    def new_message(self, message_text:str, message_type:MSG_TYPE= MSG_TYPE.MSG_TYPE_FULL, metadata=[], callback: Callable[[str, int, dict, list, AIPersonality], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(message_text, MSG_TYPE.MSG_TYPE_NEW_MESSAGE, parameters={'type':message_type.value,'metadata':metadata},personality = self.personality)",
            "",
            "    def finished_message(self, message_text:str=\"\", callback: Callable[[str, MSG_TYPE, dict, list], bool]=None):",
            "        \"\"\"This sends step rogress to front end",
            "",
            "        Args:",
            "            step_text (dict): The step progress in %",
            "            callback (callable, optional): A callable with this signature (str, MSG_TYPE) to send the progress to. Defaults to None.",
            "        \"\"\"",
            "        if not callback and self.callback:",
            "            callback = self.callback",
            "",
            "        if callback:",
            "            callback(message_text, MSG_TYPE.MSG_TYPE_FINISHED_MESSAGE)",
            "",
            "    def print_prompt(self, title, prompt):",
            "        ASCIIColors.red(\"*-*-*-*-*-*-*-* \", end=\"\")",
            "        ASCIIColors.red(title, end=\"\")",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "        ASCIIColors.yellow(prompt)",
            "        ASCIIColors.red(\" *-*-*-*-*-*-*-*\")",
            "",
            "",
            "    def fast_gen_with_images(self, prompt: str, images:list, max_generation_size: int= None, placeholders: dict = {}, sacrifice: list = [\"previous_discussion\"], debug: bool = False, callback=None, show_progress=False) -> str:",
            "        \"\"\"",
            "        Fast way to generate code",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        return self.personality.fast_gen_with_images(prompt=prompt, images=images, max_generation_size=max_generation_size,placeholders=placeholders, sacrifice=sacrifice, debug=debug, callback=callback, show_progress=show_progress)",
            "",
            "    def fast_gen(self, prompt: str, max_generation_size: int= None, placeholders: dict = {}, sacrifice: list = [\"previous_discussion\"], debug: bool = False, callback=None, show_progress=False) -> str:",
            "        \"\"\"",
            "        Fast way to generate code",
            "",
            "        This method takes in a prompt, maximum generation size, optional placeholders, sacrifice list, and debug flag.",
            "        It reshapes the context before performing text generation by adjusting and cropping the number of tokens.",
            "",
            "        Parameters:",
            "        - prompt (str): The input prompt for text generation.",
            "        - max_generation_size (int): The maximum number of tokens to generate.",
            "        - placeholders (dict, optional): A dictionary of placeholders to be replaced in the prompt. Defaults to an empty dictionary.",
            "        - sacrifice (list, optional): A list of placeholders to sacrifice if the window is bigger than the context size minus the number of tokens to generate. Defaults to [\"previous_discussion\"].",
            "        - debug (bool, optional): Flag to enable/disable debug mode. Defaults to False.",
            "",
            "        Returns:",
            "        - str: The generated text after removing special tokens (\"<s>\" and \"</s>\") and stripping any leading/trailing whitespace.",
            "        \"\"\"",
            "        return self.personality.fast_gen(prompt=prompt,max_generation_size=max_generation_size,placeholders=placeholders, sacrifice=sacrifice, debug=debug, callback=callback, show_progress=show_progress)",
            "",
            "",
            "",
            "    def generate_with_function_calls(self, context_details: dict, functions: List[Dict[str, Any]], max_answer_length: Optional[int] = None, callback = None) -> List[Dict[str, Any]]:",
            "        \"\"\"",
            "        Performs text generation with function calls.",
            "",
            "        Args:",
            "            context_details (dict): The full prompt (including conditioning, user discussion, extra data, and the user prompt).",
            "            functions (List[Dict[str, Any]]): A list of dictionaries describing functions that can be called.",
            "            max_answer_length (int, optional): Maximum string length allowed for the generated text.",
            "",
            "        Returns:",
            "            List[Dict[str, Any]]: A list of dictionaries with the function names and parameters to execute.",
            "        \"\"\"",
            "",
            "",
            "        # Upgrade the prompt with information about the function calls.",
            "        upgraded_prompt = self._upgrade_prompt_with_function_info(context_details, functions)",
            "",
            "        # Generate the initial text based on the upgraded prompt.",
            "        generated_text = self.fast_gen(upgraded_prompt, max_answer_length, callback=callback)",
            "",
            "        if self.config.debug:",
            "            self.print_prompt(\"Generated\", generated_text)",
            "",
            "        # Extract the function calls from the generated text.",
            "        function_calls = self.extract_function_calls_as_json(generated_text)",
            "",
            "        return generated_text, function_calls",
            "",
            "",
            "    def generate_with_function_calls_and_images(self, context_details: dict, images:list, functions: List[Dict[str, Any]], max_answer_length: Optional[int] = None, callback = None) -> List[Dict[str, Any]]:",
            "        \"\"\"",
            "        Performs text generation with function calls.",
            "",
            "        Args:",
            "            prompt (str): The full prompt (including conditioning, user discussion, extra data, and the user prompt).",
            "            functions (List[Dict[str, Any]]): A list of dictionaries describing functions that can be called.",
            "            max_answer_length (int, optional): Maximum string length allowed for the generated text.",
            "",
            "        Returns:",
            "            List[Dict[str, Any]]: A list of dictionaries with the function names and parameters to execute.",
            "        \"\"\"",
            "        # Upgrade the prompt with information about the function calls.",
            "        upgraded_prompt = self._upgrade_prompt_with_function_info(context_details, functions)",
            "",
            "        # Generate the initial text based on the upgraded prompt.",
            "        generated_text = self.fast_gen_with_images(upgraded_prompt, images, max_answer_length, callback=callback)",
            "",
            "        # Extract the function calls from the generated text.",
            "        function_calls = self.extract_function_calls_as_json(generated_text)",
            "",
            "        return generated_text, function_calls",
            "",
            "    def execute_function(self, code, function_definitions = None):",
            "        function_call = json.loads(code)",
            "        self.execute_function_calls([function_call], function_definitions=function_definitions )",
            "",
            "    def execute_function_calls(self, function_calls: List[Dict[str, Any]], function_definitions: List[Dict[str, Any]]) -> List[Any]:",
            "        \"\"\"",
            "        Executes the function calls with the parameters extracted from the generated text,",
            "        using the original functions list to find the right function to execute.",
            "",
            "        Args:",
            "            function_calls (List[Dict[str, Any]]): A list of dictionaries representing the function calls.",
            "            function_definitions (List[Dict[str, Any]]): The original list of functions with their descriptions and callable objects.",
            "",
            "        Returns:",
            "            List[Any]: A list of results from executing the function calls.",
            "        \"\"\"",
            "        if function_definitions is None:",
            "            function_definitions = self.function_definitions",
            "        results = []",
            "        # Convert function_definitions to a dict for easier lookup",
            "        functions_dict = {func['function_name']: func for func in function_definitions}",
            "",
            "        for call in function_calls:",
            "            keys = [k for k in call.keys()]",
            "            if not (\"function_name\" in keys):",
            "                key = keys[0] if len(keys)>0 else None",
            "                d = call[key] if key else None",
            "                function_name = key",
            "                parameters = d",
            "            else:",
            "                function_name = call.get(\"function_name\", None) or call.get(\"function\", None)",
            "                parameters = call.get(\"function_parameters\", None)",
            "            fn =  functions_dict.get(function_name)",
            "            if fn:",
            "                function = fn['function']",
            "                try:",
            "                    # Assuming parameters is a dictionary that maps directly to the function's arguments.",
            "                    if type(parameters)==list:",
            "                        f_parameters ={k:v for k,v in zip([p['name'] for p in fn['function_parameters']],parameters)}",
            "                        result = function(**f_parameters)",
            "                        results.append(result)",
            "                    elif type(parameters)==dict:",
            "                        result = function(**parameters)",
            "                        results.append(result)",
            "                except TypeError as e:",
            "                    trace_exception(e)",
            "                    # Handle cases where the function call fails due to incorrect parameters, etc.",
            "                    results.append(f\"Error calling {function_name}: {e}\")",
            "            else:",
            "                results.append(f\"Function {function_name} not found.\")",
            "",
            "        return results",
            "",
            "    def transform_functions_to_text(self, functions):",
            "        function_texts = []",
            "",
            "        for func in functions:",
            "            function_text = f'Function: {func[\"function_name\"]}\\nDescription: {func[\"function_description\"]}\\nParameters:\\n'",
            "            ",
            "            for param in func[\"function_parameters\"]:",
            "                param_type = \"string\" if param[\"type\"] == \"str\" else param[\"type\"]",
            "                param_description = param.get(\"description\", \"\")",
            "                function_text += f'  - {param[\"name\"]} ({param_type}): {param_description}\\n'",
            "            ",
            "            function_texts.append(function_text.strip())",
            "        ",
            "        return \"\\n\\n\".join(function_texts)",
            "    ",
            "    def transform_functions(self, functions):",
            "        tools = []",
            "",
            "        for func in functions:",
            "            function_dict = {",
            "                \"type\": \"function\",",
            "                \"function\": {",
            "                    \"name\": func[\"function_name\"],",
            "                    \"description\": func[\"function_description\"],",
            "                    \"parameters\": {",
            "                        \"type\": \"object\",",
            "                        \"properties\": {},",
            "                        \"required\": [],",
            "                    },",
            "                },",
            "            }",
            "            ",
            "            for param in func[\"function_parameters\"]:",
            "                function_dict[\"function\"][\"parameters\"][\"properties\"][param[\"name\"]] = {",
            "                    \"type\": \"string\" if param[\"type\"] == \"str\" else param[\"type\"],",
            "                    \"description\": param.get(\"description\", \"\"),",
            "                }",
            "                function_dict[\"function\"][\"parameters\"][\"required\"].append(param[\"name\"])",
            "            ",
            "            tools.append(function_dict)",
            "",
            "        return tools",
            "",
            "    def _upgrade_prompt_with_function_info(self, context_details: dict, functions: List[Dict[str, Any]]) -> str:",
            "        \"\"\"",
            "        Upgrades the prompt with information about function calls.",
            "",
            "        Args:",
            "            context_details (dict): The original prompt.",
            "            functions (List[Dict[str, Any]]): A list of dictionaries describing functions that can be called.",
            "",
            "        Returns:",
            "            str: The upgraded prompt that includes information about the function calls.",
            "        \"\"\"",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        separator_template          = self.config.separator_template",
            "",
            "",
            "        tools = self.transform_functions_to_text(functions)",
            "        import copy",
            "        cd = copy.deepcopy(context_details)",
            "        function_descriptions = [",
            "                                 f\"{start_header_id_template}Available functions{end_header_id_template}\\n\",",
            "                                 tools,",
            "                                 \"\",",
            "                                 cd[\"conditionning\"],",
            "                                 \"Your objective is interact with the user and if you need to call a function, then use the available functions above and call them using the following json format inside a markdown tag:\"",
            "                                 \"```function\",",
            "                                 \"{\",",
            "                                 '\"function_name\":the name of the function to be called,',",
            "                                 '\"function_parameters\": a list of  parameter values',",
            "                                 \"}\",",
            "                                 \"```\",",
            "                                 ]",
            "",
            "",
            "        # Combine the function descriptions with the original prompt.",
            "        function_info = '\\n'.join(function_descriptions)",
            "",
            "        cd[\"conditionning\"]=function_info",
            "        upgraded_prompt = self.build_prompt_from_context_details(cd)",
            "",
            "        ",
            "        return upgraded_prompt",
            "",
            "    def extract_function_calls_as_json(self, text: str) -> List[Dict[str, Any]]:",
            "        \"\"\"",
            "        Extracts function calls formatted as JSON inside markdown code blocks.",
            "",
            "        Args:",
            "            text (str): The generated text containing JSON markdown entries for function calls.",
            "",
            "        Returns:",
            "            List[Dict[str, Any]]: A list of dictionaries representing the function calls.",
            "        \"\"\"",
            "",
            "        # Extract markdown code blocks that contain JSON.",
            "        code_blocks = self.extract_code_blocks(text)",
            "",
            "        # Filter out and parse JSON entries.",
            "        function_calls = []",
            "        for block in code_blocks:",
            "            if block[\"type\"]==\"function\" or block[\"type\"]==\"json\" or block[\"type\"]==\"\":",
            "                content = block.get(\"content\", \"\")",
            "                try:",
            "                    # Attempt to parse the JSON content of the code block.",
            "                    function_call = json.loads(content)",
            "                    if type(function_call)==dict:",
            "                        function_calls.append(function_call)",
            "                    elif type(function_call)==list:",
            "                        function_calls+=function_call",
            "                except json.JSONDecodeError:",
            "                    # If the content is not valid JSON, skip it.",
            "                    continue",
            "",
            "        return function_calls",
            "",
            "",
            "    def interact(",
            "                    self, ",
            "                    context_details, ",
            "                    callback = None",
            "                    ):",
            "        upgraded_prompt = self.build_prompt_from_context_details(context_details)",
            "        if len(self.personality.image_files)>0:",
            "            # Generate the initial text based on the upgraded prompt.",
            "            generated_text = self.fast_gen_with_images(upgraded_prompt, self.personality.image_files, callback=callback)",
            "        else:    ",
            "            generated_text = self.fast_gen(upgraded_prompt, callback=callback)",
            "",
            "        return generated_text",
            "",
            "    def interact_with_function_call(",
            "                                        self, ",
            "                                        context_details, ",
            "                                        function_definitions, ",
            "                                        prompt_after_execution=True, ",
            "                                        callback = None, ",
            "                                        hide_function_call=False,",
            "                                        separate_output=False,",
            "                                        max_nested_function_calls=10):",
            "        ",
            "        start_header_id_template    = self.config.start_header_id_template",
            "        end_header_id_template      = self.config.end_header_id_template",
            "        system_message_template     = self.config.system_message_template",
            "        separator_template          = self.config.separator_template",
            "",
            "        final_output = \"\"",
            "        if len(self.personality.image_files)>0:",
            "            out, function_calls = self.generate_with_function_calls_and_images(context_details, self.personality.image_files, function_definitions, callback=callback)",
            "        else:",
            "            out, function_calls = self.generate_with_function_calls(context_details, function_definitions, callback=callback)",
            "        nested_function_calls = 0",
            "        while len(function_calls)>0 and nested_function_calls<max_nested_function_calls:",
            "            nested_function_calls += 1",
            "            self.chunk(\"\\n\") ",
            "            if hide_function_call:",
            "                self.full(\"\") #Hide function ",
            "",
            "            if self.config.debug:",
            "                self.print_prompt(\"Function calls\", json.dumps(function_calls, indent=4))",
            "",
            "            outputs = self.execute_function_calls(function_calls,function_definitions)",
            "            final_output = \"\\n\".join([str(o) if type(o)==str else str(o[0]) if (type(o)==tuple or type(0)==list) and len(o)>0 else \"\" for o in outputs])",
            "            out += f\"{separator_template}{start_header_id_template}function calls results{end_header_id_template}\\n\" + final_output + \"\\n\"",
            "            if prompt_after_execution:",
            "                if separate_output:",
            "                    self.full(final_output)",
            "                    self.new_message(\"\")",
            "                context_details[\"discussion_messages\"] +=out",
            "                if len(self.personality.image_files)>0:",
            "                    out, function_calls = self.generate_with_function_calls_and_images(context_details, self.personality.image_files, function_definitions, callback=callback)",
            "                else:",
            "                    out, function_calls = self.generate_with_function_calls(context_details, function_definitions, callback=callback)",
            "                final_output += \"\\n\" + out",
            "        else:",
            "            final_output = out",
            "        return final_output",
            "",
            "    #Helper method to convert outputs path to url",
            "    def path2url(file):",
            "        file = str(file).replace(\"\\\\\",\"/\")",
            "        pth = file.split('/')",
            "        idx = pth.index(\"outputs\")",
            "        pth = \"/\".join(pth[idx:])",
            "        file_path = f\"![](/{pth})\\n\"",
            "        return file_path",
            "",
            "    def build_a_document_block(self, title=\"Title\", link=\"\", content=\"content\"):",
            "        if link!=\"\":",
            "            return f'''",
            "<div style=\"width: 100%; border: 1px solid #ccc; border-radius: 5px; padding: 20px; font-family: Arial, sans-serif; margin-bottom: 20px; box-sizing: border-box;\">",
            "    <h3 style=\"margin-top: 0;\">",
            "        <a href=\"{link}\" target=\"_blank\" style=\"text-decoration: none; color: #333;\">{title}</a>",
            "    </h3>",
            "    <pre style=\"white-space: pre-wrap;color: #666;\">{content}</pre>",
            "</div>",
            "'''",
            "        else:",
            "            return f'''",
            "<div style=\"width: 100%; border: 1px solid #ccc; border-radius: 5px; padding: 20px; font-family: Arial, sans-serif; margin-bottom: 20px; box-sizing: border-box;\">",
            "    <h3 style=\"margin-top: 0;\">",
            "        <p style=\"text-decoration: none; color: #333;\">{title}</p>",
            "    </h3>",
            "    <pre style=\"white-space: pre-wrap;color: #666;\">{content}</pre>",
            "</div>",
            "'''",
            "",
            "    def build_a_folder_link(self, folder_path, link_text=\"Open Folder\"):",
            "        folder_path = str(folder_path).replace('\\\\','/')",
            "        return '''",
            "<a href=\"#\" onclick=\"path=\\''''+f'{folder_path}'+'''\\';",
            "fetch('/open_folder', {",
            "    method: 'POST',",
            "    headers: {",
            "        'Content-Type': 'application/json'",
            "    },",
            "    body: JSON.stringify({ path: path })",
            "    })",
            "    .then(response => response.json())",
            "    .then(data => {",
            "    if (data.status) {",
            "        console.log('Folder opened successfully');",
            "    } else {",
            "        console.error('Error opening folder:', data.error);",
            "    }",
            "    })",
            "    .catch(error => {",
            "    console.error('Error:', error);",
            "    });",
            "\">'''+f'''{link_text}</a>'''",
            "    def build_a_file_link(self, file_path, link_text=\"Open Folder\"):",
            "        file_path = str(file_path).replace('\\\\','/')",
            "        return '''",
            "<a href=\"#\" onclick=\"path=\\''''+f'{file_path}'+'''\\';",
            "fetch('/open_file', {",
            "    method: 'POST',",
            "    headers: {",
            "        'Content-Type': 'application/json'",
            "    },",
            "    body: JSON.stringify({ path: path })",
            "    })",
            "    .then(response => response.json())",
            "    .then(data => {",
            "    if (data.status) {",
            "        console.log('Folder opened successfully');",
            "    } else {",
            "        console.error('Error opening folder:', data.error);",
            "    }",
            "    })",
            "    .catch(error => {",
            "    console.error('Error:', error);",
            "    });",
            "\">'''+f'''{link_text}</a>'''",
            "# ===========================================================",
            "    def compress_js(self, code):",
            "        return compress_js(code)",
            "    def compress_python(self, code):",
            "        return compress_python(code)",
            "    def compress_html(self, code):",
            "        return compress_html(code)",
            "",
            "",
            "",
            "",
            "# ===========================================================",
            "    def select_model(self, binding_name, model_name):",
            "        self.personality.app.select_model(binding_name, model_name)",
            "    def verify_rag_entry(self, query, rag_entry):",
            "        return self.yes_no(\"Does the text entry contain the answer to the query?\", self.system_custom_header(\"Query\")+query+\"\\n\"+self.system_custom_header(\"text entry\")+\":\\n\"+rag_entry)",
            "    # Properties ===============================================",
            "    @property",
            "    def start_header_id_template(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return self.config.start_header_id_template",
            "",
            "    @property",
            "    def end_header_id_template(self) -> str:",
            "        \"\"\"Get the end_header_id_template.\"\"\"",
            "        return self.config.end_header_id_template",
            "    ",
            "    @property",
            "    def system_message_template(self) -> str:",
            "        \"\"\"Get the system_message_template.\"\"\"",
            "        return self.config.system_message_template",
            "",
            "",
            "    @property",
            "    def separator_template(self) -> str:",
            "        \"\"\"Get the separator template.\"\"\"",
            "        return self.config.separator_template",
            "",
            "",
            "    @property",
            "    def start_user_header_id_template(self) -> str:",
            "        \"\"\"Get the start_user_header_id_template.\"\"\"",
            "        return self.config.start_user_header_id_template",
            "    @property",
            "    def end_user_header_id_template(self) -> str:",
            "        \"\"\"Get the end_user_header_id_template.\"\"\"",
            "        return self.config.end_user_header_id_template",
            "    @property",
            "    def end_user_message_id_template(self) -> str:",
            "        \"\"\"Get the end_user_message_id_template.\"\"\"",
            "        return self.config.end_user_message_id_template",
            "",
            "",
            "",
            "",
            "    @property",
            "    def start_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the start_ai_header_id_template.\"\"\"",
            "        return self.config.start_ai_header_id_template",
            "    @property",
            "    def end_ai_header_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_header_id_template.\"\"\"",
            "        return self.config.end_ai_header_id_template",
            "    @property",
            "    def end_ai_message_id_template(self) -> str:",
            "        \"\"\"Get the end_ai_message_id_template.\"\"\"",
            "        return self.config.end_ai_message_id_template",
            "    @property",
            "    def system_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_header_id_template}{self.system_message_template}{self.end_header_id_template}\"",
            "    @property",
            "    def user_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.config.user_name}{self.end_user_header_id_template}\"",
            "    @property",
            "    def ai_full_header(self) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{self.personality.name}{self.end_user_header_id_template}\"",
            "",
            "    def system_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\"",
            "",
            "    def ai_custom_header(self, ai_name) -> str:",
            "        \"\"\"Get the start_header_id_template.\"\"\"",
            "        return f\"{self.start_user_header_id_template}{ai_name}{self.end_user_header_id_template}\"",
            "",
            "",
            "class AIPersonalityInstaller:",
            "    def __init__(self, personality:AIPersonality) -> None:",
            "        self.personality = personality",
            "",
            "",
            "class PersonalityBuilder:",
            "    def __init__(",
            "                    self,",
            "                    lollms_paths:LollmsPaths,",
            "                    config:LOLLMSConfig,",
            "                    model:LLMBinding,",
            "                    app=None,",
            "                    installation_option:InstallOption=InstallOption.INSTALL_IF_NECESSARY,",
            "                    callback=None",
            "                ):",
            "        self.config = config",
            "        self.lollms_paths = lollms_paths",
            "        self.model = model",
            "        self.app = app",
            "        self.installation_option = installation_option",
            "        self.callback = callback",
            "",
            "",
            "    def build_personality(self, id:int=None):",
            "        if id is None:",
            "            id = self.config[\"active_personality_id\"]",
            "            if self.config[\"active_personality_id\"]>=len(self.config[\"personalities\"]):",
            "                ASCIIColors.warning(\"Personality ID was out of range. Resetting to 0.\")",
            "                self.config[\"active_personality_id\"]=0",
            "                id = 0",
            "        else:",
            "            if id>len(self.config[\"personalities\"]):",
            "                id = len(self.config[\"personalities\"])-1",
            "",
            "        if \":\" in self.config[\"personalities\"][id]:",
            "            elements = self.config[\"personalities\"][id].split(\":\")",
            "            personality_folder = elements[0]",
            "            personality_language = elements[1]",
            "        else:",
            "            personality_folder = self.config[\"personalities\"][id]",
            "            personality_language = None",
            "",
            "        if len(self.config[\"personalities\"][id].split(\"/\"))==2:",
            "            self.personality = AIPersonality(",
            "                                            personality_folder,",
            "                                            self.lollms_paths,",
            "                                            self.config,",
            "                                            self.model,",
            "                                            app=self.app,",
            "                                            selected_language=personality_language,",
            "                                            installation_option=self.installation_option,",
            "                                            callback=self.callback",
            "                                        )",
            "        else:",
            "            self.personality = AIPersonality(",
            "                                            personality_folder,",
            "                                            self.lollms_paths,",
            "                                            self.config,",
            "                                            self.model,",
            "                                            app=self.app,",
            "                                            is_relative_path=False,",
            "                                            selected_language=personality_language,",
            "                                            installation_option=self.installation_option,",
            "                                            callback=self.callback",
            "                                        )",
            "        return self.personality",
            "",
            "    def get_personality(self):",
            "        return self.personality",
            "",
            "    def extract_function_call(self, query):",
            "        # Match the pattern @@function|param1|param2@@",
            "        lq = len(query)",
            "        parts = query.split(\"@@\")",
            "        if len(parts)>1:",
            "            query_ = parts[1].split(\"@@\")",
            "            query_=query_[0]",
            "            parts = query_.split(\"|\")",
            "            fn = parts[0]",
            "            if len(parts)>1:",
            "                params = parts[1:]",
            "            else:",
            "                params=[]",
            "            try:",
            "                end_pos = query.index(\"@@\")",
            "            except:",
            "                end_pos = lq",
            "            return fn, params, end_pos",
            "",
            "        else:",
            "            return None, None, 0"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "-1",
            "-1",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1084": [
                "AIPersonality",
                "add_file"
            ],
            "1085": [
                "AIPersonality",
                "add_file"
            ],
            "1086": [
                "AIPersonality",
                "add_file"
            ],
            "1087": [
                "AIPersonality",
                "add_file"
            ],
            "1088": [
                "AIPersonality",
                "add_file"
            ],
            "1089": [
                "AIPersonality",
                "add_file"
            ],
            "1090": [
                "AIPersonality",
                "add_file"
            ],
            "1091": [
                "AIPersonality",
                "add_file"
            ],
            "1092": [
                "AIPersonality",
                "add_file"
            ],
            "1093": [
                "AIPersonality",
                "add_file"
            ]
        },
        "addLocation": []
    }
}