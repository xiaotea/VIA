{
    "llama-index-core/llama_index/core/query_engine/cogniswitch_query_engine.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "         response = requests.post("
            },
            "1": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 42,
                "PatchRowcode": "             self.knowledge_request_endpoint,"
            },
            "2": {
                "beforePatchRowNumber": 43,
                "afterPatchRowNumber": 43,
                "PatchRowcode": "             headers=self.headers,"
            },
            "3": {
                "beforePatchRowNumber": 44,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            verify=False,"
            },
            "4": {
                "beforePatchRowNumber": 45,
                "afterPatchRowNumber": 44,
                "PatchRowcode": "             data=data,"
            },
            "5": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "         )"
            },
            "6": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "         if response.status_code == 200:"
            }
        },
        "frontPatchFile": [
            "from typing import Any, Dict",
            "",
            "import requests",
            "from llama_index.core.base.base_query_engine import BaseQueryEngine",
            "from llama_index.core.base.response.schema import Response",
            "from llama_index.core.schema import QueryBundle",
            "",
            "",
            "class CogniswitchQueryEngine(BaseQueryEngine):",
            "    def __init__(self, cs_token: str, OAI_token: str, apiKey: str) -> None:",
            "        \"\"\"The required fields.",
            "",
            "        Args:",
            "            cs_token (str): Cogniswitch token.",
            "            OAI_token (str): OpenAI token.",
            "            apiKey (str): Oauth token.",
            "        \"\"\"",
            "        self.cs_token = cs_token",
            "        self.OAI_token = OAI_token",
            "        self.apiKey = apiKey",
            "        self.knowledge_request_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeRequest\"",
            "        )",
            "        self.headers = {",
            "            \"apiKey\": self.apiKey,",
            "            \"platformToken\": self.cs_token,",
            "            \"openAIToken\": self.OAI_token,",
            "        }",
            "",
            "    def query_knowledge(self, query: str) -> Response:",
            "        \"\"\"",
            "        Send a query to the Cogniswitch service and retrieve the response.",
            "",
            "        Args:",
            "            query (str): Query to be answered.",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        data = {\"query\": query}",
            "        response = requests.post(",
            "            self.knowledge_request_endpoint,",
            "            headers=self.headers,",
            "            verify=False,",
            "            data=data,",
            "        )",
            "        if response.status_code == 200:",
            "            resp = response.json()",
            "            answer = resp[\"data\"][\"answer\"]",
            "",
            "            return Response(response=answer)",
            "        else:",
            "            error_message = response.json()[\"message\"]",
            "            return Response(response=error_message)",
            "",
            "    def _query(self, query_bundle: QueryBundle) -> Response:",
            "        return self.query_knowledge(query_bundle.query_str)",
            "",
            "    async def _aquery(self, query_bundle: QueryBundle) -> Response:",
            "        return self.query_knowledge(query_bundle.query_str)",
            "",
            "    def _get_prompt_modules(self) -> Dict[str, Any]:",
            "        \"\"\"Get prompts.\"\"\"",
            "        return {}"
        ],
        "afterPatchFile": [
            "from typing import Any, Dict",
            "",
            "import requests",
            "from llama_index.core.base.base_query_engine import BaseQueryEngine",
            "from llama_index.core.base.response.schema import Response",
            "from llama_index.core.schema import QueryBundle",
            "",
            "",
            "class CogniswitchQueryEngine(BaseQueryEngine):",
            "    def __init__(self, cs_token: str, OAI_token: str, apiKey: str) -> None:",
            "        \"\"\"The required fields.",
            "",
            "        Args:",
            "            cs_token (str): Cogniswitch token.",
            "            OAI_token (str): OpenAI token.",
            "            apiKey (str): Oauth token.",
            "        \"\"\"",
            "        self.cs_token = cs_token",
            "        self.OAI_token = OAI_token",
            "        self.apiKey = apiKey",
            "        self.knowledge_request_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeRequest\"",
            "        )",
            "        self.headers = {",
            "            \"apiKey\": self.apiKey,",
            "            \"platformToken\": self.cs_token,",
            "            \"openAIToken\": self.OAI_token,",
            "        }",
            "",
            "    def query_knowledge(self, query: str) -> Response:",
            "        \"\"\"",
            "        Send a query to the Cogniswitch service and retrieve the response.",
            "",
            "        Args:",
            "            query (str): Query to be answered.",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        data = {\"query\": query}",
            "        response = requests.post(",
            "            self.knowledge_request_endpoint,",
            "            headers=self.headers,",
            "            data=data,",
            "        )",
            "        if response.status_code == 200:",
            "            resp = response.json()",
            "            answer = resp[\"data\"][\"answer\"]",
            "",
            "            return Response(response=answer)",
            "        else:",
            "            error_message = response.json()[\"message\"]",
            "            return Response(response=error_message)",
            "",
            "    def _query(self, query_bundle: QueryBundle) -> Response:",
            "        return self.query_knowledge(query_bundle.query_str)",
            "",
            "    async def _aquery(self, query_bundle: QueryBundle) -> Response:",
            "        return self.query_knowledge(query_bundle.query_str)",
            "",
            "    def _get_prompt_modules(self) -> Dict[str, Any]:",
            "        \"\"\"Get prompts.\"\"\"",
            "        return {}"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "44": [
                "CogniswitchQueryEngine",
                "query_knowledge"
            ]
        },
        "addLocation": []
    },
    "llama-index-integrations/llms/llama-index-llms-rungpt/llama_index/llms/rungpt/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 1,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-import json"
            },
            "1": {
                "beforePatchRowNumber": 2,
                "afterPatchRowNumber": 1,
                "PatchRowcode": " from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union"
            },
            "2": {
                "beforePatchRowNumber": 3,
                "afterPatchRowNumber": 2,
                "PatchRowcode": " "
            },
            "3": {
                "beforePatchRowNumber": 4,
                "afterPatchRowNumber": 3,
                "PatchRowcode": " from llama_index.core.base.llms.types import ("
            },
            "4": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "         def gen() -> CompletionResponseGen:"
            },
            "5": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "             text = \"\""
            },
            "6": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "             for item in response_iter:"
            },
            "7": {
                "beforePatchRowNumber": 156,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                item_dict = json.loads(json.dumps(eval(item.data)))"
            },
            "8": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 155,
                "PatchRowcode": "+                item_dict = dict(item.data)"
            },
            "9": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "                 delta = item_dict[\"choices\"][0][\"text\"]"
            },
            "10": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 157,
                "PatchRowcode": "                 additional_kwargs = item_dict[\"usage\"]"
            },
            "11": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 158,
                "PatchRowcode": "                 text = text + self._space_handler(delta)"
            },
            "12": {
                "beforePatchRowNumber": 214,
                "afterPatchRowNumber": 213,
                "PatchRowcode": "         def gen() -> ChatResponseGen:"
            },
            "13": {
                "beforePatchRowNumber": 215,
                "afterPatchRowNumber": 214,
                "PatchRowcode": "             content = \"\""
            },
            "14": {
                "beforePatchRowNumber": 216,
                "afterPatchRowNumber": 215,
                "PatchRowcode": "             for item in chat_iter:"
            },
            "15": {
                "beforePatchRowNumber": 217,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                item_dict = json.loads(json.dumps(eval(item.data)))"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 216,
                "PatchRowcode": "+                item_dict = dict(item.data)"
            },
            "17": {
                "beforePatchRowNumber": 218,
                "afterPatchRowNumber": 217,
                "PatchRowcode": "                 chat_message, delta = self._message_unpacker(item_dict)"
            },
            "18": {
                "beforePatchRowNumber": 219,
                "afterPatchRowNumber": 218,
                "PatchRowcode": "                 content = content + self._space_handler(delta)"
            },
            "19": {
                "beforePatchRowNumber": 220,
                "afterPatchRowNumber": 219,
                "PatchRowcode": "                 chat_message.content = content"
            }
        },
        "frontPatchFile": [
            "import json",
            "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union",
            "",
            "from llama_index.core.base.llms.types import (",
            "    ChatMessage,",
            "    ChatResponse,",
            "    ChatResponseAsyncGen,",
            "    ChatResponseGen,",
            "    CompletionResponse,",
            "    CompletionResponseAsyncGen,",
            "    CompletionResponseGen,",
            "    LLMMetadata,",
            "    MessageRole,",
            ")",
            "from llama_index.core.bridge.pydantic import Field",
            "from llama_index.core.callbacks import CallbackManager",
            "from llama_index.core.constants import DEFAULT_CONTEXT_WINDOW, DEFAULT_NUM_OUTPUTS",
            "from llama_index.core.llms.callbacks import llm_chat_callback, llm_completion_callback",
            "from llama_index.core.llms.llm import LLM",
            "from llama_index.core.types import BaseOutputParser, PydanticProgramMode",
            "",
            "DEFAULT_RUNGPT_MODEL = \"rungpt\"",
            "DEFAULT_RUNGPT_TEMP = 0.75",
            "",
            "",
            "class RunGptLLM(LLM):",
            "    \"\"\"The opengpt of Jina AI models.\"\"\"",
            "",
            "    model: Optional[str] = Field(",
            "        default=DEFAULT_RUNGPT_MODEL, description=\"The rungpt model to use.\"",
            "    )",
            "    endpoint: str = Field(description=\"The endpoint of serving address.\")",
            "    temperature: float = Field(",
            "        default=DEFAULT_RUNGPT_TEMP,",
            "        description=\"The temperature to use for sampling.\",",
            "        gte=0.0,",
            "        lte=1.0,",
            "    )",
            "    max_tokens: int = Field(",
            "        default=DEFAULT_NUM_OUTPUTS,",
            "        description=\"Max tokens model generates.\",",
            "        gt=0,",
            "    )",
            "    context_window: int = Field(",
            "        default=DEFAULT_CONTEXT_WINDOW,",
            "        description=\"The maximum number of context tokens for the model.\",",
            "        gt=0,",
            "    )",
            "    additional_kwargs: Dict[str, Any] = Field(",
            "        default_factory=dict, description=\"Additional kwargs for the Replicate API.\"",
            "    )",
            "    base_url: str = Field(",
            "        description=\"The address of your target model served by rungpt.\"",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        model: Optional[str] = DEFAULT_RUNGPT_MODEL,",
            "        endpoint: str = \"0.0.0.0:51002\",",
            "        temperature: float = DEFAULT_RUNGPT_TEMP,",
            "        max_tokens: Optional[int] = DEFAULT_NUM_OUTPUTS,",
            "        context_window: int = DEFAULT_CONTEXT_WINDOW,",
            "        additional_kwargs: Optional[Dict[str, Any]] = None,",
            "        callback_manager: Optional[CallbackManager] = None,",
            "        system_prompt: Optional[str] = None,",
            "        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,",
            "        completion_to_prompt: Optional[Callable[[str], str]] = None,",
            "        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,",
            "        output_parser: Optional[BaseOutputParser] = None,",
            "    ):",
            "        if endpoint.startswith(\"http://\"):",
            "            base_url = endpoint",
            "        else:",
            "            base_url = \"http://\" + endpoint",
            "        super().__init__(",
            "            model=model,",
            "            endpoint=endpoint,",
            "            temperature=temperature,",
            "            max_tokens=max_tokens,",
            "            context_window=context_window,",
            "            additional_kwargs=additional_kwargs or {},",
            "            callback_manager=callback_manager or CallbackManager([]),",
            "            base_url=base_url,",
            "            system_prompt=system_prompt,",
            "            messages_to_prompt=messages_to_prompt,",
            "            completion_to_prompt=completion_to_prompt,",
            "            pydantic_program_mode=pydantic_program_mode,",
            "            output_parser=output_parser,",
            "        )",
            "",
            "    @classmethod",
            "    def class_name(cls) -> str:",
            "        return \"RunGptLLM\"",
            "",
            "    @property",
            "    def metadata(self) -> LLMMetadata:",
            "        \"\"\"LLM metadata.\"\"\"",
            "        return LLMMetadata(",
            "            context_window=self.context_window,",
            "            num_output=self.max_tokens,",
            "            model_name=self._model,",
            "        )",
            "",
            "    @llm_completion_callback()",
            "    def complete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponse:",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/generate\",",
            "            json=self._request_pack(\"complete\", prompt, **kwargs),",
            "            stream=False,",
            "        ).json()",
            "",
            "        return CompletionResponse(",
            "            text=response_gpt[\"choices\"][0][\"text\"],",
            "            additional_kwargs=response_gpt[\"usage\"],",
            "            raw=response_gpt,",
            "        )",
            "",
            "    @llm_completion_callback()",
            "    def stream_complete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponseGen:",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/generate_stream\",",
            "            json=self._request_pack(\"complete\", prompt, **kwargs),",
            "            stream=True,",
            "        )",
            "        try:",
            "            import sseclient",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import sseclient-py library.\"",
            "                \"Please install requests with `pip install sseclient-py`\"",
            "            )",
            "        client = sseclient.SSEClient(response_gpt)",
            "        response_iter = client.events()",
            "",
            "        def gen() -> CompletionResponseGen:",
            "            text = \"\"",
            "            for item in response_iter:",
            "                item_dict = json.loads(json.dumps(eval(item.data)))",
            "                delta = item_dict[\"choices\"][0][\"text\"]",
            "                additional_kwargs = item_dict[\"usage\"]",
            "                text = text + self._space_handler(delta)",
            "                yield CompletionResponse(",
            "                    text=text,",
            "                    delta=delta,",
            "                    raw=item_dict,",
            "                    additional_kwargs=additional_kwargs,",
            "                )",
            "",
            "        return gen()",
            "",
            "    @llm_chat_callback()",
            "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:",
            "        message_list = self._message_wrapper(messages)",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/chat\",",
            "            json=self._request_pack(\"chat\", message_list, **kwargs),",
            "            stream=False,",
            "        ).json()",
            "        chat_message, _ = self._message_unpacker(response_gpt)",
            "        return ChatResponse(message=chat_message, raw=response_gpt)",
            "",
            "    @llm_chat_callback()",
            "    def stream_chat(",
            "        self, messages: Sequence[ChatMessage], **kwargs: Any",
            "    ) -> ChatResponseGen:",
            "        message_list = self._message_wrapper(messages)",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/chat_stream\",",
            "            json=self._request_pack(\"chat\", message_list, **kwargs),",
            "            stream=True,",
            "        )",
            "        try:",
            "            import sseclient",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import sseclient-py library.\"",
            "                \"Please install requests with `pip install sseclient-py`\"",
            "            )",
            "        client = sseclient.SSEClient(response_gpt)",
            "        chat_iter = client.events()",
            "",
            "        def gen() -> ChatResponseGen:",
            "            content = \"\"",
            "            for item in chat_iter:",
            "                item_dict = json.loads(json.dumps(eval(item.data)))",
            "                chat_message, delta = self._message_unpacker(item_dict)",
            "                content = content + self._space_handler(delta)",
            "                chat_message.content = content",
            "                yield ChatResponse(message=chat_message, raw=item_dict, delta=delta)",
            "",
            "        return gen()",
            "",
            "    @llm_chat_callback()",
            "    async def achat(",
            "        self,",
            "        messages: Sequence[ChatMessage],",
            "        **kwargs: Any,",
            "    ) -> ChatResponse:",
            "        return self.chat(messages, **kwargs)",
            "",
            "    @llm_chat_callback()",
            "    async def astream_chat(",
            "        self,",
            "        messages: Sequence[ChatMessage],",
            "        **kwargs: Any,",
            "    ) -> ChatResponseAsyncGen:",
            "        async def gen() -> ChatResponseAsyncGen:",
            "            for message in self.stream_chat(messages, **kwargs):",
            "                yield message",
            "",
            "        # NOTE: convert generator to async generator",
            "        return gen()",
            "",
            "    @llm_completion_callback()",
            "    async def acomplete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponse:",
            "        return self.complete(prompt, **kwargs)",
            "",
            "    @llm_completion_callback()",
            "    async def astream_complete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponseAsyncGen:",
            "        async def gen() -> CompletionResponseAsyncGen:",
            "            for message in self.stream_complete(prompt, **kwargs):",
            "                yield message",
            "",
            "        return gen()",
            "",
            "    def _message_wrapper(self, messages: Sequence[ChatMessage]) -> List[Dict[str, Any]]:",
            "        message_list = []",
            "        for message in messages:",
            "            role = message.role.value",
            "            content = message.content",
            "            message_list.append({\"role\": role, \"content\": content})",
            "        return message_list",
            "",
            "    def _message_unpacker(",
            "        self, response_gpt: Dict[str, Any]",
            "    ) -> Tuple[ChatMessage, str]:",
            "        message = response_gpt[\"choices\"][0][\"message\"]",
            "        additional_kwargs = response_gpt[\"usage\"]",
            "        role = message[\"role\"]",
            "        content = message[\"content\"]",
            "        key = MessageRole.SYSTEM",
            "        for r in MessageRole:",
            "            if r.value == role:",
            "                key = r",
            "        chat_message = ChatMessage(",
            "            role=key, content=content, additional_kwargs=additional_kwargs",
            "        )",
            "        return chat_message, content",
            "",
            "    def _request_pack(",
            "        self, mode: str, prompt: Union[str, List[Dict[str, Any]]], **kwargs: Any",
            "    ) -> Optional[Dict[str, Any]]:",
            "        if mode == \"complete\":",
            "            return {",
            "                \"prompt\": prompt,",
            "                \"max_tokens\": kwargs.pop(\"max_tokens\", self.max_tokens),",
            "                \"temperature\": kwargs.pop(\"temperature\", self.temperature),",
            "                \"top_k\": kwargs.pop(\"top_k\", 50),",
            "                \"top_p\": kwargs.pop(\"top_p\", 0.95),",
            "                \"repetition_penalty\": kwargs.pop(\"repetition_penalty\", 1.2),",
            "                \"do_sample\": kwargs.pop(\"do_sample\", False),",
            "                \"echo\": kwargs.pop(\"echo\", True),",
            "                \"n\": kwargs.pop(\"n\", 1),",
            "                \"stop\": kwargs.pop(\"stop\", \".\"),",
            "            }",
            "        elif mode == \"chat\":",
            "            return {",
            "                \"messages\": prompt,",
            "                \"max_tokens\": kwargs.pop(\"max_tokens\", self.max_tokens),",
            "                \"temperature\": kwargs.pop(\"temperature\", self.temperature),",
            "                \"top_k\": kwargs.pop(\"top_k\", 50),",
            "                \"top_p\": kwargs.pop(\"top_p\", 0.95),",
            "                \"repetition_penalty\": kwargs.pop(\"repetition_penalty\", 1.2),",
            "                \"do_sample\": kwargs.pop(\"do_sample\", False),",
            "                \"echo\": kwargs.pop(\"echo\", True),",
            "                \"n\": kwargs.pop(\"n\", 1),",
            "                \"stop\": kwargs.pop(\"stop\", \".\"),",
            "            }",
            "        return None",
            "",
            "    def _space_handler(self, word: str) -> str:",
            "        if word.isalnum():",
            "            return \" \" + word",
            "        return word"
        ],
        "afterPatchFile": [
            "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union",
            "",
            "from llama_index.core.base.llms.types import (",
            "    ChatMessage,",
            "    ChatResponse,",
            "    ChatResponseAsyncGen,",
            "    ChatResponseGen,",
            "    CompletionResponse,",
            "    CompletionResponseAsyncGen,",
            "    CompletionResponseGen,",
            "    LLMMetadata,",
            "    MessageRole,",
            ")",
            "from llama_index.core.bridge.pydantic import Field",
            "from llama_index.core.callbacks import CallbackManager",
            "from llama_index.core.constants import DEFAULT_CONTEXT_WINDOW, DEFAULT_NUM_OUTPUTS",
            "from llama_index.core.llms.callbacks import llm_chat_callback, llm_completion_callback",
            "from llama_index.core.llms.llm import LLM",
            "from llama_index.core.types import BaseOutputParser, PydanticProgramMode",
            "",
            "DEFAULT_RUNGPT_MODEL = \"rungpt\"",
            "DEFAULT_RUNGPT_TEMP = 0.75",
            "",
            "",
            "class RunGptLLM(LLM):",
            "    \"\"\"The opengpt of Jina AI models.\"\"\"",
            "",
            "    model: Optional[str] = Field(",
            "        default=DEFAULT_RUNGPT_MODEL, description=\"The rungpt model to use.\"",
            "    )",
            "    endpoint: str = Field(description=\"The endpoint of serving address.\")",
            "    temperature: float = Field(",
            "        default=DEFAULT_RUNGPT_TEMP,",
            "        description=\"The temperature to use for sampling.\",",
            "        gte=0.0,",
            "        lte=1.0,",
            "    )",
            "    max_tokens: int = Field(",
            "        default=DEFAULT_NUM_OUTPUTS,",
            "        description=\"Max tokens model generates.\",",
            "        gt=0,",
            "    )",
            "    context_window: int = Field(",
            "        default=DEFAULT_CONTEXT_WINDOW,",
            "        description=\"The maximum number of context tokens for the model.\",",
            "        gt=0,",
            "    )",
            "    additional_kwargs: Dict[str, Any] = Field(",
            "        default_factory=dict, description=\"Additional kwargs for the Replicate API.\"",
            "    )",
            "    base_url: str = Field(",
            "        description=\"The address of your target model served by rungpt.\"",
            "    )",
            "",
            "    def __init__(",
            "        self,",
            "        model: Optional[str] = DEFAULT_RUNGPT_MODEL,",
            "        endpoint: str = \"0.0.0.0:51002\",",
            "        temperature: float = DEFAULT_RUNGPT_TEMP,",
            "        max_tokens: Optional[int] = DEFAULT_NUM_OUTPUTS,",
            "        context_window: int = DEFAULT_CONTEXT_WINDOW,",
            "        additional_kwargs: Optional[Dict[str, Any]] = None,",
            "        callback_manager: Optional[CallbackManager] = None,",
            "        system_prompt: Optional[str] = None,",
            "        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,",
            "        completion_to_prompt: Optional[Callable[[str], str]] = None,",
            "        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,",
            "        output_parser: Optional[BaseOutputParser] = None,",
            "    ):",
            "        if endpoint.startswith(\"http://\"):",
            "            base_url = endpoint",
            "        else:",
            "            base_url = \"http://\" + endpoint",
            "        super().__init__(",
            "            model=model,",
            "            endpoint=endpoint,",
            "            temperature=temperature,",
            "            max_tokens=max_tokens,",
            "            context_window=context_window,",
            "            additional_kwargs=additional_kwargs or {},",
            "            callback_manager=callback_manager or CallbackManager([]),",
            "            base_url=base_url,",
            "            system_prompt=system_prompt,",
            "            messages_to_prompt=messages_to_prompt,",
            "            completion_to_prompt=completion_to_prompt,",
            "            pydantic_program_mode=pydantic_program_mode,",
            "            output_parser=output_parser,",
            "        )",
            "",
            "    @classmethod",
            "    def class_name(cls) -> str:",
            "        return \"RunGptLLM\"",
            "",
            "    @property",
            "    def metadata(self) -> LLMMetadata:",
            "        \"\"\"LLM metadata.\"\"\"",
            "        return LLMMetadata(",
            "            context_window=self.context_window,",
            "            num_output=self.max_tokens,",
            "            model_name=self._model,",
            "        )",
            "",
            "    @llm_completion_callback()",
            "    def complete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponse:",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/generate\",",
            "            json=self._request_pack(\"complete\", prompt, **kwargs),",
            "            stream=False,",
            "        ).json()",
            "",
            "        return CompletionResponse(",
            "            text=response_gpt[\"choices\"][0][\"text\"],",
            "            additional_kwargs=response_gpt[\"usage\"],",
            "            raw=response_gpt,",
            "        )",
            "",
            "    @llm_completion_callback()",
            "    def stream_complete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponseGen:",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/generate_stream\",",
            "            json=self._request_pack(\"complete\", prompt, **kwargs),",
            "            stream=True,",
            "        )",
            "        try:",
            "            import sseclient",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import sseclient-py library.\"",
            "                \"Please install requests with `pip install sseclient-py`\"",
            "            )",
            "        client = sseclient.SSEClient(response_gpt)",
            "        response_iter = client.events()",
            "",
            "        def gen() -> CompletionResponseGen:",
            "            text = \"\"",
            "            for item in response_iter:",
            "                item_dict = dict(item.data)",
            "                delta = item_dict[\"choices\"][0][\"text\"]",
            "                additional_kwargs = item_dict[\"usage\"]",
            "                text = text + self._space_handler(delta)",
            "                yield CompletionResponse(",
            "                    text=text,",
            "                    delta=delta,",
            "                    raw=item_dict,",
            "                    additional_kwargs=additional_kwargs,",
            "                )",
            "",
            "        return gen()",
            "",
            "    @llm_chat_callback()",
            "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:",
            "        message_list = self._message_wrapper(messages)",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/chat\",",
            "            json=self._request_pack(\"chat\", message_list, **kwargs),",
            "            stream=False,",
            "        ).json()",
            "        chat_message, _ = self._message_unpacker(response_gpt)",
            "        return ChatResponse(message=chat_message, raw=response_gpt)",
            "",
            "    @llm_chat_callback()",
            "    def stream_chat(",
            "        self, messages: Sequence[ChatMessage], **kwargs: Any",
            "    ) -> ChatResponseGen:",
            "        message_list = self._message_wrapper(messages)",
            "        try:",
            "            import requests",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import requests library.\"",
            "                \"Please install requests with `pip install requests`\"",
            "            )",
            "        response_gpt = requests.post(",
            "            self.base_url + \"/chat_stream\",",
            "            json=self._request_pack(\"chat\", message_list, **kwargs),",
            "            stream=True,",
            "        )",
            "        try:",
            "            import sseclient",
            "        except ImportError:",
            "            raise ImportError(",
            "                \"Could not import sseclient-py library.\"",
            "                \"Please install requests with `pip install sseclient-py`\"",
            "            )",
            "        client = sseclient.SSEClient(response_gpt)",
            "        chat_iter = client.events()",
            "",
            "        def gen() -> ChatResponseGen:",
            "            content = \"\"",
            "            for item in chat_iter:",
            "                item_dict = dict(item.data)",
            "                chat_message, delta = self._message_unpacker(item_dict)",
            "                content = content + self._space_handler(delta)",
            "                chat_message.content = content",
            "                yield ChatResponse(message=chat_message, raw=item_dict, delta=delta)",
            "",
            "        return gen()",
            "",
            "    @llm_chat_callback()",
            "    async def achat(",
            "        self,",
            "        messages: Sequence[ChatMessage],",
            "        **kwargs: Any,",
            "    ) -> ChatResponse:",
            "        return self.chat(messages, **kwargs)",
            "",
            "    @llm_chat_callback()",
            "    async def astream_chat(",
            "        self,",
            "        messages: Sequence[ChatMessage],",
            "        **kwargs: Any,",
            "    ) -> ChatResponseAsyncGen:",
            "        async def gen() -> ChatResponseAsyncGen:",
            "            for message in self.stream_chat(messages, **kwargs):",
            "                yield message",
            "",
            "        # NOTE: convert generator to async generator",
            "        return gen()",
            "",
            "    @llm_completion_callback()",
            "    async def acomplete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponse:",
            "        return self.complete(prompt, **kwargs)",
            "",
            "    @llm_completion_callback()",
            "    async def astream_complete(",
            "        self, prompt: str, formatted: bool = False, **kwargs: Any",
            "    ) -> CompletionResponseAsyncGen:",
            "        async def gen() -> CompletionResponseAsyncGen:",
            "            for message in self.stream_complete(prompt, **kwargs):",
            "                yield message",
            "",
            "        return gen()",
            "",
            "    def _message_wrapper(self, messages: Sequence[ChatMessage]) -> List[Dict[str, Any]]:",
            "        message_list = []",
            "        for message in messages:",
            "            role = message.role.value",
            "            content = message.content",
            "            message_list.append({\"role\": role, \"content\": content})",
            "        return message_list",
            "",
            "    def _message_unpacker(",
            "        self, response_gpt: Dict[str, Any]",
            "    ) -> Tuple[ChatMessage, str]:",
            "        message = response_gpt[\"choices\"][0][\"message\"]",
            "        additional_kwargs = response_gpt[\"usage\"]",
            "        role = message[\"role\"]",
            "        content = message[\"content\"]",
            "        key = MessageRole.SYSTEM",
            "        for r in MessageRole:",
            "            if r.value == role:",
            "                key = r",
            "        chat_message = ChatMessage(",
            "            role=key, content=content, additional_kwargs=additional_kwargs",
            "        )",
            "        return chat_message, content",
            "",
            "    def _request_pack(",
            "        self, mode: str, prompt: Union[str, List[Dict[str, Any]]], **kwargs: Any",
            "    ) -> Optional[Dict[str, Any]]:",
            "        if mode == \"complete\":",
            "            return {",
            "                \"prompt\": prompt,",
            "                \"max_tokens\": kwargs.pop(\"max_tokens\", self.max_tokens),",
            "                \"temperature\": kwargs.pop(\"temperature\", self.temperature),",
            "                \"top_k\": kwargs.pop(\"top_k\", 50),",
            "                \"top_p\": kwargs.pop(\"top_p\", 0.95),",
            "                \"repetition_penalty\": kwargs.pop(\"repetition_penalty\", 1.2),",
            "                \"do_sample\": kwargs.pop(\"do_sample\", False),",
            "                \"echo\": kwargs.pop(\"echo\", True),",
            "                \"n\": kwargs.pop(\"n\", 1),",
            "                \"stop\": kwargs.pop(\"stop\", \".\"),",
            "            }",
            "        elif mode == \"chat\":",
            "            return {",
            "                \"messages\": prompt,",
            "                \"max_tokens\": kwargs.pop(\"max_tokens\", self.max_tokens),",
            "                \"temperature\": kwargs.pop(\"temperature\", self.temperature),",
            "                \"top_k\": kwargs.pop(\"top_k\", 50),",
            "                \"top_p\": kwargs.pop(\"top_p\", 0.95),",
            "                \"repetition_penalty\": kwargs.pop(\"repetition_penalty\", 1.2),",
            "                \"do_sample\": kwargs.pop(\"do_sample\", False),",
            "                \"echo\": kwargs.pop(\"echo\", True),",
            "                \"n\": kwargs.pop(\"n\", 1),",
            "                \"stop\": kwargs.pop(\"stop\", \".\"),",
            "            }",
            "        return None",
            "",
            "    def _space_handler(self, word: str) -> str:",
            "        if word.isalnum():",
            "            return \" \" + word",
            "        return word"
        ],
        "action": [
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "1": [],
            "156": [
                "RunGptLLM",
                "stream_complete",
                "gen"
            ],
            "217": [
                "RunGptLLM",
                "stream_chat",
                "gen"
            ]
        },
        "addLocation": []
    },
    "llama-index-integrations/tools/llama-index-tools-cogniswitch/llama_index/tools/cogniswitch/base.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 91,
                "afterPatchRowNumber": 91,
                "PatchRowcode": "                 \"documentName\": document_name,"
            },
            "1": {
                "beforePatchRowNumber": 92,
                "afterPatchRowNumber": 92,
                "PatchRowcode": "                 \"documentDescription\": document_description,"
            },
            "2": {
                "beforePatchRowNumber": 93,
                "afterPatchRowNumber": 93,
                "PatchRowcode": "             }"
            },
            "3": {
                "beforePatchRowNumber": 94,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            response = requests.post("
            },
            "4": {
                "beforePatchRowNumber": 95,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                api_url, headers=headers, verify=False, data=data, files=files"
            },
            "5": {
                "beforePatchRowNumber": 96,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "6": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 94,
                "PatchRowcode": "+            response = requests.post(api_url, headers=headers, data=data, files=files)"
            },
            "7": {
                "beforePatchRowNumber": 97,
                "afterPatchRowNumber": 95,
                "PatchRowcode": " "
            },
            "8": {
                "beforePatchRowNumber": 98,
                "afterPatchRowNumber": 96,
                "PatchRowcode": "         elif file:"
            },
            "9": {
                "beforePatchRowNumber": 99,
                "afterPatchRowNumber": 97,
                "PatchRowcode": "             api_url = self.source_file_endpoint"
            },
            "10": {
                "beforePatchRowNumber": 108,
                "afterPatchRowNumber": 106,
                "PatchRowcode": "                 \"documentName\": document_name,"
            },
            "11": {
                "beforePatchRowNumber": 109,
                "afterPatchRowNumber": 107,
                "PatchRowcode": "                 \"documentDescription\": document_description,"
            },
            "12": {
                "beforePatchRowNumber": 110,
                "afterPatchRowNumber": 108,
                "PatchRowcode": "             }"
            },
            "13": {
                "beforePatchRowNumber": 111,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            response = requests.post("
            },
            "14": {
                "beforePatchRowNumber": 112,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                api_url, headers=headers, verify=False, data=data, files=files"
            },
            "15": {
                "beforePatchRowNumber": 113,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            )"
            },
            "16": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 109,
                "PatchRowcode": "+            response = requests.post(api_url, headers=headers, data=data, files=files)"
            },
            "17": {
                "beforePatchRowNumber": 114,
                "afterPatchRowNumber": 110,
                "PatchRowcode": "         if response.status_code == 200:"
            },
            "18": {
                "beforePatchRowNumber": 115,
                "afterPatchRowNumber": 111,
                "PatchRowcode": "             return response.json()"
            },
            "19": {
                "beforePatchRowNumber": 116,
                "afterPatchRowNumber": 112,
                "PatchRowcode": "         else:"
            },
            "20": {
                "beforePatchRowNumber": 134,
                "afterPatchRowNumber": 130,
                "PatchRowcode": "         headers = self.headers"
            },
            "21": {
                "beforePatchRowNumber": 135,
                "afterPatchRowNumber": 131,
                "PatchRowcode": " "
            },
            "22": {
                "beforePatchRowNumber": 136,
                "afterPatchRowNumber": 132,
                "PatchRowcode": "         data = {\"query\": query}"
            },
            "23": {
                "beforePatchRowNumber": 137,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        response = requests.post(api_url, headers=headers, verify=False, data=data)"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 133,
                "PatchRowcode": "+        response = requests.post(api_url, headers=headers, data=data)"
            },
            "25": {
                "beforePatchRowNumber": 138,
                "afterPatchRowNumber": 134,
                "PatchRowcode": "         if response.status_code == 200:"
            },
            "26": {
                "beforePatchRowNumber": 139,
                "afterPatchRowNumber": 135,
                "PatchRowcode": "             return response.json()"
            },
            "27": {
                "beforePatchRowNumber": 140,
                "afterPatchRowNumber": 136,
                "PatchRowcode": "         else:"
            },
            "28": {
                "beforePatchRowNumber": 157,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "             self.knowledge_status_endpoint,"
            },
            "29": {
                "beforePatchRowNumber": 158,
                "afterPatchRowNumber": 154,
                "PatchRowcode": "             headers=self.headers,"
            },
            "30": {
                "beforePatchRowNumber": 159,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "             params=params,"
            },
            "31": {
                "beforePatchRowNumber": 160,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            verify=False,"
            },
            "32": {
                "beforePatchRowNumber": 161,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "         )"
            },
            "33": {
                "beforePatchRowNumber": 162,
                "afterPatchRowNumber": 157,
                "PatchRowcode": "         if response.status_code == 200:"
            },
            "34": {
                "beforePatchRowNumber": 163,
                "afterPatchRowNumber": 158,
                "PatchRowcode": "             source_info = response.json()"
            }
        },
        "frontPatchFile": [
            "import os",
            "from typing import Optional",
            "",
            "import requests",
            "from llama_index.core.tools.tool_spec.base import BaseToolSpec",
            "",
            "",
            "class CogniswitchToolSpec(BaseToolSpec):",
            "    \"\"\"Cogniswitch Tool Spec.",
            "    A toolspec to have store_data and query_knowledge as tools to store the data from a file or a url",
            "    and answer questions from the knowledge stored respectively.",
            "    \"\"\"",
            "",
            "    spec_functions = [\"store_data\", \"query_knowledge\", \"knowledge_status\"]",
            "",
            "    def __init__(",
            "        self,",
            "        cs_token: str,",
            "        apiKey: str,",
            "        OAI_token: Optional[str] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            cs_token (str): Cogniswitch token.",
            "            OAI_token (str): OpenAI token.",
            "            apiKey (str): Oauth token.",
            "        \"\"\"",
            "        self.cs_token = cs_token",
            "        if OAI_token:",
            "            self.OAI_token = OAI_token",
            "        elif os.environ[\"OPENAI_API_KEY\"]:",
            "            self.OAI_token = os.environ[\"OPENAI_API_KEY\"]",
            "        else:",
            "            raise ValueError(\"Please provide the OpenAI token\")",
            "        self.apiKey = apiKey",
            "        self.source_URL_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeSource/url\"",
            "        )",
            "        self.source_file_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeSource/file\"",
            "        )",
            "        self.knowledge_request_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeRequest\"",
            "        )",
            "        self.knowledge_status_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeSource/status\"",
            "        )",
            "        self.headers = {",
            "            \"apiKey\": self.apiKey,",
            "            \"platformToken\": self.cs_token,",
            "            \"openAIToken\": self.OAI_token,",
            "        }",
            "",
            "    def store_data(",
            "        self,",
            "        url: Optional[str] = None,",
            "        file: Optional[str] = None,",
            "        document_name: Optional[str] = None,",
            "        document_description: Optional[str] = None,",
            "    ) -> dict:",
            "        \"\"\"",
            "        Store data using the Cogniswitch service.",
            "",
            "        Args:",
            "            url (Optional[str]): URL link.",
            "            file (Optional[str]): file path of your file.",
            "            the current files supported by the files are",
            "            .txt, .pdf, .docx, .doc, .html",
            "            document_name (Optional[str]): Name of the document you are uploading.",
            "            document_description (Optional[str]): Description of the document.",
            "",
            "",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        if not file and not url:",
            "            return {",
            "                \"message\": \"No input provided\",",
            "            }",
            "        elif file and url:",
            "            return {",
            "                \"message\": \"Too many inputs, please provide either file or url\",",
            "            }",
            "        elif url:",
            "            api_url = self.source_URL_endpoint",
            "            headers = self.headers",
            "            files = None",
            "            data = {",
            "                \"url\": url,",
            "                \"documentName\": document_name,",
            "                \"documentDescription\": document_description,",
            "            }",
            "            response = requests.post(",
            "                api_url, headers=headers, verify=False, data=data, files=files",
            "            )",
            "",
            "        elif file:",
            "            api_url = self.source_file_endpoint",
            "",
            "            headers = self.headers",
            "            if file is not None:",
            "                files = {\"file\": open(file, \"rb\")}",
            "            else:",
            "                files = None",
            "            data = {",
            "                \"url\": url,",
            "                \"documentName\": document_name,",
            "                \"documentDescription\": document_description,",
            "            }",
            "            response = requests.post(",
            "                api_url, headers=headers, verify=False, data=data, files=files",
            "            )",
            "        if response.status_code == 200:",
            "            return response.json()",
            "        else:",
            "            # error_message = response.json()[\"message\"]",
            "            return {",
            "                \"message\": \"Bad Request\",",
            "            }",
            "",
            "    def query_knowledge(self, query: str) -> dict:",
            "        \"\"\"",
            "        Send a query to the Cogniswitch service and retrieve the response.",
            "",
            "        Args:",
            "            query (str): Query to be answered.",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        api_url = self.knowledge_request_endpoint",
            "",
            "        headers = self.headers",
            "",
            "        data = {\"query\": query}",
            "        response = requests.post(api_url, headers=headers, verify=False, data=data)",
            "        if response.status_code == 200:",
            "            return response.json()",
            "        else:",
            "            # error_message = response.json()[\"message\"]",
            "            return {",
            "                \"message\": \"Bad Request\",",
            "            }",
            "",
            "    def knowledge_status(self, document_name: str) -> dict:",
            "        \"\"\"",
            "        Use this function to know the status of the document or the URL uploaded",
            "        Args:",
            "            document_name (str): The document name or the url that is uploaded.",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        params = {\"docName\": document_name, \"platformToken\": self.cs_token}",
            "        response = requests.get(",
            "            self.knowledge_status_endpoint,",
            "            headers=self.headers,",
            "            params=params,",
            "            verify=False,",
            "        )",
            "        if response.status_code == 200:",
            "            source_info = response.json()",
            "            return source_info[-1]",
            "        else:",
            "            # error_message = response.json()[\"message\"]",
            "            return {",
            "                \"message\": \"Bad Request\",",
            "            }"
        ],
        "afterPatchFile": [
            "import os",
            "from typing import Optional",
            "",
            "import requests",
            "from llama_index.core.tools.tool_spec.base import BaseToolSpec",
            "",
            "",
            "class CogniswitchToolSpec(BaseToolSpec):",
            "    \"\"\"Cogniswitch Tool Spec.",
            "    A toolspec to have store_data and query_knowledge as tools to store the data from a file or a url",
            "    and answer questions from the knowledge stored respectively.",
            "    \"\"\"",
            "",
            "    spec_functions = [\"store_data\", \"query_knowledge\", \"knowledge_status\"]",
            "",
            "    def __init__(",
            "        self,",
            "        cs_token: str,",
            "        apiKey: str,",
            "        OAI_token: Optional[str] = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Args:",
            "            cs_token (str): Cogniswitch token.",
            "            OAI_token (str): OpenAI token.",
            "            apiKey (str): Oauth token.",
            "        \"\"\"",
            "        self.cs_token = cs_token",
            "        if OAI_token:",
            "            self.OAI_token = OAI_token",
            "        elif os.environ[\"OPENAI_API_KEY\"]:",
            "            self.OAI_token = os.environ[\"OPENAI_API_KEY\"]",
            "        else:",
            "            raise ValueError(\"Please provide the OpenAI token\")",
            "        self.apiKey = apiKey",
            "        self.source_URL_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeSource/url\"",
            "        )",
            "        self.source_file_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeSource/file\"",
            "        )",
            "        self.knowledge_request_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeRequest\"",
            "        )",
            "        self.knowledge_status_endpoint = (",
            "            \"https://api.cogniswitch.ai:8243/cs-api/0.0.1/cs/knowledgeSource/status\"",
            "        )",
            "        self.headers = {",
            "            \"apiKey\": self.apiKey,",
            "            \"platformToken\": self.cs_token,",
            "            \"openAIToken\": self.OAI_token,",
            "        }",
            "",
            "    def store_data(",
            "        self,",
            "        url: Optional[str] = None,",
            "        file: Optional[str] = None,",
            "        document_name: Optional[str] = None,",
            "        document_description: Optional[str] = None,",
            "    ) -> dict:",
            "        \"\"\"",
            "        Store data using the Cogniswitch service.",
            "",
            "        Args:",
            "            url (Optional[str]): URL link.",
            "            file (Optional[str]): file path of your file.",
            "            the current files supported by the files are",
            "            .txt, .pdf, .docx, .doc, .html",
            "            document_name (Optional[str]): Name of the document you are uploading.",
            "            document_description (Optional[str]): Description of the document.",
            "",
            "",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        if not file and not url:",
            "            return {",
            "                \"message\": \"No input provided\",",
            "            }",
            "        elif file and url:",
            "            return {",
            "                \"message\": \"Too many inputs, please provide either file or url\",",
            "            }",
            "        elif url:",
            "            api_url = self.source_URL_endpoint",
            "            headers = self.headers",
            "            files = None",
            "            data = {",
            "                \"url\": url,",
            "                \"documentName\": document_name,",
            "                \"documentDescription\": document_description,",
            "            }",
            "            response = requests.post(api_url, headers=headers, data=data, files=files)",
            "",
            "        elif file:",
            "            api_url = self.source_file_endpoint",
            "",
            "            headers = self.headers",
            "            if file is not None:",
            "                files = {\"file\": open(file, \"rb\")}",
            "            else:",
            "                files = None",
            "            data = {",
            "                \"url\": url,",
            "                \"documentName\": document_name,",
            "                \"documentDescription\": document_description,",
            "            }",
            "            response = requests.post(api_url, headers=headers, data=data, files=files)",
            "        if response.status_code == 200:",
            "            return response.json()",
            "        else:",
            "            # error_message = response.json()[\"message\"]",
            "            return {",
            "                \"message\": \"Bad Request\",",
            "            }",
            "",
            "    def query_knowledge(self, query: str) -> dict:",
            "        \"\"\"",
            "        Send a query to the Cogniswitch service and retrieve the response.",
            "",
            "        Args:",
            "            query (str): Query to be answered.",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        api_url = self.knowledge_request_endpoint",
            "",
            "        headers = self.headers",
            "",
            "        data = {\"query\": query}",
            "        response = requests.post(api_url, headers=headers, data=data)",
            "        if response.status_code == 200:",
            "            return response.json()",
            "        else:",
            "            # error_message = response.json()[\"message\"]",
            "            return {",
            "                \"message\": \"Bad Request\",",
            "            }",
            "",
            "    def knowledge_status(self, document_name: str) -> dict:",
            "        \"\"\"",
            "        Use this function to know the status of the document or the URL uploaded",
            "        Args:",
            "            document_name (str): The document name or the url that is uploaded.",
            "",
            "        Returns:",
            "            dict: Response JSON from the Cogniswitch service.",
            "        \"\"\"",
            "        params = {\"docName\": document_name, \"platformToken\": self.cs_token}",
            "        response = requests.get(",
            "            self.knowledge_status_endpoint,",
            "            headers=self.headers,",
            "            params=params,",
            "        )",
            "        if response.status_code == 200:",
            "            source_info = response.json()",
            "            return source_info[-1]",
            "        else:",
            "            # error_message = response.json()[\"message\"]",
            "            return {",
            "                \"message\": \"Bad Request\",",
            "            }"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "94": [
                "CogniswitchToolSpec",
                "store_data"
            ],
            "95": [
                "CogniswitchToolSpec",
                "store_data"
            ],
            "96": [
                "CogniswitchToolSpec",
                "store_data"
            ],
            "111": [
                "CogniswitchToolSpec",
                "store_data"
            ],
            "112": [
                "CogniswitchToolSpec",
                "store_data"
            ],
            "113": [
                "CogniswitchToolSpec",
                "store_data"
            ],
            "137": [
                "CogniswitchToolSpec",
                "query_knowledge"
            ],
            "160": [
                "CogniswitchToolSpec",
                "knowledge_status"
            ]
        },
        "addLocation": []
    }
}