{
    "python/paddle/distributed/fleet/launch_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 596,
                "afterPatchRowNumber": 596,
                "PatchRowcode": "         fn = None"
            },
            "1": {
                "beforePatchRowNumber": 597,
                "afterPatchRowNumber": 597,
                "PatchRowcode": "         pre_fn = None if os.name == 'nt' else os.setsid"
            },
            "2": {
                "beforePatchRowNumber": 598,
                "afterPatchRowNumber": 598,
                "PatchRowcode": "         if log_dir is not None:"
            },
            "3": {
                "beforePatchRowNumber": 599,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            os.system(f\"mkdir -p {log_dir}\")"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 599,
                "PatchRowcode": "+            os.makedirs(log_dir, exist_ok=True)"
            },
            "5": {
                "beforePatchRowNumber": 600,
                "afterPatchRowNumber": 600,
                "PatchRowcode": "             if os.path.exists(\"%s/endpoints.log\" % log_dir):"
            },
            "6": {
                "beforePatchRowNumber": 601,
                "afterPatchRowNumber": 601,
                "PatchRowcode": "                 os.system(f\"rm -f {log_dir}/endpoints.log\")"
            },
            "7": {
                "beforePatchRowNumber": 602,
                "afterPatchRowNumber": 602,
                "PatchRowcode": "             with open(\"%s/endpoints.log\" % log_dir, \"w\") as f:"
            },
            "8": {
                "beforePatchRowNumber": 1762,
                "afterPatchRowNumber": 1762,
                "PatchRowcode": "                 )"
            },
            "9": {
                "beforePatchRowNumber": 1763,
                "afterPatchRowNumber": 1763,
                "PatchRowcode": " "
            },
            "10": {
                "beforePatchRowNumber": 1764,
                "afterPatchRowNumber": 1764,
                "PatchRowcode": "             if args.log_dir is not None:"
            },
            "11": {
                "beforePatchRowNumber": 1765,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                os.system(f\"mkdir -p {args.log_dir}\")"
            },
            "12": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1765,
                "PatchRowcode": "+                os.makedirs(args.log_dir, exist_ok=True)"
            },
            "13": {
                "beforePatchRowNumber": 1766,
                "afterPatchRowNumber": 1766,
                "PatchRowcode": "                 fn = open(\"%s/serverlog.%d\" % (args.log_dir, idx), \"w\")"
            },
            "14": {
                "beforePatchRowNumber": 1767,
                "afterPatchRowNumber": 1767,
                "PatchRowcode": "                 self.log_fns[\"server\"].append(fn)"
            },
            "15": {
                "beforePatchRowNumber": 1768,
                "afterPatchRowNumber": 1768,
                "PatchRowcode": "                 proc = subprocess.Popen("
            },
            "16": {
                "beforePatchRowNumber": 1870,
                "afterPatchRowNumber": 1870,
                "PatchRowcode": "                 )"
            },
            "17": {
                "beforePatchRowNumber": 1871,
                "afterPatchRowNumber": 1871,
                "PatchRowcode": " "
            },
            "18": {
                "beforePatchRowNumber": 1872,
                "afterPatchRowNumber": 1872,
                "PatchRowcode": "             if args.log_dir is not None:"
            },
            "19": {
                "beforePatchRowNumber": 1873,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                os.system(f\"mkdir -p {args.log_dir}\")"
            },
            "20": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1873,
                "PatchRowcode": "+                os.makedirs(args.log_dir, exist_ok=True)"
            },
            "21": {
                "beforePatchRowNumber": 1874,
                "afterPatchRowNumber": 1874,
                "PatchRowcode": "                 fn = open(\"%s/workerlog.%d\" % (args.log_dir, idx), \"w\")"
            },
            "22": {
                "beforePatchRowNumber": 1875,
                "afterPatchRowNumber": 1875,
                "PatchRowcode": "                 self.log_fns[\"worker\"].append(fn)"
            },
            "23": {
                "beforePatchRowNumber": 1876,
                "afterPatchRowNumber": 1876,
                "PatchRowcode": "                 proc = subprocess.Popen("
            },
            "24": {
                "beforePatchRowNumber": 1938,
                "afterPatchRowNumber": 1938,
                "PatchRowcode": "                 )"
            },
            "25": {
                "beforePatchRowNumber": 1939,
                "afterPatchRowNumber": 1939,
                "PatchRowcode": " "
            },
            "26": {
                "beforePatchRowNumber": 1940,
                "afterPatchRowNumber": 1940,
                "PatchRowcode": "             if args.log_dir is not None:"
            },
            "27": {
                "beforePatchRowNumber": 1941,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                os.system(f\"mkdir -p {args.log_dir}\")"
            },
            "28": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 1941,
                "PatchRowcode": "+                os.makedirs(args.log_dir, exist_ok=True)"
            },
            "29": {
                "beforePatchRowNumber": 1942,
                "afterPatchRowNumber": 1942,
                "PatchRowcode": "                 fn = open(\"%s/coordinator.%d\" % (args.log_dir, idx), \"w\")"
            },
            "30": {
                "beforePatchRowNumber": 1943,
                "afterPatchRowNumber": 1943,
                "PatchRowcode": "                 self.log_fns[\"coordinator\"].append(fn)"
            },
            "31": {
                "beforePatchRowNumber": 1944,
                "afterPatchRowNumber": 1944,
                "PatchRowcode": "                 proc = subprocess.Popen("
            },
            "32": {
                "beforePatchRowNumber": 2029,
                "afterPatchRowNumber": 2029,
                "PatchRowcode": "                 )"
            },
            "33": {
                "beforePatchRowNumber": 2030,
                "afterPatchRowNumber": 2030,
                "PatchRowcode": " "
            },
            "34": {
                "beforePatchRowNumber": 2031,
                "afterPatchRowNumber": 2031,
                "PatchRowcode": "             if args.log_dir is not None:"
            },
            "35": {
                "beforePatchRowNumber": 2032,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-                os.system(f\"mkdir -p {args.log_dir}\")"
            },
            "36": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 2032,
                "PatchRowcode": "+                os.makedirs(args.log_dir, exist_ok=True)"
            },
            "37": {
                "beforePatchRowNumber": 2033,
                "afterPatchRowNumber": 2033,
                "PatchRowcode": "                 fn = open(\"%s/heterlog.%d\" % (args.log_dir, idx), \"w\")"
            },
            "38": {
                "beforePatchRowNumber": 2034,
                "afterPatchRowNumber": 2034,
                "PatchRowcode": "                 self.log_fns[\"heter_worker\"].append(fn)"
            },
            "39": {
                "beforePatchRowNumber": 2035,
                "afterPatchRowNumber": 2035,
                "PatchRowcode": "                 proc = subprocess.Popen("
            }
        },
        "frontPatchFile": [
            "# Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import copy",
            "import json",
            "import logging",
            "import multiprocessing",
            "import os",
            "import shutil",
            "import signal",
            "import socket",
            "import struct",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "from contextlib import closing",
            "from distutils.util import strtobool",
            "",
            "import paddle.utils.cpp_extension.extension_utils as utils",
            "from paddle import framework",
            "",
            "logger = logging.getLogger(\"root\")",
            "logger.propagate = False",
            "",
            "",
            "class DistributeMode:",
            "    \"\"\"",
            "    There are various mode for fleetrun, each of them is designed for different model.",
            "    \"\"\"",
            "",
            "    COLLECTIVE = 0",
            "    PS = 1",
            "    PS_HETER = 2",
            "",
            "",
            "class DeviceMode:",
            "    \"\"\"",
            "    Training devices type",
            "    \"\"\"",
            "",
            "    UNKNOWN = -1",
            "    CPU = 0",
            "    GPU = 1",
            "    KUNLUN = 2",
            "    XPU = 2",
            "    ASCEND_NPU = 3",
            "    UNKNOWN = 3",
            "",
            "",
            "class Cluster:",
            "    def __init__(self, hdfs):",
            "        self.job_server = None",
            "        self.pods = []",
            "        self.hdfs = None",
            "        self.job_stage_flag = None",
            "",
            "    def __str__(self):",
            "        return \"job_server:{} pods:{} job_stage_flag:{} hdfs:{}\".format(",
            "            self.job_server,",
            "            [str(pod) for pod in self.pods],",
            "            self.job_stage_flag,",
            "            self.hdfs,",
            "        )",
            "",
            "    def __eq__(self, cluster):",
            "        if len(self.pods) != len(cluster.pods):",
            "            return False",
            "",
            "        for a, b in zip(self.pods, cluster.pods):",
            "            if a != b:",
            "                return False",
            "",
            "        if self.job_stage_flag != cluster.job_stage_flag:",
            "            return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, cluster):",
            "        return not self.__eq__(cluster)",
            "",
            "    def update_pods(self, cluster):",
            "        self.pods = copy.copy(cluster.pods)",
            "",
            "    def trainers_nranks(self):",
            "        return len(self.trainers_endpoints())",
            "",
            "    def pods_nranks(self):",
            "        return len(self.pods)",
            "",
            "    def trainers_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            for t in pod.trainers:",
            "                r.append(t.endpoint)",
            "        return r",
            "",
            "    def world_device_ids(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            for t in pod.trainers:",
            "                str_accelerators = [str(acc) for acc in t.accelerators]",
            "                r.append(str_accelerators)",
            "        return r",
            "",
            "    def pods_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            ep = f\"{pod.addr}:{pod.port}\"",
            "            assert (",
            "                pod.port is not None and pod.addr is not None",
            "            ), f\"{ep} not a valid endpoint\"",
            "            r.append(ep)",
            "        return r",
            "",
            "    def get_pod_by_id(self, pod_id):",
            "        for pod in self.pods:",
            "            if str(pod_id) == str(pod.id):",
            "                return pod",
            "",
            "        return None",
            "",
            "",
            "class JobServer:",
            "    def __init__(self):",
            "        self.endpoint = None",
            "",
            "    def __str__(self):",
            "        return f\"{self.endpoint}\"",
            "",
            "    def __eq__(self, j):",
            "        return self.endpint == j.endpoint",
            "",
            "    def __ne__(self, j):",
            "        return not self == j",
            "",
            "",
            "class Trainer:",
            "    def __init__(self):",
            "        self.accelerators = []",
            "        self.endpoint = None",
            "        self.rank = None",
            "        self.stage = None",
            "",
            "    def __str__(self):",
            "        return \"accelerator:{} endpoint:{} rank:{}\".format(",
            "            self.accelerators, self.endpoint, self.rank",
            "        )",
            "",
            "    def __eq__(self, t):",
            "        if len(self.accelerators) != len(t.accelerators):",
            "            return False",
            "",
            "        if self.endpoint != t.endpoint or self.rank != t.rank:",
            "            return False",
            "",
            "        for a, b in zip(self.accelerators, t.accelerators):",
            "            if a != b:",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, t):",
            "        return not self == t",
            "",
            "    def rank(self):",
            "        return self.rank",
            "",
            "",
            "class Pod:",
            "    def __init__(self):",
            "        self.rank = None",
            "        self.id = None",
            "        self.addr = None",
            "        self.port = None",
            "        self.trainers = []",
            "        self.servers = []",
            "        self.workers = []",
            "        self.coordinators = []",
            "        self.heter_workers = []",
            "        self.accelerators = []",
            "        self.device_mode = None",
            "",
            "    def __str__(self):",
            "        return \"rank:{} id:{} addr:{} port:{} visible_accelerator:{} trainers:{} servers:{} \\",
            "            workers:{} heter_workers:{} coordinators:{}\".format(",
            "            self.rank,",
            "            self.id,",
            "            self.addr,",
            "            self.port,",
            "            self.accelerators,",
            "            [str(t) for t in self.trainers],",
            "            [str(s) for s in self.servers],",
            "            [str(w) for w in self.workers],",
            "            [str(h) for h in self.heter_workers],",
            "            [str(c) for c in self.coordinators],",
            "        )",
            "",
            "    def __eq__(self, pod):",
            "        if (",
            "            self.rank != pod.rank",
            "            or self.id != pod.id",
            "            or self.addr != pod.addr",
            "            or self.port != pod.port",
            "        ):",
            "            logger.debug(f\"pod {self} != {pod}\")",
            "            return False",
            "",
            "        if len(self.trainers) != len(pod.trainers):",
            "            logger.debug(f\"trainers {self.trainers} != {pod.trainers}\")",
            "            return False",
            "",
            "        for i in range(len(self.trainers)):",
            "            if self.trainers[i] != pod.trainers[i]:",
            "                logger.debug(f\"trainer {self.trainers[i]} != {pod.trainers[i]}\")",
            "                return False",
            "",
            "        if len(self.servers) != len(pod.servers):",
            "            logger.debug(f\"servers {self.servers} != {pod.servers}\")",
            "            return False",
            "",
            "        for i in range(len(self.servers)):",
            "            if self.servers[i] != pod.servers[i]:",
            "                logger.debug(f\"servers {self.servers[i]} != {pod.servers[i]}\")",
            "                return False",
            "",
            "        if len(self.workers) != len(pod.workers):",
            "            logger.debug(f\"workers {self.workers} != {pod.workers}\")",
            "            return False",
            "",
            "        for i in range(len(self.workers)):",
            "            if self.workers[i] != pod.workers[i]:",
            "                logger.debug(f\"workers {self.workers[i]} != {pod.workers[i]}\")",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, pod):",
            "        return not self == pod",
            "",
            "    def parse_response(self, res_pods):",
            "        pass",
            "",
            "    def rank(self):",
            "        return self.rank",
            "",
            "    def get_visible_accelerators(self):",
            "        r = \"\"",
            "        for g in self.accelerators:",
            "            r += f\"{g},\"",
            "",
            "        assert r != \"\", f\"this pod {self} can't see any accelerators\"",
            "",
            "        r = r[:-1]",
            "        return r",
            "",
            "",
            "def get_logger(log_level=20, name=\"root\"):",
            "    logger = logging.getLogger(name)",
            "    logger.setLevel(log_level)",
            "",
            "    log_handler = logging.StreamHandler()",
            "    log_format = logging.Formatter(",
            "        '%(levelname)s %(asctime)s %(filename)s:%(lineno)d] %(message)s'",
            "    )",
            "    log_handler.setFormatter(log_format)",
            "    logger.addHandler(log_handler)",
            "",
            "    return logger",
            "",
            "",
            "def get_cluster(",
            "    node_ips, node_ip, trainer_endpoints, device_mode, devices_per_proc",
            "):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    cluster = Cluster(hdfs=None)",
            "    trainer_rank = 0",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        pod.device_mode = device_mode",
            "",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "        # when use paddlecloud, endpoints may > devices_per_proc(user_defined)",
            "        assert len(cur_node_endpoints) >= len(",
            "            devices_per_proc",
            "        ), \"current trainer_endpoints size should be greater equal than acclerators size.\"",
            "        for i in range(len(devices_per_proc)):",
            "            trainer = Trainer()",
            "            if (",
            "                device_mode == DeviceMode.GPU",
            "                or device_mode == DeviceMode.ASCEND_NPU",
            "            ):",
            "                if isinstance(devices_per_proc[i], (list, tuple)):",
            "                    trainer.accelerators.extend(devices_per_proc[i])",
            "                    pod.accelerators.extend(devices_per_proc[i])",
            "                else:",
            "                    trainer.accelerators.append(devices_per_proc[i])",
            "                    pod.accelerators.append(devices_per_proc[i])",
            "            elif device_mode == DeviceMode.XPU:",
            "                if isinstance(devices_per_proc[i], (list, tuple)):",
            "                    trainer.accelerators.extend(devices_per_proc[i])",
            "                else:",
            "                    trainer.accelerators.append(devices_per_proc[i])",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = trainer_rank",
            "            trainer_rank += 1",
            "",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def terminate_local_procs(procs):",
            "    # try to terminate process by group, this happend in multiprocess senario in user process",
            "    if os.name != 'nt':",
            "        for p in procs:",
            "            if p.proc.poll() is None:",
            "                os.killpg(os.getpgid(p.proc.pid), signal.SIGTERM)",
            "                if p.log_fn:",
            "                    p.log_fn.close()",
            "                logger.info(f\"terminate process group gid:{p.proc.pid}\")",
            "",
            "        time.sleep(1)",
            "",
            "    for p in procs:",
            "        if p.proc.poll() is None:",
            "            p.proc.terminate()",
            "            if p.log_fn:",
            "                p.log_fn.close()",
            "            logger.debug(f\"terminate process id:{p.proc.pid}\")",
            "",
            "    # wait all process terminiated",
            "    time.sleep(3)",
            "    for step in range(0, 50):",
            "        alive = False",
            "        for p in procs:",
            "            if p.proc.poll() is None:  # not termniate",
            "                os.kill(p.proc.pid, signal.SIGKILL)",
            "                alive = True",
            "",
            "        if not alive:",
            "            logger.info(\"terminate all the procs\")",
            "            return",
            "",
            "        time.sleep(3)",
            "",
            "    logger.fatal(\"can't kill all process and exit\")",
            "    sys.exit(1)",
            "",
            "",
            "def get_host_name_ip():",
            "    try:",
            "        host_name = socket.gethostname()",
            "        host_ip = socket.gethostbyname(host_name)",
            "        return host_name, host_ip",
            "    except:",
            "        return None",
            "",
            "",
            "def add_arguments(argname, type, default, help, argparser, **kwargs):",
            "    \"\"\"Add argparse's argument.",
            "    Usage:",
            "    .. code-block:: python",
            "        parser = argparse.ArgumentParser()",
            "        add_argument(\"name\", str, \"Jonh\", \"User name.\", parser)",
            "        args = parser.parse_args()",
            "    \"\"\"",
            "    type = strtobool if type == bool else type",
            "    argparser.add_argument(",
            "        \"--\" + argname,",
            "        default=default,",
            "        type=type,",
            "        help=help + ' Default: %(default)s.',",
            "        **kwargs,",
            "    )",
            "",
            "",
            "def find_free_ports(num):",
            "    def __free_port():",
            "        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:",
            "            # Note(wangxi): Close the connection with a TCP RST instead",
            "            # of a TCP FIN, to avoid time_wait state.",
            "            s.setsockopt(",
            "                socket.SOL_SOCKET, socket.SO_LINGER, struct.pack('ii', 1, 0)",
            "            )",
            "            s.bind(('', 0))",
            "            return s.getsockname()[1]",
            "",
            "    port_set = set()",
            "    step = 0",
            "    while True:",
            "        port = __free_port()",
            "        if port not in port_set:",
            "            port_set.add(port)",
            "",
            "        if len(port_set) >= num:",
            "            return port_set",
            "",
            "        step += 1",
            "        if step > 400:",
            "            print(",
            "                \"can't find avilable port and use the specified static port now!\"",
            "            )",
            "            return None",
            "",
            "    return None",
            "",
            "",
            "def get_ports(num, offset):",
            "    if os.environ.get('FLAGS_START_PORT') is None:",
            "        ports = find_free_ports(num)",
            "        if ports is not None:",
            "            ports = list(ports)",
            "    else:",
            "        start_port = int(os.environ.get('FLAGS_START_PORT'))",
            "        ports = range(start_port + offset, start_port + offset + num, 1)",
            "    return ports",
            "",
            "",
            "def pretty_print_envs(envs, header=None):",
            "    spacing = 2",
            "    max_k = 40",
            "    max_v = 45",
            "",
            "    for k, v in envs.items():",
            "        max_k = max(max_k, len(k))",
            "",
            "    h_format = \"    \" + \"|{{:>{}s}}{}{{:^{}s}}|\\n\".format(",
            "        max_k, \" \" * spacing, max_v",
            "    )",
            "    l_format = \"    \" + f\"|{{:>{max_k}s}}{{}}{{:^{max_v}s}}|\\n\"",
            "    length = max_k + max_v + spacing",
            "",
            "    border = \"    +\" + \"\".join([\"=\"] * length) + \"+\"",
            "    line = \"    +\" + \"\".join([\"-\"] * length) + \"+\"",
            "",
            "    draws = \"\"",
            "    draws += border + \"\\n\"",
            "",
            "    if header:",
            "        draws += h_format.format(header[0], header[1])",
            "    else:",
            "        draws += h_format.format(\"fleetrun Distributed Envs\", \"Value\")",
            "",
            "    draws += line + \"\\n\"",
            "",
            "    for k, v in envs.items():",
            "        if isinstance(v, str) and len(v) >= max_v:",
            "            str_v = \"... \" + v[-41:]",
            "        else:",
            "            str_v = v",
            "",
            "        draws += l_format.format(k, \" \" * spacing, str(str_v))",
            "",
            "    draws += border",
            "",
            "    _str = f\"\\n{draws}\\n\"",
            "    return _str",
            "",
            "",
            "class TrainerProc:",
            "    def __init__(self):",
            "        self.proc = None",
            "        self.log_fn = None",
            "        self.log_offset = None",
            "        self.rank = None",
            "        self.local_rank = None",
            "        self.cmd = None",
            "",
            "",
            "_run_with_coverage = False",
            "",
            "",
            "def run_with_coverage(*args):",
            "    global _run_with_coverage",
            "    assert len(args) <= 1, f\"len(args) {len(args)} should <= 1\"",
            "    if len(args) == 1:",
            "        assert isinstance(args[0], bool)",
            "        _run_with_coverage = args[0]",
            "    return _run_with_coverage",
            "",
            "",
            "def start_local_trainers(",
            "    cluster, pod, training_script, training_script_args, log_dir=None, envs=None",
            "):",
            "",
            "    if envs is None:",
            "        current_env = copy.copy(os.environ.copy())",
            "    else:",
            "        current_env = copy.copy(envs)",
            "",
            "    # paddle broadcast ncclUniqueId use socket, and",
            "    # proxy maybe make trainers unreachable, so delete them.",
            "    # if we set them to \"\", grpc will log error message \"bad uri\"",
            "    # so just delete them.",
            "    current_env.pop(\"http_proxy\", None)",
            "    current_env.pop(\"https_proxy\", None)",
            "",
            "    ids = cluster.world_device_ids()",
            "    res = [':'.join(ele) for ele in ids]",
            "    procs = []",
            "    for idx, t in enumerate(pod.trainers):",
            "        proc_env = {",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % t.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % t.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "            \"PADDLE_RANK_IN_NODE\": str(idx),",
            "            \"PADDLE_LOCAL_DEVICE_IDS\": \",\".join(",
            "                [str(acc) for acc in t.accelerators]",
            "            ),",
            "            \"PADDLE_WORLD_DEVICE_IDS\": \",\".join(res),",
            "        }",
            "",
            "        # The following three environnement variables are used for auto mapping",
            "        if current_env.get(\"PADDLE_CLUSTER_TOPO_PATH\", None) is not None:",
            "            proc_env[\"PADDLE_CLUSTER_TOPO_PATH\"] = current_env[",
            "                \"PADDLE_CLUSTER_TOPO_PATH\"",
            "            ]",
            "        if current_env.get(\"PADDLE_RANK_MAPPING_PATH\", None) is not None:",
            "            proc_env[\"PADDLE_RANK_MAPPING_PATH\"] = current_env[",
            "                \"PADDLE_RANK_MAPPING_PATH\"",
            "            ]",
            "        if current_env.get(\"PADDLE_ENABLE_AUTO_MAPPING\", None) is not None:",
            "            proc_env[\"PADDLE_ENABLE_AUTO_MAPPING\"] = current_env[",
            "                \"PADDLE_ENABLE_AUTO_MAPPING\"",
            "            ]",
            "",
            "        if len(t.accelerators) > 0 and pod.device_mode == DeviceMode.GPU:",
            "            proc_env[\"FLAGS_selected_gpus\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "",
            "        elif (",
            "            len(t.accelerators) > 0 and pod.device_mode == DeviceMode.ASCEND_NPU",
            "        ):",
            "            proc_env[\"FLAGS_selected_npus\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "",
            "        if len(t.accelerators) > 0:",
            "            proc_env[\"FLAGS_selected_accelerators\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "        # to do: same code style in future",
            "        if framework.core.is_compiled_with_xpu() and len(t.accelerators) > 0:",
            "            proc_env[\"FLAGS_selected_xpus\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "",
            "        current_env.update(proc_env)",
            "",
            "        coverage_args = []",
            "        if (",
            "            run_with_coverage()",
            "            or os.environ.get(\"WITH_COVERAGE\", \"OFF\") == \"ON\"",
            "        ):",
            "            coverage_args = [\"-m\", \"coverage\", \"run\", \"--branch\", \"-p\"]",
            "        cmd = (",
            "            [sys.executable, \"-u\"]",
            "            + coverage_args",
            "            + [training_script]",
            "            + training_script_args",
            "        )",
            "",
            "        logger.debug(f\"start trainer proc{cmd}  env:{current_env}\")",
            "",
            "        if idx == 0:",
            "            logger.info(",
            "                \"Local start {} processes. First process distributed \"",
            "                \"environment info (Only For Debug): {}\".format(",
            "                    len(pod.trainers),",
            "                    pretty_print_envs(proc_env, (\"Distributed Envs\", \"Value\")),",
            "                )",
            "            )",
            "            logger.info(",
            "                \"details about PADDLE_TRAINER_ENDPOINTS can be found in \"",
            "                \"{}/endpoints.log, and detail running logs maybe found in \"",
            "                \"{}/workerlog.0\".format(log_dir, log_dir)",
            "            )",
            "        fn = None",
            "        pre_fn = None if os.name == 'nt' else os.setsid",
            "        if log_dir is not None:",
            "            os.system(f\"mkdir -p {log_dir}\")",
            "            if os.path.exists(\"%s/endpoints.log\" % log_dir):",
            "                os.system(f\"rm -f {log_dir}/endpoints.log\")",
            "            with open(\"%s/endpoints.log\" % log_dir, \"w\") as f:",
            "                f.write(\"PADDLE_TRAINER_ENDPOINTS: \\n\")",
            "                f.write(\"\\n\".join(cluster.trainers_endpoints()))",
            "            if (",
            "                current_env.get(\"PADDLE_ENABLE_AUTO_MAPPING\") is not None",
            "                and current_env.get(\"PADDLE_NEED_RANK_MAPPING\").lower()",
            "                == \"true\"",
            "            ):",
            "                fn = open(\"%s/prelaunchlog.%d\" % (log_dir, idx), \"a\")",
            "            else:",
            "                fn = open(\"%s/workerlog.%d\" % (log_dir, idx), \"a\")",
            "            proc = subprocess.Popen(",
            "                cmd, env=current_env, stdout=fn, stderr=fn, preexec_fn=pre_fn",
            "            )",
            "        else:",
            "            proc = subprocess.Popen(cmd, env=current_env, preexec_fn=pre_fn)",
            "",
            "        tp = TrainerProc()",
            "        tp.proc = proc",
            "        tp.rank = t.rank",
            "        tp.local_rank = idx",
            "        tp.log_fn = fn",
            "        tp.log_offset = fn.tell() if fn else None",
            "        tp.cmd = cmd",
            "",
            "        procs.append(tp)",
            "",
            "    return procs",
            "",
            "",
            "def pull_worker_log(tp):",
            "    if tp.log_fn:",
            "        with open(tp.log_fn.name, 'r') as fin:",
            "            fin.seek(tp.log_offset, 0)",
            "            for line in fin:",
            "                try:",
            "                    sys.stdout.write(line)",
            "                except UnicodeEncodeError:",
            "                    sys.stdout.write(",
            "                        'UnicodeEncodeError occurs at this line. '",
            "                        'Please refer to the original log file \"%s\"\\n'",
            "                        % tp.log_fn.name",
            "                    )",
            "            tp.log_offset = fin.tell()",
            "",
            "",
            "def watch_local_trainers(procs, nranks):",
            "    try:",
            "        error = False",
            "        error_rank = []",
            "        # wait all process finish or one error",
            "        alive = False",
            "        for p in procs:",
            "            if p.log_fn and p.local_rank == 0:",
            "                pull_worker_log(p)",
            "",
            "            ret = p.proc.poll()",
            "            if ret is None:",
            "                alive = True",
            "            elif ret != 0:",
            "                error = True",
            "                error_rank.append(p.rank)",
            "",
            "        if error:",
            "            terminate_local_procs(procs)",
            "            sys.exit(1)",
            "",
            "    except KeyboardInterrupt:",
            "        logger.warning(\"KeyboardInterrupt, exit\")",
            "        terminate_local_procs(procs)",
            "        return",
            "    except SystemExit:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        raise",
            "    except:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        return",
            "",
            "    return alive",
            "",
            "",
            "def get_gpus(gpus):",
            "    if gpus is None:",
            "        gpus_num = framework.core.get_cuda_device_count()",
            "        res_gpus = [str(x) for x in range(0, gpus_num)]",
            "    else:",
            "        cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\")",
            "        if cuda_visible_devices is None or cuda_visible_devices == \"\":",
            "            res_gpus = [x.strip() for x in gpus.split(',')]",
            "        else:",
            "            # change gpus into relative values",
            "            # e.g. CUDA_VISIBLE_DEVICES=4,5,6,7; args.gpus=4,5,6,7;",
            "            # therefore gpus=0,1,2,3",
            "            cuda_visible_devices_list = cuda_visible_devices.split(',')",
            "            for x in gpus.split(','):",
            "                assert x in cuda_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your gpus %s in CUDA_VISIBLE_DEVICES[%s].\"",
            "                    % (x, cuda_visible_devices)",
            "                )",
            "            res_gpus = [",
            "                cuda_visible_devices_list.index(x.strip())",
            "                for x in gpus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_gpus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"CUDA_VISIBLE_DEVICES:{}\".format(",
            "                    gpus, res_gpus, cuda_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return res_gpus",
            "",
            "",
            "def get_xpus(xpus):",
            "    if xpus is None:",
            "        xpus_num = framework.core.get_xpu_device_count()",
            "        res_xpus = [str(x) for x in range(0, xpus_num)]",
            "    else:",
            "        xpu_visible_devices = os.getenv(\"XPU_VISIBLE_DEVICES\")",
            "        if xpu_visible_devices is None or xpu_visible_devices == \"\":",
            "            res_xpus = [x.strip() for x in xpus.split(',')]",
            "        else:",
            "            # change xpus into relative values",
            "            # e.g. XPU_VISIBLE_DEVICES=4,5,6,7; args.xpus=4,5,6,7;",
            "            # therefore xpus=0,1,2,3",
            "            xpu_visible_devices_list = xpu_visible_devices.split(',')",
            "            for x in xpus.split(','):",
            "                assert x in xpu_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your xpus {} in XPU_VISIBLE_DEVICES[{}].\".format(",
            "                        x,",
            "                        xpu_visible_devices,",
            "                    )",
            "                )",
            "            res_xpus = [",
            "                xpu_visible_devices_list.index(x.strip())",
            "                for x in xpus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_xpus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"XPU_VISIBLE_DEVICES:{}\".format(",
            "                    xpus, res_xpus, xpu_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return res_xpus",
            "",
            "",
            "def get_npus(npus):",
            "    if npus is None:",
            "        npus_num = framework.core.get_npu_device_count()",
            "        res_npus = [str(x) for x in range(0, npus_num)]",
            "    else:",
            "        npu_visible_devices = os.getenv(\"ASCEND_VISIBLE_DEVICES\")",
            "        if npu_visible_devices is None or npu_visible_devices == \"\":",
            "            res_npus = [x.strip() for x in npus.split(',')]",
            "        else:",
            "            # change npus into relative values",
            "            # e.g. ASCEND_VISIBLE_DEVICES=4,5,6,7; args.npus=4,5,6,7;",
            "            # therefore npus=0,1,2,3",
            "            npu_visible_devices_list = npu_visible_devices.split(',')",
            "            for x in npus.split(','):",
            "                assert x in npu_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your npus %s in ASCEND_VISIBLE_DEVICES[%s].\"",
            "                    % (x, npu_visible_devices)",
            "                )",
            "            res_npus = [",
            "                npu_visible_devices_list.index(x.strip())",
            "                for x in npus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_npus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"ASCEND_VISIBLE_DEVICES:{}\".format(",
            "                    npus, res_npus, npu_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return res_npus",
            "",
            "",
            "def get_device_mode(backend):",
            "    if backend == 'heter':",
            "        if (",
            "            framework.core.is_compiled_with_cuda()",
            "            and framework.core.get_cuda_device_count() > 0",
            "        ):",
            "            print(\"launch train in heter mode with GPU device.\")",
            "            return DeviceMode.GPU",
            "        if (",
            "            framework.core.is_compiled_with_xpu()",
            "            and framework.core.get_xpu_device_count() > 0",
            "        ):",
            "            print(\"launch train in heter mode with XPU device.\")",
            "            return DeviceMode.XPU",
            "        if (",
            "            framework.core.is_compiled_with_npu()",
            "            and framework.core.get_npu_device_count() > 0",
            "        ):",
            "            print(\"launch train in heter mode with NPU device.\")",
            "            return DeviceMode.ASCEND_NPU",
            "",
            "    if backend == 'hccl' and framework.core.get_npu_device_count() > 0:",
            "        print(\"launch train in ascend npu mode!\")",
            "        return DeviceMode.ASCEND_NPU",
            "",
            "    if backend == 'nccl' and framework.core.get_cuda_device_count() > 0:",
            "        print(\"launch train in GPU mode!\")",
            "        return DeviceMode.GPU",
            "",
            "    if backend == 'bkcl' and framework.core.get_xpu_device_count() > 0:",
            "        print(\"launch train in XPU mode\")",
            "        return DeviceMode.XPU",
            "",
            "    if backend == 'gloo':",
            "        print(\"launch train in CPU mode\")",
            "        return DeviceMode.CPU",
            "",
            "    raise RuntimeError(\"Don't supported devices\")",
            "",
            "",
            "def get_device_proc_info(args):",
            "    # device_mode",
            "    device_mode = get_device_mode(args.backend)",
            "",
            "    # devices",
            "    devices_per_proc = []",
            "    if device_mode == DeviceMode.GPU:",
            "        gpus = get_gpus(args.gpus)",
            "        if args.nproc_per_node is not None:",
            "            assert (",
            "                len(gpus) % int(args.nproc_per_node)",
            "            ) == 0, \"gpus' number:{} mod args.nproc_per_node:{} must == 0\".format(",
            "                len(gpus), args.nproc_per_node",
            "            )",
            "",
            "            n = int(len(gpus) / int(args.nproc_per_node))",
            "            devices_per_proc = [gpus[i : i + n] for i in range(0, len(gpus), n)]",
            "        else:",
            "            devices_per_proc = gpus",
            "    elif device_mode == DeviceMode.ASCEND_NPU:",
            "        npus = get_npus(args.npus)",
            "        if args.nproc_per_node is not None:",
            "            assert (",
            "                len(npus) % int(args.nproc_per_node)",
            "            ) == 0, \"npus' number:{} mod args.nproc_per_node:{} must == 0\".format(",
            "                len(npus), args.nproc_per_node",
            "            )",
            "",
            "            n = int(len(npus) / int(args.nproc_per_node))",
            "            devices_per_proc = [npus[i : i + n] for i in range(0, len(npus), n)]",
            "        else:",
            "            devices_per_proc = npus",
            "    elif device_mode == DeviceMode.XPU:",
            "        xpus = get_xpus(args.xpus)",
            "        if args.nproc_per_node is not None:",
            "            assert (",
            "                len(xpus) % int(args.nproc_per_node)",
            "            ) == 0, \"xpus' number:{} mod args.nproc_per_node:{} must == 0\".format(",
            "                len(xpus), args.nproc_per_node",
            "            )",
            "",
            "            n = int(len(xpus) / int(args.nproc_per_node))",
            "            devices_per_proc = [xpus[i : i + n] for i in range(0, len(xpus), n)]",
            "        else:",
            "            devices_per_proc = xpus",
            "    elif device_mode == DeviceMode.CPU:",
            "        if hasattr(args, \"paddle_cpuonly\") and args.nproc_per_node is None:",
            "            # NOTE (xiongkun03) set it to cpu core number",
            "            args.nproc_per_node = multiprocessing.cpu_count()",
            "        if args.nproc_per_node is None:",
            "            devices_per_proc = [0]",
            "        else:",
            "            devices_per_proc = list(range(0, args.nproc_per_node))",
            "    else:",
            "        raise AssertionError(",
            "            \"Can't support device_mode:{}, support only cpu|gpu|xpu now.\".format(",
            "                device_mode",
            "            )",
            "        )",
            "",
            "    return (device_mode, devices_per_proc)",
            "",
            "",
            "def direct_start(args):",
            "    # run ps-cpu mode on paddlecloud, using given envs",
            "    cmd = [",
            "        sys.executable,",
            "        \"-u\",",
            "        args.training_script,",
            "    ] + args.training_script_args",
            "    proc = subprocess.Popen(cmd)",
            "    proc.wait()",
            "    return",
            "",
            "",
            "def get_custom_endpoints(origin_endpoints, offset=0):",
            "    \"\"\"",
            "    origin_endpoint: ip:port",
            "    user_define_endpoint: ip:(port+offset)",
            "    \"\"\"",
            "    assert origin_endpoints is not None",
            "    paddle_user_define_endpoints_list = []",
            "    for ip_port in origin_endpoints.split(\",\"):",
            "        ip = ip_port.split(\":\")[0]",
            "        port = ip_port.split(\":\")[1]",
            "        new_port = int(port) + offset",
            "        paddle_user_define_endpoints_list.append(\":\".join((ip, str(new_port))))",
            "    paddle_user_define_endpoints = \",\".join(paddle_user_define_endpoints_list)",
            "    return paddle_user_define_endpoints",
            "",
            "",
            "# def cloud_ps_heter_env_set(args):",
            "#    environs = {}",
            "#",
            "#    paddle_trainer_endpoints = os.getenv(\"TRAINER_IP_PORT_LIST\", \"\")",
            "#    assert paddle_trainer_endpoints != None",
            "#",
            "#    paddle_pserver_endpoints = os.getenv(\"PSERVER_IP_PORT_LIST\", \"\")",
            "#    assert paddle_pserver_endpoints != None",
            "#",
            "#    # hard code for paddlecloud custom-framework",
            "#    avilable_ports = os.getenv(\"TRAINER_PORTS\", \"\").split(\",\")",
            "#    assert len(",
            "#        avilable_ports",
            "#    ) >= 2, \"set paddle_ports_num >= 2 in config.ini for paddlecloud job submit\"",
            "#",
            "#    # hard code for paddlecloud custom-framework",
            "#    trainers_num = len(paddle_pserver_endpoints.split(\",\"))",
            "#    assert trainers_num != 0",
            "#    environs[\"PADDLE_TRAINERS_NUM\"] = trainers_num",
            "#    environs[\"TRAINERS_NUM\"] = trainers_num",
            "#",
            "#    # hard code for paddlecloud custom-framework",
            "#    environs[\"PADDLE_HETER_TRAINER_IP_PORT_LIST\"] = paddle_trainer_endpoints",
            "#    environs[\"PADDLE_PSERVERS_IP_PORT_LIST\"] = paddle_pserver_endpoints",
            "#    environs[\"PADDLE_TRAINER_ENDPOINTS\"] = get_custom_endpoints(",
            "#        paddle_pserver_endpoints, 1)",
            "#    heter_worker_num = len(paddle_trainer_endpoints.split(\",\"))",
            "#    if (args.heter_worker_num != None) and (",
            "#            heter_worker_num != args.heter_worker_num):",
            "#        warnings.warn(",
            "#            \"Your fleetrun setting: heter_worker_num is {}, but we find {} device can be used, this setting has been changed.\".",
            "#            format(args.heter_worker_num, heter_worker_num))",
            "#        args.heter_worker_num = heter_worker_num",
            "#",
            "#    for k, v in environs.items():",
            "#        os.environ[k] = str(v)",
            "#    logger.info(\"Set heter parameter server env: {}\".format(",
            "#        pretty_print_envs(environs)))",
            "",
            "",
            "def get_mapped_cluster_without_rank_mapping(",
            "    node_ips, node_ip, trainer_endpoints, device_mode, node_ranks",
            "):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "    cluster = Cluster(hdfs=None)",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        pod.device_mode = device_mode",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "",
            "        # choose rank from global mapped ranks and set it to the trainer.",
            "        ranks_per_node = node_ranks[node_rank]",
            "        assert len(ranks_per_node) == 1",
            "        for i in range(len(ranks_per_node)):",
            "            trainer = Trainer()",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = ranks_per_node[i]",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def get_mapped_cluster_from_args_without_rank_mapping(args, device_mode):",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "    gpus_num = framework.core.get_cuda_device_count()",
            "",
            "    # parse ip-ranks json file",
            "    cluster_topo = None",
            "    with open(args.cluster_topo_path, \"r\") as json_file:",
            "        cluster_topo = json.load(json_file)",
            "",
            "    node_ips = []",
            "    node_ranks = []",
            "    for idx, cur_cluster_topo in enumerate(cluster_topo[\"machines\"]):",
            "        node_ips.append(cur_cluster_topo['addr'])",
            "        node_ranks.append([idx])",
            "",
            "    if len(node_ips) == 1:",
            "        node_ip = node_ips[0]",
            "    else:",
            "        if args.host:",
            "            node_ip = args.host",
            "        else:",
            "            _, node_ip = get_host_name_ip()",
            "",
            "    assert (",
            "        node_ip in node_ips",
            "    ), f\"Can't find your local ip {{{node_ip}}} in node_ips: {{{node_ips}}}\"",
            "    node_rank = node_ips.index(node_ip)",
            "",
            "    assert len(node_ranks) == len(",
            "        node_ips",
            "    ), \"ranks length should be equal to ips length.\"",
            "",
            "    logger.debug(",
            "        \"parsed from args: node_ips:{} node_ip:{} \"",
            "        \"node_rank:{} node_ranks:{}\".format(",
            "            node_ips, node_ip, node_rank, node_ranks[node_rank]",
            "        )",
            "    )",
            "",
            "    # NOTE: there are different number of global mapped ranks on each node.",
            "    free_ports = []",
            "    trainer_endpoints = []",
            "    for ip in node_ips:",
            "        node_rank = node_ips.index(ip)",
            "        if os.environ.get('PADDLE_PORT') is not None:",
            "            start_port = int(os.getenv(\"PADDLE_PORT\", \"\"))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        elif os.environ.get('FLAGS_START_PORT') is not None:",
            "            start_port = int(os.environ.get('FLAGS_START_PORT'))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        else:",
            "            free_ports = find_free_ports(len(node_ranks[node_rank]))",
            "        trainer_endpoints.append([\"%s:%d\" % (ip, port) for port in free_ports])",
            "",
            "    return get_mapped_cluster_without_rank_mapping(",
            "        node_ips, node_ip, trainer_endpoints, device_mode, node_ranks",
            "    )",
            "",
            "",
            "def get_mapped_cluster_with_rank_mapping(",
            "    node_ips,",
            "    node_ip,",
            "    trainer_endpoints,",
            "    device_mode,",
            "    node_ranks,",
            "    node_rank_mappings,",
            "):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "",
            "    def get_relative_gpu_id(gpu_id):",
            "        cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\")",
            "        if cuda_visible_devices is None or cuda_visible_devices == \"\":",
            "            return gpu_id",
            "        else:",
            "            cuda_visible_devices_list = cuda_visible_devices.split(',')",
            "            relative_id = cuda_visible_devices_list.index(str(gpu_id))",
            "            logger.info(",
            "                \"Change gpu id from {} to {} based on CUDA_VISIBLE_DEVICES {}\".format(",
            "                    gpu_id, relative_id, cuda_visible_devices_list",
            "                )",
            "            )",
            "            return relative_id",
            "",
            "    cluster = Cluster(hdfs=None)",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        pod.device_mode = device_mode",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "",
            "        # choose rank from global mapped ranks and set it to the trainer.",
            "        ranks_per_node = node_ranks[node_rank]",
            "        cur_node_rank_mapping = node_rank_mappings[node_rank]",
            "        for i in range(len(ranks_per_node)):",
            "            trainer = Trainer()",
            "            local_device_ids = cur_node_rank_mapping[\"ranks\"][",
            "                str(ranks_per_node[i])",
            "            ]",
            "            assert (",
            "                len(local_device_ids) == 1",
            "            ), \"Only support one process to one device mapping\"",
            "            trainer.accelerators.append(",
            "                get_relative_gpu_id(local_device_ids[0])",
            "            )",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = ranks_per_node[i]",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def get_mapped_cluster_from_args_with_rank_mapping(args, device_mode):",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "    gpus_num = framework.core.get_cuda_device_count()",
            "",
            "    # parse ip-ranks json file",
            "    rank_mapping_path = args.rank_mapping_path or os.getenv(",
            "        \"PADDLE_RANK_MAPPING_PATH\"",
            "    )",
            "    rank_mapping = None",
            "    with open(rank_mapping_path, \"r\") as json_file:",
            "        rank_mapping = json.load(json_file)",
            "    # reset PADDLE_RANK_MAPPING_PATH env",
            "    os.environ[\"PADDLE_RANK_MAPPING_PATH\"] = \"\"",
            "",
            "    node_ips = []",
            "    node_ranks = []",
            "    node_rank_mappings = []",
            "    for cur_rank_mapping in rank_mapping:",
            "        node_ips.append(cur_rank_mapping['addr'])",
            "        cur_node_rank_list = [",
            "            int(i) for i in list(cur_rank_mapping['ranks'].keys())",
            "        ]",
            "        cur_node_rank_list.sort()",
            "        node_ranks.append(cur_node_rank_list)",
            "        node_rank_mappings.append(cur_rank_mapping)",
            "",
            "    if len(node_ips) == 1:",
            "        node_ip = node_ips[0]",
            "    else:",
            "        if args.host:",
            "            node_ip = args.host",
            "        else:",
            "            _, node_ip = get_host_name_ip()",
            "",
            "    assert (",
            "        node_ip in node_ips",
            "    ), f\"Can't find your local ip {{{node_ip}}} in node_ips: {{{node_ips}}}\"",
            "    node_rank = node_ips.index(node_ip)",
            "",
            "    assert (",
            "        len(node_ranks[node_rank]) <= gpus_num",
            "    ), \"number of ranks mapped to one node should not exceed the avaiable ones.\"",
            "    assert len(node_ranks) == len(",
            "        node_ips",
            "    ), \"ranks length should be equal to ips length.\"",
            "",
            "    logger.debug(",
            "        \"parsed from args: node_ips:{} node_ip:{} \"",
            "        \"node_rank:{} node_ranks:{}\".format(",
            "            node_ips, node_ip, node_rank, node_ranks[node_rank]",
            "        )",
            "    )",
            "",
            "    # NOTE: there are different number of global mapped ranks on each node.",
            "    free_ports = []",
            "    trainer_endpoints = []",
            "    for ip in node_ips:",
            "        node_rank = node_ips.index(ip)",
            "        if os.environ.get('PADDLE_PORT') is not None:",
            "            start_port = int(os.getenv(\"PADDLE_PORT\", \"\"))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        elif os.environ.get('FLAGS_START_PORT') is not None:",
            "            start_port = int(os.environ.get('FLAGS_START_PORT'))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        else:",
            "            free_ports = find_free_ports(len(node_ranks[node_rank]))",
            "        trainer_endpoints.append([\"%s:%d\" % (ip, port) for port in free_ports])",
            "",
            "    return get_mapped_cluster_with_rank_mapping(",
            "        node_ips,",
            "        node_ip,",
            "        trainer_endpoints,",
            "        device_mode,",
            "        node_ranks,",
            "        node_rank_mappings,",
            "    )",
            "",
            "",
            "class ParameterServerLauncher:",
            "    def __init__(self, args, distribute_mode):",
            "        self.args = args",
            "        self.distribute_mode = distribute_mode",
            "        self.with_coordinator = False",
            "        self.server_num = 0",
            "        self.worker_num = 0",
            "        self.heter_worker_num = 0",
            "        self.coordinator_num = 0",
            "",
            "        self.server_endpoints = \"\"",
            "        self.server_endpoints_ips = []",
            "        self.server_endpoints_port = []",
            "",
            "        self.worker_endpoints = \"\"",
            "        self.worker_endpoints_ips = []",
            "        self.worker_endpoints_port = []",
            "",
            "        self.heter_worker_endpoints = \"\"",
            "        self.heter_worker_endpoints_ips = []",
            "        self.heter_worker_endpoints_port = []",
            "",
            "        self.coordinator_endpoints = \"\"",
            "        self.coordinator_endpoints_ips = []",
            "        self.coordinator_endpoints_port = []",
            "",
            "        self.is_local = True",
            "        self.current_node_ip = \"\"",
            "",
            "        self.stage_trainer_num = []",
            "        self.stage_heter_map = {}",
            "        self.stage_list = []",
            "        self.stage_device_map = {}",
            "        self.stage_num = 0",
            "",
            "        self.get_role_endpoints(args)",
            "",
            "    def get_role_endpoints(self, args):",
            "        if args.server_num:",
            "            self.server_num = args.server_num",
            "            if args.servers:",
            "                assert (",
            "                    len(args.servers.split(\",\")) == self.server_num",
            "                ), \"The server_num and servers doesn't match. Expect servers endpoints num epual to server_num, but received servers enpoint num: {} and server_num {}\".format(",
            "                    len(args.servers.split(\",\")), self.server_num",
            "                )",
            "                self.server_endpoints = args.servers",
            "            else:",
            "                ports = get_ports(self.server_num, 0)",
            "                self.server_endpoints = \",\".join(",
            "                    [\"127.0.0.1:\" + str(x) for x in ports]",
            "                )",
            "        else:",
            "            assert (",
            "                args.servers != \"\"",
            "            ), \"The setting of Parameter-Server must has server_num or servers.\"",
            "            self.server_endpoints = args.servers",
            "            self.server_num = len(self.server_endpoints.split(\",\"))",
            "",
            "        # get worker envs",
            "        if args.worker_num:",
            "            self.worker_num = args.worker_num",
            "            if args.workers:",
            "                assert (",
            "                    len(args.workers.split(\",\")) == self.worker_num",
            "                ), \"The worker_num and workers doesn't match. Expect workers endpoints num epual to worker_num, but received workers enpoint num: {} and worker_num {}\".format(",
            "                    len(args.workers.split(\",\")), self.worker_num",
            "                )",
            "",
            "                self.worker_endpoints = args.workers",
            "            else:",
            "                ports = get_ports(self.worker_num, self.server_num)",
            "                self.worker_endpoints = \",\".join(",
            "                    [\"127.0.0.1:\" + str(x) for x in ports]",
            "                )",
            "        else:",
            "            assert (",
            "                args.workers != \"\"",
            "            ), \"The setting of Parameter-Server must has worker_num or workers.\"",
            "            worker_endpoints_ips = [",
            "                x.strip().split(\":\")[0] for x in args.workers.split(\",\")",
            "            ]",
            "            self.worker_num = len(worker_endpoints_ips)",
            "            worker_endpoints_len = [",
            "                len(x.strip().split(\":\")) for x in args.workers.split(\",\")",
            "            ]",
            "",
            "            if 1 in worker_endpoints_len:",
            "                # if no port value in worker_endpoints, will set default port values.",
            "                start_port = 6170",
            "                worker_endpoints_port = range(",
            "                    start_port + self.server_num,",
            "                    start_port + self.server_num + self.worker_num,",
            "                    1,",
            "                )",
            "                # create endpoints str",
            "                worker_endpoints = []",
            "                for i in range(self.worker_num):",
            "                    worker_endpoints.append(",
            "                        \":\".join(",
            "                            (",
            "                                worker_endpoints_ips[i],",
            "                                str(worker_endpoints_port[i]),",
            "                            )",
            "                        )",
            "                    )",
            "                self.worker_endpoints = \",\".join(worker_endpoints)",
            "            else:",
            "                self.worker_endpoints = args.workers",
            "",
            "        # get coordinator envs",
            "        if args.coordinator_num:",
            "            self.with_coordinator = True",
            "            self.coordinator_num = args.coordinator_num",
            "            if args.coordinators:",
            "                assert (",
            "                    len(args.coordinators.split(\",\")) == self.coordinator_num",
            "                ), \"The coordinator_num and coordinators doesn't match. Expect coordinators endpoints num epual to coordinator_num, but received coordinator enpoint num: {} and coordinator_num {}\".format(",
            "                    len(args.coordinators.split(\",\")), self.coordinator_num",
            "                )",
            "",
            "                self.coordinator_endpoints = args.coordinators",
            "            else:",
            "                ports = get_ports(self.coordinator_num, 1)",
            "                self.coordinator_endpoints = \",\".join(",
            "                    [\"127.0.0.1:\" + str(x) for x in ports]",
            "                )",
            "                print(\">>> use default coordinator addr(only one process)\")",
            "",
            "        # get heter worker envs",
            "        if self.distribute_mode == DistributeMode.PS_HETER:",
            "            assert (",
            "                args.heter_devices != \"\"",
            "            ), \"The setting of Parameter-Server heter mode must has heter_devices.\"",
            "            self.stage_device_map[1] = \"cpu\"  # for cpu trainer",
            "            heter_devices_list = args.heter_devices.split(\";\")",
            "            for i in range(len(heter_devices_list)):",
            "                self.stage_device_map[i + 2] = heter_devices_list[i]",
            "",
            "            self.stage_heter_map[1] = self.worker_endpoints",
            "            if args.heter_worker_num:",
            "                self.stage_heter_trainer_num = args.heter_worker_num.split(\";\")",
            "                self.stage_heter_trainer_num = [",
            "                    int(trainer_num)",
            "                    for trainer_num in self.stage_heter_trainer_num",
            "                ]",
            "",
            "                if args.heter_workers:",
            "                    assert len(args.heter_workers.split(\";\")) == len(",
            "                        self.stage_heter_trainer_num",
            "                    ), \"The stage_num and heter_workers doesn't match. Expect heter_workers endpoints stage num epual to heter_worker_num stage, but received heter_workers enpoint stage num: {} and heter_worker_num stage {}\".format(",
            "                        len(args.heter_workers.split(\";\")),",
            "                        len(self.stage_heter_trainer_num),",
            "                    )",
            "                    heter_worker_endpoints_list = args.heter_workers.split(\";\")",
            "                    self.heter_worker_endpoints = \"\"",
            "                    for i in range(len(self.stage_heter_trainer_num)):",
            "                        if self.heter_worker_endpoints != \"\":",
            "                            self.heter_worker_endpoints += \",\"",
            "                        heter_worker_endpoints = heter_worker_endpoints_list[",
            "                            i",
            "                        ].split(\",\")",
            "                        assert (",
            "                            len(heter_worker_endpoints)",
            "                            == self.stage_heter_trainer_num[i]",
            "                        ), \"The heter trainer num in stage {} is not equal in args.heter_worker_num and args.heter_workers\".format(",
            "                            i",
            "                        )",
            "",
            "                        heter_worker_endpoints_ips = [",
            "                            x.strip().split(\":\")[0]",
            "                            for x in heter_worker_endpoints",
            "                        ]",
            "                        heter_worker_endpoints_len = [",
            "                            len(x.strip().split(\":\"))",
            "                            for x in heter_worker_endpoints",
            "                        ]",
            "",
            "                        if 1 in heter_worker_endpoints_len:",
            "                            # if no port value in heter_worker_endpoint, will set default port values.",
            "                            heter_worker_endpoints_port = get_ports(",
            "                                len(heter_worker_endpoints_ips),",
            "                                self.worker_num",
            "                                + self.server_num",
            "                                + self.heter_worker_num,",
            "                            )",
            "                            new_heter_worker_endpoints = []",
            "                            for j in range(len(heter_worker_endpoints_ips)):",
            "                                new_heter_worker_endpoints.append(",
            "                                    \":\".join(",
            "                                        (",
            "                                            heter_worker_endpoints_ips[j],",
            "                                            str(heter_worker_endpoints_port[j]),",
            "                                        )",
            "                                    )",
            "                                )",
            "                            ip_port_list = \",\".join(new_heter_worker_endpoints)",
            "                        else:",
            "                            ip_port_list = \",\".join(heter_worker_endpoints)",
            "",
            "                        self.stage_heter_map[i + 2] = ip_port_list",
            "                        self.stage_list.extend(",
            "                            [i + 2] * len(ip_port_list.split(','))",
            "                        )",
            "",
            "                        self.heter_worker_num += self.stage_heter_trainer_num[i]",
            "                        self.heter_worker_endpoints += ip_port_list",
            "                else:",
            "                    for i in range(len(self.stage_heter_trainer_num)):",
            "                        heter_trainer_num = self.stage_heter_trainer_num[i]",
            "                        ports = get_ports(",
            "                            heter_trainer_num,",
            "                            self.server_num",
            "                            + self.worker_num",
            "                            + self.heter_worker_num,",
            "                        )",
            "                        ip_port_list = \",\".join(",
            "                            [\"127.0.0.1:\" + str(x) for x in ports]",
            "                        )",
            "                        self.stage_heter_map[i + 2] = ip_port_list",
            "                        self.stage_list.extend(",
            "                            [i + 2] * len(ip_port_list.split(','))",
            "                        )",
            "                        self.heter_worker_num += heter_trainer_num",
            "                        if self.heter_worker_endpoints != \"\":",
            "                            self.heter_worker_endpoints += \",\"",
            "                        self.heter_worker_endpoints += ip_port_list",
            "            else:",
            "                assert (",
            "                    args.heter_workers != \"\"",
            "                ), \"The setting of Parameter-Server heter mode must has heter_worker_num or heter_workers.\"",
            "                self.stage_heter_trainer_num = []",
            "                heter_worker_endpoints_list = args.heter_workers.split(\";\")",
            "                self.heter_worker_endpoints = \"\"",
            "                for i in range(len(heter_worker_endpoints_list)):",
            "                    heter_worker_endpoints = heter_worker_endpoints_list[",
            "                        i",
            "                    ].split(\",\")",
            "                    self.stage_heter_trainer_num.append(",
            "                        len(heter_worker_endpoints)",
            "                    )",
            "                    heter_worker_endpoints_ips = [",
            "                        x.strip().split(\":\")[0] for x in heter_worker_endpoints",
            "                    ]",
            "                    heter_worker_endpoints_len = [",
            "                        len(x.strip().split(\":\"))",
            "                        for x in heter_worker_endpoints",
            "                    ]",
            "                    if 1 in heter_worker_endpoints_len:",
            "                        # if no port value in heter_worker_endpoint, will set default port values.",
            "                        heter_worker_endpoints_port = get_ports(",
            "                            len(heter_worker_endpoints_ips),",
            "                            self.worker_num",
            "                            + self.server_num",
            "                            + self.heter_worker_num,",
            "                        )",
            "",
            "                        new_heter_worker_endpoints = []",
            "                        for j in range(len(heter_worker_endpoints_ips)):",
            "                            new_heter_worker_endpoints.append(",
            "                                \":\".join(",
            "                                    (",
            "                                        heter_worker_endpoints_ips[j],",
            "                                        str(heter_worker_endpoints_port[j]),",
            "                                    )",
            "                                )",
            "                            )",
            "                        ip_port_list = \",\".join(new_heter_worker_endpoints)",
            "                    else:",
            "                        ip_port_list = \",\".join(heter_worker_endpoints)",
            "",
            "                    self.stage_heter_map[i + 2] = ip_port_list",
            "                    self.stage_list.extend(",
            "                        [i + 2] * len(ip_port_list.split(','))",
            "                    )",
            "",
            "                    self.heter_worker_num += self.stage_heter_trainer_num[-1]",
            "                    if self.heter_worker_endpoints != \"\":",
            "                        self.heter_worker_endpoints += \",\"",
            "                    self.heter_worker_endpoints += ip_port_list",
            "",
            "            self.stage_trainer_num = [",
            "                self.worker_num",
            "            ] + self.stage_heter_trainer_num",
            "            self.stage_num = len(self.stage_trainer_num)",
            "",
            "        # get http_port",
            "        if args.http_port:",
            "            http_port = [args.http_port]",
            "        else:",
            "            http_port = get_ports(",
            "                1, self.server_num + self.worker_num + self.heter_worker_num",
            "            )",
            "        http_ip = self.server_endpoints.split(\",\")[0].split(\":\")[0]",
            "        self.http_port = http_ip + \":\" + str(http_port[0])",
            "",
            "        # check local or user define",
            "        self.server_endpoints_ips = [",
            "            x.strip().split(\":\")[0] for x in self.server_endpoints.split(\",\")",
            "        ]",
            "        self.worker_endpoints_ips = [",
            "            x.strip().split(\":\")[0] for x in self.worker_endpoints.split(\",\")",
            "        ]",
            "",
            "        if self.with_coordinator:",
            "            self.coordinator_endpoints_ips = [",
            "                x.strip().split(\":\")[0]",
            "                for x in self.coordinator_endpoints.split(\",\")",
            "            ]",
            "            self.coordinator_endpoints_port = [",
            "                x.strip().split(\":\")[1]",
            "                for x in self.coordinator_endpoints.split(\",\")",
            "            ]",
            "",
            "        self.server_endpoints_port = [",
            "            x.strip().split(\":\")[1] for x in self.server_endpoints.split(\",\")",
            "        ]",
            "        self.worker_endpoints_port = [",
            "            x.strip().split(\":\")[1] for x in self.worker_endpoints.split(\",\")",
            "        ]",
            "        self.node_ips = []",
            "        for ip in self.server_endpoints_ips:",
            "            if ip not in self.node_ips:",
            "                self.node_ips.append(ip)",
            "        for ip in self.worker_endpoints_ips:",
            "            if ip not in self.node_ips:",
            "                self.node_ips.append(ip)",
            "",
            "        if self.distribute_mode == DistributeMode.PS_HETER:",
            "            self.heter_worker_endpoints_ips = [",
            "                x.strip().split(\":\")[0]",
            "                for x in self.heter_worker_endpoints.split(\",\")",
            "            ]",
            "            self.heter_worker_endpoints_port = [",
            "                x.strip().split(\":\")[1]",
            "                for x in self.heter_worker_endpoints.split(\",\")",
            "            ]",
            "            for ip in self.heter_worker_endpoints_ips:",
            "                if ip not in self.node_ips:",
            "                    self.node_ips.append(ip)",
            "",
            "        if len(set(self.node_ips)) == 1:",
            "            self.is_local = True",
            "            self.current_node_ip = self.node_ips[0]",
            "        else:",
            "            self.is_local = False",
            "            pod_ip = os.getenv(\"POD_IP\", None)",
            "            if pod_ip is None:",
            "                _, self.current_node_ip = get_host_name_ip()",
            "            else:",
            "                self.current_node_ip = pod_ip",
            "            if not self.distribute_mode == DistributeMode.PS_HETER:",
            "                assert self.current_node_ip in self.node_ips, (",
            "                    \"Can't find your local ip {%s} in args.servers and args.workers ips: {%s}\"",
            "                    % (self.current_node_ip, self.node_ips)",
            "                )",
            "        if self.current_node_ip in self.node_ips:",
            "            self.node_rank = self.node_ips.index(self.current_node_ip)",
            "            logger.debug(",
            "                \"parsed from args: node_ips:{} current_node_ip:{} node_rank:{}\".format(",
            "                    self.node_ips, self.current_node_ip, self.node_rank",
            "                )",
            "            )",
            "",
            "    def start_ps(self):",
            "        if self.current_node_ip not in self.node_ips:",
            "            return",
            "        cluster = Cluster(hdfs=None)",
            "        server_rank = 0",
            "        worker_rank = 0",
            "        heter_worker_rank = 0",
            "        coordinator_rank = 0",
            "        for node_rank, ip in enumerate(self.node_ips):",
            "            pod = Pod()",
            "            pod.rank = node_rank",
            "            pod.addr = ip",
            "            for i in range(len(self.server_endpoints_ips)):",
            "                if ip == self.server_endpoints_ips[i]:",
            "                    server = Trainer()",
            "                    server.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.server_endpoints_port[i],",
            "                    )",
            "                    server.rank = server_rank",
            "                    server_rank += 1",
            "                    pod.servers.append(server)",
            "            for j in range(len(self.worker_endpoints_ips)):",
            "                if ip == self.worker_endpoints_ips[j]:",
            "                    worker = Trainer()",
            "                    worker.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.worker_endpoints_port[j],",
            "                    )",
            "                    worker.rank = worker_rank",
            "                    worker.stage = 1",
            "                    worker_rank += 1",
            "                    pod.workers.append(worker)",
            "            for m in range(len(self.coordinator_endpoints_ips)):",
            "                if ip == self.coordinator_endpoints_ips[m]:",
            "                    coordinator = Trainer()",
            "                    coordinator.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.coordinator_endpoints_port[m],",
            "                    )",
            "                    coordinator.rank = coordinator_rank",
            "                    coordinator.stage = 1",
            "                    coordinator_rank += 1",
            "                    pod.coordinators.append(coordinator)",
            "",
            "            for k in range(len(self.heter_worker_endpoints_ips)):",
            "                if ip == self.heter_worker_endpoints_ips[k]:",
            "                    heter_worker = Trainer()",
            "                    heter_worker.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.heter_worker_endpoints_port[k],",
            "                    )",
            "                    heter_worker.rank = heter_worker_rank",
            "                    heter_worker.stage = self.stage_list[k]",
            "                    heter_worker_rank += 1",
            "                    pod.heter_workers.append(heter_worker)",
            "",
            "            cluster.pods.append(pod)",
            "",
            "        pod = cluster.pods[self.node_rank]",
            "        self.gloo_rendezvous_dir = tempfile.mkdtemp()",
            "",
            "        # 3. subproces start",
            "        self.procs = {",
            "            \"worker\": [],",
            "            \"coordinator\": [],",
            "            \"server\": [],",
            "            \"heter_worker\": [],",
            "        }",
            "        self.cmds = {",
            "            \"worker\": [],",
            "            \"coordinator\": [],",
            "            \"server\": [],",
            "            \"heter_worker\": [],",
            "        }",
            "        self.log_fns = {",
            "            \"worker\": [],",
            "            \"coordinator\": [],",
            "            \"server\": [],",
            "            \"heter_worker\": [],",
            "        }",
            "",
            "        self.start_pod_server(self.args, pod)",
            "        self.start_pod_worker(self.args, pod)",
            "        if self.with_coordinator:",
            "            self.start_pod_coordinator(self.args, pod)",
            "        if self.distribute_mode == DistributeMode.PS_HETER:",
            "            self.start_pod_heter_worker(self.args, pod)",
            "",
            "        logger.info(",
            "            \"Please check servers, workers, coordinator and heter_worker logs in {}/workerlog.*, {}/serverlog.* , {}/coordinatorlog.*, and {}/heterlog.*\".format(",
            "                self.args.log_dir,",
            "                self.args.log_dir,",
            "                self.args.log_dir,",
            "                self.args.log_dir,",
            "            )",
            "        )",
            "",
            "        # 4. wait for finish training",
            "        if len(self.procs[\"worker\"]) > 0:",
            "            # if node has worker procs",
            "            # only wait worker to finish here",
            "            for i, proc in enumerate(self.procs[\"worker\"]):",
            "                self.procs[\"worker\"][i].proc.wait()",
            "                if len(self.log_fns[\"worker\"]) > 0:",
            "                    self.log_fns[\"worker\"][i].close()",
            "            logger.info(",
            "                \"all workers exit, going to finish parameter server and heter_worker.\"",
            "            )",
            "            if len(self.procs[\"heter_worker\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"heter_worker\"]):",
            "                    self.log_fns[\"heter_worker\"][i].close()",
            "                    self.procs[\"heter_worker\"][i].proc.terminate()",
            "                logger.info(\"all heter_worker are killed\")",
            "",
            "            if len(self.procs[\"server\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"server\"]):",
            "                    self.log_fns[\"server\"][i].close()",
            "                    self.procs[\"server\"][i].proc.terminate()",
            "                logger.info(\"all parameter server are killed\")",
            "",
            "            if len(self.procs[\"coordinator\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"coordinator\"]):",
            "                    self.log_fns[\"coordinator\"][i].close()",
            "                    self.procs[\"coordinator\"][i].proc.terminate()",
            "                logger.info(\"all coordinators are killed\")",
            "",
            "        else:",
            "            # if node has not worker procs",
            "            # blocking training process",
            "            if len(self.procs[\"server\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"server\"]):",
            "                    self.procs[\"server\"][i].proc.wait()",
            "",
            "            if len(self.procs[\"heter_worker\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"heter_worker\"]):",
            "                    self.procs[\"heter_worker\"][i].proc.wait()",
            "",
            "        if os.path.exists(self.gloo_rendezvous_dir):",
            "            shutil.rmtree(self.gloo_rendezvous_dir)",
            "",
            "    def start_pod_server(self, args, pod):",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "        for idx, cur_server in enumerate(pod.servers):",
            "            if self.distribute_mode == DistributeMode.PS_HETER:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"PADDLE_ALL_HETER_TRAINER_IP_PORT_LIST\": self.heter_worker_endpoints,",
            "                    \"PADDLE_PORT\": cur_server.endpoint.split(\":\")[1],",
            "                    \"TRAINING_ROLE\": \"PSERVER\",",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"POD_IP\": cur_server.endpoint.split(\":\")[0],",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "            else:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"PADDLE_PORT\": cur_server.endpoint.split(\":\")[1],",
            "                    \"TRAINING_ROLE\": \"PSERVER\",",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"POD_IP\": cur_server.endpoint.split(\":\")[0],",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "            current_env.update(proc_env)",
            "",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"server\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local server start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.servers),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.system(f\"mkdir -p {args.log_dir}\")",
            "                fn = open(\"%s/serverlog.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"server\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_server.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"server\"].append(tp)",
            "",
            "    def start_pod_worker(self, args, pod):",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "",
            "        heter_device_num = 0",
            "        device_list = []",
            "        if framework.core.is_compiled_with_cuda():",
            "            device_list = get_gpus(args.gpus)",
            "            heter_device_num = len(device_list)",
            "        elif framework.core.is_compiled_with_xpu():",
            "            heter_device_num = framework.core.get_xpu_device_count()",
            "            device_list = [str(x) for x in range(0, heter_device_num)]",
            "",
            "        for idx, cur_worker in enumerate(pod.workers):",
            "            device_id = (",
            "                \"0\"",
            "                if heter_device_num == 0",
            "                else str(device_list[(idx) % heter_device_num])",
            "            )",
            "            if self.distribute_mode == DistributeMode.PS_HETER:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"PADDLE_STAGE_TRAINERS_NUM\": str(self.stage_trainer_num),",
            "                    \"STAGE_ID\": \"1\",",
            "                    \"STAGE_NUM\": str(self.stage_num),",
            "                    \"PADDLE_PREVIOUS_HETER_TRAINER_IP_PORT_LIST\": \"\",",
            "                    \"PADDLE_NEXT_HETER_TRAINER_IP_PORT_LIST\": self.stage_heter_map[",
            "                        2",
            "                    ],",
            "                    \"PADDLE_ALL_HETER_TRAINER_IP_PORT_LIST\": self.heter_worker_endpoints,",
            "                    \"HETER_DEVICE_TYPE\": self.stage_device_map[1],",
            "                    \"TRAINING_ROLE\": \"TRAINER\",",
            "                    \"POD_IP\": cur_worker.endpoint.split(\":\")[0],",
            "                    \"PADDLE_PORT\": cur_worker.endpoint.split(\":\")[1],",
            "                    \"PADDLE_TRAINER_ID\": str(cur_worker.rank),",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"FLAGS_selected_gpus\": \"0\",",
            "                    \"FLAGS_selected_xpus\": \"0\",",
            "                    \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                    \"XPU_VISIBLE_DEVICES\": device_id,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "            else:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"TRAINING_ROLE\": \"TRAINER\",",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"POD_IP\": cur_worker.endpoint.split(\":\")[0],",
            "                    \"PADDLE_PORT\": cur_worker.endpoint.split(\":\")[1],",
            "                    \"PADDLE_TRAINER_ID\": str(cur_worker.rank),",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"FLAGS_selected_gpus\": \"0\",",
            "                    \"FLAGS_selected_xpus\": \"0\",",
            "                    \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                    \"XPU_VISIBLE_DEVICES\": device_id,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "",
            "            current_env.update(proc_env)",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"worker\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local worker start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.workers),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.system(f\"mkdir -p {args.log_dir}\")",
            "                fn = open(\"%s/workerlog.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"worker\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_worker.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"worker\"].append(tp)",
            "",
            "    def start_pod_coordinator(self, args, pod):",
            "        print(\">>> entering start_pod_coordinator\")",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "",
            "        for idx, cur_coordinator in enumerate(pod.coordinators):",
            "            device_id = \"0\"",
            "            proc_env = {",
            "                \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                \"PADDLE_COORDINATOR_NUM\": str(self.coordinator_num),",
            "                \"TRAINING_ROLE\": \"COORDINATOR\",",
            "                \"POD_IP\": cur_coordinator.endpoint.split(\":\")[0],",
            "                \"PADDLE_PORT\": cur_coordinator.endpoint.split(\":\")[1],",
            "                \"PADDLE_TRAINER_ID\": str(cur_coordinator.rank),",
            "                \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                \"FLAGS_selected_gpus\": \"0\",",
            "                \"FLAGS_selected_xpus\": \"0\",",
            "                \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                \"XPU_VISIBLE_DEVICES\": device_id,",
            "                \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "            }",
            "",
            "            current_env.update(proc_env)",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"coordinator\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local coordinator start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.coordinators),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.system(f\"mkdir -p {args.log_dir}\")",
            "                fn = open(\"%s/coordinator.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"coordinator\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_coordinator.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"coordinator\"].append(tp)",
            "",
            "    def start_pod_heter_worker(self, args, pod):",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "",
            "        heter_device_num = 0",
            "        device_list = []",
            "        if framework.core.is_compiled_with_cuda():",
            "            device_list = get_gpus(args.gpus)",
            "            heter_device_num = len(device_list)",
            "        elif framework.core.is_compiled_with_xpu():",
            "            heter_device_num = framework.core.get_xpu_device_count()",
            "            device_list = [str(x) for x in range(0, heter_device_num)]",
            "",
            "        for idx, cur_heter_worker in enumerate(pod.heter_workers):",
            "            device_id = (",
            "                \"0\"",
            "                if heter_device_num == 0",
            "                else str(device_list[(idx) % heter_device_num])",
            "            )",
            "            stage_id = cur_heter_worker.stage",
            "            proc_env = {",
            "                \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                \"PADDLE_NEXT_HETER_TRAINER_IP_PORT_LIST\": self.stage_heter_map[",
            "                    stage_id + 1",
            "                ]",
            "                if stage_id <= self.stage_num - 1",
            "                else \"\",",
            "                \"PADDLE_PREVIOUS_HETER_TRAINER_IP_PORT_LIST\": self.stage_heter_map[",
            "                    stage_id - 1",
            "                ],",
            "                \"PADDLE_ALL_HETER_TRAINER_IP_PORT_LIST\": self.heter_worker_endpoints,",
            "                \"HETER_DEVICE_TYPE\": self.stage_device_map[stage_id],",
            "                \"STAGE_ID\": str(stage_id),",
            "                \"STAGE_NUM\": str(self.stage_num),",
            "                \"PADDLE_PORT\": cur_heter_worker.endpoint.split(\":\")[1],",
            "                \"TRAINING_ROLE\": \"HETER_TRAINER\",",
            "                \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                \"PADDLE_STAGE_TRAINERS_NUM\": str(self.stage_trainer_num),",
            "                \"POD_IP\": cur_heter_worker.endpoint.split(\":\")[0],",
            "                \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                \"FLAGS_selected_gpus\": \"0\",",
            "                \"FLAGS_selected_xpus\": \"0\",",
            "                \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                \"XPU_VISIBLE_DEVICES\": device_id,",
            "                \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "            }",
            "            current_env.update(proc_env)",
            "",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"heter_worker\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local heter_worker start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.heter_workers),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.system(f\"mkdir -p {args.log_dir}\")",
            "                fn = open(\"%s/heterlog.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"heter_worker\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_heter_worker.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"heter_worker\"].append(tp)",
            "",
            "",
            "def check_backend(backend):",
            "    if backend not in [",
            "        'nccl',",
            "        'gloo',",
            "        'bkcl',",
            "        'cncl',",
            "        'auto',",
            "        'hccl',",
            "        'heter',",
            "        'xccl',",
            "    ]:",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"backend argument can only be one of \"",
            "            \"'nccl', 'gloo', 'bkcl', 'auto', 'hccl', 'heter', 'xccl' \"",
            "            \"but got %s\" % backend",
            "        )",
            "",
            "    if backend == 'nccl' and not framework.core.is_compiled_with_cuda():",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"your paddle is not compiled with cuda but you assign 'nccl' as backend.\"",
            "        )",
            "",
            "    if backend == 'bkcl' and not framework.core.is_compiled_with_xpu():",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"your paddle is not compiled with xpu but you assign 'bkcl' as backend.\"",
            "        )",
            "",
            "    if backend == 'hccl' and not framework.core.is_compiled_with_npu():",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"your paddle is not compiled with npu but you assign 'hccl' as backend.\"",
            "        )",
            "",
            "",
            "def block_windows_and_macos(backend):",
            "    if backend != 'gloo':",
            "        return",
            "    if utils.OS_NAME.startswith('darwin'):  # MACOS , block",
            "        raise ValueError(",
            "            \"You are going to using gloo on macos, but currently is not supported\"",
            "        )",
            "    if utils.IS_WINDOWS:  # MACOS , block",
            "        raise ValueError(",
            "            \"You are going to using gloo on windows, but currently is not supported\"",
            "        )",
            "",
            "",
            "def get_backend_by_compile_flag():",
            "    if framework.core.is_compiled_with_cuda():",
            "        return 'nccl'",
            "",
            "    if framework.core.is_compiled_with_xpu():",
            "        return 'bkcl'",
            "",
            "    if framework.core.is_compiled_with_npu():",
            "        return 'hccl'",
            "",
            "    return 'gloo'"
        ],
        "afterPatchFile": [
            "# Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import copy",
            "import json",
            "import logging",
            "import multiprocessing",
            "import os",
            "import shutil",
            "import signal",
            "import socket",
            "import struct",
            "import subprocess",
            "import sys",
            "import tempfile",
            "import time",
            "from contextlib import closing",
            "from distutils.util import strtobool",
            "",
            "import paddle.utils.cpp_extension.extension_utils as utils",
            "from paddle import framework",
            "",
            "logger = logging.getLogger(\"root\")",
            "logger.propagate = False",
            "",
            "",
            "class DistributeMode:",
            "    \"\"\"",
            "    There are various mode for fleetrun, each of them is designed for different model.",
            "    \"\"\"",
            "",
            "    COLLECTIVE = 0",
            "    PS = 1",
            "    PS_HETER = 2",
            "",
            "",
            "class DeviceMode:",
            "    \"\"\"",
            "    Training devices type",
            "    \"\"\"",
            "",
            "    UNKNOWN = -1",
            "    CPU = 0",
            "    GPU = 1",
            "    KUNLUN = 2",
            "    XPU = 2",
            "    ASCEND_NPU = 3",
            "    UNKNOWN = 3",
            "",
            "",
            "class Cluster:",
            "    def __init__(self, hdfs):",
            "        self.job_server = None",
            "        self.pods = []",
            "        self.hdfs = None",
            "        self.job_stage_flag = None",
            "",
            "    def __str__(self):",
            "        return \"job_server:{} pods:{} job_stage_flag:{} hdfs:{}\".format(",
            "            self.job_server,",
            "            [str(pod) for pod in self.pods],",
            "            self.job_stage_flag,",
            "            self.hdfs,",
            "        )",
            "",
            "    def __eq__(self, cluster):",
            "        if len(self.pods) != len(cluster.pods):",
            "            return False",
            "",
            "        for a, b in zip(self.pods, cluster.pods):",
            "            if a != b:",
            "                return False",
            "",
            "        if self.job_stage_flag != cluster.job_stage_flag:",
            "            return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, cluster):",
            "        return not self.__eq__(cluster)",
            "",
            "    def update_pods(self, cluster):",
            "        self.pods = copy.copy(cluster.pods)",
            "",
            "    def trainers_nranks(self):",
            "        return len(self.trainers_endpoints())",
            "",
            "    def pods_nranks(self):",
            "        return len(self.pods)",
            "",
            "    def trainers_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            for t in pod.trainers:",
            "                r.append(t.endpoint)",
            "        return r",
            "",
            "    def world_device_ids(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            for t in pod.trainers:",
            "                str_accelerators = [str(acc) for acc in t.accelerators]",
            "                r.append(str_accelerators)",
            "        return r",
            "",
            "    def pods_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            ep = f\"{pod.addr}:{pod.port}\"",
            "            assert (",
            "                pod.port is not None and pod.addr is not None",
            "            ), f\"{ep} not a valid endpoint\"",
            "            r.append(ep)",
            "        return r",
            "",
            "    def get_pod_by_id(self, pod_id):",
            "        for pod in self.pods:",
            "            if str(pod_id) == str(pod.id):",
            "                return pod",
            "",
            "        return None",
            "",
            "",
            "class JobServer:",
            "    def __init__(self):",
            "        self.endpoint = None",
            "",
            "    def __str__(self):",
            "        return f\"{self.endpoint}\"",
            "",
            "    def __eq__(self, j):",
            "        return self.endpint == j.endpoint",
            "",
            "    def __ne__(self, j):",
            "        return not self == j",
            "",
            "",
            "class Trainer:",
            "    def __init__(self):",
            "        self.accelerators = []",
            "        self.endpoint = None",
            "        self.rank = None",
            "        self.stage = None",
            "",
            "    def __str__(self):",
            "        return \"accelerator:{} endpoint:{} rank:{}\".format(",
            "            self.accelerators, self.endpoint, self.rank",
            "        )",
            "",
            "    def __eq__(self, t):",
            "        if len(self.accelerators) != len(t.accelerators):",
            "            return False",
            "",
            "        if self.endpoint != t.endpoint or self.rank != t.rank:",
            "            return False",
            "",
            "        for a, b in zip(self.accelerators, t.accelerators):",
            "            if a != b:",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, t):",
            "        return not self == t",
            "",
            "    def rank(self):",
            "        return self.rank",
            "",
            "",
            "class Pod:",
            "    def __init__(self):",
            "        self.rank = None",
            "        self.id = None",
            "        self.addr = None",
            "        self.port = None",
            "        self.trainers = []",
            "        self.servers = []",
            "        self.workers = []",
            "        self.coordinators = []",
            "        self.heter_workers = []",
            "        self.accelerators = []",
            "        self.device_mode = None",
            "",
            "    def __str__(self):",
            "        return \"rank:{} id:{} addr:{} port:{} visible_accelerator:{} trainers:{} servers:{} \\",
            "            workers:{} heter_workers:{} coordinators:{}\".format(",
            "            self.rank,",
            "            self.id,",
            "            self.addr,",
            "            self.port,",
            "            self.accelerators,",
            "            [str(t) for t in self.trainers],",
            "            [str(s) for s in self.servers],",
            "            [str(w) for w in self.workers],",
            "            [str(h) for h in self.heter_workers],",
            "            [str(c) for c in self.coordinators],",
            "        )",
            "",
            "    def __eq__(self, pod):",
            "        if (",
            "            self.rank != pod.rank",
            "            or self.id != pod.id",
            "            or self.addr != pod.addr",
            "            or self.port != pod.port",
            "        ):",
            "            logger.debug(f\"pod {self} != {pod}\")",
            "            return False",
            "",
            "        if len(self.trainers) != len(pod.trainers):",
            "            logger.debug(f\"trainers {self.trainers} != {pod.trainers}\")",
            "            return False",
            "",
            "        for i in range(len(self.trainers)):",
            "            if self.trainers[i] != pod.trainers[i]:",
            "                logger.debug(f\"trainer {self.trainers[i]} != {pod.trainers[i]}\")",
            "                return False",
            "",
            "        if len(self.servers) != len(pod.servers):",
            "            logger.debug(f\"servers {self.servers} != {pod.servers}\")",
            "            return False",
            "",
            "        for i in range(len(self.servers)):",
            "            if self.servers[i] != pod.servers[i]:",
            "                logger.debug(f\"servers {self.servers[i]} != {pod.servers[i]}\")",
            "                return False",
            "",
            "        if len(self.workers) != len(pod.workers):",
            "            logger.debug(f\"workers {self.workers} != {pod.workers}\")",
            "            return False",
            "",
            "        for i in range(len(self.workers)):",
            "            if self.workers[i] != pod.workers[i]:",
            "                logger.debug(f\"workers {self.workers[i]} != {pod.workers[i]}\")",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, pod):",
            "        return not self == pod",
            "",
            "    def parse_response(self, res_pods):",
            "        pass",
            "",
            "    def rank(self):",
            "        return self.rank",
            "",
            "    def get_visible_accelerators(self):",
            "        r = \"\"",
            "        for g in self.accelerators:",
            "            r += f\"{g},\"",
            "",
            "        assert r != \"\", f\"this pod {self} can't see any accelerators\"",
            "",
            "        r = r[:-1]",
            "        return r",
            "",
            "",
            "def get_logger(log_level=20, name=\"root\"):",
            "    logger = logging.getLogger(name)",
            "    logger.setLevel(log_level)",
            "",
            "    log_handler = logging.StreamHandler()",
            "    log_format = logging.Formatter(",
            "        '%(levelname)s %(asctime)s %(filename)s:%(lineno)d] %(message)s'",
            "    )",
            "    log_handler.setFormatter(log_format)",
            "    logger.addHandler(log_handler)",
            "",
            "    return logger",
            "",
            "",
            "def get_cluster(",
            "    node_ips, node_ip, trainer_endpoints, device_mode, devices_per_proc",
            "):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    cluster = Cluster(hdfs=None)",
            "    trainer_rank = 0",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        pod.device_mode = device_mode",
            "",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "        # when use paddlecloud, endpoints may > devices_per_proc(user_defined)",
            "        assert len(cur_node_endpoints) >= len(",
            "            devices_per_proc",
            "        ), \"current trainer_endpoints size should be greater equal than acclerators size.\"",
            "        for i in range(len(devices_per_proc)):",
            "            trainer = Trainer()",
            "            if (",
            "                device_mode == DeviceMode.GPU",
            "                or device_mode == DeviceMode.ASCEND_NPU",
            "            ):",
            "                if isinstance(devices_per_proc[i], (list, tuple)):",
            "                    trainer.accelerators.extend(devices_per_proc[i])",
            "                    pod.accelerators.extend(devices_per_proc[i])",
            "                else:",
            "                    trainer.accelerators.append(devices_per_proc[i])",
            "                    pod.accelerators.append(devices_per_proc[i])",
            "            elif device_mode == DeviceMode.XPU:",
            "                if isinstance(devices_per_proc[i], (list, tuple)):",
            "                    trainer.accelerators.extend(devices_per_proc[i])",
            "                else:",
            "                    trainer.accelerators.append(devices_per_proc[i])",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = trainer_rank",
            "            trainer_rank += 1",
            "",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def terminate_local_procs(procs):",
            "    # try to terminate process by group, this happend in multiprocess senario in user process",
            "    if os.name != 'nt':",
            "        for p in procs:",
            "            if p.proc.poll() is None:",
            "                os.killpg(os.getpgid(p.proc.pid), signal.SIGTERM)",
            "                if p.log_fn:",
            "                    p.log_fn.close()",
            "                logger.info(f\"terminate process group gid:{p.proc.pid}\")",
            "",
            "        time.sleep(1)",
            "",
            "    for p in procs:",
            "        if p.proc.poll() is None:",
            "            p.proc.terminate()",
            "            if p.log_fn:",
            "                p.log_fn.close()",
            "            logger.debug(f\"terminate process id:{p.proc.pid}\")",
            "",
            "    # wait all process terminiated",
            "    time.sleep(3)",
            "    for step in range(0, 50):",
            "        alive = False",
            "        for p in procs:",
            "            if p.proc.poll() is None:  # not termniate",
            "                os.kill(p.proc.pid, signal.SIGKILL)",
            "                alive = True",
            "",
            "        if not alive:",
            "            logger.info(\"terminate all the procs\")",
            "            return",
            "",
            "        time.sleep(3)",
            "",
            "    logger.fatal(\"can't kill all process and exit\")",
            "    sys.exit(1)",
            "",
            "",
            "def get_host_name_ip():",
            "    try:",
            "        host_name = socket.gethostname()",
            "        host_ip = socket.gethostbyname(host_name)",
            "        return host_name, host_ip",
            "    except:",
            "        return None",
            "",
            "",
            "def add_arguments(argname, type, default, help, argparser, **kwargs):",
            "    \"\"\"Add argparse's argument.",
            "    Usage:",
            "    .. code-block:: python",
            "        parser = argparse.ArgumentParser()",
            "        add_argument(\"name\", str, \"Jonh\", \"User name.\", parser)",
            "        args = parser.parse_args()",
            "    \"\"\"",
            "    type = strtobool if type == bool else type",
            "    argparser.add_argument(",
            "        \"--\" + argname,",
            "        default=default,",
            "        type=type,",
            "        help=help + ' Default: %(default)s.',",
            "        **kwargs,",
            "    )",
            "",
            "",
            "def find_free_ports(num):",
            "    def __free_port():",
            "        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:",
            "            # Note(wangxi): Close the connection with a TCP RST instead",
            "            # of a TCP FIN, to avoid time_wait state.",
            "            s.setsockopt(",
            "                socket.SOL_SOCKET, socket.SO_LINGER, struct.pack('ii', 1, 0)",
            "            )",
            "            s.bind(('', 0))",
            "            return s.getsockname()[1]",
            "",
            "    port_set = set()",
            "    step = 0",
            "    while True:",
            "        port = __free_port()",
            "        if port not in port_set:",
            "            port_set.add(port)",
            "",
            "        if len(port_set) >= num:",
            "            return port_set",
            "",
            "        step += 1",
            "        if step > 400:",
            "            print(",
            "                \"can't find avilable port and use the specified static port now!\"",
            "            )",
            "            return None",
            "",
            "    return None",
            "",
            "",
            "def get_ports(num, offset):",
            "    if os.environ.get('FLAGS_START_PORT') is None:",
            "        ports = find_free_ports(num)",
            "        if ports is not None:",
            "            ports = list(ports)",
            "    else:",
            "        start_port = int(os.environ.get('FLAGS_START_PORT'))",
            "        ports = range(start_port + offset, start_port + offset + num, 1)",
            "    return ports",
            "",
            "",
            "def pretty_print_envs(envs, header=None):",
            "    spacing = 2",
            "    max_k = 40",
            "    max_v = 45",
            "",
            "    for k, v in envs.items():",
            "        max_k = max(max_k, len(k))",
            "",
            "    h_format = \"    \" + \"|{{:>{}s}}{}{{:^{}s}}|\\n\".format(",
            "        max_k, \" \" * spacing, max_v",
            "    )",
            "    l_format = \"    \" + f\"|{{:>{max_k}s}}{{}}{{:^{max_v}s}}|\\n\"",
            "    length = max_k + max_v + spacing",
            "",
            "    border = \"    +\" + \"\".join([\"=\"] * length) + \"+\"",
            "    line = \"    +\" + \"\".join([\"-\"] * length) + \"+\"",
            "",
            "    draws = \"\"",
            "    draws += border + \"\\n\"",
            "",
            "    if header:",
            "        draws += h_format.format(header[0], header[1])",
            "    else:",
            "        draws += h_format.format(\"fleetrun Distributed Envs\", \"Value\")",
            "",
            "    draws += line + \"\\n\"",
            "",
            "    for k, v in envs.items():",
            "        if isinstance(v, str) and len(v) >= max_v:",
            "            str_v = \"... \" + v[-41:]",
            "        else:",
            "            str_v = v",
            "",
            "        draws += l_format.format(k, \" \" * spacing, str(str_v))",
            "",
            "    draws += border",
            "",
            "    _str = f\"\\n{draws}\\n\"",
            "    return _str",
            "",
            "",
            "class TrainerProc:",
            "    def __init__(self):",
            "        self.proc = None",
            "        self.log_fn = None",
            "        self.log_offset = None",
            "        self.rank = None",
            "        self.local_rank = None",
            "        self.cmd = None",
            "",
            "",
            "_run_with_coverage = False",
            "",
            "",
            "def run_with_coverage(*args):",
            "    global _run_with_coverage",
            "    assert len(args) <= 1, f\"len(args) {len(args)} should <= 1\"",
            "    if len(args) == 1:",
            "        assert isinstance(args[0], bool)",
            "        _run_with_coverage = args[0]",
            "    return _run_with_coverage",
            "",
            "",
            "def start_local_trainers(",
            "    cluster, pod, training_script, training_script_args, log_dir=None, envs=None",
            "):",
            "",
            "    if envs is None:",
            "        current_env = copy.copy(os.environ.copy())",
            "    else:",
            "        current_env = copy.copy(envs)",
            "",
            "    # paddle broadcast ncclUniqueId use socket, and",
            "    # proxy maybe make trainers unreachable, so delete them.",
            "    # if we set them to \"\", grpc will log error message \"bad uri\"",
            "    # so just delete them.",
            "    current_env.pop(\"http_proxy\", None)",
            "    current_env.pop(\"https_proxy\", None)",
            "",
            "    ids = cluster.world_device_ids()",
            "    res = [':'.join(ele) for ele in ids]",
            "    procs = []",
            "    for idx, t in enumerate(pod.trainers):",
            "        proc_env = {",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % t.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % t.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "            \"PADDLE_RANK_IN_NODE\": str(idx),",
            "            \"PADDLE_LOCAL_DEVICE_IDS\": \",\".join(",
            "                [str(acc) for acc in t.accelerators]",
            "            ),",
            "            \"PADDLE_WORLD_DEVICE_IDS\": \",\".join(res),",
            "        }",
            "",
            "        # The following three environnement variables are used for auto mapping",
            "        if current_env.get(\"PADDLE_CLUSTER_TOPO_PATH\", None) is not None:",
            "            proc_env[\"PADDLE_CLUSTER_TOPO_PATH\"] = current_env[",
            "                \"PADDLE_CLUSTER_TOPO_PATH\"",
            "            ]",
            "        if current_env.get(\"PADDLE_RANK_MAPPING_PATH\", None) is not None:",
            "            proc_env[\"PADDLE_RANK_MAPPING_PATH\"] = current_env[",
            "                \"PADDLE_RANK_MAPPING_PATH\"",
            "            ]",
            "        if current_env.get(\"PADDLE_ENABLE_AUTO_MAPPING\", None) is not None:",
            "            proc_env[\"PADDLE_ENABLE_AUTO_MAPPING\"] = current_env[",
            "                \"PADDLE_ENABLE_AUTO_MAPPING\"",
            "            ]",
            "",
            "        if len(t.accelerators) > 0 and pod.device_mode == DeviceMode.GPU:",
            "            proc_env[\"FLAGS_selected_gpus\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "",
            "        elif (",
            "            len(t.accelerators) > 0 and pod.device_mode == DeviceMode.ASCEND_NPU",
            "        ):",
            "            proc_env[\"FLAGS_selected_npus\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "",
            "        if len(t.accelerators) > 0:",
            "            proc_env[\"FLAGS_selected_accelerators\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "        # to do: same code style in future",
            "        if framework.core.is_compiled_with_xpu() and len(t.accelerators) > 0:",
            "            proc_env[\"FLAGS_selected_xpus\"] = \"%s\" % \",\".join(",
            "                [str(g) for g in t.accelerators]",
            "            )",
            "",
            "        current_env.update(proc_env)",
            "",
            "        coverage_args = []",
            "        if (",
            "            run_with_coverage()",
            "            or os.environ.get(\"WITH_COVERAGE\", \"OFF\") == \"ON\"",
            "        ):",
            "            coverage_args = [\"-m\", \"coverage\", \"run\", \"--branch\", \"-p\"]",
            "        cmd = (",
            "            [sys.executable, \"-u\"]",
            "            + coverage_args",
            "            + [training_script]",
            "            + training_script_args",
            "        )",
            "",
            "        logger.debug(f\"start trainer proc{cmd}  env:{current_env}\")",
            "",
            "        if idx == 0:",
            "            logger.info(",
            "                \"Local start {} processes. First process distributed \"",
            "                \"environment info (Only For Debug): {}\".format(",
            "                    len(pod.trainers),",
            "                    pretty_print_envs(proc_env, (\"Distributed Envs\", \"Value\")),",
            "                )",
            "            )",
            "            logger.info(",
            "                \"details about PADDLE_TRAINER_ENDPOINTS can be found in \"",
            "                \"{}/endpoints.log, and detail running logs maybe found in \"",
            "                \"{}/workerlog.0\".format(log_dir, log_dir)",
            "            )",
            "        fn = None",
            "        pre_fn = None if os.name == 'nt' else os.setsid",
            "        if log_dir is not None:",
            "            os.makedirs(log_dir, exist_ok=True)",
            "            if os.path.exists(\"%s/endpoints.log\" % log_dir):",
            "                os.system(f\"rm -f {log_dir}/endpoints.log\")",
            "            with open(\"%s/endpoints.log\" % log_dir, \"w\") as f:",
            "                f.write(\"PADDLE_TRAINER_ENDPOINTS: \\n\")",
            "                f.write(\"\\n\".join(cluster.trainers_endpoints()))",
            "            if (",
            "                current_env.get(\"PADDLE_ENABLE_AUTO_MAPPING\") is not None",
            "                and current_env.get(\"PADDLE_NEED_RANK_MAPPING\").lower()",
            "                == \"true\"",
            "            ):",
            "                fn = open(\"%s/prelaunchlog.%d\" % (log_dir, idx), \"a\")",
            "            else:",
            "                fn = open(\"%s/workerlog.%d\" % (log_dir, idx), \"a\")",
            "            proc = subprocess.Popen(",
            "                cmd, env=current_env, stdout=fn, stderr=fn, preexec_fn=pre_fn",
            "            )",
            "        else:",
            "            proc = subprocess.Popen(cmd, env=current_env, preexec_fn=pre_fn)",
            "",
            "        tp = TrainerProc()",
            "        tp.proc = proc",
            "        tp.rank = t.rank",
            "        tp.local_rank = idx",
            "        tp.log_fn = fn",
            "        tp.log_offset = fn.tell() if fn else None",
            "        tp.cmd = cmd",
            "",
            "        procs.append(tp)",
            "",
            "    return procs",
            "",
            "",
            "def pull_worker_log(tp):",
            "    if tp.log_fn:",
            "        with open(tp.log_fn.name, 'r') as fin:",
            "            fin.seek(tp.log_offset, 0)",
            "            for line in fin:",
            "                try:",
            "                    sys.stdout.write(line)",
            "                except UnicodeEncodeError:",
            "                    sys.stdout.write(",
            "                        'UnicodeEncodeError occurs at this line. '",
            "                        'Please refer to the original log file \"%s\"\\n'",
            "                        % tp.log_fn.name",
            "                    )",
            "            tp.log_offset = fin.tell()",
            "",
            "",
            "def watch_local_trainers(procs, nranks):",
            "    try:",
            "        error = False",
            "        error_rank = []",
            "        # wait all process finish or one error",
            "        alive = False",
            "        for p in procs:",
            "            if p.log_fn and p.local_rank == 0:",
            "                pull_worker_log(p)",
            "",
            "            ret = p.proc.poll()",
            "            if ret is None:",
            "                alive = True",
            "            elif ret != 0:",
            "                error = True",
            "                error_rank.append(p.rank)",
            "",
            "        if error:",
            "            terminate_local_procs(procs)",
            "            sys.exit(1)",
            "",
            "    except KeyboardInterrupt:",
            "        logger.warning(\"KeyboardInterrupt, exit\")",
            "        terminate_local_procs(procs)",
            "        return",
            "    except SystemExit:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        raise",
            "    except:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        return",
            "",
            "    return alive",
            "",
            "",
            "def get_gpus(gpus):",
            "    if gpus is None:",
            "        gpus_num = framework.core.get_cuda_device_count()",
            "        res_gpus = [str(x) for x in range(0, gpus_num)]",
            "    else:",
            "        cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\")",
            "        if cuda_visible_devices is None or cuda_visible_devices == \"\":",
            "            res_gpus = [x.strip() for x in gpus.split(',')]",
            "        else:",
            "            # change gpus into relative values",
            "            # e.g. CUDA_VISIBLE_DEVICES=4,5,6,7; args.gpus=4,5,6,7;",
            "            # therefore gpus=0,1,2,3",
            "            cuda_visible_devices_list = cuda_visible_devices.split(',')",
            "            for x in gpus.split(','):",
            "                assert x in cuda_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your gpus %s in CUDA_VISIBLE_DEVICES[%s].\"",
            "                    % (x, cuda_visible_devices)",
            "                )",
            "            res_gpus = [",
            "                cuda_visible_devices_list.index(x.strip())",
            "                for x in gpus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_gpus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"CUDA_VISIBLE_DEVICES:{}\".format(",
            "                    gpus, res_gpus, cuda_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return res_gpus",
            "",
            "",
            "def get_xpus(xpus):",
            "    if xpus is None:",
            "        xpus_num = framework.core.get_xpu_device_count()",
            "        res_xpus = [str(x) for x in range(0, xpus_num)]",
            "    else:",
            "        xpu_visible_devices = os.getenv(\"XPU_VISIBLE_DEVICES\")",
            "        if xpu_visible_devices is None or xpu_visible_devices == \"\":",
            "            res_xpus = [x.strip() for x in xpus.split(',')]",
            "        else:",
            "            # change xpus into relative values",
            "            # e.g. XPU_VISIBLE_DEVICES=4,5,6,7; args.xpus=4,5,6,7;",
            "            # therefore xpus=0,1,2,3",
            "            xpu_visible_devices_list = xpu_visible_devices.split(',')",
            "            for x in xpus.split(','):",
            "                assert x in xpu_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your xpus {} in XPU_VISIBLE_DEVICES[{}].\".format(",
            "                        x,",
            "                        xpu_visible_devices,",
            "                    )",
            "                )",
            "            res_xpus = [",
            "                xpu_visible_devices_list.index(x.strip())",
            "                for x in xpus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_xpus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"XPU_VISIBLE_DEVICES:{}\".format(",
            "                    xpus, res_xpus, xpu_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return res_xpus",
            "",
            "",
            "def get_npus(npus):",
            "    if npus is None:",
            "        npus_num = framework.core.get_npu_device_count()",
            "        res_npus = [str(x) for x in range(0, npus_num)]",
            "    else:",
            "        npu_visible_devices = os.getenv(\"ASCEND_VISIBLE_DEVICES\")",
            "        if npu_visible_devices is None or npu_visible_devices == \"\":",
            "            res_npus = [x.strip() for x in npus.split(',')]",
            "        else:",
            "            # change npus into relative values",
            "            # e.g. ASCEND_VISIBLE_DEVICES=4,5,6,7; args.npus=4,5,6,7;",
            "            # therefore npus=0,1,2,3",
            "            npu_visible_devices_list = npu_visible_devices.split(',')",
            "            for x in npus.split(','):",
            "                assert x in npu_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your npus %s in ASCEND_VISIBLE_DEVICES[%s].\"",
            "                    % (x, npu_visible_devices)",
            "                )",
            "            res_npus = [",
            "                npu_visible_devices_list.index(x.strip())",
            "                for x in npus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_npus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"ASCEND_VISIBLE_DEVICES:{}\".format(",
            "                    npus, res_npus, npu_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return res_npus",
            "",
            "",
            "def get_device_mode(backend):",
            "    if backend == 'heter':",
            "        if (",
            "            framework.core.is_compiled_with_cuda()",
            "            and framework.core.get_cuda_device_count() > 0",
            "        ):",
            "            print(\"launch train in heter mode with GPU device.\")",
            "            return DeviceMode.GPU",
            "        if (",
            "            framework.core.is_compiled_with_xpu()",
            "            and framework.core.get_xpu_device_count() > 0",
            "        ):",
            "            print(\"launch train in heter mode with XPU device.\")",
            "            return DeviceMode.XPU",
            "        if (",
            "            framework.core.is_compiled_with_npu()",
            "            and framework.core.get_npu_device_count() > 0",
            "        ):",
            "            print(\"launch train in heter mode with NPU device.\")",
            "            return DeviceMode.ASCEND_NPU",
            "",
            "    if backend == 'hccl' and framework.core.get_npu_device_count() > 0:",
            "        print(\"launch train in ascend npu mode!\")",
            "        return DeviceMode.ASCEND_NPU",
            "",
            "    if backend == 'nccl' and framework.core.get_cuda_device_count() > 0:",
            "        print(\"launch train in GPU mode!\")",
            "        return DeviceMode.GPU",
            "",
            "    if backend == 'bkcl' and framework.core.get_xpu_device_count() > 0:",
            "        print(\"launch train in XPU mode\")",
            "        return DeviceMode.XPU",
            "",
            "    if backend == 'gloo':",
            "        print(\"launch train in CPU mode\")",
            "        return DeviceMode.CPU",
            "",
            "    raise RuntimeError(\"Don't supported devices\")",
            "",
            "",
            "def get_device_proc_info(args):",
            "    # device_mode",
            "    device_mode = get_device_mode(args.backend)",
            "",
            "    # devices",
            "    devices_per_proc = []",
            "    if device_mode == DeviceMode.GPU:",
            "        gpus = get_gpus(args.gpus)",
            "        if args.nproc_per_node is not None:",
            "            assert (",
            "                len(gpus) % int(args.nproc_per_node)",
            "            ) == 0, \"gpus' number:{} mod args.nproc_per_node:{} must == 0\".format(",
            "                len(gpus), args.nproc_per_node",
            "            )",
            "",
            "            n = int(len(gpus) / int(args.nproc_per_node))",
            "            devices_per_proc = [gpus[i : i + n] for i in range(0, len(gpus), n)]",
            "        else:",
            "            devices_per_proc = gpus",
            "    elif device_mode == DeviceMode.ASCEND_NPU:",
            "        npus = get_npus(args.npus)",
            "        if args.nproc_per_node is not None:",
            "            assert (",
            "                len(npus) % int(args.nproc_per_node)",
            "            ) == 0, \"npus' number:{} mod args.nproc_per_node:{} must == 0\".format(",
            "                len(npus), args.nproc_per_node",
            "            )",
            "",
            "            n = int(len(npus) / int(args.nproc_per_node))",
            "            devices_per_proc = [npus[i : i + n] for i in range(0, len(npus), n)]",
            "        else:",
            "            devices_per_proc = npus",
            "    elif device_mode == DeviceMode.XPU:",
            "        xpus = get_xpus(args.xpus)",
            "        if args.nproc_per_node is not None:",
            "            assert (",
            "                len(xpus) % int(args.nproc_per_node)",
            "            ) == 0, \"xpus' number:{} mod args.nproc_per_node:{} must == 0\".format(",
            "                len(xpus), args.nproc_per_node",
            "            )",
            "",
            "            n = int(len(xpus) / int(args.nproc_per_node))",
            "            devices_per_proc = [xpus[i : i + n] for i in range(0, len(xpus), n)]",
            "        else:",
            "            devices_per_proc = xpus",
            "    elif device_mode == DeviceMode.CPU:",
            "        if hasattr(args, \"paddle_cpuonly\") and args.nproc_per_node is None:",
            "            # NOTE (xiongkun03) set it to cpu core number",
            "            args.nproc_per_node = multiprocessing.cpu_count()",
            "        if args.nproc_per_node is None:",
            "            devices_per_proc = [0]",
            "        else:",
            "            devices_per_proc = list(range(0, args.nproc_per_node))",
            "    else:",
            "        raise AssertionError(",
            "            \"Can't support device_mode:{}, support only cpu|gpu|xpu now.\".format(",
            "                device_mode",
            "            )",
            "        )",
            "",
            "    return (device_mode, devices_per_proc)",
            "",
            "",
            "def direct_start(args):",
            "    # run ps-cpu mode on paddlecloud, using given envs",
            "    cmd = [",
            "        sys.executable,",
            "        \"-u\",",
            "        args.training_script,",
            "    ] + args.training_script_args",
            "    proc = subprocess.Popen(cmd)",
            "    proc.wait()",
            "    return",
            "",
            "",
            "def get_custom_endpoints(origin_endpoints, offset=0):",
            "    \"\"\"",
            "    origin_endpoint: ip:port",
            "    user_define_endpoint: ip:(port+offset)",
            "    \"\"\"",
            "    assert origin_endpoints is not None",
            "    paddle_user_define_endpoints_list = []",
            "    for ip_port in origin_endpoints.split(\",\"):",
            "        ip = ip_port.split(\":\")[0]",
            "        port = ip_port.split(\":\")[1]",
            "        new_port = int(port) + offset",
            "        paddle_user_define_endpoints_list.append(\":\".join((ip, str(new_port))))",
            "    paddle_user_define_endpoints = \",\".join(paddle_user_define_endpoints_list)",
            "    return paddle_user_define_endpoints",
            "",
            "",
            "# def cloud_ps_heter_env_set(args):",
            "#    environs = {}",
            "#",
            "#    paddle_trainer_endpoints = os.getenv(\"TRAINER_IP_PORT_LIST\", \"\")",
            "#    assert paddle_trainer_endpoints != None",
            "#",
            "#    paddle_pserver_endpoints = os.getenv(\"PSERVER_IP_PORT_LIST\", \"\")",
            "#    assert paddle_pserver_endpoints != None",
            "#",
            "#    # hard code for paddlecloud custom-framework",
            "#    avilable_ports = os.getenv(\"TRAINER_PORTS\", \"\").split(\",\")",
            "#    assert len(",
            "#        avilable_ports",
            "#    ) >= 2, \"set paddle_ports_num >= 2 in config.ini for paddlecloud job submit\"",
            "#",
            "#    # hard code for paddlecloud custom-framework",
            "#    trainers_num = len(paddle_pserver_endpoints.split(\",\"))",
            "#    assert trainers_num != 0",
            "#    environs[\"PADDLE_TRAINERS_NUM\"] = trainers_num",
            "#    environs[\"TRAINERS_NUM\"] = trainers_num",
            "#",
            "#    # hard code for paddlecloud custom-framework",
            "#    environs[\"PADDLE_HETER_TRAINER_IP_PORT_LIST\"] = paddle_trainer_endpoints",
            "#    environs[\"PADDLE_PSERVERS_IP_PORT_LIST\"] = paddle_pserver_endpoints",
            "#    environs[\"PADDLE_TRAINER_ENDPOINTS\"] = get_custom_endpoints(",
            "#        paddle_pserver_endpoints, 1)",
            "#    heter_worker_num = len(paddle_trainer_endpoints.split(\",\"))",
            "#    if (args.heter_worker_num != None) and (",
            "#            heter_worker_num != args.heter_worker_num):",
            "#        warnings.warn(",
            "#            \"Your fleetrun setting: heter_worker_num is {}, but we find {} device can be used, this setting has been changed.\".",
            "#            format(args.heter_worker_num, heter_worker_num))",
            "#        args.heter_worker_num = heter_worker_num",
            "#",
            "#    for k, v in environs.items():",
            "#        os.environ[k] = str(v)",
            "#    logger.info(\"Set heter parameter server env: {}\".format(",
            "#        pretty_print_envs(environs)))",
            "",
            "",
            "def get_mapped_cluster_without_rank_mapping(",
            "    node_ips, node_ip, trainer_endpoints, device_mode, node_ranks",
            "):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "    cluster = Cluster(hdfs=None)",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        pod.device_mode = device_mode",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "",
            "        # choose rank from global mapped ranks and set it to the trainer.",
            "        ranks_per_node = node_ranks[node_rank]",
            "        assert len(ranks_per_node) == 1",
            "        for i in range(len(ranks_per_node)):",
            "            trainer = Trainer()",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = ranks_per_node[i]",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def get_mapped_cluster_from_args_without_rank_mapping(args, device_mode):",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "    gpus_num = framework.core.get_cuda_device_count()",
            "",
            "    # parse ip-ranks json file",
            "    cluster_topo = None",
            "    with open(args.cluster_topo_path, \"r\") as json_file:",
            "        cluster_topo = json.load(json_file)",
            "",
            "    node_ips = []",
            "    node_ranks = []",
            "    for idx, cur_cluster_topo in enumerate(cluster_topo[\"machines\"]):",
            "        node_ips.append(cur_cluster_topo['addr'])",
            "        node_ranks.append([idx])",
            "",
            "    if len(node_ips) == 1:",
            "        node_ip = node_ips[0]",
            "    else:",
            "        if args.host:",
            "            node_ip = args.host",
            "        else:",
            "            _, node_ip = get_host_name_ip()",
            "",
            "    assert (",
            "        node_ip in node_ips",
            "    ), f\"Can't find your local ip {{{node_ip}}} in node_ips: {{{node_ips}}}\"",
            "    node_rank = node_ips.index(node_ip)",
            "",
            "    assert len(node_ranks) == len(",
            "        node_ips",
            "    ), \"ranks length should be equal to ips length.\"",
            "",
            "    logger.debug(",
            "        \"parsed from args: node_ips:{} node_ip:{} \"",
            "        \"node_rank:{} node_ranks:{}\".format(",
            "            node_ips, node_ip, node_rank, node_ranks[node_rank]",
            "        )",
            "    )",
            "",
            "    # NOTE: there are different number of global mapped ranks on each node.",
            "    free_ports = []",
            "    trainer_endpoints = []",
            "    for ip in node_ips:",
            "        node_rank = node_ips.index(ip)",
            "        if os.environ.get('PADDLE_PORT') is not None:",
            "            start_port = int(os.getenv(\"PADDLE_PORT\", \"\"))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        elif os.environ.get('FLAGS_START_PORT') is not None:",
            "            start_port = int(os.environ.get('FLAGS_START_PORT'))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        else:",
            "            free_ports = find_free_ports(len(node_ranks[node_rank]))",
            "        trainer_endpoints.append([\"%s:%d\" % (ip, port) for port in free_ports])",
            "",
            "    return get_mapped_cluster_without_rank_mapping(",
            "        node_ips, node_ip, trainer_endpoints, device_mode, node_ranks",
            "    )",
            "",
            "",
            "def get_mapped_cluster_with_rank_mapping(",
            "    node_ips,",
            "    node_ip,",
            "    trainer_endpoints,",
            "    device_mode,",
            "    node_ranks,",
            "    node_rank_mappings,",
            "):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "",
            "    def get_relative_gpu_id(gpu_id):",
            "        cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\")",
            "        if cuda_visible_devices is None or cuda_visible_devices == \"\":",
            "            return gpu_id",
            "        else:",
            "            cuda_visible_devices_list = cuda_visible_devices.split(',')",
            "            relative_id = cuda_visible_devices_list.index(str(gpu_id))",
            "            logger.info(",
            "                \"Change gpu id from {} to {} based on CUDA_VISIBLE_DEVICES {}\".format(",
            "                    gpu_id, relative_id, cuda_visible_devices_list",
            "                )",
            "            )",
            "            return relative_id",
            "",
            "    cluster = Cluster(hdfs=None)",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        pod.device_mode = device_mode",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "",
            "        # choose rank from global mapped ranks and set it to the trainer.",
            "        ranks_per_node = node_ranks[node_rank]",
            "        cur_node_rank_mapping = node_rank_mappings[node_rank]",
            "        for i in range(len(ranks_per_node)):",
            "            trainer = Trainer()",
            "            local_device_ids = cur_node_rank_mapping[\"ranks\"][",
            "                str(ranks_per_node[i])",
            "            ]",
            "            assert (",
            "                len(local_device_ids) == 1",
            "            ), \"Only support one process to one device mapping\"",
            "            trainer.accelerators.append(",
            "                get_relative_gpu_id(local_device_ids[0])",
            "            )",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = ranks_per_node[i]",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def get_mapped_cluster_from_args_with_rank_mapping(args, device_mode):",
            "    assert (",
            "        device_mode == DeviceMode.GPU",
            "    ), \"Only support get mapped cluster for gpu now.\"",
            "    gpus_num = framework.core.get_cuda_device_count()",
            "",
            "    # parse ip-ranks json file",
            "    rank_mapping_path = args.rank_mapping_path or os.getenv(",
            "        \"PADDLE_RANK_MAPPING_PATH\"",
            "    )",
            "    rank_mapping = None",
            "    with open(rank_mapping_path, \"r\") as json_file:",
            "        rank_mapping = json.load(json_file)",
            "    # reset PADDLE_RANK_MAPPING_PATH env",
            "    os.environ[\"PADDLE_RANK_MAPPING_PATH\"] = \"\"",
            "",
            "    node_ips = []",
            "    node_ranks = []",
            "    node_rank_mappings = []",
            "    for cur_rank_mapping in rank_mapping:",
            "        node_ips.append(cur_rank_mapping['addr'])",
            "        cur_node_rank_list = [",
            "            int(i) for i in list(cur_rank_mapping['ranks'].keys())",
            "        ]",
            "        cur_node_rank_list.sort()",
            "        node_ranks.append(cur_node_rank_list)",
            "        node_rank_mappings.append(cur_rank_mapping)",
            "",
            "    if len(node_ips) == 1:",
            "        node_ip = node_ips[0]",
            "    else:",
            "        if args.host:",
            "            node_ip = args.host",
            "        else:",
            "            _, node_ip = get_host_name_ip()",
            "",
            "    assert (",
            "        node_ip in node_ips",
            "    ), f\"Can't find your local ip {{{node_ip}}} in node_ips: {{{node_ips}}}\"",
            "    node_rank = node_ips.index(node_ip)",
            "",
            "    assert (",
            "        len(node_ranks[node_rank]) <= gpus_num",
            "    ), \"number of ranks mapped to one node should not exceed the avaiable ones.\"",
            "    assert len(node_ranks) == len(",
            "        node_ips",
            "    ), \"ranks length should be equal to ips length.\"",
            "",
            "    logger.debug(",
            "        \"parsed from args: node_ips:{} node_ip:{} \"",
            "        \"node_rank:{} node_ranks:{}\".format(",
            "            node_ips, node_ip, node_rank, node_ranks[node_rank]",
            "        )",
            "    )",
            "",
            "    # NOTE: there are different number of global mapped ranks on each node.",
            "    free_ports = []",
            "    trainer_endpoints = []",
            "    for ip in node_ips:",
            "        node_rank = node_ips.index(ip)",
            "        if os.environ.get('PADDLE_PORT') is not None:",
            "            start_port = int(os.getenv(\"PADDLE_PORT\", \"\"))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        elif os.environ.get('FLAGS_START_PORT') is not None:",
            "            start_port = int(os.environ.get('FLAGS_START_PORT'))",
            "            free_ports = list(",
            "                range(start_port, start_port + len(node_ranks[node_rank]))",
            "            )",
            "        else:",
            "            free_ports = find_free_ports(len(node_ranks[node_rank]))",
            "        trainer_endpoints.append([\"%s:%d\" % (ip, port) for port in free_ports])",
            "",
            "    return get_mapped_cluster_with_rank_mapping(",
            "        node_ips,",
            "        node_ip,",
            "        trainer_endpoints,",
            "        device_mode,",
            "        node_ranks,",
            "        node_rank_mappings,",
            "    )",
            "",
            "",
            "class ParameterServerLauncher:",
            "    def __init__(self, args, distribute_mode):",
            "        self.args = args",
            "        self.distribute_mode = distribute_mode",
            "        self.with_coordinator = False",
            "        self.server_num = 0",
            "        self.worker_num = 0",
            "        self.heter_worker_num = 0",
            "        self.coordinator_num = 0",
            "",
            "        self.server_endpoints = \"\"",
            "        self.server_endpoints_ips = []",
            "        self.server_endpoints_port = []",
            "",
            "        self.worker_endpoints = \"\"",
            "        self.worker_endpoints_ips = []",
            "        self.worker_endpoints_port = []",
            "",
            "        self.heter_worker_endpoints = \"\"",
            "        self.heter_worker_endpoints_ips = []",
            "        self.heter_worker_endpoints_port = []",
            "",
            "        self.coordinator_endpoints = \"\"",
            "        self.coordinator_endpoints_ips = []",
            "        self.coordinator_endpoints_port = []",
            "",
            "        self.is_local = True",
            "        self.current_node_ip = \"\"",
            "",
            "        self.stage_trainer_num = []",
            "        self.stage_heter_map = {}",
            "        self.stage_list = []",
            "        self.stage_device_map = {}",
            "        self.stage_num = 0",
            "",
            "        self.get_role_endpoints(args)",
            "",
            "    def get_role_endpoints(self, args):",
            "        if args.server_num:",
            "            self.server_num = args.server_num",
            "            if args.servers:",
            "                assert (",
            "                    len(args.servers.split(\",\")) == self.server_num",
            "                ), \"The server_num and servers doesn't match. Expect servers endpoints num epual to server_num, but received servers enpoint num: {} and server_num {}\".format(",
            "                    len(args.servers.split(\",\")), self.server_num",
            "                )",
            "                self.server_endpoints = args.servers",
            "            else:",
            "                ports = get_ports(self.server_num, 0)",
            "                self.server_endpoints = \",\".join(",
            "                    [\"127.0.0.1:\" + str(x) for x in ports]",
            "                )",
            "        else:",
            "            assert (",
            "                args.servers != \"\"",
            "            ), \"The setting of Parameter-Server must has server_num or servers.\"",
            "            self.server_endpoints = args.servers",
            "            self.server_num = len(self.server_endpoints.split(\",\"))",
            "",
            "        # get worker envs",
            "        if args.worker_num:",
            "            self.worker_num = args.worker_num",
            "            if args.workers:",
            "                assert (",
            "                    len(args.workers.split(\",\")) == self.worker_num",
            "                ), \"The worker_num and workers doesn't match. Expect workers endpoints num epual to worker_num, but received workers enpoint num: {} and worker_num {}\".format(",
            "                    len(args.workers.split(\",\")), self.worker_num",
            "                )",
            "",
            "                self.worker_endpoints = args.workers",
            "            else:",
            "                ports = get_ports(self.worker_num, self.server_num)",
            "                self.worker_endpoints = \",\".join(",
            "                    [\"127.0.0.1:\" + str(x) for x in ports]",
            "                )",
            "        else:",
            "            assert (",
            "                args.workers != \"\"",
            "            ), \"The setting of Parameter-Server must has worker_num or workers.\"",
            "            worker_endpoints_ips = [",
            "                x.strip().split(\":\")[0] for x in args.workers.split(\",\")",
            "            ]",
            "            self.worker_num = len(worker_endpoints_ips)",
            "            worker_endpoints_len = [",
            "                len(x.strip().split(\":\")) for x in args.workers.split(\",\")",
            "            ]",
            "",
            "            if 1 in worker_endpoints_len:",
            "                # if no port value in worker_endpoints, will set default port values.",
            "                start_port = 6170",
            "                worker_endpoints_port = range(",
            "                    start_port + self.server_num,",
            "                    start_port + self.server_num + self.worker_num,",
            "                    1,",
            "                )",
            "                # create endpoints str",
            "                worker_endpoints = []",
            "                for i in range(self.worker_num):",
            "                    worker_endpoints.append(",
            "                        \":\".join(",
            "                            (",
            "                                worker_endpoints_ips[i],",
            "                                str(worker_endpoints_port[i]),",
            "                            )",
            "                        )",
            "                    )",
            "                self.worker_endpoints = \",\".join(worker_endpoints)",
            "            else:",
            "                self.worker_endpoints = args.workers",
            "",
            "        # get coordinator envs",
            "        if args.coordinator_num:",
            "            self.with_coordinator = True",
            "            self.coordinator_num = args.coordinator_num",
            "            if args.coordinators:",
            "                assert (",
            "                    len(args.coordinators.split(\",\")) == self.coordinator_num",
            "                ), \"The coordinator_num and coordinators doesn't match. Expect coordinators endpoints num epual to coordinator_num, but received coordinator enpoint num: {} and coordinator_num {}\".format(",
            "                    len(args.coordinators.split(\",\")), self.coordinator_num",
            "                )",
            "",
            "                self.coordinator_endpoints = args.coordinators",
            "            else:",
            "                ports = get_ports(self.coordinator_num, 1)",
            "                self.coordinator_endpoints = \",\".join(",
            "                    [\"127.0.0.1:\" + str(x) for x in ports]",
            "                )",
            "                print(\">>> use default coordinator addr(only one process)\")",
            "",
            "        # get heter worker envs",
            "        if self.distribute_mode == DistributeMode.PS_HETER:",
            "            assert (",
            "                args.heter_devices != \"\"",
            "            ), \"The setting of Parameter-Server heter mode must has heter_devices.\"",
            "            self.stage_device_map[1] = \"cpu\"  # for cpu trainer",
            "            heter_devices_list = args.heter_devices.split(\";\")",
            "            for i in range(len(heter_devices_list)):",
            "                self.stage_device_map[i + 2] = heter_devices_list[i]",
            "",
            "            self.stage_heter_map[1] = self.worker_endpoints",
            "            if args.heter_worker_num:",
            "                self.stage_heter_trainer_num = args.heter_worker_num.split(\";\")",
            "                self.stage_heter_trainer_num = [",
            "                    int(trainer_num)",
            "                    for trainer_num in self.stage_heter_trainer_num",
            "                ]",
            "",
            "                if args.heter_workers:",
            "                    assert len(args.heter_workers.split(\";\")) == len(",
            "                        self.stage_heter_trainer_num",
            "                    ), \"The stage_num and heter_workers doesn't match. Expect heter_workers endpoints stage num epual to heter_worker_num stage, but received heter_workers enpoint stage num: {} and heter_worker_num stage {}\".format(",
            "                        len(args.heter_workers.split(\";\")),",
            "                        len(self.stage_heter_trainer_num),",
            "                    )",
            "                    heter_worker_endpoints_list = args.heter_workers.split(\";\")",
            "                    self.heter_worker_endpoints = \"\"",
            "                    for i in range(len(self.stage_heter_trainer_num)):",
            "                        if self.heter_worker_endpoints != \"\":",
            "                            self.heter_worker_endpoints += \",\"",
            "                        heter_worker_endpoints = heter_worker_endpoints_list[",
            "                            i",
            "                        ].split(\",\")",
            "                        assert (",
            "                            len(heter_worker_endpoints)",
            "                            == self.stage_heter_trainer_num[i]",
            "                        ), \"The heter trainer num in stage {} is not equal in args.heter_worker_num and args.heter_workers\".format(",
            "                            i",
            "                        )",
            "",
            "                        heter_worker_endpoints_ips = [",
            "                            x.strip().split(\":\")[0]",
            "                            for x in heter_worker_endpoints",
            "                        ]",
            "                        heter_worker_endpoints_len = [",
            "                            len(x.strip().split(\":\"))",
            "                            for x in heter_worker_endpoints",
            "                        ]",
            "",
            "                        if 1 in heter_worker_endpoints_len:",
            "                            # if no port value in heter_worker_endpoint, will set default port values.",
            "                            heter_worker_endpoints_port = get_ports(",
            "                                len(heter_worker_endpoints_ips),",
            "                                self.worker_num",
            "                                + self.server_num",
            "                                + self.heter_worker_num,",
            "                            )",
            "                            new_heter_worker_endpoints = []",
            "                            for j in range(len(heter_worker_endpoints_ips)):",
            "                                new_heter_worker_endpoints.append(",
            "                                    \":\".join(",
            "                                        (",
            "                                            heter_worker_endpoints_ips[j],",
            "                                            str(heter_worker_endpoints_port[j]),",
            "                                        )",
            "                                    )",
            "                                )",
            "                            ip_port_list = \",\".join(new_heter_worker_endpoints)",
            "                        else:",
            "                            ip_port_list = \",\".join(heter_worker_endpoints)",
            "",
            "                        self.stage_heter_map[i + 2] = ip_port_list",
            "                        self.stage_list.extend(",
            "                            [i + 2] * len(ip_port_list.split(','))",
            "                        )",
            "",
            "                        self.heter_worker_num += self.stage_heter_trainer_num[i]",
            "                        self.heter_worker_endpoints += ip_port_list",
            "                else:",
            "                    for i in range(len(self.stage_heter_trainer_num)):",
            "                        heter_trainer_num = self.stage_heter_trainer_num[i]",
            "                        ports = get_ports(",
            "                            heter_trainer_num,",
            "                            self.server_num",
            "                            + self.worker_num",
            "                            + self.heter_worker_num,",
            "                        )",
            "                        ip_port_list = \",\".join(",
            "                            [\"127.0.0.1:\" + str(x) for x in ports]",
            "                        )",
            "                        self.stage_heter_map[i + 2] = ip_port_list",
            "                        self.stage_list.extend(",
            "                            [i + 2] * len(ip_port_list.split(','))",
            "                        )",
            "                        self.heter_worker_num += heter_trainer_num",
            "                        if self.heter_worker_endpoints != \"\":",
            "                            self.heter_worker_endpoints += \",\"",
            "                        self.heter_worker_endpoints += ip_port_list",
            "            else:",
            "                assert (",
            "                    args.heter_workers != \"\"",
            "                ), \"The setting of Parameter-Server heter mode must has heter_worker_num or heter_workers.\"",
            "                self.stage_heter_trainer_num = []",
            "                heter_worker_endpoints_list = args.heter_workers.split(\";\")",
            "                self.heter_worker_endpoints = \"\"",
            "                for i in range(len(heter_worker_endpoints_list)):",
            "                    heter_worker_endpoints = heter_worker_endpoints_list[",
            "                        i",
            "                    ].split(\",\")",
            "                    self.stage_heter_trainer_num.append(",
            "                        len(heter_worker_endpoints)",
            "                    )",
            "                    heter_worker_endpoints_ips = [",
            "                        x.strip().split(\":\")[0] for x in heter_worker_endpoints",
            "                    ]",
            "                    heter_worker_endpoints_len = [",
            "                        len(x.strip().split(\":\"))",
            "                        for x in heter_worker_endpoints",
            "                    ]",
            "                    if 1 in heter_worker_endpoints_len:",
            "                        # if no port value in heter_worker_endpoint, will set default port values.",
            "                        heter_worker_endpoints_port = get_ports(",
            "                            len(heter_worker_endpoints_ips),",
            "                            self.worker_num",
            "                            + self.server_num",
            "                            + self.heter_worker_num,",
            "                        )",
            "",
            "                        new_heter_worker_endpoints = []",
            "                        for j in range(len(heter_worker_endpoints_ips)):",
            "                            new_heter_worker_endpoints.append(",
            "                                \":\".join(",
            "                                    (",
            "                                        heter_worker_endpoints_ips[j],",
            "                                        str(heter_worker_endpoints_port[j]),",
            "                                    )",
            "                                )",
            "                            )",
            "                        ip_port_list = \",\".join(new_heter_worker_endpoints)",
            "                    else:",
            "                        ip_port_list = \",\".join(heter_worker_endpoints)",
            "",
            "                    self.stage_heter_map[i + 2] = ip_port_list",
            "                    self.stage_list.extend(",
            "                        [i + 2] * len(ip_port_list.split(','))",
            "                    )",
            "",
            "                    self.heter_worker_num += self.stage_heter_trainer_num[-1]",
            "                    if self.heter_worker_endpoints != \"\":",
            "                        self.heter_worker_endpoints += \",\"",
            "                    self.heter_worker_endpoints += ip_port_list",
            "",
            "            self.stage_trainer_num = [",
            "                self.worker_num",
            "            ] + self.stage_heter_trainer_num",
            "            self.stage_num = len(self.stage_trainer_num)",
            "",
            "        # get http_port",
            "        if args.http_port:",
            "            http_port = [args.http_port]",
            "        else:",
            "            http_port = get_ports(",
            "                1, self.server_num + self.worker_num + self.heter_worker_num",
            "            )",
            "        http_ip = self.server_endpoints.split(\",\")[0].split(\":\")[0]",
            "        self.http_port = http_ip + \":\" + str(http_port[0])",
            "",
            "        # check local or user define",
            "        self.server_endpoints_ips = [",
            "            x.strip().split(\":\")[0] for x in self.server_endpoints.split(\",\")",
            "        ]",
            "        self.worker_endpoints_ips = [",
            "            x.strip().split(\":\")[0] for x in self.worker_endpoints.split(\",\")",
            "        ]",
            "",
            "        if self.with_coordinator:",
            "            self.coordinator_endpoints_ips = [",
            "                x.strip().split(\":\")[0]",
            "                for x in self.coordinator_endpoints.split(\",\")",
            "            ]",
            "            self.coordinator_endpoints_port = [",
            "                x.strip().split(\":\")[1]",
            "                for x in self.coordinator_endpoints.split(\",\")",
            "            ]",
            "",
            "        self.server_endpoints_port = [",
            "            x.strip().split(\":\")[1] for x in self.server_endpoints.split(\",\")",
            "        ]",
            "        self.worker_endpoints_port = [",
            "            x.strip().split(\":\")[1] for x in self.worker_endpoints.split(\",\")",
            "        ]",
            "        self.node_ips = []",
            "        for ip in self.server_endpoints_ips:",
            "            if ip not in self.node_ips:",
            "                self.node_ips.append(ip)",
            "        for ip in self.worker_endpoints_ips:",
            "            if ip not in self.node_ips:",
            "                self.node_ips.append(ip)",
            "",
            "        if self.distribute_mode == DistributeMode.PS_HETER:",
            "            self.heter_worker_endpoints_ips = [",
            "                x.strip().split(\":\")[0]",
            "                for x in self.heter_worker_endpoints.split(\",\")",
            "            ]",
            "            self.heter_worker_endpoints_port = [",
            "                x.strip().split(\":\")[1]",
            "                for x in self.heter_worker_endpoints.split(\",\")",
            "            ]",
            "            for ip in self.heter_worker_endpoints_ips:",
            "                if ip not in self.node_ips:",
            "                    self.node_ips.append(ip)",
            "",
            "        if len(set(self.node_ips)) == 1:",
            "            self.is_local = True",
            "            self.current_node_ip = self.node_ips[0]",
            "        else:",
            "            self.is_local = False",
            "            pod_ip = os.getenv(\"POD_IP\", None)",
            "            if pod_ip is None:",
            "                _, self.current_node_ip = get_host_name_ip()",
            "            else:",
            "                self.current_node_ip = pod_ip",
            "            if not self.distribute_mode == DistributeMode.PS_HETER:",
            "                assert self.current_node_ip in self.node_ips, (",
            "                    \"Can't find your local ip {%s} in args.servers and args.workers ips: {%s}\"",
            "                    % (self.current_node_ip, self.node_ips)",
            "                )",
            "        if self.current_node_ip in self.node_ips:",
            "            self.node_rank = self.node_ips.index(self.current_node_ip)",
            "            logger.debug(",
            "                \"parsed from args: node_ips:{} current_node_ip:{} node_rank:{}\".format(",
            "                    self.node_ips, self.current_node_ip, self.node_rank",
            "                )",
            "            )",
            "",
            "    def start_ps(self):",
            "        if self.current_node_ip not in self.node_ips:",
            "            return",
            "        cluster = Cluster(hdfs=None)",
            "        server_rank = 0",
            "        worker_rank = 0",
            "        heter_worker_rank = 0",
            "        coordinator_rank = 0",
            "        for node_rank, ip in enumerate(self.node_ips):",
            "            pod = Pod()",
            "            pod.rank = node_rank",
            "            pod.addr = ip",
            "            for i in range(len(self.server_endpoints_ips)):",
            "                if ip == self.server_endpoints_ips[i]:",
            "                    server = Trainer()",
            "                    server.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.server_endpoints_port[i],",
            "                    )",
            "                    server.rank = server_rank",
            "                    server_rank += 1",
            "                    pod.servers.append(server)",
            "            for j in range(len(self.worker_endpoints_ips)):",
            "                if ip == self.worker_endpoints_ips[j]:",
            "                    worker = Trainer()",
            "                    worker.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.worker_endpoints_port[j],",
            "                    )",
            "                    worker.rank = worker_rank",
            "                    worker.stage = 1",
            "                    worker_rank += 1",
            "                    pod.workers.append(worker)",
            "            for m in range(len(self.coordinator_endpoints_ips)):",
            "                if ip == self.coordinator_endpoints_ips[m]:",
            "                    coordinator = Trainer()",
            "                    coordinator.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.coordinator_endpoints_port[m],",
            "                    )",
            "                    coordinator.rank = coordinator_rank",
            "                    coordinator.stage = 1",
            "                    coordinator_rank += 1",
            "                    pod.coordinators.append(coordinator)",
            "",
            "            for k in range(len(self.heter_worker_endpoints_ips)):",
            "                if ip == self.heter_worker_endpoints_ips[k]:",
            "                    heter_worker = Trainer()",
            "                    heter_worker.endpoint = \"{}:{}\".format(",
            "                        ip,",
            "                        self.heter_worker_endpoints_port[k],",
            "                    )",
            "                    heter_worker.rank = heter_worker_rank",
            "                    heter_worker.stage = self.stage_list[k]",
            "                    heter_worker_rank += 1",
            "                    pod.heter_workers.append(heter_worker)",
            "",
            "            cluster.pods.append(pod)",
            "",
            "        pod = cluster.pods[self.node_rank]",
            "        self.gloo_rendezvous_dir = tempfile.mkdtemp()",
            "",
            "        # 3. subproces start",
            "        self.procs = {",
            "            \"worker\": [],",
            "            \"coordinator\": [],",
            "            \"server\": [],",
            "            \"heter_worker\": [],",
            "        }",
            "        self.cmds = {",
            "            \"worker\": [],",
            "            \"coordinator\": [],",
            "            \"server\": [],",
            "            \"heter_worker\": [],",
            "        }",
            "        self.log_fns = {",
            "            \"worker\": [],",
            "            \"coordinator\": [],",
            "            \"server\": [],",
            "            \"heter_worker\": [],",
            "        }",
            "",
            "        self.start_pod_server(self.args, pod)",
            "        self.start_pod_worker(self.args, pod)",
            "        if self.with_coordinator:",
            "            self.start_pod_coordinator(self.args, pod)",
            "        if self.distribute_mode == DistributeMode.PS_HETER:",
            "            self.start_pod_heter_worker(self.args, pod)",
            "",
            "        logger.info(",
            "            \"Please check servers, workers, coordinator and heter_worker logs in {}/workerlog.*, {}/serverlog.* , {}/coordinatorlog.*, and {}/heterlog.*\".format(",
            "                self.args.log_dir,",
            "                self.args.log_dir,",
            "                self.args.log_dir,",
            "                self.args.log_dir,",
            "            )",
            "        )",
            "",
            "        # 4. wait for finish training",
            "        if len(self.procs[\"worker\"]) > 0:",
            "            # if node has worker procs",
            "            # only wait worker to finish here",
            "            for i, proc in enumerate(self.procs[\"worker\"]):",
            "                self.procs[\"worker\"][i].proc.wait()",
            "                if len(self.log_fns[\"worker\"]) > 0:",
            "                    self.log_fns[\"worker\"][i].close()",
            "            logger.info(",
            "                \"all workers exit, going to finish parameter server and heter_worker.\"",
            "            )",
            "            if len(self.procs[\"heter_worker\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"heter_worker\"]):",
            "                    self.log_fns[\"heter_worker\"][i].close()",
            "                    self.procs[\"heter_worker\"][i].proc.terminate()",
            "                logger.info(\"all heter_worker are killed\")",
            "",
            "            if len(self.procs[\"server\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"server\"]):",
            "                    self.log_fns[\"server\"][i].close()",
            "                    self.procs[\"server\"][i].proc.terminate()",
            "                logger.info(\"all parameter server are killed\")",
            "",
            "            if len(self.procs[\"coordinator\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"coordinator\"]):",
            "                    self.log_fns[\"coordinator\"][i].close()",
            "                    self.procs[\"coordinator\"][i].proc.terminate()",
            "                logger.info(\"all coordinators are killed\")",
            "",
            "        else:",
            "            # if node has not worker procs",
            "            # blocking training process",
            "            if len(self.procs[\"server\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"server\"]):",
            "                    self.procs[\"server\"][i].proc.wait()",
            "",
            "            if len(self.procs[\"heter_worker\"]) > 0:",
            "                for i, proc in enumerate(self.procs[\"heter_worker\"]):",
            "                    self.procs[\"heter_worker\"][i].proc.wait()",
            "",
            "        if os.path.exists(self.gloo_rendezvous_dir):",
            "            shutil.rmtree(self.gloo_rendezvous_dir)",
            "",
            "    def start_pod_server(self, args, pod):",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "        for idx, cur_server in enumerate(pod.servers):",
            "            if self.distribute_mode == DistributeMode.PS_HETER:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"PADDLE_ALL_HETER_TRAINER_IP_PORT_LIST\": self.heter_worker_endpoints,",
            "                    \"PADDLE_PORT\": cur_server.endpoint.split(\":\")[1],",
            "                    \"TRAINING_ROLE\": \"PSERVER\",",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"POD_IP\": cur_server.endpoint.split(\":\")[0],",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "            else:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"PADDLE_PORT\": cur_server.endpoint.split(\":\")[1],",
            "                    \"TRAINING_ROLE\": \"PSERVER\",",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"POD_IP\": cur_server.endpoint.split(\":\")[0],",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "            current_env.update(proc_env)",
            "",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"server\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local server start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.servers),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.makedirs(args.log_dir, exist_ok=True)",
            "                fn = open(\"%s/serverlog.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"server\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_server.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"server\"].append(tp)",
            "",
            "    def start_pod_worker(self, args, pod):",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "",
            "        heter_device_num = 0",
            "        device_list = []",
            "        if framework.core.is_compiled_with_cuda():",
            "            device_list = get_gpus(args.gpus)",
            "            heter_device_num = len(device_list)",
            "        elif framework.core.is_compiled_with_xpu():",
            "            heter_device_num = framework.core.get_xpu_device_count()",
            "            device_list = [str(x) for x in range(0, heter_device_num)]",
            "",
            "        for idx, cur_worker in enumerate(pod.workers):",
            "            device_id = (",
            "                \"0\"",
            "                if heter_device_num == 0",
            "                else str(device_list[(idx) % heter_device_num])",
            "            )",
            "            if self.distribute_mode == DistributeMode.PS_HETER:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"PADDLE_STAGE_TRAINERS_NUM\": str(self.stage_trainer_num),",
            "                    \"STAGE_ID\": \"1\",",
            "                    \"STAGE_NUM\": str(self.stage_num),",
            "                    \"PADDLE_PREVIOUS_HETER_TRAINER_IP_PORT_LIST\": \"\",",
            "                    \"PADDLE_NEXT_HETER_TRAINER_IP_PORT_LIST\": self.stage_heter_map[",
            "                        2",
            "                    ],",
            "                    \"PADDLE_ALL_HETER_TRAINER_IP_PORT_LIST\": self.heter_worker_endpoints,",
            "                    \"HETER_DEVICE_TYPE\": self.stage_device_map[1],",
            "                    \"TRAINING_ROLE\": \"TRAINER\",",
            "                    \"POD_IP\": cur_worker.endpoint.split(\":\")[0],",
            "                    \"PADDLE_PORT\": cur_worker.endpoint.split(\":\")[1],",
            "                    \"PADDLE_TRAINER_ID\": str(cur_worker.rank),",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"FLAGS_selected_gpus\": \"0\",",
            "                    \"FLAGS_selected_xpus\": \"0\",",
            "                    \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                    \"XPU_VISIBLE_DEVICES\": device_id,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "            else:",
            "                proc_env = {",
            "                    \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                    \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                    \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                    \"TRAINING_ROLE\": \"TRAINER\",",
            "                    \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                    \"POD_IP\": cur_worker.endpoint.split(\":\")[0],",
            "                    \"PADDLE_PORT\": cur_worker.endpoint.split(\":\")[1],",
            "                    \"PADDLE_TRAINER_ID\": str(cur_worker.rank),",
            "                    \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                    \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                    \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                    \"FLAGS_selected_gpus\": \"0\",",
            "                    \"FLAGS_selected_xpus\": \"0\",",
            "                    \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                    \"XPU_VISIBLE_DEVICES\": device_id,",
            "                    \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "                }",
            "",
            "            current_env.update(proc_env)",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"worker\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local worker start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.workers),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.makedirs(args.log_dir, exist_ok=True)",
            "                fn = open(\"%s/workerlog.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"worker\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_worker.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"worker\"].append(tp)",
            "",
            "    def start_pod_coordinator(self, args, pod):",
            "        print(\">>> entering start_pod_coordinator\")",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "",
            "        for idx, cur_coordinator in enumerate(pod.coordinators):",
            "            device_id = \"0\"",
            "            proc_env = {",
            "                \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                \"PADDLE_COORDINATOR_ENDPOINTS\": self.coordinator_endpoints,",
            "                \"PADDLE_COORDINATOR_NUM\": str(self.coordinator_num),",
            "                \"TRAINING_ROLE\": \"COORDINATOR\",",
            "                \"POD_IP\": cur_coordinator.endpoint.split(\":\")[0],",
            "                \"PADDLE_PORT\": cur_coordinator.endpoint.split(\":\")[1],",
            "                \"PADDLE_TRAINER_ID\": str(cur_coordinator.rank),",
            "                \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                \"FLAGS_selected_gpus\": \"0\",",
            "                \"FLAGS_selected_xpus\": \"0\",",
            "                \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                \"XPU_VISIBLE_DEVICES\": device_id,",
            "                \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "            }",
            "",
            "            current_env.update(proc_env)",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"coordinator\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local coordinator start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.coordinators),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.makedirs(args.log_dir, exist_ok=True)",
            "                fn = open(\"%s/coordinator.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"coordinator\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_coordinator.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"coordinator\"].append(tp)",
            "",
            "    def start_pod_heter_worker(self, args, pod):",
            "        default_env = os.environ.copy()",
            "        current_env = copy.copy(default_env)",
            "        current_env.pop(\"http_proxy\", None)",
            "        current_env.pop(\"https_proxy\", None)",
            "",
            "        heter_device_num = 0",
            "        device_list = []",
            "        if framework.core.is_compiled_with_cuda():",
            "            device_list = get_gpus(args.gpus)",
            "            heter_device_num = len(device_list)",
            "        elif framework.core.is_compiled_with_xpu():",
            "            heter_device_num = framework.core.get_xpu_device_count()",
            "            device_list = [str(x) for x in range(0, heter_device_num)]",
            "",
            "        for idx, cur_heter_worker in enumerate(pod.heter_workers):",
            "            device_id = (",
            "                \"0\"",
            "                if heter_device_num == 0",
            "                else str(device_list[(idx) % heter_device_num])",
            "            )",
            "            stage_id = cur_heter_worker.stage",
            "            proc_env = {",
            "                \"PADDLE_PSERVERS_IP_PORT_LIST\": self.server_endpoints,",
            "                \"PADDLE_TRAINER_ENDPOINTS\": self.worker_endpoints,",
            "                \"PADDLE_NEXT_HETER_TRAINER_IP_PORT_LIST\": self.stage_heter_map[",
            "                    stage_id + 1",
            "                ]",
            "                if stage_id <= self.stage_num - 1",
            "                else \"\",",
            "                \"PADDLE_PREVIOUS_HETER_TRAINER_IP_PORT_LIST\": self.stage_heter_map[",
            "                    stage_id - 1",
            "                ],",
            "                \"PADDLE_ALL_HETER_TRAINER_IP_PORT_LIST\": self.heter_worker_endpoints,",
            "                \"HETER_DEVICE_TYPE\": self.stage_device_map[stage_id],",
            "                \"STAGE_ID\": str(stage_id),",
            "                \"STAGE_NUM\": str(self.stage_num),",
            "                \"PADDLE_PORT\": cur_heter_worker.endpoint.split(\":\")[1],",
            "                \"TRAINING_ROLE\": \"HETER_TRAINER\",",
            "                \"PADDLE_TRAINERS_NUM\": str(self.worker_num),",
            "                \"PADDLE_STAGE_TRAINERS_NUM\": str(self.stage_trainer_num),",
            "                \"POD_IP\": cur_heter_worker.endpoint.split(\":\")[0],",
            "                \"PADDLE_WITH_GLOO\": str(os.getenv(\"PADDLE_WITH_GLOO\", \"0\")),",
            "                \"PADDLE_GLOO_RENDEZVOUS\": \"3\",",
            "                \"PADDLE_GLOO_FS_PATH\": self.gloo_rendezvous_dir,",
            "                \"FLAGS_selected_gpus\": \"0\",",
            "                \"FLAGS_selected_xpus\": \"0\",",
            "                \"CUDA_VISIBLE_DEVICES\": device_id,",
            "                \"XPU_VISIBLE_DEVICES\": device_id,",
            "                \"PADDLE_GLOO_HTTP_ENDPOINT\": self.http_port,",
            "            }",
            "            current_env.update(proc_env)",
            "",
            "            cmd = [",
            "                sys.executable,",
            "                \"-u\",",
            "                args.training_script,",
            "            ] + args.training_script_args",
            "            self.cmds[\"heter_worker\"].append(cmd)",
            "",
            "            if idx == 0:",
            "                logger.info(",
            "                    \"Local heter_worker start {} processes. First process distributed \"",
            "                    \"environment info (Only For Debug): {}\".format(",
            "                        len(pod.heter_workers),",
            "                        pretty_print_envs(",
            "                            proc_env, (\"Distributed Envs\", \"Value\")",
            "                        ),",
            "                    )",
            "                )",
            "",
            "            if args.log_dir is not None:",
            "                os.makedirs(args.log_dir, exist_ok=True)",
            "                fn = open(\"%s/heterlog.%d\" % (args.log_dir, idx), \"w\")",
            "                self.log_fns[\"heter_worker\"].append(fn)",
            "                proc = subprocess.Popen(",
            "                    cmd, env=current_env, stdout=fn, stderr=fn",
            "                )",
            "            else:",
            "                proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "            tp = TrainerProc()",
            "            tp.proc = proc",
            "            tp.rank = cur_heter_worker.rank",
            "            tp.local_rank = idx",
            "            tp.log_fn = fn",
            "            tp.log_offset = fn.tell() if fn else None",
            "            tp.cmd = cmd",
            "",
            "            self.procs[\"heter_worker\"].append(tp)",
            "",
            "",
            "def check_backend(backend):",
            "    if backend not in [",
            "        'nccl',",
            "        'gloo',",
            "        'bkcl',",
            "        'cncl',",
            "        'auto',",
            "        'hccl',",
            "        'heter',",
            "        'xccl',",
            "    ]:",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"backend argument can only be one of \"",
            "            \"'nccl', 'gloo', 'bkcl', 'auto', 'hccl', 'heter', 'xccl' \"",
            "            \"but got %s\" % backend",
            "        )",
            "",
            "    if backend == 'nccl' and not framework.core.is_compiled_with_cuda():",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"your paddle is not compiled with cuda but you assign 'nccl' as backend.\"",
            "        )",
            "",
            "    if backend == 'bkcl' and not framework.core.is_compiled_with_xpu():",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"your paddle is not compiled with xpu but you assign 'bkcl' as backend.\"",
            "        )",
            "",
            "    if backend == 'hccl' and not framework.core.is_compiled_with_npu():",
            "        raise ValueError(",
            "            \"paddle.distributed initialize error, \"",
            "            \"your paddle is not compiled with npu but you assign 'hccl' as backend.\"",
            "        )",
            "",
            "",
            "def block_windows_and_macos(backend):",
            "    if backend != 'gloo':",
            "        return",
            "    if utils.OS_NAME.startswith('darwin'):  # MACOS , block",
            "        raise ValueError(",
            "            \"You are going to using gloo on macos, but currently is not supported\"",
            "        )",
            "    if utils.IS_WINDOWS:  # MACOS , block",
            "        raise ValueError(",
            "            \"You are going to using gloo on windows, but currently is not supported\"",
            "        )",
            "",
            "",
            "def get_backend_by_compile_flag():",
            "    if framework.core.is_compiled_with_cuda():",
            "        return 'nccl'",
            "",
            "    if framework.core.is_compiled_with_xpu():",
            "        return 'bkcl'",
            "",
            "    if framework.core.is_compiled_with_npu():",
            "        return 'hccl'",
            "",
            "    return 'gloo'"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "599": [
                "start_local_trainers"
            ],
            "1765": [
                "ParameterServerLauncher",
                "start_pod_server"
            ],
            "1873": [
                "ParameterServerLauncher",
                "start_pod_worker"
            ],
            "1941": [
                "ParameterServerLauncher",
                "start_pod_coordinator"
            ],
            "2032": [
                "ParameterServerLauncher",
                "start_pod_heter_worker"
            ]
        },
        "addLocation": []
    },
    "python/paddle/distributed/fleet/utils/fs.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 174,
                "afterPatchRowNumber": 174,
                "PatchRowcode": "         assert not os.path.isfile(fs_path), \"{} is already a file\".format("
            },
            "1": {
                "beforePatchRowNumber": 175,
                "afterPatchRowNumber": 175,
                "PatchRowcode": "             fs_path"
            },
            "2": {
                "beforePatchRowNumber": 176,
                "afterPatchRowNumber": 176,
                "PatchRowcode": "         )"
            },
            "3": {
                "beforePatchRowNumber": 177,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        os.system(f\"mkdir -p {fs_path}\")"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 177,
                "PatchRowcode": "+        os.makedirs(fs_path, exist_ok=True)"
            },
            "5": {
                "beforePatchRowNumber": 178,
                "afterPatchRowNumber": 178,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 179,
                "afterPatchRowNumber": 179,
                "PatchRowcode": "     def rename(self, fs_src_path, fs_dst_path):"
            },
            "7": {
                "beforePatchRowNumber": 180,
                "afterPatchRowNumber": 180,
                "PatchRowcode": "         \"\"\""
            }
        },
        "frontPatchFile": [
            "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import abc",
            "import functools",
            "import multiprocessing",
            "import os",
            "import re",
            "import shutil",
            "import time",
            "",
            "# (TODO: GhostScreaming) It will be removed later.",
            "from paddle.fluid import core",
            "",
            "from .log_util import logger",
            "",
            "__all__ = []",
            "",
            "",
            "class ExecuteError(Exception):",
            "    pass",
            "",
            "",
            "class FSFileExistsError(Exception):",
            "    pass",
            "",
            "",
            "class FSFileNotExistsError(Exception):",
            "    pass",
            "",
            "",
            "class FSTimeOut(Exception):",
            "    pass",
            "",
            "",
            "class FSShellCmdAborted(ExecuteError):",
            "    pass",
            "",
            "",
            "class FS:",
            "    @abc.abstractmethod",
            "    def ls_dir(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def is_file(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def is_dir(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def is_exist(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def upload(self, local_path, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def download(self, fs_path, local_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def mkdirs(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def delete(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def need_upload_download(self):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def rename(self, fs_src_path, fs_dst_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def mv(self, fs_src_path, fs_dst_path, overwrite=False, test_exists=False):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def upload_dir(self, local_dir, dest_dir):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def list_dirs(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def touch(self, fs_path, exist_ok=True):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def cat(self, fs_path=None):",
            "        raise NotImplementedError",
            "",
            "",
            "class LocalFS(FS):",
            "    \"\"\"",
            "    A tool of local file system.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            from paddle.distributed.fleet.utils import LocalFS",
            "",
            "            client = LocalFS()",
            "            subdirs, files = client.ls_dir(\"./\")",
            "    \"\"\"",
            "",
            "    def ls_dir(self, fs_path):",
            "        \"\"\"",
            "        List directorys and files under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Tuple: Return a 2-tuple, the first is a list of all its subdirectories,",
            "            and the second is a list of all its subfiles, e.g. ([subdirname1, subdirname1, ...], [filename1, filename2, ...]).",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                subdirs, files = client.ls_dir(\"./\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return [], []",
            "",
            "        dirs = []",
            "        files = []",
            "        for f in os.listdir(fs_path):",
            "            if os.path.isdir(fs_path + \"/\" + f):",
            "                dirs.append(f)",
            "            else:",
            "                files.append(f)",
            "",
            "        return dirs, files",
            "",
            "    def mkdirs(self, fs_path):",
            "        \"\"\"",
            "        Create a local directory.",
            "",
            "        Args:",
            "            fs_path(str): The local directory path.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.mkdirs(\"test_mkdirs\")",
            "                client.delete(\"test_mkdirs\")",
            "        \"\"\"",
            "        assert not os.path.isfile(fs_path), \"{} is already a file\".format(",
            "            fs_path",
            "        )",
            "        os.system(f\"mkdir -p {fs_path}\")",
            "",
            "    def rename(self, fs_src_path, fs_dst_path):",
            "        \"\"\"",
            "        Rename the file.",
            "",
            "        Args:",
            "            fs_src_path(str): The actual name of the file or directory",
            "            fs_dst_path(str): The new name of the file or directory.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_rename_src\")",
            "                print(client.is_exists(\"test_rename_src\")) # True",
            "                client.rename(\"test_rename_src\", \"test_rename_dst\")",
            "                print(client.is_exists(\"test_rename_src\")) # False",
            "                print(client.is_exists(\"test_rename_dst\")) # True",
            "                client.delete(\"test_rename_dst\")",
            "        \"\"\"",
            "        os.rename(fs_src_path, fs_dst_path)",
            "",
            "    def _rmr(self, fs_path):",
            "        shutil.rmtree(fs_path)",
            "",
            "    def _rm(self, fs_path):",
            "        os.remove(fs_path)",
            "",
            "    def delete(self, fs_path):",
            "        \"\"\"",
            "        Delete the local file path, whether it's a file or directory.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.mkdirs(\"test_localFS_mkdirs\")",
            "                client.delete(\"test_localFS_mkdirs\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return",
            "",
            "        if os.path.isfile(fs_path):",
            "            return self._rm(fs_path)",
            "",
            "        return self._rmr(fs_path)",
            "",
            "    def need_upload_download(self):",
            "        return False",
            "",
            "    def is_file(self, fs_path):",
            "        \"\"\"",
            "        Whether the local file path is a file.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a file, otherwise return false.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_is_file\")",
            "                print(client.is_file(\"test_is_file\")) # True",
            "                client.delete(\"test_is_file\")",
            "        \"\"\"",
            "        return os.path.isfile(fs_path)",
            "",
            "    def is_dir(self, fs_path):",
            "        \"\"\"",
            "        Whether the local file path is a directory.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a directory, otherwise return false.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.mkdirs(\"test_is_dir\")",
            "                print(client.is_dir(\"test_is_file\")) # True",
            "                client.delete(\"test_is_dir\")",
            "        \"\"\"",
            "        return os.path.isdir(fs_path)",
            "",
            "    def is_exist(self, fs_path):",
            "        \"\"\"",
            "        Whether the local file path exists.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Bool: Wheter it's a file or directory, return true if the path exists,",
            "            otherwise return false.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                ret = local_fs.is_exist(\"test_is_exist\")",
            "        \"\"\"",
            "        return os.path.exists(fs_path)",
            "",
            "    def touch(self, fs_path, exist_ok=True):",
            "        \"\"\"",
            "        Create a local file.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "            exist_ok(bool): When `fs_path` exists, if `exist_ok` is set false,",
            "            program will throw an Exception. Default is true.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_touch\")",
            "                client.delete(\"test_touch\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            if exist_ok:",
            "                return",
            "            raise FSFileExistsError",
            "",
            "        os.system(f\"touch {fs_path}\")",
            "",
            "    def mv(self, src_path, dst_path, overwrite=False, test_exists=False):",
            "        \"\"\"",
            "        Move a local file or directory from `src_path` to `dst_path` .",
            "",
            "        Args:",
            "            src_path(str):  Name of the file or directory, that's needed to be moved.",
            "            dst_path(str):  Name of the file or directory to which to move to.",
            "            overwrite(bool): Whether to re-write `dst_path` if that exists. Default is False.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_mv_src\")",
            "                client.mv(\"test_mv_src\", \"test_mv_dst\")",
            "                client.delete(\"test_mv_dst\")",
            "        \"\"\"",
            "        if not self.is_exist(src_path):",
            "            raise FSFileNotExistsError",
            "",
            "        if overwrite and self.is_exist(dst_path):",
            "            self.delete(dst_path)",
            "",
            "        if self.is_exist(dst_path):",
            "            raise FSFileExistsError",
            "",
            "        return self.rename(src_path, dst_path)",
            "",
            "    def list_dirs(self, fs_path):",
            "        \"\"\"",
            "        Only list directorys under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            List: A list of all its subdirectories, e.g. [subdirname1, subdirname1, ...].",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                subdirs = client.list_dirs(\"./\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return []",
            "",
            "        dirs = [",
            "            f for f in os.listdir(fs_path) if os.path.isdir(fs_path + \"/\" + f)",
            "        ]",
            "",
            "        return dirs",
            "",
            "",
            "def _handle_errors(max_time_out=None):",
            "    def decorator(f):",
            "        @functools.wraps(f)",
            "        def handler(*args, **kwargs):",
            "            o = args[0]",
            "            time_out = max_time_out",
            "            if time_out is None:",
            "                time_out = float(o._time_out) / 1000.0",
            "            else:",
            "                time_out /= 1000.0",
            "            inter = float(o._sleep_inter) / 1000.0",
            "",
            "            start = time.time()",
            "            last_print_time = start",
            "            while True:",
            "                try:",
            "                    return f(*args, **kwargs)",
            "                # important: only ExecuteError need to retry",
            "                except ExecuteError as e:",
            "                    if time.time() - start >= time_out:",
            "                        raise FSTimeOut(",
            "                            \"args:{} timeout:{}\".format(",
            "                                args, time.time() - start",
            "                            )",
            "                        )",
            "",
            "                    time.sleep(inter)",
            "",
            "                if time.time() - last_print_time > 30:",
            "                    print(",
            "                        \"hadoop operator timeout:args:{} timeout:{}\".format(",
            "                            args, time.time() - start",
            "                        )",
            "                    )",
            "                    last_print_time = time.time()",
            "",
            "        return handler",
            "",
            "    return decorator",
            "",
            "",
            "class HDFSClient(FS):",
            "    \"\"\"",
            "    A tool of HDFS.",
            "",
            "    Args:",
            "        hadoop_home(str): Hadoop home.",
            "        configs(dict): Hadoop config. It is a dictionary and needs to contain the",
            "            keys: \"fs.default.name\" and \"hadoop.job.ugi\".",
            "",
            "    Examples:",
            "",
            "        .. code-block:: text",
            "",
            "            from paddle.distributed.fleet.utils import HDFSClient",
            "            hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "",
            "            configs = {",
            "                \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                \"hadoop.job.ugi\": \"hello,hello123\"",
            "            }",
            "",
            "            client = HDFSClient(hadoop_home, configs)",
            "            client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hadoop_home,",
            "        configs,",
            "        time_out=5 * 60 * 1000,  # ms",
            "        sleep_inter=1000,",
            "    ):  # ms",
            "        self.pre_commands = []",
            "        hadoop_bin = '%s/bin/hadoop' % hadoop_home",
            "        self.pre_commands.append(hadoop_bin)",
            "        dfs = 'fs'",
            "        self.pre_commands.append(dfs)",
            "",
            "        if configs:",
            "            for k, v in configs.items():",
            "                config_command = f'-D{k}={v}'",
            "                self.pre_commands.append(config_command)",
            "",
            "        self._time_out = time_out",
            "        self._sleep_inter = sleep_inter",
            "        self._base_cmd = \" \".join(self.pre_commands)",
            "        self._bd_err_re = re.compile(",
            "            r'\\s?responseErrorMsg\\s?\\:.*, errorCode\\:\\s?[0-9]+, path\\:'",
            "        )",
            "",
            "    def _run_cmd(self, cmd, redirect_stderr=False, retry_times=5):",
            "        exe_cmd = f\"{self._base_cmd} -{cmd}\"",
            "        ret = 0",
            "        output = None",
            "        retry_sleep_second = 3",
            "        for x in range(retry_times + 1):",
            "            ret, output = core.shell_execute_cmd(exe_cmd, 0, 0, redirect_stderr)",
            "            ret = int(ret)",
            "            if ret == 0:",
            "                break",
            "            time.sleep(retry_sleep_second)",
            "        if ret == 134:",
            "            raise FSShellCmdAborted(cmd)",
            "",
            "        return ret, output.splitlines()",
            "",
            "    @_handle_errors()",
            "    def list_dirs(self, fs_path):",
            "        \"\"\"",
            "        Only list directorys under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            List: A list of all its subdirectories, e.g. [subdirname1, subdirname1, ...].",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                subdirs = client.list_dirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return []",
            "",
            "        dirs, files = self._ls_dir(fs_path)",
            "        return dirs",
            "",
            "    @_handle_errors()",
            "    def ls_dir(self, fs_path):",
            "        \"\"\"",
            "        List directorys and files under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Tuple: Return a 2-tuple, the first element is the list of all its subdirectories,",
            "            and the second one is the list of all its subfiles, e.g. ([subdirname1, subdirname1, ...], [filename1, filename2, ...]).",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                subdirs, files = client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return [], []",
            "",
            "        return self._ls_dir(fs_path)",
            "",
            "    def _ls_dir(self, fs_path):",
            "        cmd = f\"ls {fs_path}\"",
            "        ret, lines = self._run_cmd(cmd)",
            "",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "        dirs = []",
            "        files = []",
            "        for line in lines:",
            "            arr = line.split()",
            "            if len(arr) != 8:",
            "                continue",
            "",
            "            p = os.path.basename(arr[7])",
            "            if arr[0][0] == 'd':",
            "                dirs.append(p)",
            "            else:",
            "                files.append(p)",
            "",
            "        return dirs, files",
            "",
            "    def _test_match(self, lines):",
            "        for l in lines:",
            "            m = self._bd_err_re.match(l)",
            "            if m is not None:",
            "                return m",
            "",
            "        return None",
            "",
            "    @_handle_errors()",
            "    def is_dir(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a directory, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return self._is_dir(fs_path)",
            "",
            "    def _is_dir(self, fs_path):",
            "        cmd = f\"test -d {fs_path}\"",
            "        ret, lines = self._run_cmd(cmd, redirect_stderr=True, retry_times=1)",
            "        if ret:",
            "            # other error",
            "            if self._test_match(lines):",
            "                print('raise exception: ')",
            "                print('\\n'.join(lines))",
            "                raise ExecuteError(cmd)",
            "",
            "            return False",
            "",
            "        return True",
            "",
            "    def is_file(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a file, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return not self._is_dir(fs_path)",
            "",
            "    @_handle_errors()",
            "    def is_exist(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path exists.",
            "",
            "        Args:",
            "            fs_path(str): The hdfs file path.",
            "",
            "        Returns:",
            "            Bool: Whether it's is file or directory, return true if the path exists,",
            "            otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                ret = client.is_exist(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        cmd = f\"test -e {fs_path} \"",
            "        ret, out = self._run_cmd(cmd, redirect_stderr=True, retry_times=1)",
            "        if ret != 0:",
            "            return False",
            "",
            "        return True",
            "",
            "    def upload_dir(self, local_dir, dest_dir, overwrite=False):",
            "        \"\"\"",
            "        upload dir to hdfs",
            "        Args:",
            "            local_dir(str): local dir",
            "            dest_dir(str): hdfs dest dir",
            "            overwrite(bool): is overwrite",
            "        Returns:",
            "            return code",
            "        \"\"\"",
            "        local_dir = local_dir.rstrip(\"/\")",
            "        dest_dir = dest_dir.rstrip(\"/\")",
            "        local_basename = os.path.basename(local_dir)",
            "        if self.is_exist(dest_dir + \"/\" + local_basename) and overwrite:",
            "            self.delete(dest_dir + \"/\" + local_basename)",
            "        if not self.is_exist(dest_dir):",
            "            self.mkdirs(dest_dir)",
            "        self._try_upload(local_dir, dest_dir)",
            "",
            "    # can't retry",
            "    def upload(self, local_path, fs_path, multi_processes=5, overwrite=False):",
            "        \"\"\"",
            "        Upload the local path to remote HDFS.",
            "",
            "        Args:",
            "            local_path(str): The local path.",
            "            fs_path(str): The HDFS path.",
            "            multi_processes(int|1): the upload data process at the same time, default=5",
            "            overwrite(bool|False): will overwrite file on HDFS or not",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.upload(\"test_hdfs_client\", \"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "",
            "        def __subprocess_upload(hdfs_path_single, datas):",
            "            for data in datas:",
            "                self._try_upload(data, hdfs_path_single)",
            "",
            "        def get_local_files(path):",
            "            \"\"\"",
            "            get local files",
            "            Args:",
            "                path(str): local path",
            "            Returns:",
            "                list of local files",
            "            \"\"\"",
            "            rlist = []",
            "",
            "            if not os.path.exists(path):",
            "                return rlist",
            "",
            "            if os.path.isdir(path):",
            "                for file in os.listdir(path):",
            "                    t = os.path.join(path, file)",
            "                    rlist.append(t)",
            "            else:",
            "                rlist.append(path)",
            "            return rlist",
            "",
            "        local = LocalFS()",
            "        if not local.is_exist(local_path):",
            "            raise FSFileNotExistsError(f\"{local_path} not exists\")",
            "",
            "        all_files = get_local_files(local_path)",
            "        if not all_files:",
            "            print(\"there are nothing need to upload, function exit\")",
            "            return",
            "",
            "        if self.is_exist(fs_path) and overwrite:",
            "            self.delete(fs_path)",
            "            self.mkdirs(fs_path)",
            "",
            "        procs = []",
            "        for i in range(multi_processes):",
            "            process_datas = self._split_files(all_files, i, multi_processes)",
            "            p = multiprocessing.Process(",
            "                target=__subprocess_upload, args=(fs_path, process_datas)",
            "            )",
            "            procs.append(p)",
            "            p.start()",
            "",
            "        # complete the processes",
            "        for proc in procs:",
            "            proc.join()",
            "",
            "    @_handle_errors()",
            "    def _try_upload(self, local_path, fs_path):",
            "        cmd = f\"put {local_path} {fs_path}\"",
            "        ret = 0",
            "        try:",
            "            ret, _ = self._run_cmd(cmd)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "        except Exception as e:",
            "            self.delete(fs_path)",
            "            raise e",
            "",
            "    # can't retry",
            "    def download(self, fs_path, local_path, multi_processes=5, overwrite=False):",
            "        \"\"\"",
            "        Download remote HDFS path to the local.",
            "",
            "        Args:",
            "            fs_path(str):  The HDFS path.",
            "            local_path(str): The local path.",
            "            multi_processes(int|1): the download data process at the same time, default=1",
            "            overwrite(bool): is overwrite",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.download(\"hdfs:/test_hdfs_client\", \"./\")",
            "        \"\"\"",
            "",
            "        def __subprocess_download(local_path, datas):",
            "            \"\"\"",
            "            download file from HDFS",
            "            Args:",
            "                local_path(str): the local file path",
            "                datas(str): the hdfs file path list",
            "            \"\"\"",
            "            for data in datas:",
            "                self._try_download(data, local_path)",
            "",
            "        if not self.is_exist(fs_path):",
            "            raise FSFileNotExistsError(f\"{fs_path} not exits\")",
            "        # download file",
            "        if self.is_file(fs_path):",
            "            return self._try_download(fs_path, local_path)",
            "        # download dir",
            "        dirs, all_filenames = self.ls_dir(fs_path)",
            "        all_files = [fs_path + \"/\" + i for i in all_filenames]",
            "        all_files.extend([fs_path + \"/\" + i for i in dirs])",
            "        procs = []",
            "        for i in range(multi_processes):",
            "            process_datas = self._split_files(all_files, i, multi_processes)",
            "            p = multiprocessing.Process(",
            "                target=__subprocess_download, args=(local_path, process_datas)",
            "            )",
            "            procs.append(p)",
            "            p.start()",
            "",
            "        # complete the processes",
            "        for proc in procs:",
            "            proc.join()",
            "",
            "    @_handle_errors()",
            "    def _try_download(self, fs_path, local_path):",
            "        cmd = f\"get {fs_path} {local_path}\"",
            "        ret = 0",
            "        try:",
            "            ret, _ = self._run_cmd(cmd)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "        except Exception as e:",
            "            local_fs = LocalFS()",
            "            local_fs.delete(local_path)",
            "            raise e",
            "",
            "    @_handle_errors()",
            "    def mkdirs(self, fs_path):",
            "        \"\"\"",
            "        Create a remote HDFS directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS directory path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.mkdirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            return",
            "",
            "        out_hdfs = False",
            "",
            "        cmd = f\"mkdir {fs_path} \"",
            "        ret, out = self._run_cmd(cmd, redirect_stderr=True)",
            "        if ret != 0:",
            "            for l in out:",
            "                if \"No such file or directory\" in l:",
            "                    out_hdfs = True",
            "                    break",
            "            if not out_hdfs:",
            "                raise ExecuteError(cmd)",
            "",
            "        if out_hdfs and not self.is_exist(fs_path):",
            "            cmd = f\"mkdir -p {fs_path}\"",
            "            ret, _ = self._run_cmd(cmd)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "",
            "    def mv(self, fs_src_path, fs_dst_path, overwrite=False, test_exists=True):",
            "        \"\"\"",
            "        Move a remote HDFS file or directory from `fs_src_path` to `fs_dst_path` .",
            "",
            "        Args:",
            "            fs_src_path(str):  Name of the file or directory, that's needed to be moved.",
            "            fs_dst_path(str):  Name of the file or directory to which to move to.",
            "            overwrite(bool): Whether to re-write `fs_dst_path` if that exists. Default is False.",
            "            test_exists(bool): Check the existence of `fs_src_path` and `fs_dst_path` . When `test_exists` is set true, if `fs_src_path` doesn't exist or `fs_dst_path` exists, program will throw an Excetption.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.mv(\"hdfs:/test_hdfs_client\", \"hdfs:/test_hdfs_client2\")",
            "        \"\"\"",
            "        if overwrite and self.is_exist(fs_dst_path):",
            "            self.delete(fs_dst_path)",
            "",
            "        if test_exists:",
            "            if not self.is_exist(fs_src_path):",
            "                raise FSFileNotExistsError(f\"{fs_src_path} is not exists\")",
            "",
            "            if self.is_exist(fs_dst_path):",
            "                raise FSFileExistsError(f\"{fs_dst_path} exists already\")",
            "",
            "        return self._try_mv(fs_src_path, fs_dst_path)",
            "",
            "    @_handle_errors()",
            "    def _try_mv(self, fs_src_path, fs_dst_path):",
            "        cmd = f\"mv {fs_src_path} {fs_dst_path}\"",
            "        ret = 0",
            "        try:",
            "            ret, _ = self._run_cmd(cmd, retry_times=1)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "        except Exception as e:",
            "            if not self.is_exist(fs_src_path) and self.is_exist(fs_dst_path):",
            "                return",
            "            raise e",
            "",
            "    def _rmr(self, fs_path):",
            "        cmd = f\"rmr {fs_path}\"",
            "        ret, _ = self._run_cmd(cmd)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "    def _rm(self, fs_path):",
            "        cmd = f\"rm {fs_path}\"",
            "        ret, _ = self._run_cmd(cmd)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "    @_handle_errors()",
            "    def delete(self, fs_path):",
            "        \"\"\"",
            "        Delete a remote HDFS path, whether it's a file or directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.delete(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return",
            "",
            "        is_dir = self._is_dir(fs_path)",
            "        if is_dir:",
            "            return self._rmr(fs_path)",
            "",
            "        return self._rm(fs_path)",
            "",
            "    def touch(self, fs_path, exist_ok=True):",
            "        \"\"\"",
            "        Create a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "            exist_ok(bool): When `fs_path` exists, if `exist_ok` is set false,",
            "            program will throw an Exception. Default is true.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.touch(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            if exist_ok:",
            "                return",
            "            raise FSFileExistsError",
            "",
            "        return self._touchz(fs_path)",
            "",
            "    @_handle_errors()",
            "    def _touchz(self, fs_path):",
            "        cmd = f\"touchz {fs_path}\"",
            "        ret, _ = self._run_cmd(cmd)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "    def need_upload_download(self):",
            "        return True",
            "",
            "    def cat(self, fs_path=None):",
            "        \"\"\"",
            "        Cat a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            file content",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.cat(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_file(fs_path):",
            "            output = self._try_cat(fs_path)",
            "            return \"\\n\".join(output)",
            "        else:",
            "            return \"\"",
            "",
            "    @_handle_errors()",
            "    def _try_cat(self, fs_path):",
            "        cmd = f\"cat {fs_path}\"",
            "        ret, output = self._run_cmd(cmd, retry_times=1)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "        return output",
            "",
            "    def _split_files(self, files, trainer_id, trainers):",
            "        \"\"\"",
            "        split file list",
            "        Args:",
            "            files(list): file list",
            "            trainer_id(int): trainer mpi rank id",
            "            trainers(int): all trainers num",
            "        Returns:",
            "            fileist(list): file list of current trainer",
            "        \"\"\"",
            "        remainder = len(files) % trainers",
            "        blocksize = len(files) // trainers",
            "",
            "        blocks = [blocksize] * trainers",
            "        for i in range(remainder):",
            "            blocks[i] += 1",
            "",
            "        trainer_files = [[]] * trainers",
            "        begin = 0",
            "        for i in range(trainers):",
            "            trainer_files[i] = files[begin : begin + blocks[i]]",
            "            begin += blocks[i]",
            "",
            "        return trainer_files[trainer_id]",
            "",
            "    def list_files_info(self, path_list):",
            "        \"\"\"",
            "        list_files return file path and size",
            "        Args:",
            "            path_list(list): file list",
            "        Returns:",
            "            fileist(list): file list with file path and size",
            "        \"\"\"",
            "        if len(path_list) <= 0:",
            "            return []",
            "",
            "        file_list = []",
            "",
            "        # concat filelist can speed up 'hadoop ls'",
            "        str_concat = \"\"",
            "        for path in path_list:",
            "            str_concat += path + \" \"",
            "        cmd = (",
            "            \"ls \" + str_concat + \" | awk '{if ($8 != \\\"\\\") {print $5\\\" \\\"$8 }}'\"",
            "        )",
            "        ret, lines = self._run_cmd(cmd)",
            "        if len(lines) == 0:",
            "            logger.warning(\"list_files empty, path[%s]\" % path_list)",
            "            return []",
            "        for line in lines:",
            "            arr = line.split(' ')",
            "            if len(arr) < 2:",
            "                continue",
            "            file_path = arr[1]",
            "            file_size = int(arr[0])",
            "            file_list.append({'path': file_path, 'size': file_size})",
            "",
            "        return file_list",
            "",
            "",
            "class AFSClient(FS):",
            "    \"\"\"",
            "    A tool of AFS. Use AfsWrapper.",
            "",
            "    Examples:",
            "",
            "        .. code-block:: text",
            "",
            "            from paddle.distributed.fleet.utils import AFSClient",
            "            client = AFSClient()",
            "            client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "            client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "    \"\"\"",
            "",
            "    def __init__(self, time_out=5 * 60 * 1000, sleep_inter=1000):  # ms  # ms",
            "        self._fs = core.AfsWrapper()",
            "        self._time_out = time_out",
            "",
            "    def init(self, fs_name, fs_user, fs_passwd, fs_conf):",
            "        self._fs.init(fs_name, fs_user, fs_passwd, fs_conf)",
            "",
            "    def list_dirs(self, fs_path):",
            "        \"\"\"",
            "        Only list directorys under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            List: A list of all its subdirectories, e.g. [subdirname1, subdirname1, ...].",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                subdirs = client.list_dirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return []",
            "        # TODO:fengdanlei",
            "        dirs, files = self._ls_dir(fs_path)",
            "        return dirs",
            "",
            "    def ls_dir(self, fs_path):",
            "        \"\"\"",
            "        List directorys and files under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Tuple: Return a 2-tuple, the first element is the list of all its subdirectories,",
            "            and the second one is the list of all its subfiles, e.g. ([subdirname1, subdirname1, ...], [filename1, filename2, ...]).",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                subdirs, files = client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return [], []",
            "",
            "        return self._ls_dir(fs_path)",
            "",
            "    def _ls_dir(self, fs_path):",
            "",
            "        files = self._fs.list(fs_path)",
            "        dirs = [fs_path]",
            "        return dirs, files",
            "",
            "    def is_dir(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a directory, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return self._is_dir(fs_path)",
            "",
            "    def _is_dir(self, fs_path):",
            "        list_path = self._fs.list(fs_path)",
            "        if (len(list_path)) > 0:",
            "            return True",
            "        else:",
            "            return False",
            "",
            "    def is_file(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a file, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return not self._is_dir(fs_path)",
            "",
            "    def is_exist(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path exists.",
            "",
            "        Args:",
            "            fs_path(str): The hdfs file path.",
            "",
            "        Returns:",
            "            Bool: Whether it's is file or directory, return true if the path exists,",
            "            otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                ret = client.is_exist(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        return self._fs.exist(fs_path)",
            "",
            "    def upload_dir(self, local_dir, dest_dir, overwrite=False):",
            "        \"\"\"",
            "        upload dir to hdfs",
            "        Args:",
            "            local_dir(str): local dir",
            "            dest_dir(str): hdfs dest dir",
            "            overwrite(bool): is overwrite",
            "        Returns:",
            "            return code",
            "        \"\"\"",
            "        local_dir = local_dir.rstrip(\"/\")",
            "        dest_dir = dest_dir.rstrip(\"/\")",
            "        local_basename = os.path.basename(local_dir)",
            "        if self.is_exist(dest_dir + \"/\" + local_basename) and overwrite:",
            "            self.delete(dest_dir + \"/\" + local_basename)",
            "        if not self.is_exist(dest_dir):",
            "            self.mkdirs(dest_dir)",
            "        self._fs.upload(local_dir, dest_dir)",
            "",
            "    # can't retry",
            "    def upload(self, local_path, fs_path, multi_processes=1, overwrite=False):",
            "        \"\"\"",
            "        Upload the local path to remote HDFS.",
            "",
            "        Args:",
            "            local_path(str): The local path.",
            "            fs_path(str): The HDFS path.",
            "            multi_processes(int|1): the upload data process at the same time, default=5",
            "            overwrite(bool|False): will overwrite file on HDFS or not",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.upload(\"test_hdfs_client\", \"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "",
            "        local = LocalFS()",
            "        if not local.is_exist(local_path):",
            "            raise FSFileNotExistsError(f\"{local_path} not exists\")",
            "",
            "        self._fs.upload(local_path, fs_path)",
            "",
            "    def download(self, fs_path, local_path, multi_processes=1, overwrite=False):",
            "        \"\"\"",
            "        Download remote HDFS path to the local.",
            "",
            "        Args:",
            "            fs_path(str):  The HDFS path.",
            "            local_path(str): The local path.",
            "            multi_processes(int|1): the download data process at the same time, default=1",
            "            overwrite(bool): is overwrite",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.download(\"hdfs:/test_hdfs_client\", \"./\")",
            "        \"\"\"",
            "",
            "        def __subprocess_download(local_path, datas):",
            "            \"\"\"",
            "            download file from HDFS",
            "            Args:",
            "                local_path(str): the local file path",
            "                datas(str): the hdfs file path list",
            "            \"\"\"",
            "            for data in datas:",
            "                self._fs.download(local_path, data)",
            "",
            "        if not self.is_exist(fs_path):",
            "            raise FSFileNotExistsError(f\"{fs_path} not exits\")",
            "        # download file",
            "        if self.is_file(fs_path):",
            "            return self._fs.download(local_path, fs_path)",
            "        # download dir",
            "        _, all_filenames = self.ls_dir(fs_path)",
            "        all_files = [fs_path + i for i in all_filenames]",
            "        procs = []",
            "        for i in range(multi_processes):",
            "            process_datas = self._split_files(all_files, i, multi_processes)",
            "            p = multiprocessing.Process(",
            "                target=__subprocess_download, args=(local_path, process_datas)",
            "            )",
            "            procs.append(p)",
            "            p.start()",
            "",
            "        # complete the processes",
            "        for proc in procs:",
            "            proc.join()",
            "",
            "    def mkdirs(self, fs_path):",
            "        \"\"\"",
            "        Create a remote HDFS directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS directory path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.mkdirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            return",
            "        self._fs.mkdir(fs_path)",
            "",
            "    def mv(self, fs_src_path, fs_dst_path, overwrite=False, test_exists=True):",
            "        \"\"\"",
            "        Move a remote HDFS file or directory from `fs_src_path` to `fs_dst_path` .",
            "",
            "        Args:",
            "            fs_src_path(str):  Name of the file or directory, that's needed to be moved.",
            "            fs_dst_path(str):  Name of the file or directory to which to move to.",
            "            overwrite(bool): Whether to re-write `fs_dst_path` if that exists. Default is False.",
            "            test_exists(bool): Check the existence of `fs_src_path` and `fs_dst_path` . When `test_exists` is set true, if `fs_src_path` doesn't exist or `fs_dst_path` exists, program will throw an Excetption.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.mv(\"hdfs:/test_hdfs_client\", \"hdfs:/test_hdfs_client2\")",
            "        \"\"\"",
            "        if overwrite and self.is_exist(fs_dst_path):",
            "            self.delete(fs_dst_path)",
            "",
            "        if test_exists:",
            "            if not self.is_exist(fs_src_path):",
            "                raise FSFileNotExistsError(f\"{fs_src_path} is not exists\")",
            "",
            "            if self.is_exist(fs_dst_path):",
            "                raise FSFileExistsError(f\"{fs_dst_path} exists already\")",
            "",
            "        self._fs.mv(fs_src_path, fs_dst_path)",
            "",
            "    def delete(self, fs_path):",
            "        \"\"\"",
            "        Delete a remote HDFS path, whether it's a file or directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.delete(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return",
            "        self._fs.remove(fs_path)",
            "",
            "    def touch(self, fs_path, exist_ok=True):",
            "        \"\"\"",
            "        Create a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "            exist_ok(bool): When `fs_path` exists, if `exist_ok` is set false,",
            "            program will throw an Exception. Default is true.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.touch(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            if exist_ok:",
            "                return",
            "            raise FSFileExistsError",
            "",
            "        return self._fs.touchz(fs_path)",
            "",
            "    def need_upload_download(self):",
            "        return True",
            "",
            "    def cat(self, fs_path=None):",
            "        \"\"\"",
            "        Cat a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            file content",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.cat(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_file(fs_path):",
            "            return self._fs.cat(fs_path)",
            "        else:",
            "            return \"\"",
            "",
            "    def _split_files(self, files, trainer_id, trainers):",
            "        \"\"\"",
            "        split file list",
            "        Args:",
            "            files(list): file list",
            "            trainer_id(int): trainer mpi rank id",
            "            trainers(int): all trainers num",
            "        Returns:",
            "            fileist(list): file list of current trainer",
            "        \"\"\"",
            "        remainder = len(files) % trainers",
            "        blocksize = len(files) // trainers",
            "",
            "        blocks = [blocksize] * trainers",
            "        for i in range(remainder):",
            "            blocks[i] += 1",
            "",
            "        trainer_files = [[]] * trainers",
            "        begin = 0",
            "        for i in range(trainers):",
            "            trainer_files[i] = files[begin : begin + blocks[i]]",
            "            begin += blocks[i]",
            "",
            "        return trainer_files[trainer_id]"
        ],
        "afterPatchFile": [
            "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import abc",
            "import functools",
            "import multiprocessing",
            "import os",
            "import re",
            "import shutil",
            "import time",
            "",
            "# (TODO: GhostScreaming) It will be removed later.",
            "from paddle.fluid import core",
            "",
            "from .log_util import logger",
            "",
            "__all__ = []",
            "",
            "",
            "class ExecuteError(Exception):",
            "    pass",
            "",
            "",
            "class FSFileExistsError(Exception):",
            "    pass",
            "",
            "",
            "class FSFileNotExistsError(Exception):",
            "    pass",
            "",
            "",
            "class FSTimeOut(Exception):",
            "    pass",
            "",
            "",
            "class FSShellCmdAborted(ExecuteError):",
            "    pass",
            "",
            "",
            "class FS:",
            "    @abc.abstractmethod",
            "    def ls_dir(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def is_file(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def is_dir(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def is_exist(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def upload(self, local_path, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def download(self, fs_path, local_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def mkdirs(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def delete(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def need_upload_download(self):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def rename(self, fs_src_path, fs_dst_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def mv(self, fs_src_path, fs_dst_path, overwrite=False, test_exists=False):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def upload_dir(self, local_dir, dest_dir):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def list_dirs(self, fs_path):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def touch(self, fs_path, exist_ok=True):",
            "        raise NotImplementedError",
            "",
            "    @abc.abstractmethod",
            "    def cat(self, fs_path=None):",
            "        raise NotImplementedError",
            "",
            "",
            "class LocalFS(FS):",
            "    \"\"\"",
            "    A tool of local file system.",
            "",
            "    Examples:",
            "        .. code-block:: python",
            "",
            "            from paddle.distributed.fleet.utils import LocalFS",
            "",
            "            client = LocalFS()",
            "            subdirs, files = client.ls_dir(\"./\")",
            "    \"\"\"",
            "",
            "    def ls_dir(self, fs_path):",
            "        \"\"\"",
            "        List directorys and files under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Tuple: Return a 2-tuple, the first is a list of all its subdirectories,",
            "            and the second is a list of all its subfiles, e.g. ([subdirname1, subdirname1, ...], [filename1, filename2, ...]).",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                subdirs, files = client.ls_dir(\"./\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return [], []",
            "",
            "        dirs = []",
            "        files = []",
            "        for f in os.listdir(fs_path):",
            "            if os.path.isdir(fs_path + \"/\" + f):",
            "                dirs.append(f)",
            "            else:",
            "                files.append(f)",
            "",
            "        return dirs, files",
            "",
            "    def mkdirs(self, fs_path):",
            "        \"\"\"",
            "        Create a local directory.",
            "",
            "        Args:",
            "            fs_path(str): The local directory path.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.mkdirs(\"test_mkdirs\")",
            "                client.delete(\"test_mkdirs\")",
            "        \"\"\"",
            "        assert not os.path.isfile(fs_path), \"{} is already a file\".format(",
            "            fs_path",
            "        )",
            "        os.makedirs(fs_path, exist_ok=True)",
            "",
            "    def rename(self, fs_src_path, fs_dst_path):",
            "        \"\"\"",
            "        Rename the file.",
            "",
            "        Args:",
            "            fs_src_path(str): The actual name of the file or directory",
            "            fs_dst_path(str): The new name of the file or directory.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_rename_src\")",
            "                print(client.is_exists(\"test_rename_src\")) # True",
            "                client.rename(\"test_rename_src\", \"test_rename_dst\")",
            "                print(client.is_exists(\"test_rename_src\")) # False",
            "                print(client.is_exists(\"test_rename_dst\")) # True",
            "                client.delete(\"test_rename_dst\")",
            "        \"\"\"",
            "        os.rename(fs_src_path, fs_dst_path)",
            "",
            "    def _rmr(self, fs_path):",
            "        shutil.rmtree(fs_path)",
            "",
            "    def _rm(self, fs_path):",
            "        os.remove(fs_path)",
            "",
            "    def delete(self, fs_path):",
            "        \"\"\"",
            "        Delete the local file path, whether it's a file or directory.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.mkdirs(\"test_localFS_mkdirs\")",
            "                client.delete(\"test_localFS_mkdirs\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return",
            "",
            "        if os.path.isfile(fs_path):",
            "            return self._rm(fs_path)",
            "",
            "        return self._rmr(fs_path)",
            "",
            "    def need_upload_download(self):",
            "        return False",
            "",
            "    def is_file(self, fs_path):",
            "        \"\"\"",
            "        Whether the local file path is a file.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a file, otherwise return false.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_is_file\")",
            "                print(client.is_file(\"test_is_file\")) # True",
            "                client.delete(\"test_is_file\")",
            "        \"\"\"",
            "        return os.path.isfile(fs_path)",
            "",
            "    def is_dir(self, fs_path):",
            "        \"\"\"",
            "        Whether the local file path is a directory.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a directory, otherwise return false.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.mkdirs(\"test_is_dir\")",
            "                print(client.is_dir(\"test_is_file\")) # True",
            "                client.delete(\"test_is_dir\")",
            "        \"\"\"",
            "        return os.path.isdir(fs_path)",
            "",
            "    def is_exist(self, fs_path):",
            "        \"\"\"",
            "        Whether the local file path exists.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            Bool: Wheter it's a file or directory, return true if the path exists,",
            "            otherwise return false.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                ret = local_fs.is_exist(\"test_is_exist\")",
            "        \"\"\"",
            "        return os.path.exists(fs_path)",
            "",
            "    def touch(self, fs_path, exist_ok=True):",
            "        \"\"\"",
            "        Create a local file.",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "            exist_ok(bool): When `fs_path` exists, if `exist_ok` is set false,",
            "            program will throw an Exception. Default is true.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_touch\")",
            "                client.delete(\"test_touch\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            if exist_ok:",
            "                return",
            "            raise FSFileExistsError",
            "",
            "        os.system(f\"touch {fs_path}\")",
            "",
            "    def mv(self, src_path, dst_path, overwrite=False, test_exists=False):",
            "        \"\"\"",
            "        Move a local file or directory from `src_path` to `dst_path` .",
            "",
            "        Args:",
            "            src_path(str):  Name of the file or directory, that's needed to be moved.",
            "            dst_path(str):  Name of the file or directory to which to move to.",
            "            overwrite(bool): Whether to re-write `dst_path` if that exists. Default is False.",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                client.touch(\"test_mv_src\")",
            "                client.mv(\"test_mv_src\", \"test_mv_dst\")",
            "                client.delete(\"test_mv_dst\")",
            "        \"\"\"",
            "        if not self.is_exist(src_path):",
            "            raise FSFileNotExistsError",
            "",
            "        if overwrite and self.is_exist(dst_path):",
            "            self.delete(dst_path)",
            "",
            "        if self.is_exist(dst_path):",
            "            raise FSFileExistsError",
            "",
            "        return self.rename(src_path, dst_path)",
            "",
            "    def list_dirs(self, fs_path):",
            "        \"\"\"",
            "        Only list directorys under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The local file path.",
            "",
            "        Returns:",
            "            List: A list of all its subdirectories, e.g. [subdirname1, subdirname1, ...].",
            "",
            "        Examples:",
            "            .. code-block:: python",
            "",
            "                from paddle.distributed.fleet.utils import LocalFS",
            "",
            "                client = LocalFS()",
            "                subdirs = client.list_dirs(\"./\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return []",
            "",
            "        dirs = [",
            "            f for f in os.listdir(fs_path) if os.path.isdir(fs_path + \"/\" + f)",
            "        ]",
            "",
            "        return dirs",
            "",
            "",
            "def _handle_errors(max_time_out=None):",
            "    def decorator(f):",
            "        @functools.wraps(f)",
            "        def handler(*args, **kwargs):",
            "            o = args[0]",
            "            time_out = max_time_out",
            "            if time_out is None:",
            "                time_out = float(o._time_out) / 1000.0",
            "            else:",
            "                time_out /= 1000.0",
            "            inter = float(o._sleep_inter) / 1000.0",
            "",
            "            start = time.time()",
            "            last_print_time = start",
            "            while True:",
            "                try:",
            "                    return f(*args, **kwargs)",
            "                # important: only ExecuteError need to retry",
            "                except ExecuteError as e:",
            "                    if time.time() - start >= time_out:",
            "                        raise FSTimeOut(",
            "                            \"args:{} timeout:{}\".format(",
            "                                args, time.time() - start",
            "                            )",
            "                        )",
            "",
            "                    time.sleep(inter)",
            "",
            "                if time.time() - last_print_time > 30:",
            "                    print(",
            "                        \"hadoop operator timeout:args:{} timeout:{}\".format(",
            "                            args, time.time() - start",
            "                        )",
            "                    )",
            "                    last_print_time = time.time()",
            "",
            "        return handler",
            "",
            "    return decorator",
            "",
            "",
            "class HDFSClient(FS):",
            "    \"\"\"",
            "    A tool of HDFS.",
            "",
            "    Args:",
            "        hadoop_home(str): Hadoop home.",
            "        configs(dict): Hadoop config. It is a dictionary and needs to contain the",
            "            keys: \"fs.default.name\" and \"hadoop.job.ugi\".",
            "",
            "    Examples:",
            "",
            "        .. code-block:: text",
            "",
            "            from paddle.distributed.fleet.utils import HDFSClient",
            "            hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "",
            "            configs = {",
            "                \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                \"hadoop.job.ugi\": \"hello,hello123\"",
            "            }",
            "",
            "            client = HDFSClient(hadoop_home, configs)",
            "            client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "    \"\"\"",
            "",
            "    def __init__(",
            "        self,",
            "        hadoop_home,",
            "        configs,",
            "        time_out=5 * 60 * 1000,  # ms",
            "        sleep_inter=1000,",
            "    ):  # ms",
            "        self.pre_commands = []",
            "        hadoop_bin = '%s/bin/hadoop' % hadoop_home",
            "        self.pre_commands.append(hadoop_bin)",
            "        dfs = 'fs'",
            "        self.pre_commands.append(dfs)",
            "",
            "        if configs:",
            "            for k, v in configs.items():",
            "                config_command = f'-D{k}={v}'",
            "                self.pre_commands.append(config_command)",
            "",
            "        self._time_out = time_out",
            "        self._sleep_inter = sleep_inter",
            "        self._base_cmd = \" \".join(self.pre_commands)",
            "        self._bd_err_re = re.compile(",
            "            r'\\s?responseErrorMsg\\s?\\:.*, errorCode\\:\\s?[0-9]+, path\\:'",
            "        )",
            "",
            "    def _run_cmd(self, cmd, redirect_stderr=False, retry_times=5):",
            "        exe_cmd = f\"{self._base_cmd} -{cmd}\"",
            "        ret = 0",
            "        output = None",
            "        retry_sleep_second = 3",
            "        for x in range(retry_times + 1):",
            "            ret, output = core.shell_execute_cmd(exe_cmd, 0, 0, redirect_stderr)",
            "            ret = int(ret)",
            "            if ret == 0:",
            "                break",
            "            time.sleep(retry_sleep_second)",
            "        if ret == 134:",
            "            raise FSShellCmdAborted(cmd)",
            "",
            "        return ret, output.splitlines()",
            "",
            "    @_handle_errors()",
            "    def list_dirs(self, fs_path):",
            "        \"\"\"",
            "        Only list directorys under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            List: A list of all its subdirectories, e.g. [subdirname1, subdirname1, ...].",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                subdirs = client.list_dirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return []",
            "",
            "        dirs, files = self._ls_dir(fs_path)",
            "        return dirs",
            "",
            "    @_handle_errors()",
            "    def ls_dir(self, fs_path):",
            "        \"\"\"",
            "        List directorys and files under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Tuple: Return a 2-tuple, the first element is the list of all its subdirectories,",
            "            and the second one is the list of all its subfiles, e.g. ([subdirname1, subdirname1, ...], [filename1, filename2, ...]).",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                subdirs, files = client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return [], []",
            "",
            "        return self._ls_dir(fs_path)",
            "",
            "    def _ls_dir(self, fs_path):",
            "        cmd = f\"ls {fs_path}\"",
            "        ret, lines = self._run_cmd(cmd)",
            "",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "        dirs = []",
            "        files = []",
            "        for line in lines:",
            "            arr = line.split()",
            "            if len(arr) != 8:",
            "                continue",
            "",
            "            p = os.path.basename(arr[7])",
            "            if arr[0][0] == 'd':",
            "                dirs.append(p)",
            "            else:",
            "                files.append(p)",
            "",
            "        return dirs, files",
            "",
            "    def _test_match(self, lines):",
            "        for l in lines:",
            "            m = self._bd_err_re.match(l)",
            "            if m is not None:",
            "                return m",
            "",
            "        return None",
            "",
            "    @_handle_errors()",
            "    def is_dir(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a directory, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return self._is_dir(fs_path)",
            "",
            "    def _is_dir(self, fs_path):",
            "        cmd = f\"test -d {fs_path}\"",
            "        ret, lines = self._run_cmd(cmd, redirect_stderr=True, retry_times=1)",
            "        if ret:",
            "            # other error",
            "            if self._test_match(lines):",
            "                print('raise exception: ')",
            "                print('\\n'.join(lines))",
            "                raise ExecuteError(cmd)",
            "",
            "            return False",
            "",
            "        return True",
            "",
            "    def is_file(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a file, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return not self._is_dir(fs_path)",
            "",
            "    @_handle_errors()",
            "    def is_exist(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path exists.",
            "",
            "        Args:",
            "            fs_path(str): The hdfs file path.",
            "",
            "        Returns:",
            "            Bool: Whether it's is file or directory, return true if the path exists,",
            "            otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                ret = client.is_exist(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        cmd = f\"test -e {fs_path} \"",
            "        ret, out = self._run_cmd(cmd, redirect_stderr=True, retry_times=1)",
            "        if ret != 0:",
            "            return False",
            "",
            "        return True",
            "",
            "    def upload_dir(self, local_dir, dest_dir, overwrite=False):",
            "        \"\"\"",
            "        upload dir to hdfs",
            "        Args:",
            "            local_dir(str): local dir",
            "            dest_dir(str): hdfs dest dir",
            "            overwrite(bool): is overwrite",
            "        Returns:",
            "            return code",
            "        \"\"\"",
            "        local_dir = local_dir.rstrip(\"/\")",
            "        dest_dir = dest_dir.rstrip(\"/\")",
            "        local_basename = os.path.basename(local_dir)",
            "        if self.is_exist(dest_dir + \"/\" + local_basename) and overwrite:",
            "            self.delete(dest_dir + \"/\" + local_basename)",
            "        if not self.is_exist(dest_dir):",
            "            self.mkdirs(dest_dir)",
            "        self._try_upload(local_dir, dest_dir)",
            "",
            "    # can't retry",
            "    def upload(self, local_path, fs_path, multi_processes=5, overwrite=False):",
            "        \"\"\"",
            "        Upload the local path to remote HDFS.",
            "",
            "        Args:",
            "            local_path(str): The local path.",
            "            fs_path(str): The HDFS path.",
            "            multi_processes(int|1): the upload data process at the same time, default=5",
            "            overwrite(bool|False): will overwrite file on HDFS or not",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.upload(\"test_hdfs_client\", \"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "",
            "        def __subprocess_upload(hdfs_path_single, datas):",
            "            for data in datas:",
            "                self._try_upload(data, hdfs_path_single)",
            "",
            "        def get_local_files(path):",
            "            \"\"\"",
            "            get local files",
            "            Args:",
            "                path(str): local path",
            "            Returns:",
            "                list of local files",
            "            \"\"\"",
            "            rlist = []",
            "",
            "            if not os.path.exists(path):",
            "                return rlist",
            "",
            "            if os.path.isdir(path):",
            "                for file in os.listdir(path):",
            "                    t = os.path.join(path, file)",
            "                    rlist.append(t)",
            "            else:",
            "                rlist.append(path)",
            "            return rlist",
            "",
            "        local = LocalFS()",
            "        if not local.is_exist(local_path):",
            "            raise FSFileNotExistsError(f\"{local_path} not exists\")",
            "",
            "        all_files = get_local_files(local_path)",
            "        if not all_files:",
            "            print(\"there are nothing need to upload, function exit\")",
            "            return",
            "",
            "        if self.is_exist(fs_path) and overwrite:",
            "            self.delete(fs_path)",
            "            self.mkdirs(fs_path)",
            "",
            "        procs = []",
            "        for i in range(multi_processes):",
            "            process_datas = self._split_files(all_files, i, multi_processes)",
            "            p = multiprocessing.Process(",
            "                target=__subprocess_upload, args=(fs_path, process_datas)",
            "            )",
            "            procs.append(p)",
            "            p.start()",
            "",
            "        # complete the processes",
            "        for proc in procs:",
            "            proc.join()",
            "",
            "    @_handle_errors()",
            "    def _try_upload(self, local_path, fs_path):",
            "        cmd = f\"put {local_path} {fs_path}\"",
            "        ret = 0",
            "        try:",
            "            ret, _ = self._run_cmd(cmd)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "        except Exception as e:",
            "            self.delete(fs_path)",
            "            raise e",
            "",
            "    # can't retry",
            "    def download(self, fs_path, local_path, multi_processes=5, overwrite=False):",
            "        \"\"\"",
            "        Download remote HDFS path to the local.",
            "",
            "        Args:",
            "            fs_path(str):  The HDFS path.",
            "            local_path(str): The local path.",
            "            multi_processes(int|1): the download data process at the same time, default=1",
            "            overwrite(bool): is overwrite",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.download(\"hdfs:/test_hdfs_client\", \"./\")",
            "        \"\"\"",
            "",
            "        def __subprocess_download(local_path, datas):",
            "            \"\"\"",
            "            download file from HDFS",
            "            Args:",
            "                local_path(str): the local file path",
            "                datas(str): the hdfs file path list",
            "            \"\"\"",
            "            for data in datas:",
            "                self._try_download(data, local_path)",
            "",
            "        if not self.is_exist(fs_path):",
            "            raise FSFileNotExistsError(f\"{fs_path} not exits\")",
            "        # download file",
            "        if self.is_file(fs_path):",
            "            return self._try_download(fs_path, local_path)",
            "        # download dir",
            "        dirs, all_filenames = self.ls_dir(fs_path)",
            "        all_files = [fs_path + \"/\" + i for i in all_filenames]",
            "        all_files.extend([fs_path + \"/\" + i for i in dirs])",
            "        procs = []",
            "        for i in range(multi_processes):",
            "            process_datas = self._split_files(all_files, i, multi_processes)",
            "            p = multiprocessing.Process(",
            "                target=__subprocess_download, args=(local_path, process_datas)",
            "            )",
            "            procs.append(p)",
            "            p.start()",
            "",
            "        # complete the processes",
            "        for proc in procs:",
            "            proc.join()",
            "",
            "    @_handle_errors()",
            "    def _try_download(self, fs_path, local_path):",
            "        cmd = f\"get {fs_path} {local_path}\"",
            "        ret = 0",
            "        try:",
            "            ret, _ = self._run_cmd(cmd)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "        except Exception as e:",
            "            local_fs = LocalFS()",
            "            local_fs.delete(local_path)",
            "            raise e",
            "",
            "    @_handle_errors()",
            "    def mkdirs(self, fs_path):",
            "        \"\"\"",
            "        Create a remote HDFS directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS directory path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.mkdirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            return",
            "",
            "        out_hdfs = False",
            "",
            "        cmd = f\"mkdir {fs_path} \"",
            "        ret, out = self._run_cmd(cmd, redirect_stderr=True)",
            "        if ret != 0:",
            "            for l in out:",
            "                if \"No such file or directory\" in l:",
            "                    out_hdfs = True",
            "                    break",
            "            if not out_hdfs:",
            "                raise ExecuteError(cmd)",
            "",
            "        if out_hdfs and not self.is_exist(fs_path):",
            "            cmd = f\"mkdir -p {fs_path}\"",
            "            ret, _ = self._run_cmd(cmd)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "",
            "    def mv(self, fs_src_path, fs_dst_path, overwrite=False, test_exists=True):",
            "        \"\"\"",
            "        Move a remote HDFS file or directory from `fs_src_path` to `fs_dst_path` .",
            "",
            "        Args:",
            "            fs_src_path(str):  Name of the file or directory, that's needed to be moved.",
            "            fs_dst_path(str):  Name of the file or directory to which to move to.",
            "            overwrite(bool): Whether to re-write `fs_dst_path` if that exists. Default is False.",
            "            test_exists(bool): Check the existence of `fs_src_path` and `fs_dst_path` . When `test_exists` is set true, if `fs_src_path` doesn't exist or `fs_dst_path` exists, program will throw an Excetption.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.mv(\"hdfs:/test_hdfs_client\", \"hdfs:/test_hdfs_client2\")",
            "        \"\"\"",
            "        if overwrite and self.is_exist(fs_dst_path):",
            "            self.delete(fs_dst_path)",
            "",
            "        if test_exists:",
            "            if not self.is_exist(fs_src_path):",
            "                raise FSFileNotExistsError(f\"{fs_src_path} is not exists\")",
            "",
            "            if self.is_exist(fs_dst_path):",
            "                raise FSFileExistsError(f\"{fs_dst_path} exists already\")",
            "",
            "        return self._try_mv(fs_src_path, fs_dst_path)",
            "",
            "    @_handle_errors()",
            "    def _try_mv(self, fs_src_path, fs_dst_path):",
            "        cmd = f\"mv {fs_src_path} {fs_dst_path}\"",
            "        ret = 0",
            "        try:",
            "            ret, _ = self._run_cmd(cmd, retry_times=1)",
            "            if ret != 0:",
            "                raise ExecuteError(cmd)",
            "        except Exception as e:",
            "            if not self.is_exist(fs_src_path) and self.is_exist(fs_dst_path):",
            "                return",
            "            raise e",
            "",
            "    def _rmr(self, fs_path):",
            "        cmd = f\"rmr {fs_path}\"",
            "        ret, _ = self._run_cmd(cmd)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "    def _rm(self, fs_path):",
            "        cmd = f\"rm {fs_path}\"",
            "        ret, _ = self._run_cmd(cmd)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "    @_handle_errors()",
            "    def delete(self, fs_path):",
            "        \"\"\"",
            "        Delete a remote HDFS path, whether it's a file or directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.delete(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return",
            "",
            "        is_dir = self._is_dir(fs_path)",
            "        if is_dir:",
            "            return self._rmr(fs_path)",
            "",
            "        return self._rm(fs_path)",
            "",
            "    def touch(self, fs_path, exist_ok=True):",
            "        \"\"\"",
            "        Create a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "            exist_ok(bool): When `fs_path` exists, if `exist_ok` is set false,",
            "            program will throw an Exception. Default is true.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.touch(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            if exist_ok:",
            "                return",
            "            raise FSFileExistsError",
            "",
            "        return self._touchz(fs_path)",
            "",
            "    @_handle_errors()",
            "    def _touchz(self, fs_path):",
            "        cmd = f\"touchz {fs_path}\"",
            "        ret, _ = self._run_cmd(cmd)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "",
            "    def need_upload_download(self):",
            "        return True",
            "",
            "    def cat(self, fs_path=None):",
            "        \"\"\"",
            "        Cat a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            file content",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                hadoop_home = \"/home/client/hadoop-client/hadoop/\"",
            "                configs = {",
            "                    \"fs.default.name\": \"hdfs://xxx.hadoop.com:54310\",",
            "                    \"hadoop.job.ugi\": \"hello,hello123\"",
            "                }",
            "",
            "                client = HDFSClient(hadoop_home, configs)",
            "                client.cat(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_file(fs_path):",
            "            output = self._try_cat(fs_path)",
            "            return \"\\n\".join(output)",
            "        else:",
            "            return \"\"",
            "",
            "    @_handle_errors()",
            "    def _try_cat(self, fs_path):",
            "        cmd = f\"cat {fs_path}\"",
            "        ret, output = self._run_cmd(cmd, retry_times=1)",
            "        if ret != 0:",
            "            raise ExecuteError(cmd)",
            "        return output",
            "",
            "    def _split_files(self, files, trainer_id, trainers):",
            "        \"\"\"",
            "        split file list",
            "        Args:",
            "            files(list): file list",
            "            trainer_id(int): trainer mpi rank id",
            "            trainers(int): all trainers num",
            "        Returns:",
            "            fileist(list): file list of current trainer",
            "        \"\"\"",
            "        remainder = len(files) % trainers",
            "        blocksize = len(files) // trainers",
            "",
            "        blocks = [blocksize] * trainers",
            "        for i in range(remainder):",
            "            blocks[i] += 1",
            "",
            "        trainer_files = [[]] * trainers",
            "        begin = 0",
            "        for i in range(trainers):",
            "            trainer_files[i] = files[begin : begin + blocks[i]]",
            "            begin += blocks[i]",
            "",
            "        return trainer_files[trainer_id]",
            "",
            "    def list_files_info(self, path_list):",
            "        \"\"\"",
            "        list_files return file path and size",
            "        Args:",
            "            path_list(list): file list",
            "        Returns:",
            "            fileist(list): file list with file path and size",
            "        \"\"\"",
            "        if len(path_list) <= 0:",
            "            return []",
            "",
            "        file_list = []",
            "",
            "        # concat filelist can speed up 'hadoop ls'",
            "        str_concat = \"\"",
            "        for path in path_list:",
            "            str_concat += path + \" \"",
            "        cmd = (",
            "            \"ls \" + str_concat + \" | awk '{if ($8 != \\\"\\\") {print $5\\\" \\\"$8 }}'\"",
            "        )",
            "        ret, lines = self._run_cmd(cmd)",
            "        if len(lines) == 0:",
            "            logger.warning(\"list_files empty, path[%s]\" % path_list)",
            "            return []",
            "        for line in lines:",
            "            arr = line.split(' ')",
            "            if len(arr) < 2:",
            "                continue",
            "            file_path = arr[1]",
            "            file_size = int(arr[0])",
            "            file_list.append({'path': file_path, 'size': file_size})",
            "",
            "        return file_list",
            "",
            "",
            "class AFSClient(FS):",
            "    \"\"\"",
            "    A tool of AFS. Use AfsWrapper.",
            "",
            "    Examples:",
            "",
            "        .. code-block:: text",
            "",
            "            from paddle.distributed.fleet.utils import AFSClient",
            "            client = AFSClient()",
            "            client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "            client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "    \"\"\"",
            "",
            "    def __init__(self, time_out=5 * 60 * 1000, sleep_inter=1000):  # ms  # ms",
            "        self._fs = core.AfsWrapper()",
            "        self._time_out = time_out",
            "",
            "    def init(self, fs_name, fs_user, fs_passwd, fs_conf):",
            "        self._fs.init(fs_name, fs_user, fs_passwd, fs_conf)",
            "",
            "    def list_dirs(self, fs_path):",
            "        \"\"\"",
            "        Only list directorys under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            List: A list of all its subdirectories, e.g. [subdirname1, subdirname1, ...].",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                subdirs = client.list_dirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return []",
            "        # TODO:fengdanlei",
            "        dirs, files = self._ls_dir(fs_path)",
            "        return dirs",
            "",
            "    def ls_dir(self, fs_path):",
            "        \"\"\"",
            "        List directorys and files under `fs_path` .",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Tuple: Return a 2-tuple, the first element is the list of all its subdirectories,",
            "            and the second one is the list of all its subfiles, e.g. ([subdirname1, subdirname1, ...], [filename1, filename2, ...]).",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                subdirs, files = client.ls_dir(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return [], []",
            "",
            "        return self._ls_dir(fs_path)",
            "",
            "    def _ls_dir(self, fs_path):",
            "",
            "        files = self._fs.list(fs_path)",
            "        dirs = [fs_path]",
            "        return dirs, files",
            "",
            "    def is_dir(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a directory, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return self._is_dir(fs_path)",
            "",
            "    def _is_dir(self, fs_path):",
            "        list_path = self._fs.list(fs_path)",
            "        if (len(list_path)) > 0:",
            "            return True",
            "        else:",
            "            return False",
            "",
            "    def is_file(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path is a file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            Bool: Return true if the path exists and it's a file, otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                ret = client.is_file(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return False",
            "",
            "        return not self._is_dir(fs_path)",
            "",
            "    def is_exist(self, fs_path):",
            "        \"\"\"",
            "        Whether the remote HDFS path exists.",
            "",
            "        Args:",
            "            fs_path(str): The hdfs file path.",
            "",
            "        Returns:",
            "            Bool: Whether it's is file or directory, return true if the path exists,",
            "            otherwise return false.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                ret = client.is_exist(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        return self._fs.exist(fs_path)",
            "",
            "    def upload_dir(self, local_dir, dest_dir, overwrite=False):",
            "        \"\"\"",
            "        upload dir to hdfs",
            "        Args:",
            "            local_dir(str): local dir",
            "            dest_dir(str): hdfs dest dir",
            "            overwrite(bool): is overwrite",
            "        Returns:",
            "            return code",
            "        \"\"\"",
            "        local_dir = local_dir.rstrip(\"/\")",
            "        dest_dir = dest_dir.rstrip(\"/\")",
            "        local_basename = os.path.basename(local_dir)",
            "        if self.is_exist(dest_dir + \"/\" + local_basename) and overwrite:",
            "            self.delete(dest_dir + \"/\" + local_basename)",
            "        if not self.is_exist(dest_dir):",
            "            self.mkdirs(dest_dir)",
            "        self._fs.upload(local_dir, dest_dir)",
            "",
            "    # can't retry",
            "    def upload(self, local_path, fs_path, multi_processes=1, overwrite=False):",
            "        \"\"\"",
            "        Upload the local path to remote HDFS.",
            "",
            "        Args:",
            "            local_path(str): The local path.",
            "            fs_path(str): The HDFS path.",
            "            multi_processes(int|1): the upload data process at the same time, default=5",
            "            overwrite(bool|False): will overwrite file on HDFS or not",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.upload(\"test_hdfs_client\", \"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "",
            "        local = LocalFS()",
            "        if not local.is_exist(local_path):",
            "            raise FSFileNotExistsError(f\"{local_path} not exists\")",
            "",
            "        self._fs.upload(local_path, fs_path)",
            "",
            "    def download(self, fs_path, local_path, multi_processes=1, overwrite=False):",
            "        \"\"\"",
            "        Download remote HDFS path to the local.",
            "",
            "        Args:",
            "            fs_path(str):  The HDFS path.",
            "            local_path(str): The local path.",
            "            multi_processes(int|1): the download data process at the same time, default=1",
            "            overwrite(bool): is overwrite",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.download(\"hdfs:/test_hdfs_client\", \"./\")",
            "        \"\"\"",
            "",
            "        def __subprocess_download(local_path, datas):",
            "            \"\"\"",
            "            download file from HDFS",
            "            Args:",
            "                local_path(str): the local file path",
            "                datas(str): the hdfs file path list",
            "            \"\"\"",
            "            for data in datas:",
            "                self._fs.download(local_path, data)",
            "",
            "        if not self.is_exist(fs_path):",
            "            raise FSFileNotExistsError(f\"{fs_path} not exits\")",
            "        # download file",
            "        if self.is_file(fs_path):",
            "            return self._fs.download(local_path, fs_path)",
            "        # download dir",
            "        _, all_filenames = self.ls_dir(fs_path)",
            "        all_files = [fs_path + i for i in all_filenames]",
            "        procs = []",
            "        for i in range(multi_processes):",
            "            process_datas = self._split_files(all_files, i, multi_processes)",
            "            p = multiprocessing.Process(",
            "                target=__subprocess_download, args=(local_path, process_datas)",
            "            )",
            "            procs.append(p)",
            "            p.start()",
            "",
            "        # complete the processes",
            "        for proc in procs:",
            "            proc.join()",
            "",
            "    def mkdirs(self, fs_path):",
            "        \"\"\"",
            "        Create a remote HDFS directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS directory path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.mkdirs(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            return",
            "        self._fs.mkdir(fs_path)",
            "",
            "    def mv(self, fs_src_path, fs_dst_path, overwrite=False, test_exists=True):",
            "        \"\"\"",
            "        Move a remote HDFS file or directory from `fs_src_path` to `fs_dst_path` .",
            "",
            "        Args:",
            "            fs_src_path(str):  Name of the file or directory, that's needed to be moved.",
            "            fs_dst_path(str):  Name of the file or directory to which to move to.",
            "            overwrite(bool): Whether to re-write `fs_dst_path` if that exists. Default is False.",
            "            test_exists(bool): Check the existence of `fs_src_path` and `fs_dst_path` . When `test_exists` is set true, if `fs_src_path` doesn't exist or `fs_dst_path` exists, program will throw an Excetption.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.mv(\"hdfs:/test_hdfs_client\", \"hdfs:/test_hdfs_client2\")",
            "        \"\"\"",
            "        if overwrite and self.is_exist(fs_dst_path):",
            "            self.delete(fs_dst_path)",
            "",
            "        if test_exists:",
            "            if not self.is_exist(fs_src_path):",
            "                raise FSFileNotExistsError(f\"{fs_src_path} is not exists\")",
            "",
            "            if self.is_exist(fs_dst_path):",
            "                raise FSFileExistsError(f\"{fs_dst_path} exists already\")",
            "",
            "        self._fs.mv(fs_src_path, fs_dst_path)",
            "",
            "    def delete(self, fs_path):",
            "        \"\"\"",
            "        Delete a remote HDFS path, whether it's a file or directory.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import HDFSClient",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.delete(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if not self.is_exist(fs_path):",
            "            return",
            "        self._fs.remove(fs_path)",
            "",
            "    def touch(self, fs_path, exist_ok=True):",
            "        \"\"\"",
            "        Create a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "            exist_ok(bool): When `fs_path` exists, if `exist_ok` is set false,",
            "            program will throw an Exception. Default is true.",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.touch(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_exist(fs_path):",
            "            if exist_ok:",
            "                return",
            "            raise FSFileExistsError",
            "",
            "        return self._fs.touchz(fs_path)",
            "",
            "    def need_upload_download(self):",
            "        return True",
            "",
            "    def cat(self, fs_path=None):",
            "        \"\"\"",
            "        Cat a remote HDFS file.",
            "",
            "        Args:",
            "            fs_path(str): The HDFS file path.",
            "",
            "        Returns:",
            "            file content",
            "",
            "        Examples:",
            "",
            "            .. code-block:: text",
            "",
            "                from paddle.distributed.fleet.utils import AFSClient",
            "",
            "                client = AFSClient()",
            "                client.init(\"hdfs://xxx.hadoop.com:54310\", \"hello\", \"hello123\", \"./fs_conf\")",
            "                client.cat(\"hdfs:/test_hdfs_client\")",
            "        \"\"\"",
            "        if self.is_file(fs_path):",
            "            return self._fs.cat(fs_path)",
            "        else:",
            "            return \"\"",
            "",
            "    def _split_files(self, files, trainer_id, trainers):",
            "        \"\"\"",
            "        split file list",
            "        Args:",
            "            files(list): file list",
            "            trainer_id(int): trainer mpi rank id",
            "            trainers(int): all trainers num",
            "        Returns:",
            "            fileist(list): file list of current trainer",
            "        \"\"\"",
            "        remainder = len(files) % trainers",
            "        blocksize = len(files) // trainers",
            "",
            "        blocks = [blocksize] * trainers",
            "        for i in range(remainder):",
            "            blocks[i] += 1",
            "",
            "        trainer_files = [[]] * trainers",
            "        begin = 0",
            "        for i in range(trainers):",
            "            trainer_files[i] = files[begin : begin + blocks[i]]",
            "            begin += blocks[i]",
            "",
            "        return trainer_files[trainer_id]"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "177": [
                "LocalFS",
                "mkdirs"
            ]
        },
        "addLocation": []
    },
    "python/paddle/distributed/utils/launch_utils.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 485,
                "afterPatchRowNumber": 485,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 486,
                "afterPatchRowNumber": 486,
                "PatchRowcode": "         fn = None"
            },
            "2": {
                "beforePatchRowNumber": 487,
                "afterPatchRowNumber": 487,
                "PatchRowcode": "         if log_dir is not None:"
            },
            "3": {
                "beforePatchRowNumber": 488,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-            os.system(f\"mkdir -p {log_dir}\")"
            },
            "4": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 488,
                "PatchRowcode": "+            os.makedirs(log_dir, exist_ok=True)"
            },
            "5": {
                "beforePatchRowNumber": 489,
                "afterPatchRowNumber": 489,
                "PatchRowcode": "             fn = open(\"%s/workerlog.%d\" % (log_dir, idx), \"a\")"
            },
            "6": {
                "beforePatchRowNumber": 490,
                "afterPatchRowNumber": 490,
                "PatchRowcode": "             proc = subprocess.Popen(cmd, env=current_env, stdout=fn, stderr=fn)"
            },
            "7": {
                "beforePatchRowNumber": 491,
                "afterPatchRowNumber": 491,
                "PatchRowcode": "         else:"
            }
        },
        "frontPatchFile": [
            "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import copy",
            "import os",
            "import signal",
            "import socket",
            "import subprocess",
            "import sys",
            "import time",
            "from contextlib import closing",
            "from distutils.util import strtobool",
            "",
            "from paddle.distributed.fleet.launch_utils import get_backend_by_compile_flag",
            "",
            "from ..utils.log_utils import get_logger",
            "",
            "logger = get_logger(\"INFO\", \"root\")",
            "",
            "",
            "def get_cluster_from_args(args, selected_gpus):",
            "    node_ips = [x.strip() for x in args.cluster_node_ips.split(',')]",
            "    node_ip = args.node_ip",
            "    node_rank = node_ips.index(node_ip)",
            "",
            "    logger.debug(",
            "        \"parsed from args:node_ips:{} node_ip:{} node_rank:{}\".format(",
            "            node_ips, node_ip, node_rank",
            "        )",
            "    )",
            "",
            "    free_ports = None",
            "    if (",
            "        not args.use_paddlecloud",
            "        and len(node_ips) <= 1",
            "        and args.started_port is None",
            "    ):",
            "        free_ports = find_free_ports(len(selected_gpus))",
            "        if free_ports is not None:",
            "            free_ports = list(free_ports)",
            "    else:",
            "        started_port = 6070",
            "        if args.started_port is not None:",
            "            started_port = args.started_port",
            "",
            "        free_ports = list(",
            "            range(started_port, started_port + len(selected_gpus))",
            "        )",
            "",
            "    trainer_endpoints = []",
            "    for ip in node_ips:",
            "        trainer_endpoints.append([\"%s:%d\" % (ip, port) for port in free_ports])",
            "    return get_cluster(node_ips, node_ip, trainer_endpoints, selected_gpus)",
            "",
            "",
            "def get_gpus(selected_gpus):",
            "    if selected_gpus is None:",
            "        from paddle.framework import core",
            "",
            "        gpus_num = core.get_cuda_device_count()",
            "        gpus = [str(x) for x in range(0, gpus_num)]",
            "    else:",
            "        cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\")",
            "        if cuda_visible_devices is None or cuda_visible_devices == \"\":",
            "            gpus = [x.strip() for x in selected_gpus.split(',')]",
            "        else:",
            "            # change selected_gpus into relative values",
            "            # e.g. CUDA_VISIBLE_DEVICES=4,5,6,7; args.selected_gpus=4,5,6,7;",
            "            # therefore selected_gpus=0,1,2,3",
            "            cuda_visible_devices_list = cuda_visible_devices.split(',')",
            "            for x in selected_gpus.split(','):",
            "                assert x in cuda_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your selected_gpus %s in CUDA_VISIBLE_DEVICES[%s].\"",
            "                    % (x, cuda_visible_devices)",
            "                )",
            "            gpus = [",
            "                cuda_visible_devices_list.index(x.strip())",
            "                for x in selected_gpus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_gpus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"CUDA_VISIBLE_DEVICES:{}\".format(",
            "                    selected_gpus, gpus, cuda_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return gpus",
            "",
            "",
            "class Hdfs:",
            "    def __init__(self):",
            "        self.hdfs_ugi = None",
            "        self.hdfs_name = None",
            "        self.hdfs_path = None",
            "",
            "    def is_valid(self):",
            "        return (",
            "            self.hdfs_ugi is not None",
            "            and self.hdfs_name is not None",
            "            and self.hdfs_path is not None",
            "        )",
            "",
            "    def __str__(self):",
            "        return \"hdfs_ugi:{} hdfs_name:{} hdfs_path{}\".format(",
            "            self.hdfs_ugi, self.hdfs_name, self.hdfs_path",
            "        )",
            "",
            "    def __eq__(self, n):",
            "        return (",
            "            self.hdfs_ugi == n.hdfs_ugi",
            "            and self.hdfs_name == n.hdfs_name",
            "            and self.hdfs_path == n.hdfs_path",
            "        )",
            "",
            "    def __ne__(self, n):",
            "        return not self == n",
            "",
            "",
            "class Cluster:",
            "    def __init__(self, hdfs):",
            "        self.job_server = None",
            "        self.pods = []",
            "        self.hdfs = None",
            "        self.job_stage_flag = None",
            "",
            "    def __str__(self):",
            "        return \"job_server:{} pods:{} job_stage_flag:{} hdfs:{}\".format(",
            "            self.job_server,",
            "            [str(pod) for pod in self.pods],",
            "            self.job_stage_flag,",
            "            self.hdfs,",
            "        )",
            "",
            "    def __eq__(self, cluster):",
            "        if len(self.pods) != len(cluster.pods):",
            "            return False",
            "",
            "        for a, b in zip(self.pods, cluster.pods):",
            "            if a != b:",
            "                return False",
            "",
            "        if self.job_stage_flag != cluster.job_stage_flag:",
            "            return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, cluster):",
            "        return not self.__eq__(cluster)",
            "",
            "    def update_pods(self, cluster):",
            "        self.pods = copy.copy(cluster.pods)",
            "",
            "    def trainers_nranks(self):",
            "        return len(self.trainers_endpoints())",
            "",
            "    def pods_nranks(self):",
            "        return len(self.pods)",
            "",
            "    def trainers_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            for t in pod.trainers:",
            "                r.append(t.endpoint)",
            "        return r",
            "",
            "    def pods_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            ep = f\"{pod.addr}:{pod.port}\"",
            "            assert (",
            "                pod.port is not None and pod.addr is not None",
            "            ), f\"{ep} not a valid endpoint\"",
            "            r.append(ep)",
            "",
            "        return r",
            "",
            "    def get_pod_by_id(self, pod_id):",
            "        for pod in self.pods:",
            "            if str(pod_id) == str(pod.id):",
            "                return pod",
            "",
            "        return None",
            "",
            "",
            "class JobServer:",
            "    def __init__(self):",
            "        self.endpoint = None",
            "",
            "    def __str__(self):",
            "        return f\"{self.endpoint}\"",
            "",
            "    def __eq__(self, j):",
            "        return self.endpint == j.endpoint",
            "",
            "    def __ne__(self, j):",
            "        return not self == j",
            "",
            "",
            "class Trainer:",
            "    def __init__(self):",
            "        self.gpus = []",
            "        self.endpoint = None",
            "        self.rank = None",
            "",
            "    def __str__(self):",
            "        return \"gpu:{} endpoint:{} rank:{}\".format(",
            "            self.gpus, self.endpoint, self.rank",
            "        )",
            "",
            "    def __eq__(self, t):",
            "        if len(self.gpus) != len(t.gpus):",
            "            return False",
            "",
            "        if self.endpoint != t.endpoint or self.rank != t.rank:",
            "            return False",
            "",
            "        for a, b in zip(self.gpus, t.gpus):",
            "            if a != b:",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, t):",
            "        return not self == t",
            "",
            "    def get_rank(self):",
            "        return self.rank",
            "",
            "",
            "class Pod:",
            "    def __init__(self):",
            "        self.rank = None",
            "        self.id = None",
            "        self.addr = None",
            "        self.port = None",
            "        self.trainers = []",
            "        self.gpus = []",
            "",
            "    def __str__(self):",
            "        return (",
            "            \"rank:{} id:{} addr:{} port:{} visible_gpu:{} trainers:{}\".format(",
            "                self.rank,",
            "                self.id,",
            "                self.addr,",
            "                self.port,",
            "                self.gpus,",
            "                [str(t) for t in self.trainers],",
            "            )",
            "        )",
            "",
            "    def __eq__(self, pod):",
            "        if (",
            "            self.rank != pod.rank",
            "            or self.id != pod.id",
            "            or self.addr != pod.addr",
            "            or self.port != pod.port",
            "        ):",
            "            logger.debug(f\"pod {self} != {pod}\")",
            "            return False",
            "",
            "        if len(self.trainers) != len(pod.trainers):",
            "            logger.debug(f\"trainers {self.trainers} != {pod.trainers}\")",
            "            return False",
            "",
            "        for i in range(len(self.trainers)):",
            "            if self.trainers[i] != pod.trainers[i]:",
            "                logger.debug(f\"trainer {self.trainers[i]} != {pod.trainers[i]}\")",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, pod):",
            "        return not self == pod",
            "",
            "    def parse_response(self, res_pods):",
            "        pass",
            "",
            "    def get_visible_gpus(self):",
            "        r = \"\"",
            "        for g in self.gpus:",
            "            r += f\"{g},\"",
            "",
            "        assert r != \"\", f\"this pod {self} can't see any gpus\"",
            "",
            "        r = r[:-1]",
            "        return r",
            "",
            "",
            "def get_cluster(node_ips, node_ip, trainer_endpoints, selected_gpus):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    cluster = Cluster(hdfs=None)",
            "    trainer_rank = 0",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "        # when use paddlecloud, endpoints may > selected_gpus(user_defined)",
            "        assert len(cur_node_endpoints) >= len(",
            "            selected_gpus",
            "        ), \"current trainer_endpoints size should be greater equal than selected_gpus size.\"",
            "        for i in range(len(selected_gpus)):",
            "            trainer = Trainer()",
            "            trainer.gpus.append(selected_gpus[i])",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = trainer_rank",
            "            trainer_rank += 1",
            "",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def terminate_local_procs(procs):",
            "    for p in procs:",
            "        if p.proc.poll() is None:",
            "            p.proc.terminate()",
            "            if p.log_fn:",
            "                p.log_fn.close()",
            "            logger.debug(f\"terminate process id:{p.proc.pid}\")",
            "",
            "    # wait all process terminiated",
            "    time.sleep(3)",
            "    for step in range(0, 50):",
            "        alive = False",
            "        for p in procs:",
            "            if p.proc.poll() is None:  # not termniate",
            "                os.kill(p.proc.pid, signal.SIGKILL)",
            "                alive = True",
            "",
            "        if not alive:",
            "            logger.info(\"terminate all the procs\")",
            "            return",
            "",
            "        time.sleep(3)",
            "",
            "    logger.fatal(\"can't kill all process and exit\")",
            "    sys.exit(1)",
            "",
            "",
            "def get_host_name_ip():",
            "    try:",
            "        host_name = socket.gethostname()",
            "        host_ip = socket.gethostbyname(host_name)",
            "        return host_name, host_ip",
            "    except:",
            "        return None",
            "",
            "",
            "def add_arguments(argname, type, default, help, argparser, **kwargs):",
            "    \"\"\"Add argparse's argument.",
            "    Usage:",
            "    .. code-block:: python",
            "        parser = argparse.ArgumentParser()",
            "        add_argument(\"name\", str, \"Jonh\", \"User name.\", parser)",
            "        args = parser.parse_args()",
            "    \"\"\"",
            "    type = strtobool if type == bool else type",
            "    argparser.add_argument(",
            "        \"--\" + argname,",
            "        default=default,",
            "        type=type,",
            "        help=help + ' Default: %(default)s.',",
            "        **kwargs,",
            "    )",
            "",
            "",
            "def find_free_ports(num):",
            "    def __free_port():",
            "        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:",
            "            s.bind(('', 0))",
            "            return s.getsockname()[1]",
            "",
            "    port_set = set()",
            "    step = 0",
            "    while True:",
            "        port = __free_port()",
            "        if port not in port_set:",
            "            port_set.add(port)",
            "",
            "        if len(port_set) >= num:",
            "            return port_set",
            "",
            "        step += 1",
            "        if step > 100:",
            "            print(",
            "                \"can't find avilable port and use the specified static port now!\"",
            "            )",
            "            return None",
            "",
            "    return None",
            "",
            "",
            "def _prepare_trainer_env(cluster, trainer, backend=None):",
            "    if backend is None:",
            "        backend = get_backend_by_compile_flag()  # for compatibility",
            "    if backend == 'bkcl':",
            "        proc_env = {",
            "            \"FLAGS_selected_xpus\": \"%s\"",
            "            % \",\".join([str(g) for g in trainer.gpus]),",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "        }",
            "    elif backend == 'nccl':",
            "        proc_env = {",
            "            \"FLAGS_selected_gpus\": \"%s\"",
            "            % \",\".join([str(g) for g in trainer.gpus]),",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "        }",
            "    elif backend == 'cncl':",
            "        proc_env = {",
            "            \"FLAGS_selected_mlus\": \"%s\"",
            "            % \",\".join([str(g) for g in trainer.gpus]),",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "        }",
            "    elif backend == 'gloo':",
            "        # NOTE (xiongkun) default fall back into cpu only",
            "        proc_env = {",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "            \"PADDLE_DISTRI_BACKEND\": backend,  # only add here, other will be auto",
            "        }",
            "    else:",
            "        raise ValueError(\"backend must be one of 'gloo, nccl, bkcl'\")",
            "",
            "    return proc_env",
            "",
            "",
            "class TrainerProc:",
            "    def __init__(self):",
            "        self.proc = None",
            "        self.log_fn = None",
            "        self.log_offset = None",
            "        self.rank = None",
            "        self.local_rank = None",
            "        self.cmd = None",
            "",
            "",
            "def start_local_trainers(",
            "    cluster, pod, training_script, training_script_args, log_dir=None",
            "):",
            "    current_env = copy.copy(os.environ.copy())",
            "    # paddle broadcast ncclUniqueId use socket, and",
            "    # proxy maybe make trainers unreachable, so delete them.",
            "    # if we set them to \"\", grpc will log error message \"bad uri\"",
            "    # so just delete them.",
            "    current_env.pop(\"http_proxy\", None)",
            "    current_env.pop(\"https_proxy\", None)",
            "",
            "    procs = []",
            "    for idx, t in enumerate(pod.trainers):",
            "        proc_env = _prepare_trainer_env(cluster, t)",
            "        current_env.update(proc_env)",
            "",
            "        logger.debug(f\"trainer proc env:{current_env}\")",
            "",
            "        cmd = [sys.executable, \"-u\", training_script] + training_script_args",
            "",
            "        logger.info(f\"start trainer proc:{cmd} env:{proc_env}\")",
            "",
            "        fn = None",
            "        if log_dir is not None:",
            "            os.system(f\"mkdir -p {log_dir}\")",
            "            fn = open(\"%s/workerlog.%d\" % (log_dir, idx), \"a\")",
            "            proc = subprocess.Popen(cmd, env=current_env, stdout=fn, stderr=fn)",
            "        else:",
            "            proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "        tp = TrainerProc()",
            "        tp.proc = proc",
            "        tp.rank = t.rank",
            "        tp.local_rank = idx",
            "        tp.log_fn = fn",
            "        tp.log_offset = fn.tell() if fn else None",
            "        tp.cmd = cmd",
            "",
            "        procs.append(tp)",
            "",
            "    return procs",
            "",
            "",
            "def pull_worker_log(tp):",
            "    if tp.log_fn:",
            "        with open(tp.log_fn.name, 'r') as fin:",
            "            fin.seek(tp.log_offset, 0)",
            "            for line in fin:",
            "                try:",
            "                    sys.stdout.write(line)",
            "                except UnicodeEncodeError:",
            "                    sys.stdout.write(",
            "                        'UnicodeEncodeError occurs at this line. '",
            "                        'Please refer to the original log file \"%s\"\\n'",
            "                        % tp.log_fn.name",
            "                    )",
            "            tp.log_offset = fin.tell()",
            "",
            "",
            "def watch_local_trainers(procs, nranks):",
            "    try:",
            "        error = False",
            "        error_rank = []",
            "        # wait all process finish or one error",
            "        alive = False",
            "        for p in procs:",
            "            if p.log_fn and p.local_rank == 0:",
            "                pull_worker_log(p)",
            "",
            "            ret = p.proc.poll()",
            "            if ret is None:",
            "                alive = True",
            "            elif ret != 0:",
            "                error = True",
            "                error_rank.append(p.rank)",
            "",
            "        if error:",
            "            terminate_local_procs(procs)",
            "            sys.exit(1)",
            "",
            "    except KeyboardInterrupt:",
            "        logger.warning(\"KeyboardInterrupt, exit\")",
            "        terminate_local_procs(procs)",
            "        raise",
            "    except SystemExit:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        raise",
            "    except:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        raise",
            "",
            "    return alive",
            "",
            "",
            "def _print_arguments(args):",
            "    print(\"-----------  Configuration Arguments -----------\")",
            "    for arg, value in sorted(vars(args).items()):",
            "        print(f\"{arg}: {value}\")",
            "    print(\"------------------------------------------------\")"
        ],
        "afterPatchFile": [
            "# Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.",
            "#",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");",
            "# you may not use this file except in compliance with the License.",
            "# You may obtain a copy of the License at",
            "#",
            "#     http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing, software",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
            "# See the License for the specific language governing permissions and",
            "# limitations under the License.",
            "",
            "import copy",
            "import os",
            "import signal",
            "import socket",
            "import subprocess",
            "import sys",
            "import time",
            "from contextlib import closing",
            "from distutils.util import strtobool",
            "",
            "from paddle.distributed.fleet.launch_utils import get_backend_by_compile_flag",
            "",
            "from ..utils.log_utils import get_logger",
            "",
            "logger = get_logger(\"INFO\", \"root\")",
            "",
            "",
            "def get_cluster_from_args(args, selected_gpus):",
            "    node_ips = [x.strip() for x in args.cluster_node_ips.split(',')]",
            "    node_ip = args.node_ip",
            "    node_rank = node_ips.index(node_ip)",
            "",
            "    logger.debug(",
            "        \"parsed from args:node_ips:{} node_ip:{} node_rank:{}\".format(",
            "            node_ips, node_ip, node_rank",
            "        )",
            "    )",
            "",
            "    free_ports = None",
            "    if (",
            "        not args.use_paddlecloud",
            "        and len(node_ips) <= 1",
            "        and args.started_port is None",
            "    ):",
            "        free_ports = find_free_ports(len(selected_gpus))",
            "        if free_ports is not None:",
            "            free_ports = list(free_ports)",
            "    else:",
            "        started_port = 6070",
            "        if args.started_port is not None:",
            "            started_port = args.started_port",
            "",
            "        free_ports = list(",
            "            range(started_port, started_port + len(selected_gpus))",
            "        )",
            "",
            "    trainer_endpoints = []",
            "    for ip in node_ips:",
            "        trainer_endpoints.append([\"%s:%d\" % (ip, port) for port in free_ports])",
            "    return get_cluster(node_ips, node_ip, trainer_endpoints, selected_gpus)",
            "",
            "",
            "def get_gpus(selected_gpus):",
            "    if selected_gpus is None:",
            "        from paddle.framework import core",
            "",
            "        gpus_num = core.get_cuda_device_count()",
            "        gpus = [str(x) for x in range(0, gpus_num)]",
            "    else:",
            "        cuda_visible_devices = os.getenv(\"CUDA_VISIBLE_DEVICES\")",
            "        if cuda_visible_devices is None or cuda_visible_devices == \"\":",
            "            gpus = [x.strip() for x in selected_gpus.split(',')]",
            "        else:",
            "            # change selected_gpus into relative values",
            "            # e.g. CUDA_VISIBLE_DEVICES=4,5,6,7; args.selected_gpus=4,5,6,7;",
            "            # therefore selected_gpus=0,1,2,3",
            "            cuda_visible_devices_list = cuda_visible_devices.split(',')",
            "            for x in selected_gpus.split(','):",
            "                assert x in cuda_visible_devices_list, (",
            "                    \"Can't find \"",
            "                    \"your selected_gpus %s in CUDA_VISIBLE_DEVICES[%s].\"",
            "                    % (x, cuda_visible_devices)",
            "                )",
            "            gpus = [",
            "                cuda_visible_devices_list.index(x.strip())",
            "                for x in selected_gpus.split(',')",
            "            ]",
            "            logger.info(",
            "                \"Change selected_gpus into reletive values. --ips:{} \"",
            "                \"will change into relative_ips:{} according to your \"",
            "                \"CUDA_VISIBLE_DEVICES:{}\".format(",
            "                    selected_gpus, gpus, cuda_visible_devices_list",
            "                )",
            "            )",
            "",
            "    return gpus",
            "",
            "",
            "class Hdfs:",
            "    def __init__(self):",
            "        self.hdfs_ugi = None",
            "        self.hdfs_name = None",
            "        self.hdfs_path = None",
            "",
            "    def is_valid(self):",
            "        return (",
            "            self.hdfs_ugi is not None",
            "            and self.hdfs_name is not None",
            "            and self.hdfs_path is not None",
            "        )",
            "",
            "    def __str__(self):",
            "        return \"hdfs_ugi:{} hdfs_name:{} hdfs_path{}\".format(",
            "            self.hdfs_ugi, self.hdfs_name, self.hdfs_path",
            "        )",
            "",
            "    def __eq__(self, n):",
            "        return (",
            "            self.hdfs_ugi == n.hdfs_ugi",
            "            and self.hdfs_name == n.hdfs_name",
            "            and self.hdfs_path == n.hdfs_path",
            "        )",
            "",
            "    def __ne__(self, n):",
            "        return not self == n",
            "",
            "",
            "class Cluster:",
            "    def __init__(self, hdfs):",
            "        self.job_server = None",
            "        self.pods = []",
            "        self.hdfs = None",
            "        self.job_stage_flag = None",
            "",
            "    def __str__(self):",
            "        return \"job_server:{} pods:{} job_stage_flag:{} hdfs:{}\".format(",
            "            self.job_server,",
            "            [str(pod) for pod in self.pods],",
            "            self.job_stage_flag,",
            "            self.hdfs,",
            "        )",
            "",
            "    def __eq__(self, cluster):",
            "        if len(self.pods) != len(cluster.pods):",
            "            return False",
            "",
            "        for a, b in zip(self.pods, cluster.pods):",
            "            if a != b:",
            "                return False",
            "",
            "        if self.job_stage_flag != cluster.job_stage_flag:",
            "            return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, cluster):",
            "        return not self.__eq__(cluster)",
            "",
            "    def update_pods(self, cluster):",
            "        self.pods = copy.copy(cluster.pods)",
            "",
            "    def trainers_nranks(self):",
            "        return len(self.trainers_endpoints())",
            "",
            "    def pods_nranks(self):",
            "        return len(self.pods)",
            "",
            "    def trainers_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            for t in pod.trainers:",
            "                r.append(t.endpoint)",
            "        return r",
            "",
            "    def pods_endpoints(self):",
            "        r = []",
            "        for pod in self.pods:",
            "            ep = f\"{pod.addr}:{pod.port}\"",
            "            assert (",
            "                pod.port is not None and pod.addr is not None",
            "            ), f\"{ep} not a valid endpoint\"",
            "            r.append(ep)",
            "",
            "        return r",
            "",
            "    def get_pod_by_id(self, pod_id):",
            "        for pod in self.pods:",
            "            if str(pod_id) == str(pod.id):",
            "                return pod",
            "",
            "        return None",
            "",
            "",
            "class JobServer:",
            "    def __init__(self):",
            "        self.endpoint = None",
            "",
            "    def __str__(self):",
            "        return f\"{self.endpoint}\"",
            "",
            "    def __eq__(self, j):",
            "        return self.endpint == j.endpoint",
            "",
            "    def __ne__(self, j):",
            "        return not self == j",
            "",
            "",
            "class Trainer:",
            "    def __init__(self):",
            "        self.gpus = []",
            "        self.endpoint = None",
            "        self.rank = None",
            "",
            "    def __str__(self):",
            "        return \"gpu:{} endpoint:{} rank:{}\".format(",
            "            self.gpus, self.endpoint, self.rank",
            "        )",
            "",
            "    def __eq__(self, t):",
            "        if len(self.gpus) != len(t.gpus):",
            "            return False",
            "",
            "        if self.endpoint != t.endpoint or self.rank != t.rank:",
            "            return False",
            "",
            "        for a, b in zip(self.gpus, t.gpus):",
            "            if a != b:",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, t):",
            "        return not self == t",
            "",
            "    def get_rank(self):",
            "        return self.rank",
            "",
            "",
            "class Pod:",
            "    def __init__(self):",
            "        self.rank = None",
            "        self.id = None",
            "        self.addr = None",
            "        self.port = None",
            "        self.trainers = []",
            "        self.gpus = []",
            "",
            "    def __str__(self):",
            "        return (",
            "            \"rank:{} id:{} addr:{} port:{} visible_gpu:{} trainers:{}\".format(",
            "                self.rank,",
            "                self.id,",
            "                self.addr,",
            "                self.port,",
            "                self.gpus,",
            "                [str(t) for t in self.trainers],",
            "            )",
            "        )",
            "",
            "    def __eq__(self, pod):",
            "        if (",
            "            self.rank != pod.rank",
            "            or self.id != pod.id",
            "            or self.addr != pod.addr",
            "            or self.port != pod.port",
            "        ):",
            "            logger.debug(f\"pod {self} != {pod}\")",
            "            return False",
            "",
            "        if len(self.trainers) != len(pod.trainers):",
            "            logger.debug(f\"trainers {self.trainers} != {pod.trainers}\")",
            "            return False",
            "",
            "        for i in range(len(self.trainers)):",
            "            if self.trainers[i] != pod.trainers[i]:",
            "                logger.debug(f\"trainer {self.trainers[i]} != {pod.trainers[i]}\")",
            "                return False",
            "",
            "        return True",
            "",
            "    def __ne__(self, pod):",
            "        return not self == pod",
            "",
            "    def parse_response(self, res_pods):",
            "        pass",
            "",
            "    def get_visible_gpus(self):",
            "        r = \"\"",
            "        for g in self.gpus:",
            "            r += f\"{g},\"",
            "",
            "        assert r != \"\", f\"this pod {self} can't see any gpus\"",
            "",
            "        r = r[:-1]",
            "        return r",
            "",
            "",
            "def get_cluster(node_ips, node_ip, trainer_endpoints, selected_gpus):",
            "    assert type(trainer_endpoints) is list, \"trainer_endpoints must be list\"",
            "    cluster = Cluster(hdfs=None)",
            "    trainer_rank = 0",
            "    for node_rank, ip in enumerate(node_ips):",
            "        pod = Pod()",
            "        pod.rank = node_rank",
            "        pod.addr = ip",
            "        cur_node_endpoints = trainer_endpoints[node_rank]",
            "        # when use paddlecloud, endpoints may > selected_gpus(user_defined)",
            "        assert len(cur_node_endpoints) >= len(",
            "            selected_gpus",
            "        ), \"current trainer_endpoints size should be greater equal than selected_gpus size.\"",
            "        for i in range(len(selected_gpus)):",
            "            trainer = Trainer()",
            "            trainer.gpus.append(selected_gpus[i])",
            "            trainer.endpoint = \"%s\" % (cur_node_endpoints[i])",
            "            trainer.rank = trainer_rank",
            "            trainer_rank += 1",
            "",
            "            pod.trainers.append(trainer)",
            "        cluster.pods.append(pod)",
            "",
            "    pod_rank = node_ips.index(node_ip)",
            "    return cluster, cluster.pods[pod_rank]",
            "",
            "",
            "def terminate_local_procs(procs):",
            "    for p in procs:",
            "        if p.proc.poll() is None:",
            "            p.proc.terminate()",
            "            if p.log_fn:",
            "                p.log_fn.close()",
            "            logger.debug(f\"terminate process id:{p.proc.pid}\")",
            "",
            "    # wait all process terminiated",
            "    time.sleep(3)",
            "    for step in range(0, 50):",
            "        alive = False",
            "        for p in procs:",
            "            if p.proc.poll() is None:  # not termniate",
            "                os.kill(p.proc.pid, signal.SIGKILL)",
            "                alive = True",
            "",
            "        if not alive:",
            "            logger.info(\"terminate all the procs\")",
            "            return",
            "",
            "        time.sleep(3)",
            "",
            "    logger.fatal(\"can't kill all process and exit\")",
            "    sys.exit(1)",
            "",
            "",
            "def get_host_name_ip():",
            "    try:",
            "        host_name = socket.gethostname()",
            "        host_ip = socket.gethostbyname(host_name)",
            "        return host_name, host_ip",
            "    except:",
            "        return None",
            "",
            "",
            "def add_arguments(argname, type, default, help, argparser, **kwargs):",
            "    \"\"\"Add argparse's argument.",
            "    Usage:",
            "    .. code-block:: python",
            "        parser = argparse.ArgumentParser()",
            "        add_argument(\"name\", str, \"Jonh\", \"User name.\", parser)",
            "        args = parser.parse_args()",
            "    \"\"\"",
            "    type = strtobool if type == bool else type",
            "    argparser.add_argument(",
            "        \"--\" + argname,",
            "        default=default,",
            "        type=type,",
            "        help=help + ' Default: %(default)s.',",
            "        **kwargs,",
            "    )",
            "",
            "",
            "def find_free_ports(num):",
            "    def __free_port():",
            "        with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:",
            "            s.bind(('', 0))",
            "            return s.getsockname()[1]",
            "",
            "    port_set = set()",
            "    step = 0",
            "    while True:",
            "        port = __free_port()",
            "        if port not in port_set:",
            "            port_set.add(port)",
            "",
            "        if len(port_set) >= num:",
            "            return port_set",
            "",
            "        step += 1",
            "        if step > 100:",
            "            print(",
            "                \"can't find avilable port and use the specified static port now!\"",
            "            )",
            "            return None",
            "",
            "    return None",
            "",
            "",
            "def _prepare_trainer_env(cluster, trainer, backend=None):",
            "    if backend is None:",
            "        backend = get_backend_by_compile_flag()  # for compatibility",
            "    if backend == 'bkcl':",
            "        proc_env = {",
            "            \"FLAGS_selected_xpus\": \"%s\"",
            "            % \",\".join([str(g) for g in trainer.gpus]),",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "        }",
            "    elif backend == 'nccl':",
            "        proc_env = {",
            "            \"FLAGS_selected_gpus\": \"%s\"",
            "            % \",\".join([str(g) for g in trainer.gpus]),",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "        }",
            "    elif backend == 'cncl':",
            "        proc_env = {",
            "            \"FLAGS_selected_mlus\": \"%s\"",
            "            % \",\".join([str(g) for g in trainer.gpus]),",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "        }",
            "    elif backend == 'gloo':",
            "        # NOTE (xiongkun) default fall back into cpu only",
            "        proc_env = {",
            "            \"PADDLE_TRAINER_ID\": \"%d\" % trainer.rank,",
            "            \"PADDLE_CURRENT_ENDPOINT\": \"%s\" % trainer.endpoint,",
            "            \"PADDLE_TRAINERS_NUM\": \"%d\" % cluster.trainers_nranks(),",
            "            \"PADDLE_TRAINER_ENDPOINTS\": \",\".join(cluster.trainers_endpoints()),",
            "            \"PADDLE_DISTRI_BACKEND\": backend,  # only add here, other will be auto",
            "        }",
            "    else:",
            "        raise ValueError(\"backend must be one of 'gloo, nccl, bkcl'\")",
            "",
            "    return proc_env",
            "",
            "",
            "class TrainerProc:",
            "    def __init__(self):",
            "        self.proc = None",
            "        self.log_fn = None",
            "        self.log_offset = None",
            "        self.rank = None",
            "        self.local_rank = None",
            "        self.cmd = None",
            "",
            "",
            "def start_local_trainers(",
            "    cluster, pod, training_script, training_script_args, log_dir=None",
            "):",
            "    current_env = copy.copy(os.environ.copy())",
            "    # paddle broadcast ncclUniqueId use socket, and",
            "    # proxy maybe make trainers unreachable, so delete them.",
            "    # if we set them to \"\", grpc will log error message \"bad uri\"",
            "    # so just delete them.",
            "    current_env.pop(\"http_proxy\", None)",
            "    current_env.pop(\"https_proxy\", None)",
            "",
            "    procs = []",
            "    for idx, t in enumerate(pod.trainers):",
            "        proc_env = _prepare_trainer_env(cluster, t)",
            "        current_env.update(proc_env)",
            "",
            "        logger.debug(f\"trainer proc env:{current_env}\")",
            "",
            "        cmd = [sys.executable, \"-u\", training_script] + training_script_args",
            "",
            "        logger.info(f\"start trainer proc:{cmd} env:{proc_env}\")",
            "",
            "        fn = None",
            "        if log_dir is not None:",
            "            os.makedirs(log_dir, exist_ok=True)",
            "            fn = open(\"%s/workerlog.%d\" % (log_dir, idx), \"a\")",
            "            proc = subprocess.Popen(cmd, env=current_env, stdout=fn, stderr=fn)",
            "        else:",
            "            proc = subprocess.Popen(cmd, env=current_env)",
            "",
            "        tp = TrainerProc()",
            "        tp.proc = proc",
            "        tp.rank = t.rank",
            "        tp.local_rank = idx",
            "        tp.log_fn = fn",
            "        tp.log_offset = fn.tell() if fn else None",
            "        tp.cmd = cmd",
            "",
            "        procs.append(tp)",
            "",
            "    return procs",
            "",
            "",
            "def pull_worker_log(tp):",
            "    if tp.log_fn:",
            "        with open(tp.log_fn.name, 'r') as fin:",
            "            fin.seek(tp.log_offset, 0)",
            "            for line in fin:",
            "                try:",
            "                    sys.stdout.write(line)",
            "                except UnicodeEncodeError:",
            "                    sys.stdout.write(",
            "                        'UnicodeEncodeError occurs at this line. '",
            "                        'Please refer to the original log file \"%s\"\\n'",
            "                        % tp.log_fn.name",
            "                    )",
            "            tp.log_offset = fin.tell()",
            "",
            "",
            "def watch_local_trainers(procs, nranks):",
            "    try:",
            "        error = False",
            "        error_rank = []",
            "        # wait all process finish or one error",
            "        alive = False",
            "        for p in procs:",
            "            if p.log_fn and p.local_rank == 0:",
            "                pull_worker_log(p)",
            "",
            "            ret = p.proc.poll()",
            "            if ret is None:",
            "                alive = True",
            "            elif ret != 0:",
            "                error = True",
            "                error_rank.append(p.rank)",
            "",
            "        if error:",
            "            terminate_local_procs(procs)",
            "            sys.exit(1)",
            "",
            "    except KeyboardInterrupt:",
            "        logger.warning(\"KeyboardInterrupt, exit\")",
            "        terminate_local_procs(procs)",
            "        raise",
            "    except SystemExit:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        raise",
            "    except:",
            "        logger.error(",
            "            \"ABORT!!! Out of all {} trainers, the trainer process with rank={} was aborted. Please check its log.\".format(",
            "                nranks, error_rank",
            "            )",
            "        )",
            "        terminate_local_procs(procs)",
            "        raise",
            "",
            "    return alive",
            "",
            "",
            "def _print_arguments(args):",
            "    print(\"-----------  Configuration Arguments -----------\")",
            "    for arg, value in sorted(vars(args).items()):",
            "        print(f\"{arg}: {value}\")",
            "    print(\"------------------------------------------------\")"
        ],
        "action": [
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "488": [
                "start_local_trainers"
            ]
        },
        "addLocation": []
    }
}