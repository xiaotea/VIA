{
    "airflow/providers/apache/sqoop/hooks/sqoop.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 36,
                "afterPatchRowNumber": 36,
                "PatchRowcode": " "
            },
            "1": {
                "beforePatchRowNumber": 37,
                "afterPatchRowNumber": 37,
                "PatchRowcode": "         * ``job_tracker``: Job tracker local|jobtracker:port."
            },
            "2": {
                "beforePatchRowNumber": 38,
                "afterPatchRowNumber": 38,
                "PatchRowcode": "         * ``namenode``: Namenode."
            },
            "3": {
                "beforePatchRowNumber": 39,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        * ``lib_jars``: Comma separated jar files to include in the classpath."
            },
            "4": {
                "beforePatchRowNumber": 40,
                "afterPatchRowNumber": 39,
                "PatchRowcode": "         * ``files``: Comma separated files to be copied to the map reduce cluster."
            },
            "5": {
                "beforePatchRowNumber": 41,
                "afterPatchRowNumber": 40,
                "PatchRowcode": "         * ``archives``: Comma separated archives to be unarchived on the compute"
            },
            "6": {
                "beforePatchRowNumber": 42,
                "afterPatchRowNumber": 41,
                "PatchRowcode": "             machines."
            },
            "7": {
                "beforePatchRowNumber": 46,
                "afterPatchRowNumber": 45,
                "PatchRowcode": "     :param verbose: Set sqoop to verbose."
            },
            "8": {
                "beforePatchRowNumber": 47,
                "afterPatchRowNumber": 46,
                "PatchRowcode": "     :param num_mappers: Number of map tasks to import in parallel."
            },
            "9": {
                "beforePatchRowNumber": 48,
                "afterPatchRowNumber": 47,
                "PatchRowcode": "     :param properties: Properties to set via the -D argument"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 48,
                "PatchRowcode": "+    :param libjars: Optional Comma separated jar files to include in the classpath."
            },
            "11": {
                "beforePatchRowNumber": 49,
                "afterPatchRowNumber": 49,
                "PatchRowcode": "     \"\"\""
            },
            "12": {
                "beforePatchRowNumber": 50,
                "afterPatchRowNumber": 50,
                "PatchRowcode": " "
            },
            "13": {
                "beforePatchRowNumber": 51,
                "afterPatchRowNumber": 51,
                "PatchRowcode": "     conn_name_attr = \"conn_id\""
            },
            "14": {
                "beforePatchRowNumber": 61,
                "afterPatchRowNumber": 61,
                "PatchRowcode": "         hcatalog_database: str | None = None,"
            },
            "15": {
                "beforePatchRowNumber": 62,
                "afterPatchRowNumber": 62,
                "PatchRowcode": "         hcatalog_table: str | None = None,"
            },
            "16": {
                "beforePatchRowNumber": 63,
                "afterPatchRowNumber": 63,
                "PatchRowcode": "         properties: dict[str, Any] | None = None,"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 64,
                "PatchRowcode": "+        libjars: str | None = None,"
            },
            "18": {
                "beforePatchRowNumber": 64,
                "afterPatchRowNumber": 65,
                "PatchRowcode": "     ) -> None:"
            },
            "19": {
                "beforePatchRowNumber": 65,
                "afterPatchRowNumber": 66,
                "PatchRowcode": "         # No mutable types in the default parameters"
            },
            "20": {
                "beforePatchRowNumber": 66,
                "afterPatchRowNumber": 67,
                "PatchRowcode": "         super().__init__()"
            },
            "21": {
                "beforePatchRowNumber": 67,
                "afterPatchRowNumber": 68,
                "PatchRowcode": "         self.conn = self.get_connection(conn_id)"
            },
            "22": {
                "beforePatchRowNumber": 68,
                "afterPatchRowNumber": 69,
                "PatchRowcode": "         connection_parameters = self.conn.extra_dejson"
            },
            "23": {
                "beforePatchRowNumber": 69,
                "afterPatchRowNumber": 70,
                "PatchRowcode": "         self.job_tracker = connection_parameters.get(\"job_tracker\", None)"
            },
            "24": {
                "beforePatchRowNumber": 70,
                "afterPatchRowNumber": 71,
                "PatchRowcode": "         self.namenode = connection_parameters.get(\"namenode\", None)"
            },
            "25": {
                "beforePatchRowNumber": 71,
                "afterPatchRowNumber": "",
                "PatchRowcode": "-        self.libjars = connection_parameters.get(\"libjars\", None)"
            },
            "26": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 72,
                "PatchRowcode": "+        self.libjars = libjars"
            },
            "27": {
                "beforePatchRowNumber": 72,
                "afterPatchRowNumber": 73,
                "PatchRowcode": "         self.files = connection_parameters.get(\"files\", None)"
            },
            "28": {
                "beforePatchRowNumber": 73,
                "afterPatchRowNumber": 74,
                "PatchRowcode": "         self.archives = connection_parameters.get(\"archives\", None)"
            },
            "29": {
                "beforePatchRowNumber": 74,
                "afterPatchRowNumber": 75,
                "PatchRowcode": "         self.password_file = connection_parameters.get(\"password_file\", None)"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1.x hook\"\"\"",
            "from __future__ import annotations",
            "",
            "import subprocess",
            "from copy import deepcopy",
            "from typing import Any",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.hooks.base import BaseHook",
            "",
            "",
            "class SqoopHook(BaseHook):",
            "    \"\"\"",
            "    This hook is a wrapper around the sqoop 1 binary. To be able to use the hook",
            "    it is required that \"sqoop\" is in the PATH.",
            "",
            "    Additional arguments that can be passed via the 'extra' JSON field of the",
            "    sqoop connection:",
            "",
            "        * ``job_tracker``: Job tracker local|jobtracker:port.",
            "        * ``namenode``: Namenode.",
            "        * ``lib_jars``: Comma separated jar files to include in the classpath.",
            "        * ``files``: Comma separated files to be copied to the map reduce cluster.",
            "        * ``archives``: Comma separated archives to be unarchived on the compute",
            "            machines.",
            "        * ``password_file``: Path to file containing the password.",
            "",
            "    :param conn_id: Reference to the sqoop connection.",
            "    :param verbose: Set sqoop to verbose.",
            "    :param num_mappers: Number of map tasks to import in parallel.",
            "    :param properties: Properties to set via the -D argument",
            "    \"\"\"",
            "",
            "    conn_name_attr = \"conn_id\"",
            "    default_conn_name = \"sqoop_default\"",
            "    conn_type = \"sqoop\"",
            "    hook_name = \"Sqoop\"",
            "",
            "    def __init__(",
            "        self,",
            "        conn_id: str = default_conn_name,",
            "        verbose: bool = False,",
            "        num_mappers: int | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        properties: dict[str, Any] | None = None,",
            "    ) -> None:",
            "        # No mutable types in the default parameters",
            "        super().__init__()",
            "        self.conn = self.get_connection(conn_id)",
            "        connection_parameters = self.conn.extra_dejson",
            "        self.job_tracker = connection_parameters.get(\"job_tracker\", None)",
            "        self.namenode = connection_parameters.get(\"namenode\", None)",
            "        self.libjars = connection_parameters.get(\"libjars\", None)",
            "        self.files = connection_parameters.get(\"files\", None)",
            "        self.archives = connection_parameters.get(\"archives\", None)",
            "        self.password_file = connection_parameters.get(\"password_file\", None)",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.verbose = verbose",
            "        self.num_mappers = num_mappers",
            "        self.properties = properties or {}",
            "        self.sub_process_pid: int",
            "        self.log.info(\"Using connection to: %s:%s/%s\", self.conn.host, self.conn.port, self.conn.schema)",
            "",
            "    def get_conn(self) -> Any:",
            "        return self.conn",
            "",
            "    def cmd_mask_password(self, cmd_orig: list[str]) -> list[str]:",
            "        \"\"\"Mask command password for safety\"\"\"",
            "        cmd = deepcopy(cmd_orig)",
            "        try:",
            "            password_index = cmd.index(\"--password\")",
            "            cmd[password_index + 1] = \"MASKED\"",
            "        except ValueError:",
            "            self.log.debug(\"No password in sqoop cmd\")",
            "        return cmd",
            "",
            "    def popen(self, cmd: list[str], **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Remote Popen",
            "",
            "        :param cmd: command to remotely execute",
            "        :param kwargs: extra arguments to Popen (see subprocess.Popen)",
            "        :return: handle to subprocess",
            "        \"\"\"",
            "        masked_cmd = \" \".join(self.cmd_mask_password(cmd))",
            "        self.log.info(\"Executing command: %s\", masked_cmd)",
            "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) as sub_process:",
            "            self.sub_process_pid = sub_process.pid",
            "            for line in iter(sub_process.stdout):  # type: ignore",
            "                self.log.info(line.strip())",
            "            sub_process.wait()",
            "            self.log.info(\"Command exited with return code %s\", sub_process.returncode)",
            "            if sub_process.returncode:",
            "                raise AirflowException(f\"Sqoop command failed: {masked_cmd}\")",
            "",
            "    def _prepare_command(self, export: bool = False) -> list[str]:",
            "        sqoop_cmd_type = \"export\" if export else \"import\"",
            "        connection_cmd = [\"sqoop\", sqoop_cmd_type]",
            "",
            "        for key, value in self.properties.items():",
            "            connection_cmd += [\"-D\", f\"{key}={value}\"]",
            "",
            "        if self.namenode:",
            "            connection_cmd += [\"-fs\", self.namenode]",
            "        if self.job_tracker:",
            "            connection_cmd += [\"-jt\", self.job_tracker]",
            "        if self.libjars:",
            "            connection_cmd += [\"-libjars\", self.libjars]",
            "        if self.files:",
            "            connection_cmd += [\"-files\", self.files]",
            "        if self.archives:",
            "            connection_cmd += [\"-archives\", self.archives]",
            "        if self.conn.login:",
            "            connection_cmd += [\"--username\", self.conn.login]",
            "        if self.conn.password:",
            "            connection_cmd += [\"--password\", self.conn.password]",
            "        if self.password_file:",
            "            connection_cmd += [\"--password-file\", self.password_file]",
            "        if self.verbose:",
            "            connection_cmd += [\"--verbose\"]",
            "        if self.num_mappers:",
            "            connection_cmd += [\"--num-mappers\", str(self.num_mappers)]",
            "        if self.hcatalog_database:",
            "            connection_cmd += [\"--hcatalog-database\", self.hcatalog_database]",
            "        if self.hcatalog_table:",
            "            connection_cmd += [\"--hcatalog-table\", self.hcatalog_table]",
            "",
            "        connect_str = self.conn.host",
            "        if self.conn.port:",
            "            connect_str += f\":{self.conn.port}\"",
            "        if self.conn.schema:",
            "            self.log.info(\"CONNECTION TYPE %s\", self.conn.conn_type)",
            "            if self.conn.conn_type != \"mssql\":",
            "                connect_str += f\"/{self.conn.schema}\"",
            "            else:",
            "                connect_str += f\";databaseName={self.conn.schema}\"",
            "        connection_cmd += [\"--connect\", connect_str]",
            "",
            "        return connection_cmd",
            "",
            "    @staticmethod",
            "    def _get_export_format_argument(file_type: str = \"text\") -> list[str]:",
            "        if file_type == \"avro\":",
            "            return [\"--as-avrodatafile\"]",
            "        elif file_type == \"sequence\":",
            "            return [\"--as-sequencefile\"]",
            "        elif file_type == \"parquet\":",
            "            return [\"--as-parquetfile\"]",
            "        elif file_type == \"text\":",
            "            return [\"--as-textfile\"]",
            "        else:",
            "            raise AirflowException(\"Argument file_type should be 'avro', 'sequence', 'parquet' or 'text'.\")",
            "",
            "    def _import_cmd(",
            "        self,",
            "        target_dir: str | None,",
            "        append: bool,",
            "        file_type: str,",
            "        split_by: str | None,",
            "        direct: bool | None,",
            "        driver: Any,",
            "        extra_import_options: Any,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=False)",
            "",
            "        if target_dir:",
            "            cmd += [\"--target-dir\", target_dir]",
            "",
            "        if append:",
            "            cmd += [\"--append\"]",
            "",
            "        cmd += self._get_export_format_argument(file_type)",
            "",
            "        if split_by:",
            "            cmd += [\"--split-by\", split_by]",
            "",
            "        if direct:",
            "            cmd += [\"--direct\"]",
            "",
            "        if driver:",
            "            cmd += [\"--driver\", driver]",
            "",
            "        if extra_import_options:",
            "            for key, value in extra_import_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        return cmd",
            "",
            "    def import_table(",
            "        self,",
            "        table: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        direct: bool = False,",
            "        driver: Any = None,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> Any:",
            "        \"\"\"",
            "        Imports table from remote location to target dir. Arguments are",
            "        copies of direct sqoop command line arguments",
            "",
            "        :param table: Table to read",
            "        :param schema: Schema name",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".",
            "            Imports data to into the specified format. Defaults to text.",
            "        :param columns: <col,col,col\u2026> Columns to import from table",
            "        :param split_by: Column of the table used to split work units",
            "        :param where: WHERE clause to use during import",
            "        :param direct: Use direct connector if exists for the database",
            "        :param driver: Manually specify JDBC driver class to use",
            "        :param extra_import_options: Extra import options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)",
            "",
            "        cmd += [\"--table\", table]",
            "",
            "        if columns:",
            "            cmd += [\"--columns\", columns]",
            "        if where:",
            "            cmd += [\"--where\", where]",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        self.popen(cmd)",
            "",
            "    def import_query(",
            "        self,",
            "        query: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        split_by: str | None = None,",
            "        direct: bool | None = None,",
            "        driver: Any | None = None,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "    ) -> Any:",
            "        \"\"\"",
            "        Imports a specific query from the rdbms to hdfs",
            "",
            "        :param query: Free format query to run",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"",
            "            Imports data to hdfs into the specified format. Defaults to text.",
            "        :param split_by: Column of the table used to split work units",
            "        :param direct: Use direct import fast path",
            "        :param driver: Manually specify JDBC driver class to use",
            "        :param extra_import_options: Extra import options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)",
            "        cmd += [\"--query\", query]",
            "",
            "        self.popen(cmd)",
            "",
            "    def _export_cmd(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=True)",
            "",
            "        if input_null_string:",
            "            cmd += [\"--input-null-string\", input_null_string]",
            "",
            "        if input_null_non_string:",
            "            cmd += [\"--input-null-non-string\", input_null_non_string]",
            "",
            "        if staging_table:",
            "            cmd += [\"--staging-table\", staging_table]",
            "",
            "        if clear_staging_table:",
            "            cmd += [\"--clear-staging-table\"]",
            "",
            "        if enclosed_by:",
            "            cmd += [\"--enclosed-by\", enclosed_by]",
            "",
            "        if escaped_by:",
            "            cmd += [\"--escaped-by\", escaped_by]",
            "",
            "        if input_fields_terminated_by:",
            "            cmd += [\"--input-fields-terminated-by\", input_fields_terminated_by]",
            "",
            "        if input_lines_terminated_by:",
            "            cmd += [\"--input-lines-terminated-by\", input_lines_terminated_by]",
            "",
            "        if input_optionally_enclosed_by:",
            "            cmd += [\"--input-optionally-enclosed-by\", input_optionally_enclosed_by]",
            "",
            "        if batch:",
            "            cmd += [\"--batch\"]",
            "",
            "        if relaxed_isolation:",
            "            cmd += [\"--relaxed-isolation\"]",
            "",
            "        if export_dir:",
            "            cmd += [\"--export-dir\", export_dir]",
            "",
            "        if extra_export_options:",
            "            for key, value in extra_export_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        # The required option",
            "        cmd += [\"--table\", table]",
            "",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        return cmd",
            "",
            "    def export_table(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Exports Hive table to remote location. Arguments are copies of direct",
            "        sqoop command line Arguments",
            "",
            "        :param table: Table remote destination",
            "        :param schema: Schema name",
            "        :param export_dir: Hive table to export",
            "        :param input_null_string: The string to be interpreted as null for",
            "            string columns",
            "        :param input_null_non_string: The string to be interpreted as null",
            "            for non-string columns",
            "        :param staging_table: The table in which data will be staged before",
            "            being inserted into the destination table",
            "        :param clear_staging_table: Indicate that any data present in the",
            "            staging table can be deleted",
            "        :param enclosed_by: Sets a required field enclosing character",
            "        :param escaped_by: Sets the escape character",
            "        :param input_fields_terminated_by: Sets the field separator character",
            "        :param input_lines_terminated_by: Sets the end-of-line character",
            "        :param input_optionally_enclosed_by: Sets a field enclosing character",
            "        :param batch: Use batch mode for underlying statement execution",
            "        :param relaxed_isolation: Transaction isolation to read uncommitted",
            "            for the mappers",
            "        :param extra_export_options: Extra export options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._export_cmd(",
            "            table,",
            "            export_dir,",
            "            input_null_string,",
            "            input_null_non_string,",
            "            staging_table,",
            "            clear_staging_table,",
            "            enclosed_by,",
            "            escaped_by,",
            "            input_fields_terminated_by,",
            "            input_lines_terminated_by,",
            "            input_optionally_enclosed_by,",
            "            batch,",
            "            relaxed_isolation,",
            "            extra_export_options,",
            "            schema,",
            "        )",
            "",
            "        self.popen(cmd)"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1.x hook\"\"\"",
            "from __future__ import annotations",
            "",
            "import subprocess",
            "from copy import deepcopy",
            "from typing import Any",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.hooks.base import BaseHook",
            "",
            "",
            "class SqoopHook(BaseHook):",
            "    \"\"\"",
            "    This hook is a wrapper around the sqoop 1 binary. To be able to use the hook",
            "    it is required that \"sqoop\" is in the PATH.",
            "",
            "    Additional arguments that can be passed via the 'extra' JSON field of the",
            "    sqoop connection:",
            "",
            "        * ``job_tracker``: Job tracker local|jobtracker:port.",
            "        * ``namenode``: Namenode.",
            "        * ``files``: Comma separated files to be copied to the map reduce cluster.",
            "        * ``archives``: Comma separated archives to be unarchived on the compute",
            "            machines.",
            "        * ``password_file``: Path to file containing the password.",
            "",
            "    :param conn_id: Reference to the sqoop connection.",
            "    :param verbose: Set sqoop to verbose.",
            "    :param num_mappers: Number of map tasks to import in parallel.",
            "    :param properties: Properties to set via the -D argument",
            "    :param libjars: Optional Comma separated jar files to include in the classpath.",
            "    \"\"\"",
            "",
            "    conn_name_attr = \"conn_id\"",
            "    default_conn_name = \"sqoop_default\"",
            "    conn_type = \"sqoop\"",
            "    hook_name = \"Sqoop\"",
            "",
            "    def __init__(",
            "        self,",
            "        conn_id: str = default_conn_name,",
            "        verbose: bool = False,",
            "        num_mappers: int | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        properties: dict[str, Any] | None = None,",
            "        libjars: str | None = None,",
            "    ) -> None:",
            "        # No mutable types in the default parameters",
            "        super().__init__()",
            "        self.conn = self.get_connection(conn_id)",
            "        connection_parameters = self.conn.extra_dejson",
            "        self.job_tracker = connection_parameters.get(\"job_tracker\", None)",
            "        self.namenode = connection_parameters.get(\"namenode\", None)",
            "        self.libjars = libjars",
            "        self.files = connection_parameters.get(\"files\", None)",
            "        self.archives = connection_parameters.get(\"archives\", None)",
            "        self.password_file = connection_parameters.get(\"password_file\", None)",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.verbose = verbose",
            "        self.num_mappers = num_mappers",
            "        self.properties = properties or {}",
            "        self.sub_process_pid: int",
            "        self.log.info(\"Using connection to: %s:%s/%s\", self.conn.host, self.conn.port, self.conn.schema)",
            "",
            "    def get_conn(self) -> Any:",
            "        return self.conn",
            "",
            "    def cmd_mask_password(self, cmd_orig: list[str]) -> list[str]:",
            "        \"\"\"Mask command password for safety\"\"\"",
            "        cmd = deepcopy(cmd_orig)",
            "        try:",
            "            password_index = cmd.index(\"--password\")",
            "            cmd[password_index + 1] = \"MASKED\"",
            "        except ValueError:",
            "            self.log.debug(\"No password in sqoop cmd\")",
            "        return cmd",
            "",
            "    def popen(self, cmd: list[str], **kwargs: Any) -> None:",
            "        \"\"\"",
            "        Remote Popen",
            "",
            "        :param cmd: command to remotely execute",
            "        :param kwargs: extra arguments to Popen (see subprocess.Popen)",
            "        :return: handle to subprocess",
            "        \"\"\"",
            "        masked_cmd = \" \".join(self.cmd_mask_password(cmd))",
            "        self.log.info(\"Executing command: %s\", masked_cmd)",
            "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, **kwargs) as sub_process:",
            "            self.sub_process_pid = sub_process.pid",
            "            for line in iter(sub_process.stdout):  # type: ignore",
            "                self.log.info(line.strip())",
            "            sub_process.wait()",
            "            self.log.info(\"Command exited with return code %s\", sub_process.returncode)",
            "            if sub_process.returncode:",
            "                raise AirflowException(f\"Sqoop command failed: {masked_cmd}\")",
            "",
            "    def _prepare_command(self, export: bool = False) -> list[str]:",
            "        sqoop_cmd_type = \"export\" if export else \"import\"",
            "        connection_cmd = [\"sqoop\", sqoop_cmd_type]",
            "",
            "        for key, value in self.properties.items():",
            "            connection_cmd += [\"-D\", f\"{key}={value}\"]",
            "",
            "        if self.namenode:",
            "            connection_cmd += [\"-fs\", self.namenode]",
            "        if self.job_tracker:",
            "            connection_cmd += [\"-jt\", self.job_tracker]",
            "        if self.libjars:",
            "            connection_cmd += [\"-libjars\", self.libjars]",
            "        if self.files:",
            "            connection_cmd += [\"-files\", self.files]",
            "        if self.archives:",
            "            connection_cmd += [\"-archives\", self.archives]",
            "        if self.conn.login:",
            "            connection_cmd += [\"--username\", self.conn.login]",
            "        if self.conn.password:",
            "            connection_cmd += [\"--password\", self.conn.password]",
            "        if self.password_file:",
            "            connection_cmd += [\"--password-file\", self.password_file]",
            "        if self.verbose:",
            "            connection_cmd += [\"--verbose\"]",
            "        if self.num_mappers:",
            "            connection_cmd += [\"--num-mappers\", str(self.num_mappers)]",
            "        if self.hcatalog_database:",
            "            connection_cmd += [\"--hcatalog-database\", self.hcatalog_database]",
            "        if self.hcatalog_table:",
            "            connection_cmd += [\"--hcatalog-table\", self.hcatalog_table]",
            "",
            "        connect_str = self.conn.host",
            "        if self.conn.port:",
            "            connect_str += f\":{self.conn.port}\"",
            "        if self.conn.schema:",
            "            self.log.info(\"CONNECTION TYPE %s\", self.conn.conn_type)",
            "            if self.conn.conn_type != \"mssql\":",
            "                connect_str += f\"/{self.conn.schema}\"",
            "            else:",
            "                connect_str += f\";databaseName={self.conn.schema}\"",
            "        connection_cmd += [\"--connect\", connect_str]",
            "",
            "        return connection_cmd",
            "",
            "    @staticmethod",
            "    def _get_export_format_argument(file_type: str = \"text\") -> list[str]:",
            "        if file_type == \"avro\":",
            "            return [\"--as-avrodatafile\"]",
            "        elif file_type == \"sequence\":",
            "            return [\"--as-sequencefile\"]",
            "        elif file_type == \"parquet\":",
            "            return [\"--as-parquetfile\"]",
            "        elif file_type == \"text\":",
            "            return [\"--as-textfile\"]",
            "        else:",
            "            raise AirflowException(\"Argument file_type should be 'avro', 'sequence', 'parquet' or 'text'.\")",
            "",
            "    def _import_cmd(",
            "        self,",
            "        target_dir: str | None,",
            "        append: bool,",
            "        file_type: str,",
            "        split_by: str | None,",
            "        direct: bool | None,",
            "        driver: Any,",
            "        extra_import_options: Any,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=False)",
            "",
            "        if target_dir:",
            "            cmd += [\"--target-dir\", target_dir]",
            "",
            "        if append:",
            "            cmd += [\"--append\"]",
            "",
            "        cmd += self._get_export_format_argument(file_type)",
            "",
            "        if split_by:",
            "            cmd += [\"--split-by\", split_by]",
            "",
            "        if direct:",
            "            cmd += [\"--direct\"]",
            "",
            "        if driver:",
            "            cmd += [\"--driver\", driver]",
            "",
            "        if extra_import_options:",
            "            for key, value in extra_import_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        return cmd",
            "",
            "    def import_table(",
            "        self,",
            "        table: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        direct: bool = False,",
            "        driver: Any = None,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> Any:",
            "        \"\"\"",
            "        Imports table from remote location to target dir. Arguments are",
            "        copies of direct sqoop command line arguments",
            "",
            "        :param table: Table to read",
            "        :param schema: Schema name",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\".",
            "            Imports data to into the specified format. Defaults to text.",
            "        :param columns: <col,col,col\u2026> Columns to import from table",
            "        :param split_by: Column of the table used to split work units",
            "        :param where: WHERE clause to use during import",
            "        :param direct: Use direct connector if exists for the database",
            "        :param driver: Manually specify JDBC driver class to use",
            "        :param extra_import_options: Extra import options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)",
            "",
            "        cmd += [\"--table\", table]",
            "",
            "        if columns:",
            "            cmd += [\"--columns\", columns]",
            "        if where:",
            "            cmd += [\"--where\", where]",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        self.popen(cmd)",
            "",
            "    def import_query(",
            "        self,",
            "        query: str,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        split_by: str | None = None,",
            "        direct: bool | None = None,",
            "        driver: Any | None = None,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "    ) -> Any:",
            "        \"\"\"",
            "        Imports a specific query from the rdbms to hdfs",
            "",
            "        :param query: Free format query to run",
            "        :param target_dir: HDFS destination dir",
            "        :param append: Append data to an existing dataset in HDFS",
            "        :param file_type: \"avro\", \"sequence\", \"text\" or \"parquet\"",
            "            Imports data to hdfs into the specified format. Defaults to text.",
            "        :param split_by: Column of the table used to split work units",
            "        :param direct: Use direct import fast path",
            "        :param driver: Manually specify JDBC driver class to use",
            "        :param extra_import_options: Extra import options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._import_cmd(target_dir, append, file_type, split_by, direct, driver, extra_import_options)",
            "        cmd += [\"--query\", query]",
            "",
            "        self.popen(cmd)",
            "",
            "    def _export_cmd(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> list[str]:",
            "",
            "        cmd = self._prepare_command(export=True)",
            "",
            "        if input_null_string:",
            "            cmd += [\"--input-null-string\", input_null_string]",
            "",
            "        if input_null_non_string:",
            "            cmd += [\"--input-null-non-string\", input_null_non_string]",
            "",
            "        if staging_table:",
            "            cmd += [\"--staging-table\", staging_table]",
            "",
            "        if clear_staging_table:",
            "            cmd += [\"--clear-staging-table\"]",
            "",
            "        if enclosed_by:",
            "            cmd += [\"--enclosed-by\", enclosed_by]",
            "",
            "        if escaped_by:",
            "            cmd += [\"--escaped-by\", escaped_by]",
            "",
            "        if input_fields_terminated_by:",
            "            cmd += [\"--input-fields-terminated-by\", input_fields_terminated_by]",
            "",
            "        if input_lines_terminated_by:",
            "            cmd += [\"--input-lines-terminated-by\", input_lines_terminated_by]",
            "",
            "        if input_optionally_enclosed_by:",
            "            cmd += [\"--input-optionally-enclosed-by\", input_optionally_enclosed_by]",
            "",
            "        if batch:",
            "            cmd += [\"--batch\"]",
            "",
            "        if relaxed_isolation:",
            "            cmd += [\"--relaxed-isolation\"]",
            "",
            "        if export_dir:",
            "            cmd += [\"--export-dir\", export_dir]",
            "",
            "        if extra_export_options:",
            "            for key, value in extra_export_options.items():",
            "                cmd += [f\"--{key}\"]",
            "                if value:",
            "                    cmd += [str(value)]",
            "",
            "        # The required option",
            "        cmd += [\"--table\", table]",
            "",
            "        if schema:",
            "            cmd += [\"--\", \"--schema\", schema]",
            "",
            "        return cmd",
            "",
            "    def export_table(",
            "        self,",
            "        table: str,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "    ) -> None:",
            "        \"\"\"",
            "        Exports Hive table to remote location. Arguments are copies of direct",
            "        sqoop command line Arguments",
            "",
            "        :param table: Table remote destination",
            "        :param schema: Schema name",
            "        :param export_dir: Hive table to export",
            "        :param input_null_string: The string to be interpreted as null for",
            "            string columns",
            "        :param input_null_non_string: The string to be interpreted as null",
            "            for non-string columns",
            "        :param staging_table: The table in which data will be staged before",
            "            being inserted into the destination table",
            "        :param clear_staging_table: Indicate that any data present in the",
            "            staging table can be deleted",
            "        :param enclosed_by: Sets a required field enclosing character",
            "        :param escaped_by: Sets the escape character",
            "        :param input_fields_terminated_by: Sets the field separator character",
            "        :param input_lines_terminated_by: Sets the end-of-line character",
            "        :param input_optionally_enclosed_by: Sets a field enclosing character",
            "        :param batch: Use batch mode for underlying statement execution",
            "        :param relaxed_isolation: Transaction isolation to read uncommitted",
            "            for the mappers",
            "        :param extra_export_options: Extra export options to pass as dict.",
            "            If a key doesn't have a value, just pass an empty string to it.",
            "            Don't include prefix of -- for sqoop options.",
            "        \"\"\"",
            "        cmd = self._export_cmd(",
            "            table,",
            "            export_dir,",
            "            input_null_string,",
            "            input_null_non_string,",
            "            staging_table,",
            "            clear_staging_table,",
            "            enclosed_by,",
            "            escaped_by,",
            "            input_fields_terminated_by,",
            "            input_lines_terminated_by,",
            "            input_optionally_enclosed_by,",
            "            batch,",
            "            relaxed_isolation,",
            "            extra_export_options,",
            "            schema,",
            "        )",
            "",
            "        self.popen(cmd)"
        ],
        "action": [
            "0",
            "0",
            "0",
            "1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "2",
            "2",
            "0",
            "0",
            "0"
        ],
        "dele_reviseLocation": {
            "39": [
                "SqoopHook"
            ],
            "71": [
                "SqoopHook",
                "__init__"
            ]
        },
        "addLocation": []
    },
    "airflow/providers/apache/sqoop/operators/sqoop.py": {
        "Patch": {
            "0": {
                "beforePatchRowNumber": 82,
                "afterPatchRowNumber": 82,
                "PatchRowcode": "     :param extra_export_options: Extra export options to pass as dict."
            },
            "1": {
                "beforePatchRowNumber": 83,
                "afterPatchRowNumber": 83,
                "PatchRowcode": "         If a key doesn't have a value, just pass an empty string to it."
            },
            "2": {
                "beforePatchRowNumber": 84,
                "afterPatchRowNumber": 84,
                "PatchRowcode": "         Don't include prefix of -- for sqoop options."
            },
            "3": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 85,
                "PatchRowcode": "+    :param libjars: Optional Comma separated jar files to include in the classpath."
            },
            "4": {
                "beforePatchRowNumber": 85,
                "afterPatchRowNumber": 86,
                "PatchRowcode": "     \"\"\""
            },
            "5": {
                "beforePatchRowNumber": 86,
                "afterPatchRowNumber": 87,
                "PatchRowcode": " "
            },
            "6": {
                "beforePatchRowNumber": 87,
                "afterPatchRowNumber": 88,
                "PatchRowcode": "     template_fields: Sequence[str] = ("
            },
            "7": {
                "beforePatchRowNumber": 150,
                "afterPatchRowNumber": 151,
                "PatchRowcode": "         extra_import_options: dict[str, Any] | None = None,"
            },
            "8": {
                "beforePatchRowNumber": 151,
                "afterPatchRowNumber": 152,
                "PatchRowcode": "         extra_export_options: dict[str, Any] | None = None,"
            },
            "9": {
                "beforePatchRowNumber": 152,
                "afterPatchRowNumber": 153,
                "PatchRowcode": "         schema: str | None = None,"
            },
            "10": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 154,
                "PatchRowcode": "+        libjars: str | None = None,"
            },
            "11": {
                "beforePatchRowNumber": 153,
                "afterPatchRowNumber": 155,
                "PatchRowcode": "         **kwargs: Any,"
            },
            "12": {
                "beforePatchRowNumber": 154,
                "afterPatchRowNumber": 156,
                "PatchRowcode": "     ) -> None:"
            },
            "13": {
                "beforePatchRowNumber": 155,
                "afterPatchRowNumber": 157,
                "PatchRowcode": "         super().__init__(**kwargs)"
            },
            "14": {
                "beforePatchRowNumber": 187,
                "afterPatchRowNumber": 189,
                "PatchRowcode": "         self.extra_export_options = extra_export_options or {}"
            },
            "15": {
                "beforePatchRowNumber": 188,
                "afterPatchRowNumber": 190,
                "PatchRowcode": "         self.hook: SqoopHook | None = None"
            },
            "16": {
                "beforePatchRowNumber": 189,
                "afterPatchRowNumber": 191,
                "PatchRowcode": "         self.schema = schema"
            },
            "17": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 192,
                "PatchRowcode": "+        self.libjars = libjars"
            },
            "18": {
                "beforePatchRowNumber": 190,
                "afterPatchRowNumber": 193,
                "PatchRowcode": " "
            },
            "19": {
                "beforePatchRowNumber": 191,
                "afterPatchRowNumber": 194,
                "PatchRowcode": "     def execute(self, context: Context) -> None:"
            },
            "20": {
                "beforePatchRowNumber": 192,
                "afterPatchRowNumber": 195,
                "PatchRowcode": "         \"\"\"Execute sqoop job\"\"\""
            },
            "21": {
                "beforePatchRowNumber": 265,
                "afterPatchRowNumber": 268,
                "PatchRowcode": "             hcatalog_database=self.hcatalog_database,"
            },
            "22": {
                "beforePatchRowNumber": 266,
                "afterPatchRowNumber": 269,
                "PatchRowcode": "             hcatalog_table=self.hcatalog_table,"
            },
            "23": {
                "beforePatchRowNumber": 267,
                "afterPatchRowNumber": 270,
                "PatchRowcode": "             properties=self.properties,"
            },
            "24": {
                "beforePatchRowNumber": "",
                "afterPatchRowNumber": 271,
                "PatchRowcode": "+            libjars=self.libjars,"
            },
            "25": {
                "beforePatchRowNumber": 268,
                "afterPatchRowNumber": 272,
                "PatchRowcode": "         )"
            }
        },
        "frontPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1 operator\"\"\"",
            "from __future__ import annotations",
            "",
            "import os",
            "import signal",
            "from typing import TYPE_CHECKING, Any, Sequence",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.models import BaseOperator",
            "from airflow.providers.apache.sqoop.hooks.sqoop import SqoopHook",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.utils.context import Context",
            "",
            "",
            "class SqoopOperator(BaseOperator):",
            "    \"\"\"",
            "    Execute a Sqoop job.",
            "    Documentation for Apache Sqoop can be found here:",
            "    https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html",
            "",
            "    :param conn_id: str",
            "    :param cmd_type: str specify command to execute \"export\" or \"import\"",
            "    :param schema: Schema name",
            "    :param table: Table to read",
            "    :param query: Import result of arbitrary SQL query. Instead of using the table,",
            "        columns and where arguments, you can specify a SQL statement with the query",
            "        argument. Must also specify a destination directory with target_dir.",
            "    :param target_dir: HDFS destination directory where the data",
            "        from the rdbms will be written",
            "    :param append: Append data to an existing dataset in HDFS",
            "    :param file_type: \"avro\", \"sequence\", \"text\" Imports data to",
            "        into the specified format. Defaults to text.",
            "    :param columns: <col,col,col> Columns to import from table",
            "    :param num_mappers: Use n mapper tasks to import/export in parallel",
            "    :param split_by: Column of the table used to split work units",
            "    :param where: WHERE clause to use during import",
            "    :param export_dir: HDFS Hive database directory to export to the rdbms",
            "    :param input_null_string: The string to be interpreted as null",
            "        for string columns",
            "    :param input_null_non_string: The string to be interpreted as null",
            "        for non-string columns",
            "    :param staging_table: The table in which data will be staged before",
            "        being inserted into the destination table",
            "    :param clear_staging_table: Indicate that any data present in the",
            "        staging table can be deleted",
            "    :param enclosed_by: Sets a required field enclosing character",
            "    :param escaped_by: Sets the escape character",
            "    :param input_fields_terminated_by: Sets the input field separator",
            "    :param input_lines_terminated_by: Sets the input end-of-line character",
            "    :param input_optionally_enclosed_by: Sets a field enclosing character",
            "    :param batch: Use batch mode for underlying statement execution",
            "    :param direct: Use direct export fast path",
            "    :param driver: Manually specify JDBC driver class to use",
            "    :param verbose: Switch to more verbose logging for debug purposes",
            "    :param relaxed_isolation: use read uncommitted isolation level",
            "    :param hcatalog_database: Specifies the database name for the HCatalog table",
            "    :param hcatalog_table: The argument value for this option is the HCatalog table",
            "    :param create_hcatalog_table: Have sqoop create the hcatalog table passed",
            "        in or not",
            "    :param properties: additional JVM properties passed to sqoop",
            "    :param extra_import_options: Extra import options to pass as dict.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    :param extra_export_options: Extra export options to pass as dict.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    \"\"\"",
            "",
            "    template_fields: Sequence[str] = (",
            "        \"conn_id\",",
            "        \"cmd_type\",",
            "        \"table\",",
            "        \"query\",",
            "        \"target_dir\",",
            "        \"file_type\",",
            "        \"columns\",",
            "        \"split_by\",",
            "        \"where\",",
            "        \"export_dir\",",
            "        \"input_null_string\",",
            "        \"input_null_non_string\",",
            "        \"staging_table\",",
            "        \"enclosed_by\",",
            "        \"escaped_by\",",
            "        \"input_fields_terminated_by\",",
            "        \"input_lines_terminated_by\",",
            "        \"input_optionally_enclosed_by\",",
            "        \"properties\",",
            "        \"extra_import_options\",",
            "        \"driver\",",
            "        \"extra_export_options\",",
            "        \"hcatalog_database\",",
            "        \"hcatalog_table\",",
            "        \"schema\",",
            "    )",
            "    template_fields_renderers = {\"query\": \"sql\"}",
            "    ui_color = \"#7D8CA4\"",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        conn_id: str = \"sqoop_default\",",
            "        cmd_type: str = \"import\",",
            "        table: str | None = None,",
            "        query: str | None = None,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        num_mappers: int | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        direct: bool = False,",
            "        driver: Any | None = None,",
            "        verbose: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        properties: dict[str, Any] | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        create_hcatalog_table: bool = False,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        super().__init__(**kwargs)",
            "        self.conn_id = conn_id",
            "        self.cmd_type = cmd_type",
            "        self.table = table",
            "        self.query = query",
            "        self.target_dir = target_dir",
            "        self.append = append",
            "        self.file_type = file_type",
            "        self.columns = columns",
            "        self.num_mappers = num_mappers",
            "        self.split_by = split_by",
            "        self.where = where",
            "        self.export_dir = export_dir",
            "        self.input_null_string = input_null_string",
            "        self.input_null_non_string = input_null_non_string",
            "        self.staging_table = staging_table",
            "        self.clear_staging_table = clear_staging_table",
            "        self.enclosed_by = enclosed_by",
            "        self.escaped_by = escaped_by",
            "        self.input_fields_terminated_by = input_fields_terminated_by",
            "        self.input_lines_terminated_by = input_lines_terminated_by",
            "        self.input_optionally_enclosed_by = input_optionally_enclosed_by",
            "        self.batch = batch",
            "        self.direct = direct",
            "        self.driver = driver",
            "        self.verbose = verbose",
            "        self.relaxed_isolation = relaxed_isolation",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.create_hcatalog_table = create_hcatalog_table",
            "        self.properties = properties",
            "        self.extra_import_options = extra_import_options or {}",
            "        self.extra_export_options = extra_export_options or {}",
            "        self.hook: SqoopHook | None = None",
            "        self.schema = schema",
            "",
            "    def execute(self, context: Context) -> None:",
            "        \"\"\"Execute sqoop job\"\"\"",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "",
            "        if self.cmd_type == \"export\":",
            "            self.hook.export_table(",
            "                table=self.table,  # type: ignore",
            "                export_dir=self.export_dir,",
            "                input_null_string=self.input_null_string,",
            "                input_null_non_string=self.input_null_non_string,",
            "                staging_table=self.staging_table,",
            "                clear_staging_table=self.clear_staging_table,",
            "                enclosed_by=self.enclosed_by,",
            "                escaped_by=self.escaped_by,",
            "                input_fields_terminated_by=self.input_fields_terminated_by,",
            "                input_lines_terminated_by=self.input_lines_terminated_by,",
            "                input_optionally_enclosed_by=self.input_optionally_enclosed_by,",
            "                batch=self.batch,",
            "                relaxed_isolation=self.relaxed_isolation,",
            "                extra_export_options=self.extra_export_options,",
            "                schema=self.schema,",
            "            )",
            "        elif self.cmd_type == \"import\":",
            "            # add create hcatalog table to extra import options if option passed",
            "            # if new params are added to constructor can pass them in here",
            "            # so don't modify sqoop_hook for each param",
            "            if self.create_hcatalog_table:",
            "                self.extra_import_options[\"create-hcatalog-table\"] = \"\"",
            "",
            "            if self.table and self.query:",
            "                raise AirflowException(\"Cannot specify query and table together. Need to specify either or.\")",
            "",
            "            if self.table:",
            "                self.hook.import_table(",
            "                    table=self.table,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    columns=self.columns,",
            "                    split_by=self.split_by,",
            "                    where=self.where,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                    extra_import_options=self.extra_import_options,",
            "                    schema=self.schema,",
            "                )",
            "            elif self.query:",
            "                self.hook.import_query(",
            "                    query=self.query,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    split_by=self.split_by,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                    extra_import_options=self.extra_import_options,",
            "                )",
            "            else:",
            "                raise AirflowException(\"Provide query or table parameter to import using Sqoop\")",
            "        else:",
            "            raise AirflowException(\"cmd_type should be 'import' or 'export'\")",
            "",
            "    def on_kill(self) -> None:",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "        self.log.info(\"Sending SIGTERM signal to bash process group\")",
            "        os.killpg(os.getpgid(self.hook.sub_process_pid), signal.SIGTERM)",
            "",
            "    def _get_hook(self) -> SqoopHook:",
            "        return SqoopHook(",
            "            conn_id=self.conn_id,",
            "            verbose=self.verbose,",
            "            num_mappers=self.num_mappers,",
            "            hcatalog_database=self.hcatalog_database,",
            "            hcatalog_table=self.hcatalog_table,",
            "            properties=self.properties,",
            "        )"
        ],
        "afterPatchFile": [
            "#",
            "# Licensed to the Apache Software Foundation (ASF) under one",
            "# or more contributor license agreements.  See the NOTICE file",
            "# distributed with this work for additional information",
            "# regarding copyright ownership.  The ASF licenses this file",
            "# to you under the Apache License, Version 2.0 (the",
            "# \"License\"); you may not use this file except in compliance",
            "# with the License.  You may obtain a copy of the License at",
            "#",
            "#   http://www.apache.org/licenses/LICENSE-2.0",
            "#",
            "# Unless required by applicable law or agreed to in writing,",
            "# software distributed under the License is distributed on an",
            "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
            "# KIND, either express or implied.  See the License for the",
            "# specific language governing permissions and limitations",
            "# under the License.",
            "\"\"\"This module contains a sqoop 1 operator\"\"\"",
            "from __future__ import annotations",
            "",
            "import os",
            "import signal",
            "from typing import TYPE_CHECKING, Any, Sequence",
            "",
            "from airflow.exceptions import AirflowException",
            "from airflow.models import BaseOperator",
            "from airflow.providers.apache.sqoop.hooks.sqoop import SqoopHook",
            "",
            "if TYPE_CHECKING:",
            "    from airflow.utils.context import Context",
            "",
            "",
            "class SqoopOperator(BaseOperator):",
            "    \"\"\"",
            "    Execute a Sqoop job.",
            "    Documentation for Apache Sqoop can be found here:",
            "    https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html",
            "",
            "    :param conn_id: str",
            "    :param cmd_type: str specify command to execute \"export\" or \"import\"",
            "    :param schema: Schema name",
            "    :param table: Table to read",
            "    :param query: Import result of arbitrary SQL query. Instead of using the table,",
            "        columns and where arguments, you can specify a SQL statement with the query",
            "        argument. Must also specify a destination directory with target_dir.",
            "    :param target_dir: HDFS destination directory where the data",
            "        from the rdbms will be written",
            "    :param append: Append data to an existing dataset in HDFS",
            "    :param file_type: \"avro\", \"sequence\", \"text\" Imports data to",
            "        into the specified format. Defaults to text.",
            "    :param columns: <col,col,col> Columns to import from table",
            "    :param num_mappers: Use n mapper tasks to import/export in parallel",
            "    :param split_by: Column of the table used to split work units",
            "    :param where: WHERE clause to use during import",
            "    :param export_dir: HDFS Hive database directory to export to the rdbms",
            "    :param input_null_string: The string to be interpreted as null",
            "        for string columns",
            "    :param input_null_non_string: The string to be interpreted as null",
            "        for non-string columns",
            "    :param staging_table: The table in which data will be staged before",
            "        being inserted into the destination table",
            "    :param clear_staging_table: Indicate that any data present in the",
            "        staging table can be deleted",
            "    :param enclosed_by: Sets a required field enclosing character",
            "    :param escaped_by: Sets the escape character",
            "    :param input_fields_terminated_by: Sets the input field separator",
            "    :param input_lines_terminated_by: Sets the input end-of-line character",
            "    :param input_optionally_enclosed_by: Sets a field enclosing character",
            "    :param batch: Use batch mode for underlying statement execution",
            "    :param direct: Use direct export fast path",
            "    :param driver: Manually specify JDBC driver class to use",
            "    :param verbose: Switch to more verbose logging for debug purposes",
            "    :param relaxed_isolation: use read uncommitted isolation level",
            "    :param hcatalog_database: Specifies the database name for the HCatalog table",
            "    :param hcatalog_table: The argument value for this option is the HCatalog table",
            "    :param create_hcatalog_table: Have sqoop create the hcatalog table passed",
            "        in or not",
            "    :param properties: additional JVM properties passed to sqoop",
            "    :param extra_import_options: Extra import options to pass as dict.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    :param extra_export_options: Extra export options to pass as dict.",
            "        If a key doesn't have a value, just pass an empty string to it.",
            "        Don't include prefix of -- for sqoop options.",
            "    :param libjars: Optional Comma separated jar files to include in the classpath.",
            "    \"\"\"",
            "",
            "    template_fields: Sequence[str] = (",
            "        \"conn_id\",",
            "        \"cmd_type\",",
            "        \"table\",",
            "        \"query\",",
            "        \"target_dir\",",
            "        \"file_type\",",
            "        \"columns\",",
            "        \"split_by\",",
            "        \"where\",",
            "        \"export_dir\",",
            "        \"input_null_string\",",
            "        \"input_null_non_string\",",
            "        \"staging_table\",",
            "        \"enclosed_by\",",
            "        \"escaped_by\",",
            "        \"input_fields_terminated_by\",",
            "        \"input_lines_terminated_by\",",
            "        \"input_optionally_enclosed_by\",",
            "        \"properties\",",
            "        \"extra_import_options\",",
            "        \"driver\",",
            "        \"extra_export_options\",",
            "        \"hcatalog_database\",",
            "        \"hcatalog_table\",",
            "        \"schema\",",
            "    )",
            "    template_fields_renderers = {\"query\": \"sql\"}",
            "    ui_color = \"#7D8CA4\"",
            "",
            "    def __init__(",
            "        self,",
            "        *,",
            "        conn_id: str = \"sqoop_default\",",
            "        cmd_type: str = \"import\",",
            "        table: str | None = None,",
            "        query: str | None = None,",
            "        target_dir: str | None = None,",
            "        append: bool = False,",
            "        file_type: str = \"text\",",
            "        columns: str | None = None,",
            "        num_mappers: int | None = None,",
            "        split_by: str | None = None,",
            "        where: str | None = None,",
            "        export_dir: str | None = None,",
            "        input_null_string: str | None = None,",
            "        input_null_non_string: str | None = None,",
            "        staging_table: str | None = None,",
            "        clear_staging_table: bool = False,",
            "        enclosed_by: str | None = None,",
            "        escaped_by: str | None = None,",
            "        input_fields_terminated_by: str | None = None,",
            "        input_lines_terminated_by: str | None = None,",
            "        input_optionally_enclosed_by: str | None = None,",
            "        batch: bool = False,",
            "        direct: bool = False,",
            "        driver: Any | None = None,",
            "        verbose: bool = False,",
            "        relaxed_isolation: bool = False,",
            "        properties: dict[str, Any] | None = None,",
            "        hcatalog_database: str | None = None,",
            "        hcatalog_table: str | None = None,",
            "        create_hcatalog_table: bool = False,",
            "        extra_import_options: dict[str, Any] | None = None,",
            "        extra_export_options: dict[str, Any] | None = None,",
            "        schema: str | None = None,",
            "        libjars: str | None = None,",
            "        **kwargs: Any,",
            "    ) -> None:",
            "        super().__init__(**kwargs)",
            "        self.conn_id = conn_id",
            "        self.cmd_type = cmd_type",
            "        self.table = table",
            "        self.query = query",
            "        self.target_dir = target_dir",
            "        self.append = append",
            "        self.file_type = file_type",
            "        self.columns = columns",
            "        self.num_mappers = num_mappers",
            "        self.split_by = split_by",
            "        self.where = where",
            "        self.export_dir = export_dir",
            "        self.input_null_string = input_null_string",
            "        self.input_null_non_string = input_null_non_string",
            "        self.staging_table = staging_table",
            "        self.clear_staging_table = clear_staging_table",
            "        self.enclosed_by = enclosed_by",
            "        self.escaped_by = escaped_by",
            "        self.input_fields_terminated_by = input_fields_terminated_by",
            "        self.input_lines_terminated_by = input_lines_terminated_by",
            "        self.input_optionally_enclosed_by = input_optionally_enclosed_by",
            "        self.batch = batch",
            "        self.direct = direct",
            "        self.driver = driver",
            "        self.verbose = verbose",
            "        self.relaxed_isolation = relaxed_isolation",
            "        self.hcatalog_database = hcatalog_database",
            "        self.hcatalog_table = hcatalog_table",
            "        self.create_hcatalog_table = create_hcatalog_table",
            "        self.properties = properties",
            "        self.extra_import_options = extra_import_options or {}",
            "        self.extra_export_options = extra_export_options or {}",
            "        self.hook: SqoopHook | None = None",
            "        self.schema = schema",
            "        self.libjars = libjars",
            "",
            "    def execute(self, context: Context) -> None:",
            "        \"\"\"Execute sqoop job\"\"\"",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "",
            "        if self.cmd_type == \"export\":",
            "            self.hook.export_table(",
            "                table=self.table,  # type: ignore",
            "                export_dir=self.export_dir,",
            "                input_null_string=self.input_null_string,",
            "                input_null_non_string=self.input_null_non_string,",
            "                staging_table=self.staging_table,",
            "                clear_staging_table=self.clear_staging_table,",
            "                enclosed_by=self.enclosed_by,",
            "                escaped_by=self.escaped_by,",
            "                input_fields_terminated_by=self.input_fields_terminated_by,",
            "                input_lines_terminated_by=self.input_lines_terminated_by,",
            "                input_optionally_enclosed_by=self.input_optionally_enclosed_by,",
            "                batch=self.batch,",
            "                relaxed_isolation=self.relaxed_isolation,",
            "                extra_export_options=self.extra_export_options,",
            "                schema=self.schema,",
            "            )",
            "        elif self.cmd_type == \"import\":",
            "            # add create hcatalog table to extra import options if option passed",
            "            # if new params are added to constructor can pass them in here",
            "            # so don't modify sqoop_hook for each param",
            "            if self.create_hcatalog_table:",
            "                self.extra_import_options[\"create-hcatalog-table\"] = \"\"",
            "",
            "            if self.table and self.query:",
            "                raise AirflowException(\"Cannot specify query and table together. Need to specify either or.\")",
            "",
            "            if self.table:",
            "                self.hook.import_table(",
            "                    table=self.table,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    columns=self.columns,",
            "                    split_by=self.split_by,",
            "                    where=self.where,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                    extra_import_options=self.extra_import_options,",
            "                    schema=self.schema,",
            "                )",
            "            elif self.query:",
            "                self.hook.import_query(",
            "                    query=self.query,",
            "                    target_dir=self.target_dir,",
            "                    append=self.append,",
            "                    file_type=self.file_type,",
            "                    split_by=self.split_by,",
            "                    direct=self.direct,",
            "                    driver=self.driver,",
            "                    extra_import_options=self.extra_import_options,",
            "                )",
            "            else:",
            "                raise AirflowException(\"Provide query or table parameter to import using Sqoop\")",
            "        else:",
            "            raise AirflowException(\"cmd_type should be 'import' or 'export'\")",
            "",
            "    def on_kill(self) -> None:",
            "        if self.hook is None:",
            "            self.hook = self._get_hook()",
            "        self.log.info(\"Sending SIGTERM signal to bash process group\")",
            "        os.killpg(os.getpgid(self.hook.sub_process_pid), signal.SIGTERM)",
            "",
            "    def _get_hook(self) -> SqoopHook:",
            "        return SqoopHook(",
            "            conn_id=self.conn_id,",
            "            verbose=self.verbose,",
            "            num_mappers=self.num_mappers,",
            "            hcatalog_database=self.hcatalog_database,",
            "            hcatalog_table=self.hcatalog_table,",
            "            properties=self.properties,",
            "            libjars=self.libjars,",
            "        )"
        ],
        "action": [
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0",
            "0",
            "0",
            "0",
            "0",
            "0",
            "-1",
            "0"
        ],
        "dele_reviseLocation": {},
        "addLocation": [
            "cps.cli",
            "airflow.providers.apache.sqoop.operators.sqoop.SqoopOperator.self",
            "airflow.providers.apache.sqoop.operators.sqoop.SqoopOperator.execute",
            "airflow.providers.apache.sqoop.operators.sqoop.SqoopOperator.on_kill",
            "airflow.providers.apache.sqoop.operators.sqoop.SqoopOperator",
            "airflow.providers.apache.sqoop.operators.sqoop.SqoopOperator.template_fields_renderers"
        ]
    }
}